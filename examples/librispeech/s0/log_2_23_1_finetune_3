/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_21_rnnt_bias_0-3word_finetune.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_21_rnnt_bias_loss_2_class_3word_finetune/ddp_init
2023-02-24 18:56:25,427 INFO training on multiple gpus, this gpu 7
2023-02-24 18:56:25,427 INFO training on multiple gpus, this gpu 1
2023-02-24 18:56:25,427 INFO training on multiple gpus, this gpu 5
2023-02-24 18:56:25,428 INFO training on multiple gpus, this gpu 6
2023-02-24 18:56:25,439 INFO training on multiple gpus, this gpu 2
2023-02-24 18:56:25,439 INFO training on multiple gpus, this gpu 3
2023-02-24 18:56:25,439 INFO training on multiple gpus, this gpu 0
2023-02-24 18:56:25,456 INFO training on multiple gpus, this gpu 4
2023-02-24 19:21:07,859 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-24 19:21:09,816 INFO Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-24 19:21:09,825 INFO Added key: store_based_barrier_key:1 to store for rank: 7
2023-02-24 19:21:10,337 INFO Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-24 19:21:10,717 INFO Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-24 19:21:11,445 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-24 19:21:13,058 INFO Added key: store_based_barrier_key:1 to store for rank: 6
2023-02-24 19:21:19,174 INFO Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=8, worker_count=7, timeout=0:30:00)
2023-02-24 19:21:20,393 INFO Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=8, worker_count=7, timeout=0:30:00)
2023-02-24 19:21:21,389 INFO Waiting in store based barrier to initialize process group for rank: 4, key: store_based_barrier_key:1 (world_size=8, worker_count=7, timeout=0:30:00)
2023-02-24 19:21:21,551 INFO Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=8, worker_count=7, timeout=0:30:00)
2023-02-24 19:21:22,607 INFO Waiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=8, worker_count=7, timeout=0:30:00)
2023-02-24 19:21:23,673 INFO Waiting in store based barrier to initialize process group for rank: 6, key: store_based_barrier_key:1 (world_size=8, worker_count=7, timeout=0:30:00)
2023-02-24 19:21:25,466 INFO Added key: store_based_barrier_key:1 to store for rank: 5
2023-02-24 19:21:25,608 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-24 19:21:25,672 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-24 19:21:25,905 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-24 19:21:26,460 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-24 19:21:26,585 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-24 19:21:27,073 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-24 19:21:30,314 INFO Waiting in store based barrier to initialize process group for rank: 7, key: store_based_barrier_key:1 (world_size=8, worker_count=8, timeout=0:30:00)
2023-02-24 19:21:30,314 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-24 19:21:30,799 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-24 19:21:36,828 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/18.pt for GPU
2023-02-24 19:21:36,852 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/18.pt for GPU
2023-02-24 19:21:36,877 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/18.pt for GPU
2023-02-24 19:21:36,908 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/18.pt for GPU
2023-02-24 19:21:36,931 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/18.pt for GPU
2023-02-24 19:21:36,960 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/18.pt for GPU
2023-02-24 19:21:36,982 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/18.pt for GPU
2023-02-24 19:21:37,006 INFO Checkpoint: loading from checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/18.pt for GPU
2023-02-24 19:22:09,234 INFO Epoch 19 TRAIN info lr 4e-08
2023-02-24 19:22:09,236 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-24 19:22:09,291 INFO Epoch 19 TRAIN info lr 4e-08
2023-02-24 19:22:09,291 INFO Epoch 19 TRAIN info lr 4e-08
2023-02-24 19:22:09,293 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 19:22:09,293 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-24 19:22:09,307 INFO Epoch 19 TRAIN info lr 4e-08
2023-02-24 19:22:09,310 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-24 19:22:09,427 INFO Epoch 19 TRAIN info lr 4e-08
2023-02-24 19:22:09,429 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-24 19:22:09,439 INFO Epoch 19 TRAIN info lr 4e-08
2023-02-24 19:22:09,442 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-24 19:22:09,504 INFO Epoch 19 TRAIN info lr 4e-08
2023-02-24 19:22:09,507 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-24 19:22:09,538 INFO Epoch 19 TRAIN info lr 4e-08
2023-02-24 19:22:09,552 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-24 19:23:13,174 DEBUG TRAIN Batch 19/0 loss 11.796243 loss_att 11.206171 loss_ctc 13.889701 loss_rnnt 11.522443 hw_loss 0.211286 lr 0.00039722 rank 0
2023-02-24 19:23:13,198 DEBUG TRAIN Batch 19/0 loss 12.778885 loss_att 11.394809 loss_ctc 15.228614 loss_rnnt 12.599762 hw_loss 0.242452 lr 0.00039722 rank 4
2023-02-24 19:23:13,201 DEBUG TRAIN Batch 19/0 loss 6.594926 loss_att 6.934876 loss_ctc 7.970612 loss_rnnt 6.146877 hw_loss 0.368690 lr 0.00039722 rank 5
2023-02-24 19:23:13,234 DEBUG TRAIN Batch 19/0 loss 12.472253 loss_att 11.318713 loss_ctc 14.796119 loss_rnnt 12.207431 hw_loss 0.348151 lr 0.00039722 rank 6
2023-02-24 19:23:13,252 DEBUG TRAIN Batch 19/0 loss 8.833218 loss_att 9.121189 loss_ctc 11.954356 loss_rnnt 8.240167 hw_loss 0.223697 lr 0.00039722 rank 2
2023-02-24 19:23:13,255 DEBUG TRAIN Batch 19/0 loss 10.096624 loss_att 8.333745 loss_ctc 11.343064 loss_rnnt 10.150199 hw_loss 0.249016 lr 0.00039722 rank 7
2023-02-24 19:23:13,261 DEBUG TRAIN Batch 19/0 loss 8.213748 loss_att 7.420606 loss_ctc 10.787244 loss_rnnt 7.877601 hw_loss 0.284331 lr 0.00039722 rank 1
2023-02-24 19:23:13,305 DEBUG TRAIN Batch 19/0 loss 12.406432 loss_att 11.854578 loss_ctc 15.248127 loss_rnnt 11.985182 hw_loss 0.286366 lr 0.00039722 rank 3
2023-02-24 19:24:28,579 DEBUG TRAIN Batch 19/100 loss 6.841384 loss_att 10.255938 loss_ctc 8.064863 loss_rnnt 5.907000 hw_loss 0.165644 lr 0.00039709 rank 0
2023-02-24 19:24:28,579 DEBUG TRAIN Batch 19/100 loss 9.896824 loss_att 13.598037 loss_ctc 17.041363 loss_rnnt 8.056606 hw_loss 0.276317 lr 0.00039709 rank 1
2023-02-24 19:24:28,586 DEBUG TRAIN Batch 19/100 loss 5.052443 loss_att 9.358408 loss_ctc 9.293287 loss_rnnt 3.538206 hw_loss 0.164246 lr 0.00039709 rank 6
2023-02-24 19:24:28,587 DEBUG TRAIN Batch 19/100 loss 7.449590 loss_att 8.891913 loss_ctc 10.019827 loss_rnnt 6.722886 hw_loss 0.179140 lr 0.00039709 rank 4
2023-02-24 19:24:28,587 DEBUG TRAIN Batch 19/100 loss 7.251447 loss_att 13.462370 loss_ctc 16.042013 loss_rnnt 4.727378 hw_loss 0.205892 lr 0.00039709 rank 2
2023-02-24 19:24:28,587 DEBUG TRAIN Batch 19/100 loss 3.207988 loss_att 6.940070 loss_ctc 6.140491 loss_rnnt 1.974882 hw_loss 0.179417 lr 0.00039709 rank 5
2023-02-24 19:24:28,588 DEBUG TRAIN Batch 19/100 loss 4.345780 loss_att 7.915513 loss_ctc 6.467419 loss_rnnt 3.225438 hw_loss 0.231582 lr 0.00039709 rank 3
2023-02-24 19:24:28,598 DEBUG TRAIN Batch 19/100 loss 11.875593 loss_att 18.897680 loss_ctc 16.473715 loss_rnnt 9.755187 hw_loss 0.192949 lr 0.00039709 rank 7
2023-02-24 19:25:42,761 DEBUG TRAIN Batch 19/200 loss 16.063368 loss_att 15.154770 loss_ctc 25.246481 loss_rnnt 14.886962 hw_loss 0.250708 lr 0.00039697 rank 1
2023-02-24 19:25:42,763 DEBUG TRAIN Batch 19/200 loss 5.816358 loss_att 10.427977 loss_ctc 6.603951 loss_rnnt 4.613607 hw_loss 0.328903 lr 0.00039697 rank 6
2023-02-24 19:25:42,768 DEBUG TRAIN Batch 19/200 loss 5.995171 loss_att 10.905057 loss_ctc 8.348023 loss_rnnt 4.587340 hw_loss 0.210262 lr 0.00039697 rank 0
2023-02-24 19:25:42,769 DEBUG TRAIN Batch 19/200 loss 3.680242 loss_att 7.166745 loss_ctc 6.940997 loss_rnnt 2.416296 hw_loss 0.247271 lr 0.00039697 rank 7
2023-02-24 19:25:42,770 DEBUG TRAIN Batch 19/200 loss 3.877093 loss_att 7.175883 loss_ctc 6.012218 loss_rnnt 2.818099 hw_loss 0.214787 lr 0.00039697 rank 3
2023-02-24 19:25:42,770 DEBUG TRAIN Batch 19/200 loss 6.014657 loss_att 11.571102 loss_ctc 11.637320 loss_rnnt 3.963440 hw_loss 0.356699 lr 0.00039697 rank 2
2023-02-24 19:25:42,773 DEBUG TRAIN Batch 19/200 loss 16.874304 loss_att 19.960615 loss_ctc 23.184597 loss_rnnt 15.315077 hw_loss 0.188613 lr 0.00039697 rank 5
2023-02-24 19:25:42,774 DEBUG TRAIN Batch 19/200 loss 6.361095 loss_att 9.997879 loss_ctc 8.888638 loss_rnnt 5.150383 hw_loss 0.274407 lr 0.00039697 rank 4
2023-02-24 19:26:56,414 DEBUG TRAIN Batch 19/300 loss 9.201475 loss_att 11.577791 loss_ctc 14.540735 loss_rnnt 7.899329 hw_loss 0.215590 lr 0.00039684 rank 1
2023-02-24 19:26:56,416 DEBUG TRAIN Batch 19/300 loss 6.984248 loss_att 10.240519 loss_ctc 8.993200 loss_rnnt 5.903116 hw_loss 0.303784 lr 0.00039684 rank 0
2023-02-24 19:26:56,416 DEBUG TRAIN Batch 19/300 loss 6.771069 loss_att 13.659983 loss_ctc 11.809440 loss_rnnt 4.606227 hw_loss 0.216143 lr 0.00039684 rank 2
2023-02-24 19:26:56,418 DEBUG TRAIN Batch 19/300 loss 12.439530 loss_att 17.734875 loss_ctc 20.399540 loss_rnnt 10.188093 hw_loss 0.245687 lr 0.00039684 rank 5
2023-02-24 19:26:56,421 DEBUG TRAIN Batch 19/300 loss 11.695357 loss_att 18.365540 loss_ctc 17.197645 loss_rnnt 9.543645 hw_loss 0.157568 lr 0.00039684 rank 7
2023-02-24 19:26:56,427 DEBUG TRAIN Batch 19/300 loss 15.728457 loss_att 19.706812 loss_ctc 22.506590 loss_rnnt 13.967639 hw_loss 0.115120 lr 0.00039684 rank 3
2023-02-24 19:26:56,443 DEBUG TRAIN Batch 19/300 loss 5.967801 loss_att 9.632314 loss_ctc 10.855365 loss_rnnt 4.478194 hw_loss 0.196929 lr 0.00039684 rank 4
2023-02-24 19:26:56,449 DEBUG TRAIN Batch 19/300 loss 11.046931 loss_att 14.024454 loss_ctc 15.543306 loss_rnnt 9.778535 hw_loss 0.137578 lr 0.00039684 rank 6
2023-02-24 19:28:11,183 DEBUG TRAIN Batch 19/400 loss 9.214138 loss_att 13.406832 loss_ctc 13.665498 loss_rnnt 7.635089 hw_loss 0.275616 lr 0.00039672 rank 5
2023-02-24 19:28:11,199 DEBUG TRAIN Batch 19/400 loss 17.431089 loss_att 21.275660 loss_ctc 20.667114 loss_rnnt 16.163355 hw_loss 0.126281 lr 0.00039672 rank 1
2023-02-24 19:28:11,200 DEBUG TRAIN Batch 19/400 loss 4.430009 loss_att 7.551529 loss_ctc 6.730749 loss_rnnt 3.396112 hw_loss 0.192802 lr 0.00039672 rank 3
2023-02-24 19:28:11,201 DEBUG TRAIN Batch 19/400 loss 6.204551 loss_att 8.473982 loss_ctc 9.444086 loss_rnnt 5.238904 hw_loss 0.149667 lr 0.00039672 rank 0
2023-02-24 19:28:11,203 DEBUG TRAIN Batch 19/400 loss 6.209550 loss_att 8.910516 loss_ctc 12.635702 loss_rnnt 4.689609 hw_loss 0.230491 lr 0.00039672 rank 2
2023-02-24 19:28:11,203 DEBUG TRAIN Batch 19/400 loss 12.364941 loss_att 17.085739 loss_ctc 18.445415 loss_rnnt 10.432360 hw_loss 0.333172 lr 0.00039672 rank 4
2023-02-24 19:28:11,208 DEBUG TRAIN Batch 19/400 loss 8.903824 loss_att 10.902258 loss_ctc 11.314116 loss_rnnt 8.084673 hw_loss 0.183921 lr 0.00039672 rank 7
2023-02-24 19:28:11,247 DEBUG TRAIN Batch 19/400 loss 8.679599 loss_att 11.245855 loss_ctc 11.766134 loss_rnnt 7.650992 hw_loss 0.194657 lr 0.00039672 rank 6
2023-02-24 19:29:24,345 DEBUG TRAIN Batch 19/500 loss 14.658431 loss_att 19.510723 loss_ctc 23.657764 loss_rnnt 12.327809 hw_loss 0.300471 lr 0.00039659 rank 0
2023-02-24 19:29:24,350 DEBUG TRAIN Batch 19/500 loss 7.048287 loss_att 8.800062 loss_ctc 9.493735 loss_rnnt 6.244392 hw_loss 0.239024 lr 0.00039659 rank 7
2023-02-24 19:29:24,351 DEBUG TRAIN Batch 19/500 loss 6.050591 loss_att 9.121036 loss_ctc 11.757162 loss_rnnt 4.557064 hw_loss 0.222303 lr 0.00039659 rank 3
2023-02-24 19:29:24,352 DEBUG TRAIN Batch 19/500 loss 8.061490 loss_att 10.575834 loss_ctc 10.578382 loss_rnnt 7.098188 hw_loss 0.234087 lr 0.00039659 rank 4
2023-02-24 19:29:24,359 DEBUG TRAIN Batch 19/500 loss 7.934329 loss_att 10.487757 loss_ctc 11.729128 loss_rnnt 6.788955 hw_loss 0.241343 lr 0.00039659 rank 5
2023-02-24 19:29:24,380 DEBUG TRAIN Batch 19/500 loss 6.805968 loss_att 9.856562 loss_ctc 9.753233 loss_rnnt 5.698275 hw_loss 0.196135 lr 0.00039659 rank 6
2023-02-24 19:29:24,388 DEBUG TRAIN Batch 19/500 loss 6.435310 loss_att 7.440742 loss_ctc 9.655896 loss_rnnt 5.665699 hw_loss 0.260837 lr 0.00039659 rank 2
2023-02-24 19:29:24,404 DEBUG TRAIN Batch 19/500 loss 5.884096 loss_att 7.915806 loss_ctc 7.329781 loss_rnnt 5.152562 hw_loss 0.248312 lr 0.00039659 rank 1
2023-02-24 19:30:37,829 DEBUG TRAIN Batch 19/600 loss 14.832607 loss_att 16.886711 loss_ctc 18.510696 loss_rnnt 13.833610 hw_loss 0.183312 lr 0.00039647 rank 1
2023-02-24 19:30:37,830 DEBUG TRAIN Batch 19/600 loss 5.191481 loss_att 7.270357 loss_ctc 7.411530 loss_rnnt 4.377916 hw_loss 0.190844 lr 0.00039647 rank 0
2023-02-24 19:30:37,830 DEBUG TRAIN Batch 19/600 loss 15.414856 loss_att 15.362057 loss_ctc 20.367363 loss_rnnt 14.604752 hw_loss 0.300617 lr 0.00039647 rank 6
2023-02-24 19:30:37,835 DEBUG TRAIN Batch 19/600 loss 10.119918 loss_att 10.176002 loss_ctc 13.633531 loss_rnnt 9.534345 hw_loss 0.198514 lr 0.00039647 rank 4
2023-02-24 19:30:37,836 DEBUG TRAIN Batch 19/600 loss 7.883113 loss_att 7.422164 loss_ctc 11.063143 loss_rnnt 7.377764 hw_loss 0.325377 lr 0.00039647 rank 3
2023-02-24 19:30:37,840 DEBUG TRAIN Batch 19/600 loss 12.049150 loss_att 12.975769 loss_ctc 16.588257 loss_rnnt 11.106768 hw_loss 0.284705 lr 0.00039647 rank 7
2023-02-24 19:30:37,844 DEBUG TRAIN Batch 19/600 loss 17.432587 loss_att 17.856112 loss_ctc 22.408428 loss_rnnt 16.561363 hw_loss 0.230762 lr 0.00039647 rank 5
2023-02-24 19:30:37,846 DEBUG TRAIN Batch 19/600 loss 5.667033 loss_att 7.142260 loss_ctc 8.114502 loss_rnnt 4.885453 hw_loss 0.300385 lr 0.00039647 rank 2
2023-02-24 19:31:53,705 DEBUG TRAIN Batch 19/700 loss 6.469347 loss_att 9.659414 loss_ctc 8.241243 loss_rnnt 5.531377 hw_loss 0.119446 lr 0.00039634 rank 7
2023-02-24 19:31:53,715 DEBUG TRAIN Batch 19/700 loss 5.593518 loss_att 9.976987 loss_ctc 7.622507 loss_rnnt 4.298056 hw_loss 0.277943 lr 0.00039634 rank 4
2023-02-24 19:31:53,716 DEBUG TRAIN Batch 19/700 loss 9.391939 loss_att 12.959506 loss_ctc 13.399257 loss_rnnt 8.091913 hw_loss 0.097880 lr 0.00039634 rank 3
2023-02-24 19:31:53,716 DEBUG TRAIN Batch 19/700 loss 4.210202 loss_att 8.409746 loss_ctc 8.622622 loss_rnnt 2.641949 hw_loss 0.262540 lr 0.00039634 rank 1
2023-02-24 19:31:53,719 DEBUG TRAIN Batch 19/700 loss 8.562912 loss_att 11.851027 loss_ctc 10.275614 loss_rnnt 7.580846 hw_loss 0.180155 lr 0.00039634 rank 0
2023-02-24 19:31:53,725 DEBUG TRAIN Batch 19/700 loss 6.805588 loss_att 10.539948 loss_ctc 8.548347 loss_rnnt 5.683168 hw_loss 0.268463 lr 0.00039634 rank 6
2023-02-24 19:31:53,740 DEBUG TRAIN Batch 19/700 loss 12.590564 loss_att 13.627904 loss_ctc 15.729828 loss_rnnt 11.822119 hw_loss 0.267017 lr 0.00039634 rank 2
2023-02-24 19:31:53,747 DEBUG TRAIN Batch 19/700 loss 13.462511 loss_att 17.292553 loss_ctc 22.145508 loss_rnnt 11.419467 hw_loss 0.223695 lr 0.00039634 rank 5
2023-02-24 19:33:07,426 DEBUG TRAIN Batch 19/800 loss 2.536093 loss_att 5.046341 loss_ctc 7.349374 loss_rnnt 1.293774 hw_loss 0.184684 lr 0.00039622 rank 0
2023-02-24 19:33:07,428 DEBUG TRAIN Batch 19/800 loss 5.009409 loss_att 7.222136 loss_ctc 8.392620 loss_rnnt 3.990889 hw_loss 0.234150 lr 0.00039622 rank 3
2023-02-24 19:33:07,435 DEBUG TRAIN Batch 19/800 loss 9.710934 loss_att 12.841334 loss_ctc 16.278492 loss_rnnt 8.150404 hw_loss 0.110205 lr 0.00039622 rank 6
2023-02-24 19:33:07,435 DEBUG TRAIN Batch 19/800 loss 11.501376 loss_att 18.110880 loss_ctc 14.774647 loss_rnnt 9.638022 hw_loss 0.196907 lr 0.00039622 rank 1
2023-02-24 19:33:07,437 DEBUG TRAIN Batch 19/800 loss 9.696416 loss_att 8.780048 loss_ctc 12.717185 loss_rnnt 9.368427 hw_loss 0.203426 lr 0.00039622 rank 4
2023-02-24 19:33:07,438 DEBUG TRAIN Batch 19/800 loss 12.643723 loss_att 17.974480 loss_ctc 21.680332 loss_rnnt 10.244987 hw_loss 0.239444 lr 0.00039622 rank 5
2023-02-24 19:33:07,439 DEBUG TRAIN Batch 19/800 loss 8.750431 loss_att 16.805128 loss_ctc 13.969309 loss_rnnt 6.290933 hw_loss 0.286328 lr 0.00039622 rank 2
2023-02-24 19:33:07,442 DEBUG TRAIN Batch 19/800 loss 16.818481 loss_att 15.925625 loss_ctc 16.924629 loss_rnnt 16.862627 hw_loss 0.225513 lr 0.00039622 rank 7
2023-02-24 19:34:20,861 DEBUG TRAIN Batch 19/900 loss 13.987261 loss_att 19.238960 loss_ctc 25.042873 loss_rnnt 11.312130 hw_loss 0.282579 lr 0.00039609 rank 6
2023-02-24 19:34:20,861 DEBUG TRAIN Batch 19/900 loss 8.858214 loss_att 10.740759 loss_ctc 14.451034 loss_rnnt 7.594062 hw_loss 0.266127 lr 0.00039609 rank 0
2023-02-24 19:34:20,861 DEBUG TRAIN Batch 19/900 loss 11.721186 loss_att 14.179152 loss_ctc 15.052998 loss_rnnt 10.677284 hw_loss 0.202624 lr 0.00039609 rank 3
2023-02-24 19:34:20,862 DEBUG TRAIN Batch 19/900 loss 6.112310 loss_att 12.600153 loss_ctc 7.902641 loss_rnnt 4.445111 hw_loss 0.245473 lr 0.00039609 rank 7
2023-02-24 19:34:20,864 DEBUG TRAIN Batch 19/900 loss 10.416600 loss_att 15.384243 loss_ctc 16.837635 loss_rnnt 8.468952 hw_loss 0.183717 lr 0.00039609 rank 1
2023-02-24 19:34:20,864 DEBUG TRAIN Batch 19/900 loss 2.504572 loss_att 4.882841 loss_ctc 2.856296 loss_rnnt 1.914806 hw_loss 0.126030 lr 0.00039609 rank 4
2023-02-24 19:34:20,866 DEBUG TRAIN Batch 19/900 loss 8.759414 loss_att 10.817193 loss_ctc 12.340029 loss_rnnt 7.798942 hw_loss 0.134066 lr 0.00039609 rank 2
2023-02-24 19:34:20,868 DEBUG TRAIN Batch 19/900 loss 11.749879 loss_att 16.631332 loss_ctc 18.222000 loss_rnnt 9.824423 hw_loss 0.161655 lr 0.00039609 rank 5
2023-02-24 19:35:34,589 DEBUG TRAIN Batch 19/1000 loss 8.994128 loss_att 12.459591 loss_ctc 14.304539 loss_rnnt 7.487679 hw_loss 0.197438 lr 0.00039597 rank 1
2023-02-24 19:35:34,596 DEBUG TRAIN Batch 19/1000 loss 7.459529 loss_att 11.114386 loss_ctc 10.391163 loss_rnnt 6.174901 hw_loss 0.305197 lr 0.00039597 rank 4
2023-02-24 19:35:34,598 DEBUG TRAIN Batch 19/1000 loss 7.188308 loss_att 8.752711 loss_ctc 8.667848 loss_rnnt 6.571794 hw_loss 0.199428 lr 0.00039597 rank 3
2023-02-24 19:35:34,599 DEBUG TRAIN Batch 19/1000 loss 1.981462 loss_att 4.584580 loss_ctc 2.838035 loss_rnnt 1.213168 hw_loss 0.250239 lr 0.00039597 rank 0
2023-02-24 19:35:34,601 DEBUG TRAIN Batch 19/1000 loss 7.441426 loss_att 11.007242 loss_ctc 10.863410 loss_rnnt 6.186963 hw_loss 0.159440 lr 0.00039597 rank 6
2023-02-24 19:35:34,603 DEBUG TRAIN Batch 19/1000 loss 5.206119 loss_att 7.186863 loss_ctc 7.698651 loss_rnnt 4.335044 hw_loss 0.267352 lr 0.00039597 rank 7
2023-02-24 19:35:34,609 DEBUG TRAIN Batch 19/1000 loss 9.873517 loss_att 11.667820 loss_ctc 13.097015 loss_rnnt 8.917220 hw_loss 0.314319 lr 0.00039597 rank 5
2023-02-24 19:35:34,629 DEBUG TRAIN Batch 19/1000 loss 6.186311 loss_att 7.242359 loss_ctc 10.734543 loss_rnnt 5.263641 hw_loss 0.196929 lr 0.00039597 rank 2
2023-02-24 19:36:49,673 DEBUG TRAIN Batch 19/1100 loss 16.718420 loss_att 20.609537 loss_ctc 24.383526 loss_rnnt 14.790970 hw_loss 0.238525 lr 0.00039584 rank 0
2023-02-24 19:36:49,677 DEBUG TRAIN Batch 19/1100 loss 11.360541 loss_att 13.282179 loss_ctc 16.240389 loss_rnnt 10.220895 hw_loss 0.196262 lr 0.00039584 rank 1
2023-02-24 19:36:49,679 DEBUG TRAIN Batch 19/1100 loss 7.435838 loss_att 9.390785 loss_ctc 12.537247 loss_rnnt 6.232173 hw_loss 0.248413 lr 0.00039584 rank 7
2023-02-24 19:36:49,680 DEBUG TRAIN Batch 19/1100 loss 3.529279 loss_att 5.815201 loss_ctc 5.078341 loss_rnnt 2.741022 hw_loss 0.233496 lr 0.00039584 rank 6
2023-02-24 19:36:49,680 DEBUG TRAIN Batch 19/1100 loss 5.510732 loss_att 6.702399 loss_ctc 7.155780 loss_rnnt 4.945243 hw_loss 0.202156 lr 0.00039584 rank 3
2023-02-24 19:36:49,683 DEBUG TRAIN Batch 19/1100 loss 12.275485 loss_att 14.179756 loss_ctc 15.302723 loss_rnnt 11.388452 hw_loss 0.192276 lr 0.00039584 rank 2
2023-02-24 19:36:49,686 DEBUG TRAIN Batch 19/1100 loss 20.690916 loss_att 26.521612 loss_ctc 29.849909 loss_rnnt 18.213150 hw_loss 0.169551 lr 0.00039584 rank 5
2023-02-24 19:36:49,733 DEBUG TRAIN Batch 19/1100 loss 16.966427 loss_att 18.404245 loss_ctc 23.788218 loss_rnnt 15.693853 hw_loss 0.141443 lr 0.00039584 rank 4
2023-02-24 19:38:02,857 DEBUG TRAIN Batch 19/1200 loss 17.195963 loss_att 16.059708 loss_ctc 19.141384 loss_rnnt 17.079943 hw_loss 0.157273 lr 0.00039572 rank 0
2023-02-24 19:38:02,860 DEBUG TRAIN Batch 19/1200 loss 6.959275 loss_att 10.238310 loss_ctc 10.701260 loss_rnnt 5.698048 hw_loss 0.199667 lr 0.00039572 rank 1
2023-02-24 19:38:02,863 DEBUG TRAIN Batch 19/1200 loss 9.169155 loss_att 12.029464 loss_ctc 15.200516 loss_rnnt 7.684172 hw_loss 0.203885 lr 0.00039572 rank 6
2023-02-24 19:38:02,863 DEBUG TRAIN Batch 19/1200 loss 7.165377 loss_att 9.764015 loss_ctc 10.008850 loss_rnnt 6.155645 hw_loss 0.207890 lr 0.00039572 rank 7
2023-02-24 19:38:02,867 DEBUG TRAIN Batch 19/1200 loss 7.346193 loss_att 9.896446 loss_ctc 10.304044 loss_rnnt 6.316298 hw_loss 0.235245 lr 0.00039572 rank 2
2023-02-24 19:38:02,869 DEBUG TRAIN Batch 19/1200 loss 15.925910 loss_att 19.002277 loss_ctc 17.702162 loss_rnnt 14.935015 hw_loss 0.260227 lr 0.00039572 rank 4
2023-02-24 19:38:02,872 DEBUG TRAIN Batch 19/1200 loss 7.212576 loss_att 8.274614 loss_ctc 10.683290 loss_rnnt 6.393038 hw_loss 0.270691 lr 0.00039572 rank 5
2023-02-24 19:38:02,873 DEBUG TRAIN Batch 19/1200 loss 11.613366 loss_att 12.100767 loss_ctc 15.856422 loss_rnnt 10.833347 hw_loss 0.218996 lr 0.00039572 rank 3
2023-02-24 19:39:16,083 DEBUG TRAIN Batch 19/1300 loss 10.467502 loss_att 12.100428 loss_ctc 15.456858 loss_rnnt 9.327324 hw_loss 0.278146 lr 0.00039560 rank 0
2023-02-24 19:39:16,083 DEBUG TRAIN Batch 19/1300 loss 8.647773 loss_att 10.621769 loss_ctc 13.549149 loss_rnnt 7.436535 hw_loss 0.305477 lr 0.00039560 rank 1
2023-02-24 19:39:16,083 DEBUG TRAIN Batch 19/1300 loss 3.090252 loss_att 6.573431 loss_ctc 5.013575 loss_rnnt 2.007982 hw_loss 0.242232 lr 0.00039560 rank 6
2023-02-24 19:39:16,085 DEBUG TRAIN Batch 19/1300 loss 8.699349 loss_att 10.623302 loss_ctc 8.294535 loss_rnnt 8.288942 hw_loss 0.149236 lr 0.00039560 rank 2
2023-02-24 19:39:16,087 DEBUG TRAIN Batch 19/1300 loss 10.105535 loss_att 11.016262 loss_ctc 16.194710 loss_rnnt 8.915983 hw_loss 0.366592 lr 0.00039560 rank 4
2023-02-24 19:39:16,088 DEBUG TRAIN Batch 19/1300 loss 3.452623 loss_att 8.300141 loss_ctc 4.908987 loss_rnnt 2.147761 hw_loss 0.264705 lr 0.00039560 rank 3
2023-02-24 19:39:16,093 DEBUG TRAIN Batch 19/1300 loss 5.947844 loss_att 10.827426 loss_ctc 8.383539 loss_rnnt 4.501670 hw_loss 0.272808 lr 0.00039560 rank 5
2023-02-24 19:39:16,094 DEBUG TRAIN Batch 19/1300 loss 9.024561 loss_att 8.334082 loss_ctc 10.666489 loss_rnnt 8.777425 hw_loss 0.311828 lr 0.00039560 rank 7
2023-02-24 19:40:31,039 DEBUG TRAIN Batch 19/1400 loss 8.108974 loss_att 11.210872 loss_ctc 10.505469 loss_rnnt 7.043115 hw_loss 0.236152 lr 0.00039547 rank 0
2023-02-24 19:40:31,046 DEBUG TRAIN Batch 19/1400 loss 19.738535 loss_att 21.842121 loss_ctc 22.267111 loss_rnnt 18.901968 hw_loss 0.147579 lr 0.00039547 rank 4
2023-02-24 19:40:31,048 DEBUG TRAIN Batch 19/1400 loss 7.781894 loss_att 11.132460 loss_ctc 10.696068 loss_rnnt 6.558543 hw_loss 0.308777 lr 0.00039547 rank 5
2023-02-24 19:40:31,048 DEBUG TRAIN Batch 19/1400 loss 7.201990 loss_att 9.150076 loss_ctc 13.341835 loss_rnnt 5.900827 hw_loss 0.174188 lr 0.00039547 rank 7
2023-02-24 19:40:31,050 DEBUG TRAIN Batch 19/1400 loss 4.646758 loss_att 8.239504 loss_ctc 6.849723 loss_rnnt 3.509223 hw_loss 0.234857 lr 0.00039547 rank 3
2023-02-24 19:40:31,057 DEBUG TRAIN Batch 19/1400 loss 12.051311 loss_att 14.240242 loss_ctc 17.420536 loss_rnnt 10.782179 hw_loss 0.216468 lr 0.00039547 rank 1
2023-02-24 19:40:31,085 DEBUG TRAIN Batch 19/1400 loss 9.411180 loss_att 12.252234 loss_ctc 11.745600 loss_rnnt 8.433064 hw_loss 0.184967 lr 0.00039547 rank 6
2023-02-24 19:40:31,117 DEBUG TRAIN Batch 19/1400 loss 10.281658 loss_att 12.140663 loss_ctc 11.557518 loss_rnnt 9.592747 hw_loss 0.275616 lr 0.00039547 rank 2
2023-02-24 19:41:45,581 DEBUG TRAIN Batch 19/1500 loss 21.793177 loss_att 22.877386 loss_ctc 29.275110 loss_rnnt 20.450243 hw_loss 0.240934 lr 0.00039535 rank 3
2023-02-24 19:41:45,586 DEBUG TRAIN Batch 19/1500 loss 8.826753 loss_att 12.401823 loss_ctc 14.931255 loss_rnnt 7.216460 hw_loss 0.152521 lr 0.00039535 rank 6
2023-02-24 19:41:45,586 DEBUG TRAIN Batch 19/1500 loss 9.962197 loss_att 12.864950 loss_ctc 11.882721 loss_rnnt 8.988166 hw_loss 0.257647 lr 0.00039535 rank 7
2023-02-24 19:41:45,588 DEBUG TRAIN Batch 19/1500 loss 10.692349 loss_att 14.330841 loss_ctc 15.457069 loss_rnnt 9.239112 hw_loss 0.169208 lr 0.00039535 rank 0
2023-02-24 19:41:45,589 DEBUG TRAIN Batch 19/1500 loss 5.651645 loss_att 10.981380 loss_ctc 9.889251 loss_rnnt 3.904296 hw_loss 0.218225 lr 0.00039535 rank 4
2023-02-24 19:41:45,592 DEBUG TRAIN Batch 19/1500 loss 10.952918 loss_att 14.490585 loss_ctc 21.766798 loss_rnnt 8.737033 hw_loss 0.124690 lr 0.00039535 rank 2
2023-02-24 19:41:45,601 DEBUG TRAIN Batch 19/1500 loss 9.518271 loss_att 13.483922 loss_ctc 15.214840 loss_rnnt 7.857648 hw_loss 0.202407 lr 0.00039535 rank 5
2023-02-24 19:41:45,641 DEBUG TRAIN Batch 19/1500 loss 2.503517 loss_att 5.401642 loss_ctc 3.199463 loss_rnnt 1.746499 hw_loss 0.158626 lr 0.00039535 rank 1
2023-02-24 19:42:58,908 DEBUG TRAIN Batch 19/1600 loss 11.126350 loss_att 16.860390 loss_ctc 14.124430 loss_rnnt 9.520416 hw_loss 0.111341 lr 0.00039523 rank 6
2023-02-24 19:42:58,912 DEBUG TRAIN Batch 19/1600 loss 8.031655 loss_att 9.727942 loss_ctc 16.622974 loss_rnnt 6.441185 hw_loss 0.198193 lr 0.00039523 rank 0
2023-02-24 19:42:58,911 DEBUG TRAIN Batch 19/1600 loss 6.632971 loss_att 9.302784 loss_ctc 8.679924 loss_rnnt 5.713175 hw_loss 0.211700 lr 0.00039523 rank 3
2023-02-24 19:42:58,913 DEBUG TRAIN Batch 19/1600 loss 7.245692 loss_att 11.029976 loss_ctc 13.600870 loss_rnnt 5.520841 hw_loss 0.226194 lr 0.00039523 rank 4
2023-02-24 19:42:58,913 DEBUG TRAIN Batch 19/1600 loss 24.326187 loss_att 26.595362 loss_ctc 36.154831 loss_rnnt 22.165192 hw_loss 0.243770 lr 0.00039523 rank 1
2023-02-24 19:42:58,917 DEBUG TRAIN Batch 19/1600 loss 11.457902 loss_att 14.311887 loss_ctc 16.053787 loss_rnnt 10.168091 hw_loss 0.199179 lr 0.00039523 rank 2
2023-02-24 19:42:58,922 DEBUG TRAIN Batch 19/1600 loss 9.151292 loss_att 12.803110 loss_ctc 14.043427 loss_rnnt 7.608089 hw_loss 0.301038 lr 0.00039523 rank 7
2023-02-24 19:42:58,965 DEBUG TRAIN Batch 19/1600 loss 15.240481 loss_att 18.859446 loss_ctc 21.618664 loss_rnnt 13.609268 hw_loss 0.106867 lr 0.00039523 rank 5
2023-02-24 19:44:12,115 DEBUG TRAIN Batch 19/1700 loss 8.334487 loss_att 12.437676 loss_ctc 10.521878 loss_rnnt 7.114214 hw_loss 0.202467 lr 0.00039510 rank 7
2023-02-24 19:44:12,121 DEBUG TRAIN Batch 19/1700 loss 7.575034 loss_att 13.547922 loss_ctc 12.776969 loss_rnnt 5.568698 hw_loss 0.221564 lr 0.00039510 rank 0
2023-02-24 19:44:12,123 DEBUG TRAIN Batch 19/1700 loss 8.391689 loss_att 11.033110 loss_ctc 12.692840 loss_rnnt 7.141126 hw_loss 0.278985 lr 0.00039510 rank 4
2023-02-24 19:44:12,128 DEBUG TRAIN Batch 19/1700 loss 17.276268 loss_att 18.991619 loss_ctc 19.098259 loss_rnnt 16.562717 hw_loss 0.239153 lr 0.00039510 rank 2
2023-02-24 19:44:12,129 DEBUG TRAIN Batch 19/1700 loss 6.412233 loss_att 6.987379 loss_ctc 10.270954 loss_rnnt 5.669000 hw_loss 0.213202 lr 0.00039510 rank 5
2023-02-24 19:44:12,131 DEBUG TRAIN Batch 19/1700 loss 9.345526 loss_att 11.298205 loss_ctc 13.285848 loss_rnnt 8.301206 hw_loss 0.240765 lr 0.00039510 rank 6
2023-02-24 19:44:12,167 DEBUG TRAIN Batch 19/1700 loss 17.477057 loss_att 19.590374 loss_ctc 22.201935 loss_rnnt 16.325827 hw_loss 0.184839 lr 0.00039510 rank 1
2023-02-24 19:44:12,197 DEBUG TRAIN Batch 19/1700 loss 8.260048 loss_att 10.397463 loss_ctc 11.529217 loss_rnnt 7.289724 hw_loss 0.200533 lr 0.00039510 rank 3
2023-02-24 19:45:28,630 DEBUG TRAIN Batch 19/1800 loss 9.447756 loss_att 10.296650 loss_ctc 13.044516 loss_rnnt 8.695045 hw_loss 0.193806 lr 0.00039498 rank 0
2023-02-24 19:45:28,638 DEBUG TRAIN Batch 19/1800 loss 4.506945 loss_att 7.113759 loss_ctc 7.281451 loss_rnnt 3.473160 hw_loss 0.267163 lr 0.00039498 rank 1
2023-02-24 19:45:28,638 DEBUG TRAIN Batch 19/1800 loss 9.581014 loss_att 10.589971 loss_ctc 13.438506 loss_rnnt 8.720429 hw_loss 0.270862 lr 0.00039498 rank 7
2023-02-24 19:45:28,639 DEBUG TRAIN Batch 19/1800 loss 11.031525 loss_att 12.181814 loss_ctc 16.133915 loss_rnnt 10.007446 hw_loss 0.213190 lr 0.00039498 rank 3
2023-02-24 19:45:28,642 DEBUG TRAIN Batch 19/1800 loss 5.285367 loss_att 5.248997 loss_ctc 6.150974 loss_rnnt 4.970812 hw_loss 0.387025 lr 0.00039498 rank 6
2023-02-24 19:45:28,642 DEBUG TRAIN Batch 19/1800 loss 4.949978 loss_att 6.322622 loss_ctc 7.091667 loss_rnnt 4.273690 hw_loss 0.217877 lr 0.00039498 rank 5
2023-02-24 19:45:28,673 DEBUG TRAIN Batch 19/1800 loss 7.844216 loss_att 10.802787 loss_ctc 11.955684 loss_rnnt 6.592678 hw_loss 0.209303 lr 0.00039498 rank 4
2023-02-24 19:45:28,674 DEBUG TRAIN Batch 19/1800 loss 8.545200 loss_att 11.655605 loss_ctc 13.130031 loss_rnnt 7.182193 hw_loss 0.243030 lr 0.00039498 rank 2
2023-02-24 19:46:42,041 DEBUG TRAIN Batch 19/1900 loss 4.949277 loss_att 10.785285 loss_ctc 8.009163 loss_rnnt 3.271543 hw_loss 0.192275 lr 0.00039486 rank 3
2023-02-24 19:46:42,042 DEBUG TRAIN Batch 19/1900 loss 11.454725 loss_att 13.711241 loss_ctc 17.094559 loss_rnnt 10.075796 hw_loss 0.329341 lr 0.00039486 rank 0
2023-02-24 19:46:42,043 DEBUG TRAIN Batch 19/1900 loss 12.400612 loss_att 13.335819 loss_ctc 15.437995 loss_rnnt 11.661454 hw_loss 0.275872 lr 0.00039486 rank 4
2023-02-24 19:46:42,045 DEBUG TRAIN Batch 19/1900 loss 12.066527 loss_att 11.482645 loss_ctc 17.712412 loss_rnnt 11.254354 hw_loss 0.330310 lr 0.00039486 rank 7
2023-02-24 19:46:42,046 DEBUG TRAIN Batch 19/1900 loss 16.834053 loss_att 18.113445 loss_ctc 21.824232 loss_rnnt 15.768758 hw_loss 0.270108 lr 0.00039486 rank 1
2023-02-24 19:46:42,046 DEBUG TRAIN Batch 19/1900 loss 10.445718 loss_att 9.505660 loss_ctc 12.892443 loss_rnnt 10.131731 hw_loss 0.329567 lr 0.00039486 rank 2
2023-02-24 19:46:42,049 DEBUG TRAIN Batch 19/1900 loss 4.839189 loss_att 10.115452 loss_ctc 5.475852 loss_rnnt 3.599314 hw_loss 0.187000 lr 0.00039486 rank 5
2023-02-24 19:46:42,094 DEBUG TRAIN Batch 19/1900 loss 5.243059 loss_att 8.079247 loss_ctc 10.895756 loss_rnnt 3.813133 hw_loss 0.204367 lr 0.00039486 rank 6
2023-02-24 19:47:55,729 DEBUG TRAIN Batch 19/2000 loss 5.869337 loss_att 8.851268 loss_ctc 6.203716 loss_rnnt 5.099010 hw_loss 0.242543 lr 0.00039473 rank 0
2023-02-24 19:47:55,736 DEBUG TRAIN Batch 19/2000 loss 4.738268 loss_att 9.221586 loss_ctc 11.132565 loss_rnnt 2.880363 hw_loss 0.203754 lr 0.00039473 rank 1
2023-02-24 19:47:55,737 DEBUG TRAIN Batch 19/2000 loss 13.714850 loss_att 16.514889 loss_ctc 26.828474 loss_rnnt 11.264874 hw_loss 0.265287 lr 0.00039473 rank 6
2023-02-24 19:47:55,740 DEBUG TRAIN Batch 19/2000 loss 9.017323 loss_att 13.332220 loss_ctc 17.149162 loss_rnnt 6.918615 hw_loss 0.284033 lr 0.00039473 rank 2
2023-02-24 19:47:55,740 DEBUG TRAIN Batch 19/2000 loss 9.919197 loss_att 14.744449 loss_ctc 10.860680 loss_rnnt 8.675085 hw_loss 0.287870 lr 0.00039473 rank 3
2023-02-24 19:47:55,741 DEBUG TRAIN Batch 19/2000 loss 6.364956 loss_att 11.511709 loss_ctc 9.109452 loss_rnnt 4.886950 hw_loss 0.155104 lr 0.00039473 rank 7
2023-02-24 19:47:55,746 DEBUG TRAIN Batch 19/2000 loss 8.520706 loss_att 11.103154 loss_ctc 13.595144 loss_rnnt 7.248687 hw_loss 0.148011 lr 0.00039473 rank 5
2023-02-24 19:47:55,749 DEBUG TRAIN Batch 19/2000 loss 5.791127 loss_att 9.686193 loss_ctc 6.020944 loss_rnnt 4.898613 hw_loss 0.155359 lr 0.00039473 rank 4
2023-02-24 19:49:10,357 DEBUG TRAIN Batch 19/2100 loss 13.277222 loss_att 17.751802 loss_ctc 18.102730 loss_rnnt 11.671700 hw_loss 0.126009 lr 0.00039461 rank 0
2023-02-24 19:49:10,358 DEBUG TRAIN Batch 19/2100 loss 12.210339 loss_att 13.930032 loss_ctc 22.481426 loss_rnnt 10.361060 hw_loss 0.254739 lr 0.00039461 rank 4
2023-02-24 19:49:10,360 DEBUG TRAIN Batch 19/2100 loss 9.321188 loss_att 11.810304 loss_ctc 16.745808 loss_rnnt 7.718923 hw_loss 0.214674 lr 0.00039461 rank 3
2023-02-24 19:49:10,367 DEBUG TRAIN Batch 19/2100 loss 7.621060 loss_att 9.977337 loss_ctc 10.849522 loss_rnnt 6.617705 hw_loss 0.190571 lr 0.00039461 rank 1
2023-02-24 19:49:10,376 DEBUG TRAIN Batch 19/2100 loss 10.286254 loss_att 14.984394 loss_ctc 12.856907 loss_rnnt 8.854110 hw_loss 0.280805 lr 0.00039461 rank 2
2023-02-24 19:49:10,389 DEBUG TRAIN Batch 19/2100 loss 12.275565 loss_att 17.448868 loss_ctc 18.108725 loss_rnnt 10.374481 hw_loss 0.166255 lr 0.00039461 rank 5
2023-02-24 19:49:10,390 DEBUG TRAIN Batch 19/2100 loss 5.108200 loss_att 7.394279 loss_ctc 6.980770 loss_rnnt 4.299579 hw_loss 0.190742 lr 0.00039461 rank 6
2023-02-24 19:49:10,400 DEBUG TRAIN Batch 19/2100 loss 8.629064 loss_att 11.327270 loss_ctc 12.626822 loss_rnnt 7.459556 hw_loss 0.181557 lr 0.00039461 rank 7
2023-02-24 19:50:26,860 DEBUG TRAIN Batch 19/2200 loss 15.416198 loss_att 19.208115 loss_ctc 21.089985 loss_rnnt 13.801113 hw_loss 0.187870 lr 0.00039449 rank 1
2023-02-24 19:50:26,862 DEBUG TRAIN Batch 19/2200 loss 6.638873 loss_att 11.355865 loss_ctc 8.640852 loss_rnnt 5.333185 hw_loss 0.178798 lr 0.00039449 rank 0
2023-02-24 19:50:26,862 DEBUG TRAIN Batch 19/2200 loss 9.182817 loss_att 15.010934 loss_ctc 12.913216 loss_rnnt 7.399995 hw_loss 0.224647 lr 0.00039449 rank 6
2023-02-24 19:50:26,862 DEBUG TRAIN Batch 19/2200 loss 6.235777 loss_att 11.756728 loss_ctc 9.243954 loss_rnnt 4.631732 hw_loss 0.185185 lr 0.00039449 rank 7
2023-02-24 19:50:26,865 DEBUG TRAIN Batch 19/2200 loss 7.880722 loss_att 10.150806 loss_ctc 10.717360 loss_rnnt 6.917574 hw_loss 0.245461 lr 0.00039449 rank 3
2023-02-24 19:50:26,865 DEBUG TRAIN Batch 19/2200 loss 6.626059 loss_att 9.151119 loss_ctc 8.043661 loss_rnnt 5.817897 hw_loss 0.214007 lr 0.00039449 rank 5
2023-02-24 19:50:26,867 DEBUG TRAIN Batch 19/2200 loss 9.939945 loss_att 14.911239 loss_ctc 14.889605 loss_rnnt 8.126161 hw_loss 0.299197 lr 0.00039449 rank 2
2023-02-24 19:50:26,872 DEBUG TRAIN Batch 19/2200 loss 8.631892 loss_att 11.221783 loss_ctc 12.903361 loss_rnnt 7.377752 hw_loss 0.312435 lr 0.00039449 rank 4
2023-02-24 19:51:39,793 DEBUG TRAIN Batch 19/2300 loss 5.908430 loss_att 8.145859 loss_ctc 8.101770 loss_rnnt 5.051489 hw_loss 0.219393 lr 0.00039436 rank 0
2023-02-24 19:51:39,795 DEBUG TRAIN Batch 19/2300 loss 6.215784 loss_att 7.830235 loss_ctc 10.514396 loss_rnnt 5.175567 hw_loss 0.270334 lr 0.00039436 rank 3
2023-02-24 19:51:39,796 DEBUG TRAIN Batch 19/2300 loss 11.347293 loss_att 16.388552 loss_ctc 14.192901 loss_rnnt 9.858776 hw_loss 0.189097 lr 0.00039436 rank 1
2023-02-24 19:51:39,796 DEBUG TRAIN Batch 19/2300 loss 10.903120 loss_att 13.157059 loss_ctc 14.926977 loss_rnnt 9.838321 hw_loss 0.145306 lr 0.00039436 rank 2
2023-02-24 19:51:39,800 DEBUG TRAIN Batch 19/2300 loss 16.749701 loss_att 21.097290 loss_ctc 17.238699 loss_rnnt 15.691311 hw_loss 0.231883 lr 0.00039436 rank 7
2023-02-24 19:51:39,803 DEBUG TRAIN Batch 19/2300 loss 7.924988 loss_att 10.196904 loss_ctc 8.924469 loss_rnnt 7.182507 hw_loss 0.290314 lr 0.00039436 rank 4
2023-02-24 19:51:39,803 DEBUG TRAIN Batch 19/2300 loss 9.058390 loss_att 10.470837 loss_ctc 11.900545 loss_rnnt 8.267755 hw_loss 0.242232 lr 0.00039436 rank 6
2023-02-24 19:51:39,855 DEBUG TRAIN Batch 19/2300 loss 5.591024 loss_att 9.276787 loss_ctc 9.197812 loss_rnnt 4.258619 hw_loss 0.214402 lr 0.00039436 rank 5
2023-02-24 19:52:52,951 DEBUG TRAIN Batch 19/2400 loss 10.219833 loss_att 12.892068 loss_ctc 15.420319 loss_rnnt 8.829646 hw_loss 0.304392 lr 0.00039424 rank 3
2023-02-24 19:52:52,952 DEBUG TRAIN Batch 19/2400 loss 4.761786 loss_att 6.760341 loss_ctc 7.739701 loss_rnnt 3.847933 hw_loss 0.219539 lr 0.00039424 rank 6
2023-02-24 19:52:52,954 DEBUG TRAIN Batch 19/2400 loss 7.377838 loss_att 9.267870 loss_ctc 10.584932 loss_rnnt 6.441148 hw_loss 0.245759 lr 0.00039424 rank 2
2023-02-24 19:52:52,955 DEBUG TRAIN Batch 19/2400 loss 6.887509 loss_att 9.220801 loss_ctc 13.999291 loss_rnnt 5.390487 hw_loss 0.153986 lr 0.00039424 rank 1
2023-02-24 19:52:52,957 DEBUG TRAIN Batch 19/2400 loss 16.537577 loss_att 20.088234 loss_ctc 25.741093 loss_rnnt 14.470518 hw_loss 0.243358 lr 0.00039424 rank 4
2023-02-24 19:52:52,958 DEBUG TRAIN Batch 19/2400 loss 9.890332 loss_att 14.411047 loss_ctc 16.745008 loss_rnnt 7.975114 hw_loss 0.182097 lr 0.00039424 rank 5
2023-02-24 19:52:52,959 DEBUG TRAIN Batch 19/2400 loss 6.322440 loss_att 7.197964 loss_ctc 7.708345 loss_rnnt 5.865390 hw_loss 0.182172 lr 0.00039424 rank 0
2023-02-24 19:52:52,963 DEBUG TRAIN Batch 19/2400 loss 7.705184 loss_att 10.330222 loss_ctc 11.590237 loss_rnnt 6.540858 hw_loss 0.227458 lr 0.00039424 rank 7
2023-02-24 19:54:09,062 DEBUG TRAIN Batch 19/2500 loss 8.219637 loss_att 12.944812 loss_ctc 11.963333 loss_rnnt 6.647357 hw_loss 0.240161 lr 0.00039412 rank 0
2023-02-24 19:54:09,066 DEBUG TRAIN Batch 19/2500 loss 20.597225 loss_att 23.215567 loss_ctc 28.613953 loss_rnnt 18.900999 hw_loss 0.194370 lr 0.00039412 rank 1
2023-02-24 19:54:09,065 DEBUG TRAIN Batch 19/2500 loss 8.937944 loss_att 11.273358 loss_ctc 11.784013 loss_rnnt 7.973844 hw_loss 0.220390 lr 0.00039412 rank 2
2023-02-24 19:54:09,066 DEBUG TRAIN Batch 19/2500 loss 11.786407 loss_att 13.018808 loss_ctc 15.734206 loss_rnnt 10.947597 hw_loss 0.123668 lr 0.00039412 rank 6
2023-02-24 19:54:09,069 DEBUG TRAIN Batch 19/2500 loss 9.332601 loss_att 9.958519 loss_ctc 14.108363 loss_rnnt 8.447317 hw_loss 0.231247 lr 0.00039412 rank 7
2023-02-24 19:54:09,071 DEBUG TRAIN Batch 19/2500 loss 9.736835 loss_att 10.028517 loss_ctc 12.242381 loss_rnnt 9.189924 hw_loss 0.289693 lr 0.00039412 rank 5
2023-02-24 19:54:09,075 DEBUG TRAIN Batch 19/2500 loss 10.781733 loss_att 11.860935 loss_ctc 16.129805 loss_rnnt 9.729420 hw_loss 0.231365 lr 0.00039412 rank 4
2023-02-24 19:54:09,108 DEBUG TRAIN Batch 19/2500 loss 10.708368 loss_att 13.727993 loss_ctc 15.404895 loss_rnnt 9.404709 hw_loss 0.137869 lr 0.00039412 rank 3
2023-02-24 19:55:21,462 DEBUG TRAIN Batch 19/2600 loss 7.662857 loss_att 9.891944 loss_ctc 8.977111 loss_rnnt 6.939758 hw_loss 0.191341 lr 0.00039400 rank 7
2023-02-24 19:55:21,463 DEBUG TRAIN Batch 19/2600 loss 7.211712 loss_att 8.149052 loss_ctc 12.674883 loss_rnnt 6.150730 hw_loss 0.272048 lr 0.00039400 rank 0
2023-02-24 19:55:21,464 DEBUG TRAIN Batch 19/2600 loss 6.357997 loss_att 9.110541 loss_ctc 8.184754 loss_rnnt 5.460055 hw_loss 0.194747 lr 0.00039400 rank 6
2023-02-24 19:55:21,464 DEBUG TRAIN Batch 19/2600 loss 12.895295 loss_att 13.720800 loss_ctc 20.509903 loss_rnnt 11.571047 hw_loss 0.269749 lr 0.00039400 rank 2
2023-02-24 19:55:21,465 DEBUG TRAIN Batch 19/2600 loss 4.582908 loss_att 9.509706 loss_ctc 5.986958 loss_rnnt 3.286265 hw_loss 0.232643 lr 0.00039400 rank 4
2023-02-24 19:55:21,466 DEBUG TRAIN Batch 19/2600 loss 9.157115 loss_att 9.042880 loss_ctc 13.123217 loss_rnnt 8.474444 hw_loss 0.331321 lr 0.00039400 rank 1
2023-02-24 19:55:21,474 DEBUG TRAIN Batch 19/2600 loss 5.671729 loss_att 8.330304 loss_ctc 7.274236 loss_rnnt 4.839418 hw_loss 0.162990 lr 0.00039400 rank 5
2023-02-24 19:55:21,514 DEBUG TRAIN Batch 19/2600 loss 12.588397 loss_att 18.255089 loss_ctc 18.797726 loss_rnnt 10.537315 hw_loss 0.168437 lr 0.00039400 rank 3
2023-02-24 19:56:33,791 DEBUG TRAIN Batch 19/2700 loss 5.980510 loss_att 9.082980 loss_ctc 11.251770 loss_rnnt 4.566883 hw_loss 0.169312 lr 0.00039387 rank 2
2023-02-24 19:56:33,793 DEBUG TRAIN Batch 19/2700 loss 7.979819 loss_att 11.720164 loss_ctc 12.585613 loss_rnnt 6.508664 hw_loss 0.204338 lr 0.00039387 rank 0
2023-02-24 19:56:33,797 DEBUG TRAIN Batch 19/2700 loss 5.912348 loss_att 8.992891 loss_ctc 7.522573 loss_rnnt 4.979897 hw_loss 0.190587 lr 0.00039387 rank 6
2023-02-24 19:56:33,798 DEBUG TRAIN Batch 19/2700 loss 7.166692 loss_att 10.297829 loss_ctc 9.111728 loss_rnnt 6.186422 hw_loss 0.177570 lr 0.00039387 rank 3
2023-02-24 19:56:33,799 DEBUG TRAIN Batch 19/2700 loss 13.247314 loss_att 13.203796 loss_ctc 14.722038 loss_rnnt 12.978376 hw_loss 0.151895 lr 0.00039387 rank 7
2023-02-24 19:56:33,803 DEBUG TRAIN Batch 19/2700 loss 8.095139 loss_att 10.964041 loss_ctc 13.446844 loss_rnnt 6.745574 hw_loss 0.116667 lr 0.00039387 rank 4
2023-02-24 19:56:33,803 DEBUG TRAIN Batch 19/2700 loss 3.679354 loss_att 7.647702 loss_ctc 4.841133 loss_rnnt 2.615411 hw_loss 0.216316 lr 0.00039387 rank 5
2023-02-24 19:56:33,845 DEBUG TRAIN Batch 19/2700 loss 8.544359 loss_att 12.479235 loss_ctc 7.864276 loss_rnnt 7.729668 hw_loss 0.221989 lr 0.00039387 rank 1
2023-02-24 19:57:47,604 DEBUG TRAIN Batch 19/2800 loss 8.180826 loss_att 11.470925 loss_ctc 9.797753 loss_rnnt 7.124351 hw_loss 0.342872 lr 0.00039375 rank 2
2023-02-24 19:57:47,613 DEBUG TRAIN Batch 19/2800 loss 4.442310 loss_att 8.159193 loss_ctc 7.175656 loss_rnnt 3.219728 hw_loss 0.215174 lr 0.00039375 rank 4
2023-02-24 19:57:47,616 DEBUG TRAIN Batch 19/2800 loss 9.225962 loss_att 13.940532 loss_ctc 14.957865 loss_rnnt 7.437587 hw_loss 0.152260 lr 0.00039375 rank 0
2023-02-24 19:57:47,617 DEBUG TRAIN Batch 19/2800 loss 8.846762 loss_att 13.615419 loss_ctc 15.002850 loss_rnnt 6.956285 hw_loss 0.217376 lr 0.00039375 rank 7
2023-02-24 19:57:47,617 DEBUG TRAIN Batch 19/2800 loss 6.644897 loss_att 10.658365 loss_ctc 13.095786 loss_rnnt 4.839704 hw_loss 0.266963 lr 0.00039375 rank 6
2023-02-24 19:57:47,617 DEBUG TRAIN Batch 19/2800 loss 6.330948 loss_att 9.016628 loss_ctc 8.301163 loss_rnnt 5.470224 hw_loss 0.114174 lr 0.00039375 rank 1
2023-02-24 19:57:47,639 DEBUG TRAIN Batch 19/2800 loss 3.192826 loss_att 6.045134 loss_ctc 3.995784 loss_rnnt 2.404791 hw_loss 0.207210 lr 0.00039375 rank 5
2023-02-24 19:57:47,673 DEBUG TRAIN Batch 19/2800 loss 10.249633 loss_att 15.298306 loss_ctc 20.492907 loss_rnnt 7.749599 hw_loss 0.233496 lr 0.00039375 rank 3
2023-02-24 19:59:01,215 DEBUG TRAIN Batch 19/2900 loss 11.221457 loss_att 14.188515 loss_ctc 13.792244 loss_rnnt 10.227580 hw_loss 0.108175 lr 0.00039363 rank 0
2023-02-24 19:59:01,217 DEBUG TRAIN Batch 19/2900 loss 9.445164 loss_att 15.649397 loss_ctc 12.495519 loss_rnnt 7.685761 hw_loss 0.209704 lr 0.00039363 rank 3
2023-02-24 19:59:01,218 DEBUG TRAIN Batch 19/2900 loss 13.647943 loss_att 15.075180 loss_ctc 17.324278 loss_rnnt 12.757283 hw_loss 0.215691 lr 0.00039363 rank 1
2023-02-24 19:59:01,220 DEBUG TRAIN Batch 19/2900 loss 16.639904 loss_att 20.274628 loss_ctc 25.501560 loss_rnnt 14.604864 hw_loss 0.237267 lr 0.00039363 rank 6
2023-02-24 19:59:01,220 DEBUG TRAIN Batch 19/2900 loss 10.822659 loss_att 14.111173 loss_ctc 18.137257 loss_rnnt 9.039102 hw_loss 0.282328 lr 0.00039363 rank 5
2023-02-24 19:59:01,222 DEBUG TRAIN Batch 19/2900 loss 5.413372 loss_att 7.326043 loss_ctc 9.775059 loss_rnnt 4.331127 hw_loss 0.221536 lr 0.00039363 rank 2
2023-02-24 19:59:01,224 DEBUG TRAIN Batch 19/2900 loss 8.471991 loss_att 13.094506 loss_ctc 12.996692 loss_rnnt 6.825687 hw_loss 0.222201 lr 0.00039363 rank 7
2023-02-24 19:59:01,224 DEBUG TRAIN Batch 19/2900 loss 7.648393 loss_att 10.875620 loss_ctc 10.530828 loss_rnnt 6.513666 hw_loss 0.196793 lr 0.00039363 rank 4
2023-02-24 20:00:13,290 DEBUG TRAIN Batch 19/3000 loss 5.608574 loss_att 8.808455 loss_ctc 9.982882 loss_rnnt 4.303797 hw_loss 0.152925 lr 0.00039351 rank 0
2023-02-24 20:00:13,290 DEBUG TRAIN Batch 19/3000 loss 9.937072 loss_att 12.589619 loss_ctc 13.026491 loss_rnnt 8.810812 hw_loss 0.344677 lr 0.00039351 rank 6
2023-02-24 20:00:13,295 DEBUG TRAIN Batch 19/3000 loss 9.620932 loss_att 13.597910 loss_ctc 13.744986 loss_rnnt 8.150526 hw_loss 0.234627 lr 0.00039351 rank 7
2023-02-24 20:00:13,297 DEBUG TRAIN Batch 19/3000 loss 4.733816 loss_att 7.556890 loss_ctc 6.080547 loss_rnnt 3.862458 hw_loss 0.238460 lr 0.00039351 rank 5
2023-02-24 20:00:13,302 DEBUG TRAIN Batch 19/3000 loss 6.624262 loss_att 9.600113 loss_ctc 9.045080 loss_rnnt 5.580214 hw_loss 0.236442 lr 0.00039351 rank 4
2023-02-24 20:00:13,309 DEBUG TRAIN Batch 19/3000 loss 3.725880 loss_att 6.078378 loss_ctc 7.125232 loss_rnnt 2.730522 hw_loss 0.134271 lr 0.00039351 rank 3
2023-02-24 20:00:13,309 DEBUG TRAIN Batch 19/3000 loss 14.617048 loss_att 15.713290 loss_ctc 16.671535 loss_rnnt 14.060942 hw_loss 0.117989 lr 0.00039351 rank 1
2023-02-24 20:00:13,339 DEBUG TRAIN Batch 19/3000 loss 13.308687 loss_att 17.331608 loss_ctc 16.427067 loss_rnnt 11.973020 hw_loss 0.216185 lr 0.00039351 rank 2
2023-02-24 20:01:25,924 DEBUG TRAIN Batch 19/3100 loss 12.861485 loss_att 15.737723 loss_ctc 14.965395 loss_rnnt 11.886125 hw_loss 0.224233 lr 0.00039339 rank 0
2023-02-24 20:01:25,925 DEBUG TRAIN Batch 19/3100 loss 11.896163 loss_att 15.543079 loss_ctc 12.024319 loss_rnnt 11.049185 hw_loss 0.188454 lr 0.00039339 rank 1
2023-02-24 20:01:25,925 DEBUG TRAIN Batch 19/3100 loss 10.641596 loss_att 12.902410 loss_ctc 13.479994 loss_rnnt 9.727447 hw_loss 0.156624 lr 0.00039339 rank 6
2023-02-24 20:01:25,928 DEBUG TRAIN Batch 19/3100 loss 14.956213 loss_att 17.031013 loss_ctc 20.808407 loss_rnnt 13.676998 hw_loss 0.157428 lr 0.00039339 rank 2
2023-02-24 20:01:25,931 DEBUG TRAIN Batch 19/3100 loss 7.852632 loss_att 10.657400 loss_ctc 13.122900 loss_rnnt 6.444828 hw_loss 0.270276 lr 0.00039339 rank 4
2023-02-24 20:01:25,931 DEBUG TRAIN Batch 19/3100 loss 7.503618 loss_att 8.633415 loss_ctc 10.110652 loss_rnnt 6.767569 hw_loss 0.304657 lr 0.00039339 rank 7
2023-02-24 20:01:25,938 DEBUG TRAIN Batch 19/3100 loss 9.787775 loss_att 10.407562 loss_ctc 12.777554 loss_rnnt 9.128187 hw_loss 0.256864 lr 0.00039339 rank 5
2023-02-24 20:01:25,974 DEBUG TRAIN Batch 19/3100 loss 5.103309 loss_att 6.212452 loss_ctc 6.034944 loss_rnnt 4.642921 hw_loss 0.214387 lr 0.00039339 rank 3
2023-02-24 20:02:41,042 DEBUG TRAIN Batch 19/3200 loss 8.551942 loss_att 9.924392 loss_ctc 13.189999 loss_rnnt 7.484612 hw_loss 0.327061 lr 0.00039326 rank 5
2023-02-24 20:02:41,043 DEBUG TRAIN Batch 19/3200 loss 7.822711 loss_att 9.221013 loss_ctc 10.392844 loss_rnnt 7.018015 hw_loss 0.341907 lr 0.00039326 rank 0
2023-02-24 20:02:41,052 DEBUG TRAIN Batch 19/3200 loss 13.249974 loss_att 17.024685 loss_ctc 15.929790 loss_rnnt 12.047854 hw_loss 0.168502 lr 0.00039326 rank 7
2023-02-24 20:02:41,052 DEBUG TRAIN Batch 19/3200 loss 4.104468 loss_att 7.047164 loss_ctc 6.371099 loss_rnnt 3.144820 hw_loss 0.129171 lr 0.00039326 rank 6
2023-02-24 20:02:41,054 DEBUG TRAIN Batch 19/3200 loss 11.989593 loss_att 17.964525 loss_ctc 20.085051 loss_rnnt 9.607420 hw_loss 0.202110 lr 0.00039326 rank 3
2023-02-24 20:02:41,057 DEBUG TRAIN Batch 19/3200 loss 7.139349 loss_att 7.573915 loss_ctc 8.297244 loss_rnnt 6.767240 hw_loss 0.245269 lr 0.00039326 rank 2
2023-02-24 20:02:41,059 DEBUG TRAIN Batch 19/3200 loss 18.489855 loss_att 21.456469 loss_ctc 23.690983 loss_rnnt 17.034525 hw_loss 0.315981 lr 0.00039326 rank 1
2023-02-24 20:02:41,105 DEBUG TRAIN Batch 19/3200 loss 10.838490 loss_att 12.213375 loss_ctc 12.343349 loss_rnnt 10.226947 hw_loss 0.254845 lr 0.00039326 rank 4
2023-02-24 20:03:54,500 DEBUG TRAIN Batch 19/3300 loss 7.860927 loss_att 9.252802 loss_ctc 7.537475 loss_rnnt 7.515941 hw_loss 0.205760 lr 0.00039314 rank 3
2023-02-24 20:03:54,503 DEBUG TRAIN Batch 19/3300 loss 10.142757 loss_att 14.068487 loss_ctc 16.198982 loss_rnnt 8.416849 hw_loss 0.249871 lr 0.00039314 rank 0
2023-02-24 20:03:54,508 DEBUG TRAIN Batch 19/3300 loss 6.052009 loss_att 10.589270 loss_ctc 10.692816 loss_rnnt 4.399391 hw_loss 0.236986 lr 0.00039314 rank 6
2023-02-24 20:03:54,510 DEBUG TRAIN Batch 19/3300 loss 12.057643 loss_att 14.266538 loss_ctc 13.895782 loss_rnnt 11.268904 hw_loss 0.191018 lr 0.00039314 rank 7
2023-02-24 20:03:54,511 DEBUG TRAIN Batch 19/3300 loss 7.036342 loss_att 10.803741 loss_ctc 12.671484 loss_rnnt 5.469260 hw_loss 0.116719 lr 0.00039314 rank 5
2023-02-24 20:03:54,513 DEBUG TRAIN Batch 19/3300 loss 10.819614 loss_att 16.116585 loss_ctc 15.417186 loss_rnnt 9.009565 hw_loss 0.258084 lr 0.00039314 rank 2
2023-02-24 20:03:54,551 DEBUG TRAIN Batch 19/3300 loss 9.303072 loss_att 13.695281 loss_ctc 16.029984 loss_rnnt 7.450323 hw_loss 0.145100 lr 0.00039314 rank 1
2023-02-24 20:03:54,582 DEBUG TRAIN Batch 19/3300 loss 11.004454 loss_att 12.516573 loss_ctc 11.348104 loss_rnnt 10.522491 hw_loss 0.250721 lr 0.00039314 rank 4
2023-02-24 20:05:06,907 DEBUG TRAIN Batch 19/3400 loss 9.591073 loss_att 12.670652 loss_ctc 16.837059 loss_rnnt 7.897630 hw_loss 0.208868 lr 0.00039302 rank 1
2023-02-24 20:05:06,907 DEBUG TRAIN Batch 19/3400 loss 13.841304 loss_att 17.232983 loss_ctc 19.810143 loss_rnnt 12.244830 hw_loss 0.229300 lr 0.00039302 rank 7
2023-02-24 20:05:06,909 DEBUG TRAIN Batch 19/3400 loss 14.081882 loss_att 14.505390 loss_ctc 24.036850 loss_rnnt 12.569807 hw_loss 0.187584 lr 0.00039302 rank 0
2023-02-24 20:05:06,918 DEBUG TRAIN Batch 19/3400 loss 7.267579 loss_att 9.395915 loss_ctc 9.386686 loss_rnnt 6.435670 hw_loss 0.231927 lr 0.00039302 rank 5
2023-02-24 20:05:06,919 DEBUG TRAIN Batch 19/3400 loss 5.595006 loss_att 8.246721 loss_ctc 6.209472 loss_rnnt 4.898782 hw_loss 0.157412 lr 0.00039302 rank 4
2023-02-24 20:05:06,933 DEBUG TRAIN Batch 19/3400 loss 9.641284 loss_att 14.170054 loss_ctc 11.687786 loss_rnnt 8.364046 hw_loss 0.184905 lr 0.00039302 rank 3
2023-02-24 20:05:06,934 DEBUG TRAIN Batch 19/3400 loss 8.756744 loss_att 12.027567 loss_ctc 15.259016 loss_rnnt 7.100989 hw_loss 0.252414 lr 0.00039302 rank 2
2023-02-24 20:05:06,956 DEBUG TRAIN Batch 19/3400 loss 7.100248 loss_att 11.843232 loss_ctc 9.262611 loss_rnnt 5.735836 hw_loss 0.239063 lr 0.00039302 rank 6
2023-02-24 20:06:19,524 DEBUG TRAIN Batch 19/3500 loss 11.983537 loss_att 14.403844 loss_ctc 14.189613 loss_rnnt 11.067670 hw_loss 0.258115 lr 0.00039290 rank 3
2023-02-24 20:06:19,529 DEBUG TRAIN Batch 19/3500 loss 11.624196 loss_att 13.942240 loss_ctc 16.072849 loss_rnnt 10.454351 hw_loss 0.212028 lr 0.00039290 rank 2
2023-02-24 20:06:19,530 DEBUG TRAIN Batch 19/3500 loss 12.392290 loss_att 15.096278 loss_ctc 18.818213 loss_rnnt 10.871970 hw_loss 0.230125 lr 0.00039290 rank 6
2023-02-24 20:06:19,530 DEBUG TRAIN Batch 19/3500 loss 9.805975 loss_att 14.231739 loss_ctc 14.252521 loss_rnnt 8.225969 hw_loss 0.191212 lr 0.00039290 rank 5
2023-02-24 20:06:19,532 DEBUG TRAIN Batch 19/3500 loss 5.994897 loss_att 8.951971 loss_ctc 7.499448 loss_rnnt 5.051836 hw_loss 0.283197 lr 0.00039290 rank 7
2023-02-24 20:06:19,535 DEBUG TRAIN Batch 19/3500 loss 9.150202 loss_att 12.783426 loss_ctc 14.930820 loss_rnnt 7.542905 hw_loss 0.206068 lr 0.00039290 rank 1
2023-02-24 20:06:19,535 DEBUG TRAIN Batch 19/3500 loss 20.902622 loss_att 26.101915 loss_ctc 27.375128 loss_rnnt 18.919928 hw_loss 0.149696 lr 0.00039290 rank 4
2023-02-24 20:06:19,537 DEBUG TRAIN Batch 19/3500 loss 8.004949 loss_att 13.859303 loss_ctc 10.891733 loss_rnnt 6.334661 hw_loss 0.214709 lr 0.00039290 rank 0
2023-02-24 20:07:33,881 DEBUG TRAIN Batch 19/3600 loss 6.262353 loss_att 10.142595 loss_ctc 10.275833 loss_rnnt 4.808447 hw_loss 0.267613 lr 0.00039278 rank 1
2023-02-24 20:07:33,881 DEBUG TRAIN Batch 19/3600 loss 7.478373 loss_att 9.595534 loss_ctc 8.968644 loss_rnnt 6.769030 hw_loss 0.163514 lr 0.00039278 rank 0
2023-02-24 20:07:33,884 DEBUG TRAIN Batch 19/3600 loss 9.532253 loss_att 11.721718 loss_ctc 17.808681 loss_rnnt 7.865166 hw_loss 0.235633 lr 0.00039278 rank 7
2023-02-24 20:07:33,884 DEBUG TRAIN Batch 19/3600 loss 5.167861 loss_att 5.830186 loss_ctc 5.194269 loss_rnnt 4.873623 hw_loss 0.296721 lr 0.00039278 rank 3
2023-02-24 20:07:33,886 DEBUG TRAIN Batch 19/3600 loss 6.283932 loss_att 10.343959 loss_ctc 9.784674 loss_rnnt 4.908788 hw_loss 0.180699 lr 0.00039278 rank 6
2023-02-24 20:07:33,890 DEBUG TRAIN Batch 19/3600 loss 7.527769 loss_att 10.236675 loss_ctc 10.957911 loss_rnnt 6.330187 hw_loss 0.372089 lr 0.00039278 rank 4
2023-02-24 20:07:33,892 DEBUG TRAIN Batch 19/3600 loss 10.813301 loss_att 11.470325 loss_ctc 11.025760 loss_rnnt 10.568876 hw_loss 0.158798 lr 0.00039278 rank 5
2023-02-24 20:07:33,894 DEBUG TRAIN Batch 19/3600 loss 1.957724 loss_att 3.974070 loss_ctc 2.008696 loss_rnnt 1.430215 hw_loss 0.220206 lr 0.00039278 rank 2
2023-02-24 20:08:48,864 DEBUG TRAIN Batch 19/3700 loss 6.184070 loss_att 6.281997 loss_ctc 7.473321 loss_rnnt 5.846026 hw_loss 0.274797 lr 0.00039266 rank 3
2023-02-24 20:08:48,864 DEBUG TRAIN Batch 19/3700 loss 10.595733 loss_att 11.368597 loss_ctc 12.958312 loss_rnnt 10.039981 hw_loss 0.161563 lr 0.00039266 rank 2
2023-02-24 20:08:48,867 DEBUG TRAIN Batch 19/3700 loss 6.964585 loss_att 8.151096 loss_ctc 8.131176 loss_rnnt 6.418081 hw_loss 0.288106 lr 0.00039266 rank 0
2023-02-24 20:08:48,868 DEBUG TRAIN Batch 19/3700 loss 11.040706 loss_att 12.847189 loss_ctc 15.941678 loss_rnnt 9.948883 hw_loss 0.144494 lr 0.00039266 rank 7
2023-02-24 20:08:48,867 DEBUG TRAIN Batch 19/3700 loss 11.326430 loss_att 11.614132 loss_ctc 14.518313 loss_rnnt 10.747515 hw_loss 0.179606 lr 0.00039266 rank 4
2023-02-24 20:08:48,869 DEBUG TRAIN Batch 19/3700 loss 7.695735 loss_att 11.399474 loss_ctc 8.474058 loss_rnnt 6.755342 hw_loss 0.179754 lr 0.00039266 rank 1
2023-02-24 20:08:48,870 DEBUG TRAIN Batch 19/3700 loss 10.485511 loss_att 10.880927 loss_ctc 13.495239 loss_rnnt 9.829894 hw_loss 0.328570 lr 0.00039266 rank 5
2023-02-24 20:08:48,870 DEBUG TRAIN Batch 19/3700 loss 5.029227 loss_att 7.497635 loss_ctc 9.410701 loss_rnnt 3.849004 hw_loss 0.191897 lr 0.00039266 rank 6
2023-02-24 20:10:02,176 DEBUG TRAIN Batch 19/3800 loss 6.573426 loss_att 10.556632 loss_ctc 10.110497 loss_rnnt 5.182092 hw_loss 0.230781 lr 0.00039254 rank 1
2023-02-24 20:10:02,177 DEBUG TRAIN Batch 19/3800 loss 3.830225 loss_att 5.433801 loss_ctc 4.991001 loss_rnnt 3.188458 hw_loss 0.311777 lr 0.00039254 rank 0
2023-02-24 20:10:02,178 DEBUG TRAIN Batch 19/3800 loss 9.416041 loss_att 11.628523 loss_ctc 14.092915 loss_rnnt 8.248064 hw_loss 0.191056 lr 0.00039254 rank 3
2023-02-24 20:10:02,180 DEBUG TRAIN Batch 19/3800 loss 12.744071 loss_att 15.042080 loss_ctc 16.638430 loss_rnnt 11.631412 hw_loss 0.250893 lr 0.00039254 rank 2
2023-02-24 20:10:02,181 DEBUG TRAIN Batch 19/3800 loss 11.141456 loss_att 10.854035 loss_ctc 13.529269 loss_rnnt 10.722434 hw_loss 0.296496 lr 0.00039254 rank 7
2023-02-24 20:10:02,191 DEBUG TRAIN Batch 19/3800 loss 13.784591 loss_att 14.643620 loss_ctc 18.587494 loss_rnnt 12.888824 hw_loss 0.156703 lr 0.00039254 rank 5
2023-02-24 20:10:02,234 DEBUG TRAIN Batch 19/3800 loss 3.756878 loss_att 6.328085 loss_ctc 4.580389 loss_rnnt 2.997006 hw_loss 0.254680 lr 0.00039254 rank 6
2023-02-24 20:10:02,243 DEBUG TRAIN Batch 19/3800 loss 14.671672 loss_att 15.387094 loss_ctc 21.978735 loss_rnnt 13.401164 hw_loss 0.287150 lr 0.00039254 rank 4
2023-02-24 20:11:17,416 DEBUG TRAIN Batch 19/3900 loss 10.308283 loss_att 11.726433 loss_ctc 12.286665 loss_rnnt 9.724302 hw_loss 0.068559 lr 0.00039242 rank 4
2023-02-24 20:11:17,422 DEBUG TRAIN Batch 19/3900 loss 10.785418 loss_att 11.016042 loss_ctc 13.890597 loss_rnnt 10.133955 hw_loss 0.358712 lr 0.00039242 rank 1
2023-02-24 20:11:17,423 DEBUG TRAIN Batch 19/3900 loss 3.955003 loss_att 6.880249 loss_ctc 5.546419 loss_rnnt 3.043563 hw_loss 0.214129 lr 0.00039242 rank 3
2023-02-24 20:11:17,423 DEBUG TRAIN Batch 19/3900 loss 11.328705 loss_att 13.147928 loss_ctc 14.624827 loss_rnnt 10.464008 hw_loss 0.115067 lr 0.00039242 rank 0
2023-02-24 20:11:17,424 DEBUG TRAIN Batch 19/3900 loss 10.519506 loss_att 14.800989 loss_ctc 11.528671 loss_rnnt 9.434065 hw_loss 0.177351 lr 0.00039242 rank 7
2023-02-24 20:11:17,427 DEBUG TRAIN Batch 19/3900 loss 12.434263 loss_att 15.607254 loss_ctc 19.685215 loss_rnnt 10.732962 hw_loss 0.187333 lr 0.00039242 rank 2
2023-02-24 20:11:17,432 DEBUG TRAIN Batch 19/3900 loss 15.956498 loss_att 24.332458 loss_ctc 23.176115 loss_rnnt 13.178282 hw_loss 0.263266 lr 0.00039242 rank 6
2023-02-24 20:11:17,450 DEBUG TRAIN Batch 19/3900 loss 3.960136 loss_att 4.861261 loss_ctc 5.192918 loss_rnnt 3.508356 hw_loss 0.200971 lr 0.00039242 rank 5
2023-02-24 20:12:30,040 DEBUG TRAIN Batch 19/4000 loss 7.444171 loss_att 9.417706 loss_ctc 8.157536 loss_rnnt 6.873712 hw_loss 0.151193 lr 0.00039230 rank 0
2023-02-24 20:12:30,054 DEBUG TRAIN Batch 19/4000 loss 10.358953 loss_att 12.339375 loss_ctc 12.531359 loss_rnnt 9.584477 hw_loss 0.166383 lr 0.00039230 rank 3
2023-02-24 20:12:30,055 DEBUG TRAIN Batch 19/4000 loss 3.328203 loss_att 8.141643 loss_ctc 3.465400 loss_rnnt 2.231463 hw_loss 0.217048 lr 0.00039230 rank 1
2023-02-24 20:12:30,056 DEBUG TRAIN Batch 19/4000 loss 6.113169 loss_att 10.517827 loss_ctc 10.702684 loss_rnnt 4.519061 hw_loss 0.189827 lr 0.00039230 rank 4
2023-02-24 20:12:30,057 DEBUG TRAIN Batch 19/4000 loss 3.571960 loss_att 7.895020 loss_ctc 4.514801 loss_rnnt 2.451596 hw_loss 0.243823 lr 0.00039230 rank 2
2023-02-24 20:12:30,059 DEBUG TRAIN Batch 19/4000 loss 10.804070 loss_att 12.347469 loss_ctc 12.270277 loss_rnnt 10.209075 hw_loss 0.170288 lr 0.00039230 rank 6
2023-02-24 20:12:30,061 DEBUG TRAIN Batch 19/4000 loss 12.962926 loss_att 18.975418 loss_ctc 16.990273 loss_rnnt 11.143461 hw_loss 0.149977 lr 0.00039230 rank 7
2023-02-24 20:12:30,070 DEBUG TRAIN Batch 19/4000 loss 6.227663 loss_att 10.161169 loss_ctc 8.397697 loss_rnnt 5.055549 hw_loss 0.180139 lr 0.00039230 rank 5
2023-02-24 20:13:42,164 DEBUG TRAIN Batch 19/4100 loss 27.330353 loss_att 30.316448 loss_ctc 37.837143 loss_rnnt 25.217136 hw_loss 0.215796 lr 0.00039217 rank 0
2023-02-24 20:13:42,170 DEBUG TRAIN Batch 19/4100 loss 3.239913 loss_att 6.240473 loss_ctc 6.720290 loss_rnnt 2.073644 hw_loss 0.191452 lr 0.00039217 rank 2
2023-02-24 20:13:42,172 DEBUG TRAIN Batch 19/4100 loss 6.159368 loss_att 10.636362 loss_ctc 9.834192 loss_rnnt 4.656247 hw_loss 0.220772 lr 0.00039217 rank 3
2023-02-24 20:13:42,171 DEBUG TRAIN Batch 19/4100 loss 7.026456 loss_att 9.924636 loss_ctc 10.351053 loss_rnnt 5.874092 hw_loss 0.242715 lr 0.00039217 rank 6
2023-02-24 20:13:42,171 DEBUG TRAIN Batch 19/4100 loss 12.113541 loss_att 15.502785 loss_ctc 19.199051 loss_rnnt 10.370910 hw_loss 0.225091 lr 0.00039217 rank 1
2023-02-24 20:13:42,173 DEBUG TRAIN Batch 19/4100 loss 8.109203 loss_att 9.343019 loss_ctc 10.261660 loss_rnnt 7.465546 hw_loss 0.206064 lr 0.00039217 rank 5
2023-02-24 20:13:42,174 DEBUG TRAIN Batch 19/4100 loss 21.715250 loss_att 26.135302 loss_ctc 39.185249 loss_rnnt 18.367723 hw_loss 0.251588 lr 0.00039217 rank 7
2023-02-24 20:13:42,173 DEBUG TRAIN Batch 19/4100 loss 4.296120 loss_att 7.123392 loss_ctc 4.104173 loss_rnnt 3.600174 hw_loss 0.292659 lr 0.00039217 rank 4
2023-02-24 20:14:55,000 DEBUG TRAIN Batch 19/4200 loss 15.877550 loss_att 19.853771 loss_ctc 18.446989 loss_rnnt 14.605278 hw_loss 0.252067 lr 0.00039205 rank 4
2023-02-24 20:14:55,000 DEBUG TRAIN Batch 19/4200 loss 12.603554 loss_att 14.418892 loss_ctc 16.241468 loss_rnnt 11.589415 hw_loss 0.311282 lr 0.00039205 rank 6
2023-02-24 20:14:55,001 DEBUG TRAIN Batch 19/4200 loss 9.788090 loss_att 11.512863 loss_ctc 12.944530 loss_rnnt 8.902633 hw_loss 0.224334 lr 0.00039205 rank 3
2023-02-24 20:14:55,001 DEBUG TRAIN Batch 19/4200 loss 7.664817 loss_att 12.378242 loss_ctc 12.753482 loss_rnnt 5.893222 hw_loss 0.282040 lr 0.00039205 rank 1
2023-02-24 20:14:55,005 DEBUG TRAIN Batch 19/4200 loss 11.503445 loss_att 15.296400 loss_ctc 21.754955 loss_rnnt 9.266683 hw_loss 0.208692 lr 0.00039205 rank 0
2023-02-24 20:14:55,005 DEBUG TRAIN Batch 19/4200 loss 12.581362 loss_att 15.569149 loss_ctc 15.123153 loss_rnnt 11.493390 hw_loss 0.284075 lr 0.00039205 rank 2
2023-02-24 20:14:55,006 DEBUG TRAIN Batch 19/4200 loss 5.170019 loss_att 8.094351 loss_ctc 8.784422 loss_rnnt 3.981003 hw_loss 0.229180 lr 0.00039205 rank 5
2023-02-24 20:14:55,011 DEBUG TRAIN Batch 19/4200 loss 7.413332 loss_att 10.579350 loss_ctc 9.318800 loss_rnnt 6.412766 hw_loss 0.212436 lr 0.00039205 rank 7
2023-02-24 20:16:09,593 DEBUG TRAIN Batch 19/4300 loss 5.074102 loss_att 7.522578 loss_ctc 7.972190 loss_rnnt 4.056422 hw_loss 0.265451 lr 0.00039193 rank 0
2023-02-24 20:16:09,596 DEBUG TRAIN Batch 19/4300 loss 11.217909 loss_att 13.198236 loss_ctc 13.431497 loss_rnnt 10.407001 hw_loss 0.224431 lr 0.00039193 rank 7
2023-02-24 20:16:09,595 DEBUG TRAIN Batch 19/4300 loss 5.738929 loss_att 7.899301 loss_ctc 6.606671 loss_rnnt 5.110235 hw_loss 0.151729 lr 0.00039193 rank 2
2023-02-24 20:16:09,597 DEBUG TRAIN Batch 19/4300 loss 5.974854 loss_att 8.400698 loss_ctc 8.216639 loss_rnnt 5.047313 hw_loss 0.269004 lr 0.00039193 rank 3
2023-02-24 20:16:09,599 DEBUG TRAIN Batch 19/4300 loss 14.499790 loss_att 15.583538 loss_ctc 23.710487 loss_rnnt 12.970852 hw_loss 0.157678 lr 0.00039193 rank 1
2023-02-24 20:16:09,602 DEBUG TRAIN Batch 19/4300 loss 5.486586 loss_att 7.195177 loss_ctc 9.835716 loss_rnnt 4.405133 hw_loss 0.299720 lr 0.00039193 rank 5
2023-02-24 20:16:09,608 DEBUG TRAIN Batch 19/4300 loss 6.853836 loss_att 10.113936 loss_ctc 11.386539 loss_rnnt 5.479351 hw_loss 0.221444 lr 0.00039193 rank 4
2023-02-24 20:16:09,649 DEBUG TRAIN Batch 19/4300 loss 13.881963 loss_att 16.196190 loss_ctc 19.066710 loss_rnnt 12.613556 hw_loss 0.214241 lr 0.00039193 rank 6
2023-02-24 20:17:23,425 DEBUG TRAIN Batch 19/4400 loss 10.676059 loss_att 14.282498 loss_ctc 18.260790 loss_rnnt 8.823349 hw_loss 0.225232 lr 0.00039181 rank 1
2023-02-24 20:17:23,426 DEBUG TRAIN Batch 19/4400 loss 8.083264 loss_att 9.808795 loss_ctc 10.793475 loss_rnnt 7.250112 hw_loss 0.237533 lr 0.00039181 rank 0
2023-02-24 20:17:23,434 DEBUG TRAIN Batch 19/4400 loss 6.877146 loss_att 10.595398 loss_ctc 7.946029 loss_rnnt 5.864174 hw_loss 0.237758 lr 0.00039181 rank 3
2023-02-24 20:17:23,435 DEBUG TRAIN Batch 19/4400 loss 9.992023 loss_att 11.276438 loss_ctc 11.972903 loss_rnnt 9.346207 hw_loss 0.234033 lr 0.00039181 rank 2
2023-02-24 20:17:23,440 DEBUG TRAIN Batch 19/4400 loss 6.700063 loss_att 9.882550 loss_ctc 8.616641 loss_rnnt 5.688851 hw_loss 0.223445 lr 0.00039181 rank 7
2023-02-24 20:17:23,441 DEBUG TRAIN Batch 19/4400 loss 9.294324 loss_att 10.094383 loss_ctc 12.410947 loss_rnnt 8.605990 hw_loss 0.211448 lr 0.00039181 rank 4
2023-02-24 20:17:23,444 DEBUG TRAIN Batch 19/4400 loss 3.727021 loss_att 9.321925 loss_ctc 3.153482 loss_rnnt 2.599322 hw_loss 0.159731 lr 0.00039181 rank 6
2023-02-24 20:17:23,446 DEBUG TRAIN Batch 19/4400 loss 8.652022 loss_att 9.439704 loss_ctc 10.768172 loss_rnnt 8.089159 hw_loss 0.230951 lr 0.00039181 rank 5
2023-02-24 20:18:35,511 DEBUG TRAIN Batch 19/4500 loss 11.275683 loss_att 11.870958 loss_ctc 13.121756 loss_rnnt 10.728063 hw_loss 0.342043 lr 0.00039169 rank 0
2023-02-24 20:18:35,519 DEBUG TRAIN Batch 19/4500 loss 6.032561 loss_att 7.218022 loss_ctc 10.154480 loss_rnnt 5.120516 hw_loss 0.235056 lr 0.00039169 rank 1
2023-02-24 20:18:35,520 DEBUG TRAIN Batch 19/4500 loss 17.730825 loss_att 22.086794 loss_ctc 23.287262 loss_rnnt 15.941482 hw_loss 0.332424 lr 0.00039169 rank 6
2023-02-24 20:18:35,522 DEBUG TRAIN Batch 19/4500 loss 12.857129 loss_att 15.602055 loss_ctc 18.530012 loss_rnnt 11.487007 hw_loss 0.121410 lr 0.00039169 rank 7
2023-02-24 20:18:35,522 DEBUG TRAIN Batch 19/4500 loss 14.865251 loss_att 21.730488 loss_ctc 19.324137 loss_rnnt 12.768672 hw_loss 0.241900 lr 0.00039169 rank 3
2023-02-24 20:18:35,522 DEBUG TRAIN Batch 19/4500 loss 30.115496 loss_att 32.697090 loss_ctc 43.132294 loss_rnnt 27.759548 hw_loss 0.195105 lr 0.00039169 rank 2
2023-02-24 20:18:35,523 DEBUG TRAIN Batch 19/4500 loss 6.635649 loss_att 11.049478 loss_ctc 10.617788 loss_rnnt 5.107702 hw_loss 0.214181 lr 0.00039169 rank 5
2023-02-24 20:18:35,524 DEBUG TRAIN Batch 19/4500 loss 12.555743 loss_att 13.675265 loss_ctc 17.058252 loss_rnnt 11.580769 hw_loss 0.282631 lr 0.00039169 rank 4
2023-02-24 20:19:49,686 DEBUG TRAIN Batch 19/4600 loss 10.345226 loss_att 17.494213 loss_ctc 17.008141 loss_rnnt 7.950846 hw_loss 0.142864 lr 0.00039157 rank 4
2023-02-24 20:19:49,687 DEBUG TRAIN Batch 19/4600 loss 2.982334 loss_att 5.692430 loss_ctc 4.230681 loss_rnnt 2.161233 hw_loss 0.211191 lr 0.00039157 rank 0
2023-02-24 20:19:49,688 DEBUG TRAIN Batch 19/4600 loss 16.068699 loss_att 17.582209 loss_ctc 17.072580 loss_rnnt 15.521086 hw_loss 0.208241 lr 0.00039157 rank 6
2023-02-24 20:19:49,689 DEBUG TRAIN Batch 19/4600 loss 7.997857 loss_att 10.736240 loss_ctc 10.005362 loss_rnnt 7.049022 hw_loss 0.250298 lr 0.00039157 rank 2
2023-02-24 20:19:49,693 DEBUG TRAIN Batch 19/4600 loss 19.977741 loss_att 25.318367 loss_ctc 26.822042 loss_rnnt 17.861443 hw_loss 0.254252 lr 0.00039157 rank 1
2023-02-24 20:19:49,694 DEBUG TRAIN Batch 19/4600 loss 10.372356 loss_att 13.212323 loss_ctc 14.685486 loss_rnnt 9.132526 hw_loss 0.181410 lr 0.00039157 rank 7
2023-02-24 20:19:49,701 DEBUG TRAIN Batch 19/4600 loss 11.751257 loss_att 14.474876 loss_ctc 17.725939 loss_rnnt 10.320528 hw_loss 0.167589 lr 0.00039157 rank 5
2023-02-24 20:19:49,737 DEBUG TRAIN Batch 19/4600 loss 14.174431 loss_att 15.925802 loss_ctc 15.356956 loss_rnnt 13.537090 hw_loss 0.242619 lr 0.00039157 rank 3
2023-02-24 20:21:03,120 DEBUG TRAIN Batch 19/4700 loss 4.679884 loss_att 8.032105 loss_ctc 7.766706 loss_rnnt 3.491526 hw_loss 0.199384 lr 0.00039145 rank 2
2023-02-24 20:21:03,132 DEBUG TRAIN Batch 19/4700 loss 9.281169 loss_att 10.742643 loss_ctc 14.739085 loss_rnnt 8.168150 hw_loss 0.174380 lr 0.00039145 rank 0
2023-02-24 20:21:03,133 DEBUG TRAIN Batch 19/4700 loss 14.143819 loss_att 20.654684 loss_ctc 21.615849 loss_rnnt 11.758190 hw_loss 0.163472 lr 0.00039145 rank 3
2023-02-24 20:21:03,134 DEBUG TRAIN Batch 19/4700 loss 6.674529 loss_att 9.905895 loss_ctc 8.292586 loss_rnnt 5.720478 hw_loss 0.172570 lr 0.00039145 rank 4
2023-02-24 20:21:03,137 DEBUG TRAIN Batch 19/4700 loss 7.227695 loss_att 9.959287 loss_ctc 12.054289 loss_rnnt 5.925582 hw_loss 0.210468 lr 0.00039145 rank 1
2023-02-24 20:21:03,139 DEBUG TRAIN Batch 19/4700 loss 4.278644 loss_att 7.403748 loss_ctc 8.267434 loss_rnnt 2.976256 hw_loss 0.272867 lr 0.00039145 rank 5
2023-02-24 20:21:03,142 DEBUG TRAIN Batch 19/4700 loss 12.752501 loss_att 14.369174 loss_ctc 18.985088 loss_rnnt 11.480209 hw_loss 0.221147 lr 0.00039145 rank 6
2023-02-24 20:21:03,143 DEBUG TRAIN Batch 19/4700 loss 14.310671 loss_att 18.458145 loss_ctc 19.463356 loss_rnnt 12.651270 hw_loss 0.267902 lr 0.00039145 rank 7
2023-02-24 20:22:15,354 DEBUG TRAIN Batch 19/4800 loss 6.709763 loss_att 10.165900 loss_ctc 9.797784 loss_rnnt 5.492987 hw_loss 0.213396 lr 0.00039133 rank 0
2023-02-24 20:22:15,353 DEBUG TRAIN Batch 19/4800 loss 10.949016 loss_att 16.898489 loss_ctc 15.484138 loss_rnnt 9.075945 hw_loss 0.147174 lr 0.00039133 rank 2
2023-02-24 20:22:15,354 DEBUG TRAIN Batch 19/4800 loss 11.875196 loss_att 13.881601 loss_ctc 16.962482 loss_rnnt 10.653913 hw_loss 0.265682 lr 0.00039133 rank 1
2023-02-24 20:22:15,354 DEBUG TRAIN Batch 19/4800 loss 16.488911 loss_att 21.863855 loss_ctc 26.883640 loss_rnnt 13.878520 hw_loss 0.280196 lr 0.00039133 rank 3
2023-02-24 20:22:15,357 DEBUG TRAIN Batch 19/4800 loss 8.249543 loss_att 8.895711 loss_ctc 10.575396 loss_rnnt 7.655612 hw_loss 0.289846 lr 0.00039133 rank 4
2023-02-24 20:22:15,360 DEBUG TRAIN Batch 19/4800 loss 7.062016 loss_att 8.749070 loss_ctc 9.180902 loss_rnnt 6.363514 hw_loss 0.147325 lr 0.00039133 rank 7
2023-02-24 20:22:15,419 DEBUG TRAIN Batch 19/4800 loss 16.019899 loss_att 18.237377 loss_ctc 24.563656 loss_rnnt 14.301235 hw_loss 0.255001 lr 0.00039133 rank 6
2023-02-24 20:22:15,435 DEBUG TRAIN Batch 19/4800 loss 8.259939 loss_att 9.954262 loss_ctc 10.901104 loss_rnnt 7.526116 hw_loss 0.080256 lr 0.00039133 rank 5
2023-02-24 20:23:28,503 DEBUG TRAIN Batch 19/4900 loss 14.491912 loss_att 17.899387 loss_ctc 24.397575 loss_rnnt 12.392010 hw_loss 0.183099 lr 0.00039121 rank 2
2023-02-24 20:23:28,503 DEBUG TRAIN Batch 19/4900 loss 6.145453 loss_att 9.546674 loss_ctc 7.651987 loss_rnnt 5.137886 hw_loss 0.237096 lr 0.00039121 rank 1
2023-02-24 20:23:28,507 DEBUG TRAIN Batch 19/4900 loss 17.367802 loss_att 18.057388 loss_ctc 22.236816 loss_rnnt 16.506060 hw_loss 0.139916 lr 0.00039121 rank 7
2023-02-24 20:23:28,507 DEBUG TRAIN Batch 19/4900 loss 14.831028 loss_att 17.651134 loss_ctc 25.381151 loss_rnnt 12.772932 hw_loss 0.163860 lr 0.00039121 rank 5
2023-02-24 20:23:28,521 DEBUG TRAIN Batch 19/4900 loss 3.275010 loss_att 5.472328 loss_ctc 4.650179 loss_rnnt 2.505644 hw_loss 0.274775 lr 0.00039121 rank 0
2023-02-24 20:23:28,531 DEBUG TRAIN Batch 19/4900 loss 6.808190 loss_att 9.352530 loss_ctc 11.443533 loss_rnnt 5.566692 hw_loss 0.214846 lr 0.00039121 rank 3
2023-02-24 20:23:28,536 DEBUG TRAIN Batch 19/4900 loss 4.286781 loss_att 7.758657 loss_ctc 7.714902 loss_rnnt 3.003435 hw_loss 0.247289 lr 0.00039121 rank 6
2023-02-24 20:23:28,580 DEBUG TRAIN Batch 19/4900 loss 11.632331 loss_att 15.885017 loss_ctc 17.663147 loss_rnnt 9.886290 hw_loss 0.171366 lr 0.00039121 rank 4
2023-02-24 20:24:43,813 DEBUG TRAIN Batch 19/5000 loss 9.076464 loss_att 12.324297 loss_ctc 13.760233 loss_rnnt 7.656717 hw_loss 0.273146 lr 0.00039109 rank 3
2023-02-24 20:24:43,817 DEBUG TRAIN Batch 19/5000 loss 16.749397 loss_att 17.939205 loss_ctc 20.489386 loss_rnnt 15.961293 hw_loss 0.096523 lr 0.00039109 rank 0
2023-02-24 20:24:43,819 DEBUG TRAIN Batch 19/5000 loss 8.288835 loss_att 10.962358 loss_ctc 12.337782 loss_rnnt 7.096679 hw_loss 0.220483 lr 0.00039109 rank 1
2023-02-24 20:24:43,819 DEBUG TRAIN Batch 19/5000 loss 9.323479 loss_att 12.123121 loss_ctc 12.909869 loss_rnnt 8.190071 hw_loss 0.178675 lr 0.00039109 rank 2
2023-02-24 20:24:43,820 DEBUG TRAIN Batch 19/5000 loss 8.816821 loss_att 10.329079 loss_ctc 10.432875 loss_rnnt 8.185232 hw_loss 0.213117 lr 0.00039109 rank 4
2023-02-24 20:24:43,821 DEBUG TRAIN Batch 19/5000 loss 10.885701 loss_att 10.956587 loss_ctc 14.772555 loss_rnnt 10.224281 hw_loss 0.241866 lr 0.00039109 rank 5
2023-02-24 20:24:43,824 DEBUG TRAIN Batch 19/5000 loss 11.068396 loss_att 11.247606 loss_ctc 14.826222 loss_rnnt 10.339091 hw_loss 0.360785 lr 0.00039109 rank 6
2023-02-24 20:24:43,825 DEBUG TRAIN Batch 19/5000 loss 17.443548 loss_att 19.601200 loss_ctc 22.663837 loss_rnnt 16.248346 hw_loss 0.126809 lr 0.00039109 rank 7
2023-02-24 20:25:57,042 DEBUG TRAIN Batch 19/5100 loss 2.747744 loss_att 5.433816 loss_ctc 3.495190 loss_rnnt 1.984800 hw_loss 0.236381 lr 0.00039097 rank 0
2023-02-24 20:25:57,051 DEBUG TRAIN Batch 19/5100 loss 12.358642 loss_att 17.694990 loss_ctc 18.812576 loss_rnnt 10.255263 hw_loss 0.329220 lr 0.00039097 rank 6
2023-02-24 20:25:57,052 DEBUG TRAIN Batch 19/5100 loss 4.107820 loss_att 6.508665 loss_ctc 5.587961 loss_rnnt 3.343895 hw_loss 0.162008 lr 0.00039097 rank 4
2023-02-24 20:25:57,053 DEBUG TRAIN Batch 19/5100 loss 5.229752 loss_att 5.846280 loss_ctc 7.049088 loss_rnnt 4.734812 hw_loss 0.241978 lr 0.00039097 rank 2
2023-02-24 20:25:57,055 DEBUG TRAIN Batch 19/5100 loss 6.278469 loss_att 10.139975 loss_ctc 12.289341 loss_rnnt 4.584447 hw_loss 0.225509 lr 0.00039097 rank 7
2023-02-24 20:25:57,057 DEBUG TRAIN Batch 19/5100 loss 13.545128 loss_att 13.344030 loss_ctc 17.680080 loss_rnnt 12.890354 hw_loss 0.269374 lr 0.00039097 rank 5
2023-02-24 20:25:57,077 DEBUG TRAIN Batch 19/5100 loss 4.504056 loss_att 5.782615 loss_ctc 5.495114 loss_rnnt 3.936904 hw_loss 0.336186 lr 0.00039097 rank 1
2023-02-24 20:25:57,077 DEBUG TRAIN Batch 19/5100 loss 11.736409 loss_att 14.820929 loss_ctc 18.776648 loss_rnnt 10.129673 hw_loss 0.095875 lr 0.00039097 rank 3
2023-02-24 20:27:08,883 DEBUG TRAIN Batch 19/5200 loss 15.368570 loss_att 16.394176 loss_ctc 18.021355 loss_rnnt 14.702221 hw_loss 0.201607 lr 0.00039085 rank 3
2023-02-24 20:27:08,883 DEBUG TRAIN Batch 19/5200 loss 6.938404 loss_att 8.994022 loss_ctc 8.049892 loss_rnnt 6.265817 hw_loss 0.212372 lr 0.00039085 rank 6
2023-02-24 20:27:08,884 DEBUG TRAIN Batch 19/5200 loss 8.435263 loss_att 12.269892 loss_ctc 13.331783 loss_rnnt 6.948503 hw_loss 0.125557 lr 0.00039085 rank 0
2023-02-24 20:27:08,887 DEBUG TRAIN Batch 19/5200 loss 6.113901 loss_att 9.532606 loss_ctc 10.557642 loss_rnnt 4.755811 hw_loss 0.153469 lr 0.00039085 rank 2
2023-02-24 20:27:08,887 DEBUG TRAIN Batch 19/5200 loss 13.823427 loss_att 17.264627 loss_ctc 15.891939 loss_rnnt 12.728363 hw_loss 0.245667 lr 0.00039085 rank 4
2023-02-24 20:27:08,890 DEBUG TRAIN Batch 19/5200 loss 7.305832 loss_att 9.786423 loss_ctc 7.785194 loss_rnnt 6.656848 hw_loss 0.166783 lr 0.00039085 rank 7
2023-02-24 20:27:08,891 DEBUG TRAIN Batch 19/5200 loss 16.522350 loss_att 18.522341 loss_ctc 20.185392 loss_rnnt 15.542852 hw_loss 0.170803 lr 0.00039085 rank 1
2023-02-24 20:27:08,892 DEBUG TRAIN Batch 19/5200 loss 13.512869 loss_att 15.372148 loss_ctc 13.240049 loss_rnnt 13.078588 hw_loss 0.185252 lr 0.00039085 rank 5
2023-02-24 20:28:23,456 DEBUG TRAIN Batch 19/5300 loss 6.706758 loss_att 10.272486 loss_ctc 6.387915 loss_rnnt 5.941394 hw_loss 0.177622 lr 0.00039073 rank 6
2023-02-24 20:28:23,456 DEBUG TRAIN Batch 19/5300 loss 8.198713 loss_att 11.079628 loss_ctc 11.986773 loss_rnnt 6.964881 hw_loss 0.286079 lr 0.00039073 rank 4
2023-02-24 20:28:23,464 DEBUG TRAIN Batch 19/5300 loss 10.867216 loss_att 12.635693 loss_ctc 16.116192 loss_rnnt 9.733013 hw_loss 0.151206 lr 0.00039073 rank 3
2023-02-24 20:28:23,466 DEBUG TRAIN Batch 19/5300 loss 6.927059 loss_att 12.600651 loss_ctc 11.063871 loss_rnnt 5.114777 hw_loss 0.236229 lr 0.00039073 rank 1
2023-02-24 20:28:23,467 DEBUG TRAIN Batch 19/5300 loss 11.273428 loss_att 13.355015 loss_ctc 13.335105 loss_rnnt 10.482452 hw_loss 0.187065 lr 0.00039073 rank 2
2023-02-24 20:28:23,467 DEBUG TRAIN Batch 19/5300 loss 11.979843 loss_att 14.208923 loss_ctc 15.065153 loss_rnnt 11.009681 hw_loss 0.211821 lr 0.00039073 rank 5
2023-02-24 20:28:23,470 DEBUG TRAIN Batch 19/5300 loss 6.035472 loss_att 9.566406 loss_ctc 7.997377 loss_rnnt 4.962634 hw_loss 0.196994 lr 0.00039073 rank 7
2023-02-24 20:28:23,473 DEBUG TRAIN Batch 19/5300 loss 13.833798 loss_att 14.623655 loss_ctc 18.274637 loss_rnnt 12.957583 hw_loss 0.236497 lr 0.00039073 rank 0
2023-02-24 20:29:37,133 DEBUG TRAIN Batch 19/5400 loss 14.325044 loss_att 18.229893 loss_ctc 20.469929 loss_rnnt 12.611746 hw_loss 0.211894 lr 0.00039062 rank 0
2023-02-24 20:29:37,135 DEBUG TRAIN Batch 19/5400 loss 10.961445 loss_att 13.506984 loss_ctc 15.687787 loss_rnnt 9.726331 hw_loss 0.179674 lr 0.00039062 rank 1
2023-02-24 20:29:37,135 DEBUG TRAIN Batch 19/5400 loss 4.647489 loss_att 7.857595 loss_ctc 6.661860 loss_rnnt 3.640669 hw_loss 0.180406 lr 0.00039062 rank 5
2023-02-24 20:29:37,139 DEBUG TRAIN Batch 19/5400 loss 4.242880 loss_att 7.196126 loss_ctc 7.104368 loss_rnnt 3.197544 hw_loss 0.137166 lr 0.00039062 rank 3
2023-02-24 20:29:37,140 DEBUG TRAIN Batch 19/5400 loss 10.747774 loss_att 11.656760 loss_ctc 17.398197 loss_rnnt 9.579935 hw_loss 0.186223 lr 0.00039062 rank 6
2023-02-24 20:29:37,139 DEBUG TRAIN Batch 19/5400 loss 11.818209 loss_att 12.594886 loss_ctc 17.056995 loss_rnnt 10.889844 hw_loss 0.139732 lr 0.00039062 rank 7
2023-02-24 20:29:37,142 DEBUG TRAIN Batch 19/5400 loss 7.459149 loss_att 10.247773 loss_ctc 7.606240 loss_rnnt 6.776527 hw_loss 0.197409 lr 0.00039062 rank 4
2023-02-24 20:29:37,151 DEBUG TRAIN Batch 19/5400 loss 5.092799 loss_att 7.807545 loss_ctc 8.390529 loss_rnnt 4.051990 hw_loss 0.109055 lr 0.00039062 rank 2
2023-02-24 20:30:50,027 DEBUG TRAIN Batch 19/5500 loss 8.765850 loss_att 11.973759 loss_ctc 13.242426 loss_rnnt 7.448843 hw_loss 0.147276 lr 0.00039050 rank 1
2023-02-24 20:30:50,028 DEBUG TRAIN Batch 19/5500 loss 8.797612 loss_att 9.471854 loss_ctc 13.149172 loss_rnnt 7.984871 hw_loss 0.183159 lr 0.00039050 rank 2
2023-02-24 20:30:50,030 DEBUG TRAIN Batch 19/5500 loss 4.829937 loss_att 10.085981 loss_ctc 6.647780 loss_rnnt 3.440145 hw_loss 0.180384 lr 0.00039050 rank 4
2023-02-24 20:30:50,032 DEBUG TRAIN Batch 19/5500 loss 13.938049 loss_att 17.251820 loss_ctc 21.719067 loss_rnnt 12.126829 hw_loss 0.208120 lr 0.00039050 rank 0
2023-02-24 20:30:50,035 DEBUG TRAIN Batch 19/5500 loss 10.155963 loss_att 13.623503 loss_ctc 15.091825 loss_rnnt 8.689034 hw_loss 0.216198 lr 0.00039050 rank 3
2023-02-24 20:30:50,037 DEBUG TRAIN Batch 19/5500 loss 15.354845 loss_att 17.570389 loss_ctc 24.194136 loss_rnnt 13.628159 hw_loss 0.196885 lr 0.00039050 rank 6
2023-02-24 20:30:50,036 DEBUG TRAIN Batch 19/5500 loss 11.665556 loss_att 13.637918 loss_ctc 17.874474 loss_rnnt 10.309073 hw_loss 0.251542 lr 0.00039050 rank 7
2023-02-24 20:30:50,041 DEBUG TRAIN Batch 19/5500 loss 12.411727 loss_att 14.929762 loss_ctc 18.237177 loss_rnnt 10.992775 hw_loss 0.259907 lr 0.00039050 rank 5
2023-02-24 20:32:03,488 DEBUG TRAIN Batch 19/5600 loss 4.149786 loss_att 6.181187 loss_ctc 6.177844 loss_rnnt 3.293164 hw_loss 0.337377 lr 0.00039038 rank 1
2023-02-24 20:32:03,492 DEBUG TRAIN Batch 19/5600 loss 4.723128 loss_att 8.531796 loss_ctc 8.190591 loss_rnnt 3.377969 hw_loss 0.227058 lr 0.00039038 rank 0
2023-02-24 20:32:03,494 DEBUG TRAIN Batch 19/5600 loss 9.815906 loss_att 12.872957 loss_ctc 11.695278 loss_rnnt 8.817276 hw_loss 0.256192 lr 0.00039038 rank 2
2023-02-24 20:32:03,495 DEBUG TRAIN Batch 19/5600 loss 7.066934 loss_att 7.464127 loss_ctc 8.110001 loss_rnnt 6.715732 hw_loss 0.248791 lr 0.00039038 rank 7
2023-02-24 20:32:03,496 DEBUG TRAIN Batch 19/5600 loss 7.438848 loss_att 10.264365 loss_ctc 13.573938 loss_rnnt 5.896497 hw_loss 0.298567 lr 0.00039038 rank 6
2023-02-24 20:32:03,497 DEBUG TRAIN Batch 19/5600 loss 14.089656 loss_att 19.166504 loss_ctc 18.882761 loss_rnnt 12.324301 hw_loss 0.207947 lr 0.00039038 rank 5
2023-02-24 20:32:03,501 DEBUG TRAIN Batch 19/5600 loss 7.518388 loss_att 9.393747 loss_ctc 9.897145 loss_rnnt 6.695754 hw_loss 0.244489 lr 0.00039038 rank 3
2023-02-24 20:32:03,540 DEBUG TRAIN Batch 19/5600 loss 5.143437 loss_att 7.773536 loss_ctc 7.693464 loss_rnnt 4.184690 hw_loss 0.173857 lr 0.00039038 rank 4
2023-02-24 20:33:18,890 DEBUG TRAIN Batch 19/5700 loss 7.865578 loss_att 12.008465 loss_ctc 12.653336 loss_rnnt 6.260227 hw_loss 0.259509 lr 0.00039026 rank 0
2023-02-24 20:33:18,890 DEBUG TRAIN Batch 19/5700 loss 14.642771 loss_att 14.111551 loss_ctc 18.215979 loss_rnnt 14.143776 hw_loss 0.241520 lr 0.00039026 rank 1
2023-02-24 20:33:18,892 DEBUG TRAIN Batch 19/5700 loss 11.729751 loss_att 13.619077 loss_ctc 13.770668 loss_rnnt 10.986510 hw_loss 0.174850 lr 0.00039026 rank 3
2023-02-24 20:33:18,896 DEBUG TRAIN Batch 19/5700 loss 10.018744 loss_att 12.670753 loss_ctc 14.431186 loss_rnnt 8.842514 hw_loss 0.107818 lr 0.00039026 rank 2
2023-02-24 20:33:18,896 DEBUG TRAIN Batch 19/5700 loss 2.561567 loss_att 5.416688 loss_ctc 4.417964 loss_rnnt 1.634829 hw_loss 0.202864 lr 0.00039026 rank 7
2023-02-24 20:33:18,904 DEBUG TRAIN Batch 19/5700 loss 2.514268 loss_att 5.051457 loss_ctc 4.400637 loss_rnnt 1.664614 hw_loss 0.170062 lr 0.00039026 rank 4
2023-02-24 20:33:18,903 DEBUG TRAIN Batch 19/5700 loss 9.367724 loss_att 10.093480 loss_ctc 14.519048 loss_rnnt 8.346093 hw_loss 0.355569 lr 0.00039026 rank 5
2023-02-24 20:33:18,911 DEBUG TRAIN Batch 19/5700 loss 10.231712 loss_att 10.738327 loss_ctc 10.078846 loss_rnnt 10.067724 hw_loss 0.155713 lr 0.00039026 rank 6
2023-02-24 20:34:31,881 DEBUG TRAIN Batch 19/5800 loss 6.646620 loss_att 6.584221 loss_ctc 8.667608 loss_rnnt 6.277876 hw_loss 0.209547 lr 0.00039014 rank 0
2023-02-24 20:34:31,883 DEBUG TRAIN Batch 19/5800 loss 12.578626 loss_att 18.900064 loss_ctc 17.265707 loss_rnnt 10.602843 hw_loss 0.162281 lr 0.00039014 rank 6
2023-02-24 20:34:31,887 DEBUG TRAIN Batch 19/5800 loss 6.805511 loss_att 9.754775 loss_ctc 11.411929 loss_rnnt 5.474998 hw_loss 0.237135 lr 0.00039014 rank 5
2023-02-24 20:34:31,891 DEBUG TRAIN Batch 19/5800 loss 6.543606 loss_att 11.754981 loss_ctc 9.763383 loss_rnnt 4.915357 hw_loss 0.293758 lr 0.00039014 rank 2
2023-02-24 20:34:31,893 DEBUG TRAIN Batch 19/5800 loss 4.855649 loss_att 8.039356 loss_ctc 6.706480 loss_rnnt 3.823611 hw_loss 0.278473 lr 0.00039014 rank 4
2023-02-24 20:34:31,894 DEBUG TRAIN Batch 19/5800 loss 13.672844 loss_att 15.739157 loss_ctc 17.751741 loss_rnnt 12.564483 hw_loss 0.283585 lr 0.00039014 rank 7
2023-02-24 20:34:31,940 DEBUG TRAIN Batch 19/5800 loss 9.919832 loss_att 13.113913 loss_ctc 10.126577 loss_rnnt 9.105848 hw_loss 0.276754 lr 0.00039014 rank 3
2023-02-24 20:34:31,962 DEBUG TRAIN Batch 19/5800 loss 9.137191 loss_att 13.446145 loss_ctc 12.064509 loss_rnnt 7.779922 hw_loss 0.197191 lr 0.00039014 rank 1
2023-02-24 20:35:44,476 DEBUG TRAIN Batch 19/5900 loss 8.347517 loss_att 14.089462 loss_ctc 14.180443 loss_rnnt 6.359037 hw_loss 0.116940 lr 0.00039002 rank 6
2023-02-24 20:35:44,477 DEBUG TRAIN Batch 19/5900 loss 7.060869 loss_att 8.476936 loss_ctc 9.830420 loss_rnnt 6.314023 hw_loss 0.176922 lr 0.00039002 rank 0
2023-02-24 20:35:44,477 DEBUG TRAIN Batch 19/5900 loss 11.509616 loss_att 19.228580 loss_ctc 19.524424 loss_rnnt 8.768887 hw_loss 0.240556 lr 0.00039002 rank 1
2023-02-24 20:35:44,480 DEBUG TRAIN Batch 19/5900 loss 11.439830 loss_att 12.612261 loss_ctc 14.337493 loss_rnnt 10.644028 hw_loss 0.328051 lr 0.00039002 rank 2
2023-02-24 20:35:44,483 DEBUG TRAIN Batch 19/5900 loss 10.939805 loss_att 15.160280 loss_ctc 19.741409 loss_rnnt 8.748301 hw_loss 0.325991 lr 0.00039002 rank 5
2023-02-24 20:35:44,485 DEBUG TRAIN Batch 19/5900 loss 7.243801 loss_att 10.357285 loss_ctc 8.394903 loss_rnnt 6.341299 hw_loss 0.236858 lr 0.00039002 rank 7
2023-02-24 20:35:44,485 DEBUG TRAIN Batch 19/5900 loss 10.087038 loss_att 11.071248 loss_ctc 13.026168 loss_rnnt 9.415791 hw_loss 0.154726 lr 0.00039002 rank 4
2023-02-24 20:35:44,526 DEBUG TRAIN Batch 19/5900 loss 16.771433 loss_att 19.263504 loss_ctc 29.453922 loss_rnnt 14.455909 hw_loss 0.236460 lr 0.00039002 rank 3
2023-02-24 20:36:58,193 DEBUG TRAIN Batch 19/6000 loss 7.322465 loss_att 11.758166 loss_ctc 10.950289 loss_rnnt 5.843387 hw_loss 0.202927 lr 0.00038990 rank 0
2023-02-24 20:36:58,200 DEBUG TRAIN Batch 19/6000 loss 4.637146 loss_att 6.079896 loss_ctc 6.476921 loss_rnnt 4.031260 hw_loss 0.135060 lr 0.00038990 rank 1
2023-02-24 20:36:58,201 DEBUG TRAIN Batch 19/6000 loss 23.392439 loss_att 26.667370 loss_ctc 30.408092 loss_rnnt 21.712463 hw_loss 0.167939 lr 0.00038990 rank 3
2023-02-24 20:36:58,204 DEBUG TRAIN Batch 19/6000 loss 15.533293 loss_att 16.020403 loss_ctc 16.963253 loss_rnnt 15.095996 hw_loss 0.279774 lr 0.00038990 rank 4
2023-02-24 20:36:58,205 DEBUG TRAIN Batch 19/6000 loss 3.516810 loss_att 7.248552 loss_ctc 6.475585 loss_rnnt 2.247454 hw_loss 0.240945 lr 0.00038990 rank 2
2023-02-24 20:36:58,210 DEBUG TRAIN Batch 19/6000 loss 5.063837 loss_att 6.972018 loss_ctc 7.124147 loss_rnnt 4.316970 hw_loss 0.169729 lr 0.00038990 rank 6
2023-02-24 20:36:58,210 DEBUG TRAIN Batch 19/6000 loss 6.578948 loss_att 10.473263 loss_ctc 7.350424 loss_rnnt 5.569963 hw_loss 0.238609 lr 0.00038990 rank 7
2023-02-24 20:36:58,215 DEBUG TRAIN Batch 19/6000 loss 9.826751 loss_att 12.402493 loss_ctc 15.782488 loss_rnnt 8.407696 hw_loss 0.205892 lr 0.00038990 rank 5
2023-02-24 20:38:11,151 DEBUG TRAIN Batch 19/6100 loss 4.400081 loss_att 6.350945 loss_ctc 8.426376 loss_rnnt 3.351182 hw_loss 0.228537 lr 0.00038978 rank 0
2023-02-24 20:38:11,152 DEBUG TRAIN Batch 19/6100 loss 6.025431 loss_att 8.547261 loss_ctc 9.936983 loss_rnnt 4.885804 hw_loss 0.213226 lr 0.00038978 rank 1
2023-02-24 20:38:11,152 DEBUG TRAIN Batch 19/6100 loss 13.945209 loss_att 18.005959 loss_ctc 21.167347 loss_rnnt 12.023663 hw_loss 0.274585 lr 0.00038978 rank 7
2023-02-24 20:38:11,153 DEBUG TRAIN Batch 19/6100 loss 7.748188 loss_att 11.804833 loss_ctc 10.807177 loss_rnnt 6.441489 hw_loss 0.164070 lr 0.00038978 rank 4
2023-02-24 20:38:11,155 DEBUG TRAIN Batch 19/6100 loss 9.101796 loss_att 10.308347 loss_ctc 12.255064 loss_rnnt 8.323833 hw_loss 0.217908 lr 0.00038978 rank 3
2023-02-24 20:38:11,157 DEBUG TRAIN Batch 19/6100 loss 11.174500 loss_att 14.990601 loss_ctc 18.827744 loss_rnnt 9.281984 hw_loss 0.204118 lr 0.00038978 rank 2
2023-02-24 20:38:11,159 DEBUG TRAIN Batch 19/6100 loss 11.458328 loss_att 13.788498 loss_ctc 14.607429 loss_rnnt 10.445536 hw_loss 0.237898 lr 0.00038978 rank 6
2023-02-24 20:38:11,162 DEBUG TRAIN Batch 19/6100 loss 9.082211 loss_att 12.354981 loss_ctc 13.181459 loss_rnnt 7.758787 hw_loss 0.229318 lr 0.00038978 rank 5
2023-02-24 20:39:23,555 DEBUG TRAIN Batch 19/6200 loss 7.417169 loss_att 11.820205 loss_ctc 9.993837 loss_rnnt 6.021973 hw_loss 0.320685 lr 0.00038967 rank 0
2023-02-24 20:39:23,555 DEBUG TRAIN Batch 19/6200 loss 9.227748 loss_att 12.431708 loss_ctc 11.641755 loss_rnnt 8.155703 hw_loss 0.205098 lr 0.00038967 rank 3
2023-02-24 20:39:23,559 DEBUG TRAIN Batch 19/6200 loss 7.029847 loss_att 9.388173 loss_ctc 10.559010 loss_rnnt 5.934458 hw_loss 0.287190 lr 0.00038967 rank 7
2023-02-24 20:39:23,560 DEBUG TRAIN Batch 19/6200 loss 7.534601 loss_att 9.862627 loss_ctc 9.218798 loss_rnnt 6.734686 hw_loss 0.205782 lr 0.00038967 rank 1
2023-02-24 20:39:23,562 DEBUG TRAIN Batch 19/6200 loss 12.356680 loss_att 13.210185 loss_ctc 16.862623 loss_rnnt 11.459813 hw_loss 0.235076 lr 0.00038967 rank 6
2023-02-24 20:39:23,562 DEBUG TRAIN Batch 19/6200 loss 22.704420 loss_att 26.037268 loss_ctc 37.544567 loss_rnnt 19.902411 hw_loss 0.293911 lr 0.00038967 rank 2
2023-02-24 20:39:23,562 DEBUG TRAIN Batch 19/6200 loss 9.636188 loss_att 13.587467 loss_ctc 15.660713 loss_rnnt 7.931872 hw_loss 0.207728 lr 0.00038967 rank 5
2023-02-24 20:39:23,564 DEBUG TRAIN Batch 19/6200 loss 7.901464 loss_att 10.017311 loss_ctc 13.923157 loss_rnnt 6.591107 hw_loss 0.158053 lr 0.00038967 rank 4
2023-02-24 20:40:35,874 DEBUG TRAIN Batch 19/6300 loss 4.896765 loss_att 6.607890 loss_ctc 6.576964 loss_rnnt 4.185043 hw_loss 0.272757 lr 0.00038955 rank 5
2023-02-24 20:40:35,875 DEBUG TRAIN Batch 19/6300 loss 6.029110 loss_att 8.398014 loss_ctc 7.771022 loss_rnnt 5.201967 hw_loss 0.227077 lr 0.00038955 rank 0
2023-02-24 20:40:35,875 DEBUG TRAIN Batch 19/6300 loss 7.537766 loss_att 8.766098 loss_ctc 10.272507 loss_rnnt 6.797567 hw_loss 0.243565 lr 0.00038955 rank 1
2023-02-24 20:40:35,876 DEBUG TRAIN Batch 19/6300 loss 6.398613 loss_att 7.738548 loss_ctc 10.489555 loss_rnnt 5.430202 hw_loss 0.290558 lr 0.00038955 rank 3
2023-02-24 20:40:35,881 DEBUG TRAIN Batch 19/6300 loss 11.019891 loss_att 10.329398 loss_ctc 14.729020 loss_rnnt 10.511549 hw_loss 0.284794 lr 0.00038955 rank 7
2023-02-24 20:40:35,907 DEBUG TRAIN Batch 19/6300 loss 7.569118 loss_att 10.962082 loss_ctc 10.624701 loss_rnnt 6.355721 hw_loss 0.238861 lr 0.00038955 rank 2
2023-02-24 20:40:35,910 DEBUG TRAIN Batch 19/6300 loss 6.487894 loss_att 9.784622 loss_ctc 9.287279 loss_rnnt 5.383412 hw_loss 0.134783 lr 0.00038955 rank 4
2023-02-24 20:40:35,921 DEBUG TRAIN Batch 19/6300 loss 8.883275 loss_att 10.320763 loss_ctc 12.473634 loss_rnnt 7.974603 hw_loss 0.267112 lr 0.00038955 rank 6
2023-02-24 20:41:51,131 DEBUG TRAIN Batch 19/6400 loss 4.181993 loss_att 6.561369 loss_ctc 4.132427 loss_rnnt 3.604251 hw_loss 0.203391 lr 0.00038943 rank 3
2023-02-24 20:41:51,132 DEBUG TRAIN Batch 19/6400 loss 14.529270 loss_att 17.446159 loss_ctc 19.997030 loss_rnnt 13.077436 hw_loss 0.261414 lr 0.00038943 rank 0
2023-02-24 20:41:51,135 DEBUG TRAIN Batch 19/6400 loss 12.618323 loss_att 18.257147 loss_ctc 15.517912 loss_rnnt 10.942893 hw_loss 0.301976 lr 0.00038943 rank 1
2023-02-24 20:41:51,137 DEBUG TRAIN Batch 19/6400 loss 12.541676 loss_att 14.551090 loss_ctc 18.018896 loss_rnnt 11.222177 hw_loss 0.351224 lr 0.00038943 rank 4
2023-02-24 20:41:51,139 DEBUG TRAIN Batch 19/6400 loss 11.730665 loss_att 19.196255 loss_ctc 19.833670 loss_rnnt 9.061259 hw_loss 0.179789 lr 0.00038943 rank 6
2023-02-24 20:41:51,139 DEBUG TRAIN Batch 19/6400 loss 8.601195 loss_att 12.447517 loss_ctc 14.521629 loss_rnnt 6.935088 hw_loss 0.201474 lr 0.00038943 rank 5
2023-02-24 20:41:51,140 DEBUG TRAIN Batch 19/6400 loss 6.945297 loss_att 7.132226 loss_ctc 8.884022 loss_rnnt 6.524131 hw_loss 0.234910 lr 0.00038943 rank 2
2023-02-24 20:41:51,172 DEBUG TRAIN Batch 19/6400 loss 14.676890 loss_att 15.902558 loss_ctc 19.824984 loss_rnnt 13.608904 hw_loss 0.255827 lr 0.00038943 rank 7
2023-02-24 20:43:04,298 DEBUG TRAIN Batch 19/6500 loss 15.479016 loss_att 19.664513 loss_ctc 21.889690 loss_rnnt 13.682699 hw_loss 0.195868 lr 0.00038931 rank 0
2023-02-24 20:43:04,304 DEBUG TRAIN Batch 19/6500 loss 5.774301 loss_att 10.036149 loss_ctc 10.426394 loss_rnnt 4.225729 hw_loss 0.142355 lr 0.00038931 rank 3
2023-02-24 20:43:04,307 DEBUG TRAIN Batch 19/6500 loss 3.113263 loss_att 6.275816 loss_ctc 3.965812 loss_rnnt 2.267663 hw_loss 0.186405 lr 0.00038931 rank 7
2023-02-24 20:43:04,309 DEBUG TRAIN Batch 19/6500 loss 12.190328 loss_att 13.351427 loss_ctc 17.158422 loss_rnnt 11.136204 hw_loss 0.299045 lr 0.00038931 rank 2
2023-02-24 20:43:04,313 DEBUG TRAIN Batch 19/6500 loss 16.208168 loss_att 23.167685 loss_ctc 20.123730 loss_rnnt 14.209800 hw_loss 0.158235 lr 0.00038931 rank 5
2023-02-24 20:43:04,314 DEBUG TRAIN Batch 19/6500 loss 12.879929 loss_att 18.687702 loss_ctc 23.077328 loss_rnnt 10.248708 hw_loss 0.206276 lr 0.00038931 rank 4
2023-02-24 20:43:04,321 DEBUG TRAIN Batch 19/6500 loss 20.129009 loss_att 23.316748 loss_ctc 26.329634 loss_rnnt 18.531414 hw_loss 0.249931 lr 0.00038931 rank 6
2023-02-24 20:43:04,324 DEBUG TRAIN Batch 19/6500 loss 4.360809 loss_att 10.965025 loss_ctc 8.397376 loss_rnnt 2.371996 hw_loss 0.243303 lr 0.00038931 rank 1
2023-02-24 20:44:16,560 DEBUG TRAIN Batch 19/6600 loss 10.125430 loss_att 10.579893 loss_ctc 12.217800 loss_rnnt 9.667496 hw_loss 0.165113 lr 0.00038919 rank 0
2023-02-24 20:44:16,564 DEBUG TRAIN Batch 19/6600 loss 4.261773 loss_att 7.787148 loss_ctc 7.005996 loss_rnnt 3.079865 hw_loss 0.208005 lr 0.00038919 rank 1
2023-02-24 20:44:16,567 DEBUG TRAIN Batch 19/6600 loss 8.274555 loss_att 10.682110 loss_ctc 11.218605 loss_rnnt 7.316485 hw_loss 0.157534 lr 0.00038919 rank 4
2023-02-24 20:44:16,569 DEBUG TRAIN Batch 19/6600 loss 10.959674 loss_att 15.257984 loss_ctc 17.250429 loss_rnnt 9.193261 hw_loss 0.127466 lr 0.00038919 rank 5
2023-02-24 20:44:16,571 DEBUG TRAIN Batch 19/6600 loss 5.109020 loss_att 7.609625 loss_ctc 8.568048 loss_rnnt 4.043704 hw_loss 0.194983 lr 0.00038919 rank 3
2023-02-24 20:44:16,570 DEBUG TRAIN Batch 19/6600 loss 9.362937 loss_att 12.389584 loss_ctc 13.953494 loss_rnnt 8.008425 hw_loss 0.257076 lr 0.00038919 rank 6
2023-02-24 20:44:16,572 DEBUG TRAIN Batch 19/6600 loss 5.538310 loss_att 8.534515 loss_ctc 7.506288 loss_rnnt 4.566255 hw_loss 0.207029 lr 0.00038919 rank 2
2023-02-24 20:44:16,572 DEBUG TRAIN Batch 19/6600 loss 7.932583 loss_att 12.199432 loss_ctc 11.720112 loss_rnnt 6.486546 hw_loss 0.164367 lr 0.00038919 rank 7
2023-02-24 20:45:29,529 DEBUG TRAIN Batch 19/6700 loss 10.094799 loss_att 12.153329 loss_ctc 13.768080 loss_rnnt 9.100619 hw_loss 0.173819 lr 0.00038908 rank 4
2023-02-24 20:45:29,537 DEBUG TRAIN Batch 19/6700 loss 10.925831 loss_att 12.100159 loss_ctc 16.136782 loss_rnnt 9.938961 hw_loss 0.107269 lr 0.00038908 rank 1
2023-02-24 20:45:29,540 DEBUG TRAIN Batch 19/6700 loss 10.855660 loss_att 13.162769 loss_ctc 13.835634 loss_rnnt 9.889605 hw_loss 0.201194 lr 0.00038908 rank 6
2023-02-24 20:45:29,540 DEBUG TRAIN Batch 19/6700 loss 5.471286 loss_att 8.759153 loss_ctc 10.124962 loss_rnnt 4.032196 hw_loss 0.301925 lr 0.00038908 rank 2
2023-02-24 20:45:29,542 DEBUG TRAIN Batch 19/6700 loss 7.819263 loss_att 9.295263 loss_ctc 8.952419 loss_rnnt 7.220773 hw_loss 0.285382 lr 0.00038908 rank 5
2023-02-24 20:45:29,544 DEBUG TRAIN Batch 19/6700 loss 3.726817 loss_att 6.450597 loss_ctc 5.078051 loss_rnnt 2.943226 hw_loss 0.110008 lr 0.00038908 rank 0
2023-02-24 20:45:29,547 DEBUG TRAIN Batch 19/6700 loss 12.952687 loss_att 12.977854 loss_ctc 16.501934 loss_rnnt 12.392323 hw_loss 0.153935 lr 0.00038908 rank 7
2023-02-24 20:45:29,585 DEBUG TRAIN Batch 19/6700 loss 7.928581 loss_att 13.284369 loss_ctc 13.780163 loss_rnnt 5.916811 hw_loss 0.300752 lr 0.00038908 rank 3
2023-02-24 20:46:43,679 DEBUG TRAIN Batch 19/6800 loss 12.458142 loss_att 14.030807 loss_ctc 18.814322 loss_rnnt 11.186159 hw_loss 0.206172 lr 0.00038896 rank 0
2023-02-24 20:46:43,681 DEBUG TRAIN Batch 19/6800 loss 11.948103 loss_att 15.978188 loss_ctc 16.753864 loss_rnnt 10.397490 hw_loss 0.194679 lr 0.00038896 rank 3
2023-02-24 20:46:43,681 DEBUG TRAIN Batch 19/6800 loss 6.817707 loss_att 9.604010 loss_ctc 10.530003 loss_rnnt 5.595456 hw_loss 0.318782 lr 0.00038896 rank 4
2023-02-24 20:46:43,686 DEBUG TRAIN Batch 19/6800 loss 14.190269 loss_att 14.774630 loss_ctc 18.735510 loss_rnnt 13.358056 hw_loss 0.204954 lr 0.00038896 rank 7
2023-02-24 20:46:43,688 DEBUG TRAIN Batch 19/6800 loss 13.384759 loss_att 13.916651 loss_ctc 14.862114 loss_rnnt 12.945278 hw_loss 0.255227 lr 0.00038896 rank 1
2023-02-24 20:46:43,690 DEBUG TRAIN Batch 19/6800 loss 3.994383 loss_att 6.310977 loss_ctc 6.436387 loss_rnnt 3.061703 hw_loss 0.269551 lr 0.00038896 rank 5
2023-02-24 20:46:43,690 DEBUG TRAIN Batch 19/6800 loss 11.179590 loss_att 14.907831 loss_ctc 18.049242 loss_rnnt 9.414858 hw_loss 0.193370 lr 0.00038896 rank 2
2023-02-24 20:46:43,698 DEBUG TRAIN Batch 19/6800 loss 7.168340 loss_att 10.985828 loss_ctc 12.701559 loss_rnnt 5.537132 hw_loss 0.243652 lr 0.00038896 rank 6
2023-02-24 20:47:56,811 DEBUG TRAIN Batch 19/6900 loss 10.200883 loss_att 13.726798 loss_ctc 16.374302 loss_rnnt 8.564481 hw_loss 0.202680 lr 0.00038884 rank 0
2023-02-24 20:47:56,819 DEBUG TRAIN Batch 19/6900 loss 5.879952 loss_att 9.420595 loss_ctc 8.694311 loss_rnnt 4.680287 hw_loss 0.218041 lr 0.00038884 rank 1
2023-02-24 20:47:56,820 DEBUG TRAIN Batch 19/6900 loss 15.359540 loss_att 18.192081 loss_ctc 24.775875 loss_rnnt 13.384766 hw_loss 0.286416 lr 0.00038884 rank 4
2023-02-24 20:47:56,822 DEBUG TRAIN Batch 19/6900 loss 6.856476 loss_att 8.009330 loss_ctc 8.302190 loss_rnnt 6.297760 hw_loss 0.253844 lr 0.00038884 rank 7
2023-02-24 20:47:56,824 DEBUG TRAIN Batch 19/6900 loss 17.567677 loss_att 21.272621 loss_ctc 25.142494 loss_rnnt 15.683517 hw_loss 0.249737 lr 0.00038884 rank 2
2023-02-24 20:47:56,851 DEBUG TRAIN Batch 19/6900 loss 7.854914 loss_att 9.989850 loss_ctc 11.240458 loss_rnnt 6.852295 hw_loss 0.232923 lr 0.00038884 rank 6
2023-02-24 20:47:56,857 DEBUG TRAIN Batch 19/6900 loss 5.913340 loss_att 7.186017 loss_ctc 6.082548 loss_rnnt 5.527145 hw_loss 0.204560 lr 0.00038884 rank 3
2023-02-24 20:47:56,868 DEBUG TRAIN Batch 19/6900 loss 11.065865 loss_att 12.691351 loss_ctc 16.979691 loss_rnnt 9.858195 hw_loss 0.176366 lr 0.00038884 rank 5
2023-02-24 20:49:10,447 DEBUG TRAIN Batch 19/7000 loss 25.847837 loss_att 24.931831 loss_ctc 34.208649 loss_rnnt 24.768927 hw_loss 0.276259 lr 0.00038872 rank 6
2023-02-24 20:49:10,451 DEBUG TRAIN Batch 19/7000 loss 6.454885 loss_att 10.286291 loss_ctc 9.113667 loss_rnnt 5.184291 hw_loss 0.280893 lr 0.00038872 rank 3
2023-02-24 20:49:10,451 DEBUG TRAIN Batch 19/7000 loss 11.121580 loss_att 14.077103 loss_ctc 15.580637 loss_rnnt 9.771708 hw_loss 0.307925 lr 0.00038872 rank 1
2023-02-24 20:49:10,452 DEBUG TRAIN Batch 19/7000 loss 10.472569 loss_att 11.965311 loss_ctc 14.448707 loss_rnnt 9.510250 hw_loss 0.250532 lr 0.00038872 rank 2
2023-02-24 20:49:10,455 DEBUG TRAIN Batch 19/7000 loss 6.581105 loss_att 7.619675 loss_ctc 9.269232 loss_rnnt 5.896865 hw_loss 0.221455 lr 0.00038872 rank 0
2023-02-24 20:49:10,457 DEBUG TRAIN Batch 19/7000 loss 10.340846 loss_att 15.441534 loss_ctc 12.832476 loss_rnnt 8.874491 hw_loss 0.213749 lr 0.00038872 rank 7
2023-02-24 20:49:10,460 DEBUG TRAIN Batch 19/7000 loss 10.696755 loss_att 13.096186 loss_ctc 14.001314 loss_rnnt 9.626307 hw_loss 0.281164 lr 0.00038872 rank 5
2023-02-24 20:49:10,504 DEBUG TRAIN Batch 19/7000 loss 14.290893 loss_att 18.057835 loss_ctc 17.283033 loss_rnnt 13.019257 hw_loss 0.223679 lr 0.00038872 rank 4
2023-02-24 20:50:25,588 DEBUG TRAIN Batch 19/7100 loss 4.731761 loss_att 6.079107 loss_ctc 6.615017 loss_rnnt 4.062284 hw_loss 0.279200 lr 0.00038860 rank 4
2023-02-24 20:50:25,600 DEBUG TRAIN Batch 19/7100 loss 14.338940 loss_att 22.511389 loss_ctc 25.773216 loss_rnnt 11.026529 hw_loss 0.287527 lr 0.00038860 rank 0
2023-02-24 20:50:25,601 DEBUG TRAIN Batch 19/7100 loss 5.125180 loss_att 7.760243 loss_ctc 7.625223 loss_rnnt 4.137631 hw_loss 0.238493 lr 0.00038860 rank 7
2023-02-24 20:50:25,604 DEBUG TRAIN Batch 19/7100 loss 6.101090 loss_att 8.452586 loss_ctc 10.141673 loss_rnnt 4.979885 hw_loss 0.210304 lr 0.00038860 rank 5
2023-02-24 20:50:25,626 DEBUG TRAIN Batch 19/7100 loss 7.927667 loss_att 10.824233 loss_ctc 12.888698 loss_rnnt 6.542510 hw_loss 0.270698 lr 0.00038860 rank 6
2023-02-24 20:50:25,627 DEBUG TRAIN Batch 19/7100 loss 10.253761 loss_att 10.833486 loss_ctc 13.626308 loss_rnnt 9.566294 hw_loss 0.228466 lr 0.00038860 rank 3
2023-02-24 20:50:25,630 DEBUG TRAIN Batch 19/7100 loss 9.004922 loss_att 11.520763 loss_ctc 13.560653 loss_rnnt 7.807374 hw_loss 0.163029 lr 0.00038860 rank 2
2023-02-24 20:50:25,654 DEBUG TRAIN Batch 19/7100 loss 13.375735 loss_att 16.872498 loss_ctc 16.886505 loss_rnnt 12.152461 hw_loss 0.104662 lr 0.00038860 rank 1
2023-02-24 20:51:38,962 DEBUG TRAIN Batch 19/7200 loss 10.291309 loss_att 16.335699 loss_ctc 15.384401 loss_rnnt 8.289483 hw_loss 0.213503 lr 0.00038849 rank 5
2023-02-24 20:51:38,966 DEBUG TRAIN Batch 19/7200 loss 5.799416 loss_att 9.763704 loss_ctc 9.069815 loss_rnnt 4.466774 hw_loss 0.194498 lr 0.00038849 rank 7
2023-02-24 20:51:38,968 DEBUG TRAIN Batch 19/7200 loss 7.284847 loss_att 7.582686 loss_ctc 7.809789 loss_rnnt 6.974579 hw_loss 0.338829 lr 0.00038849 rank 1
2023-02-24 20:51:38,969 DEBUG TRAIN Batch 19/7200 loss 12.280232 loss_att 13.749256 loss_ctc 14.513759 loss_rnnt 11.535070 hw_loss 0.287915 lr 0.00038849 rank 3
2023-02-24 20:51:38,970 DEBUG TRAIN Batch 19/7200 loss 6.459985 loss_att 10.124747 loss_ctc 9.046289 loss_rnnt 5.244874 hw_loss 0.257472 lr 0.00038849 rank 0
2023-02-24 20:51:38,970 DEBUG TRAIN Batch 19/7200 loss 6.445181 loss_att 7.670313 loss_ctc 9.394463 loss_rnnt 5.761356 hw_loss 0.085427 lr 0.00038849 rank 2
2023-02-24 20:51:38,975 DEBUG TRAIN Batch 19/7200 loss 7.247157 loss_att 8.841534 loss_ctc 8.045176 loss_rnnt 6.683660 hw_loss 0.259160 lr 0.00038849 rank 6
2023-02-24 20:51:38,976 DEBUG TRAIN Batch 19/7200 loss 7.310260 loss_att 9.760635 loss_ctc 11.004486 loss_rnnt 6.162992 hw_loss 0.308680 lr 0.00038849 rank 4
2023-02-24 20:52:51,114 DEBUG TRAIN Batch 19/7300 loss 9.011315 loss_att 12.350206 loss_ctc 13.779676 loss_rnnt 7.626653 hw_loss 0.152065 lr 0.00038837 rank 0
2023-02-24 20:52:51,117 DEBUG TRAIN Batch 19/7300 loss 15.588318 loss_att 18.220867 loss_ctc 23.290314 loss_rnnt 13.893906 hw_loss 0.264318 lr 0.00038837 rank 6
2023-02-24 20:52:51,118 DEBUG TRAIN Batch 19/7300 loss 22.982552 loss_att 25.589537 loss_ctc 40.401707 loss_rnnt 20.006531 hw_loss 0.247631 lr 0.00038837 rank 7
2023-02-24 20:52:51,121 DEBUG TRAIN Batch 19/7300 loss 4.081815 loss_att 7.072604 loss_ctc 6.154806 loss_rnnt 3.122839 hw_loss 0.158284 lr 0.00038837 rank 1
2023-02-24 20:52:51,121 DEBUG TRAIN Batch 19/7300 loss 7.317739 loss_att 9.326580 loss_ctc 8.336437 loss_rnnt 6.661528 hw_loss 0.222406 lr 0.00038837 rank 4
2023-02-24 20:52:51,124 DEBUG TRAIN Batch 19/7300 loss 8.456084 loss_att 10.357190 loss_ctc 9.705386 loss_rnnt 7.822670 hw_loss 0.162410 lr 0.00038837 rank 5
2023-02-24 20:52:51,125 DEBUG TRAIN Batch 19/7300 loss 5.832951 loss_att 8.090989 loss_ctc 7.889649 loss_rnnt 4.965078 hw_loss 0.266323 lr 0.00038837 rank 2
2023-02-24 20:52:51,167 DEBUG TRAIN Batch 19/7300 loss 8.220678 loss_att 11.194296 loss_ctc 13.257163 loss_rnnt 6.857227 hw_loss 0.182243 lr 0.00038837 rank 3
2023-02-24 20:54:04,585 DEBUG TRAIN Batch 19/7400 loss 12.277723 loss_att 17.841805 loss_ctc 18.670887 loss_rnnt 10.209017 hw_loss 0.194004 lr 0.00038825 rank 6
2023-02-24 20:54:04,588 DEBUG TRAIN Batch 19/7400 loss 3.668592 loss_att 4.907900 loss_ctc 6.560653 loss_rnnt 2.954579 hw_loss 0.151020 lr 0.00038825 rank 4
2023-02-24 20:54:04,593 DEBUG TRAIN Batch 19/7400 loss 9.120120 loss_att 12.569304 loss_ctc 15.136419 loss_rnnt 7.532461 hw_loss 0.179344 lr 0.00038825 rank 0
2023-02-24 20:54:04,597 DEBUG TRAIN Batch 19/7400 loss 8.629094 loss_att 12.376163 loss_ctc 11.194832 loss_rnnt 7.437895 hw_loss 0.186914 lr 0.00038825 rank 1
2023-02-24 20:54:04,602 DEBUG TRAIN Batch 19/7400 loss 9.008318 loss_att 12.278273 loss_ctc 9.609999 loss_rnnt 8.154078 hw_loss 0.225046 lr 0.00038825 rank 7
2023-02-24 20:54:04,604 DEBUG TRAIN Batch 19/7400 loss 7.756558 loss_att 14.191536 loss_ctc 13.907156 loss_rnnt 5.471407 hw_loss 0.333892 lr 0.00038825 rank 2
2023-02-24 20:54:04,606 DEBUG TRAIN Batch 19/7400 loss 11.413095 loss_att 15.224737 loss_ctc 15.050094 loss_rnnt 10.067972 hw_loss 0.183490 lr 0.00038825 rank 3
2023-02-24 20:54:04,607 DEBUG TRAIN Batch 19/7400 loss 5.635535 loss_att 7.476991 loss_ctc 6.077638 loss_rnnt 5.123837 hw_loss 0.158362 lr 0.00038825 rank 5
2023-02-24 20:55:19,073 DEBUG TRAIN Batch 19/7500 loss 5.825340 loss_att 7.285160 loss_ctc 9.405685 loss_rnnt 4.914303 hw_loss 0.265675 lr 0.00038814 rank 3
2023-02-24 20:55:19,074 DEBUG TRAIN Batch 19/7500 loss 1.335896 loss_att 3.419360 loss_ctc 1.806610 loss_rnnt 0.756792 hw_loss 0.186842 lr 0.00038814 rank 0
2023-02-24 20:55:19,075 DEBUG TRAIN Batch 19/7500 loss 11.446921 loss_att 11.795782 loss_ctc 12.886398 loss_rnnt 11.089984 hw_loss 0.178566 lr 0.00038814 rank 2
2023-02-24 20:55:19,077 DEBUG TRAIN Batch 19/7500 loss 11.203687 loss_att 13.335251 loss_ctc 16.533165 loss_rnnt 9.953929 hw_loss 0.211590 lr 0.00038814 rank 6
2023-02-24 20:55:19,078 DEBUG TRAIN Batch 19/7500 loss 5.914188 loss_att 7.785775 loss_ctc 8.214067 loss_rnnt 5.124846 hw_loss 0.203201 lr 0.00038814 rank 7
2023-02-24 20:55:19,082 DEBUG TRAIN Batch 19/7500 loss 12.806360 loss_att 16.917608 loss_ctc 16.815464 loss_rnnt 11.306826 hw_loss 0.267631 lr 0.00038814 rank 1
2023-02-24 20:55:19,083 DEBUG TRAIN Batch 19/7500 loss 8.958604 loss_att 15.538143 loss_ctc 16.490139 loss_rnnt 6.508418 hw_loss 0.243887 lr 0.00038814 rank 4
2023-02-24 20:55:19,085 DEBUG TRAIN Batch 19/7500 loss 5.228690 loss_att 8.955576 loss_ctc 6.165579 loss_rnnt 4.252221 hw_loss 0.199074 lr 0.00038814 rank 5
2023-02-24 20:56:33,867 DEBUG TRAIN Batch 19/7600 loss 3.794957 loss_att 6.521937 loss_ctc 4.890795 loss_rnnt 2.974646 hw_loss 0.241506 lr 0.00038802 rank 7
2023-02-24 20:56:33,876 DEBUG TRAIN Batch 19/7600 loss 12.720936 loss_att 14.245609 loss_ctc 18.964478 loss_rnnt 11.465166 hw_loss 0.221926 lr 0.00038802 rank 0
2023-02-24 20:56:33,883 DEBUG TRAIN Batch 19/7600 loss 14.525230 loss_att 18.406309 loss_ctc 15.023334 loss_rnnt 13.500927 hw_loss 0.340637 lr 0.00038802 rank 6
2023-02-24 20:56:33,885 DEBUG TRAIN Batch 19/7600 loss 8.933279 loss_att 9.321124 loss_ctc 13.009250 loss_rnnt 8.208970 hw_loss 0.193643 lr 0.00038802 rank 5
2023-02-24 20:56:33,886 DEBUG TRAIN Batch 19/7600 loss 10.191067 loss_att 10.204124 loss_ctc 14.891599 loss_rnnt 9.404431 hw_loss 0.294911 lr 0.00038802 rank 3
2023-02-24 20:56:33,889 DEBUG TRAIN Batch 19/7600 loss 6.450970 loss_att 9.389297 loss_ctc 10.165477 loss_rnnt 5.200212 hw_loss 0.314673 lr 0.00038802 rank 2
2023-02-24 20:56:33,890 DEBUG TRAIN Batch 19/7600 loss 7.742686 loss_att 8.996130 loss_ctc 9.974360 loss_rnnt 7.074117 hw_loss 0.225605 lr 0.00038802 rank 4
2023-02-24 20:56:33,941 DEBUG TRAIN Batch 19/7600 loss 6.474667 loss_att 8.287643 loss_ctc 9.552928 loss_rnnt 5.583080 hw_loss 0.222294 lr 0.00038802 rank 1
2023-02-24 20:57:46,994 DEBUG TRAIN Batch 19/7700 loss 8.189666 loss_att 10.459288 loss_ctc 10.730230 loss_rnnt 7.317779 hw_loss 0.148536 lr 0.00038790 rank 7
2023-02-24 20:57:46,996 DEBUG TRAIN Batch 19/7700 loss 6.457549 loss_att 10.514821 loss_ctc 11.574463 loss_rnnt 4.838912 hw_loss 0.234236 lr 0.00038790 rank 3
2023-02-24 20:57:46,997 DEBUG TRAIN Batch 19/7700 loss 7.288979 loss_att 9.239140 loss_ctc 9.604202 loss_rnnt 6.509033 hw_loss 0.152281 lr 0.00038790 rank 6
2023-02-24 20:57:46,998 DEBUG TRAIN Batch 19/7700 loss 13.476953 loss_att 14.993084 loss_ctc 19.156876 loss_rnnt 12.294024 hw_loss 0.229461 lr 0.00038790 rank 2
2023-02-24 20:57:46,998 DEBUG TRAIN Batch 19/7700 loss 6.773974 loss_att 6.391683 loss_ctc 8.050385 loss_rnnt 6.539917 hw_loss 0.263115 lr 0.00038790 rank 0
2023-02-24 20:57:46,999 DEBUG TRAIN Batch 19/7700 loss 8.968695 loss_att 13.105244 loss_ctc 13.204726 loss_rnnt 7.435949 hw_loss 0.263683 lr 0.00038790 rank 5
2023-02-24 20:57:47,003 DEBUG TRAIN Batch 19/7700 loss 10.224125 loss_att 14.965576 loss_ctc 12.681870 loss_rnnt 8.868992 hw_loss 0.148395 lr 0.00038790 rank 1
2023-02-24 20:57:47,004 DEBUG TRAIN Batch 19/7700 loss 14.467320 loss_att 17.165936 loss_ctc 19.605503 loss_rnnt 13.096034 hw_loss 0.274637 lr 0.00038790 rank 4
2023-02-24 20:59:01,626 DEBUG TRAIN Batch 19/7800 loss 7.055630 loss_att 10.599358 loss_ctc 9.475424 loss_rnnt 5.872634 hw_loss 0.284271 lr 0.00038779 rank 2
2023-02-24 20:59:01,631 DEBUG TRAIN Batch 19/7800 loss 10.928969 loss_att 14.994164 loss_ctc 16.292183 loss_rnnt 9.242568 hw_loss 0.296751 lr 0.00038779 rank 0
2023-02-24 20:59:01,637 DEBUG TRAIN Batch 19/7800 loss 5.079635 loss_att 6.745008 loss_ctc 6.320711 loss_rnnt 4.498445 hw_loss 0.154948 lr 0.00038779 rank 1
2023-02-24 20:59:01,639 DEBUG TRAIN Batch 19/7800 loss 9.166510 loss_att 12.632824 loss_ctc 13.724083 loss_rnnt 7.730241 hw_loss 0.253741 lr 0.00038779 rank 3
2023-02-24 20:59:01,643 DEBUG TRAIN Batch 19/7800 loss 6.919195 loss_att 8.087974 loss_ctc 7.690362 loss_rnnt 6.458015 hw_loss 0.233627 lr 0.00038779 rank 6
2023-02-24 20:59:01,643 DEBUG TRAIN Batch 19/7800 loss 11.351988 loss_att 14.270585 loss_ctc 15.076643 loss_rnnt 10.132673 hw_loss 0.260577 lr 0.00038779 rank 7
2023-02-24 20:59:01,647 DEBUG TRAIN Batch 19/7800 loss 9.615413 loss_att 12.863340 loss_ctc 16.537991 loss_rnnt 7.937829 hw_loss 0.196850 lr 0.00038779 rank 5
2023-02-24 20:59:01,647 DEBUG TRAIN Batch 19/7800 loss 7.083101 loss_att 9.214096 loss_ctc 9.224215 loss_rnnt 6.235484 hw_loss 0.254880 lr 0.00038779 rank 4
2023-02-24 21:00:15,285 DEBUG TRAIN Batch 19/7900 loss 13.464659 loss_att 16.782820 loss_ctc 19.175026 loss_rnnt 11.907495 hw_loss 0.247780 lr 0.00038767 rank 3
2023-02-24 21:00:15,286 DEBUG TRAIN Batch 19/7900 loss 16.492134 loss_att 16.832195 loss_ctc 20.707672 loss_rnnt 15.786734 hw_loss 0.141220 lr 0.00038767 rank 0
2023-02-24 21:00:15,287 DEBUG TRAIN Batch 19/7900 loss 10.442332 loss_att 12.945065 loss_ctc 15.521174 loss_rnnt 9.112766 hw_loss 0.284701 lr 0.00038767 rank 1
2023-02-24 21:00:15,289 DEBUG TRAIN Batch 19/7900 loss 13.961492 loss_att 16.200054 loss_ctc 19.957912 loss_rnnt 12.569250 hw_loss 0.271889 lr 0.00038767 rank 4
2023-02-24 21:00:15,291 DEBUG TRAIN Batch 19/7900 loss 16.673952 loss_att 19.342806 loss_ctc 23.025238 loss_rnnt 15.202084 hw_loss 0.171115 lr 0.00038767 rank 7
2023-02-24 21:00:15,293 DEBUG TRAIN Batch 19/7900 loss 18.066509 loss_att 21.763872 loss_ctc 24.009413 loss_rnnt 16.452488 hw_loss 0.154053 lr 0.00038767 rank 2
2023-02-24 21:00:15,292 DEBUG TRAIN Batch 19/7900 loss 9.852431 loss_att 13.036785 loss_ctc 18.294439 loss_rnnt 7.958451 hw_loss 0.246577 lr 0.00038767 rank 6
2023-02-24 21:00:15,340 DEBUG TRAIN Batch 19/7900 loss 7.121362 loss_att 10.508726 loss_ctc 11.232225 loss_rnnt 5.798862 hw_loss 0.181710 lr 0.00038767 rank 5
2023-02-24 21:01:27,920 DEBUG TRAIN Batch 19/8000 loss 7.283166 loss_att 9.573106 loss_ctc 9.990484 loss_rnnt 6.398378 hw_loss 0.123422 lr 0.00038755 rank 0
2023-02-24 21:01:27,927 DEBUG TRAIN Batch 19/8000 loss 3.345735 loss_att 5.629185 loss_ctc 3.538187 loss_rnnt 2.775113 hw_loss 0.165510 lr 0.00038755 rank 3
2023-02-24 21:01:27,928 DEBUG TRAIN Batch 19/8000 loss 5.888172 loss_att 7.348388 loss_ctc 6.068355 loss_rnnt 5.459272 hw_loss 0.211560 lr 0.00038755 rank 6
2023-02-24 21:01:27,930 DEBUG TRAIN Batch 19/8000 loss 5.260715 loss_att 7.996922 loss_ctc 5.673249 loss_rnnt 4.565366 hw_loss 0.174571 lr 0.00038755 rank 1
2023-02-24 21:01:27,930 DEBUG TRAIN Batch 19/8000 loss 15.987082 loss_att 18.065617 loss_ctc 22.767467 loss_rnnt 14.562910 hw_loss 0.195775 lr 0.00038755 rank 5
2023-02-24 21:01:27,931 DEBUG TRAIN Batch 19/8000 loss 3.472664 loss_att 7.663240 loss_ctc 8.175190 loss_rnnt 1.941011 hw_loss 0.124750 lr 0.00038755 rank 7
2023-02-24 21:01:27,934 DEBUG TRAIN Batch 19/8000 loss 15.791039 loss_att 16.514935 loss_ctc 20.770607 loss_rnnt 14.879223 hw_loss 0.193302 lr 0.00038755 rank 2
2023-02-24 21:01:27,934 DEBUG TRAIN Batch 19/8000 loss 8.877141 loss_att 12.452405 loss_ctc 12.700684 loss_rnnt 7.532800 hw_loss 0.224029 lr 0.00038755 rank 4
2023-02-24 21:02:40,621 DEBUG TRAIN Batch 19/8100 loss 4.838226 loss_att 7.789066 loss_ctc 8.456620 loss_rnnt 3.684875 hw_loss 0.151369 lr 0.00038744 rank 0
2023-02-24 21:02:40,621 DEBUG TRAIN Batch 19/8100 loss 4.202348 loss_att 8.183801 loss_ctc 6.952381 loss_rnnt 2.952299 hw_loss 0.163290 lr 0.00038744 rank 3
2023-02-24 21:02:40,624 DEBUG TRAIN Batch 19/8100 loss 11.655195 loss_att 15.572262 loss_ctc 16.517193 loss_rnnt 10.145432 hw_loss 0.146409 lr 0.00038744 rank 5
2023-02-24 21:02:40,625 DEBUG TRAIN Batch 19/8100 loss 7.694377 loss_att 10.154099 loss_ctc 10.615451 loss_rnnt 6.688622 hw_loss 0.233125 lr 0.00038744 rank 6
2023-02-24 21:02:40,625 DEBUG TRAIN Batch 19/8100 loss 10.310599 loss_att 11.509490 loss_ctc 13.267219 loss_rnnt 9.512568 hw_loss 0.307570 lr 0.00038744 rank 2
2023-02-24 21:02:40,626 DEBUG TRAIN Batch 19/8100 loss 6.996936 loss_att 8.796628 loss_ctc 9.101270 loss_rnnt 6.256019 hw_loss 0.188252 lr 0.00038744 rank 4
2023-02-24 21:02:40,629 DEBUG TRAIN Batch 19/8100 loss 9.749359 loss_att 13.880304 loss_ctc 17.380478 loss_rnnt 7.767763 hw_loss 0.258610 lr 0.00038744 rank 7
2023-02-24 21:02:40,672 DEBUG TRAIN Batch 19/8100 loss 4.186345 loss_att 7.633408 loss_ctc 5.532722 loss_rnnt 3.221205 hw_loss 0.180395 lr 0.00038744 rank 1
2023-02-24 21:03:53,824 DEBUG TRAIN Batch 19/8200 loss 10.491410 loss_att 11.196757 loss_ctc 13.382775 loss_rnnt 9.842723 hw_loss 0.228941 lr 0.00038732 rank 3
2023-02-24 21:03:53,825 DEBUG TRAIN Batch 19/8200 loss 9.361279 loss_att 12.187370 loss_ctc 12.046000 loss_rnnt 8.351346 hw_loss 0.162658 lr 0.00038732 rank 1
2023-02-24 21:03:53,825 DEBUG TRAIN Batch 19/8200 loss 8.588034 loss_att 7.415036 loss_ctc 10.355717 loss_rnnt 8.409990 hw_loss 0.331785 lr 0.00038732 rank 7
2023-02-24 21:03:53,828 DEBUG TRAIN Batch 19/8200 loss 6.904182 loss_att 10.842760 loss_ctc 8.944584 loss_rnnt 5.769413 hw_loss 0.140624 lr 0.00038732 rank 0
2023-02-24 21:03:53,832 DEBUG TRAIN Batch 19/8200 loss 10.174537 loss_att 13.508548 loss_ctc 12.963160 loss_rnnt 8.971138 hw_loss 0.308961 lr 0.00038732 rank 5
2023-02-24 21:03:53,832 DEBUG TRAIN Batch 19/8200 loss 23.097996 loss_att 24.526659 loss_ctc 25.797901 loss_rnnt 22.279875 hw_loss 0.323254 lr 0.00038732 rank 4
2023-02-24 21:03:53,835 DEBUG TRAIN Batch 19/8200 loss 17.833744 loss_att 21.663139 loss_ctc 26.686478 loss_rnnt 15.734808 hw_loss 0.286299 lr 0.00038732 rank 6
2023-02-24 21:03:53,878 DEBUG TRAIN Batch 19/8200 loss 11.596593 loss_att 13.497747 loss_ctc 15.720575 loss_rnnt 10.552014 hw_loss 0.214656 lr 0.00038732 rank 2
2023-02-24 21:05:06,783 DEBUG TRAIN Batch 19/8300 loss 12.948377 loss_att 16.626556 loss_ctc 19.312674 loss_rnnt 11.244025 hw_loss 0.225268 lr 0.00038720 rank 7
2023-02-24 21:05:06,791 DEBUG TRAIN Batch 19/8300 loss 8.959641 loss_att 10.815497 loss_ctc 11.887635 loss_rnnt 8.138810 hw_loss 0.111110 lr 0.00038720 rank 0
2023-02-24 21:05:06,798 DEBUG TRAIN Batch 19/8300 loss 7.103896 loss_att 13.426458 loss_ctc 16.445236 loss_rnnt 4.522661 hw_loss 0.133518 lr 0.00038720 rank 3
2023-02-24 21:05:06,798 DEBUG TRAIN Batch 19/8300 loss 3.911296 loss_att 5.137006 loss_ctc 5.549282 loss_rnnt 3.376163 hw_loss 0.134236 lr 0.00038720 rank 1
2023-02-24 21:05:06,799 DEBUG TRAIN Batch 19/8300 loss 7.798038 loss_att 8.601215 loss_ctc 9.183429 loss_rnnt 7.346619 hw_loss 0.198870 lr 0.00038720 rank 2
2023-02-24 21:05:06,800 DEBUG TRAIN Batch 19/8300 loss 10.302855 loss_att 13.348463 loss_ctc 11.724751 loss_rnnt 9.391066 hw_loss 0.212025 lr 0.00038720 rank 4
2023-02-24 21:05:06,801 DEBUG TRAIN Batch 19/8300 loss 13.787741 loss_att 17.218159 loss_ctc 21.745199 loss_rnnt 11.882975 hw_loss 0.295662 lr 0.00038720 rank 6
2023-02-24 21:05:06,804 DEBUG TRAIN Batch 19/8300 loss 7.269506 loss_att 7.075430 loss_ctc 7.740152 loss_rnnt 7.081268 hw_loss 0.308061 lr 0.00038720 rank 5
2023-02-24 21:05:56,272 DEBUG CV Batch 19/0 loss 2.252826 loss_att 2.282295 loss_ctc 2.895600 loss_rnnt 2.030564 hw_loss 0.244996 history loss 2.169388 rank 2
2023-02-24 21:05:56,279 DEBUG CV Batch 19/0 loss 2.252826 loss_att 2.282295 loss_ctc 2.895600 loss_rnnt 2.030564 hw_loss 0.244996 history loss 2.169388 rank 1
2023-02-24 21:05:56,282 DEBUG CV Batch 19/0 loss 2.252826 loss_att 2.282295 loss_ctc 2.895600 loss_rnnt 2.030564 hw_loss 0.244996 history loss 2.169388 rank 0
2023-02-24 21:05:56,282 DEBUG CV Batch 19/0 loss 2.252826 loss_att 2.282295 loss_ctc 2.895600 loss_rnnt 2.030564 hw_loss 0.244996 history loss 2.169388 rank 7
2023-02-24 21:05:56,288 DEBUG CV Batch 19/0 loss 2.252826 loss_att 2.282295 loss_ctc 2.895600 loss_rnnt 2.030564 hw_loss 0.244996 history loss 2.169388 rank 6
2023-02-24 21:05:56,297 DEBUG CV Batch 19/0 loss 2.252826 loss_att 2.282295 loss_ctc 2.895600 loss_rnnt 2.030564 hw_loss 0.244996 history loss 2.169388 rank 5
2023-02-24 21:05:56,307 DEBUG CV Batch 19/0 loss 2.252826 loss_att 2.282295 loss_ctc 2.895600 loss_rnnt 2.030564 hw_loss 0.244996 history loss 2.169388 rank 3
2023-02-24 21:05:56,313 DEBUG CV Batch 19/0 loss 2.252826 loss_att 2.282295 loss_ctc 2.895600 loss_rnnt 2.030564 hw_loss 0.244996 history loss 2.169388 rank 4
2023-02-24 21:06:07,683 DEBUG CV Batch 19/100 loss 6.010865 loss_att 6.842583 loss_ctc 9.598722 loss_rnnt 5.263536 hw_loss 0.192381 history loss 3.308819 rank 0
2023-02-24 21:06:07,863 DEBUG CV Batch 19/100 loss 6.010865 loss_att 6.842583 loss_ctc 9.598722 loss_rnnt 5.263536 hw_loss 0.192381 history loss 3.308819 rank 1
2023-02-24 21:06:08,513 DEBUG CV Batch 19/100 loss 6.010865 loss_att 6.842583 loss_ctc 9.598722 loss_rnnt 5.263536 hw_loss 0.192381 history loss 3.308819 rank 3
2023-02-24 21:06:08,529 DEBUG CV Batch 19/100 loss 6.010865 loss_att 6.842583 loss_ctc 9.598722 loss_rnnt 5.263536 hw_loss 0.192381 history loss 3.308819 rank 7
2023-02-24 21:06:08,560 DEBUG CV Batch 19/100 loss 6.010865 loss_att 6.842583 loss_ctc 9.598722 loss_rnnt 5.263536 hw_loss 0.192381 history loss 3.308819 rank 2
2023-02-24 21:06:08,612 DEBUG CV Batch 19/100 loss 6.010865 loss_att 6.842583 loss_ctc 9.598722 loss_rnnt 5.263536 hw_loss 0.192381 history loss 3.308819 rank 5
2023-02-24 21:06:08,634 DEBUG CV Batch 19/100 loss 6.010865 loss_att 6.842583 loss_ctc 9.598722 loss_rnnt 5.263536 hw_loss 0.192381 history loss 3.308819 rank 6
2023-02-24 21:06:08,691 DEBUG CV Batch 19/100 loss 6.010865 loss_att 6.842583 loss_ctc 9.598722 loss_rnnt 5.263536 hw_loss 0.192381 history loss 3.308819 rank 4
2023-02-24 21:06:21,182 DEBUG CV Batch 19/200 loss 6.152955 loss_att 12.297145 loss_ctc 6.876553 loss_rnnt 4.758402 hw_loss 0.129815 history loss 4.006242 rank 0
2023-02-24 21:06:21,487 DEBUG CV Batch 19/200 loss 6.152955 loss_att 12.297145 loss_ctc 6.876553 loss_rnnt 4.758402 hw_loss 0.129815 history loss 4.006242 rank 1
2023-02-24 21:06:22,139 DEBUG CV Batch 19/200 loss 6.152955 loss_att 12.297145 loss_ctc 6.876553 loss_rnnt 4.758402 hw_loss 0.129815 history loss 4.006242 rank 7
2023-02-24 21:06:22,191 DEBUG CV Batch 19/200 loss 6.152955 loss_att 12.297145 loss_ctc 6.876553 loss_rnnt 4.758402 hw_loss 0.129815 history loss 4.006242 rank 2
2023-02-24 21:06:22,347 DEBUG CV Batch 19/200 loss 6.152955 loss_att 12.297145 loss_ctc 6.876553 loss_rnnt 4.758402 hw_loss 0.129815 history loss 4.006242 rank 3
2023-02-24 21:06:22,453 DEBUG CV Batch 19/200 loss 6.152955 loss_att 12.297145 loss_ctc 6.876553 loss_rnnt 4.758402 hw_loss 0.129815 history loss 4.006242 rank 6
2023-02-24 21:06:22,583 DEBUG CV Batch 19/200 loss 6.152955 loss_att 12.297145 loss_ctc 6.876553 loss_rnnt 4.758402 hw_loss 0.129815 history loss 4.006242 rank 5
2023-02-24 21:06:22,669 DEBUG CV Batch 19/200 loss 6.152955 loss_att 12.297145 loss_ctc 6.876553 loss_rnnt 4.758402 hw_loss 0.129815 history loss 4.006242 rank 4
2023-02-24 21:06:33,854 DEBUG CV Batch 19/300 loss 4.039258 loss_att 4.465497 loss_ctc 5.600933 loss_rnnt 3.624364 hw_loss 0.227666 history loss 4.153277 rank 0
2023-02-24 21:06:34,442 DEBUG CV Batch 19/300 loss 4.039258 loss_att 4.465497 loss_ctc 5.600933 loss_rnnt 3.624364 hw_loss 0.227666 history loss 4.153277 rank 7
2023-02-24 21:06:34,492 DEBUG CV Batch 19/300 loss 4.039258 loss_att 4.465497 loss_ctc 5.600933 loss_rnnt 3.624364 hw_loss 0.227666 history loss 4.153277 rank 1
2023-02-24 21:06:34,670 DEBUG CV Batch 19/300 loss 4.039258 loss_att 4.465497 loss_ctc 5.600933 loss_rnnt 3.624364 hw_loss 0.227666 history loss 4.153277 rank 3
2023-02-24 21:06:34,688 DEBUG CV Batch 19/300 loss 4.039258 loss_att 4.465497 loss_ctc 5.600933 loss_rnnt 3.624364 hw_loss 0.227666 history loss 4.153277 rank 6
2023-02-24 21:06:34,840 DEBUG CV Batch 19/300 loss 4.039258 loss_att 4.465497 loss_ctc 5.600933 loss_rnnt 3.624364 hw_loss 0.227666 history loss 4.153277 rank 2
2023-02-24 21:06:34,939 DEBUG CV Batch 19/300 loss 4.039258 loss_att 4.465497 loss_ctc 5.600933 loss_rnnt 3.624364 hw_loss 0.227666 history loss 4.153277 rank 5
2023-02-24 21:06:35,402 DEBUG CV Batch 19/300 loss 4.039258 loss_att 4.465497 loss_ctc 5.600933 loss_rnnt 3.624364 hw_loss 0.227666 history loss 4.153277 rank 4
2023-02-24 21:06:45,945 DEBUG CV Batch 19/400 loss 19.522970 loss_att 91.998230 loss_ctc 11.800204 loss_rnnt 5.943120 hw_loss 0.214688 history loss 5.080249 rank 0
2023-02-24 21:06:46,645 DEBUG CV Batch 19/400 loss 19.522970 loss_att 91.998230 loss_ctc 11.800204 loss_rnnt 5.943120 hw_loss 0.214688 history loss 5.080249 rank 7
2023-02-24 21:06:46,875 DEBUG CV Batch 19/400 loss 19.522970 loss_att 91.998230 loss_ctc 11.800204 loss_rnnt 5.943120 hw_loss 0.214688 history loss 5.080249 rank 3
2023-02-24 21:06:46,877 DEBUG CV Batch 19/400 loss 19.522970 loss_att 91.998230 loss_ctc 11.800204 loss_rnnt 5.943120 hw_loss 0.214688 history loss 5.080249 rank 6
2023-02-24 21:06:46,993 DEBUG CV Batch 19/400 loss 19.522970 loss_att 91.998230 loss_ctc 11.800204 loss_rnnt 5.943120 hw_loss 0.214688 history loss 5.080249 rank 1
2023-02-24 21:06:47,156 DEBUG CV Batch 19/400 loss 19.522970 loss_att 91.998230 loss_ctc 11.800204 loss_rnnt 5.943120 hw_loss 0.214688 history loss 5.080249 rank 2
2023-02-24 21:06:47,625 DEBUG CV Batch 19/400 loss 19.522970 loss_att 91.998230 loss_ctc 11.800204 loss_rnnt 5.943120 hw_loss 0.214688 history loss 5.080249 rank 5
2023-02-24 21:06:47,830 DEBUG CV Batch 19/400 loss 19.522970 loss_att 91.998230 loss_ctc 11.800204 loss_rnnt 5.943120 hw_loss 0.214688 history loss 5.080249 rank 4
2023-02-24 21:06:56,391 DEBUG CV Batch 19/500 loss 5.900213 loss_att 6.754057 loss_ctc 7.885577 loss_rnnt 5.362287 hw_loss 0.192078 history loss 5.802999 rank 0
2023-02-24 21:06:57,283 DEBUG CV Batch 19/500 loss 5.900213 loss_att 6.754057 loss_ctc 7.885577 loss_rnnt 5.362287 hw_loss 0.192078 history loss 5.802999 rank 7
2023-02-24 21:06:57,399 DEBUG CV Batch 19/500 loss 5.900213 loss_att 6.754057 loss_ctc 7.885577 loss_rnnt 5.362287 hw_loss 0.192078 history loss 5.802999 rank 6
2023-02-24 21:06:57,648 DEBUG CV Batch 19/500 loss 5.900213 loss_att 6.754057 loss_ctc 7.885577 loss_rnnt 5.362287 hw_loss 0.192078 history loss 5.802999 rank 2
2023-02-24 21:06:57,915 DEBUG CV Batch 19/500 loss 5.900213 loss_att 6.754057 loss_ctc 7.885577 loss_rnnt 5.362287 hw_loss 0.192078 history loss 5.802999 rank 1
2023-02-24 21:06:58,059 DEBUG CV Batch 19/500 loss 5.900213 loss_att 6.754057 loss_ctc 7.885577 loss_rnnt 5.362287 hw_loss 0.192078 history loss 5.802999 rank 3
2023-02-24 21:06:58,643 DEBUG CV Batch 19/500 loss 5.900213 loss_att 6.754057 loss_ctc 7.885577 loss_rnnt 5.362287 hw_loss 0.192078 history loss 5.802999 rank 5
2023-02-24 21:06:58,760 DEBUG CV Batch 19/500 loss 5.900213 loss_att 6.754057 loss_ctc 7.885577 loss_rnnt 5.362287 hw_loss 0.192078 history loss 5.802999 rank 4
2023-02-24 21:07:08,569 DEBUG CV Batch 19/600 loss 7.497055 loss_att 7.666872 loss_ctc 9.572770 loss_rnnt 7.036204 hw_loss 0.281485 history loss 6.760705 rank 0
2023-02-24 21:07:09,550 DEBUG CV Batch 19/600 loss 7.497055 loss_att 7.666872 loss_ctc 9.572770 loss_rnnt 7.036204 hw_loss 0.281485 history loss 6.760705 rank 7
2023-02-24 21:07:09,685 DEBUG CV Batch 19/600 loss 7.497055 loss_att 7.666872 loss_ctc 9.572770 loss_rnnt 7.036204 hw_loss 0.281485 history loss 6.760705 rank 6
2023-02-24 21:07:09,897 DEBUG CV Batch 19/600 loss 7.497055 loss_att 7.666872 loss_ctc 9.572770 loss_rnnt 7.036204 hw_loss 0.281485 history loss 6.760705 rank 2
2023-02-24 21:07:10,230 DEBUG CV Batch 19/600 loss 7.497055 loss_att 7.666872 loss_ctc 9.572770 loss_rnnt 7.036204 hw_loss 0.281485 history loss 6.760705 rank 1
2023-02-24 21:07:10,742 DEBUG CV Batch 19/600 loss 7.497055 loss_att 7.666872 loss_ctc 9.572770 loss_rnnt 7.036204 hw_loss 0.281485 history loss 6.760705 rank 3
2023-02-24 21:07:11,103 DEBUG CV Batch 19/600 loss 7.497055 loss_att 7.666872 loss_ctc 9.572770 loss_rnnt 7.036204 hw_loss 0.281485 history loss 6.760705 rank 4
2023-02-24 21:07:11,614 DEBUG CV Batch 19/600 loss 7.497055 loss_att 7.666872 loss_ctc 9.572770 loss_rnnt 7.036204 hw_loss 0.281485 history loss 6.760705 rank 5
2023-02-24 21:07:20,033 DEBUG CV Batch 19/700 loss 12.034038 loss_att 30.620243 loss_ctc 21.767977 loss_rnnt 6.920246 hw_loss 0.185047 history loss 7.445316 rank 0
2023-02-24 21:07:21,213 DEBUG CV Batch 19/700 loss 12.034038 loss_att 30.620243 loss_ctc 21.767977 loss_rnnt 6.920246 hw_loss 0.185047 history loss 7.445316 rank 7
2023-02-24 21:07:21,338 DEBUG CV Batch 19/700 loss 12.034038 loss_att 30.620243 loss_ctc 21.767977 loss_rnnt 6.920246 hw_loss 0.185047 history loss 7.445316 rank 2
2023-02-24 21:07:21,706 DEBUG CV Batch 19/700 loss 12.034038 loss_att 30.620243 loss_ctc 21.767977 loss_rnnt 6.920246 hw_loss 0.185047 history loss 7.445316 rank 6
2023-02-24 21:07:21,964 DEBUG CV Batch 19/700 loss 12.034038 loss_att 30.620243 loss_ctc 21.767977 loss_rnnt 6.920246 hw_loss 0.185047 history loss 7.445316 rank 1
2023-02-24 21:07:22,853 DEBUG CV Batch 19/700 loss 12.034038 loss_att 30.620243 loss_ctc 21.767977 loss_rnnt 6.920246 hw_loss 0.185047 history loss 7.445316 rank 4
2023-02-24 21:07:23,081 DEBUG CV Batch 19/700 loss 12.034038 loss_att 30.620243 loss_ctc 21.767977 loss_rnnt 6.920246 hw_loss 0.185047 history loss 7.445316 rank 3
2023-02-24 21:07:23,707 DEBUG CV Batch 19/700 loss 12.034038 loss_att 30.620243 loss_ctc 21.767977 loss_rnnt 6.920246 hw_loss 0.185047 history loss 7.445316 rank 5
2023-02-24 21:07:31,198 DEBUG CV Batch 19/800 loss 10.379964 loss_att 10.657057 loss_ctc 16.281527 loss_rnnt 9.365592 hw_loss 0.322646 history loss 6.884408 rank 0
2023-02-24 21:07:32,515 DEBUG CV Batch 19/800 loss 10.379964 loss_att 10.657057 loss_ctc 16.281527 loss_rnnt 9.365592 hw_loss 0.322646 history loss 6.884408 rank 7
2023-02-24 21:07:33,236 DEBUG CV Batch 19/800 loss 10.379964 loss_att 10.657057 loss_ctc 16.281527 loss_rnnt 9.365592 hw_loss 0.322646 history loss 6.884408 rank 1
2023-02-24 21:07:33,601 DEBUG CV Batch 19/800 loss 10.379964 loss_att 10.657057 loss_ctc 16.281527 loss_rnnt 9.365592 hw_loss 0.322646 history loss 6.884408 rank 2
2023-02-24 21:07:34,260 DEBUG CV Batch 19/800 loss 10.379964 loss_att 10.657057 loss_ctc 16.281527 loss_rnnt 9.365592 hw_loss 0.322646 history loss 6.884408 rank 6
2023-02-24 21:07:35,556 DEBUG CV Batch 19/800 loss 10.379964 loss_att 10.657057 loss_ctc 16.281527 loss_rnnt 9.365592 hw_loss 0.322646 history loss 6.884408 rank 5
2023-02-24 21:07:35,660 DEBUG CV Batch 19/800 loss 10.379964 loss_att 10.657057 loss_ctc 16.281527 loss_rnnt 9.365592 hw_loss 0.322646 history loss 6.884408 rank 4
2023-02-24 21:07:35,969 DEBUG CV Batch 19/800 loss 10.379964 loss_att 10.657057 loss_ctc 16.281527 loss_rnnt 9.365592 hw_loss 0.322646 history loss 6.884408 rank 3
2023-02-24 21:07:44,721 DEBUG CV Batch 19/900 loss 13.753510 loss_att 15.868864 loss_ctc 23.242693 loss_rnnt 11.969759 hw_loss 0.178977 history loss 6.696713 rank 0
2023-02-24 21:07:46,078 DEBUG CV Batch 19/900 loss 13.753510 loss_att 15.868864 loss_ctc 23.242693 loss_rnnt 11.969759 hw_loss 0.178977 history loss 6.696713 rank 7
2023-02-24 21:07:47,122 DEBUG CV Batch 19/900 loss 13.753510 loss_att 15.868864 loss_ctc 23.242693 loss_rnnt 11.969759 hw_loss 0.178977 history loss 6.696713 rank 1
2023-02-24 21:07:47,873 DEBUG CV Batch 19/900 loss 13.753510 loss_att 15.868864 loss_ctc 23.242693 loss_rnnt 11.969759 hw_loss 0.178977 history loss 6.696713 rank 2
2023-02-24 21:07:48,585 DEBUG CV Batch 19/900 loss 13.753510 loss_att 15.868864 loss_ctc 23.242693 loss_rnnt 11.969759 hw_loss 0.178977 history loss 6.696713 rank 6
2023-02-24 21:07:49,731 DEBUG CV Batch 19/900 loss 13.753510 loss_att 15.868864 loss_ctc 23.242693 loss_rnnt 11.969759 hw_loss 0.178977 history loss 6.696713 rank 5
2023-02-24 21:07:49,953 DEBUG CV Batch 19/900 loss 13.753510 loss_att 15.868864 loss_ctc 23.242693 loss_rnnt 11.969759 hw_loss 0.178977 history loss 6.696713 rank 4
2023-02-24 21:07:49,991 DEBUG CV Batch 19/900 loss 13.753510 loss_att 15.868864 loss_ctc 23.242693 loss_rnnt 11.969759 hw_loss 0.178977 history loss 6.696713 rank 3
2023-02-24 21:07:57,236 DEBUG CV Batch 19/1000 loss 3.237485 loss_att 3.769822 loss_ctc 4.132377 loss_rnnt 2.900278 hw_loss 0.208913 history loss 6.471417 rank 0
2023-02-24 21:07:58,678 DEBUG CV Batch 19/1000 loss 3.237485 loss_att 3.769822 loss_ctc 4.132377 loss_rnnt 2.900278 hw_loss 0.208913 history loss 6.471417 rank 7
2023-02-24 21:08:00,013 DEBUG CV Batch 19/1000 loss 3.237485 loss_att 3.769822 loss_ctc 4.132377 loss_rnnt 2.900278 hw_loss 0.208913 history loss 6.471417 rank 1
2023-02-24 21:08:00,081 DEBUG CV Batch 19/1000 loss 3.237485 loss_att 3.769822 loss_ctc 4.132377 loss_rnnt 2.900278 hw_loss 0.208913 history loss 6.471417 rank 2
2023-02-24 21:08:00,999 DEBUG CV Batch 19/1000 loss 3.237485 loss_att 3.769822 loss_ctc 4.132377 loss_rnnt 2.900278 hw_loss 0.208913 history loss 6.471417 rank 6
2023-02-24 21:08:02,263 DEBUG CV Batch 19/1000 loss 3.237485 loss_att 3.769822 loss_ctc 4.132377 loss_rnnt 2.900278 hw_loss 0.208913 history loss 6.471417 rank 4
2023-02-24 21:08:02,391 DEBUG CV Batch 19/1000 loss 3.237485 loss_att 3.769822 loss_ctc 4.132377 loss_rnnt 2.900278 hw_loss 0.208913 history loss 6.471417 rank 3
2023-02-24 21:08:02,893 DEBUG CV Batch 19/1000 loss 3.237485 loss_att 3.769822 loss_ctc 4.132377 loss_rnnt 2.900278 hw_loss 0.208913 history loss 6.471417 rank 5
2023-02-24 21:08:09,221 DEBUG CV Batch 19/1100 loss 5.889521 loss_att 5.595877 loss_ctc 8.529162 loss_rnnt 5.474872 hw_loss 0.227674 history loss 6.441600 rank 0
2023-02-24 21:08:10,832 DEBUG CV Batch 19/1100 loss 5.889521 loss_att 5.595877 loss_ctc 8.529162 loss_rnnt 5.474872 hw_loss 0.227674 history loss 6.441600 rank 7
2023-02-24 21:08:12,085 DEBUG CV Batch 19/1100 loss 5.889521 loss_att 5.595877 loss_ctc 8.529162 loss_rnnt 5.474872 hw_loss 0.227674 history loss 6.441600 rank 2
2023-02-24 21:08:12,169 DEBUG CV Batch 19/1100 loss 5.889521 loss_att 5.595877 loss_ctc 8.529162 loss_rnnt 5.474872 hw_loss 0.227674 history loss 6.441600 rank 1
2023-02-24 21:08:13,010 DEBUG CV Batch 19/1100 loss 5.889521 loss_att 5.595877 loss_ctc 8.529162 loss_rnnt 5.474872 hw_loss 0.227674 history loss 6.441600 rank 6
2023-02-24 21:08:14,205 DEBUG CV Batch 19/1100 loss 5.889521 loss_att 5.595877 loss_ctc 8.529162 loss_rnnt 5.474872 hw_loss 0.227674 history loss 6.441600 rank 4
2023-02-24 21:08:14,540 DEBUG CV Batch 19/1100 loss 5.889521 loss_att 5.595877 loss_ctc 8.529162 loss_rnnt 5.474872 hw_loss 0.227674 history loss 6.441600 rank 3
2023-02-24 21:08:15,387 DEBUG CV Batch 19/1100 loss 5.889521 loss_att 5.595877 loss_ctc 8.529162 loss_rnnt 5.474872 hw_loss 0.227674 history loss 6.441600 rank 5
2023-02-24 21:08:20,116 DEBUG CV Batch 19/1200 loss 8.918804 loss_att 10.123510 loss_ctc 10.981505 loss_rnnt 8.262832 hw_loss 0.262508 history loss 6.757494 rank 0
2023-02-24 21:08:21,938 DEBUG CV Batch 19/1200 loss 8.918804 loss_att 10.123510 loss_ctc 10.981505 loss_rnnt 8.262832 hw_loss 0.262508 history loss 6.757494 rank 7
2023-02-24 21:08:22,890 DEBUG CV Batch 19/1200 loss 8.918804 loss_att 10.123510 loss_ctc 10.981505 loss_rnnt 8.262832 hw_loss 0.262508 history loss 6.757494 rank 2
2023-02-24 21:08:23,213 DEBUG CV Batch 19/1200 loss 8.918804 loss_att 10.123510 loss_ctc 10.981505 loss_rnnt 8.262832 hw_loss 0.262508 history loss 6.757494 rank 1
2023-02-24 21:08:23,701 DEBUG CV Batch 19/1200 loss 8.918804 loss_att 10.123510 loss_ctc 10.981505 loss_rnnt 8.262832 hw_loss 0.262508 history loss 6.757494 rank 6
2023-02-24 21:08:25,327 DEBUG CV Batch 19/1200 loss 8.918804 loss_att 10.123510 loss_ctc 10.981505 loss_rnnt 8.262832 hw_loss 0.262508 history loss 6.757494 rank 3
2023-02-24 21:08:25,533 DEBUG CV Batch 19/1200 loss 8.918804 loss_att 10.123510 loss_ctc 10.981505 loss_rnnt 8.262832 hw_loss 0.262508 history loss 6.757494 rank 4
2023-02-24 21:08:26,931 DEBUG CV Batch 19/1200 loss 8.918804 loss_att 10.123510 loss_ctc 10.981505 loss_rnnt 8.262832 hw_loss 0.262508 history loss 6.757494 rank 5
2023-02-24 21:08:32,301 DEBUG CV Batch 19/1300 loss 5.244876 loss_att 5.429843 loss_ctc 7.432742 loss_rnnt 4.800746 hw_loss 0.216414 history loss 7.078955 rank 0
2023-02-24 21:08:34,283 DEBUG CV Batch 19/1300 loss 5.244876 loss_att 5.429843 loss_ctc 7.432742 loss_rnnt 4.800746 hw_loss 0.216414 history loss 7.078955 rank 7
2023-02-24 21:08:35,433 DEBUG CV Batch 19/1300 loss 5.244876 loss_att 5.429843 loss_ctc 7.432742 loss_rnnt 4.800746 hw_loss 0.216414 history loss 7.078955 rank 2
2023-02-24 21:08:35,608 DEBUG CV Batch 19/1300 loss 5.244876 loss_att 5.429843 loss_ctc 7.432742 loss_rnnt 4.800746 hw_loss 0.216414 history loss 7.078955 rank 1
2023-02-24 21:08:35,931 DEBUG CV Batch 19/1300 loss 5.244876 loss_att 5.429843 loss_ctc 7.432742 loss_rnnt 4.800746 hw_loss 0.216414 history loss 7.078955 rank 6
2023-02-24 21:08:37,681 DEBUG CV Batch 19/1300 loss 5.244876 loss_att 5.429843 loss_ctc 7.432742 loss_rnnt 4.800746 hw_loss 0.216414 history loss 7.078955 rank 3
2023-02-24 21:08:38,060 DEBUG CV Batch 19/1300 loss 5.244876 loss_att 5.429843 loss_ctc 7.432742 loss_rnnt 4.800746 hw_loss 0.216414 history loss 7.078955 rank 4
2023-02-24 21:08:39,662 DEBUG CV Batch 19/1300 loss 5.244876 loss_att 5.429843 loss_ctc 7.432742 loss_rnnt 4.800746 hw_loss 0.216414 history loss 7.078955 rank 5
2023-02-24 21:08:43,758 DEBUG CV Batch 19/1400 loss 5.714693 loss_att 20.037056 loss_ctc 3.379179 loss_rnnt 3.071393 hw_loss 0.169180 history loss 7.402313 rank 0
2023-02-24 21:08:45,924 DEBUG CV Batch 19/1400 loss 5.714693 loss_att 20.037056 loss_ctc 3.379179 loss_rnnt 3.071393 hw_loss 0.169180 history loss 7.402313 rank 7
2023-02-24 21:08:47,263 DEBUG CV Batch 19/1400 loss 5.714693 loss_att 20.037056 loss_ctc 3.379179 loss_rnnt 3.071393 hw_loss 0.169180 history loss 7.402313 rank 1
2023-02-24 21:08:47,964 DEBUG CV Batch 19/1400 loss 5.714693 loss_att 20.037056 loss_ctc 3.379179 loss_rnnt 3.071393 hw_loss 0.169180 history loss 7.402313 rank 2
2023-02-24 21:08:48,561 DEBUG CV Batch 19/1400 loss 5.714693 loss_att 20.037056 loss_ctc 3.379179 loss_rnnt 3.071393 hw_loss 0.169180 history loss 7.402313 rank 6
2023-02-24 21:08:49,559 DEBUG CV Batch 19/1400 loss 5.714693 loss_att 20.037056 loss_ctc 3.379179 loss_rnnt 3.071393 hw_loss 0.169180 history loss 7.402313 rank 3
2023-02-24 21:08:50,835 DEBUG CV Batch 19/1400 loss 5.714693 loss_att 20.037056 loss_ctc 3.379179 loss_rnnt 3.071393 hw_loss 0.169180 history loss 7.402313 rank 4
2023-02-24 21:08:51,575 DEBUG CV Batch 19/1400 loss 5.714693 loss_att 20.037056 loss_ctc 3.379179 loss_rnnt 3.071393 hw_loss 0.169180 history loss 7.402313 rank 5
2023-02-24 21:08:55,477 DEBUG CV Batch 19/1500 loss 6.068725 loss_att 7.247068 loss_ctc 6.690918 loss_rnnt 5.648960 hw_loss 0.189633 history loss 7.229442 rank 0
2023-02-24 21:08:57,722 DEBUG CV Batch 19/1500 loss 6.068725 loss_att 7.247068 loss_ctc 6.690918 loss_rnnt 5.648960 hw_loss 0.189633 history loss 7.229442 rank 7
2023-02-24 21:08:59,083 DEBUG CV Batch 19/1500 loss 6.068725 loss_att 7.247068 loss_ctc 6.690918 loss_rnnt 5.648960 hw_loss 0.189633 history loss 7.229442 rank 1
2023-02-24 21:09:01,053 DEBUG CV Batch 19/1500 loss 6.068725 loss_att 7.247068 loss_ctc 6.690918 loss_rnnt 5.648960 hw_loss 0.189633 history loss 7.229442 rank 2
2023-02-24 21:09:01,434 DEBUG CV Batch 19/1500 loss 6.068725 loss_att 7.247068 loss_ctc 6.690918 loss_rnnt 5.648960 hw_loss 0.189633 history loss 7.229442 rank 6
2023-02-24 21:09:02,265 DEBUG CV Batch 19/1500 loss 6.068725 loss_att 7.247068 loss_ctc 6.690918 loss_rnnt 5.648960 hw_loss 0.189633 history loss 7.229442 rank 3
2023-02-24 21:09:03,737 DEBUG CV Batch 19/1500 loss 6.068725 loss_att 7.247068 loss_ctc 6.690918 loss_rnnt 5.648960 hw_loss 0.189633 history loss 7.229442 rank 5
2023-02-24 21:09:04,302 DEBUG CV Batch 19/1500 loss 6.068725 loss_att 7.247068 loss_ctc 6.690918 loss_rnnt 5.648960 hw_loss 0.189633 history loss 7.229442 rank 4
2023-02-24 21:09:08,738 DEBUG CV Batch 19/1600 loss 10.646003 loss_att 17.300425 loss_ctc 8.622598 loss_rnnt 9.434042 hw_loss 0.282869 history loss 7.169945 rank 0
2023-02-24 21:09:10,984 DEBUG CV Batch 19/1600 loss 10.646003 loss_att 17.300425 loss_ctc 8.622598 loss_rnnt 9.434042 hw_loss 0.282869 history loss 7.169945 rank 7
2023-02-24 21:09:12,509 DEBUG CV Batch 19/1600 loss 10.646003 loss_att 17.300425 loss_ctc 8.622598 loss_rnnt 9.434042 hw_loss 0.282869 history loss 7.169945 rank 1
2023-02-24 21:09:14,736 DEBUG CV Batch 19/1600 loss 10.646003 loss_att 17.300425 loss_ctc 8.622598 loss_rnnt 9.434042 hw_loss 0.282869 history loss 7.169945 rank 2
2023-02-24 21:09:15,651 DEBUG CV Batch 19/1600 loss 10.646003 loss_att 17.300425 loss_ctc 8.622598 loss_rnnt 9.434042 hw_loss 0.282869 history loss 7.169945 rank 6
2023-02-24 21:09:16,073 DEBUG CV Batch 19/1600 loss 10.646003 loss_att 17.300425 loss_ctc 8.622598 loss_rnnt 9.434042 hw_loss 0.282869 history loss 7.169945 rank 3
2023-02-24 21:09:17,295 DEBUG CV Batch 19/1600 loss 10.646003 loss_att 17.300425 loss_ctc 8.622598 loss_rnnt 9.434042 hw_loss 0.282869 history loss 7.169945 rank 5
2023-02-24 21:09:18,459 DEBUG CV Batch 19/1600 loss 10.646003 loss_att 17.300425 loss_ctc 8.622598 loss_rnnt 9.434042 hw_loss 0.282869 history loss 7.169945 rank 4
2023-02-24 21:09:21,247 DEBUG CV Batch 19/1700 loss 8.735627 loss_att 8.126123 loss_ctc 14.253071 loss_rnnt 8.009724 hw_loss 0.210272 history loss 7.077744 rank 0
2023-02-24 21:09:23,614 DEBUG CV Batch 19/1700 loss 8.735627 loss_att 8.126123 loss_ctc 14.253071 loss_rnnt 8.009724 hw_loss 0.210272 history loss 7.077744 rank 7
2023-02-24 21:09:25,213 DEBUG CV Batch 19/1700 loss 8.735627 loss_att 8.126123 loss_ctc 14.253071 loss_rnnt 8.009724 hw_loss 0.210272 history loss 7.077744 rank 1
2023-02-24 21:09:27,732 DEBUG CV Batch 19/1700 loss 8.735627 loss_att 8.126123 loss_ctc 14.253071 loss_rnnt 8.009724 hw_loss 0.210272 history loss 7.077744 rank 2
2023-02-24 21:09:28,119 DEBUG CV Batch 19/1700 loss 8.735627 loss_att 8.126123 loss_ctc 14.253071 loss_rnnt 8.009724 hw_loss 0.210272 history loss 7.077744 rank 6
2023-02-24 21:09:28,610 DEBUG CV Batch 19/1700 loss 8.735627 loss_att 8.126123 loss_ctc 14.253071 loss_rnnt 8.009724 hw_loss 0.210272 history loss 7.077744 rank 3
2023-02-24 21:09:30,102 DEBUG CV Batch 19/1700 loss 8.735627 loss_att 8.126123 loss_ctc 14.253071 loss_rnnt 8.009724 hw_loss 0.210272 history loss 7.077744 rank 5
2023-02-24 21:09:30,459 DEBUG CV Batch 19/1700 loss 8.735627 loss_att 8.126123 loss_ctc 14.253071 loss_rnnt 8.009724 hw_loss 0.210272 history loss 7.077744 rank 4
2023-02-24 21:09:30,552 INFO Epoch 19 CV info cv_loss 7.045896175336176
2023-02-24 21:09:30,553 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/19.pt
2023-02-24 21:09:34,776 INFO Epoch 19 CV info cv_loss 7.045896175840133
2023-02-24 21:09:34,777 INFO Epoch 20 TRAIN info lr 0.00038713654535167777
2023-02-24 21:09:34,777 INFO Epoch 19 CV info cv_loss 7.045896176236405
2023-02-24 21:09:34,778 INFO Epoch 20 TRAIN info lr 0.0003871597562291052
2023-02-24 21:09:34,782 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 21:09:34,783 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 21:09:34,935 INFO Epoch 20 TRAIN info lr 0.0003871725239913507
2023-02-24 21:09:34,941 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 21:09:37,234 INFO Epoch 19 CV info cv_loss 7.0458961767618975
2023-02-24 21:09:37,234 INFO Epoch 20 TRAIN info lr 0.0003871667203064686
2023-02-24 21:09:37,236 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 21:09:37,584 INFO Epoch 19 CV info cv_loss 7.045896175710912
2023-02-24 21:09:37,584 INFO Epoch 20 TRAIN info lr 0.00038713538491738674
2023-02-24 21:09:37,589 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 21:09:38,071 INFO Epoch 19 CV info cv_loss 7.045896176477615
2023-02-24 21:09:38,072 INFO Epoch 20 TRAIN info lr 0.0003871469897299013
2023-02-24 21:09:38,075 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 21:09:39,546 INFO Epoch 19 CV info cv_loss 7.045896177018183
2023-02-24 21:09:39,547 INFO Epoch 20 TRAIN info lr 0.0003871829712818768
2023-02-24 21:09:39,549 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 21:09:39,795 INFO Epoch 19 CV info cv_loss 7.0458961760511905
2023-02-24 21:09:39,796 INFO Epoch 20 TRAIN info lr 0.0003871678810225657
2023-02-24 21:09:39,799 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 21:10:42,291 DEBUG TRAIN Batch 20/0 loss 9.957482 loss_att 9.238847 loss_ctc 12.961492 loss_rnnt 9.601107 hw_loss 0.186692 lr 0.00038717 rank 0
2023-02-24 21:10:42,294 DEBUG TRAIN Batch 20/0 loss 8.777747 loss_att 8.018124 loss_ctc 11.620259 loss_rnnt 8.348443 hw_loss 0.379175 lr 0.00038717 rank 2
2023-02-24 21:10:42,303 DEBUG TRAIN Batch 20/0 loss 10.924565 loss_att 10.238012 loss_ctc 12.637674 loss_rnnt 10.671507 hw_loss 0.303667 lr 0.00038716 rank 1
2023-02-24 21:10:42,304 DEBUG TRAIN Batch 20/0 loss 10.764398 loss_att 10.506860 loss_ctc 13.526025 loss_rnnt 10.307542 hw_loss 0.262772 lr 0.00038714 rank 7
2023-02-24 21:10:42,323 DEBUG TRAIN Batch 20/0 loss 9.470797 loss_att 9.065752 loss_ctc 13.653139 loss_rnnt 8.856207 hw_loss 0.258659 lr 0.00038718 rank 4
2023-02-24 21:10:42,325 DEBUG TRAIN Batch 20/0 loss 8.107437 loss_att 8.302190 loss_ctc 10.970886 loss_rnnt 7.537973 hw_loss 0.278849 lr 0.00038713 rank 6
2023-02-24 21:10:42,346 DEBUG TRAIN Batch 20/0 loss 9.385715 loss_att 8.922072 loss_ctc 11.751491 loss_rnnt 9.006595 hw_loss 0.293271 lr 0.00038715 rank 3
2023-02-24 21:10:42,379 DEBUG TRAIN Batch 20/0 loss 9.878855 loss_att 9.492449 loss_ctc 11.537145 loss_rnnt 9.601549 hw_loss 0.250279 lr 0.00038717 rank 5
2023-02-24 21:11:54,189 DEBUG TRAIN Batch 20/100 loss 5.581950 loss_att 8.149758 loss_ctc 7.662977 loss_rnnt 4.725777 hw_loss 0.122139 lr 0.00038706 rank 0
2023-02-24 21:11:54,194 DEBUG TRAIN Batch 20/100 loss 16.786825 loss_att 22.428572 loss_ctc 21.604042 loss_rnnt 14.878256 hw_loss 0.258606 lr 0.00038707 rank 4
2023-02-24 21:11:54,199 DEBUG TRAIN Batch 20/100 loss 11.016939 loss_att 16.622314 loss_ctc 21.617552 loss_rnnt 8.400822 hw_loss 0.153051 lr 0.00038703 rank 3
2023-02-24 21:11:54,199 DEBUG TRAIN Batch 20/100 loss 3.717739 loss_att 5.946904 loss_ctc 6.262489 loss_rnnt 2.833005 hw_loss 0.186751 lr 0.00038705 rank 2
2023-02-24 21:11:54,200 DEBUG TRAIN Batch 20/100 loss 6.244336 loss_att 9.383176 loss_ctc 10.470211 loss_rnnt 4.970299 hw_loss 0.155285 lr 0.00038705 rank 5
2023-02-24 21:11:54,201 DEBUG TRAIN Batch 20/100 loss 3.925489 loss_att 7.319707 loss_ctc 6.223296 loss_rnnt 2.820052 hw_loss 0.225411 lr 0.00038702 rank 6
2023-02-24 21:11:54,205 DEBUG TRAIN Batch 20/100 loss 6.964289 loss_att 11.306822 loss_ctc 9.411058 loss_rnnt 5.632403 hw_loss 0.257143 lr 0.00038702 rank 7
2023-02-24 21:11:54,245 DEBUG TRAIN Batch 20/100 loss 7.038093 loss_att 9.020318 loss_ctc 11.578028 loss_rnnt 5.976997 hw_loss 0.111235 lr 0.00038704 rank 1
2023-02-24 21:13:06,834 DEBUG TRAIN Batch 20/200 loss 3.140204 loss_att 5.464589 loss_ctc 4.735426 loss_rnnt 2.352382 hw_loss 0.206715 lr 0.00038695 rank 4
2023-02-24 21:13:06,837 DEBUG TRAIN Batch 20/200 loss 5.594442 loss_att 8.403189 loss_ctc 8.657328 loss_rnnt 4.491064 hw_loss 0.249832 lr 0.00038691 rank 3
2023-02-24 21:13:06,841 DEBUG TRAIN Batch 20/200 loss 13.939239 loss_att 15.941847 loss_ctc 18.101349 loss_rnnt 12.887774 hw_loss 0.179989 lr 0.00038693 rank 2
2023-02-24 21:13:06,842 DEBUG TRAIN Batch 20/200 loss 2.931963 loss_att 4.894230 loss_ctc 3.902441 loss_rnnt 2.286546 hw_loss 0.231688 lr 0.00038694 rank 0
2023-02-24 21:13:06,845 DEBUG TRAIN Batch 20/200 loss 6.010577 loss_att 10.252288 loss_ctc 7.591272 loss_rnnt 4.858367 hw_loss 0.174578 lr 0.00038690 rank 7
2023-02-24 21:13:06,846 DEBUG TRAIN Batch 20/200 loss 9.687882 loss_att 15.811945 loss_ctc 16.308207 loss_rnnt 7.470145 hw_loss 0.206655 lr 0.00038693 rank 1
2023-02-24 21:13:06,872 DEBUG TRAIN Batch 20/200 loss 7.557605 loss_att 8.210560 loss_ctc 10.873484 loss_rnnt 6.870576 hw_loss 0.214350 lr 0.00038693 rank 5
2023-02-24 21:13:06,883 DEBUG TRAIN Batch 20/200 loss 7.653809 loss_att 9.265579 loss_ctc 9.775051 loss_rnnt 6.939822 hw_loss 0.204001 lr 0.00038690 rank 6
2023-02-24 21:14:19,161 DEBUG TRAIN Batch 20/300 loss 9.894403 loss_att 14.474799 loss_ctc 14.334883 loss_rnnt 8.307419 hw_loss 0.147823 lr 0.00038682 rank 2
2023-02-24 21:14:19,170 DEBUG TRAIN Batch 20/300 loss 17.710590 loss_att 19.979340 loss_ctc 23.985512 loss_rnnt 16.305141 hw_loss 0.215706 lr 0.00038681 rank 1
2023-02-24 21:14:19,169 DEBUG TRAIN Batch 20/300 loss 15.020039 loss_att 19.056328 loss_ctc 20.060104 loss_rnnt 13.388946 hw_loss 0.284674 lr 0.00038680 rank 3
2023-02-24 21:14:19,174 DEBUG TRAIN Batch 20/300 loss 11.772538 loss_att 15.889193 loss_ctc 19.206234 loss_rnnt 9.848053 hw_loss 0.206240 lr 0.00038683 rank 4
2023-02-24 21:14:19,175 DEBUG TRAIN Batch 20/300 loss 17.504618 loss_att 21.986414 loss_ctc 23.176949 loss_rnnt 15.761640 hw_loss 0.169331 lr 0.00038682 rank 5
2023-02-24 21:14:19,176 DEBUG TRAIN Batch 20/300 loss 6.669629 loss_att 9.717587 loss_ctc 10.239399 loss_rnnt 5.470334 hw_loss 0.213251 lr 0.00038679 rank 6
2023-02-24 21:14:19,177 DEBUG TRAIN Batch 20/300 loss 17.483406 loss_att 25.653854 loss_ctc 25.321012 loss_rnnt 14.711373 hw_loss 0.174242 lr 0.00038682 rank 0
2023-02-24 21:14:19,180 DEBUG TRAIN Batch 20/300 loss 6.849996 loss_att 9.106311 loss_ctc 9.314320 loss_rnnt 5.918264 hw_loss 0.284797 lr 0.00038679 rank 7
2023-02-24 21:15:32,355 DEBUG TRAIN Batch 20/400 loss 15.718538 loss_att 20.994854 loss_ctc 19.319187 loss_rnnt 14.011198 hw_loss 0.322481 lr 0.00038672 rank 4
2023-02-24 21:15:32,357 DEBUG TRAIN Batch 20/400 loss 18.984432 loss_att 23.293528 loss_ctc 26.701515 loss_rnnt 16.940670 hw_loss 0.286870 lr 0.00038671 rank 0
2023-02-24 21:15:32,360 DEBUG TRAIN Batch 20/400 loss 11.134071 loss_att 16.633850 loss_ctc 18.089767 loss_rnnt 9.007769 hw_loss 0.185476 lr 0.00038670 rank 5
2023-02-24 21:15:32,363 DEBUG TRAIN Batch 20/400 loss 17.259670 loss_att 18.301592 loss_ctc 20.917236 loss_rnnt 16.439137 hw_loss 0.233387 lr 0.00038670 rank 1
2023-02-24 21:15:32,364 DEBUG TRAIN Batch 20/400 loss 5.854492 loss_att 8.457963 loss_ctc 7.633433 loss_rnnt 5.010320 hw_loss 0.161786 lr 0.00038667 rank 6
2023-02-24 21:15:32,364 DEBUG TRAIN Batch 20/400 loss 6.676881 loss_att 11.540455 loss_ctc 13.367981 loss_rnnt 4.755933 hw_loss 0.105162 lr 0.00038667 rank 7
2023-02-24 21:15:32,365 DEBUG TRAIN Batch 20/400 loss 7.816933 loss_att 11.361099 loss_ctc 11.018227 loss_rnnt 6.567238 hw_loss 0.213791 lr 0.00038668 rank 3
2023-02-24 21:15:32,411 DEBUG TRAIN Batch 20/400 loss 3.194077 loss_att 6.231952 loss_ctc 4.139052 loss_rnnt 2.356415 hw_loss 0.195169 lr 0.00038670 rank 2
2023-02-24 21:16:44,668 DEBUG TRAIN Batch 20/500 loss 6.770319 loss_att 9.442325 loss_ctc 11.076261 loss_rnnt 5.515840 hw_loss 0.273660 lr 0.00038657 rank 3
2023-02-24 21:16:44,668 DEBUG TRAIN Batch 20/500 loss 8.317158 loss_att 8.666539 loss_ctc 12.221531 loss_rnnt 7.616653 hw_loss 0.206334 lr 0.00038659 rank 0
2023-02-24 21:16:44,671 DEBUG TRAIN Batch 20/500 loss 11.733571 loss_att 15.920811 loss_ctc 15.597144 loss_rnnt 10.259226 hw_loss 0.228288 lr 0.00038656 rank 7
2023-02-24 21:16:44,674 DEBUG TRAIN Batch 20/500 loss 3.897112 loss_att 6.122622 loss_ctc 6.175136 loss_rnnt 2.975855 hw_loss 0.323284 lr 0.00038656 rank 6
2023-02-24 21:16:44,678 DEBUG TRAIN Batch 20/500 loss 12.310857 loss_att 15.241570 loss_ctc 18.355322 loss_rnnt 10.781932 hw_loss 0.256600 lr 0.00038659 rank 5
2023-02-24 21:16:44,681 DEBUG TRAIN Batch 20/500 loss 8.352447 loss_att 11.069054 loss_ctc 10.355158 loss_rnnt 7.441002 hw_loss 0.189551 lr 0.00038660 rank 4
2023-02-24 21:16:44,693 DEBUG TRAIN Batch 20/500 loss 8.594529 loss_att 11.223497 loss_ctc 11.510468 loss_rnnt 7.562373 hw_loss 0.220444 lr 0.00038659 rank 2
2023-02-24 21:16:44,697 DEBUG TRAIN Batch 20/500 loss 17.942303 loss_att 20.729584 loss_ctc 26.092751 loss_rnnt 16.125135 hw_loss 0.324348 lr 0.00038658 rank 1
2023-02-24 21:17:56,858 DEBUG TRAIN Batch 20/600 loss 6.905598 loss_att 10.283899 loss_ctc 15.241962 loss_rnnt 5.043676 hw_loss 0.140150 lr 0.00038649 rank 4
2023-02-24 21:17:56,861 DEBUG TRAIN Batch 20/600 loss 8.794871 loss_att 9.559067 loss_ctc 11.217850 loss_rnnt 8.138021 hw_loss 0.339276 lr 0.00038648 rank 0
2023-02-24 21:17:56,863 DEBUG TRAIN Batch 20/600 loss 9.135571 loss_att 11.003599 loss_ctc 10.485128 loss_rnnt 8.448348 hw_loss 0.250642 lr 0.00038645 rank 3
2023-02-24 21:17:56,863 DEBUG TRAIN Batch 20/600 loss 7.964895 loss_att 10.502194 loss_ctc 11.048023 loss_rnnt 6.900377 hw_loss 0.273700 lr 0.00038644 rank 7
2023-02-24 21:17:56,866 DEBUG TRAIN Batch 20/600 loss 6.826204 loss_att 9.023672 loss_ctc 9.453156 loss_rnnt 5.876153 hw_loss 0.300558 lr 0.00038646 rank 1
2023-02-24 21:17:56,867 DEBUG TRAIN Batch 20/600 loss 5.747916 loss_att 7.038582 loss_ctc 8.688390 loss_rnnt 4.959472 hw_loss 0.259215 lr 0.00038644 rank 6
2023-02-24 21:17:56,876 DEBUG TRAIN Batch 20/600 loss 10.384441 loss_att 9.676008 loss_ctc 11.564532 loss_rnnt 10.249289 hw_loss 0.224051 lr 0.00038647 rank 5
2023-02-24 21:17:56,913 DEBUG TRAIN Batch 20/600 loss 7.986856 loss_att 10.119906 loss_ctc 10.895753 loss_rnnt 7.056216 hw_loss 0.217833 lr 0.00038647 rank 2
2023-02-24 21:19:11,305 DEBUG TRAIN Batch 20/700 loss 11.173584 loss_att 15.918239 loss_ctc 15.172970 loss_rnnt 9.573675 hw_loss 0.220737 lr 0.00038632 rank 6
2023-02-24 21:19:11,305 DEBUG TRAIN Batch 20/700 loss 11.810446 loss_att 12.289272 loss_ctc 13.350246 loss_rnnt 11.404392 hw_loss 0.196837 lr 0.00038637 rank 4
2023-02-24 21:19:11,306 DEBUG TRAIN Batch 20/700 loss 11.444336 loss_att 14.230222 loss_ctc 18.247013 loss_rnnt 9.910759 hw_loss 0.130081 lr 0.00038634 rank 3
2023-02-24 21:19:11,307 DEBUG TRAIN Batch 20/700 loss 2.895134 loss_att 4.968152 loss_ctc 4.307344 loss_rnnt 2.211405 hw_loss 0.151558 lr 0.00038635 rank 1
2023-02-24 21:19:11,310 DEBUG TRAIN Batch 20/700 loss 7.898809 loss_att 10.616056 loss_ctc 14.163070 loss_rnnt 6.419548 hw_loss 0.188582 lr 0.00038636 rank 5
2023-02-24 21:19:11,311 DEBUG TRAIN Batch 20/700 loss 4.451475 loss_att 9.555222 loss_ctc 6.479131 loss_rnnt 3.065133 hw_loss 0.178573 lr 0.00038636 rank 2
2023-02-24 21:19:11,340 DEBUG TRAIN Batch 20/700 loss 11.240616 loss_att 14.149976 loss_ctc 14.649292 loss_rnnt 10.108611 hw_loss 0.179329 lr 0.00038633 rank 7
2023-02-24 21:19:11,344 DEBUG TRAIN Batch 20/700 loss 5.839640 loss_att 9.544874 loss_ctc 8.701141 loss_rnnt 4.605802 hw_loss 0.208608 lr 0.00038636 rank 0
2023-02-24 21:20:23,464 DEBUG TRAIN Batch 20/800 loss 2.360057 loss_att 4.105344 loss_ctc 2.419603 loss_rnnt 1.927530 hw_loss 0.141617 lr 0.00038621 rank 6
2023-02-24 21:20:23,464 DEBUG TRAIN Batch 20/800 loss 4.173690 loss_att 10.407549 loss_ctc 4.245626 loss_rnnt 2.795822 hw_loss 0.227820 lr 0.00038625 rank 0
2023-02-24 21:20:23,464 DEBUG TRAIN Batch 20/800 loss 14.689642 loss_att 18.874802 loss_ctc 21.300053 loss_rnnt 12.827987 hw_loss 0.268564 lr 0.00038626 rank 4
2023-02-24 21:20:23,467 DEBUG TRAIN Batch 20/800 loss 4.623145 loss_att 7.949322 loss_ctc 5.808427 loss_rnnt 3.699236 hw_loss 0.188692 lr 0.00038621 rank 7
2023-02-24 21:20:23,470 DEBUG TRAIN Batch 20/800 loss 6.311450 loss_att 10.159396 loss_ctc 9.031611 loss_rnnt 5.059419 hw_loss 0.224539 lr 0.00038624 rank 2
2023-02-24 21:20:23,473 DEBUG TRAIN Batch 20/800 loss 9.941390 loss_att 12.981606 loss_ctc 16.006010 loss_rnnt 8.401925 hw_loss 0.230260 lr 0.00038622 rank 3
2023-02-24 21:20:23,502 DEBUG TRAIN Batch 20/800 loss 12.891215 loss_att 17.983074 loss_ctc 19.002232 loss_rnnt 10.939262 hw_loss 0.222712 lr 0.00038623 rank 1
2023-02-24 21:20:23,514 DEBUG TRAIN Batch 20/800 loss 5.282104 loss_att 11.225010 loss_ctc 7.725128 loss_rnnt 3.604092 hw_loss 0.306926 lr 0.00038624 rank 5
2023-02-24 21:21:35,667 DEBUG TRAIN Batch 20/900 loss 18.317091 loss_att 21.150856 loss_ctc 22.517483 loss_rnnt 17.041870 hw_loss 0.278276 lr 0.00038613 rank 0
2023-02-24 21:21:35,669 DEBUG TRAIN Batch 20/900 loss 6.988552 loss_att 9.059570 loss_ctc 7.570396 loss_rnnt 6.392170 hw_loss 0.196122 lr 0.00038611 rank 3
2023-02-24 21:21:35,669 DEBUG TRAIN Batch 20/900 loss 5.789414 loss_att 7.926208 loss_ctc 6.846885 loss_rnnt 5.131911 hw_loss 0.167153 lr 0.00038614 rank 4
2023-02-24 21:21:35,671 DEBUG TRAIN Batch 20/900 loss 8.615117 loss_att 11.954185 loss_ctc 11.707766 loss_rnnt 7.442159 hw_loss 0.173984 lr 0.00038612 rank 1
2023-02-24 21:21:35,671 DEBUG TRAIN Batch 20/900 loss 10.987657 loss_att 12.172132 loss_ctc 17.483452 loss_rnnt 9.753029 hw_loss 0.246798 lr 0.00038610 rank 7
2023-02-24 21:21:35,672 DEBUG TRAIN Batch 20/900 loss 5.630647 loss_att 8.781908 loss_ctc 8.379029 loss_rnnt 4.522235 hw_loss 0.209453 lr 0.00038613 rank 2
2023-02-24 21:21:35,678 DEBUG TRAIN Batch 20/900 loss 17.067738 loss_att 21.890778 loss_ctc 31.564247 loss_rnnt 14.093182 hw_loss 0.144527 lr 0.00038609 rank 6
2023-02-24 21:21:35,681 DEBUG TRAIN Batch 20/900 loss 6.269807 loss_att 8.188741 loss_ctc 10.733637 loss_rnnt 5.233032 hw_loss 0.108396 lr 0.00038613 rank 5
2023-02-24 21:22:48,315 DEBUG TRAIN Batch 20/1000 loss 10.474535 loss_att 12.622707 loss_ctc 11.685740 loss_rnnt 9.779108 hw_loss 0.195560 lr 0.00038598 rank 7
2023-02-24 21:22:48,324 DEBUG TRAIN Batch 20/1000 loss 15.395567 loss_att 17.302887 loss_ctc 18.253794 loss_rnnt 14.531876 hw_loss 0.189620 lr 0.00038599 rank 3
2023-02-24 21:22:48,324 DEBUG TRAIN Batch 20/1000 loss 13.159554 loss_att 16.554564 loss_ctc 16.184183 loss_rnnt 11.986666 hw_loss 0.169879 lr 0.00038601 rank 5
2023-02-24 21:22:48,328 DEBUG TRAIN Batch 20/1000 loss 6.696516 loss_att 10.272041 loss_ctc 9.528502 loss_rnnt 5.474635 hw_loss 0.242208 lr 0.00038603 rank 4
2023-02-24 21:22:48,328 DEBUG TRAIN Batch 20/1000 loss 11.367604 loss_att 14.638096 loss_ctc 14.583666 loss_rnnt 10.127451 hw_loss 0.294836 lr 0.00038602 rank 0
2023-02-24 21:22:48,348 DEBUG TRAIN Batch 20/1000 loss 9.634980 loss_att 12.585456 loss_ctc 13.077320 loss_rnnt 8.500766 hw_loss 0.159638 lr 0.00038601 rank 2
2023-02-24 21:22:48,352 DEBUG TRAIN Batch 20/1000 loss 11.720822 loss_att 13.601640 loss_ctc 12.589726 loss_rnnt 11.121465 hw_loss 0.201264 lr 0.00038598 rank 6
2023-02-24 21:22:48,364 DEBUG TRAIN Batch 20/1000 loss 8.276781 loss_att 13.055244 loss_ctc 14.992615 loss_rnnt 6.309668 hw_loss 0.217456 lr 0.00038600 rank 1
2023-02-24 21:24:02,479 DEBUG TRAIN Batch 20/1100 loss 28.914303 loss_att 30.781265 loss_ctc 43.472313 loss_rnnt 26.497631 hw_loss 0.191649 lr 0.00038590 rank 0
2023-02-24 21:24:02,481 DEBUG TRAIN Batch 20/1100 loss 7.554785 loss_att 12.489493 loss_ctc 10.049048 loss_rnnt 6.133547 hw_loss 0.190739 lr 0.00038591 rank 4
2023-02-24 21:24:02,484 DEBUG TRAIN Batch 20/1100 loss 8.439282 loss_att 11.679858 loss_ctc 11.200346 loss_rnnt 7.327874 hw_loss 0.178408 lr 0.00038587 rank 7
2023-02-24 21:24:02,487 DEBUG TRAIN Batch 20/1100 loss 9.148811 loss_att 13.382017 loss_ctc 9.952953 loss_rnnt 8.067064 hw_loss 0.239786 lr 0.00038589 rank 1
2023-02-24 21:24:02,489 DEBUG TRAIN Batch 20/1100 loss 6.048372 loss_att 8.309112 loss_ctc 8.402824 loss_rnnt 5.197651 hw_loss 0.158713 lr 0.00038586 rank 6
2023-02-24 21:24:02,489 DEBUG TRAIN Batch 20/1100 loss 10.455534 loss_att 13.703431 loss_ctc 15.035185 loss_rnnt 9.083603 hw_loss 0.209500 lr 0.00038588 rank 3
2023-02-24 21:24:02,493 DEBUG TRAIN Batch 20/1100 loss 11.680971 loss_att 13.403177 loss_ctc 17.465229 loss_rnnt 10.458337 hw_loss 0.200549 lr 0.00038590 rank 5
2023-02-24 21:24:02,535 DEBUG TRAIN Batch 20/1100 loss 8.127679 loss_att 9.950560 loss_ctc 11.305630 loss_rnnt 7.221383 hw_loss 0.221237 lr 0.00038590 rank 2
2023-02-24 21:25:14,855 DEBUG TRAIN Batch 20/1200 loss 9.864279 loss_att 11.211824 loss_ctc 12.971298 loss_rnnt 9.111040 hw_loss 0.130236 lr 0.00038579 rank 0
2023-02-24 21:25:14,857 DEBUG TRAIN Batch 20/1200 loss 7.039979 loss_att 7.996112 loss_ctc 9.729194 loss_rnnt 6.373843 hw_loss 0.218151 lr 0.00038577 rank 1
2023-02-24 21:25:14,858 DEBUG TRAIN Batch 20/1200 loss 7.852134 loss_att 10.186419 loss_ctc 11.117339 loss_rnnt 6.794808 hw_loss 0.290825 lr 0.00038575 rank 6
2023-02-24 21:25:14,860 DEBUG TRAIN Batch 20/1200 loss 8.626172 loss_att 10.937838 loss_ctc 12.115816 loss_rnnt 7.592500 hw_loss 0.198848 lr 0.00038580 rank 4
2023-02-24 21:25:14,860 DEBUG TRAIN Batch 20/1200 loss 13.292899 loss_att 15.181559 loss_ctc 18.127630 loss_rnnt 12.157910 hw_loss 0.211172 lr 0.00038576 rank 3
2023-02-24 21:25:14,866 DEBUG TRAIN Batch 20/1200 loss 7.459869 loss_att 9.684878 loss_ctc 8.622092 loss_rnnt 6.739969 hw_loss 0.224878 lr 0.00038575 rank 7
2023-02-24 21:25:14,870 DEBUG TRAIN Batch 20/1200 loss 25.088028 loss_att 26.896923 loss_ctc 34.278687 loss_rnnt 23.389225 hw_loss 0.209254 lr 0.00038578 rank 2
2023-02-24 21:25:14,916 DEBUG TRAIN Batch 20/1200 loss 3.955512 loss_att 4.703675 loss_ctc 7.016036 loss_rnnt 3.227658 hw_loss 0.319032 lr 0.00038578 rank 5
2023-02-24 21:26:27,403 DEBUG TRAIN Batch 20/1300 loss 9.936261 loss_att 13.525871 loss_ctc 13.109066 loss_rnnt 8.681557 hw_loss 0.213266 lr 0.00038566 rank 1
2023-02-24 21:26:27,412 DEBUG TRAIN Batch 20/1300 loss 10.869716 loss_att 16.303158 loss_ctc 12.310637 loss_rnnt 9.446155 hw_loss 0.271406 lr 0.00038568 rank 4
2023-02-24 21:26:27,413 DEBUG TRAIN Batch 20/1300 loss 7.820900 loss_att 7.448784 loss_ctc 11.555991 loss_rnnt 7.241174 hw_loss 0.292757 lr 0.00038565 rank 3
2023-02-24 21:26:27,416 DEBUG TRAIN Batch 20/1300 loss 4.916347 loss_att 5.750045 loss_ctc 8.173877 loss_rnnt 4.149700 hw_loss 0.310444 lr 0.00038567 rank 0
2023-02-24 21:26:27,420 DEBUG TRAIN Batch 20/1300 loss 8.531962 loss_att 9.573482 loss_ctc 9.806740 loss_rnnt 8.008884 hw_loss 0.271508 lr 0.00038563 rank 6
2023-02-24 21:26:27,423 DEBUG TRAIN Batch 20/1300 loss 3.588697 loss_att 4.405885 loss_ctc 5.097136 loss_rnnt 3.142114 hw_loss 0.153787 lr 0.00038564 rank 7
2023-02-24 21:26:27,428 DEBUG TRAIN Batch 20/1300 loss 8.205416 loss_att 9.229314 loss_ctc 14.280879 loss_rnnt 7.102736 hw_loss 0.164697 lr 0.00038567 rank 5
2023-02-24 21:26:27,464 DEBUG TRAIN Batch 20/1300 loss 10.818886 loss_att 12.107147 loss_ctc 13.874850 loss_rnnt 10.075248 hw_loss 0.147233 lr 0.00038567 rank 2
2023-02-24 21:27:41,588 DEBUG TRAIN Batch 20/1400 loss 7.525137 loss_att 9.709711 loss_ctc 8.537947 loss_rnnt 6.827669 hw_loss 0.235337 lr 0.00038552 rank 6
2023-02-24 21:27:41,595 DEBUG TRAIN Batch 20/1400 loss 2.534549 loss_att 5.339898 loss_ctc 4.531361 loss_rnnt 1.581035 hw_loss 0.236629 lr 0.00038553 rank 3
2023-02-24 21:27:41,602 DEBUG TRAIN Batch 20/1400 loss 2.601332 loss_att 5.948852 loss_ctc 2.046836 loss_rnnt 1.933409 hw_loss 0.135660 lr 0.00038556 rank 0
2023-02-24 21:27:41,603 DEBUG TRAIN Batch 20/1400 loss 10.851144 loss_att 13.090209 loss_ctc 14.811976 loss_rnnt 9.784606 hw_loss 0.169902 lr 0.00038554 rank 1
2023-02-24 21:27:41,603 DEBUG TRAIN Batch 20/1400 loss 28.273525 loss_att 30.053822 loss_ctc 40.715469 loss_rnnt 26.170795 hw_loss 0.164519 lr 0.00038555 rank 2
2023-02-24 21:27:41,603 DEBUG TRAIN Batch 20/1400 loss 9.169363 loss_att 11.553867 loss_ctc 14.728851 loss_rnnt 7.867095 hw_loss 0.157693 lr 0.00038555 rank 5
2023-02-24 21:27:41,606 DEBUG TRAIN Batch 20/1400 loss 10.276569 loss_att 12.292969 loss_ctc 14.387228 loss_rnnt 9.276016 hw_loss 0.092224 lr 0.00038557 rank 4
2023-02-24 21:27:41,611 DEBUG TRAIN Batch 20/1400 loss 11.003715 loss_att 16.320189 loss_ctc 14.837561 loss_rnnt 9.413719 hw_loss 0.029101 lr 0.00038552 rank 7
2023-02-24 21:28:55,188 DEBUG TRAIN Batch 20/1500 loss 9.948632 loss_att 11.954207 loss_ctc 9.312778 loss_rnnt 9.555343 hw_loss 0.144290 lr 0.00038543 rank 1
2023-02-24 21:28:55,197 DEBUG TRAIN Batch 20/1500 loss 13.976236 loss_att 14.623293 loss_ctc 18.785290 loss_rnnt 13.103924 hw_loss 0.190678 lr 0.00038545 rank 4
2023-02-24 21:28:55,199 DEBUG TRAIN Batch 20/1500 loss 14.205523 loss_att 17.256140 loss_ctc 23.425070 loss_rnnt 12.252101 hw_loss 0.213797 lr 0.00038541 rank 6
2023-02-24 21:28:55,200 DEBUG TRAIN Batch 20/1500 loss 9.380731 loss_att 11.910311 loss_ctc 12.549835 loss_rnnt 8.321084 hw_loss 0.245969 lr 0.00038544 rank 2
2023-02-24 21:28:55,201 DEBUG TRAIN Batch 20/1500 loss 9.645981 loss_att 11.582827 loss_ctc 15.511162 loss_rnnt 8.349545 hw_loss 0.238207 lr 0.00038544 rank 0
2023-02-24 21:28:55,202 DEBUG TRAIN Batch 20/1500 loss 6.711311 loss_att 10.783596 loss_ctc 9.704962 loss_rnnt 5.414593 hw_loss 0.155827 lr 0.00038541 rank 7
2023-02-24 21:28:55,205 DEBUG TRAIN Batch 20/1500 loss 7.480164 loss_att 11.750731 loss_ctc 9.104747 loss_rnnt 6.337677 hw_loss 0.134553 lr 0.00038542 rank 3
2023-02-24 21:28:55,205 DEBUG TRAIN Batch 20/1500 loss 7.025789 loss_att 9.907881 loss_ctc 8.895054 loss_rnnt 6.079914 hw_loss 0.225416 lr 0.00038544 rank 5
2023-02-24 21:30:06,602 DEBUG TRAIN Batch 20/1600 loss 4.625430 loss_att 6.445120 loss_ctc 6.638077 loss_rnnt 3.921350 hw_loss 0.134603 lr 0.00038529 rank 6
2023-02-24 21:30:06,612 DEBUG TRAIN Batch 20/1600 loss 13.501366 loss_att 15.824472 loss_ctc 20.654287 loss_rnnt 11.996274 hw_loss 0.162649 lr 0.00038533 rank 0
2023-02-24 21:30:06,611 DEBUG TRAIN Batch 20/1600 loss 6.699670 loss_att 8.244360 loss_ctc 7.073970 loss_rnnt 6.214191 hw_loss 0.237440 lr 0.00038529 rank 7
2023-02-24 21:30:06,613 DEBUG TRAIN Batch 20/1600 loss 11.451815 loss_att 12.390181 loss_ctc 16.929609 loss_rnnt 10.481089 hw_loss 0.098774 lr 0.00038534 rank 4
2023-02-24 21:30:06,616 DEBUG TRAIN Batch 20/1600 loss 11.154787 loss_att 12.242012 loss_ctc 12.306843 loss_rnnt 10.669320 hw_loss 0.214527 lr 0.00038530 rank 3
2023-02-24 21:30:06,619 DEBUG TRAIN Batch 20/1600 loss 9.695377 loss_att 14.700245 loss_ctc 18.104746 loss_rnnt 7.455612 hw_loss 0.220393 lr 0.00038531 rank 1
2023-02-24 21:30:06,627 DEBUG TRAIN Batch 20/1600 loss 2.721662 loss_att 5.170187 loss_ctc 5.205582 loss_rnnt 1.721615 hw_loss 0.335912 lr 0.00038532 rank 5
2023-02-24 21:30:06,662 DEBUG TRAIN Batch 20/1600 loss 5.244661 loss_att 9.521851 loss_ctc 10.036851 loss_rnnt 3.598595 hw_loss 0.284382 lr 0.00038532 rank 2
2023-02-24 21:31:19,214 DEBUG TRAIN Batch 20/1700 loss 4.821505 loss_att 8.084869 loss_ctc 7.417977 loss_rnnt 3.645296 hw_loss 0.332514 lr 0.00038520 rank 1
2023-02-24 21:31:19,215 DEBUG TRAIN Batch 20/1700 loss 9.414541 loss_att 11.572706 loss_ctc 10.190586 loss_rnnt 8.781903 hw_loss 0.182871 lr 0.00038521 rank 2
2023-02-24 21:31:19,216 DEBUG TRAIN Batch 20/1700 loss 10.416812 loss_att 12.342050 loss_ctc 14.599588 loss_rnnt 9.423254 hw_loss 0.095262 lr 0.00038518 rank 6
2023-02-24 21:31:19,217 DEBUG TRAIN Batch 20/1700 loss 7.714113 loss_att 8.614126 loss_ctc 8.807049 loss_rnnt 7.229673 hw_loss 0.297584 lr 0.00038521 rank 0
2023-02-24 21:31:19,219 DEBUG TRAIN Batch 20/1700 loss 9.416041 loss_att 11.010064 loss_ctc 12.707570 loss_rnnt 8.478034 hw_loss 0.338123 lr 0.00038522 rank 4
2023-02-24 21:31:19,223 DEBUG TRAIN Batch 20/1700 loss 6.789870 loss_att 8.657797 loss_ctc 9.994444 loss_rnnt 5.823010 hw_loss 0.311245 lr 0.00038518 rank 7
2023-02-24 21:31:19,234 DEBUG TRAIN Batch 20/1700 loss 9.375049 loss_att 11.402311 loss_ctc 11.975096 loss_rnnt 8.485119 hw_loss 0.258381 lr 0.00038521 rank 5
2023-02-24 21:31:19,264 DEBUG TRAIN Batch 20/1700 loss 19.315826 loss_att 21.474768 loss_ctc 26.581123 loss_rnnt 17.785381 hw_loss 0.243656 lr 0.00038519 rank 3
2023-02-24 21:32:34,627 DEBUG TRAIN Batch 20/1800 loss 8.818002 loss_att 11.787057 loss_ctc 11.294677 loss_rnnt 7.762979 hw_loss 0.245602 lr 0.00038511 rank 4
2023-02-24 21:32:34,627 DEBUG TRAIN Batch 20/1800 loss 8.606523 loss_att 10.271993 loss_ctc 15.947619 loss_rnnt 7.206889 hw_loss 0.164488 lr 0.00038509 rank 1
2023-02-24 21:32:34,632 DEBUG TRAIN Batch 20/1800 loss 9.752270 loss_att 11.991832 loss_ctc 13.915053 loss_rnnt 8.643477 hw_loss 0.198455 lr 0.00038506 rank 6
2023-02-24 21:32:34,633 DEBUG TRAIN Batch 20/1800 loss 6.928148 loss_att 9.973814 loss_ctc 10.244575 loss_rnnt 5.744092 hw_loss 0.248873 lr 0.00038506 rank 7
2023-02-24 21:32:34,634 DEBUG TRAIN Batch 20/1800 loss 7.440581 loss_att 10.622511 loss_ctc 9.024585 loss_rnnt 6.492639 hw_loss 0.188167 lr 0.00038509 rank 2
2023-02-24 21:32:34,634 DEBUG TRAIN Batch 20/1800 loss 6.278171 loss_att 9.392657 loss_ctc 9.906996 loss_rnnt 5.074323 hw_loss 0.182076 lr 0.00038507 rank 3
2023-02-24 21:32:34,631 DEBUG TRAIN Batch 20/1800 loss 12.347133 loss_att 13.683333 loss_ctc 17.461647 loss_rnnt 11.215792 hw_loss 0.341561 lr 0.00038510 rank 0
2023-02-24 21:32:34,640 DEBUG TRAIN Batch 20/1800 loss 10.504371 loss_att 11.277529 loss_ctc 14.172546 loss_rnnt 9.736614 hw_loss 0.232565 lr 0.00038509 rank 5
2023-02-24 21:33:47,421 DEBUG TRAIN Batch 20/1900 loss 10.923149 loss_att 10.646399 loss_ctc 12.617279 loss_rnnt 10.643875 hw_loss 0.203885 lr 0.00038495 rank 6
2023-02-24 21:33:47,435 DEBUG TRAIN Batch 20/1900 loss 6.520795 loss_att 10.292957 loss_ctc 10.638997 loss_rnnt 5.067869 hw_loss 0.280127 lr 0.00038498 rank 0
2023-02-24 21:33:47,436 DEBUG TRAIN Batch 20/1900 loss 11.150175 loss_att 12.460247 loss_ctc 15.046162 loss_rnnt 10.213025 hw_loss 0.291881 lr 0.00038499 rank 4
2023-02-24 21:33:47,439 DEBUG TRAIN Batch 20/1900 loss 5.746015 loss_att 8.201506 loss_ctc 8.414966 loss_rnnt 4.793078 hw_loss 0.198710 lr 0.00038496 rank 3
2023-02-24 21:33:47,440 DEBUG TRAIN Batch 20/1900 loss 3.831250 loss_att 5.359200 loss_ctc 4.593101 loss_rnnt 3.300818 hw_loss 0.231116 lr 0.00038498 rank 2
2023-02-24 21:33:47,441 DEBUG TRAIN Batch 20/1900 loss 10.592000 loss_att 12.623754 loss_ctc 15.844327 loss_rnnt 9.371646 hw_loss 0.213175 lr 0.00038495 rank 7
2023-02-24 21:33:47,443 DEBUG TRAIN Batch 20/1900 loss 3.668918 loss_att 5.986694 loss_ctc 4.920573 loss_rnnt 2.907022 hw_loss 0.246475 lr 0.00038498 rank 5
2023-02-24 21:33:47,483 DEBUG TRAIN Batch 20/1900 loss 5.695475 loss_att 5.696696 loss_ctc 7.805402 loss_rnnt 5.287086 hw_loss 0.237790 lr 0.00038497 rank 1
2023-02-24 21:34:59,817 DEBUG TRAIN Batch 20/2000 loss 10.686122 loss_att 13.424282 loss_ctc 15.405838 loss_rnnt 9.387936 hw_loss 0.227360 lr 0.00038488 rank 4
2023-02-24 21:34:59,819 DEBUG TRAIN Batch 20/2000 loss 13.527762 loss_att 16.526958 loss_ctc 18.607174 loss_rnnt 12.128311 hw_loss 0.229418 lr 0.00038486 rank 1
2023-02-24 21:34:59,823 DEBUG TRAIN Batch 20/2000 loss 12.340420 loss_att 14.321192 loss_ctc 15.561369 loss_rnnt 11.406881 hw_loss 0.202357 lr 0.00038487 rank 0
2023-02-24 21:34:59,825 DEBUG TRAIN Batch 20/2000 loss 8.360209 loss_att 9.646997 loss_ctc 12.050159 loss_rnnt 7.517727 hw_loss 0.174619 lr 0.00038484 rank 7
2023-02-24 21:34:59,828 DEBUG TRAIN Batch 20/2000 loss 1.529886 loss_att 4.258782 loss_ctc 2.679528 loss_rnnt 0.764938 hw_loss 0.123531 lr 0.00038483 rank 6
2023-02-24 21:34:59,854 DEBUG TRAIN Batch 20/2000 loss 12.039178 loss_att 12.741867 loss_ctc 16.643116 loss_rnnt 11.173626 hw_loss 0.208419 lr 0.00038485 rank 3
2023-02-24 21:34:59,862 DEBUG TRAIN Batch 20/2000 loss 6.480281 loss_att 9.147555 loss_ctc 8.461952 loss_rnnt 5.608949 hw_loss 0.138101 lr 0.00038486 rank 2
2023-02-24 21:34:59,869 DEBUG TRAIN Batch 20/2000 loss 4.655111 loss_att 7.932036 loss_ctc 8.744042 loss_rnnt 3.341037 hw_loss 0.212808 lr 0.00038487 rank 5
2023-02-24 21:36:14,434 DEBUG TRAIN Batch 20/2100 loss 11.350225 loss_att 14.827589 loss_ctc 11.568693 loss_rnnt 10.496181 hw_loss 0.242704 lr 0.00038476 rank 0
2023-02-24 21:36:14,445 DEBUG TRAIN Batch 20/2100 loss 5.499714 loss_att 8.246401 loss_ctc 5.660219 loss_rnnt 4.820867 hw_loss 0.202706 lr 0.00038473 rank 3
2023-02-24 21:36:14,445 DEBUG TRAIN Batch 20/2100 loss 8.556356 loss_att 14.188997 loss_ctc 13.487368 loss_rnnt 6.651031 hw_loss 0.227490 lr 0.00038474 rank 1
2023-02-24 21:36:14,452 DEBUG TRAIN Batch 20/2100 loss 10.737121 loss_att 15.177497 loss_ctc 18.297451 loss_rnnt 8.747275 hw_loss 0.175735 lr 0.00038475 rank 2
2023-02-24 21:36:14,451 DEBUG TRAIN Batch 20/2100 loss 7.719778 loss_att 9.440143 loss_ctc 11.965361 loss_rnnt 6.656919 hw_loss 0.286329 lr 0.00038472 rank 7
2023-02-24 21:36:14,451 DEBUG TRAIN Batch 20/2100 loss 6.824502 loss_att 11.792578 loss_ctc 10.084372 loss_rnnt 5.294438 hw_loss 0.190872 lr 0.00038477 rank 4
2023-02-24 21:36:14,456 DEBUG TRAIN Batch 20/2100 loss 9.767824 loss_att 10.734440 loss_ctc 13.683554 loss_rnnt 8.918101 hw_loss 0.251818 lr 0.00038475 rank 5
2023-02-24 21:36:14,457 DEBUG TRAIN Batch 20/2100 loss 9.845485 loss_att 14.605465 loss_ctc 14.531299 loss_rnnt 8.168016 hw_loss 0.188806 lr 0.00038472 rank 6
2023-02-24 21:37:28,059 DEBUG TRAIN Batch 20/2200 loss 6.856605 loss_att 10.686382 loss_ctc 10.070724 loss_rnnt 5.580184 hw_loss 0.153592 lr 0.00038464 rank 5
2023-02-24 21:37:28,066 DEBUG TRAIN Batch 20/2200 loss 9.278700 loss_att 12.427601 loss_ctc 14.978535 loss_rnnt 7.810427 hw_loss 0.147215 lr 0.00038464 rank 0
2023-02-24 21:37:28,069 DEBUG TRAIN Batch 20/2200 loss 5.003069 loss_att 7.007700 loss_ctc 9.172250 loss_rnnt 3.914638 hw_loss 0.246776 lr 0.00038463 rank 1
2023-02-24 21:37:28,074 DEBUG TRAIN Batch 20/2200 loss 9.116307 loss_att 12.361874 loss_ctc 15.803038 loss_rnnt 7.388341 hw_loss 0.351168 lr 0.00038461 rank 6
2023-02-24 21:37:28,074 DEBUG TRAIN Batch 20/2200 loss 12.951546 loss_att 15.476610 loss_ctc 23.869123 loss_rnnt 10.918834 hw_loss 0.135041 lr 0.00038461 rank 7
2023-02-24 21:37:28,076 DEBUG TRAIN Batch 20/2200 loss 8.913929 loss_att 13.025230 loss_ctc 12.295429 loss_rnnt 7.529258 hw_loss 0.209146 lr 0.00038465 rank 4
2023-02-24 21:37:28,077 DEBUG TRAIN Batch 20/2200 loss 2.328978 loss_att 4.342156 loss_ctc 3.772817 loss_rnnt 1.614018 hw_loss 0.224648 lr 0.00038464 rank 2
2023-02-24 21:37:28,080 DEBUG TRAIN Batch 20/2200 loss 7.660736 loss_att 10.091236 loss_ctc 8.319765 loss_rnnt 7.026780 hw_loss 0.112470 lr 0.00038462 rank 3
2023-02-24 21:38:40,290 DEBUG TRAIN Batch 20/2300 loss 7.713043 loss_att 9.765528 loss_ctc 11.650225 loss_rnnt 6.694278 hw_loss 0.156209 lr 0.00038450 rank 3
2023-02-24 21:38:40,291 DEBUG TRAIN Batch 20/2300 loss 10.464457 loss_att 13.704939 loss_ctc 17.099186 loss_rnnt 8.790712 hw_loss 0.264406 lr 0.00038449 rank 7
2023-02-24 21:38:40,296 DEBUG TRAIN Batch 20/2300 loss 9.946574 loss_att 12.026976 loss_ctc 12.479101 loss_rnnt 9.101604 hw_loss 0.171036 lr 0.00038454 rank 4
2023-02-24 21:38:40,297 DEBUG TRAIN Batch 20/2300 loss 2.203423 loss_att 4.401235 loss_ctc 3.096262 loss_rnnt 1.528446 hw_loss 0.218193 lr 0.00038452 rank 1
2023-02-24 21:38:40,298 DEBUG TRAIN Batch 20/2300 loss 9.856633 loss_att 12.187132 loss_ctc 14.832621 loss_rnnt 8.639209 hw_loss 0.164739 lr 0.00038452 rank 5
2023-02-24 21:38:40,300 DEBUG TRAIN Batch 20/2300 loss 10.209413 loss_att 11.888338 loss_ctc 20.822653 loss_rnnt 8.339294 hw_loss 0.223564 lr 0.00038453 rank 0
2023-02-24 21:38:40,303 DEBUG TRAIN Batch 20/2300 loss 12.970975 loss_att 16.214918 loss_ctc 22.577595 loss_rnnt 10.932512 hw_loss 0.203984 lr 0.00038452 rank 2
2023-02-24 21:38:40,340 DEBUG TRAIN Batch 20/2300 loss 4.186403 loss_att 8.035054 loss_ctc 8.289387 loss_rnnt 2.803498 hw_loss 0.123957 lr 0.00038449 rank 6
2023-02-24 21:39:52,993 DEBUG TRAIN Batch 20/2400 loss 15.495384 loss_att 16.998247 loss_ctc 21.081268 loss_rnnt 14.305595 hw_loss 0.270809 lr 0.00038440 rank 1
2023-02-24 21:39:52,993 DEBUG TRAIN Batch 20/2400 loss 14.668755 loss_att 14.286903 loss_ctc 15.669151 loss_rnnt 14.526671 hw_loss 0.159499 lr 0.00038439 rank 3
2023-02-24 21:39:52,993 DEBUG TRAIN Batch 20/2400 loss 5.090161 loss_att 7.824640 loss_ctc 7.893613 loss_rnnt 4.020002 hw_loss 0.280257 lr 0.00038442 rank 0
2023-02-24 21:39:52,995 DEBUG TRAIN Batch 20/2400 loss 6.410028 loss_att 8.595760 loss_ctc 7.347030 loss_rnnt 5.738430 hw_loss 0.205347 lr 0.00038443 rank 4
2023-02-24 21:39:52,995 DEBUG TRAIN Batch 20/2400 loss 10.297731 loss_att 12.487808 loss_ctc 11.639969 loss_rnnt 9.624519 hw_loss 0.105434 lr 0.00038438 rank 6
2023-02-24 21:39:52,996 DEBUG TRAIN Batch 20/2400 loss 6.574229 loss_att 8.121780 loss_ctc 7.365302 loss_rnnt 6.026069 hw_loss 0.249700 lr 0.00038441 rank 5
2023-02-24 21:39:52,999 DEBUG TRAIN Batch 20/2400 loss 4.688197 loss_att 6.360971 loss_ctc 6.759874 loss_rnnt 3.943166 hw_loss 0.251721 lr 0.00038441 rank 2
2023-02-24 21:39:53,005 DEBUG TRAIN Batch 20/2400 loss 5.348401 loss_att 7.460588 loss_ctc 7.708004 loss_rnnt 4.459787 hw_loss 0.284180 lr 0.00038438 rank 7
2023-02-24 21:41:07,923 DEBUG TRAIN Batch 20/2500 loss 6.050581 loss_att 6.896805 loss_ctc 8.476568 loss_rnnt 5.417310 hw_loss 0.263554 lr 0.00038430 rank 2
2023-02-24 21:41:07,924 DEBUG TRAIN Batch 20/2500 loss 8.520277 loss_att 8.375324 loss_ctc 10.022992 loss_rnnt 8.169538 hw_loss 0.336313 lr 0.00038429 rank 1
2023-02-24 21:41:07,925 DEBUG TRAIN Batch 20/2500 loss 11.867360 loss_att 12.934504 loss_ctc 15.347819 loss_rnnt 11.095652 hw_loss 0.176657 lr 0.00038430 rank 0
2023-02-24 21:41:07,925 DEBUG TRAIN Batch 20/2500 loss 9.314092 loss_att 9.212571 loss_ctc 13.843289 loss_rnnt 8.580790 hw_loss 0.280711 lr 0.00038431 rank 4
2023-02-24 21:41:07,925 DEBUG TRAIN Batch 20/2500 loss 9.775274 loss_att 13.943959 loss_ctc 14.817182 loss_rnnt 8.191826 hw_loss 0.145232 lr 0.00038428 rank 3
2023-02-24 21:41:07,928 DEBUG TRAIN Batch 20/2500 loss 11.216852 loss_att 12.148794 loss_ctc 14.295221 loss_rnnt 10.466636 hw_loss 0.287585 lr 0.00038427 rank 6
2023-02-24 21:41:07,929 DEBUG TRAIN Batch 20/2500 loss 5.432973 loss_att 6.958559 loss_ctc 7.222316 loss_rnnt 4.800694 hw_loss 0.166095 lr 0.00038427 rank 7
2023-02-24 21:41:07,941 DEBUG TRAIN Batch 20/2500 loss 7.011003 loss_att 6.803537 loss_ctc 9.247885 loss_rnnt 6.627607 hw_loss 0.237447 lr 0.00038430 rank 5
2023-02-24 21:42:19,875 DEBUG TRAIN Batch 20/2600 loss 8.848715 loss_att 12.106846 loss_ctc 13.570836 loss_rnnt 7.415404 hw_loss 0.285127 lr 0.00038416 rank 3
2023-02-24 21:42:19,875 DEBUG TRAIN Batch 20/2600 loss 7.905514 loss_att 9.233641 loss_ctc 12.520191 loss_rnnt 6.913169 hw_loss 0.208933 lr 0.00038415 rank 7
2023-02-24 21:42:19,876 DEBUG TRAIN Batch 20/2600 loss 5.669911 loss_att 10.629780 loss_ctc 15.375136 loss_rnnt 3.217551 hw_loss 0.311919 lr 0.00038419 rank 0
2023-02-24 21:42:19,877 DEBUG TRAIN Batch 20/2600 loss 6.614118 loss_att 8.922354 loss_ctc 8.488445 loss_rnnt 5.783458 hw_loss 0.223317 lr 0.00038420 rank 4
2023-02-24 21:42:19,878 DEBUG TRAIN Batch 20/2600 loss 14.739595 loss_att 16.243532 loss_ctc 18.187231 loss_rnnt 13.925390 hw_loss 0.100751 lr 0.00038418 rank 2
2023-02-24 21:42:19,879 DEBUG TRAIN Batch 20/2600 loss 4.647214 loss_att 6.777350 loss_ctc 5.921414 loss_rnnt 3.919329 hw_loss 0.247432 lr 0.00038418 rank 5
2023-02-24 21:42:19,880 DEBUG TRAIN Batch 20/2600 loss 13.839771 loss_att 17.466759 loss_ctc 23.142624 loss_rnnt 11.791693 hw_loss 0.154314 lr 0.00038415 rank 6
2023-02-24 21:42:19,933 DEBUG TRAIN Batch 20/2600 loss 6.514177 loss_att 10.204442 loss_ctc 8.050697 loss_rnnt 5.471615 hw_loss 0.186825 lr 0.00038418 rank 1
2023-02-24 21:43:32,439 DEBUG TRAIN Batch 20/2700 loss 10.103472 loss_att 13.868978 loss_ctc 12.425827 loss_rnnt 8.848999 hw_loss 0.359482 lr 0.00038407 rank 0
2023-02-24 21:43:32,439 DEBUG TRAIN Batch 20/2700 loss 9.211310 loss_att 12.105003 loss_ctc 12.792559 loss_rnnt 8.053535 hw_loss 0.190380 lr 0.00038409 rank 4
2023-02-24 21:43:32,440 DEBUG TRAIN Batch 20/2700 loss 7.316227 loss_att 13.598228 loss_ctc 9.226110 loss_rnnt 5.703570 hw_loss 0.190511 lr 0.00038406 rank 1
2023-02-24 21:43:32,442 DEBUG TRAIN Batch 20/2700 loss 10.220787 loss_att 13.910255 loss_ctc 17.872543 loss_rnnt 8.393417 hw_loss 0.129829 lr 0.00038407 rank 2
2023-02-24 21:43:32,446 DEBUG TRAIN Batch 20/2700 loss 12.396890 loss_att 16.092600 loss_ctc 16.398697 loss_rnnt 11.027874 hw_loss 0.180561 lr 0.00038404 rank 7
2023-02-24 21:43:32,447 DEBUG TRAIN Batch 20/2700 loss 5.018413 loss_att 7.460819 loss_ctc 10.140140 loss_rnnt 3.790637 hw_loss 0.105746 lr 0.00038405 rank 3
2023-02-24 21:43:32,456 DEBUG TRAIN Batch 20/2700 loss 14.201183 loss_att 16.314116 loss_ctc 17.869368 loss_rnnt 13.162003 hw_loss 0.239067 lr 0.00038407 rank 5
2023-02-24 21:43:32,489 DEBUG TRAIN Batch 20/2700 loss 11.683309 loss_att 14.740344 loss_ctc 16.303669 loss_rnnt 10.361500 hw_loss 0.176912 lr 0.00038404 rank 6
2023-02-24 21:44:46,397 DEBUG TRAIN Batch 20/2800 loss 8.262158 loss_att 9.906180 loss_ctc 13.113922 loss_rnnt 7.177705 hw_loss 0.203900 lr 0.00038396 rank 5
2023-02-24 21:44:46,398 DEBUG TRAIN Batch 20/2800 loss 9.790342 loss_att 11.711255 loss_ctc 10.158548 loss_rnnt 9.267797 hw_loss 0.167377 lr 0.00038396 rank 2
2023-02-24 21:44:46,410 DEBUG TRAIN Batch 20/2800 loss 6.747763 loss_att 8.717192 loss_ctc 8.668364 loss_rnnt 5.999226 hw_loss 0.184821 lr 0.00038397 rank 4
2023-02-24 21:44:46,411 DEBUG TRAIN Batch 20/2800 loss 14.701517 loss_att 16.870529 loss_ctc 18.066786 loss_rnnt 13.679767 hw_loss 0.261085 lr 0.00038396 rank 0
2023-02-24 21:44:46,412 DEBUG TRAIN Batch 20/2800 loss 18.048780 loss_att 21.257881 loss_ctc 20.243500 loss_rnnt 16.965414 hw_loss 0.279223 lr 0.00038393 rank 7
2023-02-24 21:44:46,429 DEBUG TRAIN Batch 20/2800 loss 6.748883 loss_att 10.164832 loss_ctc 9.764552 loss_rnnt 5.595526 hw_loss 0.127647 lr 0.00038393 rank 6
2023-02-24 21:44:46,453 DEBUG TRAIN Batch 20/2800 loss 13.592628 loss_att 15.100576 loss_ctc 20.346294 loss_rnnt 12.291477 hw_loss 0.185759 lr 0.00038394 rank 3
2023-02-24 21:44:46,453 DEBUG TRAIN Batch 20/2800 loss 17.802980 loss_att 18.981371 loss_ctc 21.427626 loss_rnnt 16.955595 hw_loss 0.240790 lr 0.00038395 rank 1
2023-02-24 21:46:01,116 DEBUG TRAIN Batch 20/2900 loss 8.822820 loss_att 12.846604 loss_ctc 14.349885 loss_rnnt 7.139648 hw_loss 0.265261 lr 0.00038386 rank 4
2023-02-24 21:46:01,119 DEBUG TRAIN Batch 20/2900 loss 15.395836 loss_att 15.251920 loss_ctc 18.135618 loss_rnnt 14.949627 hw_loss 0.205665 lr 0.00038382 rank 3
2023-02-24 21:46:01,119 DEBUG TRAIN Batch 20/2900 loss 6.865693 loss_att 9.420292 loss_ctc 6.707469 loss_rnnt 6.216509 hw_loss 0.298800 lr 0.00038381 rank 6
2023-02-24 21:46:01,120 DEBUG TRAIN Batch 20/2900 loss 7.840975 loss_att 12.069089 loss_ctc 12.243154 loss_rnnt 6.272516 hw_loss 0.254774 lr 0.00038384 rank 2
2023-02-24 21:46:01,124 DEBUG TRAIN Batch 20/2900 loss 5.017190 loss_att 7.767580 loss_ctc 5.488913 loss_rnnt 4.300944 hw_loss 0.193634 lr 0.00038381 rank 7
2023-02-24 21:46:01,130 DEBUG TRAIN Batch 20/2900 loss 11.986439 loss_att 14.026300 loss_ctc 14.407911 loss_rnnt 11.093688 hw_loss 0.303593 lr 0.00038384 rank 5
2023-02-24 21:46:01,133 DEBUG TRAIN Batch 20/2900 loss 13.201707 loss_att 15.763580 loss_ctc 18.416416 loss_rnnt 11.920493 hw_loss 0.137898 lr 0.00038385 rank 0
2023-02-24 21:46:01,174 DEBUG TRAIN Batch 20/2900 loss 5.804232 loss_att 7.539399 loss_ctc 8.007531 loss_rnnt 4.997202 hw_loss 0.311668 lr 0.00038384 rank 1
2023-02-24 21:47:14,471 DEBUG TRAIN Batch 20/3000 loss 6.889197 loss_att 10.047546 loss_ctc 8.339787 loss_rnnt 5.936303 hw_loss 0.239648 lr 0.00038370 rank 6
2023-02-24 21:47:14,471 DEBUG TRAIN Batch 20/3000 loss 9.206162 loss_att 11.865059 loss_ctc 13.584581 loss_rnnt 7.934653 hw_loss 0.292390 lr 0.00038374 rank 0
2023-02-24 21:47:14,472 DEBUG TRAIN Batch 20/3000 loss 9.135191 loss_att 13.621494 loss_ctc 11.810490 loss_rnnt 7.770552 hw_loss 0.207510 lr 0.00038371 rank 3
2023-02-24 21:47:14,473 DEBUG TRAIN Batch 20/3000 loss 9.985092 loss_att 14.344261 loss_ctc 13.751022 loss_rnnt 8.520825 hw_loss 0.169329 lr 0.00038375 rank 4
2023-02-24 21:47:14,477 DEBUG TRAIN Batch 20/3000 loss 2.880907 loss_att 5.827287 loss_ctc 5.756867 loss_rnnt 1.810748 hw_loss 0.182664 lr 0.00038373 rank 5
2023-02-24 21:47:14,479 DEBUG TRAIN Batch 20/3000 loss 8.670914 loss_att 12.072615 loss_ctc 13.026728 loss_rnnt 7.270759 hw_loss 0.260699 lr 0.00038370 rank 7
2023-02-24 21:47:14,480 DEBUG TRAIN Batch 20/3000 loss 7.074152 loss_att 9.078565 loss_ctc 6.913715 loss_rnnt 6.598077 hw_loss 0.181095 lr 0.00038372 rank 1
2023-02-24 21:47:14,480 DEBUG TRAIN Batch 20/3000 loss 6.056026 loss_att 7.528021 loss_ctc 6.247633 loss_rnnt 5.642560 hw_loss 0.175347 lr 0.00038373 rank 2
2023-02-24 21:48:29,064 DEBUG TRAIN Batch 20/3100 loss 14.601285 loss_att 16.577717 loss_ctc 18.377926 loss_rnnt 13.543988 hw_loss 0.297110 lr 0.00038363 rank 4
2023-02-24 21:48:29,064 DEBUG TRAIN Batch 20/3100 loss 3.274903 loss_att 5.086193 loss_ctc 4.607902 loss_rnnt 2.598698 hw_loss 0.255400 lr 0.00038359 rank 6
2023-02-24 21:48:29,066 DEBUG TRAIN Batch 20/3100 loss 8.925273 loss_att 11.012980 loss_ctc 11.476743 loss_rnnt 8.068841 hw_loss 0.185051 lr 0.00038361 rank 1
2023-02-24 21:48:29,066 DEBUG TRAIN Batch 20/3100 loss 11.780463 loss_att 11.597404 loss_ctc 17.904440 loss_rnnt 10.851881 hw_loss 0.278745 lr 0.00038362 rank 0
2023-02-24 21:48:29,068 DEBUG TRAIN Batch 20/3100 loss 2.719138 loss_att 4.460675 loss_ctc 2.851609 loss_rnnt 2.231290 hw_loss 0.228522 lr 0.00038360 rank 3
2023-02-24 21:48:29,074 DEBUG TRAIN Batch 20/3100 loss 10.086865 loss_att 11.748828 loss_ctc 14.222427 loss_rnnt 9.045231 hw_loss 0.295937 lr 0.00038362 rank 5
2023-02-24 21:48:29,074 DEBUG TRAIN Batch 20/3100 loss 3.471277 loss_att 5.337263 loss_ctc 5.923258 loss_rnnt 2.645017 hw_loss 0.236497 lr 0.00038359 rank 7
2023-02-24 21:48:29,084 DEBUG TRAIN Batch 20/3100 loss 11.499438 loss_att 13.418036 loss_ctc 15.216403 loss_rnnt 10.480122 hw_loss 0.262506 lr 0.00038362 rank 2
2023-02-24 21:49:45,716 DEBUG TRAIN Batch 20/3200 loss 9.859796 loss_att 10.412145 loss_ctc 12.093430 loss_rnnt 9.294851 hw_loss 0.293731 lr 0.00038347 rank 6
2023-02-24 21:49:45,720 DEBUG TRAIN Batch 20/3200 loss 9.163723 loss_att 11.230080 loss_ctc 13.721229 loss_rnnt 7.999466 hw_loss 0.268721 lr 0.00038347 rank 7
2023-02-24 21:49:45,720 DEBUG TRAIN Batch 20/3200 loss 2.850572 loss_att 5.633639 loss_ctc 2.853057 loss_rnnt 2.167335 hw_loss 0.236797 lr 0.00038351 rank 0
2023-02-24 21:49:45,721 DEBUG TRAIN Batch 20/3200 loss 17.136795 loss_att 17.587410 loss_ctc 22.062223 loss_rnnt 16.282362 hw_loss 0.201727 lr 0.00038348 rank 3
2023-02-24 21:49:45,726 DEBUG TRAIN Batch 20/3200 loss 7.516897 loss_att 11.414626 loss_ctc 7.673219 loss_rnnt 6.617622 hw_loss 0.185410 lr 0.00038351 rank 5
2023-02-24 21:49:45,739 DEBUG TRAIN Batch 20/3200 loss 14.581187 loss_att 14.271090 loss_ctc 20.191853 loss_rnnt 13.804522 hw_loss 0.169866 lr 0.00038350 rank 2
2023-02-24 21:49:45,764 DEBUG TRAIN Batch 20/3200 loss 9.475286 loss_att 15.191216 loss_ctc 13.685298 loss_rnnt 7.661476 hw_loss 0.204918 lr 0.00038352 rank 4
2023-02-24 21:49:45,771 DEBUG TRAIN Batch 20/3200 loss 9.116971 loss_att 9.491922 loss_ctc 14.492694 loss_rnnt 8.200918 hw_loss 0.233060 lr 0.00038350 rank 1
2023-02-24 21:50:57,980 DEBUG TRAIN Batch 20/3300 loss 10.068837 loss_att 13.845217 loss_ctc 15.611504 loss_rnnt 8.457869 hw_loss 0.218757 lr 0.00038340 rank 0
2023-02-24 21:50:57,986 DEBUG TRAIN Batch 20/3300 loss 10.889049 loss_att 16.016254 loss_ctc 12.984762 loss_rnnt 9.517333 hw_loss 0.125337 lr 0.00038336 rank 7
2023-02-24 21:50:57,988 DEBUG TRAIN Batch 20/3300 loss 7.055180 loss_att 12.922748 loss_ctc 10.461615 loss_rnnt 5.313132 hw_loss 0.214392 lr 0.00038338 rank 1
2023-02-24 21:50:57,991 DEBUG TRAIN Batch 20/3300 loss 7.792393 loss_att 11.065384 loss_ctc 7.878320 loss_rnnt 7.025194 hw_loss 0.189646 lr 0.00038339 rank 2
2023-02-24 21:50:57,996 DEBUG TRAIN Batch 20/3300 loss 6.106832 loss_att 10.106291 loss_ctc 9.859251 loss_rnnt 4.672593 hw_loss 0.251296 lr 0.00038339 rank 5
2023-02-24 21:50:58,000 DEBUG TRAIN Batch 20/3300 loss 9.244373 loss_att 15.460775 loss_ctc 11.290729 loss_rnnt 7.629045 hw_loss 0.185999 lr 0.00038341 rank 4
2023-02-24 21:50:58,002 DEBUG TRAIN Batch 20/3300 loss 3.552096 loss_att 5.553909 loss_ctc 4.046396 loss_rnnt 2.956836 hw_loss 0.241858 lr 0.00038336 rank 6
2023-02-24 21:50:58,033 DEBUG TRAIN Batch 20/3300 loss 3.918852 loss_att 6.291545 loss_ctc 5.766206 loss_rnnt 3.120942 hw_loss 0.144482 lr 0.00038337 rank 3
2023-02-24 21:52:10,824 DEBUG TRAIN Batch 20/3400 loss 13.827583 loss_att 13.732782 loss_ctc 16.776194 loss_rnnt 13.307819 hw_loss 0.272957 lr 0.00038328 rank 0
2023-02-24 21:52:10,826 DEBUG TRAIN Batch 20/3400 loss 7.849412 loss_att 11.144118 loss_ctc 13.285782 loss_rnnt 6.355115 hw_loss 0.207201 lr 0.00038325 rank 7
2023-02-24 21:52:10,827 DEBUG TRAIN Batch 20/3400 loss 4.091724 loss_att 7.024917 loss_ctc 8.313231 loss_rnnt 2.860258 hw_loss 0.153675 lr 0.00038329 rank 4
2023-02-24 21:52:10,830 DEBUG TRAIN Batch 20/3400 loss 5.188786 loss_att 7.778337 loss_ctc 5.417475 loss_rnnt 4.545364 hw_loss 0.178160 lr 0.00038328 rank 5
2023-02-24 21:52:10,832 DEBUG TRAIN Batch 20/3400 loss 5.397077 loss_att 8.591060 loss_ctc 9.042194 loss_rnnt 4.092208 hw_loss 0.337606 lr 0.00038328 rank 2
2023-02-24 21:52:10,838 DEBUG TRAIN Batch 20/3400 loss 5.496107 loss_att 8.512465 loss_ctc 6.518097 loss_rnnt 4.663699 hw_loss 0.174133 lr 0.00038327 rank 1
2023-02-24 21:52:10,844 DEBUG TRAIN Batch 20/3400 loss 17.600338 loss_att 24.244684 loss_ctc 23.164343 loss_rnnt 15.373615 hw_loss 0.292472 lr 0.00038325 rank 6
2023-02-24 21:52:10,876 DEBUG TRAIN Batch 20/3400 loss 2.890829 loss_att 7.482528 loss_ctc 5.186173 loss_rnnt 1.606240 hw_loss 0.112880 lr 0.00038326 rank 3
2023-02-24 21:53:25,052 DEBUG TRAIN Batch 20/3500 loss 5.321565 loss_att 7.112011 loss_ctc 7.568057 loss_rnnt 4.526694 hw_loss 0.257343 lr 0.00038317 rank 0
2023-02-24 21:53:25,052 DEBUG TRAIN Batch 20/3500 loss 8.791348 loss_att 10.503779 loss_ctc 17.099676 loss_rnnt 7.201385 hw_loss 0.261939 lr 0.00038314 rank 6
2023-02-24 21:53:25,054 DEBUG TRAIN Batch 20/3500 loss 4.641155 loss_att 7.508667 loss_ctc 5.755383 loss_rnnt 3.820133 hw_loss 0.185543 lr 0.00038314 rank 7
2023-02-24 21:53:25,056 DEBUG TRAIN Batch 20/3500 loss 4.623941 loss_att 6.775601 loss_ctc 6.960060 loss_rnnt 3.785823 hw_loss 0.180570 lr 0.00038317 rank 5
2023-02-24 21:53:25,058 DEBUG TRAIN Batch 20/3500 loss 15.577729 loss_att 18.053614 loss_ctc 21.337879 loss_rnnt 14.199812 hw_loss 0.215102 lr 0.00038318 rank 4
2023-02-24 21:53:25,061 DEBUG TRAIN Batch 20/3500 loss 5.721403 loss_att 8.949254 loss_ctc 10.730652 loss_rnnt 4.265449 hw_loss 0.267156 lr 0.00038316 rank 1
2023-02-24 21:53:25,079 DEBUG TRAIN Batch 20/3500 loss 16.081907 loss_att 18.783384 loss_ctc 20.211136 loss_rnnt 14.911604 hw_loss 0.148959 lr 0.00038317 rank 2
2023-02-24 21:53:25,080 DEBUG TRAIN Batch 20/3500 loss 8.408734 loss_att 11.560778 loss_ctc 13.234024 loss_rnnt 7.017301 hw_loss 0.220600 lr 0.00038315 rank 3
2023-02-24 21:54:38,442 DEBUG TRAIN Batch 20/3600 loss 11.929996 loss_att 15.027191 loss_ctc 16.138451 loss_rnnt 10.663806 hw_loss 0.160546 lr 0.00038306 rank 0
2023-02-24 21:54:38,452 DEBUG TRAIN Batch 20/3600 loss 9.205283 loss_att 13.063358 loss_ctc 12.236290 loss_rnnt 7.938631 hw_loss 0.170444 lr 0.00038305 rank 2
2023-02-24 21:54:38,454 DEBUG TRAIN Batch 20/3600 loss 4.638097 loss_att 8.227699 loss_ctc 6.121319 loss_rnnt 3.607813 hw_loss 0.214875 lr 0.00038302 rank 7
2023-02-24 21:54:38,455 DEBUG TRAIN Batch 20/3600 loss 9.080764 loss_att 12.603151 loss_ctc 11.722206 loss_rnnt 7.872236 hw_loss 0.284733 lr 0.00038305 rank 1
2023-02-24 21:54:38,455 DEBUG TRAIN Batch 20/3600 loss 8.065996 loss_att 9.741583 loss_ctc 10.652388 loss_rnnt 7.282842 hw_loss 0.193470 lr 0.00038303 rank 3
2023-02-24 21:54:38,456 DEBUG TRAIN Batch 20/3600 loss 8.047914 loss_att 10.761976 loss_ctc 10.868731 loss_rnnt 7.042422 hw_loss 0.162320 lr 0.00038307 rank 4
2023-02-24 21:54:38,456 DEBUG TRAIN Batch 20/3600 loss 15.699384 loss_att 16.120281 loss_ctc 17.418694 loss_rnnt 15.305510 hw_loss 0.150853 lr 0.00038302 rank 6
2023-02-24 21:54:38,459 DEBUG TRAIN Batch 20/3600 loss 11.781998 loss_att 14.196543 loss_ctc 18.350407 loss_rnnt 10.308893 hw_loss 0.214516 lr 0.00038305 rank 5
2023-02-24 21:55:50,853 DEBUG TRAIN Batch 20/3700 loss 6.067658 loss_att 7.463919 loss_ctc 7.172250 loss_rnnt 5.521079 hw_loss 0.225090 lr 0.00038295 rank 0
2023-02-24 21:55:50,859 DEBUG TRAIN Batch 20/3700 loss 7.953484 loss_att 11.448856 loss_ctc 8.896094 loss_rnnt 7.004418 hw_loss 0.233081 lr 0.00038291 rank 6
2023-02-24 21:55:50,862 DEBUG TRAIN Batch 20/3700 loss 12.932048 loss_att 14.346847 loss_ctc 15.309395 loss_rnnt 12.231039 hw_loss 0.189508 lr 0.00038292 rank 3
2023-02-24 21:55:50,862 DEBUG TRAIN Batch 20/3700 loss 15.940874 loss_att 19.551477 loss_ctc 26.668068 loss_rnnt 13.687572 hw_loss 0.189165 lr 0.00038291 rank 7
2023-02-24 21:55:50,864 DEBUG TRAIN Batch 20/3700 loss 17.252590 loss_att 17.210873 loss_ctc 24.269203 loss_rnnt 16.206455 hw_loss 0.222992 lr 0.00038294 rank 2
2023-02-24 21:55:50,865 DEBUG TRAIN Batch 20/3700 loss 7.245451 loss_att 9.698562 loss_ctc 12.932062 loss_rnnt 5.907365 hw_loss 0.167342 lr 0.00038293 rank 1
2023-02-24 21:55:50,867 DEBUG TRAIN Batch 20/3700 loss 7.100611 loss_att 8.666402 loss_ctc 8.682149 loss_rnnt 6.466846 hw_loss 0.205753 lr 0.00038294 rank 5
2023-02-24 21:55:50,868 DEBUG TRAIN Batch 20/3700 loss 9.389264 loss_att 9.576241 loss_ctc 13.545853 loss_rnnt 8.585862 hw_loss 0.397115 lr 0.00038296 rank 4
2023-02-24 21:57:03,791 DEBUG TRAIN Batch 20/3800 loss 6.897270 loss_att 10.454946 loss_ctc 13.949603 loss_rnnt 5.082700 hw_loss 0.305106 lr 0.00038280 rank 6
2023-02-24 21:57:03,797 DEBUG TRAIN Batch 20/3800 loss 14.661781 loss_att 19.972151 loss_ctc 19.942724 loss_rnnt 12.804645 hw_loss 0.170507 lr 0.00038283 rank 0
2023-02-24 21:57:03,799 DEBUG TRAIN Batch 20/3800 loss 8.695295 loss_att 9.832132 loss_ctc 11.810169 loss_rnnt 7.974574 hw_loss 0.146320 lr 0.00038283 rank 2
2023-02-24 21:57:03,805 DEBUG TRAIN Batch 20/3800 loss 13.104766 loss_att 14.218143 loss_ctc 19.980377 loss_rnnt 11.888491 hw_loss 0.144099 lr 0.00038284 rank 4
2023-02-24 21:57:03,805 DEBUG TRAIN Batch 20/3800 loss 26.386509 loss_att 27.006836 loss_ctc 34.136814 loss_rnnt 25.125637 hw_loss 0.193937 lr 0.00038281 rank 3
2023-02-24 21:57:03,807 DEBUG TRAIN Batch 20/3800 loss 14.107805 loss_att 18.369858 loss_ctc 22.760735 loss_rnnt 12.074016 hw_loss 0.051853 lr 0.00038283 rank 5
2023-02-24 21:57:03,810 DEBUG TRAIN Batch 20/3800 loss 11.059219 loss_att 14.524975 loss_ctc 17.454975 loss_rnnt 9.370467 hw_loss 0.267814 lr 0.00038280 rank 7
2023-02-24 21:57:03,850 DEBUG TRAIN Batch 20/3800 loss 15.564234 loss_att 18.841187 loss_ctc 18.721884 loss_rnnt 14.376388 hw_loss 0.208941 lr 0.00038282 rank 1
2023-02-24 21:58:18,878 DEBUG TRAIN Batch 20/3900 loss 10.926264 loss_att 10.515298 loss_ctc 12.014166 loss_rnnt 10.716590 hw_loss 0.275275 lr 0.00038269 rank 7
2023-02-24 21:58:18,890 DEBUG TRAIN Batch 20/3900 loss 7.592092 loss_att 8.217555 loss_ctc 10.084394 loss_rnnt 7.005756 hw_loss 0.241754 lr 0.00038269 rank 6
2023-02-24 21:58:18,892 DEBUG TRAIN Batch 20/3900 loss 9.109297 loss_att 14.075195 loss_ctc 13.186682 loss_rnnt 7.468205 hw_loss 0.195487 lr 0.00038273 rank 4
2023-02-24 21:58:18,898 DEBUG TRAIN Batch 20/3900 loss 5.535434 loss_att 7.931849 loss_ctc 6.414601 loss_rnnt 4.818654 hw_loss 0.225516 lr 0.00038272 rank 2
2023-02-24 21:58:18,899 DEBUG TRAIN Batch 20/3900 loss 6.774045 loss_att 8.603283 loss_ctc 8.442792 loss_rnnt 6.107170 hw_loss 0.147240 lr 0.00038272 rank 0
2023-02-24 21:58:18,913 DEBUG TRAIN Batch 20/3900 loss 2.775693 loss_att 5.977533 loss_ctc 3.856346 loss_rnnt 1.909591 hw_loss 0.153087 lr 0.00038271 rank 1
2023-02-24 21:58:18,921 DEBUG TRAIN Batch 20/3900 loss 8.721209 loss_att 13.121662 loss_ctc 10.455517 loss_rnnt 7.453226 hw_loss 0.293720 lr 0.00038272 rank 5
2023-02-24 21:58:18,941 DEBUG TRAIN Batch 20/3900 loss 9.931273 loss_att 11.398145 loss_ctc 17.858833 loss_rnnt 8.427393 hw_loss 0.287808 lr 0.00038270 rank 3
2023-02-24 21:59:31,773 DEBUG TRAIN Batch 20/4000 loss 8.619255 loss_att 10.388201 loss_ctc 11.707220 loss_rnnt 7.755473 hw_loss 0.184245 lr 0.00038261 rank 0
2023-02-24 21:59:31,777 DEBUG TRAIN Batch 20/4000 loss 8.754819 loss_att 11.276747 loss_ctc 12.875048 loss_rnnt 7.614266 hw_loss 0.162756 lr 0.00038258 rank 7
2023-02-24 21:59:31,779 DEBUG TRAIN Batch 20/4000 loss 8.280438 loss_att 9.840335 loss_ctc 9.584129 loss_rnnt 7.667689 hw_loss 0.238020 lr 0.00038260 rank 1
2023-02-24 21:59:31,780 DEBUG TRAIN Batch 20/4000 loss 2.528559 loss_att 6.131485 loss_ctc 2.842321 loss_rnnt 1.657727 hw_loss 0.203274 lr 0.00038259 rank 3
2023-02-24 21:59:31,781 DEBUG TRAIN Batch 20/4000 loss 10.198316 loss_att 12.271858 loss_ctc 13.777319 loss_rnnt 9.235087 hw_loss 0.133722 lr 0.00038262 rank 4
2023-02-24 21:59:31,789 DEBUG TRAIN Batch 20/4000 loss 7.060179 loss_att 12.264565 loss_ctc 10.056345 loss_rnnt 5.539853 hw_loss 0.149924 lr 0.00038260 rank 2
2023-02-24 21:59:31,792 DEBUG TRAIN Batch 20/4000 loss 2.985710 loss_att 5.688451 loss_ctc 4.924659 loss_rnnt 2.058931 hw_loss 0.239446 lr 0.00038257 rank 6
2023-02-24 21:59:31,844 DEBUG TRAIN Batch 20/4000 loss 8.163911 loss_att 12.991602 loss_ctc 14.807124 loss_rnnt 6.192062 hw_loss 0.226028 lr 0.00038261 rank 5
2023-02-24 22:00:43,878 DEBUG TRAIN Batch 20/4100 loss 16.723141 loss_att 22.251667 loss_ctc 24.889784 loss_rnnt 14.403727 hw_loss 0.234041 lr 0.00038246 rank 6
2023-02-24 22:00:43,884 DEBUG TRAIN Batch 20/4100 loss 10.386335 loss_att 14.558109 loss_ctc 11.543571 loss_rnnt 9.266501 hw_loss 0.245964 lr 0.00038250 rank 0
2023-02-24 22:00:43,883 DEBUG TRAIN Batch 20/4100 loss 3.576215 loss_att 6.264510 loss_ctc 4.686495 loss_rnnt 2.791917 hw_loss 0.184878 lr 0.00038247 rank 3
2023-02-24 22:00:43,886 DEBUG TRAIN Batch 20/4100 loss 4.777775 loss_att 8.701947 loss_ctc 5.522518 loss_rnnt 3.777522 hw_loss 0.217724 lr 0.00038251 rank 4
2023-02-24 22:00:43,888 DEBUG TRAIN Batch 20/4100 loss 4.990964 loss_att 8.517457 loss_ctc 6.553645 loss_rnnt 3.987609 hw_loss 0.168185 lr 0.00038246 rank 7
2023-02-24 22:00:43,889 DEBUG TRAIN Batch 20/4100 loss 11.510164 loss_att 13.483738 loss_ctc 16.616138 loss_rnnt 10.338560 hw_loss 0.180172 lr 0.00038249 rank 1
2023-02-24 22:00:43,889 DEBUG TRAIN Batch 20/4100 loss 8.830670 loss_att 10.878588 loss_ctc 12.558475 loss_rnnt 7.859643 hw_loss 0.120757 lr 0.00038249 rank 2
2023-02-24 22:00:43,893 DEBUG TRAIN Batch 20/4100 loss 9.594172 loss_att 10.706660 loss_ctc 10.114149 loss_rnnt 9.174493 hw_loss 0.239719 lr 0.00038249 rank 5
2023-02-24 22:01:57,495 DEBUG TRAIN Batch 20/4200 loss 11.212818 loss_att 17.998232 loss_ctc 19.505707 loss_rnnt 8.666492 hw_loss 0.156608 lr 0.00038239 rank 0
2023-02-24 22:01:57,500 DEBUG TRAIN Batch 20/4200 loss 4.056035 loss_att 7.901114 loss_ctc 7.021276 loss_rnnt 2.704793 hw_loss 0.350362 lr 0.00038235 rank 6
2023-02-24 22:01:57,501 DEBUG TRAIN Batch 20/4200 loss 8.888255 loss_att 13.579154 loss_ctc 13.337864 loss_rnnt 7.228899 hw_loss 0.239801 lr 0.00038238 rank 5
2023-02-24 22:01:57,502 DEBUG TRAIN Batch 20/4200 loss 11.814006 loss_att 15.043957 loss_ctc 24.963322 loss_rnnt 9.340679 hw_loss 0.138927 lr 0.00038235 rank 7
2023-02-24 22:01:57,511 DEBUG TRAIN Batch 20/4200 loss 4.762694 loss_att 7.819534 loss_ctc 5.146065 loss_rnnt 4.000394 hw_loss 0.187156 lr 0.00038240 rank 4
2023-02-24 22:01:57,510 DEBUG TRAIN Batch 20/4200 loss 7.455394 loss_att 9.826617 loss_ctc 9.266630 loss_rnnt 6.663868 hw_loss 0.142092 lr 0.00038236 rank 3
2023-02-24 22:01:57,521 DEBUG TRAIN Batch 20/4200 loss 16.035295 loss_att 18.143822 loss_ctc 15.617575 loss_rnnt 15.583050 hw_loss 0.161696 lr 0.00038238 rank 2
2023-02-24 22:01:57,520 DEBUG TRAIN Batch 20/4200 loss 12.225072 loss_att 14.418408 loss_ctc 17.357765 loss_rnnt 11.013042 hw_loss 0.166879 lr 0.00038237 rank 1
2023-02-24 22:03:12,274 DEBUG TRAIN Batch 20/4300 loss 18.250671 loss_att 19.307699 loss_ctc 24.679554 loss_rnnt 17.057037 hw_loss 0.234457 lr 0.00038227 rank 0
2023-02-24 22:03:12,275 DEBUG TRAIN Batch 20/4300 loss 8.966154 loss_att 13.042551 loss_ctc 11.170364 loss_rnnt 7.760307 hw_loss 0.181259 lr 0.00038226 rank 1
2023-02-24 22:03:12,279 DEBUG TRAIN Batch 20/4300 loss 20.137280 loss_att 30.488319 loss_ctc 32.184299 loss_rnnt 16.359688 hw_loss 0.189593 lr 0.00038224 rank 7
2023-02-24 22:03:12,286 DEBUG TRAIN Batch 20/4300 loss 8.043207 loss_att 10.502578 loss_ctc 9.157595 loss_rnnt 7.230422 hw_loss 0.323109 lr 0.00038228 rank 4
2023-02-24 22:03:12,286 DEBUG TRAIN Batch 20/4300 loss 10.444403 loss_att 13.694962 loss_ctc 11.425859 loss_rnnt 9.532348 hw_loss 0.245780 lr 0.00038224 rank 6
2023-02-24 22:03:12,287 DEBUG TRAIN Batch 20/4300 loss 12.632704 loss_att 15.166290 loss_ctc 16.806963 loss_rnnt 11.464140 hw_loss 0.197399 lr 0.00038225 rank 3
2023-02-24 22:03:12,288 DEBUG TRAIN Batch 20/4300 loss 10.232885 loss_att 12.541677 loss_ctc 14.342186 loss_rnnt 9.114458 hw_loss 0.203928 lr 0.00038227 rank 5
2023-02-24 22:03:12,288 DEBUG TRAIN Batch 20/4300 loss 9.062725 loss_att 11.474244 loss_ctc 13.246866 loss_rnnt 7.867112 hw_loss 0.291419 lr 0.00038227 rank 2
2023-02-24 22:04:24,948 DEBUG TRAIN Batch 20/4400 loss 9.633939 loss_att 12.379348 loss_ctc 13.840095 loss_rnnt 8.399050 hw_loss 0.234348 lr 0.00038214 rank 3
2023-02-24 22:04:24,949 DEBUG TRAIN Batch 20/4400 loss 8.694368 loss_att 10.592102 loss_ctc 9.857089 loss_rnnt 8.035194 hw_loss 0.233621 lr 0.00038216 rank 2
2023-02-24 22:04:24,952 DEBUG TRAIN Batch 20/4400 loss 4.241594 loss_att 7.899516 loss_ctc 7.004663 loss_rnnt 2.995713 hw_loss 0.273540 lr 0.00038215 rank 1
2023-02-24 22:04:24,952 DEBUG TRAIN Batch 20/4400 loss 4.028005 loss_att 6.090280 loss_ctc 5.066975 loss_rnnt 3.395485 hw_loss 0.152877 lr 0.00038213 rank 7
2023-02-24 22:04:24,954 DEBUG TRAIN Batch 20/4400 loss 12.333825 loss_att 11.923021 loss_ctc 15.866869 loss_rnnt 11.823792 hw_loss 0.227102 lr 0.00038216 rank 0
2023-02-24 22:04:24,955 DEBUG TRAIN Batch 20/4400 loss 11.778279 loss_att 13.378336 loss_ctc 16.775295 loss_rnnt 10.669227 hw_loss 0.230201 lr 0.00038213 rank 6
2023-02-24 22:04:24,960 DEBUG TRAIN Batch 20/4400 loss 18.910854 loss_att 21.214241 loss_ctc 22.216940 loss_rnnt 17.929504 hw_loss 0.149743 lr 0.00038217 rank 4
2023-02-24 22:04:24,961 DEBUG TRAIN Batch 20/4400 loss 21.056419 loss_att 22.250362 loss_ctc 26.359665 loss_rnnt 19.990198 hw_loss 0.225622 lr 0.00038216 rank 5
2023-02-24 22:05:38,002 DEBUG TRAIN Batch 20/4500 loss 9.270988 loss_att 12.583898 loss_ctc 10.384101 loss_rnnt 8.302588 hw_loss 0.295131 lr 0.00038202 rank 6
2023-02-24 22:05:38,003 DEBUG TRAIN Batch 20/4500 loss 2.886613 loss_att 4.305746 loss_ctc 5.330966 loss_rnnt 2.171465 hw_loss 0.197639 lr 0.00038205 rank 0
2023-02-24 22:05:38,006 DEBUG TRAIN Batch 20/4500 loss 8.139532 loss_att 9.365031 loss_ctc 8.592121 loss_rnnt 7.721071 hw_loss 0.211905 lr 0.00038205 rank 2
2023-02-24 22:05:38,007 DEBUG TRAIN Batch 20/4500 loss 1.643449 loss_att 5.305445 loss_ctc 1.848013 loss_rnnt 0.740585 hw_loss 0.268480 lr 0.00038204 rank 1
2023-02-24 22:05:38,008 DEBUG TRAIN Batch 20/4500 loss 3.837630 loss_att 5.452050 loss_ctc 5.531941 loss_rnnt 3.122729 hw_loss 0.311455 lr 0.00038203 rank 3
2023-02-24 22:05:38,011 DEBUG TRAIN Batch 20/4500 loss 17.169367 loss_att 21.060665 loss_ctc 26.943806 loss_rnnt 14.990244 hw_loss 0.183004 lr 0.00038202 rank 7
2023-02-24 22:05:38,013 DEBUG TRAIN Batch 20/4500 loss 9.181745 loss_att 12.790762 loss_ctc 15.083476 loss_rnnt 7.587257 hw_loss 0.160849 lr 0.00038206 rank 4
2023-02-24 22:05:38,015 DEBUG TRAIN Batch 20/4500 loss 10.747842 loss_att 13.888772 loss_ctc 15.238398 loss_rnnt 9.416153 hw_loss 0.196427 lr 0.00038205 rank 5
2023-02-24 22:06:52,919 DEBUG TRAIN Batch 20/4600 loss 17.789858 loss_att 21.048174 loss_ctc 21.606695 loss_rnnt 16.516836 hw_loss 0.210839 lr 0.00038193 rank 1
2023-02-24 22:06:52,920 DEBUG TRAIN Batch 20/4600 loss 9.427951 loss_att 15.461025 loss_ctc 15.723061 loss_rnnt 7.286644 hw_loss 0.178768 lr 0.00038194 rank 0
2023-02-24 22:06:52,921 DEBUG TRAIN Batch 20/4600 loss 4.641437 loss_att 8.197168 loss_ctc 7.395643 loss_rnnt 3.448551 hw_loss 0.214709 lr 0.00038194 rank 5
2023-02-24 22:06:52,923 DEBUG TRAIN Batch 20/4600 loss 7.833095 loss_att 9.597226 loss_ctc 11.181633 loss_rnnt 6.944324 hw_loss 0.167764 lr 0.00038195 rank 4
2023-02-24 22:06:52,924 DEBUG TRAIN Batch 20/4600 loss 13.092449 loss_att 19.072762 loss_ctc 21.935308 loss_rnnt 10.576611 hw_loss 0.263864 lr 0.00038190 rank 6
2023-02-24 22:06:52,925 DEBUG TRAIN Batch 20/4600 loss 6.356216 loss_att 9.274926 loss_ctc 9.983866 loss_rnnt 5.199435 hw_loss 0.167536 lr 0.00038192 rank 3
2023-02-24 22:06:52,927 DEBUG TRAIN Batch 20/4600 loss 4.480472 loss_att 11.436940 loss_ctc 5.943920 loss_rnnt 2.779979 hw_loss 0.213885 lr 0.00038193 rank 2
2023-02-24 22:06:52,929 DEBUG TRAIN Batch 20/4600 loss 5.636985 loss_att 10.928135 loss_ctc 14.590841 loss_rnnt 3.282620 hw_loss 0.191788 lr 0.00038191 rank 7
2023-02-24 22:08:05,710 DEBUG TRAIN Batch 20/4700 loss 15.066220 loss_att 16.804363 loss_ctc 19.258877 loss_rnnt 14.065830 hw_loss 0.175762 lr 0.00038179 rank 6
2023-02-24 22:08:05,710 DEBUG TRAIN Batch 20/4700 loss 4.448236 loss_att 6.603218 loss_ctc 3.860068 loss_rnnt 4.014204 hw_loss 0.152736 lr 0.00038182 rank 1
2023-02-24 22:08:05,713 DEBUG TRAIN Batch 20/4700 loss 7.005539 loss_att 8.951627 loss_ctc 10.736018 loss_rnnt 6.016967 hw_loss 0.191169 lr 0.00038180 rank 3
2023-02-24 22:08:05,715 DEBUG TRAIN Batch 20/4700 loss 8.954792 loss_att 11.759953 loss_ctc 13.030439 loss_rnnt 7.747314 hw_loss 0.193175 lr 0.00038183 rank 0
2023-02-24 22:08:05,717 DEBUG TRAIN Batch 20/4700 loss 6.483670 loss_att 9.186919 loss_ctc 9.686333 loss_rnnt 5.415634 hw_loss 0.188184 lr 0.00038179 rank 7
2023-02-24 22:08:05,719 DEBUG TRAIN Batch 20/4700 loss 5.058691 loss_att 8.885300 loss_ctc 8.109226 loss_rnnt 3.766966 hw_loss 0.224370 lr 0.00038184 rank 4
2023-02-24 22:08:05,720 DEBUG TRAIN Batch 20/4700 loss 6.414985 loss_att 8.334220 loss_ctc 10.951320 loss_rnnt 5.330787 hw_loss 0.179074 lr 0.00038182 rank 2
2023-02-24 22:08:05,768 DEBUG TRAIN Batch 20/4700 loss 10.713980 loss_att 12.574409 loss_ctc 14.329712 loss_rnnt 9.758906 hw_loss 0.189169 lr 0.00038182 rank 5
2023-02-24 22:09:18,105 DEBUG TRAIN Batch 20/4800 loss 4.816940 loss_att 7.952090 loss_ctc 8.143433 loss_rnnt 3.566147 hw_loss 0.337932 lr 0.00038168 rank 6
2023-02-24 22:09:18,106 DEBUG TRAIN Batch 20/4800 loss 16.979740 loss_att 20.330620 loss_ctc 23.773346 loss_rnnt 15.303816 hw_loss 0.187377 lr 0.00038172 rank 0
2023-02-24 22:09:18,107 DEBUG TRAIN Batch 20/4800 loss 6.523894 loss_att 11.328037 loss_ctc 9.578171 loss_rnnt 5.059176 hw_loss 0.181224 lr 0.00038170 rank 1
2023-02-24 22:09:18,110 DEBUG TRAIN Batch 20/4800 loss 15.275962 loss_att 18.689259 loss_ctc 18.425192 loss_rnnt 14.102518 hw_loss 0.132914 lr 0.00038171 rank 2
2023-02-24 22:09:18,112 DEBUG TRAIN Batch 20/4800 loss 11.940499 loss_att 15.906044 loss_ctc 18.351307 loss_rnnt 10.160844 hw_loss 0.247070 lr 0.00038168 rank 7
2023-02-24 22:09:18,116 DEBUG TRAIN Batch 20/4800 loss 6.564410 loss_att 9.288696 loss_ctc 9.533211 loss_rnnt 5.516883 hw_loss 0.200307 lr 0.00038169 rank 3
2023-02-24 22:09:18,117 DEBUG TRAIN Batch 20/4800 loss 10.342201 loss_att 15.083721 loss_ctc 16.880905 loss_rnnt 8.370403 hw_loss 0.284377 lr 0.00038173 rank 4
2023-02-24 22:09:18,119 DEBUG TRAIN Batch 20/4800 loss 9.370429 loss_att 10.954007 loss_ctc 13.023773 loss_rnnt 8.459469 hw_loss 0.200874 lr 0.00038171 rank 5
2023-02-24 22:10:31,264 DEBUG TRAIN Batch 20/4900 loss 5.313188 loss_att 7.431499 loss_ctc 6.236498 loss_rnnt 4.603456 hw_loss 0.305552 lr 0.00038160 rank 5
2023-02-24 22:10:31,274 DEBUG TRAIN Batch 20/4900 loss 9.753128 loss_att 10.918572 loss_ctc 14.128043 loss_rnnt 8.819186 hw_loss 0.220371 lr 0.00038159 rank 1
2023-02-24 22:10:31,276 DEBUG TRAIN Batch 20/4900 loss 8.031067 loss_att 9.818993 loss_ctc 11.246652 loss_rnnt 7.120161 hw_loss 0.233580 lr 0.00038162 rank 4
2023-02-24 22:10:31,283 DEBUG TRAIN Batch 20/4900 loss 3.335703 loss_att 6.554962 loss_ctc 7.030591 loss_rnnt 2.014603 hw_loss 0.346119 lr 0.00038160 rank 2
2023-02-24 22:10:31,286 DEBUG TRAIN Batch 20/4900 loss 9.927862 loss_att 13.107107 loss_ctc 12.923524 loss_rnnt 8.806091 hw_loss 0.162186 lr 0.00038157 rank 7
2023-02-24 22:10:31,290 DEBUG TRAIN Batch 20/4900 loss 6.428826 loss_att 7.793317 loss_ctc 6.706129 loss_rnnt 6.020391 hw_loss 0.184806 lr 0.00038161 rank 0
2023-02-24 22:10:31,296 DEBUG TRAIN Batch 20/4900 loss 17.167511 loss_att 21.245377 loss_ctc 23.099796 loss_rnnt 15.489089 hw_loss 0.134772 lr 0.00038158 rank 3
2023-02-24 22:10:31,315 DEBUG TRAIN Batch 20/4900 loss 9.115819 loss_att 11.043534 loss_ctc 12.571630 loss_rnnt 8.133080 hw_loss 0.255790 lr 0.00038157 rank 6
2023-02-24 22:11:46,491 DEBUG TRAIN Batch 20/5000 loss 4.961210 loss_att 10.994354 loss_ctc 8.641232 loss_rnnt 3.178355 hw_loss 0.160417 lr 0.00038148 rank 1
2023-02-24 22:11:46,494 DEBUG TRAIN Batch 20/5000 loss 6.069570 loss_att 8.547389 loss_ctc 9.013292 loss_rnnt 5.020382 hw_loss 0.302115 lr 0.00038149 rank 0
2023-02-24 22:11:46,494 DEBUG TRAIN Batch 20/5000 loss 9.394395 loss_att 13.673471 loss_ctc 12.519197 loss_rnnt 7.979414 hw_loss 0.267237 lr 0.00038147 rank 3
2023-02-24 22:11:46,497 DEBUG TRAIN Batch 20/5000 loss 10.620763 loss_att 14.635751 loss_ctc 14.611104 loss_rnnt 9.163924 hw_loss 0.228368 lr 0.00038149 rank 2
2023-02-24 22:11:46,497 DEBUG TRAIN Batch 20/5000 loss 12.327595 loss_att 15.034719 loss_ctc 20.300028 loss_rnnt 10.566031 hw_loss 0.294651 lr 0.00038146 rank 7
2023-02-24 22:11:46,497 DEBUG TRAIN Batch 20/5000 loss 8.794029 loss_att 11.219541 loss_ctc 7.798099 loss_rnnt 8.384828 hw_loss 0.106669 lr 0.00038146 rank 6
2023-02-24 22:11:46,498 DEBUG TRAIN Batch 20/5000 loss 11.690486 loss_att 13.335148 loss_ctc 16.373499 loss_rnnt 10.564390 hw_loss 0.323927 lr 0.00038150 rank 4
2023-02-24 22:11:46,497 DEBUG TRAIN Batch 20/5000 loss 12.295987 loss_att 12.484510 loss_ctc 15.725681 loss_rnnt 11.657619 hw_loss 0.268822 lr 0.00038149 rank 5
2023-02-24 22:12:58,907 DEBUG TRAIN Batch 20/5100 loss 10.267815 loss_att 12.279914 loss_ctc 14.011156 loss_rnnt 9.245346 hw_loss 0.226756 lr 0.00038136 rank 3
2023-02-24 22:12:58,909 DEBUG TRAIN Batch 20/5100 loss 5.430133 loss_att 7.535989 loss_ctc 6.442775 loss_rnnt 4.771613 hw_loss 0.191870 lr 0.00038138 rank 0
2023-02-24 22:12:58,913 DEBUG TRAIN Batch 20/5100 loss 10.222685 loss_att 11.342983 loss_ctc 12.243984 loss_rnnt 9.607494 hw_loss 0.228044 lr 0.00038137 rank 1
2023-02-24 22:12:58,917 DEBUG TRAIN Batch 20/5100 loss 13.014281 loss_att 12.070854 loss_ctc 18.288034 loss_rnnt 12.364910 hw_loss 0.252920 lr 0.00038138 rank 2
2023-02-24 22:12:58,916 DEBUG TRAIN Batch 20/5100 loss 7.743719 loss_att 12.191096 loss_ctc 11.494023 loss_rnnt 6.248572 hw_loss 0.198055 lr 0.00038135 rank 7
2023-02-24 22:12:58,916 DEBUG TRAIN Batch 20/5100 loss 15.494166 loss_att 16.325823 loss_ctc 17.372034 loss_rnnt 14.976228 hw_loss 0.189796 lr 0.00038135 rank 6
2023-02-24 22:12:58,917 DEBUG TRAIN Batch 20/5100 loss 8.395703 loss_att 13.189388 loss_ctc 9.186007 loss_rnnt 7.208358 hw_loss 0.231066 lr 0.00038139 rank 4
2023-02-24 22:12:58,917 DEBUG TRAIN Batch 20/5100 loss 7.433974 loss_att 11.474661 loss_ctc 11.016426 loss_rnnt 6.072928 hw_loss 0.141090 lr 0.00038138 rank 5
2023-02-24 22:14:11,415 DEBUG TRAIN Batch 20/5200 loss 5.522755 loss_att 8.536442 loss_ctc 7.935487 loss_rnnt 4.554173 hw_loss 0.082774 lr 0.00038128 rank 4
2023-02-24 22:14:11,415 DEBUG TRAIN Batch 20/5200 loss 8.258256 loss_att 11.188498 loss_ctc 7.487932 loss_rnnt 7.626247 hw_loss 0.278756 lr 0.00038127 rank 0
2023-02-24 22:14:11,415 DEBUG TRAIN Batch 20/5200 loss 8.936468 loss_att 9.878663 loss_ctc 10.989461 loss_rnnt 8.296468 hw_loss 0.333430 lr 0.00038124 rank 7
2023-02-24 22:14:11,417 DEBUG TRAIN Batch 20/5200 loss 17.744587 loss_att 19.587500 loss_ctc 24.671116 loss_rnnt 16.335403 hw_loss 0.219494 lr 0.00038126 rank 1
2023-02-24 22:14:11,422 DEBUG TRAIN Batch 20/5200 loss 6.835607 loss_att 7.928423 loss_ctc 11.123089 loss_rnnt 5.916838 hw_loss 0.241012 lr 0.00038124 rank 6
2023-02-24 22:14:11,424 DEBUG TRAIN Batch 20/5200 loss 7.929761 loss_att 13.439974 loss_ctc 15.384295 loss_rnnt 5.699688 hw_loss 0.251424 lr 0.00038127 rank 2
2023-02-24 22:14:11,424 DEBUG TRAIN Batch 20/5200 loss 15.780519 loss_att 17.420506 loss_ctc 24.705046 loss_rnnt 14.196455 hw_loss 0.123995 lr 0.00038127 rank 5
2023-02-24 22:14:11,469 DEBUG TRAIN Batch 20/5200 loss 9.643113 loss_att 8.820652 loss_ctc 12.448245 loss_rnnt 9.296895 hw_loss 0.256297 lr 0.00038125 rank 3
2023-02-24 22:15:25,451 DEBUG TRAIN Batch 20/5300 loss 9.787074 loss_att 15.646441 loss_ctc 15.690281 loss_rnnt 7.674678 hw_loss 0.287678 lr 0.00038113 rank 6
2023-02-24 22:15:25,452 DEBUG TRAIN Batch 20/5300 loss 8.348471 loss_att 11.722926 loss_ctc 11.987136 loss_rnnt 7.025573 hw_loss 0.305347 lr 0.00038117 rank 4
2023-02-24 22:15:25,457 DEBUG TRAIN Batch 20/5300 loss 7.420010 loss_att 8.997032 loss_ctc 10.268372 loss_rnnt 6.579004 hw_loss 0.273410 lr 0.00038116 rank 5
2023-02-24 22:15:25,457 DEBUG TRAIN Batch 20/5300 loss 10.166190 loss_att 14.335357 loss_ctc 13.031902 loss_rnnt 8.842795 hw_loss 0.201500 lr 0.00038116 rank 0
2023-02-24 22:15:25,459 DEBUG TRAIN Batch 20/5300 loss 20.423752 loss_att 25.801466 loss_ctc 26.467926 loss_rnnt 18.412020 hw_loss 0.244309 lr 0.00038113 rank 7
2023-02-24 22:15:25,473 DEBUG TRAIN Batch 20/5300 loss 14.011260 loss_att 20.140240 loss_ctc 21.151432 loss_rnnt 11.733226 hw_loss 0.187905 lr 0.00038116 rank 2
2023-02-24 22:15:25,476 DEBUG TRAIN Batch 20/5300 loss 6.849627 loss_att 9.233118 loss_ctc 7.999651 loss_rnnt 6.126800 hw_loss 0.173987 lr 0.00038114 rank 3
2023-02-24 22:15:25,516 DEBUG TRAIN Batch 20/5300 loss 3.358176 loss_att 5.771605 loss_ctc 2.869047 loss_rnnt 2.849652 hw_loss 0.170727 lr 0.00038115 rank 1
2023-02-24 22:16:39,907 DEBUG TRAIN Batch 20/5400 loss 10.225931 loss_att 13.654155 loss_ctc 15.781721 loss_rnnt 8.703987 hw_loss 0.179115 lr 0.00038105 rank 0
2023-02-24 22:16:39,909 DEBUG TRAIN Batch 20/5400 loss 17.653412 loss_att 21.384796 loss_ctc 23.423008 loss_rnnt 16.026321 hw_loss 0.209127 lr 0.00038105 rank 2
2023-02-24 22:16:39,908 DEBUG TRAIN Batch 20/5400 loss 9.010089 loss_att 12.282677 loss_ctc 12.035219 loss_rnnt 7.878692 hw_loss 0.137865 lr 0.00038102 rank 7
2023-02-24 22:16:39,909 DEBUG TRAIN Batch 20/5400 loss 6.012883 loss_att 9.739684 loss_ctc 9.970015 loss_rnnt 4.618840 hw_loss 0.226998 lr 0.00038103 rank 3
2023-02-24 22:16:39,910 DEBUG TRAIN Batch 20/5400 loss 3.699644 loss_att 6.754189 loss_ctc 4.769184 loss_rnnt 2.874967 hw_loss 0.133431 lr 0.00038102 rank 6
2023-02-24 22:16:39,913 DEBUG TRAIN Batch 20/5400 loss 14.426448 loss_att 15.127913 loss_ctc 17.935110 loss_rnnt 13.720246 hw_loss 0.183911 lr 0.00038105 rank 5
2023-02-24 22:16:39,916 DEBUG TRAIN Batch 20/5400 loss 6.946392 loss_att 9.698343 loss_ctc 8.181616 loss_rnnt 6.097511 hw_loss 0.250863 lr 0.00038104 rank 1
2023-02-24 22:16:39,917 DEBUG TRAIN Batch 20/5400 loss 9.324231 loss_att 12.439104 loss_ctc 14.623988 loss_rnnt 7.882568 hw_loss 0.210101 lr 0.00038106 rank 4
2023-02-24 22:17:52,437 DEBUG TRAIN Batch 20/5500 loss 19.944412 loss_att 21.257690 loss_ctc 25.489021 loss_rnnt 18.804033 hw_loss 0.259577 lr 0.00038092 rank 3
2023-02-24 22:17:52,437 DEBUG TRAIN Batch 20/5500 loss 19.852930 loss_att 20.373949 loss_ctc 27.184851 loss_rnnt 18.624166 hw_loss 0.275569 lr 0.00038093 rank 1
2023-02-24 22:17:52,440 DEBUG TRAIN Batch 20/5500 loss 11.938864 loss_att 14.398549 loss_ctc 17.317680 loss_rnnt 10.615288 hw_loss 0.214619 lr 0.00038094 rank 0
2023-02-24 22:17:52,441 DEBUG TRAIN Batch 20/5500 loss 11.847299 loss_att 15.829897 loss_ctc 16.002897 loss_rnnt 10.376660 hw_loss 0.225075 lr 0.00038095 rank 4
2023-02-24 22:17:52,442 DEBUG TRAIN Batch 20/5500 loss 17.062462 loss_att 18.745659 loss_ctc 22.556087 loss_rnnt 15.939977 hw_loss 0.100053 lr 0.00038091 rank 7
2023-02-24 22:17:52,442 DEBUG TRAIN Batch 20/5500 loss 6.127356 loss_att 9.908896 loss_ctc 10.433275 loss_rnnt 4.685010 hw_loss 0.209839 lr 0.00038091 rank 6
2023-02-24 22:17:52,444 DEBUG TRAIN Batch 20/5500 loss 20.075092 loss_att 20.872852 loss_ctc 29.193399 loss_rnnt 18.611256 hw_loss 0.165958 lr 0.00038094 rank 2
2023-02-24 22:17:52,449 DEBUG TRAIN Batch 20/5500 loss 9.373848 loss_att 9.721764 loss_ctc 11.991447 loss_rnnt 8.781425 hw_loss 0.325925 lr 0.00038094 rank 5
2023-02-24 23:05:30,245 DEBUG TRAIN Batch 20/5600 loss 8.412287 loss_att 9.987916 loss_ctc 13.039447 loss_rnnt 7.378697 hw_loss 0.190329 lr 0.00038080 rank 7
2023-02-24 23:05:30,244 DEBUG TRAIN Batch 20/5600 loss 9.229177 loss_att 10.972244 loss_ctc 14.787970 loss_rnnt 8.015619 hw_loss 0.232075 lr 0.00038083 rank 0
2023-02-24 23:05:30,246 DEBUG TRAIN Batch 20/5600 loss 10.464846 loss_att 11.713453 loss_ctc 15.732227 loss_rnnt 9.407618 hw_loss 0.197229 lr 0.00038082 rank 1
2023-02-24 23:05:30,247 DEBUG TRAIN Batch 20/5600 loss 11.341972 loss_att 14.901726 loss_ctc 17.644115 loss_rnnt 9.667387 hw_loss 0.229403 lr 0.00038081 rank 3
2023-02-24 23:05:30,247 DEBUG TRAIN Batch 20/5600 loss 9.223991 loss_att 10.164731 loss_ctc 13.912758 loss_rnnt 8.266307 hw_loss 0.270690 lr 0.00038083 rank 5
2023-02-24 23:05:30,249 DEBUG TRAIN Batch 20/5600 loss 4.443798 loss_att 7.614055 loss_ctc 7.070533 loss_rnnt 3.350335 hw_loss 0.204713 lr 0.00038082 rank 2
2023-02-24 23:05:30,248 DEBUG TRAIN Batch 20/5600 loss 5.549779 loss_att 11.033508 loss_ctc 8.697033 loss_rnnt 3.924473 hw_loss 0.204236 lr 0.00038080 rank 6
2023-02-24 23:05:30,249 DEBUG TRAIN Batch 20/5600 loss 11.538515 loss_att 14.899284 loss_ctc 17.021229 loss_rnnt 10.022263 hw_loss 0.212006 lr 0.00038084 rank 4
2023-02-24 23:06:46,148 DEBUG TRAIN Batch 20/5700 loss 11.531006 loss_att 14.718730 loss_ctc 19.872772 loss_rnnt 9.621990 hw_loss 0.298567 lr 0.00038068 rank 6
2023-02-24 23:06:46,150 DEBUG TRAIN Batch 20/5700 loss 4.692585 loss_att 6.214278 loss_ctc 4.958553 loss_rnnt 4.261133 hw_loss 0.171845 lr 0.00038070 rank 3
2023-02-24 23:06:46,151 DEBUG TRAIN Batch 20/5700 loss 10.987493 loss_att 10.131893 loss_ctc 14.730194 loss_rnnt 10.520096 hw_loss 0.261544 lr 0.00038071 rank 2
2023-02-24 23:06:46,152 DEBUG TRAIN Batch 20/5700 loss 10.842866 loss_att 10.414726 loss_ctc 17.206129 loss_rnnt 9.942455 hw_loss 0.258007 lr 0.00038073 rank 4
2023-02-24 23:06:46,154 DEBUG TRAIN Batch 20/5700 loss 10.859975 loss_att 13.927822 loss_ctc 15.242045 loss_rnnt 9.604671 hw_loss 0.107733 lr 0.00038072 rank 5
2023-02-24 23:06:46,154 DEBUG TRAIN Batch 20/5700 loss 3.730147 loss_att 6.147608 loss_ctc 4.737610 loss_rnnt 3.001468 hw_loss 0.207858 lr 0.00038072 rank 0
2023-02-24 23:06:46,156 DEBUG TRAIN Batch 20/5700 loss 17.324074 loss_att 19.176914 loss_ctc 23.460091 loss_rnnt 16.086569 hw_loss 0.091506 lr 0.00038071 rank 1
2023-02-24 23:06:46,159 DEBUG TRAIN Batch 20/5700 loss 7.534773 loss_att 10.829213 loss_ctc 14.360939 loss_rnnt 5.859468 hw_loss 0.199240 lr 0.00038069 rank 7
2023-02-24 23:07:58,342 DEBUG TRAIN Batch 20/5800 loss 12.127494 loss_att 16.547054 loss_ctc 19.697426 loss_rnnt 10.140882 hw_loss 0.175077 lr 0.00038061 rank 0
2023-02-24 23:07:58,343 DEBUG TRAIN Batch 20/5800 loss 16.614382 loss_att 22.525719 loss_ctc 26.191078 loss_rnnt 14.038852 hw_loss 0.218192 lr 0.00038060 rank 1
2023-02-24 23:07:58,343 DEBUG TRAIN Batch 20/5800 loss 10.861833 loss_att 12.865538 loss_ctc 15.405681 loss_rnnt 9.737140 hw_loss 0.221448 lr 0.00038062 rank 4
2023-02-24 23:07:58,344 DEBUG TRAIN Batch 20/5800 loss 8.196726 loss_att 9.666515 loss_ctc 10.435886 loss_rnnt 7.489660 hw_loss 0.214787 lr 0.00038057 rank 6
2023-02-24 23:07:58,346 DEBUG TRAIN Batch 20/5800 loss 10.152610 loss_att 11.521954 loss_ctc 12.260390 loss_rnnt 9.544160 hw_loss 0.100394 lr 0.00038060 rank 2
2023-02-24 23:07:58,350 DEBUG TRAIN Batch 20/5800 loss 14.702864 loss_att 18.040426 loss_ctc 22.292515 loss_rnnt 12.898236 hw_loss 0.234676 lr 0.00038059 rank 3
2023-02-24 23:07:58,352 DEBUG TRAIN Batch 20/5800 loss 17.790745 loss_att 16.212498 loss_ctc 23.408810 loss_rnnt 17.234213 hw_loss 0.230825 lr 0.00038058 rank 7
2023-02-24 23:07:58,356 DEBUG TRAIN Batch 20/5800 loss 11.820162 loss_att 13.288437 loss_ctc 14.807121 loss_rnnt 11.023906 hw_loss 0.195637 lr 0.00038061 rank 5
2023-02-24 23:09:11,074 DEBUG TRAIN Batch 20/5900 loss 10.809682 loss_att 17.376953 loss_ctc 16.471251 loss_rnnt 8.666418 hw_loss 0.140504 lr 0.00038047 rank 7
2023-02-24 23:09:11,090 DEBUG TRAIN Batch 20/5900 loss 9.022071 loss_att 12.272946 loss_ctc 14.119802 loss_rnnt 7.556252 hw_loss 0.254899 lr 0.00038050 rank 0
2023-02-24 23:09:11,094 DEBUG TRAIN Batch 20/5900 loss 8.616996 loss_att 11.353158 loss_ctc 10.517344 loss_rnnt 7.762749 hw_loss 0.100566 lr 0.00038051 rank 4
2023-02-24 23:09:11,097 DEBUG TRAIN Batch 20/5900 loss 11.198692 loss_att 14.101908 loss_ctc 16.286087 loss_rnnt 9.817459 hw_loss 0.229260 lr 0.00038049 rank 2
2023-02-24 23:09:11,101 DEBUG TRAIN Batch 20/5900 loss 7.691440 loss_att 11.796326 loss_ctc 9.452584 loss_rnnt 6.545089 hw_loss 0.169790 lr 0.00038048 rank 3
2023-02-24 23:09:11,101 DEBUG TRAIN Batch 20/5900 loss 6.734956 loss_att 10.300402 loss_ctc 9.901964 loss_rnnt 5.484087 hw_loss 0.216587 lr 0.00038046 rank 6
2023-02-24 23:09:11,103 DEBUG TRAIN Batch 20/5900 loss 7.530101 loss_att 10.242649 loss_ctc 8.455161 loss_rnnt 6.707189 hw_loss 0.294489 lr 0.00038050 rank 5
2023-02-24 23:09:11,143 DEBUG TRAIN Batch 20/5900 loss 16.156942 loss_att 19.251690 loss_ctc 23.151150 loss_rnnt 14.483048 hw_loss 0.229470 lr 0.00038049 rank 1
2023-02-24 23:10:24,811 DEBUG TRAIN Batch 20/6000 loss 16.715002 loss_att 21.988590 loss_ctc 17.672943 loss_rnnt 15.433346 hw_loss 0.186025 lr 0.00038035 rank 6
2023-02-24 23:10:24,812 DEBUG TRAIN Batch 20/6000 loss 8.139244 loss_att 9.227715 loss_ctc 9.541553 loss_rnnt 7.634447 hw_loss 0.187739 lr 0.00038037 rank 3
2023-02-24 23:10:24,821 DEBUG TRAIN Batch 20/6000 loss 12.805233 loss_att 15.750797 loss_ctc 21.782345 loss_rnnt 10.935115 hw_loss 0.157609 lr 0.00038038 rank 1
2023-02-24 23:10:24,825 DEBUG TRAIN Batch 20/6000 loss 17.785490 loss_att 21.763439 loss_ctc 26.981344 loss_rnnt 15.671150 hw_loss 0.173691 lr 0.00038038 rank 5
2023-02-24 23:10:24,827 DEBUG TRAIN Batch 20/6000 loss 5.550513 loss_att 7.476842 loss_ctc 8.818036 loss_rnnt 4.692024 hw_loss 0.070413 lr 0.00038039 rank 0
2023-02-24 23:10:24,828 DEBUG TRAIN Batch 20/6000 loss 9.947501 loss_att 12.750643 loss_ctc 15.298319 loss_rnnt 8.528138 hw_loss 0.272421 lr 0.00038040 rank 4
2023-02-24 23:10:24,834 DEBUG TRAIN Batch 20/6000 loss 15.781764 loss_att 18.082722 loss_ctc 23.684460 loss_rnnt 14.170367 hw_loss 0.182836 lr 0.00038036 rank 7
2023-02-24 23:10:24,849 DEBUG TRAIN Batch 20/6000 loss 7.677610 loss_att 11.026436 loss_ctc 11.665254 loss_rnnt 6.382423 hw_loss 0.175754 lr 0.00038038 rank 2
2023-02-24 23:11:38,947 DEBUG TRAIN Batch 20/6100 loss 5.708385 loss_att 6.783792 loss_ctc 8.252582 loss_rnnt 5.065559 hw_loss 0.165973 lr 0.00038027 rank 1
2023-02-24 23:11:38,954 DEBUG TRAIN Batch 20/6100 loss 6.171651 loss_att 7.021908 loss_ctc 6.693370 loss_rnnt 5.823407 hw_loss 0.203681 lr 0.00038026 rank 3
2023-02-24 23:11:38,954 DEBUG TRAIN Batch 20/6100 loss 5.036035 loss_att 6.268655 loss_ctc 8.711916 loss_rnnt 4.177176 hw_loss 0.229159 lr 0.00038024 rank 6
2023-02-24 23:11:38,958 DEBUG TRAIN Batch 20/6100 loss 23.617428 loss_att 26.814669 loss_ctc 28.412069 loss_rnnt 22.141993 hw_loss 0.368813 lr 0.00038028 rank 0
2023-02-24 23:11:38,958 DEBUG TRAIN Batch 20/6100 loss 8.454802 loss_att 11.745381 loss_ctc 10.383542 loss_rnnt 7.446134 hw_loss 0.175098 lr 0.00038029 rank 4
2023-02-24 23:11:38,959 DEBUG TRAIN Batch 20/6100 loss 6.987144 loss_att 12.064349 loss_ctc 11.126453 loss_rnnt 5.275730 hw_loss 0.270122 lr 0.00038025 rank 7
2023-02-24 23:11:38,962 DEBUG TRAIN Batch 20/6100 loss 8.930768 loss_att 12.391028 loss_ctc 14.436572 loss_rnnt 7.398179 hw_loss 0.199554 lr 0.00038027 rank 2
2023-02-24 23:11:39,008 DEBUG TRAIN Batch 20/6100 loss 8.944552 loss_att 10.507190 loss_ctc 10.688667 loss_rnnt 8.262069 hw_loss 0.257639 lr 0.00038027 rank 5
2023-02-24 23:12:51,790 DEBUG TRAIN Batch 20/6200 loss 13.810226 loss_att 15.140906 loss_ctc 19.211285 loss_rnnt 12.676295 hw_loss 0.276851 lr 0.00038017 rank 0
2023-02-24 23:12:51,801 DEBUG TRAIN Batch 20/6200 loss 10.012748 loss_att 13.918015 loss_ctc 9.798326 loss_rnnt 9.156471 hw_loss 0.194648 lr 0.00038016 rank 2
2023-02-24 23:12:51,803 DEBUG TRAIN Batch 20/6200 loss 5.873527 loss_att 8.374183 loss_ctc 7.293565 loss_rnnt 5.059977 hw_loss 0.232650 lr 0.00038016 rank 5
2023-02-24 23:12:51,803 DEBUG TRAIN Batch 20/6200 loss 8.885144 loss_att 12.191298 loss_ctc 12.673454 loss_rnnt 7.580124 hw_loss 0.260027 lr 0.00038018 rank 4
2023-02-24 23:12:51,807 DEBUG TRAIN Batch 20/6200 loss 13.637765 loss_att 15.557032 loss_ctc 15.409441 loss_rnnt 12.946393 hw_loss 0.133680 lr 0.00038013 rank 6
2023-02-24 23:12:51,807 DEBUG TRAIN Batch 20/6200 loss 6.423617 loss_att 10.104386 loss_ctc 9.483666 loss_rnnt 5.211345 hw_loss 0.127708 lr 0.00038016 rank 1
2023-02-24 23:12:51,810 DEBUG TRAIN Batch 20/6200 loss 8.547416 loss_att 11.178431 loss_ctc 12.313072 loss_rnnt 7.423815 hw_loss 0.178706 lr 0.00038015 rank 3
2023-02-24 23:12:51,812 DEBUG TRAIN Batch 20/6200 loss 13.164157 loss_att 20.075541 loss_ctc 19.741222 loss_rnnt 10.824665 hw_loss 0.150512 lr 0.00038014 rank 7
2023-02-24 23:14:04,898 DEBUG TRAIN Batch 20/6300 loss 11.819942 loss_att 14.677271 loss_ctc 16.525148 loss_rnnt 10.492168 hw_loss 0.241776 lr 0.00038005 rank 1
2023-02-24 23:14:04,900 DEBUG TRAIN Batch 20/6300 loss 7.742747 loss_att 10.855691 loss_ctc 13.215387 loss_rnnt 6.317507 hw_loss 0.136811 lr 0.00038007 rank 4
2023-02-24 23:14:04,907 DEBUG TRAIN Batch 20/6300 loss 6.063561 loss_att 10.108997 loss_ctc 6.857533 loss_rnnt 5.011055 hw_loss 0.257917 lr 0.00038002 rank 6
2023-02-24 23:14:04,907 DEBUG TRAIN Batch 20/6300 loss 12.409198 loss_att 16.937807 loss_ctc 13.376650 loss_rnnt 11.310514 hw_loss 0.119941 lr 0.00038006 rank 0
2023-02-24 23:14:04,908 DEBUG TRAIN Batch 20/6300 loss 11.162719 loss_att 13.204889 loss_ctc 16.287437 loss_rnnt 9.965809 hw_loss 0.197211 lr 0.00038003 rank 7
2023-02-24 23:14:04,911 DEBUG TRAIN Batch 20/6300 loss 11.477341 loss_att 12.438274 loss_ctc 14.885806 loss_rnnt 10.706760 hw_loss 0.232372 lr 0.00038004 rank 3
2023-02-24 23:14:04,915 DEBUG TRAIN Batch 20/6300 loss 7.083536 loss_att 11.577683 loss_ctc 12.985057 loss_rnnt 5.248102 hw_loss 0.280751 lr 0.00038005 rank 2
2023-02-24 23:14:04,920 DEBUG TRAIN Batch 20/6300 loss 14.465280 loss_att 18.175829 loss_ctc 17.404373 loss_rnnt 13.197112 hw_loss 0.251585 lr 0.00038006 rank 5
2023-02-24 23:15:19,775 DEBUG TRAIN Batch 20/6400 loss 10.674767 loss_att 16.707554 loss_ctc 14.966848 loss_rnnt 8.855602 hw_loss 0.075620 lr 0.00037995 rank 5
2023-02-24 23:15:19,785 DEBUG TRAIN Batch 20/6400 loss 8.067819 loss_att 9.453454 loss_ctc 12.946093 loss_rnnt 6.962220 hw_loss 0.333815 lr 0.00037991 rank 6
2023-02-24 23:15:19,787 DEBUG TRAIN Batch 20/6400 loss 6.023829 loss_att 9.456174 loss_ctc 10.840377 loss_rnnt 4.624758 hw_loss 0.131992 lr 0.00037995 rank 0
2023-02-24 23:15:19,789 DEBUG TRAIN Batch 20/6400 loss 5.785560 loss_att 7.215364 loss_ctc 8.167337 loss_rnnt 5.040261 hw_loss 0.265815 lr 0.00037992 rank 7
2023-02-24 23:15:19,790 DEBUG TRAIN Batch 20/6400 loss 11.549281 loss_att 15.403176 loss_ctc 21.259773 loss_rnnt 9.355848 hw_loss 0.239854 lr 0.00037994 rank 1
2023-02-24 23:15:19,792 DEBUG TRAIN Batch 20/6400 loss 11.465822 loss_att 13.580905 loss_ctc 16.334391 loss_rnnt 10.312763 hw_loss 0.151687 lr 0.00037993 rank 3
2023-02-24 23:15:19,794 DEBUG TRAIN Batch 20/6400 loss 2.664382 loss_att 5.079676 loss_ctc 2.986862 loss_rnnt 1.990204 hw_loss 0.277729 lr 0.00037996 rank 4
2023-02-24 23:15:19,835 DEBUG TRAIN Batch 20/6400 loss 8.206326 loss_att 10.999769 loss_ctc 11.747299 loss_rnnt 7.030080 hw_loss 0.272677 lr 0.00037994 rank 2
2023-02-24 23:16:32,618 DEBUG TRAIN Batch 20/6500 loss 8.865684 loss_att 12.470908 loss_ctc 12.558722 loss_rnnt 7.556346 hw_loss 0.179790 lr 0.00037981 rank 7
2023-02-24 23:16:32,618 DEBUG TRAIN Batch 20/6500 loss 7.033861 loss_att 9.636989 loss_ctc 10.624371 loss_rnnt 5.934863 hw_loss 0.186821 lr 0.00037981 rank 6
2023-02-24 23:16:32,618 DEBUG TRAIN Batch 20/6500 loss 9.753700 loss_att 11.602453 loss_ctc 11.987728 loss_rnnt 9.001516 hw_loss 0.158555 lr 0.00037983 rank 2
2023-02-24 23:16:32,619 DEBUG TRAIN Batch 20/6500 loss 8.128754 loss_att 11.511250 loss_ctc 9.062526 loss_rnnt 7.206923 hw_loss 0.226553 lr 0.00037984 rank 0
2023-02-24 23:16:32,622 DEBUG TRAIN Batch 20/6500 loss 9.294044 loss_att 12.881559 loss_ctc 13.265483 loss_rnnt 7.918564 hw_loss 0.240848 lr 0.00037984 rank 5
2023-02-24 23:16:32,626 DEBUG TRAIN Batch 20/6500 loss 6.428223 loss_att 6.159956 loss_ctc 9.568544 loss_rnnt 5.914719 hw_loss 0.278339 lr 0.00037982 rank 3
2023-02-24 23:16:32,656 DEBUG TRAIN Batch 20/6500 loss 12.751427 loss_att 16.225576 loss_ctc 15.911561 loss_rnnt 11.515327 hw_loss 0.224847 lr 0.00037983 rank 1
2023-02-24 23:16:32,659 DEBUG TRAIN Batch 20/6500 loss 5.597881 loss_att 9.838698 loss_ctc 10.398124 loss_rnnt 3.942144 hw_loss 0.314139 lr 0.00037985 rank 4
2023-02-24 23:17:45,059 DEBUG TRAIN Batch 20/6600 loss 5.989130 loss_att 9.339874 loss_ctc 8.823521 loss_rnnt 4.815795 hw_loss 0.234874 lr 0.00037973 rank 0
2023-02-24 23:17:45,061 DEBUG TRAIN Batch 20/6600 loss 7.271369 loss_att 10.649394 loss_ctc 12.405134 loss_rnnt 5.782626 hw_loss 0.241194 lr 0.00037970 rank 7
2023-02-24 23:17:45,063 DEBUG TRAIN Batch 20/6600 loss 6.656798 loss_att 9.712236 loss_ctc 11.054705 loss_rnnt 5.337822 hw_loss 0.227814 lr 0.00037972 rank 1
2023-02-24 23:17:45,064 DEBUG TRAIN Batch 20/6600 loss 8.650946 loss_att 13.434887 loss_ctc 8.286055 loss_rnnt 7.654014 hw_loss 0.166494 lr 0.00037970 rank 6
2023-02-24 23:17:45,066 DEBUG TRAIN Batch 20/6600 loss 4.421875 loss_att 6.557217 loss_ctc 5.978142 loss_rnnt 3.678699 hw_loss 0.203636 lr 0.00037973 rank 2
2023-02-24 23:17:45,069 DEBUG TRAIN Batch 20/6600 loss 5.754198 loss_att 9.395702 loss_ctc 9.713369 loss_rnnt 4.402872 hw_loss 0.178380 lr 0.00037974 rank 4
2023-02-24 23:17:45,071 DEBUG TRAIN Batch 20/6600 loss 13.089641 loss_att 17.552738 loss_ctc 19.072361 loss_rnnt 11.279015 hw_loss 0.225581 lr 0.00037971 rank 3
2023-02-24 23:17:45,072 DEBUG TRAIN Batch 20/6600 loss 4.594937 loss_att 9.142042 loss_ctc 5.164963 loss_rnnt 3.497520 hw_loss 0.209986 lr 0.00037973 rank 5
2023-02-24 23:18:59,093 DEBUG TRAIN Batch 20/6700 loss 10.033173 loss_att 12.384729 loss_ctc 14.150880 loss_rnnt 8.904811 hw_loss 0.204418 lr 0.00037962 rank 5
2023-02-24 23:18:59,098 DEBUG TRAIN Batch 20/6700 loss 12.637529 loss_att 14.974627 loss_ctc 14.783860 loss_rnnt 11.785505 hw_loss 0.184553 lr 0.00037961 rank 1
2023-02-24 23:18:59,105 DEBUG TRAIN Batch 20/6700 loss 5.211340 loss_att 9.330447 loss_ctc 6.566398 loss_rnnt 4.107164 hw_loss 0.186902 lr 0.00037960 rank 3
2023-02-24 23:18:59,106 DEBUG TRAIN Batch 20/6700 loss 7.626377 loss_att 12.839742 loss_ctc 11.487416 loss_rnnt 5.974419 hw_loss 0.177149 lr 0.00037962 rank 2
2023-02-24 23:18:59,108 DEBUG TRAIN Batch 20/6700 loss 8.552658 loss_att 13.548400 loss_ctc 11.069885 loss_rnnt 7.155740 hw_loss 0.116511 lr 0.00037962 rank 0
2023-02-24 23:18:59,110 DEBUG TRAIN Batch 20/6700 loss 18.224718 loss_att 25.532524 loss_ctc 28.709415 loss_rnnt 15.286024 hw_loss 0.148455 lr 0.00037963 rank 4
2023-02-24 23:18:59,115 DEBUG TRAIN Batch 20/6700 loss 4.800728 loss_att 8.516911 loss_ctc 6.041621 loss_rnnt 3.844723 hw_loss 0.088716 lr 0.00037959 rank 7
2023-02-24 23:18:59,157 DEBUG TRAIN Batch 20/6700 loss 7.218360 loss_att 9.506210 loss_ctc 10.881128 loss_rnnt 6.170544 hw_loss 0.191019 lr 0.00037959 rank 6
2023-02-24 23:20:14,072 DEBUG TRAIN Batch 20/6800 loss 9.470878 loss_att 10.589264 loss_ctc 14.240015 loss_rnnt 8.493015 hw_loss 0.221813 lr 0.00037951 rank 2
2023-02-24 23:20:14,074 DEBUG TRAIN Batch 20/6800 loss 8.932535 loss_att 12.542422 loss_ctc 12.270314 loss_rnnt 7.645618 hw_loss 0.224814 lr 0.00037948 rank 6
2023-02-24 23:20:14,075 DEBUG TRAIN Batch 20/6800 loss 13.052743 loss_att 14.539787 loss_ctc 19.568094 loss_rnnt 11.735003 hw_loss 0.284283 lr 0.00037951 rank 0
2023-02-24 23:20:14,076 DEBUG TRAIN Batch 20/6800 loss 14.609982 loss_att 17.362040 loss_ctc 24.146349 loss_rnnt 12.654787 hw_loss 0.249876 lr 0.00037952 rank 4
2023-02-24 23:20:14,076 DEBUG TRAIN Batch 20/6800 loss 7.294054 loss_att 9.784690 loss_ctc 9.809420 loss_rnnt 6.372644 hw_loss 0.164815 lr 0.00037949 rank 3
2023-02-24 23:20:14,080 DEBUG TRAIN Batch 20/6800 loss 5.784290 loss_att 8.276580 loss_ctc 8.546477 loss_rnnt 4.818810 hw_loss 0.185120 lr 0.00037948 rank 7
2023-02-24 23:20:14,081 DEBUG TRAIN Batch 20/6800 loss 8.220543 loss_att 9.410314 loss_ctc 9.199297 loss_rnnt 7.706436 hw_loss 0.273100 lr 0.00037951 rank 5
2023-02-24 23:20:14,128 DEBUG TRAIN Batch 20/6800 loss 8.206783 loss_att 9.679819 loss_ctc 10.972139 loss_rnnt 7.381889 hw_loss 0.302948 lr 0.00037950 rank 1
2023-02-24 23:21:27,025 DEBUG TRAIN Batch 20/6900 loss 8.137473 loss_att 12.284789 loss_ctc 12.936604 loss_rnnt 6.563521 hw_loss 0.196136 lr 0.00037937 rank 7
2023-02-24 23:21:27,035 DEBUG TRAIN Batch 20/6900 loss 8.690856 loss_att 12.886161 loss_ctc 14.138652 loss_rnnt 7.010779 hw_loss 0.214955 lr 0.00037940 rank 0
2023-02-24 23:21:27,036 DEBUG TRAIN Batch 20/6900 loss 8.202806 loss_att 11.495451 loss_ctc 10.963555 loss_rnnt 7.070939 hw_loss 0.197323 lr 0.00037938 rank 3
2023-02-24 23:21:27,039 DEBUG TRAIN Batch 20/6900 loss 8.850740 loss_att 16.448097 loss_ctc 16.787550 loss_rnnt 6.228532 hw_loss 0.083429 lr 0.00037939 rank 1
2023-02-24 23:21:27,041 DEBUG TRAIN Batch 20/6900 loss 13.471339 loss_att 14.992888 loss_ctc 16.056452 loss_rnnt 12.682517 hw_loss 0.262182 lr 0.00037940 rank 2
2023-02-24 23:21:27,042 DEBUG TRAIN Batch 20/6900 loss 6.130668 loss_att 8.639467 loss_ctc 10.582819 loss_rnnt 4.917225 hw_loss 0.221367 lr 0.00037940 rank 5
2023-02-24 23:21:27,044 DEBUG TRAIN Batch 20/6900 loss 13.353495 loss_att 18.133778 loss_ctc 19.554707 loss_rnnt 11.474323 hw_loss 0.180536 lr 0.00037937 rank 6
2023-02-24 23:21:27,047 DEBUG TRAIN Batch 20/6900 loss 17.203165 loss_att 18.321671 loss_ctc 28.011524 loss_rnnt 15.394103 hw_loss 0.270460 lr 0.00037941 rank 4
2023-02-24 23:22:40,954 DEBUG TRAIN Batch 20/7000 loss 6.928002 loss_att 10.632417 loss_ctc 9.194929 loss_rnnt 5.764223 hw_loss 0.226200 lr 0.00037929 rank 5
2023-02-24 23:22:40,964 DEBUG TRAIN Batch 20/7000 loss 10.497866 loss_att 14.532320 loss_ctc 15.967521 loss_rnnt 8.839212 hw_loss 0.229639 lr 0.00037929 rank 0
2023-02-24 23:22:40,965 DEBUG TRAIN Batch 20/7000 loss 9.075171 loss_att 11.091180 loss_ctc 10.403685 loss_rnnt 8.433333 hw_loss 0.115316 lr 0.00037927 rank 3
2023-02-24 23:22:40,965 DEBUG TRAIN Batch 20/7000 loss 7.347078 loss_att 8.059431 loss_ctc 11.167936 loss_rnnt 6.599992 hw_loss 0.178439 lr 0.00037926 rank 7
2023-02-24 23:22:40,970 DEBUG TRAIN Batch 20/7000 loss 15.479945 loss_att 20.757381 loss_ctc 22.016556 loss_rnnt 13.478569 hw_loss 0.139389 lr 0.00037930 rank 4
2023-02-24 23:22:40,974 DEBUG TRAIN Batch 20/7000 loss 7.836473 loss_att 8.423700 loss_ctc 9.442122 loss_rnnt 7.376546 hw_loss 0.240739 lr 0.00037926 rank 6
2023-02-24 23:22:40,975 DEBUG TRAIN Batch 20/7000 loss 10.893756 loss_att 12.276620 loss_ctc 15.541312 loss_rnnt 9.877398 hw_loss 0.225210 lr 0.00037929 rank 2
2023-02-24 23:22:41,020 DEBUG TRAIN Batch 20/7000 loss 5.275231 loss_att 6.773934 loss_ctc 6.723476 loss_rnnt 4.663745 hw_loss 0.222461 lr 0.00037928 rank 1
2023-02-24 23:23:56,694 DEBUG TRAIN Batch 20/7100 loss 10.068081 loss_att 13.082965 loss_ctc 15.726513 loss_rnnt 8.551700 hw_loss 0.298024 lr 0.00037917 rank 1
2023-02-24 23:23:56,694 DEBUG TRAIN Batch 20/7100 loss 11.587394 loss_att 11.238103 loss_ctc 21.814596 loss_rnnt 10.140741 hw_loss 0.286655 lr 0.00037916 rank 3
2023-02-24 23:23:56,698 DEBUG TRAIN Batch 20/7100 loss 3.895066 loss_att 7.027251 loss_ctc 7.563570 loss_rnnt 2.631469 hw_loss 0.277550 lr 0.00037918 rank 0
2023-02-24 23:23:56,698 DEBUG TRAIN Batch 20/7100 loss 11.393594 loss_att 14.037907 loss_ctc 13.031836 loss_rnnt 10.480266 hw_loss 0.311312 lr 0.00037918 rank 2
2023-02-24 23:23:56,701 DEBUG TRAIN Batch 20/7100 loss 5.601961 loss_att 11.172788 loss_ctc 6.942949 loss_rnnt 4.243431 hw_loss 0.122936 lr 0.00037919 rank 4
2023-02-24 23:23:56,702 DEBUG TRAIN Batch 20/7100 loss 7.948657 loss_att 7.850110 loss_ctc 11.125856 loss_rnnt 7.363660 hw_loss 0.339525 lr 0.00037915 rank 7
2023-02-24 23:23:56,706 DEBUG TRAIN Batch 20/7100 loss 18.942190 loss_att 23.756271 loss_ctc 24.884876 loss_rnnt 17.099884 hw_loss 0.163374 lr 0.00037918 rank 5
2023-02-24 23:23:56,748 DEBUG TRAIN Batch 20/7100 loss 8.855098 loss_att 10.060949 loss_ctc 11.464540 loss_rnnt 8.114166 hw_loss 0.284691 lr 0.00037915 rank 6
2023-02-24 23:25:09,388 DEBUG TRAIN Batch 20/7200 loss 8.915942 loss_att 11.239756 loss_ctc 14.863281 loss_rnnt 7.587442 hw_loss 0.132672 lr 0.00037908 rank 0
2023-02-24 23:25:09,387 DEBUG TRAIN Batch 20/7200 loss 6.350916 loss_att 8.947781 loss_ctc 8.911476 loss_rnnt 5.386507 hw_loss 0.194303 lr 0.00037905 rank 3
2023-02-24 23:25:09,389 DEBUG TRAIN Batch 20/7200 loss 4.862787 loss_att 6.890583 loss_ctc 4.920827 loss_rnnt 4.291189 hw_loss 0.296812 lr 0.00037904 rank 6
2023-02-24 23:25:09,390 DEBUG TRAIN Batch 20/7200 loss 13.099668 loss_att 15.908993 loss_ctc 16.647440 loss_rnnt 11.983858 hw_loss 0.151701 lr 0.00037908 rank 4
2023-02-24 23:25:09,396 DEBUG TRAIN Batch 20/7200 loss 19.416245 loss_att 22.956003 loss_ctc 31.040119 loss_rnnt 17.071749 hw_loss 0.162550 lr 0.00037904 rank 7
2023-02-24 23:25:09,398 DEBUG TRAIN Batch 20/7200 loss 13.697609 loss_att 15.548640 loss_ctc 19.900215 loss_rnnt 12.416965 hw_loss 0.156417 lr 0.00037906 rank 1
2023-02-24 23:25:09,400 DEBUG TRAIN Batch 20/7200 loss 4.838232 loss_att 6.785657 loss_ctc 6.931999 loss_rnnt 4.049004 hw_loss 0.226076 lr 0.00037907 rank 5
2023-02-24 23:25:09,440 DEBUG TRAIN Batch 20/7200 loss 8.324425 loss_att 11.600228 loss_ctc 12.997726 loss_rnnt 6.962881 hw_loss 0.156143 lr 0.00037907 rank 2
2023-02-24 23:26:21,705 DEBUG TRAIN Batch 20/7300 loss 5.365267 loss_att 8.864850 loss_ctc 6.910617 loss_rnnt 4.325995 hw_loss 0.249954 lr 0.00037897 rank 0
2023-02-24 23:26:21,706 DEBUG TRAIN Batch 20/7300 loss 14.792295 loss_att 17.396730 loss_ctc 21.926693 loss_rnnt 13.242985 hw_loss 0.144691 lr 0.00037894 rank 3
2023-02-24 23:26:21,710 DEBUG TRAIN Batch 20/7300 loss 19.062576 loss_att 25.926083 loss_ctc 24.825157 loss_rnnt 16.795935 hw_loss 0.235496 lr 0.00037898 rank 4
2023-02-24 23:26:21,711 DEBUG TRAIN Batch 20/7300 loss 10.285078 loss_att 17.317776 loss_ctc 13.430083 loss_rnnt 8.318155 hw_loss 0.264467 lr 0.00037896 rank 2
2023-02-24 23:26:21,714 DEBUG TRAIN Batch 20/7300 loss 6.517938 loss_att 10.747046 loss_ctc 7.962379 loss_rnnt 5.417163 hw_loss 0.116929 lr 0.00037893 rank 7
2023-02-24 23:26:21,739 DEBUG TRAIN Batch 20/7300 loss 6.426712 loss_att 10.118724 loss_ctc 12.447962 loss_rnnt 4.819795 hw_loss 0.123151 lr 0.00037895 rank 1
2023-02-24 23:26:21,755 DEBUG TRAIN Batch 20/7300 loss 6.970078 loss_att 11.110468 loss_ctc 11.449736 loss_rnnt 5.411597 hw_loss 0.249592 lr 0.00037896 rank 5
2023-02-24 23:26:21,760 DEBUG TRAIN Batch 20/7300 loss 8.932894 loss_att 11.807262 loss_ctc 11.795497 loss_rnnt 7.895277 hw_loss 0.151992 lr 0.00037893 rank 6
2023-02-24 23:27:35,101 DEBUG TRAIN Batch 20/7400 loss 14.807027 loss_att 18.545105 loss_ctc 22.178722 loss_rnnt 12.969324 hw_loss 0.200988 lr 0.00037882 rank 6
2023-02-24 23:27:35,103 DEBUG TRAIN Batch 20/7400 loss 2.683587 loss_att 6.446802 loss_ctc 5.076078 loss_rnnt 1.499164 hw_loss 0.211467 lr 0.00037883 rank 3
2023-02-24 23:27:35,105 DEBUG TRAIN Batch 20/7400 loss 11.708027 loss_att 16.966030 loss_ctc 17.298830 loss_rnnt 9.849031 hw_loss 0.116164 lr 0.00037886 rank 0
2023-02-24 23:27:35,106 DEBUG TRAIN Batch 20/7400 loss 8.990584 loss_att 10.963606 loss_ctc 12.857113 loss_rnnt 7.936221 hw_loss 0.270417 lr 0.00037885 rank 2
2023-02-24 23:27:35,107 DEBUG TRAIN Batch 20/7400 loss 13.484342 loss_att 16.426218 loss_ctc 16.269253 loss_rnnt 12.430420 hw_loss 0.176671 lr 0.00037885 rank 5
2023-02-24 23:27:35,109 DEBUG TRAIN Batch 20/7400 loss 9.449623 loss_att 13.475487 loss_ctc 13.689734 loss_rnnt 7.979096 hw_loss 0.187509 lr 0.00037882 rank 7
2023-02-24 23:27:35,138 DEBUG TRAIN Batch 20/7400 loss 5.694483 loss_att 7.781306 loss_ctc 8.479785 loss_rnnt 4.739500 hw_loss 0.311709 lr 0.00037885 rank 1
2023-02-24 23:27:35,146 DEBUG TRAIN Batch 20/7400 loss 9.476789 loss_att 14.126322 loss_ctc 14.619073 loss_rnnt 7.721416 hw_loss 0.262179 lr 0.00037887 rank 4
2023-02-24 23:28:49,095 DEBUG TRAIN Batch 20/7500 loss 11.132068 loss_att 13.020876 loss_ctc 16.684664 loss_rnnt 9.913051 hw_loss 0.189205 lr 0.00037872 rank 3
2023-02-24 23:28:49,096 DEBUG TRAIN Batch 20/7500 loss 9.174574 loss_att 14.746265 loss_ctc 13.353689 loss_rnnt 7.391260 hw_loss 0.209552 lr 0.00037871 rank 6
2023-02-24 23:28:49,097 DEBUG TRAIN Batch 20/7500 loss 7.908692 loss_att 12.408580 loss_ctc 14.710686 loss_rnnt 6.028564 hw_loss 0.137283 lr 0.00037872 rank 7
2023-02-24 23:28:49,098 DEBUG TRAIN Batch 20/7500 loss 7.501192 loss_att 10.108955 loss_ctc 8.610846 loss_rnnt 6.705144 hw_loss 0.237263 lr 0.00037876 rank 4
2023-02-24 23:28:49,100 DEBUG TRAIN Batch 20/7500 loss 4.703379 loss_att 5.759782 loss_ctc 5.696696 loss_rnnt 4.253277 hw_loss 0.199461 lr 0.00037875 rank 0
2023-02-24 23:28:49,102 DEBUG TRAIN Batch 20/7500 loss 12.301476 loss_att 12.109347 loss_ctc 16.783747 loss_rnnt 11.644464 hw_loss 0.183380 lr 0.00037874 rank 2
2023-02-24 23:28:49,106 DEBUG TRAIN Batch 20/7500 loss 11.842520 loss_att 13.447081 loss_ctc 16.307341 loss_rnnt 10.847713 hw_loss 0.147347 lr 0.00037874 rank 5
2023-02-24 23:28:49,106 DEBUG TRAIN Batch 20/7500 loss 12.171834 loss_att 14.893150 loss_ctc 16.704483 loss_rnnt 10.919642 hw_loss 0.194204 lr 0.00037874 rank 1
2023-02-24 23:30:01,813 DEBUG TRAIN Batch 20/7600 loss 7.013799 loss_att 13.662203 loss_ctc 9.181854 loss_rnnt 5.244310 hw_loss 0.282626 lr 0.00037864 rank 5
2023-02-24 23:30:01,815 DEBUG TRAIN Batch 20/7600 loss 16.400930 loss_att 19.256147 loss_ctc 20.712889 loss_rnnt 15.146505 hw_loss 0.203354 lr 0.00037864 rank 0
2023-02-24 23:30:01,823 DEBUG TRAIN Batch 20/7600 loss 11.792910 loss_att 13.306577 loss_ctc 15.447880 loss_rnnt 10.894081 hw_loss 0.203934 lr 0.00037863 rank 2
2023-02-24 23:30:01,824 DEBUG TRAIN Batch 20/7600 loss 17.111313 loss_att 19.020742 loss_ctc 25.294554 loss_rnnt 15.569897 hw_loss 0.128308 lr 0.00037862 rank 3
2023-02-24 23:30:01,825 DEBUG TRAIN Batch 20/7600 loss 7.001701 loss_att 10.847486 loss_ctc 6.490692 loss_rnnt 6.148973 hw_loss 0.284449 lr 0.00037863 rank 1
2023-02-24 23:30:01,826 DEBUG TRAIN Batch 20/7600 loss 11.168675 loss_att 12.784904 loss_ctc 18.195457 loss_rnnt 9.800336 hw_loss 0.202854 lr 0.00037861 rank 7
2023-02-24 23:30:01,827 DEBUG TRAIN Batch 20/7600 loss 10.578844 loss_att 9.993635 loss_ctc 13.309507 loss_rnnt 10.209407 hw_loss 0.229483 lr 0.00037865 rank 4
2023-02-24 23:30:01,868 DEBUG TRAIN Batch 20/7600 loss 6.536335 loss_att 10.102271 loss_ctc 10.011299 loss_rnnt 5.273954 hw_loss 0.160996 lr 0.00037861 rank 6
2023-02-24 23:31:13,945 DEBUG TRAIN Batch 20/7700 loss 1.689662 loss_att 3.576634 loss_ctc 1.456121 loss_rnnt 1.240900 hw_loss 0.192199 lr 0.00037854 rank 4
2023-02-24 23:31:13,954 DEBUG TRAIN Batch 20/7700 loss 21.522350 loss_att 21.360872 loss_ctc 28.469671 loss_rnnt 20.475660 hw_loss 0.286268 lr 0.00037853 rank 0
2023-02-24 23:31:13,960 DEBUG TRAIN Batch 20/7700 loss 12.763124 loss_att 15.626351 loss_ctc 20.925558 loss_rnnt 10.965677 hw_loss 0.255891 lr 0.00037850 rank 7
2023-02-24 23:31:13,961 DEBUG TRAIN Batch 20/7700 loss 5.334216 loss_att 7.069545 loss_ctc 7.980500 loss_rnnt 4.542590 hw_loss 0.171979 lr 0.00037853 rank 2
2023-02-24 23:31:13,962 DEBUG TRAIN Batch 20/7700 loss 5.147564 loss_att 8.541254 loss_ctc 8.745052 loss_rnnt 3.887368 hw_loss 0.190861 lr 0.00037850 rank 6
2023-02-24 23:31:13,965 DEBUG TRAIN Batch 20/7700 loss 6.696425 loss_att 7.613359 loss_ctc 8.608859 loss_rnnt 6.123163 hw_loss 0.252907 lr 0.00037853 rank 5
2023-02-24 23:31:13,990 DEBUG TRAIN Batch 20/7700 loss 9.926984 loss_att 8.214846 loss_ctc 10.249245 loss_rnnt 10.082451 hw_loss 0.269987 lr 0.00037851 rank 3
2023-02-24 23:31:14,006 DEBUG TRAIN Batch 20/7700 loss 16.683357 loss_att 18.458736 loss_ctc 24.571465 loss_rnnt 15.132293 hw_loss 0.270455 lr 0.00037852 rank 1
2023-02-24 23:32:29,173 DEBUG TRAIN Batch 20/7800 loss 9.632654 loss_att 10.217526 loss_ctc 16.066782 loss_rnnt 8.564302 hw_loss 0.175300 lr 0.00037841 rank 1
2023-02-24 23:32:29,175 DEBUG TRAIN Batch 20/7800 loss 14.937575 loss_att 16.940010 loss_ctc 19.907263 loss_rnnt 13.738718 hw_loss 0.254521 lr 0.00037840 rank 3
2023-02-24 23:32:29,176 DEBUG TRAIN Batch 20/7800 loss 5.506907 loss_att 6.285391 loss_ctc 8.594527 loss_rnnt 4.828889 hw_loss 0.207445 lr 0.00037842 rank 0
2023-02-24 23:32:29,178 DEBUG TRAIN Batch 20/7800 loss 4.712894 loss_att 6.694128 loss_ctc 7.390151 loss_rnnt 3.815547 hw_loss 0.270249 lr 0.00037842 rank 2
2023-02-24 23:32:29,181 DEBUG TRAIN Batch 20/7800 loss 9.803305 loss_att 13.539497 loss_ctc 18.342091 loss_rnnt 7.805034 hw_loss 0.210988 lr 0.00037842 rank 5
2023-02-24 23:32:29,184 DEBUG TRAIN Batch 20/7800 loss 19.022133 loss_att 23.359907 loss_ctc 27.637600 loss_rnnt 16.878811 hw_loss 0.238196 lr 0.00037839 rank 7
2023-02-24 23:32:29,189 DEBUG TRAIN Batch 20/7800 loss 17.582794 loss_att 21.947880 loss_ctc 28.118027 loss_rnnt 15.154163 hw_loss 0.282962 lr 0.00037843 rank 4
2023-02-24 23:32:29,224 DEBUG TRAIN Batch 20/7800 loss 10.816280 loss_att 12.369186 loss_ctc 14.011887 loss_rnnt 9.947714 hw_loss 0.247320 lr 0.00037839 rank 6
2023-02-24 23:33:42,703 DEBUG TRAIN Batch 20/7900 loss 5.603720 loss_att 10.019880 loss_ctc 9.368551 loss_rnnt 4.129396 hw_loss 0.167087 lr 0.00037831 rank 0
2023-02-24 23:33:42,704 DEBUG TRAIN Batch 20/7900 loss 15.855830 loss_att 16.992102 loss_ctc 21.061438 loss_rnnt 14.766128 hw_loss 0.315688 lr 0.00037830 rank 1
2023-02-24 23:33:42,707 DEBUG TRAIN Batch 20/7900 loss 6.517518 loss_att 9.335457 loss_ctc 7.654773 loss_rnnt 5.689962 hw_loss 0.210627 lr 0.00037828 rank 7
2023-02-24 23:33:42,707 DEBUG TRAIN Batch 20/7900 loss 9.285189 loss_att 12.900967 loss_ctc 15.662298 loss_rnnt 7.614413 hw_loss 0.182510 lr 0.00037829 rank 3
2023-02-24 23:33:42,708 DEBUG TRAIN Batch 20/7900 loss 13.397806 loss_att 16.568762 loss_ctc 22.798298 loss_rnnt 11.370653 hw_loss 0.261678 lr 0.00037828 rank 6
2023-02-24 23:33:42,709 DEBUG TRAIN Batch 20/7900 loss 12.882914 loss_att 15.939732 loss_ctc 17.457535 loss_rnnt 11.578016 hw_loss 0.156722 lr 0.00037831 rank 5
2023-02-24 23:33:42,711 DEBUG TRAIN Batch 20/7900 loss 11.738657 loss_att 14.399139 loss_ctc 13.410503 loss_rnnt 10.921635 hw_loss 0.116274 lr 0.00037831 rank 2
2023-02-24 23:33:42,763 DEBUG TRAIN Batch 20/7900 loss 8.242632 loss_att 12.500734 loss_ctc 13.397776 loss_rnnt 6.593570 hw_loss 0.206418 lr 0.00037832 rank 4
2023-02-24 23:34:54,757 DEBUG TRAIN Batch 20/8000 loss 6.045594 loss_att 10.120458 loss_ctc 8.768229 loss_rnnt 4.752156 hw_loss 0.216464 lr 0.00037818 rank 3
2023-02-24 23:34:54,766 DEBUG TRAIN Batch 20/8000 loss 6.103074 loss_att 8.707502 loss_ctc 9.222862 loss_rnnt 5.090372 hw_loss 0.142208 lr 0.00037821 rank 0
2023-02-24 23:34:54,767 DEBUG TRAIN Batch 20/8000 loss 17.070839 loss_att 22.344645 loss_ctc 27.309975 loss_rnnt 14.582199 hw_loss 0.128739 lr 0.00037817 rank 6
2023-02-24 23:34:54,768 DEBUG TRAIN Batch 20/8000 loss 3.646757 loss_att 6.413146 loss_ctc 5.970005 loss_rnnt 2.670108 hw_loss 0.213008 lr 0.00037820 rank 5
2023-02-24 23:34:54,768 DEBUG TRAIN Batch 20/8000 loss 7.134594 loss_att 9.895100 loss_ctc 9.364437 loss_rnnt 6.148455 hw_loss 0.256361 lr 0.00037817 rank 7
2023-02-24 23:34:54,769 DEBUG TRAIN Batch 20/8000 loss 4.389807 loss_att 7.063073 loss_ctc 5.591726 loss_rnnt 3.550035 hw_loss 0.271617 lr 0.00037820 rank 2
2023-02-24 23:34:54,821 DEBUG TRAIN Batch 20/8000 loss 6.751850 loss_att 8.414091 loss_ctc 9.519935 loss_rnnt 5.965539 hw_loss 0.158970 lr 0.00037822 rank 4
2023-02-24 23:34:54,843 DEBUG TRAIN Batch 20/8000 loss 4.883697 loss_att 7.140762 loss_ctc 9.187198 loss_rnnt 3.788237 hw_loss 0.131710 lr 0.00037819 rank 1
2023-02-24 23:36:06,961 DEBUG TRAIN Batch 20/8100 loss 8.653818 loss_att 10.402954 loss_ctc 13.846791 loss_rnnt 7.539242 hw_loss 0.135659 lr 0.00037810 rank 0
2023-02-24 23:36:06,962 DEBUG TRAIN Batch 20/8100 loss 7.084444 loss_att 7.861935 loss_ctc 9.806732 loss_rnnt 6.408955 hw_loss 0.294408 lr 0.00037809 rank 1
2023-02-24 23:36:06,962 DEBUG TRAIN Batch 20/8100 loss 7.118102 loss_att 13.575459 loss_ctc 12.716950 loss_rnnt 4.979691 hw_loss 0.188298 lr 0.00037806 rank 7
2023-02-24 23:36:06,962 DEBUG TRAIN Batch 20/8100 loss 15.827351 loss_att 17.674173 loss_ctc 18.832123 loss_rnnt 14.968712 hw_loss 0.166196 lr 0.00037807 rank 3
2023-02-24 23:36:06,971 DEBUG TRAIN Batch 20/8100 loss 2.558360 loss_att 5.357303 loss_ctc 6.335191 loss_rnnt 1.350867 hw_loss 0.270238 lr 0.00037806 rank 6
2023-02-24 23:36:06,990 DEBUG TRAIN Batch 20/8100 loss 10.491672 loss_att 13.572551 loss_ctc 15.455585 loss_rnnt 9.130792 hw_loss 0.155341 lr 0.00037809 rank 5
2023-02-24 23:36:06,991 DEBUG TRAIN Batch 20/8100 loss 13.603354 loss_att 15.846340 loss_ctc 19.423203 loss_rnnt 12.306763 hw_loss 0.135028 lr 0.00037809 rank 2
2023-02-24 23:36:07,020 DEBUG TRAIN Batch 20/8100 loss 6.180921 loss_att 8.369574 loss_ctc 9.468437 loss_rnnt 5.185101 hw_loss 0.224539 lr 0.00037811 rank 4
2023-02-24 23:37:19,479 DEBUG TRAIN Batch 20/8200 loss 4.927931 loss_att 7.261029 loss_ctc 7.053442 loss_rnnt 4.060389 hw_loss 0.220352 lr 0.00037797 rank 3
2023-02-24 23:37:19,480 DEBUG TRAIN Batch 20/8200 loss 5.433068 loss_att 8.145467 loss_ctc 11.639158 loss_rnnt 3.926882 hw_loss 0.255426 lr 0.00037799 rank 0
2023-02-24 23:37:19,480 DEBUG TRAIN Batch 20/8200 loss 5.966806 loss_att 7.849931 loss_ctc 6.919186 loss_rnnt 5.339453 hw_loss 0.232020 lr 0.00037796 rank 6
2023-02-24 23:37:19,480 DEBUG TRAIN Batch 20/8200 loss 6.212463 loss_att 9.877735 loss_ctc 10.170057 loss_rnnt 4.852145 hw_loss 0.186721 lr 0.00037798 rank 1
2023-02-24 23:37:19,481 DEBUG TRAIN Batch 20/8200 loss 24.568789 loss_att 32.992004 loss_ctc 35.629028 loss_rnnt 21.305288 hw_loss 0.195297 lr 0.00037796 rank 7
2023-02-24 23:37:19,484 DEBUG TRAIN Batch 20/8200 loss 9.381292 loss_att 10.879001 loss_ctc 15.227574 loss_rnnt 8.166776 hw_loss 0.254007 lr 0.00037799 rank 5
2023-02-24 23:37:19,494 DEBUG TRAIN Batch 20/8200 loss 12.059041 loss_att 13.116772 loss_ctc 15.018309 loss_rnnt 11.286105 hw_loss 0.312790 lr 0.00037800 rank 4
2023-02-24 23:37:19,495 DEBUG TRAIN Batch 20/8200 loss 7.736527 loss_att 8.435209 loss_ctc 10.701697 loss_rnnt 7.074110 hw_loss 0.238735 lr 0.00037798 rank 2
2023-02-24 23:38:31,867 DEBUG TRAIN Batch 20/8300 loss 7.333147 loss_att 9.067176 loss_ctc 12.833174 loss_rnnt 6.129402 hw_loss 0.231755 lr 0.00037788 rank 0
2023-02-24 23:38:31,867 DEBUG TRAIN Batch 20/8300 loss 4.345656 loss_att 7.817171 loss_ctc 5.929590 loss_rnnt 3.378062 hw_loss 0.116438 lr 0.00037787 rank 1
2023-02-24 23:38:31,869 DEBUG TRAIN Batch 20/8300 loss 4.454467 loss_att 5.755457 loss_ctc 5.830380 loss_rnnt 3.895892 hw_loss 0.215479 lr 0.00037785 rank 7
2023-02-24 23:38:31,871 DEBUG TRAIN Batch 20/8300 loss 10.640457 loss_att 14.079472 loss_ctc 13.888241 loss_rnnt 9.437353 hw_loss 0.154243 lr 0.00037785 rank 6
2023-02-24 23:38:31,872 DEBUG TRAIN Batch 20/8300 loss 12.211152 loss_att 12.393428 loss_ctc 15.557429 loss_rnnt 11.612456 hw_loss 0.217631 lr 0.00037788 rank 2
2023-02-24 23:38:31,879 DEBUG TRAIN Batch 20/8300 loss 13.339560 loss_att 19.583988 loss_ctc 23.096138 loss_rnnt 10.654078 hw_loss 0.254473 lr 0.00037789 rank 4
2023-02-24 23:38:31,928 DEBUG TRAIN Batch 20/8300 loss 11.060392 loss_att 12.965919 loss_ctc 12.959094 loss_rnnt 10.318205 hw_loss 0.202354 lr 0.00037788 rank 5
2023-02-24 23:50:47,857 DEBUG CV Batch 20/0 loss 1.987548 loss_att 2.032546 loss_ctc 2.660882 loss_rnnt 1.717453 hw_loss 0.321220 history loss 1.913935 rank 2
2023-02-24 23:50:47,862 DEBUG CV Batch 20/0 loss 1.987548 loss_att 2.032546 loss_ctc 2.660882 loss_rnnt 1.717453 hw_loss 0.321220 history loss 1.913935 rank 6
2023-02-24 23:50:47,867 DEBUG CV Batch 20/0 loss 1.987548 loss_att 2.032546 loss_ctc 2.660882 loss_rnnt 1.717453 hw_loss 0.321220 history loss 1.913935 rank 5
2023-02-24 23:50:47,869 DEBUG CV Batch 20/0 loss 1.987548 loss_att 2.032546 loss_ctc 2.660882 loss_rnnt 1.717453 hw_loss 0.321220 history loss 1.913935 rank 4
2023-02-24 23:50:47,869 DEBUG CV Batch 20/0 loss 1.987548 loss_att 2.032546 loss_ctc 2.660882 loss_rnnt 1.717453 hw_loss 0.321220 history loss 1.913935 rank 1
2023-02-24 23:50:47,883 DEBUG CV Batch 20/0 loss 1.987548 loss_att 2.032546 loss_ctc 2.660882 loss_rnnt 1.717453 hw_loss 0.321220 history loss 1.913935 rank 3
2023-02-24 23:50:47,885 DEBUG CV Batch 20/0 loss 1.987548 loss_att 2.032546 loss_ctc 2.660882 loss_rnnt 1.717453 hw_loss 0.321220 history loss 1.913935 rank 7
2023-02-24 23:50:47,885 DEBUG CV Batch 20/0 loss 1.987548 loss_att 2.032546 loss_ctc 2.660882 loss_rnnt 1.717453 hw_loss 0.321220 history loss 1.913935 rank 0
2023-02-24 23:50:59,166 DEBUG CV Batch 20/100 loss 8.160058 loss_att 7.806624 loss_ctc 11.034647 loss_rnnt 7.700058 hw_loss 0.276392 history loss 3.467534 rank 0
2023-02-24 23:50:59,209 DEBUG CV Batch 20/100 loss 8.160058 loss_att 7.806624 loss_ctc 11.034647 loss_rnnt 7.700058 hw_loss 0.276392 history loss 3.467534 rank 7
2023-02-24 23:50:59,465 DEBUG CV Batch 20/100 loss 8.160058 loss_att 7.806624 loss_ctc 11.034647 loss_rnnt 7.700058 hw_loss 0.276392 history loss 3.467534 rank 2
2023-02-24 23:50:59,568 DEBUG CV Batch 20/100 loss 8.160058 loss_att 7.806624 loss_ctc 11.034647 loss_rnnt 7.700058 hw_loss 0.276392 history loss 3.467534 rank 3
2023-02-24 23:50:59,628 DEBUG CV Batch 20/100 loss 8.160058 loss_att 7.806624 loss_ctc 11.034647 loss_rnnt 7.700058 hw_loss 0.276392 history loss 3.467534 rank 1
2023-02-24 23:50:59,636 DEBUG CV Batch 20/100 loss 8.160058 loss_att 7.806624 loss_ctc 11.034647 loss_rnnt 7.700058 hw_loss 0.276392 history loss 3.467534 rank 5
2023-02-24 23:50:59,731 DEBUG CV Batch 20/100 loss 8.160058 loss_att 7.806624 loss_ctc 11.034647 loss_rnnt 7.700058 hw_loss 0.276392 history loss 3.467534 rank 4
2023-02-24 23:50:59,756 DEBUG CV Batch 20/100 loss 8.160058 loss_att 7.806624 loss_ctc 11.034647 loss_rnnt 7.700058 hw_loss 0.276392 history loss 3.467534 rank 6
2023-02-24 23:51:12,510 DEBUG CV Batch 20/200 loss 4.847023 loss_att 12.360771 loss_ctc 3.077802 loss_rnnt 3.501018 hw_loss 0.148408 history loss 4.100334 rank 0
2023-02-24 23:51:12,723 DEBUG CV Batch 20/200 loss 4.847023 loss_att 12.360771 loss_ctc 3.077802 loss_rnnt 3.501018 hw_loss 0.148408 history loss 4.100334 rank 7
2023-02-24 23:51:13,246 DEBUG CV Batch 20/200 loss 4.847023 loss_att 12.360771 loss_ctc 3.077802 loss_rnnt 3.501018 hw_loss 0.148408 history loss 4.100334 rank 5
2023-02-24 23:51:13,657 DEBUG CV Batch 20/200 loss 4.847023 loss_att 12.360771 loss_ctc 3.077802 loss_rnnt 3.501018 hw_loss 0.148408 history loss 4.100334 rank 1
2023-02-24 23:51:13,732 DEBUG CV Batch 20/200 loss 4.847023 loss_att 12.360771 loss_ctc 3.077802 loss_rnnt 3.501018 hw_loss 0.148408 history loss 4.100334 rank 2
2023-02-24 23:51:13,908 DEBUG CV Batch 20/200 loss 4.847023 loss_att 12.360771 loss_ctc 3.077802 loss_rnnt 3.501018 hw_loss 0.148408 history loss 4.100334 rank 3
2023-02-24 23:51:14,076 DEBUG CV Batch 20/200 loss 4.847023 loss_att 12.360771 loss_ctc 3.077802 loss_rnnt 3.501018 hw_loss 0.148408 history loss 4.100334 rank 6
2023-02-24 23:51:14,153 DEBUG CV Batch 20/200 loss 4.847023 loss_att 12.360771 loss_ctc 3.077802 loss_rnnt 3.501018 hw_loss 0.148408 history loss 4.100334 rank 4
2023-02-24 23:51:24,479 DEBUG CV Batch 20/300 loss 3.241865 loss_att 3.961329 loss_ctc 5.174975 loss_rnnt 2.649757 hw_loss 0.357125 history loss 4.186154 rank 0
2023-02-24 23:51:24,817 DEBUG CV Batch 20/300 loss 3.241865 loss_att 3.961329 loss_ctc 5.174975 loss_rnnt 2.649757 hw_loss 0.357125 history loss 4.186154 rank 7
2023-02-24 23:51:25,737 DEBUG CV Batch 20/300 loss 3.241865 loss_att 3.961329 loss_ctc 5.174975 loss_rnnt 2.649757 hw_loss 0.357125 history loss 4.186154 rank 5
2023-02-24 23:51:26,016 DEBUG CV Batch 20/300 loss 3.241865 loss_att 3.961329 loss_ctc 5.174975 loss_rnnt 2.649757 hw_loss 0.357125 history loss 4.186154 rank 2
2023-02-24 23:51:26,203 DEBUG CV Batch 20/300 loss 3.241865 loss_att 3.961329 loss_ctc 5.174975 loss_rnnt 2.649757 hw_loss 0.357125 history loss 4.186154 rank 1
2023-02-24 23:51:26,219 DEBUG CV Batch 20/300 loss 3.241865 loss_att 3.961329 loss_ctc 5.174975 loss_rnnt 2.649757 hw_loss 0.357125 history loss 4.186154 rank 3
2023-02-24 23:51:26,548 DEBUG CV Batch 20/300 loss 3.241865 loss_att 3.961329 loss_ctc 5.174975 loss_rnnt 2.649757 hw_loss 0.357125 history loss 4.186154 rank 6
2023-02-24 23:51:26,968 DEBUG CV Batch 20/300 loss 3.241865 loss_att 3.961329 loss_ctc 5.174975 loss_rnnt 2.649757 hw_loss 0.357125 history loss 4.186154 rank 4
2023-02-24 23:51:36,505 DEBUG CV Batch 20/400 loss 16.318279 loss_att 74.674210 loss_ctc 12.377179 loss_rnnt 5.083556 hw_loss 0.166907 history loss 5.097798 rank 0
2023-02-24 23:51:37,189 DEBUG CV Batch 20/400 loss 16.318279 loss_att 74.674210 loss_ctc 12.377179 loss_rnnt 5.083556 hw_loss 0.166907 history loss 5.097798 rank 7
2023-02-24 23:51:38,335 DEBUG CV Batch 20/400 loss 16.318279 loss_att 74.674210 loss_ctc 12.377179 loss_rnnt 5.083556 hw_loss 0.166907 history loss 5.097798 rank 1
2023-02-24 23:51:38,348 DEBUG CV Batch 20/400 loss 16.318279 loss_att 74.674210 loss_ctc 12.377179 loss_rnnt 5.083556 hw_loss 0.166907 history loss 5.097798 rank 2
2023-02-24 23:51:38,365 DEBUG CV Batch 20/400 loss 16.318279 loss_att 74.674210 loss_ctc 12.377179 loss_rnnt 5.083556 hw_loss 0.166907 history loss 5.097798 rank 5
2023-02-24 23:51:38,458 DEBUG CV Batch 20/400 loss 16.318279 loss_att 74.674210 loss_ctc 12.377179 loss_rnnt 5.083556 hw_loss 0.166907 history loss 5.097798 rank 3
2023-02-24 23:51:38,797 DEBUG CV Batch 20/400 loss 16.318279 loss_att 74.674210 loss_ctc 12.377179 loss_rnnt 5.083556 hw_loss 0.166907 history loss 5.097798 rank 6
2023-02-24 23:51:39,357 DEBUG CV Batch 20/400 loss 16.318279 loss_att 74.674210 loss_ctc 12.377179 loss_rnnt 5.083556 hw_loss 0.166907 history loss 5.097798 rank 4
2023-02-24 23:51:46,930 DEBUG CV Batch 20/500 loss 4.722626 loss_att 5.349433 loss_ctc 6.440634 loss_rnnt 4.253295 hw_loss 0.215440 history loss 5.940342 rank 0
2023-02-24 23:51:47,870 DEBUG CV Batch 20/500 loss 4.722626 loss_att 5.349433 loss_ctc 6.440634 loss_rnnt 4.253295 hw_loss 0.215440 history loss 5.940342 rank 7
2023-02-24 23:51:48,739 DEBUG CV Batch 20/500 loss 4.722626 loss_att 5.349433 loss_ctc 6.440634 loss_rnnt 4.253295 hw_loss 0.215440 history loss 5.940342 rank 1
2023-02-24 23:51:49,079 DEBUG CV Batch 20/500 loss 4.722626 loss_att 5.349433 loss_ctc 6.440634 loss_rnnt 4.253295 hw_loss 0.215440 history loss 5.940342 rank 3
2023-02-24 23:51:49,168 DEBUG CV Batch 20/500 loss 4.722626 loss_att 5.349433 loss_ctc 6.440634 loss_rnnt 4.253295 hw_loss 0.215440 history loss 5.940342 rank 2
2023-02-24 23:51:49,585 DEBUG CV Batch 20/500 loss 4.722626 loss_att 5.349433 loss_ctc 6.440634 loss_rnnt 4.253295 hw_loss 0.215440 history loss 5.940342 rank 5
2023-02-24 23:51:49,776 DEBUG CV Batch 20/500 loss 4.722626 loss_att 5.349433 loss_ctc 6.440634 loss_rnnt 4.253295 hw_loss 0.215440 history loss 5.940342 rank 6
2023-02-24 23:51:50,157 DEBUG CV Batch 20/500 loss 4.722626 loss_att 5.349433 loss_ctc 6.440634 loss_rnnt 4.253295 hw_loss 0.215440 history loss 5.940342 rank 4
2023-02-24 23:51:58,959 DEBUG CV Batch 20/600 loss 6.368590 loss_att 6.375466 loss_ctc 8.436432 loss_rnnt 5.911938 hw_loss 0.336684 history loss 6.887391 rank 0
2023-02-24 23:52:00,031 DEBUG CV Batch 20/600 loss 6.368590 loss_att 6.375466 loss_ctc 8.436432 loss_rnnt 5.911938 hw_loss 0.336684 history loss 6.887391 rank 7
2023-02-24 23:52:00,973 DEBUG CV Batch 20/600 loss 6.368590 loss_att 6.375466 loss_ctc 8.436432 loss_rnnt 5.911938 hw_loss 0.336684 history loss 6.887391 rank 1
2023-02-24 23:52:01,371 DEBUG CV Batch 20/600 loss 6.368590 loss_att 6.375466 loss_ctc 8.436432 loss_rnnt 5.911938 hw_loss 0.336684 history loss 6.887391 rank 3
2023-02-24 23:52:01,399 DEBUG CV Batch 20/600 loss 6.368590 loss_att 6.375466 loss_ctc 8.436432 loss_rnnt 5.911938 hw_loss 0.336684 history loss 6.887391 rank 2
2023-02-24 23:52:02,213 DEBUG CV Batch 20/600 loss 6.368590 loss_att 6.375466 loss_ctc 8.436432 loss_rnnt 5.911938 hw_loss 0.336684 history loss 6.887391 rank 5
2023-02-24 23:52:02,467 DEBUG CV Batch 20/600 loss 6.368590 loss_att 6.375466 loss_ctc 8.436432 loss_rnnt 5.911938 hw_loss 0.336684 history loss 6.887391 rank 6
2023-02-24 23:52:02,742 DEBUG CV Batch 20/600 loss 6.368590 loss_att 6.375466 loss_ctc 8.436432 loss_rnnt 5.911938 hw_loss 0.336684 history loss 6.887391 rank 4
2023-02-24 23:52:10,350 DEBUG CV Batch 20/700 loss 11.853801 loss_att 36.463554 loss_ctc 14.085632 loss_rnnt 6.534321 hw_loss 0.187408 history loss 7.561168 rank 0
2023-02-24 23:52:11,644 DEBUG CV Batch 20/700 loss 11.853801 loss_att 36.463554 loss_ctc 14.085632 loss_rnnt 6.534321 hw_loss 0.187408 history loss 7.561168 rank 7
2023-02-24 23:52:12,858 DEBUG CV Batch 20/700 loss 11.853801 loss_att 36.463554 loss_ctc 14.085632 loss_rnnt 6.534321 hw_loss 0.187408 history loss 7.561168 rank 1
2023-02-24 23:52:12,886 DEBUG CV Batch 20/700 loss 11.853801 loss_att 36.463554 loss_ctc 14.085632 loss_rnnt 6.534321 hw_loss 0.187408 history loss 7.561168 rank 2
2023-02-24 23:52:13,932 DEBUG CV Batch 20/700 loss 11.853801 loss_att 36.463554 loss_ctc 14.085632 loss_rnnt 6.534321 hw_loss 0.187408 history loss 7.561168 rank 5
2023-02-24 23:52:13,934 DEBUG CV Batch 20/700 loss 11.853801 loss_att 36.463554 loss_ctc 14.085632 loss_rnnt 6.534321 hw_loss 0.187408 history loss 7.561168 rank 3
2023-02-24 23:52:14,538 DEBUG CV Batch 20/700 loss 11.853801 loss_att 36.463554 loss_ctc 14.085632 loss_rnnt 6.534321 hw_loss 0.187408 history loss 7.561168 rank 4
2023-02-24 23:52:14,673 DEBUG CV Batch 20/700 loss 11.853801 loss_att 36.463554 loss_ctc 14.085632 loss_rnnt 6.534321 hw_loss 0.187408 history loss 7.561168 rank 6
2023-02-24 23:52:21,489 DEBUG CV Batch 20/800 loss 13.713296 loss_att 12.013895 loss_ctc 18.930714 loss_rnnt 13.225833 hw_loss 0.246916 history loss 7.013487 rank 0
2023-02-24 23:52:22,854 DEBUG CV Batch 20/800 loss 13.713296 loss_att 12.013895 loss_ctc 18.930714 loss_rnnt 13.225833 hw_loss 0.246916 history loss 7.013487 rank 7
2023-02-24 23:52:25,022 DEBUG CV Batch 20/800 loss 13.713296 loss_att 12.013895 loss_ctc 18.930714 loss_rnnt 13.225833 hw_loss 0.246916 history loss 7.013487 rank 1
2023-02-24 23:52:25,121 DEBUG CV Batch 20/800 loss 13.713296 loss_att 12.013895 loss_ctc 18.930714 loss_rnnt 13.225833 hw_loss 0.246916 history loss 7.013487 rank 2
2023-02-24 23:52:25,615 DEBUG CV Batch 20/800 loss 13.713296 loss_att 12.013895 loss_ctc 18.930714 loss_rnnt 13.225833 hw_loss 0.246916 history loss 7.013487 rank 5
2023-02-24 23:52:25,765 DEBUG CV Batch 20/800 loss 13.713296 loss_att 12.013895 loss_ctc 18.930714 loss_rnnt 13.225833 hw_loss 0.246916 history loss 7.013487 rank 4
2023-02-24 23:52:26,181 DEBUG CV Batch 20/800 loss 13.713296 loss_att 12.013895 loss_ctc 18.930714 loss_rnnt 13.225833 hw_loss 0.246916 history loss 7.013487 rank 3
2023-02-24 23:52:26,455 DEBUG CV Batch 20/800 loss 13.713296 loss_att 12.013895 loss_ctc 18.930714 loss_rnnt 13.225833 hw_loss 0.246916 history loss 7.013487 rank 6
2023-02-24 23:52:34,814 DEBUG CV Batch 20/900 loss 12.725834 loss_att 17.385036 loss_ctc 22.306915 loss_rnnt 10.442990 hw_loss 0.137858 history loss 6.816804 rank 0
2023-02-24 23:52:36,439 DEBUG CV Batch 20/900 loss 12.725834 loss_att 17.385036 loss_ctc 22.306915 loss_rnnt 10.442990 hw_loss 0.137858 history loss 6.816804 rank 7
2023-02-24 23:52:38,984 DEBUG CV Batch 20/900 loss 12.725834 loss_att 17.385036 loss_ctc 22.306915 loss_rnnt 10.442990 hw_loss 0.137858 history loss 6.816804 rank 1
2023-02-24 23:52:39,027 DEBUG CV Batch 20/900 loss 12.725834 loss_att 17.385036 loss_ctc 22.306915 loss_rnnt 10.442990 hw_loss 0.137858 history loss 6.816804 rank 2
2023-02-24 23:52:39,373 DEBUG CV Batch 20/900 loss 12.725834 loss_att 17.385036 loss_ctc 22.306915 loss_rnnt 10.442990 hw_loss 0.137858 history loss 6.816804 rank 4
2023-02-24 23:52:39,523 DEBUG CV Batch 20/900 loss 12.725834 loss_att 17.385036 loss_ctc 22.306915 loss_rnnt 10.442990 hw_loss 0.137858 history loss 6.816804 rank 5
2023-02-24 23:52:40,062 DEBUG CV Batch 20/900 loss 12.725834 loss_att 17.385036 loss_ctc 22.306915 loss_rnnt 10.442990 hw_loss 0.137858 history loss 6.816804 rank 3
2023-02-24 23:52:40,265 DEBUG CV Batch 20/900 loss 12.725834 loss_att 17.385036 loss_ctc 22.306915 loss_rnnt 10.442990 hw_loss 0.137858 history loss 6.816804 rank 6
2023-02-24 23:52:47,207 DEBUG CV Batch 20/1000 loss 5.120678 loss_att 4.870974 loss_ctc 5.267857 loss_rnnt 4.999556 hw_loss 0.283947 history loss 6.570611 rank 0
2023-02-24 23:52:49,089 DEBUG CV Batch 20/1000 loss 5.120678 loss_att 4.870974 loss_ctc 5.267857 loss_rnnt 4.999556 hw_loss 0.283947 history loss 6.570611 rank 7
2023-02-24 23:52:51,256 DEBUG CV Batch 20/1000 loss 5.120678 loss_att 4.870974 loss_ctc 5.267857 loss_rnnt 4.999556 hw_loss 0.283947 history loss 6.570611 rank 1
2023-02-24 23:52:51,287 DEBUG CV Batch 20/1000 loss 5.120678 loss_att 4.870974 loss_ctc 5.267857 loss_rnnt 4.999556 hw_loss 0.283947 history loss 6.570611 rank 2
2023-02-24 23:52:52,073 DEBUG CV Batch 20/1000 loss 5.120678 loss_att 4.870974 loss_ctc 5.267857 loss_rnnt 4.999556 hw_loss 0.283947 history loss 6.570611 rank 4
2023-02-24 23:52:52,403 DEBUG CV Batch 20/1000 loss 5.120678 loss_att 4.870974 loss_ctc 5.267857 loss_rnnt 4.999556 hw_loss 0.283947 history loss 6.570611 rank 5
2023-02-24 23:52:52,830 DEBUG CV Batch 20/1000 loss 5.120678 loss_att 4.870974 loss_ctc 5.267857 loss_rnnt 4.999556 hw_loss 0.283947 history loss 6.570611 rank 3
2023-02-24 23:52:52,866 DEBUG CV Batch 20/1000 loss 5.120678 loss_att 4.870974 loss_ctc 5.267857 loss_rnnt 4.999556 hw_loss 0.283947 history loss 6.570611 rank 6
2023-02-24 23:52:59,214 DEBUG CV Batch 20/1100 loss 6.794864 loss_att 6.188810 loss_ctc 9.379035 loss_rnnt 6.420320 hw_loss 0.283496 history loss 6.529101 rank 0
2023-02-24 23:53:01,250 DEBUG CV Batch 20/1100 loss 6.794864 loss_att 6.188810 loss_ctc 9.379035 loss_rnnt 6.420320 hw_loss 0.283496 history loss 6.529101 rank 7
2023-02-24 23:53:03,289 DEBUG CV Batch 20/1100 loss 6.794864 loss_att 6.188810 loss_ctc 9.379035 loss_rnnt 6.420320 hw_loss 0.283496 history loss 6.529101 rank 1
2023-02-24 23:53:03,569 DEBUG CV Batch 20/1100 loss 6.794864 loss_att 6.188810 loss_ctc 9.379035 loss_rnnt 6.420320 hw_loss 0.283496 history loss 6.529101 rank 2
2023-02-24 23:53:04,170 DEBUG CV Batch 20/1100 loss 6.794864 loss_att 6.188810 loss_ctc 9.379035 loss_rnnt 6.420320 hw_loss 0.283496 history loss 6.529101 rank 4
2023-02-24 23:53:04,832 DEBUG CV Batch 20/1100 loss 6.794864 loss_att 6.188810 loss_ctc 9.379035 loss_rnnt 6.420320 hw_loss 0.283496 history loss 6.529101 rank 5
2023-02-24 23:53:05,015 DEBUG CV Batch 20/1100 loss 6.794864 loss_att 6.188810 loss_ctc 9.379035 loss_rnnt 6.420320 hw_loss 0.283496 history loss 6.529101 rank 3
2023-02-24 23:53:05,231 DEBUG CV Batch 20/1100 loss 6.794864 loss_att 6.188810 loss_ctc 9.379035 loss_rnnt 6.420320 hw_loss 0.283496 history loss 6.529101 rank 6
2023-02-24 23:53:10,046 DEBUG CV Batch 20/1200 loss 9.409760 loss_att 9.914346 loss_ctc 9.920753 loss_rnnt 9.145057 hw_loss 0.179353 history loss 6.884332 rank 0
2023-02-24 23:53:12,229 DEBUG CV Batch 20/1200 loss 9.409760 loss_att 9.914346 loss_ctc 9.920753 loss_rnnt 9.145057 hw_loss 0.179353 history loss 6.884332 rank 7
2023-02-24 23:53:14,115 DEBUG CV Batch 20/1200 loss 9.409760 loss_att 9.914346 loss_ctc 9.920753 loss_rnnt 9.145057 hw_loss 0.179353 history loss 6.884332 rank 1
2023-02-24 23:53:14,301 DEBUG CV Batch 20/1200 loss 9.409760 loss_att 9.914346 loss_ctc 9.920753 loss_rnnt 9.145057 hw_loss 0.179353 history loss 6.884332 rank 2
2023-02-24 23:53:15,225 DEBUG CV Batch 20/1200 loss 9.409760 loss_att 9.914346 loss_ctc 9.920753 loss_rnnt 9.145057 hw_loss 0.179353 history loss 6.884332 rank 4
2023-02-24 23:53:15,624 DEBUG CV Batch 20/1200 loss 9.409760 loss_att 9.914346 loss_ctc 9.920753 loss_rnnt 9.145057 hw_loss 0.179353 history loss 6.884332 rank 3
2023-02-24 23:53:16,174 DEBUG CV Batch 20/1200 loss 9.409760 loss_att 9.914346 loss_ctc 9.920753 loss_rnnt 9.145057 hw_loss 0.179353 history loss 6.884332 rank 5
2023-02-24 23:53:16,237 DEBUG CV Batch 20/1200 loss 9.409760 loss_att 9.914346 loss_ctc 9.920753 loss_rnnt 9.145057 hw_loss 0.179353 history loss 6.884332 rank 6
2023-02-24 23:53:22,134 DEBUG CV Batch 20/1300 loss 5.156795 loss_att 5.340218 loss_ctc 7.248074 loss_rnnt 4.719517 hw_loss 0.228295 history loss 7.210426 rank 0
2023-02-24 23:53:24,458 DEBUG CV Batch 20/1300 loss 5.156795 loss_att 5.340218 loss_ctc 7.248074 loss_rnnt 4.719517 hw_loss 0.228295 history loss 7.210426 rank 7
2023-02-24 23:53:26,271 DEBUG CV Batch 20/1300 loss 5.156795 loss_att 5.340218 loss_ctc 7.248074 loss_rnnt 4.719517 hw_loss 0.228295 history loss 7.210426 rank 1
2023-02-24 23:53:26,350 DEBUG CV Batch 20/1300 loss 5.156795 loss_att 5.340218 loss_ctc 7.248074 loss_rnnt 4.719517 hw_loss 0.228295 history loss 7.210426 rank 2
2023-02-24 23:53:27,815 DEBUG CV Batch 20/1300 loss 5.156795 loss_att 5.340218 loss_ctc 7.248074 loss_rnnt 4.719517 hw_loss 0.228295 history loss 7.210426 rank 4
2023-02-24 23:53:27,994 DEBUG CV Batch 20/1300 loss 5.156795 loss_att 5.340218 loss_ctc 7.248074 loss_rnnt 4.719517 hw_loss 0.228295 history loss 7.210426 rank 3
2023-02-24 23:53:28,446 DEBUG CV Batch 20/1300 loss 5.156795 loss_att 5.340218 loss_ctc 7.248074 loss_rnnt 4.719517 hw_loss 0.228295 history loss 7.210426 rank 6
2023-02-24 23:53:28,623 DEBUG CV Batch 20/1300 loss 5.156795 loss_att 5.340218 loss_ctc 7.248074 loss_rnnt 4.719517 hw_loss 0.228295 history loss 7.210426 rank 5
2023-02-24 23:53:33,480 DEBUG CV Batch 20/1400 loss 7.769215 loss_att 18.001295 loss_ctc 4.523765 loss_rnnt 6.078257 hw_loss 0.144878 history loss 7.527593 rank 0
2023-02-24 23:53:35,993 DEBUG CV Batch 20/1400 loss 7.769215 loss_att 18.001295 loss_ctc 4.523765 loss_rnnt 6.078257 hw_loss 0.144878 history loss 7.527593 rank 7
2023-02-24 23:53:37,770 DEBUG CV Batch 20/1400 loss 7.769215 loss_att 18.001295 loss_ctc 4.523765 loss_rnnt 6.078257 hw_loss 0.144878 history loss 7.527593 rank 2
2023-02-24 23:53:38,020 DEBUG CV Batch 20/1400 loss 7.769215 loss_att 18.001295 loss_ctc 4.523765 loss_rnnt 6.078257 hw_loss 0.144878 history loss 7.527593 rank 1
2023-02-24 23:53:39,503 DEBUG CV Batch 20/1400 loss 7.769215 loss_att 18.001295 loss_ctc 4.523765 loss_rnnt 6.078257 hw_loss 0.144878 history loss 7.527593 rank 4
2023-02-24 23:53:39,769 DEBUG CV Batch 20/1400 loss 7.769215 loss_att 18.001295 loss_ctc 4.523765 loss_rnnt 6.078257 hw_loss 0.144878 history loss 7.527593 rank 6
2023-02-24 23:53:39,784 DEBUG CV Batch 20/1400 loss 7.769215 loss_att 18.001295 loss_ctc 4.523765 loss_rnnt 6.078257 hw_loss 0.144878 history loss 7.527593 rank 3
2023-02-24 23:53:40,441 DEBUG CV Batch 20/1400 loss 7.769215 loss_att 18.001295 loss_ctc 4.523765 loss_rnnt 6.078257 hw_loss 0.144878 history loss 7.527593 rank 5
2023-02-24 23:53:45,074 DEBUG CV Batch 20/1500 loss 9.736958 loss_att 8.491529 loss_ctc 9.355730 loss_rnnt 9.924315 hw_loss 0.211048 history loss 7.355647 rank 0
2023-02-24 23:53:47,930 DEBUG CV Batch 20/1500 loss 9.736958 loss_att 8.491529 loss_ctc 9.355730 loss_rnnt 9.924315 hw_loss 0.211048 history loss 7.355647 rank 7
2023-02-24 23:53:50,394 DEBUG CV Batch 20/1500 loss 9.736958 loss_att 8.491529 loss_ctc 9.355730 loss_rnnt 9.924315 hw_loss 0.211048 history loss 7.355647 rank 2
2023-02-24 23:53:50,618 DEBUG CV Batch 20/1500 loss 9.736958 loss_att 8.491529 loss_ctc 9.355730 loss_rnnt 9.924315 hw_loss 0.211048 history loss 7.355647 rank 1
2023-02-24 23:53:51,327 DEBUG CV Batch 20/1500 loss 9.736958 loss_att 8.491529 loss_ctc 9.355730 loss_rnnt 9.924315 hw_loss 0.211048 history loss 7.355647 rank 6
2023-02-24 23:53:51,389 DEBUG CV Batch 20/1500 loss 9.736958 loss_att 8.491529 loss_ctc 9.355730 loss_rnnt 9.924315 hw_loss 0.211048 history loss 7.355647 rank 4
2023-02-24 23:53:51,526 DEBUG CV Batch 20/1500 loss 9.736958 loss_att 8.491529 loss_ctc 9.355730 loss_rnnt 9.924315 hw_loss 0.211048 history loss 7.355647 rank 3
2023-02-24 23:53:53,194 DEBUG CV Batch 20/1500 loss 9.736958 loss_att 8.491529 loss_ctc 9.355730 loss_rnnt 9.924315 hw_loss 0.211048 history loss 7.355647 rank 5
2023-02-24 23:53:58,293 DEBUG CV Batch 20/1600 loss 7.052434 loss_att 11.114278 loss_ctc 8.890467 loss_rnnt 5.846199 hw_loss 0.278993 history loss 7.290883 rank 0
2023-02-24 23:54:01,230 DEBUG CV Batch 20/1600 loss 7.052434 loss_att 11.114278 loss_ctc 8.890467 loss_rnnt 5.846199 hw_loss 0.278993 history loss 7.290883 rank 7
2023-02-24 23:54:04,304 DEBUG CV Batch 20/1600 loss 7.052434 loss_att 11.114278 loss_ctc 8.890467 loss_rnnt 5.846199 hw_loss 0.278993 history loss 7.290883 rank 2
2023-02-24 23:54:04,580 DEBUG CV Batch 20/1600 loss 7.052434 loss_att 11.114278 loss_ctc 8.890467 loss_rnnt 5.846199 hw_loss 0.278993 history loss 7.290883 rank 1
2023-02-24 23:54:04,762 DEBUG CV Batch 20/1600 loss 7.052434 loss_att 11.114278 loss_ctc 8.890467 loss_rnnt 5.846199 hw_loss 0.278993 history loss 7.290883 rank 6
2023-02-24 23:54:05,076 DEBUG CV Batch 20/1600 loss 7.052434 loss_att 11.114278 loss_ctc 8.890467 loss_rnnt 5.846199 hw_loss 0.278993 history loss 7.290883 rank 4
2023-02-24 23:54:05,294 DEBUG CV Batch 20/1600 loss 7.052434 loss_att 11.114278 loss_ctc 8.890467 loss_rnnt 5.846199 hw_loss 0.278993 history loss 7.290883 rank 3
2023-02-24 23:54:06,818 DEBUG CV Batch 20/1600 loss 7.052434 loss_att 11.114278 loss_ctc 8.890467 loss_rnnt 5.846199 hw_loss 0.278993 history loss 7.290883 rank 5
2023-02-24 23:54:10,803 DEBUG CV Batch 20/1700 loss 10.948534 loss_att 9.304034 loss_ctc 16.917065 loss_rnnt 10.370510 hw_loss 0.208348 history loss 7.193206 rank 0
2023-02-24 23:54:13,876 DEBUG CV Batch 20/1700 loss 10.948534 loss_att 9.304034 loss_ctc 16.917065 loss_rnnt 10.370510 hw_loss 0.208348 history loss 7.193206 rank 7
2023-02-24 23:54:16,818 DEBUG CV Batch 20/1700 loss 10.948534 loss_att 9.304034 loss_ctc 16.917065 loss_rnnt 10.370510 hw_loss 0.208348 history loss 7.193206 rank 2
2023-02-24 23:54:17,113 DEBUG CV Batch 20/1700 loss 10.948534 loss_att 9.304034 loss_ctc 16.917065 loss_rnnt 10.370510 hw_loss 0.208348 history loss 7.193206 rank 1
2023-02-24 23:54:17,306 DEBUG CV Batch 20/1700 loss 10.948534 loss_att 9.304034 loss_ctc 16.917065 loss_rnnt 10.370510 hw_loss 0.208348 history loss 7.193206 rank 6
2023-02-24 23:54:17,709 DEBUG CV Batch 20/1700 loss 10.948534 loss_att 9.304034 loss_ctc 16.917065 loss_rnnt 10.370510 hw_loss 0.208348 history loss 7.193206 rank 4
2023-02-24 23:54:17,810 DEBUG CV Batch 20/1700 loss 10.948534 loss_att 9.304034 loss_ctc 16.917065 loss_rnnt 10.370510 hw_loss 0.208348 history loss 7.193206 rank 3
2023-02-24 23:54:19,511 DEBUG CV Batch 20/1700 loss 10.948534 loss_att 9.304034 loss_ctc 16.917065 loss_rnnt 10.370510 hw_loss 0.208348 history loss 7.193206 rank 5
2023-02-24 23:54:20,033 INFO Epoch 20 CV info cv_loss 7.153823002300643
2023-02-24 23:54:20,034 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/20.pt
2023-02-24 23:54:20,626 INFO Epoch 21 TRAIN info lr 0.00037781229927177316
2023-02-24 23:54:20,631 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 23:54:23,060 INFO Epoch 20 CV info cv_loss 7.1538230025633895
2023-02-24 23:54:23,061 INFO Epoch 21 TRAIN info lr 0.00037782632178315307
2023-02-24 23:54:23,065 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 23:54:26,223 INFO Epoch 20 CV info cv_loss 7.153823001895755
2023-02-24 23:54:26,224 INFO Epoch 21 TRAIN info lr 0.00037781877100609407
2023-02-24 23:54:26,229 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 23:54:26,307 INFO Epoch 20 CV info cv_loss 7.15382300177515
2023-02-24 23:54:26,307 INFO Epoch 21 TRAIN info lr 0.000377796121390839
2023-02-24 23:54:26,309 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 23:54:26,625 INFO Epoch 20 CV info cv_loss 7.1538230016028574
2023-02-24 23:54:26,626 INFO Epoch 21 TRAIN info lr 0.00037783063671616434
2023-02-24 23:54:26,631 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 23:54:26,892 INFO Epoch 20 CV info cv_loss 7.153823001831145
2023-02-24 23:54:26,892 INFO Epoch 21 TRAIN info lr 0.0003778371093928807
2023-02-24 23:54:26,898 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 23:54:27,053 INFO Epoch 20 CV info cv_loss 7.153823002938125
2023-02-24 23:54:27,054 INFO Epoch 21 TRAIN info lr 0.0003778683986871722
2023-02-24 23:54:27,059 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 23:54:28,830 INFO Epoch 20 CV info cv_loss 7.153823002425555
2023-02-24 23:54:28,831 INFO Epoch 21 TRAIN info lr 0.0003778198496608147
2023-02-24 23:54:28,837 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-24 23:55:27,741 DEBUG TRAIN Batch 21/0 loss 13.447007 loss_att 12.836908 loss_ctc 17.761763 loss_rnnt 12.802102 hw_loss 0.359295 lr 0.00037784 rank 4
2023-02-24 23:55:27,741 DEBUG TRAIN Batch 21/0 loss 7.531664 loss_att 8.235641 loss_ctc 10.991677 loss_rnnt 6.783666 hw_loss 0.273501 lr 0.00037781 rank 0
2023-02-24 23:55:27,754 DEBUG TRAIN Batch 21/0 loss 6.426753 loss_att 6.582375 loss_ctc 9.116851 loss_rnnt 5.864084 hw_loss 0.324121 lr 0.00037783 rank 7
2023-02-24 23:55:27,763 DEBUG TRAIN Batch 21/0 loss 9.589844 loss_att 9.773729 loss_ctc 12.263796 loss_rnnt 9.004525 hw_loss 0.360025 lr 0.00037780 rank 1
2023-02-24 23:55:27,765 DEBUG TRAIN Batch 21/0 loss 7.480005 loss_att 7.193892 loss_ctc 8.457074 loss_rnnt 7.248829 hw_loss 0.296478 lr 0.00037787 rank 3
2023-02-24 23:55:27,789 DEBUG TRAIN Batch 21/0 loss 11.636981 loss_att 11.397550 loss_ctc 15.691091 loss_rnnt 10.961532 hw_loss 0.342726 lr 0.00037782 rank 5
2023-02-24 23:55:27,793 DEBUG TRAIN Batch 21/0 loss 7.619383 loss_att 7.693577 loss_ctc 9.040406 loss_rnnt 7.279609 hw_loss 0.253998 lr 0.00037782 rank 2
2023-02-24 23:55:27,799 DEBUG TRAIN Batch 21/0 loss 6.258574 loss_att 6.185569 loss_ctc 7.706529 loss_rnnt 5.943943 hw_loss 0.255322 lr 0.00037783 rank 6
2023-02-24 23:56:39,666 DEBUG TRAIN Batch 21/100 loss 14.091505 loss_att 17.648060 loss_ctc 20.200539 loss_rnnt 12.431086 hw_loss 0.252319 lr 0.00037776 rank 3
2023-02-24 23:56:39,669 DEBUG TRAIN Batch 21/100 loss 7.096451 loss_att 10.675916 loss_ctc 9.640532 loss_rnnt 5.959067 hw_loss 0.154276 lr 0.00037770 rank 0
2023-02-24 23:56:39,670 DEBUG TRAIN Batch 21/100 loss 3.607759 loss_att 6.625328 loss_ctc 5.712375 loss_rnnt 2.632243 hw_loss 0.171351 lr 0.00037773 rank 4
2023-02-24 23:56:39,670 DEBUG TRAIN Batch 21/100 loss 8.257246 loss_att 10.626801 loss_ctc 9.523479 loss_rnnt 7.531959 hw_loss 0.154772 lr 0.00037772 rank 6
2023-02-24 23:56:39,675 DEBUG TRAIN Batch 21/100 loss 2.472241 loss_att 5.146994 loss_ctc 3.677142 loss_rnnt 1.650294 hw_loss 0.236893 lr 0.00037769 rank 1
2023-02-24 23:56:39,676 DEBUG TRAIN Batch 21/100 loss 9.297369 loss_att 15.515234 loss_ctc 16.428114 loss_rnnt 7.001918 hw_loss 0.189586 lr 0.00037772 rank 7
2023-02-24 23:56:39,674 DEBUG TRAIN Batch 21/100 loss 5.368951 loss_att 10.055449 loss_ctc 10.996638 loss_rnnt 3.559620 hw_loss 0.228138 lr 0.00037771 rank 5
2023-02-24 23:56:39,683 DEBUG TRAIN Batch 21/100 loss 7.782205 loss_att 14.566299 loss_ctc 15.678789 loss_rnnt 5.292584 hw_loss 0.149856 lr 0.00037771 rank 2
2023-02-24 23:57:51,442 DEBUG TRAIN Batch 21/200 loss 9.854639 loss_att 15.409489 loss_ctc 15.706835 loss_rnnt 7.856791 hw_loss 0.199847 lr 0.00037760 rank 0
2023-02-24 23:57:51,446 DEBUG TRAIN Batch 21/200 loss 12.012781 loss_att 15.244380 loss_ctc 13.515064 loss_rnnt 11.117724 hw_loss 0.090812 lr 0.00037758 rank 1
2023-02-24 23:57:51,449 DEBUG TRAIN Batch 21/200 loss 4.486271 loss_att 7.269602 loss_ctc 8.478289 loss_rnnt 3.297926 hw_loss 0.186392 lr 0.00037760 rank 2
2023-02-24 23:57:51,450 DEBUG TRAIN Batch 21/200 loss 4.738228 loss_att 6.989208 loss_ctc 4.498900 loss_rnnt 4.215205 hw_loss 0.196382 lr 0.00037761 rank 6
2023-02-24 23:57:51,450 DEBUG TRAIN Batch 21/200 loss 4.090371 loss_att 7.856177 loss_ctc 7.541002 loss_rnnt 2.705746 hw_loss 0.321337 lr 0.00037761 rank 7
2023-02-24 23:57:51,449 DEBUG TRAIN Batch 21/200 loss 9.032722 loss_att 12.134089 loss_ctc 17.394808 loss_rnnt 7.189376 hw_loss 0.202742 lr 0.00037765 rank 3
2023-02-24 23:57:51,454 DEBUG TRAIN Batch 21/200 loss 18.461996 loss_att 28.056822 loss_ctc 24.906500 loss_rnnt 15.576240 hw_loss 0.201609 lr 0.00037762 rank 4
2023-02-24 23:57:51,456 DEBUG TRAIN Batch 21/200 loss 10.983677 loss_att 13.707828 loss_ctc 19.589357 loss_rnnt 9.138468 hw_loss 0.286791 lr 0.00037760 rank 5
2023-02-24 23:59:04,165 DEBUG TRAIN Batch 21/300 loss 7.249905 loss_att 11.432694 loss_ctc 8.055992 loss_rnnt 6.186176 hw_loss 0.224423 lr 0.00037751 rank 4
2023-02-24 23:59:04,167 DEBUG TRAIN Batch 21/300 loss 14.849905 loss_att 16.382126 loss_ctc 21.182293 loss_rnnt 13.595824 hw_loss 0.193721 lr 0.00037750 rank 5
2023-02-24 23:59:04,168 DEBUG TRAIN Batch 21/300 loss 8.862205 loss_att 11.654527 loss_ctc 8.932449 loss_rnnt 8.200948 hw_loss 0.175173 lr 0.00037754 rank 3
2023-02-24 23:59:04,170 DEBUG TRAIN Batch 21/300 loss 7.526182 loss_att 10.053675 loss_ctc 9.946375 loss_rnnt 6.600612 hw_loss 0.182587 lr 0.00037750 rank 7
2023-02-24 23:59:04,186 DEBUG TRAIN Batch 21/300 loss 8.769425 loss_att 12.522552 loss_ctc 12.404095 loss_rnnt 7.409966 hw_loss 0.232898 lr 0.00037751 rank 6
2023-02-24 23:59:04,189 DEBUG TRAIN Batch 21/300 loss 13.673470 loss_att 16.171192 loss_ctc 23.421762 loss_rnnt 11.790118 hw_loss 0.157564 lr 0.00037749 rank 0
2023-02-24 23:59:04,190 DEBUG TRAIN Batch 21/300 loss 4.367338 loss_att 7.400451 loss_ctc 5.422140 loss_rnnt 3.476607 hw_loss 0.269001 lr 0.00037747 rank 1
2023-02-24 23:59:04,190 DEBUG TRAIN Batch 21/300 loss 7.397751 loss_att 11.434542 loss_ctc 12.040006 loss_rnnt 5.860039 hw_loss 0.208851 lr 0.00037749 rank 2
2023-02-25 00:00:18,093 DEBUG TRAIN Batch 21/400 loss 10.338303 loss_att 11.247525 loss_ctc 16.454758 loss_rnnt 9.258070 hw_loss 0.155363 lr 0.00037739 rank 7
2023-02-25 00:00:18,102 DEBUG TRAIN Batch 21/400 loss 11.798968 loss_att 12.704773 loss_ctc 17.221363 loss_rnnt 10.780903 hw_loss 0.213597 lr 0.00037738 rank 0
2023-02-25 00:00:18,107 DEBUG TRAIN Batch 21/400 loss 11.915917 loss_att 13.935122 loss_ctc 17.116745 loss_rnnt 10.744684 hw_loss 0.138651 lr 0.00037740 rank 6
2023-02-25 00:00:18,108 DEBUG TRAIN Batch 21/400 loss 6.894040 loss_att 8.297752 loss_ctc 10.351481 loss_rnnt 6.079363 hw_loss 0.136768 lr 0.00037736 rank 1
2023-02-25 00:00:18,111 DEBUG TRAIN Batch 21/400 loss 7.434762 loss_att 9.076817 loss_ctc 10.510849 loss_rnnt 6.643079 hw_loss 0.099614 lr 0.00037744 rank 3
2023-02-25 00:00:18,115 DEBUG TRAIN Batch 21/400 loss 11.862923 loss_att 17.385687 loss_ctc 15.910270 loss_rnnt 10.092090 hw_loss 0.237439 lr 0.00037739 rank 2
2023-02-25 00:00:18,118 DEBUG TRAIN Batch 21/400 loss 6.472533 loss_att 9.764738 loss_ctc 9.311114 loss_rnnt 5.286973 hw_loss 0.278703 lr 0.00037739 rank 5
2023-02-25 00:00:18,123 DEBUG TRAIN Batch 21/400 loss 10.013907 loss_att 11.021745 loss_ctc 14.721576 loss_rnnt 9.114454 hw_loss 0.131616 lr 0.00037741 rank 4
2023-02-25 00:01:30,720 DEBUG TRAIN Batch 21/500 loss 14.882290 loss_att 18.986557 loss_ctc 20.468904 loss_rnnt 13.210980 hw_loss 0.197951 lr 0.00037727 rank 0
2023-02-25 00:01:30,726 DEBUG TRAIN Batch 21/500 loss 6.724204 loss_att 8.365688 loss_ctc 9.836829 loss_rnnt 5.832752 hw_loss 0.277758 lr 0.00037729 rank 7
2023-02-25 00:01:30,727 DEBUG TRAIN Batch 21/500 loss 7.043633 loss_att 10.567639 loss_ctc 10.240092 loss_rnnt 5.808375 hw_loss 0.195491 lr 0.00037729 rank 6
2023-02-25 00:01:30,728 DEBUG TRAIN Batch 21/500 loss 10.560499 loss_att 14.527205 loss_ctc 14.861879 loss_rnnt 9.088467 hw_loss 0.197200 lr 0.00037726 rank 1
2023-02-25 00:01:30,731 DEBUG TRAIN Batch 21/500 loss 10.569099 loss_att 14.663589 loss_ctc 14.791454 loss_rnnt 9.055985 hw_loss 0.246068 lr 0.00037728 rank 2
2023-02-25 00:01:30,733 DEBUG TRAIN Batch 21/500 loss 14.897179 loss_att 17.438465 loss_ctc 22.997025 loss_rnnt 13.176392 hw_loss 0.248532 lr 0.00037733 rank 3
2023-02-25 00:01:30,735 DEBUG TRAIN Batch 21/500 loss 8.454511 loss_att 10.616015 loss_ctc 11.946330 loss_rnnt 7.426269 hw_loss 0.244435 lr 0.00037728 rank 5
2023-02-25 00:01:30,738 DEBUG TRAIN Batch 21/500 loss 7.412234 loss_att 8.500788 loss_ctc 8.914907 loss_rnnt 6.833392 hw_loss 0.301454 lr 0.00037730 rank 4
2023-02-25 00:02:44,185 DEBUG TRAIN Batch 21/600 loss 7.448407 loss_att 9.933208 loss_ctc 11.550215 loss_rnnt 6.269665 hw_loss 0.252887 lr 0.00037722 rank 3
2023-02-25 00:02:44,189 DEBUG TRAIN Batch 21/600 loss 13.652095 loss_att 16.829779 loss_ctc 19.224493 loss_rnnt 12.100733 hw_loss 0.324071 lr 0.00037718 rank 6
2023-02-25 00:02:44,195 DEBUG TRAIN Batch 21/600 loss 1.488820 loss_att 2.990830 loss_ctc 2.548746 loss_rnnt 0.899924 hw_loss 0.275946 lr 0.00037715 rank 1
2023-02-25 00:02:44,195 DEBUG TRAIN Batch 21/600 loss 9.409263 loss_att 10.034861 loss_ctc 11.680545 loss_rnnt 8.782527 hw_loss 0.372710 lr 0.00037718 rank 7
2023-02-25 00:02:44,200 DEBUG TRAIN Batch 21/600 loss 5.922064 loss_att 7.483310 loss_ctc 7.084868 loss_rnnt 5.317484 hw_loss 0.257418 lr 0.00037717 rank 5
2023-02-25 00:02:44,208 DEBUG TRAIN Batch 21/600 loss 13.342474 loss_att 14.463259 loss_ctc 16.140564 loss_rnnt 12.579307 hw_loss 0.311122 lr 0.00037717 rank 0
2023-02-25 00:02:44,234 DEBUG TRAIN Batch 21/600 loss 8.041877 loss_att 8.890351 loss_ctc 10.622877 loss_rnnt 7.356744 hw_loss 0.321194 lr 0.00037717 rank 2
2023-02-25 00:02:44,244 DEBUG TRAIN Batch 21/600 loss 13.238585 loss_att 15.341397 loss_ctc 17.305059 loss_rnnt 12.146394 hw_loss 0.242687 lr 0.00037719 rank 4
2023-02-25 00:03:58,579 DEBUG TRAIN Batch 21/700 loss 6.685266 loss_att 10.888271 loss_ctc 11.765918 loss_rnnt 5.028527 hw_loss 0.260095 lr 0.00037708 rank 4
2023-02-25 00:03:58,584 DEBUG TRAIN Batch 21/700 loss 3.288494 loss_att 7.097836 loss_ctc 7.549157 loss_rnnt 1.833248 hw_loss 0.234917 lr 0.00037706 rank 0
2023-02-25 00:03:58,584 DEBUG TRAIN Batch 21/700 loss 6.163022 loss_att 9.730868 loss_ctc 9.286694 loss_rnnt 4.962289 hw_loss 0.132511 lr 0.00037707 rank 5
2023-02-25 00:03:58,587 DEBUG TRAIN Batch 21/700 loss 5.171691 loss_att 8.333925 loss_ctc 6.027645 loss_rnnt 4.292147 hw_loss 0.249320 lr 0.00037704 rank 1
2023-02-25 00:03:58,589 DEBUG TRAIN Batch 21/700 loss 8.116644 loss_att 11.354761 loss_ctc 11.078806 loss_rnnt 6.991184 hw_loss 0.155404 lr 0.00037708 rank 6
2023-02-25 00:03:58,592 DEBUG TRAIN Batch 21/700 loss 11.076727 loss_att 12.887468 loss_ctc 19.845512 loss_rnnt 9.399361 hw_loss 0.273836 lr 0.00037711 rank 3
2023-02-25 00:03:58,593 DEBUG TRAIN Batch 21/700 loss 6.005399 loss_att 9.873498 loss_ctc 9.000837 loss_rnnt 4.765134 hw_loss 0.126100 lr 0.00037707 rank 7
2023-02-25 00:03:58,610 DEBUG TRAIN Batch 21/700 loss 10.855710 loss_att 13.187433 loss_ctc 12.253824 loss_rnnt 10.142632 hw_loss 0.113095 lr 0.00037706 rank 2
2023-02-25 00:05:11,461 DEBUG TRAIN Batch 21/800 loss 4.159417 loss_att 7.503631 loss_ctc 8.032268 loss_rnnt 2.846150 hw_loss 0.240083 lr 0.00037697 rank 7
2023-02-25 00:05:11,461 DEBUG TRAIN Batch 21/800 loss 10.344172 loss_att 13.607540 loss_ctc 16.019379 loss_rnnt 8.814859 hw_loss 0.224897 lr 0.00037695 rank 0
2023-02-25 00:05:11,464 DEBUG TRAIN Batch 21/800 loss 4.411232 loss_att 7.903079 loss_ctc 9.321781 loss_rnnt 2.907523 hw_loss 0.282374 lr 0.00037701 rank 3
2023-02-25 00:05:11,464 DEBUG TRAIN Batch 21/800 loss 3.054812 loss_att 6.877604 loss_ctc 3.721584 loss_rnnt 2.123433 hw_loss 0.146097 lr 0.00037696 rank 2
2023-02-25 00:05:11,468 DEBUG TRAIN Batch 21/800 loss 15.475882 loss_att 18.012203 loss_ctc 21.515793 loss_rnnt 14.077044 hw_loss 0.161723 lr 0.00037694 rank 1
2023-02-25 00:05:11,468 DEBUG TRAIN Batch 21/800 loss 5.727523 loss_att 8.378770 loss_ctc 8.062708 loss_rnnt 4.739062 hw_loss 0.275350 lr 0.00037697 rank 6
2023-02-25 00:05:11,496 DEBUG TRAIN Batch 21/800 loss 2.856803 loss_att 6.178358 loss_ctc 3.814438 loss_rnnt 1.956359 hw_loss 0.203340 lr 0.00037696 rank 5
2023-02-25 00:05:11,501 DEBUG TRAIN Batch 21/800 loss 7.754758 loss_att 10.016469 loss_ctc 14.130766 loss_rnnt 6.334955 hw_loss 0.219988 lr 0.00037698 rank 4
2023-02-25 00:06:23,125 DEBUG TRAIN Batch 21/900 loss 4.676183 loss_att 6.561426 loss_ctc 6.795064 loss_rnnt 3.917933 hw_loss 0.185032 lr 0.00037684 rank 0
2023-02-25 00:06:23,125 DEBUG TRAIN Batch 21/900 loss 5.385877 loss_att 6.739389 loss_ctc 7.522078 loss_rnnt 4.708174 hw_loss 0.229074 lr 0.00037686 rank 7
2023-02-25 00:06:23,128 DEBUG TRAIN Batch 21/900 loss 8.378446 loss_att 12.395516 loss_ctc 14.228907 loss_rnnt 6.708791 hw_loss 0.161586 lr 0.00037683 rank 1
2023-02-25 00:06:23,127 DEBUG TRAIN Batch 21/900 loss 3.198013 loss_att 5.875819 loss_ctc 4.451847 loss_rnnt 2.390769 hw_loss 0.195945 lr 0.00037686 rank 6
2023-02-25 00:06:23,132 DEBUG TRAIN Batch 21/900 loss 9.263642 loss_att 11.948182 loss_ctc 8.370495 loss_rnnt 8.753577 hw_loss 0.172956 lr 0.00037690 rank 3
2023-02-25 00:06:23,132 DEBUG TRAIN Batch 21/900 loss 11.688495 loss_att 14.217133 loss_ctc 17.202450 loss_rnnt 10.318636 hw_loss 0.241756 lr 0.00037685 rank 2
2023-02-25 00:06:23,133 DEBUG TRAIN Batch 21/900 loss 5.882490 loss_att 8.718702 loss_ctc 7.479754 loss_rnnt 5.041227 hw_loss 0.114471 lr 0.00037685 rank 5
2023-02-25 00:06:23,132 DEBUG TRAIN Batch 21/900 loss 16.129227 loss_att 15.578931 loss_ctc 15.910926 loss_rnnt 16.216591 hw_loss 0.097126 lr 0.00037687 rank 4
2023-02-25 00:07:36,425 DEBUG TRAIN Batch 21/1000 loss 11.150486 loss_att 13.308380 loss_ctc 15.891251 loss_rnnt 9.969379 hw_loss 0.220175 lr 0.00037676 rank 4
2023-02-25 00:07:36,429 DEBUG TRAIN Batch 21/1000 loss 22.041363 loss_att 25.165745 loss_ctc 30.358158 loss_rnnt 20.155851 hw_loss 0.284490 lr 0.00037679 rank 3
2023-02-25 00:07:36,431 DEBUG TRAIN Batch 21/1000 loss 14.479347 loss_att 15.126684 loss_ctc 19.857351 loss_rnnt 13.512844 hw_loss 0.224941 lr 0.00037674 rank 5
2023-02-25 00:07:36,433 DEBUG TRAIN Batch 21/1000 loss 11.110228 loss_att 14.558508 loss_ctc 17.195768 loss_rnnt 9.473427 hw_loss 0.254511 lr 0.00037676 rank 6
2023-02-25 00:07:36,438 DEBUG TRAIN Batch 21/1000 loss 9.315081 loss_att 13.832609 loss_ctc 12.707378 loss_rnnt 7.868999 hw_loss 0.169254 lr 0.00037672 rank 1
2023-02-25 00:07:36,438 DEBUG TRAIN Batch 21/1000 loss 4.885977 loss_att 10.983126 loss_ctc 9.279779 loss_rnnt 2.924438 hw_loss 0.293004 lr 0.00037674 rank 0
2023-02-25 00:07:36,439 DEBUG TRAIN Batch 21/1000 loss 8.029054 loss_att 11.004262 loss_ctc 12.391398 loss_rnnt 6.741160 hw_loss 0.208511 lr 0.00037675 rank 7
2023-02-25 00:07:36,487 DEBUG TRAIN Batch 21/1000 loss 4.742392 loss_att 7.731122 loss_ctc 6.660286 loss_rnnt 3.752642 hw_loss 0.255534 lr 0.00037674 rank 2
2023-02-25 00:08:50,939 DEBUG TRAIN Batch 21/1100 loss 5.506458 loss_att 8.761143 loss_ctc 6.774873 loss_rnnt 4.560160 hw_loss 0.236699 lr 0.00037665 rank 6
2023-02-25 00:08:50,948 DEBUG TRAIN Batch 21/1100 loss 3.142302 loss_att 4.860027 loss_ctc 5.620955 loss_rnnt 2.369097 hw_loss 0.185948 lr 0.00037663 rank 0
2023-02-25 00:08:50,952 DEBUG TRAIN Batch 21/1100 loss 11.965072 loss_att 14.811457 loss_ctc 19.172342 loss_rnnt 10.298635 hw_loss 0.255355 lr 0.00037664 rank 2
2023-02-25 00:08:50,952 DEBUG TRAIN Batch 21/1100 loss 6.192600 loss_att 9.627872 loss_ctc 9.228176 loss_rnnt 5.005439 hw_loss 0.178806 lr 0.00037661 rank 1
2023-02-25 00:08:50,952 DEBUG TRAIN Batch 21/1100 loss 2.482288 loss_att 5.721238 loss_ctc 4.561393 loss_rnnt 1.432223 hw_loss 0.234489 lr 0.00037664 rank 7
2023-02-25 00:08:50,952 DEBUG TRAIN Batch 21/1100 loss 10.232091 loss_att 12.971449 loss_ctc 16.135365 loss_rnnt 8.814489 hw_loss 0.154925 lr 0.00037665 rank 4
2023-02-25 00:08:50,958 DEBUG TRAIN Batch 21/1100 loss 4.959742 loss_att 5.662913 loss_ctc 6.038174 loss_rnnt 4.569950 hw_loss 0.197561 lr 0.00037669 rank 3
2023-02-25 00:08:50,959 DEBUG TRAIN Batch 21/1100 loss 12.528469 loss_att 15.492400 loss_ctc 17.137985 loss_rnnt 11.234623 hw_loss 0.162109 lr 0.00037664 rank 5
2023-02-25 00:10:04,410 DEBUG TRAIN Batch 21/1200 loss 7.234161 loss_att 8.473698 loss_ctc 9.190828 loss_rnnt 6.590414 hw_loss 0.253034 lr 0.00037654 rank 6
2023-02-25 00:10:04,410 DEBUG TRAIN Batch 21/1200 loss 8.138243 loss_att 10.422746 loss_ctc 11.403077 loss_rnnt 7.131888 hw_loss 0.214017 lr 0.00037658 rank 3
2023-02-25 00:10:04,413 DEBUG TRAIN Batch 21/1200 loss 13.171412 loss_att 15.025740 loss_ctc 18.916162 loss_rnnt 11.903401 hw_loss 0.245960 lr 0.00037652 rank 0
2023-02-25 00:10:04,417 DEBUG TRAIN Batch 21/1200 loss 14.577728 loss_att 14.940557 loss_ctc 19.302895 loss_rnnt 13.726462 hw_loss 0.278774 lr 0.00037653 rank 2
2023-02-25 00:10:04,419 DEBUG TRAIN Batch 21/1200 loss 6.211193 loss_att 7.775673 loss_ctc 8.259463 loss_rnnt 5.530118 hw_loss 0.178268 lr 0.00037651 rank 1
2023-02-25 00:10:04,421 DEBUG TRAIN Batch 21/1200 loss 8.921560 loss_att 10.353544 loss_ctc 13.004486 loss_rnnt 7.934673 hw_loss 0.292688 lr 0.00037654 rank 7
2023-02-25 00:10:04,422 DEBUG TRAIN Batch 21/1200 loss 8.228196 loss_att 12.047707 loss_ctc 9.826433 loss_rnnt 7.101431 hw_loss 0.280806 lr 0.00037655 rank 4
2023-02-25 00:10:04,421 DEBUG TRAIN Batch 21/1200 loss 9.130505 loss_att 10.745813 loss_ctc 11.771071 loss_rnnt 8.335431 hw_loss 0.224878 lr 0.00037653 rank 5
2023-02-25 00:11:17,339 DEBUG TRAIN Batch 21/1300 loss 11.510825 loss_att 22.412567 loss_ctc 14.763427 loss_rnnt 8.848673 hw_loss 0.090233 lr 0.00037642 rank 0
2023-02-25 00:11:17,343 DEBUG TRAIN Batch 21/1300 loss 2.876251 loss_att 6.592706 loss_ctc 5.748348 loss_rnnt 1.660086 hw_loss 0.168615 lr 0.00037647 rank 3
2023-02-25 00:11:17,345 DEBUG TRAIN Batch 21/1300 loss 4.130280 loss_att 7.614595 loss_ctc 6.218777 loss_rnnt 3.040117 hw_loss 0.215313 lr 0.00037643 rank 7
2023-02-25 00:11:17,350 DEBUG TRAIN Batch 21/1300 loss 7.948922 loss_att 13.417521 loss_ctc 12.309874 loss_rnnt 6.169032 hw_loss 0.196331 lr 0.00037642 rank 2
2023-02-25 00:11:17,351 DEBUG TRAIN Batch 21/1300 loss 7.080696 loss_att 7.436116 loss_ctc 9.689262 loss_rnnt 6.485092 hw_loss 0.331334 lr 0.00037640 rank 1
2023-02-25 00:11:17,352 DEBUG TRAIN Batch 21/1300 loss 7.766041 loss_att 10.216662 loss_ctc 14.150882 loss_rnnt 6.346205 hw_loss 0.147001 lr 0.00037642 rank 5
2023-02-25 00:11:17,383 DEBUG TRAIN Batch 21/1300 loss 7.357914 loss_att 10.064828 loss_ctc 12.400909 loss_rnnt 6.038598 hw_loss 0.197876 lr 0.00037644 rank 4
2023-02-25 00:11:17,408 DEBUG TRAIN Batch 21/1300 loss 8.148336 loss_att 11.228140 loss_ctc 7.438351 loss_rnnt 7.546710 hw_loss 0.150620 lr 0.00037643 rank 6
2023-02-25 00:12:32,407 DEBUG TRAIN Batch 21/1400 loss 10.871830 loss_att 13.088093 loss_ctc 12.709395 loss_rnnt 10.042887 hw_loss 0.263779 lr 0.00037633 rank 6
2023-02-25 00:12:32,408 DEBUG TRAIN Batch 21/1400 loss 7.263350 loss_att 9.717932 loss_ctc 10.301331 loss_rnnt 6.295711 hw_loss 0.134359 lr 0.00037632 rank 7
2023-02-25 00:12:32,409 DEBUG TRAIN Batch 21/1400 loss 4.289805 loss_att 7.757250 loss_ctc 3.289693 loss_rnnt 3.591724 hw_loss 0.258639 lr 0.00037631 rank 0
2023-02-25 00:12:32,411 DEBUG TRAIN Batch 21/1400 loss 4.734559 loss_att 7.082392 loss_ctc 5.952248 loss_rnnt 3.990686 hw_loss 0.209903 lr 0.00037633 rank 4
2023-02-25 00:12:32,412 DEBUG TRAIN Batch 21/1400 loss 11.163897 loss_att 14.987365 loss_ctc 18.709019 loss_rnnt 9.280605 hw_loss 0.211089 lr 0.00037632 rank 5
2023-02-25 00:12:32,432 DEBUG TRAIN Batch 21/1400 loss 5.450652 loss_att 7.050117 loss_ctc 6.790359 loss_rnnt 4.836233 hw_loss 0.217307 lr 0.00037637 rank 3
2023-02-25 00:12:32,438 DEBUG TRAIN Batch 21/1400 loss 13.632004 loss_att 15.968512 loss_ctc 17.723419 loss_rnnt 12.476188 hw_loss 0.268112 lr 0.00037632 rank 2
2023-02-25 00:12:32,460 DEBUG TRAIN Batch 21/1400 loss 13.849787 loss_att 18.143665 loss_ctc 20.297626 loss_rnnt 12.045082 hw_loss 0.161658 lr 0.00037629 rank 1
2023-02-25 00:13:45,722 DEBUG TRAIN Batch 21/1500 loss 13.824518 loss_att 14.797625 loss_ctc 16.991350 loss_rnnt 13.148822 hw_loss 0.110309 lr 0.00037620 rank 0
2023-02-25 00:13:45,727 DEBUG TRAIN Batch 21/1500 loss 6.675012 loss_att 12.633191 loss_ctc 7.912880 loss_rnnt 5.215057 hw_loss 0.193629 lr 0.00037622 rank 7
2023-02-25 00:13:45,727 DEBUG TRAIN Batch 21/1500 loss 10.495738 loss_att 13.404297 loss_ctc 17.814569 loss_rnnt 8.823711 hw_loss 0.214632 lr 0.00037626 rank 3
2023-02-25 00:13:45,728 DEBUG TRAIN Batch 21/1500 loss 13.686103 loss_att 18.622532 loss_ctc 15.192169 loss_rnnt 12.350371 hw_loss 0.276821 lr 0.00037622 rank 6
2023-02-25 00:13:45,730 DEBUG TRAIN Batch 21/1500 loss 15.852675 loss_att 20.070688 loss_ctc 21.539055 loss_rnnt 14.141455 hw_loss 0.205190 lr 0.00037619 rank 1
2023-02-25 00:13:45,733 DEBUG TRAIN Batch 21/1500 loss 11.033269 loss_att 13.043520 loss_ctc 17.179087 loss_rnnt 9.702386 hw_loss 0.205107 lr 0.00037621 rank 5
2023-02-25 00:13:45,748 DEBUG TRAIN Batch 21/1500 loss 7.365020 loss_att 10.605893 loss_ctc 13.784357 loss_rnnt 5.752701 hw_loss 0.202935 lr 0.00037621 rank 2
2023-02-25 00:13:45,752 DEBUG TRAIN Batch 21/1500 loss 5.339036 loss_att 8.833954 loss_ctc 7.484520 loss_rnnt 4.275925 hw_loss 0.146370 lr 0.00037623 rank 4
2023-02-25 00:14:58,361 DEBUG TRAIN Batch 21/1600 loss 11.557386 loss_att 17.712889 loss_ctc 16.048084 loss_rnnt 9.619352 hw_loss 0.202828 lr 0.00037610 rank 0
2023-02-25 00:14:58,369 DEBUG TRAIN Batch 21/1600 loss 6.368274 loss_att 8.860149 loss_ctc 9.477940 loss_rnnt 5.368493 hw_loss 0.162720 lr 0.00037608 rank 1
2023-02-25 00:14:58,372 DEBUG TRAIN Batch 21/1600 loss 11.641812 loss_att 13.678124 loss_ctc 17.671021 loss_rnnt 10.325999 hw_loss 0.196230 lr 0.00037615 rank 3
2023-02-25 00:14:58,374 DEBUG TRAIN Batch 21/1600 loss 5.777829 loss_att 9.857281 loss_ctc 6.165860 loss_rnnt 4.862511 hw_loss 0.089418 lr 0.00037610 rank 2
2023-02-25 00:14:58,375 DEBUG TRAIN Batch 21/1600 loss 4.337774 loss_att 7.213763 loss_ctc 7.194175 loss_rnnt 3.243857 hw_loss 0.258497 lr 0.00037611 rank 7
2023-02-25 00:14:58,377 DEBUG TRAIN Batch 21/1600 loss 4.772569 loss_att 7.621949 loss_ctc 6.899460 loss_rnnt 3.813638 hw_loss 0.197754 lr 0.00037610 rank 5
2023-02-25 00:14:58,377 DEBUG TRAIN Batch 21/1600 loss 6.491806 loss_att 8.523253 loss_ctc 9.305986 loss_rnnt 5.567325 hw_loss 0.268065 lr 0.00037612 rank 4
2023-02-25 00:14:58,418 DEBUG TRAIN Batch 21/1600 loss 5.755568 loss_att 7.859743 loss_ctc 8.080304 loss_rnnt 4.921739 hw_loss 0.193181 lr 0.00037612 rank 6
2023-02-25 00:16:11,675 DEBUG TRAIN Batch 21/1700 loss 10.431745 loss_att 12.938238 loss_ctc 14.455848 loss_rnnt 9.292584 hw_loss 0.189962 lr 0.00037600 rank 2
2023-02-25 00:16:11,682 DEBUG TRAIN Batch 21/1700 loss 5.800915 loss_att 8.801546 loss_ctc 8.815550 loss_rnnt 4.743341 hw_loss 0.104055 lr 0.00037602 rank 4
2023-02-25 00:16:11,694 DEBUG TRAIN Batch 21/1700 loss 8.432615 loss_att 10.998143 loss_ctc 9.664852 loss_rnnt 7.621452 hw_loss 0.250800 lr 0.00037600 rank 7
2023-02-25 00:16:11,694 DEBUG TRAIN Batch 21/1700 loss 19.751146 loss_att 21.415176 loss_ctc 29.576740 loss_rnnt 17.989967 hw_loss 0.221800 lr 0.00037605 rank 3
2023-02-25 00:16:11,693 DEBUG TRAIN Batch 21/1700 loss 13.252834 loss_att 15.156462 loss_ctc 18.217909 loss_rnnt 12.107969 hw_loss 0.191493 lr 0.00037599 rank 0
2023-02-25 00:16:11,695 DEBUG TRAIN Batch 21/1700 loss 6.048660 loss_att 9.560217 loss_ctc 14.192989 loss_rnnt 4.165894 hw_loss 0.177271 lr 0.00037597 rank 1
2023-02-25 00:16:11,698 DEBUG TRAIN Batch 21/1700 loss 15.587076 loss_att 17.151590 loss_ctc 20.133799 loss_rnnt 14.519089 hw_loss 0.279101 lr 0.00037601 rank 6
2023-02-25 00:16:11,699 DEBUG TRAIN Batch 21/1700 loss 13.936143 loss_att 17.886475 loss_ctc 19.915512 loss_rnnt 12.246893 hw_loss 0.191129 lr 0.00037600 rank 5
2023-02-25 00:17:27,424 DEBUG TRAIN Batch 21/1800 loss 10.138592 loss_att 13.007999 loss_ctc 15.723670 loss_rnnt 8.732533 hw_loss 0.164065 lr 0.00037588 rank 0
2023-02-25 00:17:27,424 DEBUG TRAIN Batch 21/1800 loss 7.018630 loss_att 8.049955 loss_ctc 8.309347 loss_rnnt 6.538229 hw_loss 0.191325 lr 0.00037587 rank 1
2023-02-25 00:17:27,426 DEBUG TRAIN Batch 21/1800 loss 8.353091 loss_att 10.419494 loss_ctc 13.955867 loss_rnnt 7.043775 hw_loss 0.279371 lr 0.00037589 rank 2
2023-02-25 00:17:27,426 DEBUG TRAIN Batch 21/1800 loss 7.506404 loss_att 10.028903 loss_ctc 10.303805 loss_rnnt 6.491946 hw_loss 0.256822 lr 0.00037594 rank 3
2023-02-25 00:17:27,426 DEBUG TRAIN Batch 21/1800 loss 5.227786 loss_att 6.371508 loss_ctc 8.903680 loss_rnnt 4.358139 hw_loss 0.282719 lr 0.00037591 rank 4
2023-02-25 00:17:27,426 DEBUG TRAIN Batch 21/1800 loss 8.114796 loss_att 10.967863 loss_ctc 10.986175 loss_rnnt 7.009990 hw_loss 0.283766 lr 0.00037590 rank 6
2023-02-25 00:17:27,427 DEBUG TRAIN Batch 21/1800 loss 6.728526 loss_att 9.130782 loss_ctc 12.930901 loss_rnnt 5.317397 hw_loss 0.194425 lr 0.00037590 rank 7
2023-02-25 00:17:27,428 DEBUG TRAIN Batch 21/1800 loss 3.010829 loss_att 5.663708 loss_ctc 4.669437 loss_rnnt 2.123178 hw_loss 0.254863 lr 0.00037589 rank 5
2023-02-25 00:18:39,845 DEBUG TRAIN Batch 21/1900 loss 8.580141 loss_att 11.348163 loss_ctc 15.839720 loss_rnnt 7.025478 hw_loss 0.062092 lr 0.00037579 rank 7
2023-02-25 00:18:39,857 DEBUG TRAIN Batch 21/1900 loss 13.221110 loss_att 17.667854 loss_ctc 21.712715 loss_rnnt 11.093933 hw_loss 0.198028 lr 0.00037578 rank 0
2023-02-25 00:18:39,860 DEBUG TRAIN Batch 21/1900 loss 6.177611 loss_att 8.066483 loss_ctc 7.353105 loss_rnnt 5.554736 hw_loss 0.165691 lr 0.00037578 rank 2
2023-02-25 00:18:39,860 DEBUG TRAIN Batch 21/1900 loss 7.879585 loss_att 7.913257 loss_ctc 9.694412 loss_rnnt 7.456560 hw_loss 0.326837 lr 0.00037580 rank 6
2023-02-25 00:18:39,865 DEBUG TRAIN Batch 21/1900 loss 9.958938 loss_att 12.698581 loss_ctc 17.233221 loss_rnnt 8.288888 hw_loss 0.285406 lr 0.00037576 rank 1
2023-02-25 00:18:39,868 DEBUG TRAIN Batch 21/1900 loss 8.483282 loss_att 10.086561 loss_ctc 14.455550 loss_rnnt 7.294082 hw_loss 0.135454 lr 0.00037580 rank 4
2023-02-25 00:18:39,872 DEBUG TRAIN Batch 21/1900 loss 8.884212 loss_att 12.009586 loss_ctc 12.825725 loss_rnnt 7.630333 hw_loss 0.193630 lr 0.00037579 rank 5
2023-02-25 00:18:39,912 DEBUG TRAIN Batch 21/1900 loss 10.646921 loss_att 14.182985 loss_ctc 15.422222 loss_rnnt 9.142013 hw_loss 0.301855 lr 0.00037583 rank 3
2023-02-25 00:19:53,077 DEBUG TRAIN Batch 21/2000 loss 7.630767 loss_att 11.468297 loss_ctc 9.198932 loss_rnnt 6.535676 hw_loss 0.222179 lr 0.00037567 rank 0
2023-02-25 00:19:53,079 DEBUG TRAIN Batch 21/2000 loss 11.026823 loss_att 15.590697 loss_ctc 19.885979 loss_rnnt 8.847079 hw_loss 0.160778 lr 0.00037566 rank 1
2023-02-25 00:19:53,086 DEBUG TRAIN Batch 21/2000 loss 9.574394 loss_att 10.460860 loss_ctc 12.610908 loss_rnnt 8.884547 hw_loss 0.201908 lr 0.00037570 rank 4
2023-02-25 00:19:53,088 DEBUG TRAIN Batch 21/2000 loss 11.338131 loss_att 13.046993 loss_ctc 15.568332 loss_rnnt 10.299415 hw_loss 0.249218 lr 0.00037569 rank 7
2023-02-25 00:19:53,089 DEBUG TRAIN Batch 21/2000 loss 3.668557 loss_att 5.961410 loss_ctc 4.592191 loss_rnnt 2.946774 hw_loss 0.262614 lr 0.00037568 rank 5
2023-02-25 00:19:53,090 DEBUG TRAIN Batch 21/2000 loss 3.971678 loss_att 7.373344 loss_ctc 5.731072 loss_rnnt 2.940191 hw_loss 0.218565 lr 0.00037573 rank 3
2023-02-25 00:19:53,091 DEBUG TRAIN Batch 21/2000 loss 2.528670 loss_att 5.626450 loss_ctc 5.402460 loss_rnnt 1.419193 hw_loss 0.200156 lr 0.00037568 rank 2
2023-02-25 00:19:53,090 DEBUG TRAIN Batch 21/2000 loss 9.986533 loss_att 15.150352 loss_ctc 13.531802 loss_rnnt 8.361245 hw_loss 0.224665 lr 0.00037569 rank 6
2023-02-25 00:21:07,025 DEBUG TRAIN Batch 21/2100 loss 3.598930 loss_att 7.495513 loss_ctc 6.969912 loss_rnnt 2.297409 hw_loss 0.136389 lr 0.00037557 rank 5
2023-02-25 00:21:07,029 DEBUG TRAIN Batch 21/2100 loss 9.780967 loss_att 15.111929 loss_ctc 14.323778 loss_rnnt 8.053406 hw_loss 0.104363 lr 0.00037555 rank 1
2023-02-25 00:21:07,032 DEBUG TRAIN Batch 21/2100 loss 11.698506 loss_att 15.559109 loss_ctc 15.066138 loss_rnnt 10.402255 hw_loss 0.140839 lr 0.00037562 rank 3
2023-02-25 00:21:07,032 DEBUG TRAIN Batch 21/2100 loss 8.133404 loss_att 11.330769 loss_ctc 14.121819 loss_rnnt 6.588623 hw_loss 0.200349 lr 0.00037557 rank 2
2023-02-25 00:21:07,033 DEBUG TRAIN Batch 21/2100 loss 11.589253 loss_att 12.601191 loss_ctc 20.264797 loss_rnnt 10.123767 hw_loss 0.199425 lr 0.00037559 rank 4
2023-02-25 00:21:07,037 DEBUG TRAIN Batch 21/2100 loss 7.215348 loss_att 11.618210 loss_ctc 13.776724 loss_rnnt 5.401208 hw_loss 0.110094 lr 0.00037557 rank 0
2023-02-25 00:21:07,039 DEBUG TRAIN Batch 21/2100 loss 4.596546 loss_att 10.451578 loss_ctc 6.836719 loss_rnnt 3.065907 hw_loss 0.114268 lr 0.00037558 rank 7
2023-02-25 00:21:07,053 DEBUG TRAIN Batch 21/2100 loss 16.954773 loss_att 19.993761 loss_ctc 21.284622 loss_rnnt 15.698683 hw_loss 0.133088 lr 0.00037558 rank 6
2023-02-25 00:22:20,577 DEBUG TRAIN Batch 21/2200 loss 6.844110 loss_att 9.583200 loss_ctc 12.069910 loss_rnnt 5.530462 hw_loss 0.129481 lr 0.00037544 rank 1
2023-02-25 00:22:20,577 DEBUG TRAIN Batch 21/2200 loss 18.122740 loss_att 22.743484 loss_ctc 29.319998 loss_rnnt 15.593966 hw_loss 0.209356 lr 0.00037546 rank 0
2023-02-25 00:22:20,578 DEBUG TRAIN Batch 21/2200 loss 4.168928 loss_att 7.950171 loss_ctc 5.203802 loss_rnnt 3.180617 hw_loss 0.176398 lr 0.00037548 rank 6
2023-02-25 00:22:20,581 DEBUG TRAIN Batch 21/2200 loss 20.511864 loss_att 26.363972 loss_ctc 26.862839 loss_rnnt 18.426243 hw_loss 0.128256 lr 0.00037547 rank 2
2023-02-25 00:22:20,581 DEBUG TRAIN Batch 21/2200 loss 4.480422 loss_att 5.633147 loss_ctc 4.970442 loss_rnnt 4.108847 hw_loss 0.141926 lr 0.00037552 rank 3
2023-02-25 00:22:20,584 DEBUG TRAIN Batch 21/2200 loss 19.024235 loss_att 21.474682 loss_ctc 21.691368 loss_rnnt 18.073162 hw_loss 0.197560 lr 0.00037547 rank 7
2023-02-25 00:22:20,589 DEBUG TRAIN Batch 21/2200 loss 19.681465 loss_att 22.827629 loss_ctc 27.131645 loss_rnnt 17.971207 hw_loss 0.164375 lr 0.00037547 rank 5
2023-02-25 00:22:20,635 DEBUG TRAIN Batch 21/2200 loss 5.323113 loss_att 9.428998 loss_ctc 10.151293 loss_rnnt 3.756114 hw_loss 0.191370 lr 0.00037548 rank 4
2023-02-25 00:23:33,367 DEBUG TRAIN Batch 21/2300 loss 10.189677 loss_att 12.498335 loss_ctc 15.249388 loss_rnnt 8.958260 hw_loss 0.178233 lr 0.00037535 rank 0
2023-02-25 00:23:33,371 DEBUG TRAIN Batch 21/2300 loss 9.132890 loss_att 14.723131 loss_ctc 11.747767 loss_rnnt 7.579676 hw_loss 0.162212 lr 0.00037538 rank 4
2023-02-25 00:23:33,372 DEBUG TRAIN Batch 21/2300 loss 8.834796 loss_att 14.560131 loss_ctc 12.102153 loss_rnnt 7.077961 hw_loss 0.330224 lr 0.00037541 rank 3
2023-02-25 00:23:33,373 DEBUG TRAIN Batch 21/2300 loss 8.986720 loss_att 12.084637 loss_ctc 14.935024 loss_rnnt 7.468802 hw_loss 0.197301 lr 0.00037537 rank 6
2023-02-25 00:23:33,375 DEBUG TRAIN Batch 21/2300 loss 17.873327 loss_att 20.269901 loss_ctc 21.798336 loss_rnnt 16.805197 hw_loss 0.122782 lr 0.00037537 rank 7
2023-02-25 00:23:33,376 DEBUG TRAIN Batch 21/2300 loss 7.270916 loss_att 8.521100 loss_ctc 9.231718 loss_rnnt 6.658199 hw_loss 0.189825 lr 0.00037534 rank 1
2023-02-25 00:23:33,379 DEBUG TRAIN Batch 21/2300 loss 18.104834 loss_att 19.545635 loss_ctc 25.354036 loss_rnnt 16.710239 hw_loss 0.262266 lr 0.00037536 rank 5
2023-02-25 00:23:33,381 DEBUG TRAIN Batch 21/2300 loss 6.775544 loss_att 8.806844 loss_ctc 10.477121 loss_rnnt 5.758146 hw_loss 0.220489 lr 0.00037536 rank 2
2023-02-25 00:24:46,592 DEBUG TRAIN Batch 21/2400 loss 13.873060 loss_att 16.554493 loss_ctc 18.201567 loss_rnnt 12.709463 hw_loss 0.094079 lr 0.00037523 rank 1
2023-02-25 00:24:46,603 DEBUG TRAIN Batch 21/2400 loss 13.609480 loss_att 16.744898 loss_ctc 19.385057 loss_rnnt 12.102711 hw_loss 0.205515 lr 0.00037530 rank 3
2023-02-25 00:24:46,605 DEBUG TRAIN Batch 21/2400 loss 11.024433 loss_att 14.220049 loss_ctc 17.074936 loss_rnnt 9.456200 hw_loss 0.229454 lr 0.00037525 rank 0
2023-02-25 00:24:46,607 DEBUG TRAIN Batch 21/2400 loss 5.617774 loss_att 7.520798 loss_ctc 9.754189 loss_rnnt 4.530876 hw_loss 0.290196 lr 0.00037527 rank 4
2023-02-25 00:24:46,609 DEBUG TRAIN Batch 21/2400 loss 6.695982 loss_att 9.317288 loss_ctc 9.425485 loss_rnnt 5.720632 hw_loss 0.163415 lr 0.00037527 rank 6
2023-02-25 00:24:46,609 DEBUG TRAIN Batch 21/2400 loss 5.746262 loss_att 9.883486 loss_ctc 10.228985 loss_rnnt 4.236416 hw_loss 0.158821 lr 0.00037526 rank 7
2023-02-25 00:24:46,613 DEBUG TRAIN Batch 21/2400 loss 20.969862 loss_att 23.249357 loss_ctc 32.225277 loss_rnnt 18.899256 hw_loss 0.213721 lr 0.00037526 rank 5
2023-02-25 00:24:46,614 DEBUG TRAIN Batch 21/2400 loss 4.281071 loss_att 6.283091 loss_ctc 5.517669 loss_rnnt 3.588499 hw_loss 0.238665 lr 0.00037526 rank 2
2023-02-25 00:26:03,436 DEBUG TRAIN Batch 21/2500 loss 12.974486 loss_att 15.235579 loss_ctc 19.284529 loss_rnnt 11.521839 hw_loss 0.298294 lr 0.00037520 rank 3
2023-02-25 00:26:03,437 DEBUG TRAIN Batch 21/2500 loss 14.330225 loss_att 18.247625 loss_ctc 20.711517 loss_rnnt 12.597210 hw_loss 0.185056 lr 0.00037514 rank 0
2023-02-25 00:26:03,438 DEBUG TRAIN Batch 21/2500 loss 9.213923 loss_att 9.793034 loss_ctc 12.807565 loss_rnnt 8.462018 hw_loss 0.294242 lr 0.00037516 rank 6
2023-02-25 00:26:03,441 DEBUG TRAIN Batch 21/2500 loss 9.004796 loss_att 12.451487 loss_ctc 12.171423 loss_rnnt 7.763185 hw_loss 0.243854 lr 0.00037513 rank 1
2023-02-25 00:26:03,442 DEBUG TRAIN Batch 21/2500 loss 10.535677 loss_att 10.176575 loss_ctc 13.443052 loss_rnnt 10.115126 hw_loss 0.196352 lr 0.00037516 rank 7
2023-02-25 00:26:03,447 DEBUG TRAIN Batch 21/2500 loss 19.398521 loss_att 17.955221 loss_ctc 25.416464 loss_rnnt 18.724695 hw_loss 0.300173 lr 0.00037515 rank 5
2023-02-25 00:26:03,452 DEBUG TRAIN Batch 21/2500 loss 9.647079 loss_att 9.108197 loss_ctc 14.084795 loss_rnnt 9.033863 hw_loss 0.242432 lr 0.00037517 rank 4
2023-02-25 00:26:03,476 DEBUG TRAIN Batch 21/2500 loss 3.805225 loss_att 7.702405 loss_ctc 7.485063 loss_rnnt 2.460001 hw_loss 0.140893 lr 0.00037515 rank 2
2023-02-25 00:27:17,971 DEBUG TRAIN Batch 21/2600 loss 10.204195 loss_att 10.696725 loss_ctc 13.651201 loss_rnnt 9.525040 hw_loss 0.226967 lr 0.00037504 rank 0
2023-02-25 00:27:17,977 DEBUG TRAIN Batch 21/2600 loss 9.431786 loss_att 11.796209 loss_ctc 12.519862 loss_rnnt 8.384865 hw_loss 0.304299 lr 0.00037506 rank 6
2023-02-25 00:27:17,977 DEBUG TRAIN Batch 21/2600 loss 7.164259 loss_att 7.511661 loss_ctc 10.092815 loss_rnnt 6.539276 hw_loss 0.309428 lr 0.00037502 rank 1
2023-02-25 00:27:17,977 DEBUG TRAIN Batch 21/2600 loss 11.834251 loss_att 17.133852 loss_ctc 18.007635 loss_rnnt 9.868771 hw_loss 0.154580 lr 0.00037505 rank 5
2023-02-25 00:27:17,979 DEBUG TRAIN Batch 21/2600 loss 6.775689 loss_att 10.750014 loss_ctc 9.143397 loss_rnnt 5.583007 hw_loss 0.153982 lr 0.00037504 rank 2
2023-02-25 00:27:17,982 DEBUG TRAIN Batch 21/2600 loss 6.026067 loss_att 11.556068 loss_ctc 6.689119 loss_rnnt 4.722621 hw_loss 0.204448 lr 0.00037506 rank 4
2023-02-25 00:27:18,007 DEBUG TRAIN Batch 21/2600 loss 4.880999 loss_att 7.956020 loss_ctc 8.433814 loss_rnnt 3.676944 hw_loss 0.216266 lr 0.00037505 rank 7
2023-02-25 00:27:18,030 DEBUG TRAIN Batch 21/2600 loss 11.601854 loss_att 11.885690 loss_ctc 16.552790 loss_rnnt 10.714960 hw_loss 0.318753 lr 0.00037509 rank 3
2023-02-25 00:28:30,976 DEBUG TRAIN Batch 21/2700 loss 7.190315 loss_att 10.072156 loss_ctc 9.000515 loss_rnnt 6.236546 hw_loss 0.255076 lr 0.00037495 rank 7
2023-02-25 00:28:30,985 DEBUG TRAIN Batch 21/2700 loss 8.656665 loss_att 12.968302 loss_ctc 11.382083 loss_rnnt 7.319923 hw_loss 0.208172 lr 0.00037493 rank 0
2023-02-25 00:28:30,990 DEBUG TRAIN Batch 21/2700 loss 4.159402 loss_att 8.903160 loss_ctc 6.556168 loss_rnnt 2.771312 hw_loss 0.224569 lr 0.00037499 rank 3
2023-02-25 00:28:30,992 DEBUG TRAIN Batch 21/2700 loss 2.701669 loss_att 5.446191 loss_ctc 3.770427 loss_rnnt 1.914452 hw_loss 0.179647 lr 0.00037496 rank 4
2023-02-25 00:28:30,995 DEBUG TRAIN Batch 21/2700 loss 4.300478 loss_att 5.997383 loss_ctc 4.557164 loss_rnnt 3.829849 hw_loss 0.181920 lr 0.00037494 rank 2
2023-02-25 00:28:30,998 DEBUG TRAIN Batch 21/2700 loss 4.707032 loss_att 9.292825 loss_ctc 9.487102 loss_rnnt 3.012771 hw_loss 0.262051 lr 0.00037492 rank 1
2023-02-25 00:28:31,002 DEBUG TRAIN Batch 21/2700 loss 13.024067 loss_att 15.249931 loss_ctc 16.398823 loss_rnnt 12.044556 hw_loss 0.158195 lr 0.00037495 rank 6
2023-02-25 00:28:31,041 DEBUG TRAIN Batch 21/2700 loss 3.633188 loss_att 6.010690 loss_ctc 5.072018 loss_rnnt 2.864109 hw_loss 0.190754 lr 0.00037494 rank 5
2023-02-25 00:29:45,232 DEBUG TRAIN Batch 21/2800 loss 3.846899 loss_att 7.230188 loss_ctc 6.296917 loss_rnnt 2.733956 hw_loss 0.205527 lr 0.00037484 rank 6
2023-02-25 00:29:45,238 DEBUG TRAIN Batch 21/2800 loss 9.129820 loss_att 15.161097 loss_ctc 16.050980 loss_rnnt 6.869968 hw_loss 0.245204 lr 0.00037481 rank 1
2023-02-25 00:29:45,238 DEBUG TRAIN Batch 21/2800 loss 5.489551 loss_att 7.974951 loss_ctc 7.432443 loss_rnnt 4.639260 hw_loss 0.176545 lr 0.00037483 rank 0
2023-02-25 00:29:45,239 DEBUG TRAIN Batch 21/2800 loss 4.339133 loss_att 7.085850 loss_ctc 7.604321 loss_rnnt 3.292270 hw_loss 0.116551 lr 0.00037488 rank 3
2023-02-25 00:29:45,245 DEBUG TRAIN Batch 21/2800 loss 4.453848 loss_att 8.085762 loss_ctc 7.601821 loss_rnnt 3.213728 hw_loss 0.176263 lr 0.00037484 rank 7
2023-02-25 00:29:45,249 DEBUG TRAIN Batch 21/2800 loss 11.765384 loss_att 14.381018 loss_ctc 17.645201 loss_rnnt 10.350273 hw_loss 0.202515 lr 0.00037483 rank 5
2023-02-25 00:29:45,260 DEBUG TRAIN Batch 21/2800 loss 13.921583 loss_att 17.475466 loss_ctc 16.287413 loss_rnnt 12.813085 hw_loss 0.154272 lr 0.00037483 rank 2
2023-02-25 00:29:45,294 DEBUG TRAIN Batch 21/2800 loss 6.049357 loss_att 9.759682 loss_ctc 9.437864 loss_rnnt 4.758255 hw_loss 0.182317 lr 0.00037485 rank 4
2023-02-25 00:30:59,097 DEBUG TRAIN Batch 21/2900 loss 5.725239 loss_att 8.757117 loss_ctc 10.695907 loss_rnnt 4.329381 hw_loss 0.237614 lr 0.00037472 rank 0
2023-02-25 00:30:59,098 DEBUG TRAIN Batch 21/2900 loss 17.476465 loss_att 19.018345 loss_ctc 22.728161 loss_rnnt 16.338921 hw_loss 0.241773 lr 0.00037474 rank 7
2023-02-25 00:30:59,102 DEBUG TRAIN Batch 21/2900 loss 7.953109 loss_att 12.846704 loss_ctc 13.131371 loss_rnnt 6.195684 hw_loss 0.165507 lr 0.00037471 rank 1
2023-02-25 00:30:59,104 DEBUG TRAIN Batch 21/2900 loss 12.096297 loss_att 14.655400 loss_ctc 14.848378 loss_rnnt 11.065949 hw_loss 0.284219 lr 0.00037474 rank 6
2023-02-25 00:30:59,104 DEBUG TRAIN Batch 21/2900 loss 15.795640 loss_att 20.719902 loss_ctc 21.881025 loss_rnnt 13.872470 hw_loss 0.237999 lr 0.00037478 rank 3
2023-02-25 00:30:59,107 DEBUG TRAIN Batch 21/2900 loss 4.492063 loss_att 6.462929 loss_ctc 5.445433 loss_rnnt 3.870086 hw_loss 0.188788 lr 0.00037473 rank 5
2023-02-25 00:30:59,113 DEBUG TRAIN Batch 21/2900 loss 5.876894 loss_att 8.052659 loss_ctc 8.144223 loss_rnnt 5.007948 hw_loss 0.246530 lr 0.00037475 rank 4
2023-02-25 00:30:59,153 DEBUG TRAIN Batch 21/2900 loss 9.480556 loss_att 12.326403 loss_ctc 16.029015 loss_rnnt 7.939065 hw_loss 0.185991 lr 0.00037473 rank 2
2023-02-25 00:32:11,450 DEBUG TRAIN Batch 21/3000 loss 8.623815 loss_att 10.362490 loss_ctc 10.101255 loss_rnnt 7.955312 hw_loss 0.232080 lr 0.00037462 rank 0
2023-02-25 00:32:11,451 DEBUG TRAIN Batch 21/3000 loss 4.392584 loss_att 6.996606 loss_ctc 6.486341 loss_rnnt 3.513972 hw_loss 0.147448 lr 0.00037467 rank 3
2023-02-25 00:32:11,452 DEBUG TRAIN Batch 21/3000 loss 7.429888 loss_att 8.893030 loss_ctc 9.410500 loss_rnnt 6.719171 hw_loss 0.288764 lr 0.00037462 rank 2
2023-02-25 00:32:11,451 DEBUG TRAIN Batch 21/3000 loss 5.688473 loss_att 7.883759 loss_ctc 7.038250 loss_rnnt 4.981934 hw_loss 0.164086 lr 0.00037460 rank 1
2023-02-25 00:32:11,454 DEBUG TRAIN Batch 21/3000 loss 3.472753 loss_att 6.210137 loss_ctc 6.221700 loss_rnnt 2.447065 hw_loss 0.209408 lr 0.00037463 rank 6
2023-02-25 00:32:11,455 DEBUG TRAIN Batch 21/3000 loss 9.136243 loss_att 12.680511 loss_ctc 15.260103 loss_rnnt 7.471549 hw_loss 0.261236 lr 0.00037463 rank 7
2023-02-25 00:32:11,462 DEBUG TRAIN Batch 21/3000 loss 15.586711 loss_att 15.605216 loss_ctc 20.059475 loss_rnnt 14.887548 hw_loss 0.185802 lr 0.00037462 rank 5
2023-02-25 00:32:11,505 DEBUG TRAIN Batch 21/3000 loss 10.510855 loss_att 11.413891 loss_ctc 12.127148 loss_rnnt 9.992363 hw_loss 0.229458 lr 0.00037464 rank 4
2023-02-25 00:33:24,243 DEBUG TRAIN Batch 21/3100 loss 7.574111 loss_att 8.285864 loss_ctc 8.983608 loss_rnnt 7.133826 hw_loss 0.206255 lr 0.00037450 rank 1
2023-02-25 00:33:24,244 DEBUG TRAIN Batch 21/3100 loss 4.731821 loss_att 8.005497 loss_ctc 7.292767 loss_rnnt 3.641593 hw_loss 0.176313 lr 0.00037451 rank 0
2023-02-25 00:33:24,247 DEBUG TRAIN Batch 21/3100 loss 12.369609 loss_att 11.376153 loss_ctc 15.385708 loss_rnnt 12.023387 hw_loss 0.267689 lr 0.00037452 rank 2
2023-02-25 00:33:24,246 DEBUG TRAIN Batch 21/3100 loss 9.077113 loss_att 9.867210 loss_ctc 10.102158 loss_rnnt 8.665779 hw_loss 0.218703 lr 0.00037453 rank 6
2023-02-25 00:33:24,251 DEBUG TRAIN Batch 21/3100 loss 2.808763 loss_att 4.847412 loss_ctc 5.269019 loss_rnnt 1.969278 hw_loss 0.194476 lr 0.00037457 rank 3
2023-02-25 00:33:24,251 DEBUG TRAIN Batch 21/3100 loss 11.458732 loss_att 13.357185 loss_ctc 14.212810 loss_rnnt 10.595041 hw_loss 0.218977 lr 0.00037452 rank 5
2023-02-25 00:33:24,252 DEBUG TRAIN Batch 21/3100 loss 4.635947 loss_att 6.913431 loss_ctc 8.472495 loss_rnnt 3.555251 hw_loss 0.213113 lr 0.00037454 rank 4
2023-02-25 00:33:24,253 DEBUG TRAIN Batch 21/3100 loss 15.425968 loss_att 18.305782 loss_ctc 22.238928 loss_rnnt 13.825647 hw_loss 0.217428 lr 0.00037453 rank 7
2023-02-25 00:34:39,090 DEBUG TRAIN Batch 21/3200 loss 6.487734 loss_att 8.123533 loss_ctc 10.719451 loss_rnnt 5.476802 hw_loss 0.224143 lr 0.00037441 rank 0
2023-02-25 00:34:39,090 DEBUG TRAIN Batch 21/3200 loss 8.554089 loss_att 9.747586 loss_ctc 11.605189 loss_rnnt 7.780360 hw_loss 0.240404 lr 0.00037446 rank 3
2023-02-25 00:34:39,094 DEBUG TRAIN Batch 21/3200 loss 3.138209 loss_att 7.719581 loss_ctc 6.605819 loss_rnnt 1.665100 hw_loss 0.177163 lr 0.00037442 rank 7
2023-02-25 00:34:39,095 DEBUG TRAIN Batch 21/3200 loss 10.088953 loss_att 9.864462 loss_ctc 13.286562 loss_rnnt 9.552929 hw_loss 0.289826 lr 0.00037443 rank 4
2023-02-25 00:34:39,097 DEBUG TRAIN Batch 21/3200 loss 9.563790 loss_att 11.087743 loss_ctc 18.958542 loss_rnnt 7.882788 hw_loss 0.231710 lr 0.00037441 rank 5
2023-02-25 00:34:39,099 DEBUG TRAIN Batch 21/3200 loss 5.789062 loss_att 8.991806 loss_ctc 8.382257 loss_rnnt 4.651581 hw_loss 0.283450 lr 0.00037441 rank 2
2023-02-25 00:34:39,099 DEBUG TRAIN Batch 21/3200 loss 5.700029 loss_att 8.797377 loss_ctc 8.217108 loss_rnnt 4.705533 hw_loss 0.073905 lr 0.00037442 rank 6
2023-02-25 00:34:39,142 DEBUG TRAIN Batch 21/3200 loss 2.492126 loss_att 4.655869 loss_ctc 3.447285 loss_rnnt 1.814528 hw_loss 0.220301 lr 0.00037439 rank 1
2023-02-25 00:35:51,833 DEBUG TRAIN Batch 21/3300 loss 8.684420 loss_att 16.150860 loss_ctc 9.989113 loss_rnnt 6.870080 hw_loss 0.275798 lr 0.00037430 rank 0
2023-02-25 00:35:51,836 DEBUG TRAIN Batch 21/3300 loss 7.940780 loss_att 15.270306 loss_ctc 13.740471 loss_rnnt 5.600231 hw_loss 0.190033 lr 0.00037429 rank 1
2023-02-25 00:35:51,838 DEBUG TRAIN Batch 21/3300 loss 3.744634 loss_att 6.837255 loss_ctc 5.409201 loss_rnnt 2.802506 hw_loss 0.190615 lr 0.00037432 rank 7
2023-02-25 00:35:51,837 DEBUG TRAIN Batch 21/3300 loss 12.016806 loss_att 15.313494 loss_ctc 14.451393 loss_rnnt 10.947798 hw_loss 0.159489 lr 0.00037432 rank 6
2023-02-25 00:35:51,838 DEBUG TRAIN Batch 21/3300 loss 11.054566 loss_att 13.154566 loss_ctc 14.228878 loss_rnnt 10.149531 hw_loss 0.115864 lr 0.00037431 rank 2
2023-02-25 00:35:51,839 DEBUG TRAIN Batch 21/3300 loss 7.924887 loss_att 8.682271 loss_ctc 8.246302 loss_rnnt 7.677146 hw_loss 0.100140 lr 0.00037436 rank 3
2023-02-25 00:35:51,843 DEBUG TRAIN Batch 21/3300 loss 12.077057 loss_att 18.564388 loss_ctc 20.243153 loss_rnnt 9.611700 hw_loss 0.148271 lr 0.00037433 rank 4
2023-02-25 00:35:51,892 DEBUG TRAIN Batch 21/3300 loss 7.022003 loss_att 9.513661 loss_ctc 10.163914 loss_rnnt 5.945323 hw_loss 0.298927 lr 0.00037431 rank 5
2023-02-25 00:37:04,378 DEBUG TRAIN Batch 21/3400 loss 9.733674 loss_att 13.029480 loss_ctc 11.848185 loss_rnnt 8.694878 hw_loss 0.183190 lr 0.00037420 rank 0
2023-02-25 00:37:04,381 DEBUG TRAIN Batch 21/3400 loss 4.401933 loss_att 6.630914 loss_ctc 6.145592 loss_rnnt 3.587011 hw_loss 0.256195 lr 0.00037418 rank 1
2023-02-25 00:37:04,384 DEBUG TRAIN Batch 21/3400 loss 4.458078 loss_att 8.629475 loss_ctc 8.345599 loss_rnnt 3.014235 hw_loss 0.171052 lr 0.00037422 rank 4
2023-02-25 00:37:04,388 DEBUG TRAIN Batch 21/3400 loss 14.786565 loss_att 17.621632 loss_ctc 25.688305 loss_rnnt 12.684818 hw_loss 0.152189 lr 0.00037421 rank 7
2023-02-25 00:37:04,388 DEBUG TRAIN Batch 21/3400 loss 9.649314 loss_att 11.259696 loss_ctc 9.822384 loss_rnnt 9.182632 hw_loss 0.227866 lr 0.00037420 rank 2
2023-02-25 00:37:04,392 DEBUG TRAIN Batch 21/3400 loss 4.704584 loss_att 7.621955 loss_ctc 7.599206 loss_rnnt 3.596090 hw_loss 0.260755 lr 0.00037420 rank 5
2023-02-25 00:37:04,392 DEBUG TRAIN Batch 21/3400 loss 7.983325 loss_att 11.869612 loss_ctc 11.694944 loss_rnnt 6.630210 hw_loss 0.151827 lr 0.00037421 rank 6
2023-02-25 00:37:04,432 DEBUG TRAIN Batch 21/3400 loss 9.899201 loss_att 12.181720 loss_ctc 13.808029 loss_rnnt 8.792930 hw_loss 0.241109 lr 0.00037425 rank 3
2023-02-25 00:38:17,935 DEBUG TRAIN Batch 21/3500 loss 8.529570 loss_att 13.719597 loss_ctc 16.936134 loss_rnnt 6.275580 hw_loss 0.178328 lr 0.00037411 rank 6
2023-02-25 00:38:17,941 DEBUG TRAIN Batch 21/3500 loss 14.111346 loss_att 16.394840 loss_ctc 14.431424 loss_rnnt 13.481295 hw_loss 0.245016 lr 0.00037415 rank 3
2023-02-25 00:38:17,943 DEBUG TRAIN Batch 21/3500 loss 8.566424 loss_att 9.939065 loss_ctc 11.853766 loss_rnnt 7.761888 hw_loss 0.171930 lr 0.00037408 rank 1
2023-02-25 00:38:17,949 DEBUG TRAIN Batch 21/3500 loss 6.662672 loss_att 7.461123 loss_ctc 6.885101 loss_rnnt 6.363340 hw_loss 0.206220 lr 0.00037411 rank 7
2023-02-25 00:38:17,950 DEBUG TRAIN Batch 21/3500 loss 10.700901 loss_att 11.531711 loss_ctc 10.114521 loss_rnnt 10.527118 hw_loss 0.160888 lr 0.00037409 rank 0
2023-02-25 00:38:17,950 DEBUG TRAIN Batch 21/3500 loss 2.978607 loss_att 6.181092 loss_ctc 6.173193 loss_rnnt 1.745211 hw_loss 0.313038 lr 0.00037410 rank 5
2023-02-25 00:38:17,971 DEBUG TRAIN Batch 21/3500 loss 11.872362 loss_att 13.040233 loss_ctc 13.766020 loss_rnnt 11.335946 hw_loss 0.094415 lr 0.00037410 rank 2
2023-02-25 00:38:18,002 DEBUG TRAIN Batch 21/3500 loss 11.403656 loss_att 13.821440 loss_ctc 17.901718 loss_rnnt 9.955166 hw_loss 0.184737 lr 0.00037412 rank 4
2023-02-25 00:39:32,073 DEBUG TRAIN Batch 21/3600 loss 16.267918 loss_att 18.853304 loss_ctc 21.232185 loss_rnnt 15.004944 hw_loss 0.157489 lr 0.00037399 rank 0
2023-02-25 00:39:32,074 DEBUG TRAIN Batch 21/3600 loss 16.707766 loss_att 20.410904 loss_ctc 22.502974 loss_rnnt 15.080838 hw_loss 0.213010 lr 0.00037400 rank 7
2023-02-25 00:39:32,074 DEBUG TRAIN Batch 21/3600 loss 5.070415 loss_att 7.687183 loss_ctc 6.299314 loss_rnnt 4.306244 hw_loss 0.144307 lr 0.00037400 rank 6
2023-02-25 00:39:32,076 DEBUG TRAIN Batch 21/3600 loss 7.832047 loss_att 11.806881 loss_ctc 11.385551 loss_rnnt 6.483005 hw_loss 0.150515 lr 0.00037397 rank 1
2023-02-25 00:39:32,077 DEBUG TRAIN Batch 21/3600 loss 7.250409 loss_att 9.266160 loss_ctc 9.785954 loss_rnnt 6.419881 hw_loss 0.167446 lr 0.00037401 rank 4
2023-02-25 00:39:32,079 DEBUG TRAIN Batch 21/3600 loss 9.732248 loss_att 11.954428 loss_ctc 10.386136 loss_rnnt 9.115400 hw_loss 0.159799 lr 0.00037404 rank 3
2023-02-25 00:39:32,082 DEBUG TRAIN Batch 21/3600 loss 13.660856 loss_att 19.385202 loss_ctc 21.292137 loss_rnnt 11.394970 hw_loss 0.194085 lr 0.00037399 rank 5
2023-02-25 00:39:32,084 DEBUG TRAIN Batch 21/3600 loss 9.183562 loss_att 12.599630 loss_ctc 12.953191 loss_rnnt 7.855487 hw_loss 0.266706 lr 0.00037399 rank 2
2023-02-25 00:40:44,732 DEBUG TRAIN Batch 21/3700 loss 11.162040 loss_att 15.283627 loss_ctc 14.725891 loss_rnnt 9.691490 hw_loss 0.320721 lr 0.00037390 rank 7
2023-02-25 00:40:44,733 DEBUG TRAIN Batch 21/3700 loss 5.430919 loss_att 7.008833 loss_ctc 10.360670 loss_rnnt 4.287907 hw_loss 0.318991 lr 0.00037389 rank 2
2023-02-25 00:40:44,735 DEBUG TRAIN Batch 21/3700 loss 9.523869 loss_att 11.878548 loss_ctc 15.756782 loss_rnnt 8.129225 hw_loss 0.173724 lr 0.00037390 rank 6
2023-02-25 00:40:44,735 DEBUG TRAIN Batch 21/3700 loss 3.585249 loss_att 7.399406 loss_ctc 6.152914 loss_rnnt 2.378721 hw_loss 0.190015 lr 0.00037389 rank 5
2023-02-25 00:40:44,737 DEBUG TRAIN Batch 21/3700 loss 4.665514 loss_att 8.856173 loss_ctc 6.128942 loss_rnnt 3.507230 hw_loss 0.234431 lr 0.00037388 rank 0
2023-02-25 00:40:44,737 DEBUG TRAIN Batch 21/3700 loss 8.818028 loss_att 11.121063 loss_ctc 14.695490 loss_rnnt 7.444106 hw_loss 0.243102 lr 0.00037391 rank 4
2023-02-25 00:40:44,740 DEBUG TRAIN Batch 21/3700 loss 13.261588 loss_att 16.196907 loss_ctc 16.930635 loss_rnnt 12.078224 hw_loss 0.200801 lr 0.00037394 rank 3
2023-02-25 00:40:44,741 DEBUG TRAIN Batch 21/3700 loss 10.020105 loss_att 13.647673 loss_ctc 14.763896 loss_rnnt 8.536416 hw_loss 0.235629 lr 0.00037387 rank 1
2023-02-25 00:41:57,660 DEBUG TRAIN Batch 21/3800 loss 5.870384 loss_att 7.829504 loss_ctc 12.186542 loss_rnnt 4.498285 hw_loss 0.258976 lr 0.00037378 rank 0
2023-02-25 00:41:57,662 DEBUG TRAIN Batch 21/3800 loss 6.526802 loss_att 11.852516 loss_ctc 11.500133 loss_rnnt 4.674728 hw_loss 0.232161 lr 0.00037380 rank 6
2023-02-25 00:41:57,666 DEBUG TRAIN Batch 21/3800 loss 5.813430 loss_att 5.946516 loss_ctc 8.325197 loss_rnnt 5.252522 hw_loss 0.373855 lr 0.00037383 rank 3
2023-02-25 00:41:57,666 DEBUG TRAIN Batch 21/3800 loss 11.361967 loss_att 12.305753 loss_ctc 12.113512 loss_rnnt 10.936084 hw_loss 0.256725 lr 0.00037380 rank 4
2023-02-25 00:41:57,666 DEBUG TRAIN Batch 21/3800 loss 4.492813 loss_att 6.981701 loss_ctc 6.152901 loss_rnnt 3.631634 hw_loss 0.266356 lr 0.00037376 rank 1
2023-02-25 00:41:57,667 DEBUG TRAIN Batch 21/3800 loss 10.575201 loss_att 11.162790 loss_ctc 12.535017 loss_rnnt 10.064409 hw_loss 0.247435 lr 0.00037379 rank 7
2023-02-25 00:41:57,678 DEBUG TRAIN Batch 21/3800 loss 12.044251 loss_att 12.275131 loss_ctc 15.712158 loss_rnnt 11.378078 hw_loss 0.245519 lr 0.00037379 rank 5
2023-02-25 00:41:57,713 DEBUG TRAIN Batch 21/3800 loss 5.788977 loss_att 8.763567 loss_ctc 8.189176 loss_rnnt 4.790771 hw_loss 0.156115 lr 0.00037378 rank 2
2023-02-25 00:43:17,271 DEBUG TRAIN Batch 21/3900 loss 8.524545 loss_att 10.212005 loss_ctc 9.007560 loss_rnnt 7.973666 hw_loss 0.279348 lr 0.00037367 rank 0
2023-02-25 00:43:17,273 DEBUG TRAIN Batch 21/3900 loss 14.019851 loss_att 20.059616 loss_ctc 21.135052 loss_rnnt 11.750541 hw_loss 0.211243 lr 0.00037373 rank 3
2023-02-25 00:43:17,276 DEBUG TRAIN Batch 21/3900 loss 6.824447 loss_att 10.056664 loss_ctc 10.520209 loss_rnnt 5.603975 hw_loss 0.152362 lr 0.00037370 rank 4
2023-02-25 00:43:17,277 DEBUG TRAIN Batch 21/3900 loss 20.608158 loss_att 19.375593 loss_ctc 28.274002 loss_rnnt 19.723419 hw_loss 0.204633 lr 0.00037368 rank 2
2023-02-25 00:43:17,278 DEBUG TRAIN Batch 21/3900 loss 10.004848 loss_att 15.718425 loss_ctc 16.974243 loss_rnnt 7.809686 hw_loss 0.230988 lr 0.00037369 rank 6
2023-02-25 00:43:17,284 DEBUG TRAIN Batch 21/3900 loss 9.003834 loss_att 10.346874 loss_ctc 11.037749 loss_rnnt 8.291203 hw_loss 0.324064 lr 0.00037368 rank 5
2023-02-25 00:43:17,299 DEBUG TRAIN Batch 21/3900 loss 14.835035 loss_att 18.401653 loss_ctc 24.321381 loss_rnnt 12.777885 hw_loss 0.148086 lr 0.00037369 rank 7
2023-02-25 00:43:17,304 DEBUG TRAIN Batch 21/3900 loss 9.371921 loss_att 9.891493 loss_ctc 14.005939 loss_rnnt 8.508762 hw_loss 0.265078 lr 0.00037366 rank 1
2023-02-25 00:44:30,904 DEBUG TRAIN Batch 21/4000 loss 6.813643 loss_att 7.955675 loss_ctc 11.982687 loss_rnnt 5.762012 hw_loss 0.251283 lr 0.00037357 rank 0
2023-02-25 00:44:30,910 DEBUG TRAIN Batch 21/4000 loss 6.877329 loss_att 9.690897 loss_ctc 11.820784 loss_rnnt 5.554821 hw_loss 0.188751 lr 0.00037355 rank 1
2023-02-25 00:44:30,910 DEBUG TRAIN Batch 21/4000 loss 3.884017 loss_att 7.520190 loss_ctc 7.471709 loss_rnnt 2.532183 hw_loss 0.274203 lr 0.00037358 rank 2
2023-02-25 00:44:30,911 DEBUG TRAIN Batch 21/4000 loss 10.848076 loss_att 15.323299 loss_ctc 16.846272 loss_rnnt 9.067060 hw_loss 0.161645 lr 0.00037362 rank 3
2023-02-25 00:44:30,912 DEBUG TRAIN Batch 21/4000 loss 12.004037 loss_att 14.566290 loss_ctc 17.650593 loss_rnnt 10.633285 hw_loss 0.197674 lr 0.00037359 rank 4
2023-02-25 00:44:30,914 DEBUG TRAIN Batch 21/4000 loss 8.044723 loss_att 16.138748 loss_ctc 16.582954 loss_rnnt 5.214754 hw_loss 0.136374 lr 0.00037358 rank 7
2023-02-25 00:44:30,915 DEBUG TRAIN Batch 21/4000 loss 6.251841 loss_att 8.856592 loss_ctc 7.370569 loss_rnnt 5.491403 hw_loss 0.169358 lr 0.00037358 rank 5
2023-02-25 00:44:30,960 DEBUG TRAIN Batch 21/4000 loss 9.556041 loss_att 11.959908 loss_ctc 16.944736 loss_rnnt 7.947165 hw_loss 0.268018 lr 0.00037359 rank 6
2023-02-25 00:45:43,054 DEBUG TRAIN Batch 21/4100 loss 10.245520 loss_att 11.617593 loss_ctc 11.409373 loss_rnnt 9.747072 hw_loss 0.129099 lr 0.00037345 rank 1
2023-02-25 00:45:43,067 DEBUG TRAIN Batch 21/4100 loss 10.570890 loss_att 15.650675 loss_ctc 15.068346 loss_rnnt 8.867053 hw_loss 0.165410 lr 0.00037347 rank 0
2023-02-25 00:45:43,068 DEBUG TRAIN Batch 21/4100 loss 3.125439 loss_att 5.245911 loss_ctc 3.170408 loss_rnnt 2.571152 hw_loss 0.232869 lr 0.00037352 rank 3
2023-02-25 00:45:43,071 DEBUG TRAIN Batch 21/4100 loss 3.999564 loss_att 6.515590 loss_ctc 5.959542 loss_rnnt 3.116283 hw_loss 0.222649 lr 0.00037347 rank 5
2023-02-25 00:45:43,073 DEBUG TRAIN Batch 21/4100 loss 4.426745 loss_att 7.026662 loss_ctc 5.616274 loss_rnnt 3.639781 hw_loss 0.203205 lr 0.00037348 rank 7
2023-02-25 00:45:43,075 DEBUG TRAIN Batch 21/4100 loss 10.306380 loss_att 15.889463 loss_ctc 12.955015 loss_rnnt 8.732962 hw_loss 0.194345 lr 0.00037348 rank 6
2023-02-25 00:45:43,080 DEBUG TRAIN Batch 21/4100 loss 18.794243 loss_att 21.089205 loss_ctc 32.923664 loss_rnnt 16.359228 hw_loss 0.172689 lr 0.00037347 rank 2
2023-02-25 00:45:43,123 DEBUG TRAIN Batch 21/4100 loss 1.237106 loss_att 3.381828 loss_ctc 2.028214 loss_rnnt 0.628102 hw_loss 0.139835 lr 0.00037349 rank 4
2023-02-25 00:46:56,687 DEBUG TRAIN Batch 21/4200 loss 8.604528 loss_att 13.518827 loss_ctc 15.894333 loss_rnnt 6.530899 hw_loss 0.222742 lr 0.00037337 rank 5
2023-02-25 00:46:56,699 DEBUG TRAIN Batch 21/4200 loss 12.738098 loss_att 14.311445 loss_ctc 17.852821 loss_rnnt 11.579368 hw_loss 0.303934 lr 0.00037335 rank 1
2023-02-25 00:46:56,701 DEBUG TRAIN Batch 21/4200 loss 7.096060 loss_att 9.726911 loss_ctc 12.081665 loss_rnnt 5.788615 hw_loss 0.218490 lr 0.00037336 rank 0
2023-02-25 00:46:56,701 DEBUG TRAIN Batch 21/4200 loss 6.371379 loss_att 8.322210 loss_ctc 12.032187 loss_rnnt 5.135616 hw_loss 0.170290 lr 0.00037338 rank 4
2023-02-25 00:46:56,703 DEBUG TRAIN Batch 21/4200 loss 11.355472 loss_att 14.482065 loss_ctc 16.149761 loss_rnnt 9.970974 hw_loss 0.224888 lr 0.00037337 rank 2
2023-02-25 00:46:56,703 DEBUG TRAIN Batch 21/4200 loss 10.390521 loss_att 13.481773 loss_ctc 16.370615 loss_rnnt 8.847763 hw_loss 0.238428 lr 0.00037338 rank 6
2023-02-25 00:46:56,704 DEBUG TRAIN Batch 21/4200 loss 9.912297 loss_att 13.128185 loss_ctc 14.372746 loss_rnnt 8.519504 hw_loss 0.290418 lr 0.00037342 rank 3
2023-02-25 00:46:56,718 DEBUG TRAIN Batch 21/4200 loss 7.888276 loss_att 11.379677 loss_ctc 9.522873 loss_rnnt 6.835319 hw_loss 0.256371 lr 0.00037337 rank 7
2023-02-25 00:48:11,647 DEBUG TRAIN Batch 21/4300 loss 4.387495 loss_att 5.642990 loss_ctc 6.061024 loss_rnnt 3.811911 hw_loss 0.190026 lr 0.00037326 rank 0
2023-02-25 00:48:11,647 DEBUG TRAIN Batch 21/4300 loss 11.803809 loss_att 12.974245 loss_ctc 16.332512 loss_rnnt 10.876110 hw_loss 0.168348 lr 0.00037328 rank 4
2023-02-25 00:48:11,648 DEBUG TRAIN Batch 21/4300 loss 5.685096 loss_att 6.952182 loss_ctc 7.284892 loss_rnnt 5.111161 hw_loss 0.201020 lr 0.00037326 rank 2
2023-02-25 00:48:11,651 DEBUG TRAIN Batch 21/4300 loss 7.302882 loss_att 10.410410 loss_ctc 10.867593 loss_rnnt 6.098892 hw_loss 0.200982 lr 0.00037326 rank 5
2023-02-25 00:48:11,651 DEBUG TRAIN Batch 21/4300 loss 13.557519 loss_att 16.649801 loss_ctc 13.187339 loss_rnnt 12.874065 hw_loss 0.214411 lr 0.00037331 rank 3
2023-02-25 00:48:11,651 DEBUG TRAIN Batch 21/4300 loss 11.935690 loss_att 14.564120 loss_ctc 15.057720 loss_rnnt 10.858143 hw_loss 0.254233 lr 0.00037324 rank 1
2023-02-25 00:48:11,653 DEBUG TRAIN Batch 21/4300 loss 6.873822 loss_att 7.068441 loss_ctc 8.573682 loss_rnnt 6.495559 hw_loss 0.211298 lr 0.00037327 rank 7
2023-02-25 00:48:11,653 DEBUG TRAIN Batch 21/4300 loss 5.811575 loss_att 6.602198 loss_ctc 7.892478 loss_rnnt 5.283366 hw_loss 0.173682 lr 0.00037327 rank 6
2023-02-25 00:49:24,476 DEBUG TRAIN Batch 21/4400 loss 4.496729 loss_att 8.247318 loss_ctc 6.900469 loss_rnnt 3.291565 hw_loss 0.252276 lr 0.00037316 rank 2
2023-02-25 00:49:24,478 DEBUG TRAIN Batch 21/4400 loss 12.189675 loss_att 13.604702 loss_ctc 13.273340 loss_rnnt 11.680809 hw_loss 0.152571 lr 0.00037314 rank 1
2023-02-25 00:49:24,480 DEBUG TRAIN Batch 21/4400 loss 10.287055 loss_att 12.356957 loss_ctc 14.197113 loss_rnnt 9.270029 hw_loss 0.153197 lr 0.00037321 rank 3
2023-02-25 00:49:24,481 DEBUG TRAIN Batch 21/4400 loss 12.288565 loss_att 13.570501 loss_ctc 15.847383 loss_rnnt 11.427725 hw_loss 0.243646 lr 0.00037315 rank 0
2023-02-25 00:49:24,482 DEBUG TRAIN Batch 21/4400 loss 8.513284 loss_att 9.946757 loss_ctc 12.053761 loss_rnnt 7.650666 hw_loss 0.194734 lr 0.00037317 rank 7
2023-02-25 00:49:24,485 DEBUG TRAIN Batch 21/4400 loss 10.473667 loss_att 9.845109 loss_ctc 12.911353 loss_rnnt 10.121130 hw_loss 0.287295 lr 0.00037317 rank 6
2023-02-25 00:49:24,485 DEBUG TRAIN Batch 21/4400 loss 10.553936 loss_att 12.305679 loss_ctc 13.460250 loss_rnnt 9.672472 hw_loss 0.269262 lr 0.00037318 rank 4
2023-02-25 00:49:24,491 DEBUG TRAIN Batch 21/4400 loss 11.225939 loss_att 11.910825 loss_ctc 13.125214 loss_rnnt 10.744662 hw_loss 0.170742 lr 0.00037316 rank 5
2023-02-25 00:50:36,982 DEBUG TRAIN Batch 21/4500 loss 2.367532 loss_att 4.299243 loss_ctc 2.460023 loss_rnnt 1.853178 hw_loss 0.216899 lr 0.00037307 rank 6
2023-02-25 00:50:36,983 DEBUG TRAIN Batch 21/4500 loss 8.991274 loss_att 9.388646 loss_ctc 11.676788 loss_rnnt 8.408999 hw_loss 0.271373 lr 0.00037305 rank 0
2023-02-25 00:50:36,986 DEBUG TRAIN Batch 21/4500 loss 6.475060 loss_att 13.352861 loss_ctc 12.586092 loss_rnnt 4.174087 hw_loss 0.207392 lr 0.00037306 rank 2
2023-02-25 00:50:36,987 DEBUG TRAIN Batch 21/4500 loss 6.492476 loss_att 6.905655 loss_ctc 6.568161 loss_rnnt 6.315905 hw_loss 0.157207 lr 0.00037303 rank 1
2023-02-25 00:50:36,989 DEBUG TRAIN Batch 21/4500 loss 9.446526 loss_att 10.560437 loss_ctc 10.902727 loss_rnnt 8.926307 hw_loss 0.193643 lr 0.00037306 rank 7
2023-02-25 00:50:36,990 DEBUG TRAIN Batch 21/4500 loss 25.620426 loss_att 26.678078 loss_ctc 35.251637 loss_rnnt 23.973761 hw_loss 0.283079 lr 0.00037307 rank 4
2023-02-25 00:50:37,003 DEBUG TRAIN Batch 21/4500 loss 4.515370 loss_att 7.192157 loss_ctc 5.128856 loss_rnnt 3.762016 hw_loss 0.255373 lr 0.00037306 rank 5
2023-02-25 00:50:37,034 DEBUG TRAIN Batch 21/4500 loss 13.673085 loss_att 13.583195 loss_ctc 18.822279 loss_rnnt 12.869073 hw_loss 0.253933 lr 0.00037310 rank 3
2023-02-25 00:51:51,831 DEBUG TRAIN Batch 21/4600 loss 5.696694 loss_att 9.872820 loss_ctc 8.351972 loss_rnnt 4.419606 hw_loss 0.164673 lr 0.00037296 rank 7
2023-02-25 00:51:51,832 DEBUG TRAIN Batch 21/4600 loss 9.592136 loss_att 9.587337 loss_ctc 13.623314 loss_rnnt 8.937583 hw_loss 0.221294 lr 0.00037293 rank 1
2023-02-25 00:51:51,832 DEBUG TRAIN Batch 21/4600 loss 2.355008 loss_att 4.482618 loss_ctc 3.655959 loss_rnnt 1.679943 hw_loss 0.142656 lr 0.00037296 rank 6
2023-02-25 00:51:51,840 DEBUG TRAIN Batch 21/4600 loss 8.591365 loss_att 12.081306 loss_ctc 11.489989 loss_rnnt 7.433157 hw_loss 0.138257 lr 0.00037300 rank 3
2023-02-25 00:51:51,839 DEBUG TRAIN Batch 21/4600 loss 7.345339 loss_att 12.698463 loss_ctc 10.761836 loss_rnnt 5.734655 hw_loss 0.158486 lr 0.00037295 rank 5
2023-02-25 00:51:51,842 DEBUG TRAIN Batch 21/4600 loss 1.285996 loss_att 4.802029 loss_ctc 1.492518 loss_rnnt 0.393765 hw_loss 0.302792 lr 0.00037295 rank 2
2023-02-25 00:51:51,847 DEBUG TRAIN Batch 21/4600 loss 4.192094 loss_att 6.127707 loss_ctc 5.394934 loss_rnnt 3.566289 hw_loss 0.146819 lr 0.00037295 rank 0
2023-02-25 00:51:51,869 DEBUG TRAIN Batch 21/4600 loss 6.676035 loss_att 10.491428 loss_ctc 6.787601 loss_rnnt 5.825134 hw_loss 0.136775 lr 0.00037297 rank 4
2023-02-25 00:53:04,322 DEBUG TRAIN Batch 21/4700 loss 3.340639 loss_att 6.135284 loss_ctc 3.462368 loss_rnnt 2.699618 hw_loss 0.123490 lr 0.00037287 rank 4
2023-02-25 00:53:04,328 DEBUG TRAIN Batch 21/4700 loss 9.021479 loss_att 12.902660 loss_ctc 12.337061 loss_rnnt 7.721858 hw_loss 0.152450 lr 0.00037284 rank 0
2023-02-25 00:53:04,329 DEBUG TRAIN Batch 21/4700 loss 23.415861 loss_att 22.434460 loss_ctc 29.951942 loss_rnnt 22.627827 hw_loss 0.211570 lr 0.00037290 rank 3
2023-02-25 00:53:04,330 DEBUG TRAIN Batch 21/4700 loss 6.719638 loss_att 9.417392 loss_ctc 9.657734 loss_rnnt 5.675244 hw_loss 0.212056 lr 0.00037286 rank 6
2023-02-25 00:53:04,332 DEBUG TRAIN Batch 21/4700 loss 3.149686 loss_att 6.344963 loss_ctc 4.089313 loss_rnnt 2.296134 hw_loss 0.167275 lr 0.00037286 rank 7
2023-02-25 00:53:04,332 DEBUG TRAIN Batch 21/4700 loss 8.492132 loss_att 8.863412 loss_ctc 9.894192 loss_rnnt 8.132505 hw_loss 0.184556 lr 0.00037285 rank 2
2023-02-25 00:53:04,334 DEBUG TRAIN Batch 21/4700 loss 2.641086 loss_att 5.445979 loss_ctc 3.924705 loss_rnnt 1.790703 hw_loss 0.221729 lr 0.00037283 rank 1
2023-02-25 00:53:04,379 DEBUG TRAIN Batch 21/4700 loss 9.807049 loss_att 12.170376 loss_ctc 12.791706 loss_rnnt 8.846401 hw_loss 0.168802 lr 0.00037285 rank 5
2023-02-25 00:54:16,916 DEBUG TRAIN Batch 21/4800 loss 15.799000 loss_att 18.948439 loss_ctc 22.942602 loss_rnnt 14.101501 hw_loss 0.215872 lr 0.00037274 rank 0
2023-02-25 00:54:16,918 DEBUG TRAIN Batch 21/4800 loss 14.750923 loss_att 17.062111 loss_ctc 18.740074 loss_rnnt 13.623259 hw_loss 0.250389 lr 0.00037274 rank 2
2023-02-25 00:54:16,922 DEBUG TRAIN Batch 21/4800 loss 5.027679 loss_att 7.069530 loss_ctc 9.046389 loss_rnnt 3.984184 hw_loss 0.186182 lr 0.00037275 rank 7
2023-02-25 00:54:16,924 DEBUG TRAIN Batch 21/4800 loss 7.085202 loss_att 10.843671 loss_ctc 8.498071 loss_rnnt 6.055583 hw_loss 0.167893 lr 0.00037275 rank 5
2023-02-25 00:54:16,924 DEBUG TRAIN Batch 21/4800 loss 10.832748 loss_att 12.534037 loss_ctc 12.529408 loss_rnnt 10.146624 hw_loss 0.224335 lr 0.00037276 rank 6
2023-02-25 00:54:16,924 DEBUG TRAIN Batch 21/4800 loss 13.206008 loss_att 14.482422 loss_ctc 17.522610 loss_rnnt 12.296447 hw_loss 0.147623 lr 0.00037276 rank 4
2023-02-25 00:54:16,927 DEBUG TRAIN Batch 21/4800 loss 13.303513 loss_att 13.997753 loss_ctc 16.669865 loss_rnnt 12.652555 hw_loss 0.118619 lr 0.00037279 rank 3
2023-02-25 00:54:16,974 DEBUG TRAIN Batch 21/4800 loss 7.414569 loss_att 10.491923 loss_ctc 9.835549 loss_rnnt 6.348133 hw_loss 0.240315 lr 0.00037272 rank 1
2023-02-25 00:55:30,268 DEBUG TRAIN Batch 21/4900 loss 13.238526 loss_att 16.322914 loss_ctc 17.280748 loss_rnnt 11.966225 hw_loss 0.218366 lr 0.00037266 rank 4
2023-02-25 00:55:30,272 DEBUG TRAIN Batch 21/4900 loss 7.227633 loss_att 11.423684 loss_ctc 11.166844 loss_rnnt 5.740953 hw_loss 0.229203 lr 0.00037269 rank 3
2023-02-25 00:55:30,275 DEBUG TRAIN Batch 21/4900 loss 12.507464 loss_att 15.009039 loss_ctc 16.785774 loss_rnnt 11.371550 hw_loss 0.122173 lr 0.00037265 rank 6
2023-02-25 00:55:30,277 DEBUG TRAIN Batch 21/4900 loss 9.186461 loss_att 14.797865 loss_ctc 13.179419 loss_rnnt 7.429423 hw_loss 0.191932 lr 0.00037262 rank 1
2023-02-25 00:55:30,279 DEBUG TRAIN Batch 21/4900 loss 15.484446 loss_att 15.658359 loss_ctc 19.905630 loss_rnnt 14.764623 hw_loss 0.179152 lr 0.00037264 rank 2
2023-02-25 00:55:30,280 DEBUG TRAIN Batch 21/4900 loss 4.239925 loss_att 7.565270 loss_ctc 6.401945 loss_rnnt 3.180146 hw_loss 0.199578 lr 0.00037265 rank 7
2023-02-25 00:55:30,283 DEBUG TRAIN Batch 21/4900 loss 6.467260 loss_att 9.959465 loss_ctc 8.677027 loss_rnnt 5.341705 hw_loss 0.248396 lr 0.00037263 rank 0
2023-02-25 00:55:30,282 DEBUG TRAIN Batch 21/4900 loss 10.414886 loss_att 12.248752 loss_ctc 14.219946 loss_rnnt 9.454626 hw_loss 0.161522 lr 0.00037264 rank 5
2023-02-25 00:56:45,795 DEBUG TRAIN Batch 21/5000 loss 7.895025 loss_att 12.188087 loss_ctc 10.938993 loss_rnnt 6.502719 hw_loss 0.239684 lr 0.00037254 rank 2
2023-02-25 00:56:45,796 DEBUG TRAIN Batch 21/5000 loss 14.175016 loss_att 14.417624 loss_ctc 15.276614 loss_rnnt 13.889366 hw_loss 0.169218 lr 0.00037254 rank 7
2023-02-25 00:56:45,797 DEBUG TRAIN Batch 21/5000 loss 6.292292 loss_att 9.518972 loss_ctc 8.954551 loss_rnnt 5.185309 hw_loss 0.200022 lr 0.00037253 rank 0
2023-02-25 00:56:45,797 DEBUG TRAIN Batch 21/5000 loss 4.608814 loss_att 6.673696 loss_ctc 5.397861 loss_rnnt 3.942747 hw_loss 0.277284 lr 0.00037255 rank 4
2023-02-25 00:56:45,800 DEBUG TRAIN Batch 21/5000 loss 9.544446 loss_att 12.069042 loss_ctc 12.663922 loss_rnnt 8.476870 hw_loss 0.275115 lr 0.00037258 rank 3
2023-02-25 00:56:45,799 DEBUG TRAIN Batch 21/5000 loss 5.894199 loss_att 7.514040 loss_ctc 8.612760 loss_rnnt 5.061430 hw_loss 0.274362 lr 0.00037255 rank 6
2023-02-25 00:56:45,804 DEBUG TRAIN Batch 21/5000 loss 3.453940 loss_att 6.511582 loss_ctc 6.565412 loss_rnnt 2.258917 hw_loss 0.316185 lr 0.00037254 rank 5
2023-02-25 00:56:45,804 DEBUG TRAIN Batch 21/5000 loss 12.399366 loss_att 14.533210 loss_ctc 19.836111 loss_rnnt 10.828949 hw_loss 0.285156 lr 0.00037252 rank 1
2023-02-25 00:57:58,848 DEBUG TRAIN Batch 21/5100 loss 6.646681 loss_att 9.572438 loss_ctc 11.610837 loss_rnnt 5.304477 hw_loss 0.178434 lr 0.00037243 rank 0
2023-02-25 00:57:58,850 DEBUG TRAIN Batch 21/5100 loss 8.947604 loss_att 11.602527 loss_ctc 13.013897 loss_rnnt 7.777258 hw_loss 0.182231 lr 0.00037248 rank 3
2023-02-25 00:57:58,849 DEBUG TRAIN Batch 21/5100 loss 7.924220 loss_att 12.098226 loss_ctc 12.386930 loss_rnnt 6.421572 hw_loss 0.136535 lr 0.00037245 rank 6
2023-02-25 00:57:58,850 DEBUG TRAIN Batch 21/5100 loss 9.073821 loss_att 15.860626 loss_ctc 21.138641 loss_rnnt 5.994131 hw_loss 0.213164 lr 0.00037243 rank 2
2023-02-25 00:57:58,850 DEBUG TRAIN Batch 21/5100 loss 11.984486 loss_att 10.869617 loss_ctc 13.709116 loss_rnnt 11.827079 hw_loss 0.282056 lr 0.00037244 rank 7
2023-02-25 00:57:58,855 DEBUG TRAIN Batch 21/5100 loss 5.928682 loss_att 9.991522 loss_ctc 9.650406 loss_rnnt 4.521852 hw_loss 0.183810 lr 0.00037245 rank 4
2023-02-25 00:57:58,855 DEBUG TRAIN Batch 21/5100 loss 5.074489 loss_att 7.097034 loss_ctc 8.121539 loss_rnnt 4.173188 hw_loss 0.169721 lr 0.00037241 rank 1
2023-02-25 00:57:58,861 DEBUG TRAIN Batch 21/5100 loss 11.716026 loss_att 11.285260 loss_ctc 13.453374 loss_rnnt 11.417324 hw_loss 0.287265 lr 0.00037243 rank 5
2023-02-25 00:59:10,700 DEBUG TRAIN Batch 21/5200 loss 6.124956 loss_att 10.961055 loss_ctc 8.161370 loss_rnnt 4.748695 hw_loss 0.257849 lr 0.00037232 rank 0
2023-02-25 00:59:10,703 DEBUG TRAIN Batch 21/5200 loss 13.076927 loss_att 14.959566 loss_ctc 21.776775 loss_rnnt 11.385674 hw_loss 0.290152 lr 0.00037233 rank 2
2023-02-25 00:59:10,704 DEBUG TRAIN Batch 21/5200 loss 7.012325 loss_att 8.786746 loss_ctc 9.049229 loss_rnnt 6.218840 hw_loss 0.313151 lr 0.00037231 rank 1
2023-02-25 00:59:10,705 DEBUG TRAIN Batch 21/5200 loss 8.914071 loss_att 11.461369 loss_ctc 12.179744 loss_rnnt 7.857217 hw_loss 0.209944 lr 0.00037238 rank 3
2023-02-25 00:59:10,714 DEBUG TRAIN Batch 21/5200 loss 17.905539 loss_att 20.017393 loss_ctc 24.028152 loss_rnnt 16.545816 hw_loss 0.226879 lr 0.00037233 rank 5
2023-02-25 00:59:10,715 DEBUG TRAIN Batch 21/5200 loss 8.163813 loss_att 10.670334 loss_ctc 16.039707 loss_rnnt 6.500469 hw_loss 0.209851 lr 0.00037234 rank 7
2023-02-25 00:59:10,742 DEBUG TRAIN Batch 21/5200 loss 8.081984 loss_att 11.041477 loss_ctc 13.037978 loss_rnnt 6.736198 hw_loss 0.174540 lr 0.00037234 rank 6
2023-02-25 00:59:10,744 DEBUG TRAIN Batch 21/5200 loss 3.453912 loss_att 5.696656 loss_ctc 3.327364 loss_rnnt 2.961065 hw_loss 0.114695 lr 0.00037235 rank 4
2023-02-25 01:00:25,567 DEBUG TRAIN Batch 21/5300 loss 3.114531 loss_att 6.966280 loss_ctc 5.909724 loss_rnnt 1.875914 hw_loss 0.179204 lr 0.00037223 rank 5
2023-02-25 01:00:25,573 DEBUG TRAIN Batch 21/5300 loss 8.119073 loss_att 10.831098 loss_ctc 15.588408 loss_rnnt 6.425756 hw_loss 0.290628 lr 0.00037224 rank 4
2023-02-25 01:00:25,574 DEBUG TRAIN Batch 21/5300 loss 1.392398 loss_att 3.629348 loss_ctc 2.826458 loss_rnnt 0.686428 hw_loss 0.126322 lr 0.00037227 rank 3
2023-02-25 01:00:25,574 DEBUG TRAIN Batch 21/5300 loss 4.364057 loss_att 7.772394 loss_ctc 4.802574 loss_rnnt 3.512761 hw_loss 0.208424 lr 0.00037222 rank 0
2023-02-25 01:00:25,576 DEBUG TRAIN Batch 21/5300 loss 16.715973 loss_att 15.737698 loss_ctc 22.578981 loss_rnnt 16.023518 hw_loss 0.199453 lr 0.00037223 rank 2
2023-02-25 01:00:25,576 DEBUG TRAIN Batch 21/5300 loss 4.908724 loss_att 7.151908 loss_ctc 4.846177 loss_rnnt 4.432393 hw_loss 0.067564 lr 0.00037223 rank 7
2023-02-25 01:00:25,577 DEBUG TRAIN Batch 21/5300 loss 5.914564 loss_att 9.718444 loss_ctc 9.910862 loss_rnnt 4.524891 hw_loss 0.180108 lr 0.00037221 rank 1
2023-02-25 01:00:25,619 DEBUG TRAIN Batch 21/5300 loss 10.358943 loss_att 14.805834 loss_ctc 14.732063 loss_rnnt 8.780697 hw_loss 0.198348 lr 0.00037224 rank 6
2023-02-25 01:01:40,403 DEBUG TRAIN Batch 21/5400 loss 5.213315 loss_att 9.012671 loss_ctc 8.602558 loss_rnnt 3.859853 hw_loss 0.265672 lr 0.00037212 rank 0
2023-02-25 01:01:40,407 DEBUG TRAIN Batch 21/5400 loss 14.215400 loss_att 16.732821 loss_ctc 18.715809 loss_rnnt 12.972134 hw_loss 0.261989 lr 0.00037217 rank 3
2023-02-25 01:01:40,409 DEBUG TRAIN Batch 21/5400 loss 9.123100 loss_att 13.117847 loss_ctc 16.143303 loss_rnnt 7.303069 hw_loss 0.159479 lr 0.00037210 rank 1
2023-02-25 01:01:40,410 DEBUG TRAIN Batch 21/5400 loss 9.240390 loss_att 12.119566 loss_ctc 14.433609 loss_rnnt 7.905428 hw_loss 0.125057 lr 0.00037213 rank 7
2023-02-25 01:01:40,413 DEBUG TRAIN Batch 21/5400 loss 6.099643 loss_att 9.633235 loss_ctc 8.851994 loss_rnnt 4.939027 hw_loss 0.162970 lr 0.00037212 rank 2
2023-02-25 01:01:40,414 DEBUG TRAIN Batch 21/5400 loss 3.830133 loss_att 5.739706 loss_ctc 4.661916 loss_rnnt 3.243696 hw_loss 0.175534 lr 0.00037214 rank 6
2023-02-25 01:01:40,421 DEBUG TRAIN Batch 21/5400 loss 13.844104 loss_att 18.053875 loss_ctc 20.519455 loss_rnnt 11.993880 hw_loss 0.221667 lr 0.00037213 rank 5
2023-02-25 01:01:40,476 DEBUG TRAIN Batch 21/5400 loss 12.653968 loss_att 15.822898 loss_ctc 11.953978 loss_rnnt 12.044486 hw_loss 0.129428 lr 0.00037214 rank 4
2023-02-25 01:02:52,738 DEBUG TRAIN Batch 21/5500 loss 6.713154 loss_att 9.200950 loss_ctc 8.647802 loss_rnnt 5.881097 hw_loss 0.143521 lr 0.00037202 rank 0
2023-02-25 01:02:52,743 DEBUG TRAIN Batch 21/5500 loss 6.102292 loss_att 8.427061 loss_ctc 7.763115 loss_rnnt 5.288216 hw_loss 0.239398 lr 0.00037207 rank 3
2023-02-25 01:02:52,743 DEBUG TRAIN Batch 21/5500 loss 8.958542 loss_att 9.901279 loss_ctc 10.725669 loss_rnnt 8.398993 hw_loss 0.253843 lr 0.00037202 rank 2
2023-02-25 01:02:52,745 DEBUG TRAIN Batch 21/5500 loss 12.159827 loss_att 15.490845 loss_ctc 17.736286 loss_rnnt 10.655604 hw_loss 0.177174 lr 0.00037202 rank 5
2023-02-25 01:02:52,745 DEBUG TRAIN Batch 21/5500 loss 10.558999 loss_att 11.788822 loss_ctc 13.345522 loss_rnnt 9.824482 hw_loss 0.219404 lr 0.00037203 rank 6
2023-02-25 01:02:52,746 DEBUG TRAIN Batch 21/5500 loss 12.515680 loss_att 15.829893 loss_ctc 17.393206 loss_rnnt 11.050530 hw_loss 0.284942 lr 0.00037203 rank 7
2023-02-25 01:02:52,748 DEBUG TRAIN Batch 21/5500 loss 4.442442 loss_att 7.181918 loss_ctc 4.688792 loss_rnnt 3.772632 hw_loss 0.167004 lr 0.00037200 rank 1
2023-02-25 01:02:52,751 DEBUG TRAIN Batch 21/5500 loss 10.394235 loss_att 12.922593 loss_ctc 12.855867 loss_rnnt 9.430473 hw_loss 0.243511 lr 0.00037204 rank 4
2023-02-25 01:04:05,545 DEBUG TRAIN Batch 21/5600 loss 11.384360 loss_att 16.153217 loss_ctc 16.723898 loss_rnnt 9.596666 hw_loss 0.228722 lr 0.00037193 rank 6
2023-02-25 01:04:05,546 DEBUG TRAIN Batch 21/5600 loss 3.968795 loss_att 6.622919 loss_ctc 5.213430 loss_rnnt 3.159859 hw_loss 0.210300 lr 0.00037190 rank 1
2023-02-25 01:04:05,548 DEBUG TRAIN Batch 21/5600 loss 10.938669 loss_att 12.521832 loss_ctc 14.359264 loss_rnnt 10.092038 hw_loss 0.138597 lr 0.00037194 rank 4
2023-02-25 01:04:05,548 DEBUG TRAIN Batch 21/5600 loss 4.385809 loss_att 8.798786 loss_ctc 5.870141 loss_rnnt 3.186743 hw_loss 0.222298 lr 0.00037197 rank 3
2023-02-25 01:04:05,551 DEBUG TRAIN Batch 21/5600 loss 9.930247 loss_att 11.379699 loss_ctc 11.362292 loss_rnnt 9.341688 hw_loss 0.201994 lr 0.00037193 rank 7
2023-02-25 01:04:05,557 DEBUG TRAIN Batch 21/5600 loss 10.185713 loss_att 14.135641 loss_ctc 13.738098 loss_rnnt 8.855890 hw_loss 0.124097 lr 0.00037191 rank 0
2023-02-25 01:04:05,559 DEBUG TRAIN Batch 21/5600 loss 7.668884 loss_att 9.861251 loss_ctc 9.979065 loss_rnnt 6.806356 hw_loss 0.217556 lr 0.00037192 rank 5
2023-02-25 01:04:05,568 DEBUG TRAIN Batch 21/5600 loss 4.035746 loss_att 8.872957 loss_ctc 5.972194 loss_rnnt 2.721265 hw_loss 0.166585 lr 0.00037192 rank 2
2023-02-25 01:05:21,495 DEBUG TRAIN Batch 21/5700 loss 6.923790 loss_att 8.217547 loss_ctc 9.135373 loss_rnnt 6.275376 hw_loss 0.177721 lr 0.00037181 rank 0
2023-02-25 01:05:21,501 DEBUG TRAIN Batch 21/5700 loss 12.951474 loss_att 14.655312 loss_ctc 18.516266 loss_rnnt 11.757519 hw_loss 0.208530 lr 0.00037182 rank 2
2023-02-25 01:05:21,504 DEBUG TRAIN Batch 21/5700 loss 13.307179 loss_att 13.949247 loss_ctc 17.216637 loss_rnnt 12.542231 hw_loss 0.216137 lr 0.00037182 rank 7
2023-02-25 01:05:21,504 DEBUG TRAIN Batch 21/5700 loss 12.156181 loss_att 12.604040 loss_ctc 13.935269 loss_rnnt 11.624983 hw_loss 0.383279 lr 0.00037186 rank 3
2023-02-25 01:05:21,504 DEBUG TRAIN Batch 21/5700 loss 14.526555 loss_att 14.934793 loss_ctc 22.625818 loss_rnnt 13.255075 hw_loss 0.206119 lr 0.00037179 rank 1
2023-02-25 01:05:21,507 DEBUG TRAIN Batch 21/5700 loss 8.085930 loss_att 10.204367 loss_ctc 9.986422 loss_rnnt 7.295501 hw_loss 0.212519 lr 0.00037183 rank 4
2023-02-25 01:05:21,529 DEBUG TRAIN Batch 21/5700 loss 11.416750 loss_att 15.963331 loss_ctc 16.547092 loss_rnnt 9.766405 hw_loss 0.106843 lr 0.00037183 rank 6
2023-02-25 01:05:21,532 DEBUG TRAIN Batch 21/5700 loss 12.815667 loss_att 11.631776 loss_ctc 16.947485 loss_rnnt 12.378555 hw_loss 0.230589 lr 0.00037182 rank 5
2023-02-25 01:06:33,909 DEBUG TRAIN Batch 21/5800 loss 5.408774 loss_att 5.652465 loss_ctc 7.822145 loss_rnnt 4.928924 hw_loss 0.204992 lr 0.00037171 rank 0
2023-02-25 01:06:33,909 DEBUG TRAIN Batch 21/5800 loss 10.387676 loss_att 10.959767 loss_ctc 13.543562 loss_rnnt 9.725736 hw_loss 0.237635 lr 0.00037176 rank 3
2023-02-25 01:06:33,912 DEBUG TRAIN Batch 21/5800 loss 7.030965 loss_att 8.537865 loss_ctc 9.812664 loss_rnnt 6.200568 hw_loss 0.296483 lr 0.00037169 rank 1
2023-02-25 01:06:33,915 DEBUG TRAIN Batch 21/5800 loss 3.731978 loss_att 5.275337 loss_ctc 4.995010 loss_rnnt 3.182891 hw_loss 0.135020 lr 0.00037172 rank 6
2023-02-25 01:06:33,915 DEBUG TRAIN Batch 21/5800 loss 8.999089 loss_att 9.928257 loss_ctc 11.457038 loss_rnnt 8.360620 hw_loss 0.234203 lr 0.00037173 rank 4
2023-02-25 01:06:33,916 DEBUG TRAIN Batch 21/5800 loss 13.309056 loss_att 13.713766 loss_ctc 15.727660 loss_rnnt 12.831378 hw_loss 0.139229 lr 0.00037172 rank 7
2023-02-25 01:06:33,917 DEBUG TRAIN Batch 21/5800 loss 1.624115 loss_att 5.304316 loss_ctc 2.840231 loss_rnnt 0.614449 hw_loss 0.209020 lr 0.00037171 rank 2
2023-02-25 01:06:33,920 DEBUG TRAIN Batch 21/5800 loss 16.684511 loss_att 19.633883 loss_ctc 22.917624 loss_rnnt 15.155833 hw_loss 0.201976 lr 0.00037171 rank 5
2023-02-25 01:07:47,171 DEBUG TRAIN Batch 21/5900 loss 11.672014 loss_att 13.507328 loss_ctc 13.454126 loss_rnnt 10.973356 hw_loss 0.176216 lr 0.00037162 rank 7
2023-02-25 01:07:47,173 DEBUG TRAIN Batch 21/5900 loss 5.065591 loss_att 8.291613 loss_ctc 5.691156 loss_rnnt 4.216033 hw_loss 0.226772 lr 0.00037162 rank 6
2023-02-25 01:07:47,173 DEBUG TRAIN Batch 21/5900 loss 13.000954 loss_att 14.920929 loss_ctc 17.746582 loss_rnnt 11.864837 hw_loss 0.223824 lr 0.00037160 rank 0
2023-02-25 01:07:47,174 DEBUG TRAIN Batch 21/5900 loss 11.858339 loss_att 12.903934 loss_ctc 17.490139 loss_rnnt 10.790586 hw_loss 0.201988 lr 0.00037159 rank 1
2023-02-25 01:07:47,176 DEBUG TRAIN Batch 21/5900 loss 17.808010 loss_att 17.216904 loss_ctc 23.312811 loss_rnnt 17.065163 hw_loss 0.238306 lr 0.00037161 rank 2
2023-02-25 01:07:47,175 DEBUG TRAIN Batch 21/5900 loss 9.442097 loss_att 12.360147 loss_ctc 16.247972 loss_rnnt 7.874084 hw_loss 0.144284 lr 0.00037163 rank 4
2023-02-25 01:07:47,177 DEBUG TRAIN Batch 21/5900 loss 13.699546 loss_att 14.436741 loss_ctc 18.499401 loss_rnnt 12.836207 hw_loss 0.142348 lr 0.00037161 rank 5
2023-02-25 01:07:47,178 DEBUG TRAIN Batch 21/5900 loss 8.128684 loss_att 10.984165 loss_ctc 13.259284 loss_rnnt 6.771472 hw_loss 0.191316 lr 0.00037166 rank 3
2023-02-25 01:09:01,259 DEBUG TRAIN Batch 21/6000 loss 1.689459 loss_att 4.151064 loss_ctc 2.488801 loss_rnnt 1.010494 hw_loss 0.150122 lr 0.00037149 rank 1
2023-02-25 01:09:01,260 DEBUG TRAIN Batch 21/6000 loss 3.967458 loss_att 6.702705 loss_ctc 6.285667 loss_rnnt 3.017560 hw_loss 0.175790 lr 0.00037152 rank 6
2023-02-25 01:09:01,261 DEBUG TRAIN Batch 21/6000 loss 9.454593 loss_att 12.195063 loss_ctc 12.522731 loss_rnnt 8.376009 hw_loss 0.227633 lr 0.00037150 rank 0
2023-02-25 01:09:01,262 DEBUG TRAIN Batch 21/6000 loss 6.820646 loss_att 9.786039 loss_ctc 9.600609 loss_rnnt 5.771046 hw_loss 0.160986 lr 0.00037151 rank 2
2023-02-25 01:09:01,262 DEBUG TRAIN Batch 21/6000 loss 6.542302 loss_att 8.695468 loss_ctc 9.688618 loss_rnnt 5.574322 hw_loss 0.220946 lr 0.00037152 rank 4
2023-02-25 01:09:01,265 DEBUG TRAIN Batch 21/6000 loss 18.703371 loss_att 21.888824 loss_ctc 25.980232 loss_rnnt 17.059036 hw_loss 0.069365 lr 0.00037151 rank 7
2023-02-25 01:09:01,269 DEBUG TRAIN Batch 21/6000 loss 4.398679 loss_att 6.371643 loss_ctc 5.193150 loss_rnnt 3.753392 hw_loss 0.271433 lr 0.00037151 rank 5
2023-02-25 01:09:01,287 DEBUG TRAIN Batch 21/6000 loss 11.376353 loss_att 15.490583 loss_ctc 14.191389 loss_rnnt 10.097269 hw_loss 0.151689 lr 0.00037155 rank 3
2023-02-25 01:10:18,548 DEBUG TRAIN Batch 21/6100 loss 14.209981 loss_att 17.946953 loss_ctc 22.086487 loss_rnnt 12.269569 hw_loss 0.267780 lr 0.00037145 rank 3
2023-02-25 01:10:18,562 DEBUG TRAIN Batch 21/6100 loss 4.087416 loss_att 6.840396 loss_ctc 6.287971 loss_rnnt 3.128752 hw_loss 0.214988 lr 0.00037140 rank 0
2023-02-25 01:10:18,566 DEBUG TRAIN Batch 21/6100 loss 10.624368 loss_att 16.193470 loss_ctc 16.726463 loss_rnnt 8.576464 hw_loss 0.225882 lr 0.00037138 rank 1
2023-02-25 01:10:18,569 DEBUG TRAIN Batch 21/6100 loss 5.249187 loss_att 7.918941 loss_ctc 9.064317 loss_rnnt 4.151798 hw_loss 0.102663 lr 0.00037142 rank 6
2023-02-25 01:10:18,569 DEBUG TRAIN Batch 21/6100 loss 11.172364 loss_att 13.279268 loss_ctc 19.393091 loss_rnnt 9.521259 hw_loss 0.250551 lr 0.00037141 rank 7
2023-02-25 01:10:18,569 DEBUG TRAIN Batch 21/6100 loss 18.843006 loss_att 27.512501 loss_ctc 31.007259 loss_rnnt 15.396357 hw_loss 0.170339 lr 0.00037142 rank 4
2023-02-25 01:10:18,574 DEBUG TRAIN Batch 21/6100 loss 8.217559 loss_att 11.803488 loss_ctc 11.296562 loss_rnnt 6.977917 hw_loss 0.209853 lr 0.00037141 rank 5
2023-02-25 01:10:18,621 DEBUG TRAIN Batch 21/6100 loss 9.865975 loss_att 12.215519 loss_ctc 15.318092 loss_rnnt 8.558819 hw_loss 0.206808 lr 0.00037141 rank 2
2023-02-25 01:11:31,582 DEBUG TRAIN Batch 21/6200 loss 18.641796 loss_att 22.616535 loss_ctc 26.892693 loss_rnnt 16.615389 hw_loss 0.246264 lr 0.00037130 rank 0
2023-02-25 01:11:31,585 DEBUG TRAIN Batch 21/6200 loss 13.168936 loss_att 15.179641 loss_ctc 15.947862 loss_rnnt 12.271896 hw_loss 0.233205 lr 0.00037128 rank 1
2023-02-25 01:11:31,585 DEBUG TRAIN Batch 21/6200 loss 11.994298 loss_att 11.158648 loss_ctc 15.230421 loss_rnnt 11.621431 hw_loss 0.203461 lr 0.00037130 rank 2
2023-02-25 01:11:31,586 DEBUG TRAIN Batch 21/6200 loss 13.696918 loss_att 16.018068 loss_ctc 21.630056 loss_rnnt 12.074148 hw_loss 0.188980 lr 0.00037135 rank 3
2023-02-25 01:11:31,587 DEBUG TRAIN Batch 21/6200 loss 14.134332 loss_att 17.157135 loss_ctc 20.048536 loss_rnnt 12.637836 hw_loss 0.193826 lr 0.00037131 rank 7
2023-02-25 01:11:31,587 DEBUG TRAIN Batch 21/6200 loss 11.551290 loss_att 17.708389 loss_ctc 15.041974 loss_rnnt 9.753420 hw_loss 0.189422 lr 0.00037132 rank 4
2023-02-25 01:11:31,593 DEBUG TRAIN Batch 21/6200 loss 11.931941 loss_att 15.879875 loss_ctc 21.176165 loss_rnnt 9.733631 hw_loss 0.330298 lr 0.00037130 rank 5
2023-02-25 01:11:31,638 DEBUG TRAIN Batch 21/6200 loss 6.178437 loss_att 9.121695 loss_ctc 8.760877 loss_rnnt 5.149015 hw_loss 0.180834 lr 0.00037131 rank 6
2023-02-25 01:12:45,954 DEBUG TRAIN Batch 21/6300 loss 9.905150 loss_att 13.097660 loss_ctc 14.180229 loss_rnnt 8.584181 hw_loss 0.210856 lr 0.00037120 rank 2
2023-02-25 01:12:45,955 DEBUG TRAIN Batch 21/6300 loss 6.341678 loss_att 9.348217 loss_ctc 8.841376 loss_rnnt 5.321389 hw_loss 0.160664 lr 0.00037118 rank 1
2023-02-25 01:12:45,956 DEBUG TRAIN Batch 21/6300 loss 18.826279 loss_att 23.827852 loss_ctc 28.042885 loss_rnnt 16.461193 hw_loss 0.254795 lr 0.00037119 rank 0
2023-02-25 01:12:45,956 DEBUG TRAIN Batch 21/6300 loss 9.014577 loss_att 12.849457 loss_ctc 13.387059 loss_rnnt 7.573241 hw_loss 0.171303 lr 0.00037125 rank 3
2023-02-25 01:12:45,955 DEBUG TRAIN Batch 21/6300 loss 6.466590 loss_att 7.295094 loss_ctc 10.135332 loss_rnnt 5.644638 hw_loss 0.313285 lr 0.00037121 rank 6
2023-02-25 01:12:45,957 DEBUG TRAIN Batch 21/6300 loss 11.878461 loss_att 12.344013 loss_ctc 18.555054 loss_rnnt 10.792356 hw_loss 0.192715 lr 0.00037122 rank 4
2023-02-25 01:12:45,959 DEBUG TRAIN Batch 21/6300 loss 7.016137 loss_att 8.656501 loss_ctc 13.658095 loss_rnnt 5.693172 hw_loss 0.204932 lr 0.00037121 rank 7
2023-02-25 01:12:45,980 DEBUG TRAIN Batch 21/6300 loss 7.253057 loss_att 9.971131 loss_ctc 10.484790 loss_rnnt 6.197642 hw_loss 0.151691 lr 0.00037120 rank 5
2023-02-25 01:14:02,075 DEBUG TRAIN Batch 21/6400 loss 11.264549 loss_att 14.298348 loss_ctc 15.804741 loss_rnnt 9.923924 hw_loss 0.240947 lr 0.00037109 rank 0
2023-02-25 01:14:02,076 DEBUG TRAIN Batch 21/6400 loss 2.670462 loss_att 7.407207 loss_ctc 3.982111 loss_rnnt 1.410056 hw_loss 0.259069 lr 0.00037110 rank 2
2023-02-25 01:14:02,078 DEBUG TRAIN Batch 21/6400 loss 7.448248 loss_att 7.726475 loss_ctc 9.910394 loss_rnnt 6.954031 hw_loss 0.206785 lr 0.00037112 rank 4
2023-02-25 01:14:02,081 DEBUG TRAIN Batch 21/6400 loss 11.087838 loss_att 12.690527 loss_ctc 18.035164 loss_rnnt 9.664984 hw_loss 0.330014 lr 0.00037115 rank 3
2023-02-25 01:14:02,081 DEBUG TRAIN Batch 21/6400 loss 8.877573 loss_att 9.614807 loss_ctc 10.406842 loss_rnnt 8.372700 hw_loss 0.287857 lr 0.00037111 rank 7
2023-02-25 01:14:02,081 DEBUG TRAIN Batch 21/6400 loss 7.774744 loss_att 9.849580 loss_ctc 8.256118 loss_rnnt 7.168446 hw_loss 0.238402 lr 0.00037110 rank 5
2023-02-25 01:14:02,085 DEBUG TRAIN Batch 21/6400 loss 6.538385 loss_att 9.451057 loss_ctc 9.540835 loss_rnnt 5.463823 hw_loss 0.171940 lr 0.00037111 rank 6
2023-02-25 01:14:02,127 DEBUG TRAIN Batch 21/6400 loss 6.094068 loss_att 8.017828 loss_ctc 8.279232 loss_rnnt 5.319530 hw_loss 0.184556 lr 0.00037108 rank 1
2023-02-25 01:15:16,505 DEBUG TRAIN Batch 21/6500 loss 11.038675 loss_att 11.364656 loss_ctc 13.911019 loss_rnnt 10.454968 hw_loss 0.254121 lr 0.00037099 rank 0
2023-02-25 01:15:16,506 DEBUG TRAIN Batch 21/6500 loss 8.900671 loss_att 9.049137 loss_ctc 11.686768 loss_rnnt 8.367964 hw_loss 0.246627 lr 0.00037104 rank 3
2023-02-25 01:15:16,506 DEBUG TRAIN Batch 21/6500 loss 8.858262 loss_att 10.134224 loss_ctc 12.067493 loss_rnnt 8.005857 hw_loss 0.317463 lr 0.00037101 rank 6
2023-02-25 01:15:16,505 DEBUG TRAIN Batch 21/6500 loss 3.520874 loss_att 7.985960 loss_ctc 3.452940 loss_rnnt 2.517812 hw_loss 0.223318 lr 0.00037101 rank 4
2023-02-25 01:15:16,509 DEBUG TRAIN Batch 21/6500 loss 9.487809 loss_att 13.231834 loss_ctc 11.614386 loss_rnnt 8.343700 hw_loss 0.209552 lr 0.00037100 rank 5
2023-02-25 01:15:16,510 DEBUG TRAIN Batch 21/6500 loss 5.569610 loss_att 7.798975 loss_ctc 17.498878 loss_rnnt 3.402970 hw_loss 0.244120 lr 0.00037100 rank 7
2023-02-25 01:15:16,511 DEBUG TRAIN Batch 21/6500 loss 6.019029 loss_att 8.746355 loss_ctc 6.787456 loss_rnnt 5.313504 hw_loss 0.108004 lr 0.00037097 rank 1
2023-02-25 01:15:16,511 DEBUG TRAIN Batch 21/6500 loss 7.540166 loss_att 9.949930 loss_ctc 12.295657 loss_rnnt 6.307067 hw_loss 0.219527 lr 0.00037100 rank 2
2023-02-25 01:16:29,751 DEBUG TRAIN Batch 21/6600 loss 12.552263 loss_att 13.191574 loss_ctc 13.249018 loss_rnnt 12.128375 hw_loss 0.380863 lr 0.00037087 rank 1
2023-02-25 01:16:29,754 DEBUG TRAIN Batch 21/6600 loss 10.048615 loss_att 11.798962 loss_ctc 11.322699 loss_rnnt 9.428507 hw_loss 0.187803 lr 0.00037089 rank 2
2023-02-25 01:16:29,754 DEBUG TRAIN Batch 21/6600 loss 3.702013 loss_att 6.822795 loss_ctc 4.696303 loss_rnnt 2.808616 hw_loss 0.256255 lr 0.00037089 rank 0
2023-02-25 01:16:29,756 DEBUG TRAIN Batch 21/6600 loss 8.492524 loss_att 10.233374 loss_ctc 12.138144 loss_rnnt 7.563078 hw_loss 0.178487 lr 0.00037090 rank 6
2023-02-25 01:16:29,755 DEBUG TRAIN Batch 21/6600 loss 9.550744 loss_att 11.513921 loss_ctc 10.696991 loss_rnnt 8.903845 hw_loss 0.190181 lr 0.00037090 rank 7
2023-02-25 01:16:29,758 DEBUG TRAIN Batch 21/6600 loss 5.436541 loss_att 6.873165 loss_ctc 13.915144 loss_rnnt 3.925635 hw_loss 0.174564 lr 0.00037094 rank 3
2023-02-25 01:16:29,766 DEBUG TRAIN Batch 21/6600 loss 6.045517 loss_att 8.051562 loss_ctc 6.493098 loss_rnnt 5.484842 hw_loss 0.187105 lr 0.00037091 rank 4
2023-02-25 01:16:29,773 DEBUG TRAIN Batch 21/6600 loss 13.859952 loss_att 20.165897 loss_ctc 18.822628 loss_rnnt 11.836821 hw_loss 0.187971 lr 0.00037089 rank 5
2023-02-25 01:17:43,979 DEBUG TRAIN Batch 21/6700 loss 5.659727 loss_att 7.970139 loss_ctc 8.347095 loss_rnnt 4.729031 hw_loss 0.206807 lr 0.00037079 rank 5
2023-02-25 01:17:43,981 DEBUG TRAIN Batch 21/6700 loss 4.706801 loss_att 8.104120 loss_ctc 4.834203 loss_rnnt 3.855504 hw_loss 0.290335 lr 0.00037084 rank 3
2023-02-25 01:17:43,988 DEBUG TRAIN Batch 21/6700 loss 9.991675 loss_att 12.441029 loss_ctc 14.570841 loss_rnnt 8.755515 hw_loss 0.254501 lr 0.00037081 rank 4
2023-02-25 01:17:43,993 DEBUG TRAIN Batch 21/6700 loss 9.223327 loss_att 11.130771 loss_ctc 15.469902 loss_rnnt 7.869301 hw_loss 0.261862 lr 0.00037077 rank 1
2023-02-25 01:17:43,994 DEBUG TRAIN Batch 21/6700 loss 11.934457 loss_att 15.199691 loss_ctc 16.288105 loss_rnnt 10.601246 hw_loss 0.186893 lr 0.00037079 rank 0
2023-02-25 01:17:43,994 DEBUG TRAIN Batch 21/6700 loss 8.030210 loss_att 8.932286 loss_ctc 11.607976 loss_rnnt 7.276093 hw_loss 0.181251 lr 0.00037079 rank 2
2023-02-25 01:17:43,998 DEBUG TRAIN Batch 21/6700 loss 20.070173 loss_att 21.462801 loss_ctc 26.482437 loss_rnnt 18.850597 hw_loss 0.161403 lr 0.00037080 rank 6
2023-02-25 01:17:44,000 DEBUG TRAIN Batch 21/6700 loss 6.012885 loss_att 8.773823 loss_ctc 7.599380 loss_rnnt 5.115001 hw_loss 0.251557 lr 0.00037080 rank 7
2023-02-25 01:18:57,921 DEBUG TRAIN Batch 21/6800 loss 19.361874 loss_att 21.074108 loss_ctc 23.329769 loss_rnnt 18.419626 hw_loss 0.132649 lr 0.00037070 rank 6
2023-02-25 01:18:57,922 DEBUG TRAIN Batch 21/6800 loss 7.496396 loss_att 10.826141 loss_ctc 10.265566 loss_rnnt 6.353192 hw_loss 0.202559 lr 0.00037074 rank 3
2023-02-25 01:18:57,923 DEBUG TRAIN Batch 21/6800 loss 8.738474 loss_att 11.816790 loss_ctc 16.166334 loss_rnnt 7.031569 hw_loss 0.189113 lr 0.00037070 rank 7
2023-02-25 01:18:57,925 DEBUG TRAIN Batch 21/6800 loss 9.123344 loss_att 11.280240 loss_ctc 12.523837 loss_rnnt 8.115070 hw_loss 0.231554 lr 0.00037071 rank 4
2023-02-25 01:18:57,926 DEBUG TRAIN Batch 21/6800 loss 11.843678 loss_att 16.698256 loss_ctc 19.971735 loss_rnnt 9.647339 hw_loss 0.265659 lr 0.00037067 rank 1
2023-02-25 01:18:57,927 DEBUG TRAIN Batch 21/6800 loss 7.861626 loss_att 10.543678 loss_ctc 10.669369 loss_rnnt 6.794125 hw_loss 0.293859 lr 0.00037069 rank 2
2023-02-25 01:18:57,931 DEBUG TRAIN Batch 21/6800 loss 5.566470 loss_att 7.169822 loss_ctc 8.035110 loss_rnnt 4.812346 hw_loss 0.195566 lr 0.00037068 rank 0
2023-02-25 01:18:57,980 DEBUG TRAIN Batch 21/6800 loss 12.816953 loss_att 13.006498 loss_ctc 16.498384 loss_rnnt 12.161854 hw_loss 0.236873 lr 0.00037069 rank 5
2023-02-25 01:20:11,199 DEBUG TRAIN Batch 21/6900 loss 4.290321 loss_att 5.959459 loss_ctc 6.651509 loss_rnnt 3.554471 hw_loss 0.163497 lr 0.00037058 rank 0
2023-02-25 01:20:11,207 DEBUG TRAIN Batch 21/6900 loss 5.743661 loss_att 6.972214 loss_ctc 5.691132 loss_rnnt 5.387379 hw_loss 0.220455 lr 0.00037063 rank 3
2023-02-25 01:20:11,209 DEBUG TRAIN Batch 21/6900 loss 7.759006 loss_att 9.491474 loss_ctc 10.757987 loss_rnnt 6.969507 hw_loss 0.080890 lr 0.00037060 rank 7
2023-02-25 01:20:11,209 DEBUG TRAIN Batch 21/6900 loss 4.410850 loss_att 7.034317 loss_ctc 7.451171 loss_rnnt 3.396654 hw_loss 0.157737 lr 0.00037057 rank 1
2023-02-25 01:20:11,210 DEBUG TRAIN Batch 21/6900 loss 8.398105 loss_att 9.174816 loss_ctc 11.113086 loss_rnnt 7.728701 hw_loss 0.285119 lr 0.00037060 rank 6
2023-02-25 01:20:11,212 DEBUG TRAIN Batch 21/6900 loss 6.551391 loss_att 7.618032 loss_ctc 7.763498 loss_rnnt 6.062745 hw_loss 0.213192 lr 0.00037061 rank 4
2023-02-25 01:20:11,222 DEBUG TRAIN Batch 21/6900 loss 8.908783 loss_att 11.133165 loss_ctc 12.631827 loss_rnnt 7.838270 hw_loss 0.242308 lr 0.00037059 rank 5
2023-02-25 01:20:11,254 DEBUG TRAIN Batch 21/6900 loss 8.141246 loss_att 9.237860 loss_ctc 9.725418 loss_rnnt 7.551110 hw_loss 0.299232 lr 0.00037059 rank 2
2023-02-25 01:21:24,282 DEBUG TRAIN Batch 21/7000 loss 10.161220 loss_att 11.689756 loss_ctc 15.806754 loss_rnnt 8.955373 hw_loss 0.276376 lr 0.00037048 rank 0
2023-02-25 01:21:24,287 DEBUG TRAIN Batch 21/7000 loss 15.132314 loss_att 21.166687 loss_ctc 26.470375 loss_rnnt 12.314722 hw_loss 0.185577 lr 0.00037046 rank 1
2023-02-25 01:21:24,288 DEBUG TRAIN Batch 21/7000 loss 2.476031 loss_att 4.743572 loss_ctc 3.862592 loss_rnnt 1.789451 hw_loss 0.090369 lr 0.00037049 rank 5
2023-02-25 01:21:24,288 DEBUG TRAIN Batch 21/7000 loss 6.573195 loss_att 9.394281 loss_ctc 11.032853 loss_rnnt 5.274229 hw_loss 0.262739 lr 0.00037053 rank 3
2023-02-25 01:21:24,290 DEBUG TRAIN Batch 21/7000 loss 6.727757 loss_att 8.600750 loss_ctc 9.289366 loss_rnnt 5.866910 hw_loss 0.271314 lr 0.00037050 rank 4
2023-02-25 01:21:24,290 DEBUG TRAIN Batch 21/7000 loss 7.534865 loss_att 8.512148 loss_ctc 10.569448 loss_rnnt 6.803039 hw_loss 0.247047 lr 0.00037049 rank 7
2023-02-25 01:21:24,291 DEBUG TRAIN Batch 21/7000 loss 5.828742 loss_att 6.950600 loss_ctc 6.223342 loss_rnnt 5.397680 hw_loss 0.288895 lr 0.00037049 rank 2
2023-02-25 01:21:24,294 DEBUG TRAIN Batch 21/7000 loss 2.194585 loss_att 5.161383 loss_ctc 1.500599 loss_rnnt 1.599331 hw_loss 0.177049 lr 0.00037050 rank 6
2023-02-25 01:22:41,700 DEBUG TRAIN Batch 21/7100 loss 4.451560 loss_att 6.612716 loss_ctc 6.025846 loss_rnnt 3.667357 hw_loss 0.266376 lr 0.00037038 rank 0
2023-02-25 01:22:41,705 DEBUG TRAIN Batch 21/7100 loss 5.253807 loss_att 7.431465 loss_ctc 7.919481 loss_rnnt 4.358109 hw_loss 0.196392 lr 0.00037038 rank 2
2023-02-25 01:22:41,706 DEBUG TRAIN Batch 21/7100 loss 4.804863 loss_att 7.921834 loss_ctc 8.109858 loss_rnnt 3.689874 hw_loss 0.095491 lr 0.00037040 rank 6
2023-02-25 01:22:41,706 DEBUG TRAIN Batch 21/7100 loss 9.327329 loss_att 9.481833 loss_ctc 12.740077 loss_rnnt 8.677942 hw_loss 0.306473 lr 0.00037036 rank 1
2023-02-25 01:22:41,711 DEBUG TRAIN Batch 21/7100 loss 2.973591 loss_att 5.122987 loss_ctc 4.259098 loss_rnnt 2.245596 hw_loss 0.237588 lr 0.00037040 rank 4
2023-02-25 01:22:41,725 DEBUG TRAIN Batch 21/7100 loss 12.999530 loss_att 12.896664 loss_ctc 14.670306 loss_rnnt 12.624839 hw_loss 0.323426 lr 0.00037039 rank 5
2023-02-25 01:22:41,732 DEBUG TRAIN Batch 21/7100 loss 5.401179 loss_att 9.097273 loss_ctc 7.361965 loss_rnnt 4.328889 hw_loss 0.134312 lr 0.00037039 rank 7
2023-02-25 01:22:41,755 DEBUG TRAIN Batch 21/7100 loss 11.847881 loss_att 15.031078 loss_ctc 20.993519 loss_rnnt 9.890189 hw_loss 0.190565 lr 0.00037043 rank 3
2023-02-25 01:23:54,711 DEBUG TRAIN Batch 21/7200 loss 5.096205 loss_att 7.967290 loss_ctc 8.072796 loss_rnnt 4.078838 hw_loss 0.086759 lr 0.00037028 rank 5
2023-02-25 01:23:54,725 DEBUG TRAIN Batch 21/7200 loss 6.994731 loss_att 10.909237 loss_ctc 10.766043 loss_rnnt 5.589406 hw_loss 0.224218 lr 0.00037029 rank 6
2023-02-25 01:23:54,726 DEBUG TRAIN Batch 21/7200 loss 7.349008 loss_att 11.252374 loss_ctc 14.224695 loss_rnnt 5.546786 hw_loss 0.196482 lr 0.00037033 rank 3
2023-02-25 01:23:54,727 DEBUG TRAIN Batch 21/7200 loss 15.672415 loss_att 17.865089 loss_ctc 24.377562 loss_rnnt 14.021426 hw_loss 0.097062 lr 0.00037029 rank 7
2023-02-25 01:23:54,729 DEBUG TRAIN Batch 21/7200 loss 10.393345 loss_att 11.701997 loss_ctc 11.747429 loss_rnnt 9.854110 hw_loss 0.181800 lr 0.00037028 rank 0
2023-02-25 01:23:54,728 DEBUG TRAIN Batch 21/7200 loss 6.799618 loss_att 9.866829 loss_ctc 9.070684 loss_rnnt 5.775179 hw_loss 0.202852 lr 0.00037028 rank 2
2023-02-25 01:23:54,728 DEBUG TRAIN Batch 21/7200 loss 18.343948 loss_att 23.211090 loss_ctc 32.918343 loss_rnnt 15.312506 hw_loss 0.215175 lr 0.00037030 rank 4
2023-02-25 01:23:54,774 DEBUG TRAIN Batch 21/7200 loss 9.214767 loss_att 12.299462 loss_ctc 13.403234 loss_rnnt 7.902515 hw_loss 0.256594 lr 0.00037026 rank 1
2023-02-25 01:25:07,460 DEBUG TRAIN Batch 21/7300 loss 6.097036 loss_att 10.202679 loss_ctc 11.833728 loss_rnnt 4.384373 hw_loss 0.237456 lr 0.00037023 rank 3
2023-02-25 01:25:07,462 DEBUG TRAIN Batch 21/7300 loss 9.356557 loss_att 12.894888 loss_ctc 11.347677 loss_rnnt 8.272476 hw_loss 0.207997 lr 0.00037018 rank 0
2023-02-25 01:25:07,462 DEBUG TRAIN Batch 21/7300 loss 9.468319 loss_att 11.666869 loss_ctc 13.916441 loss_rnnt 8.332754 hw_loss 0.192698 lr 0.00037016 rank 1
2023-02-25 01:25:07,465 DEBUG TRAIN Batch 21/7300 loss 12.285129 loss_att 16.768986 loss_ctc 19.911240 loss_rnnt 10.243488 hw_loss 0.240099 lr 0.00037018 rank 2
2023-02-25 01:25:07,470 DEBUG TRAIN Batch 21/7300 loss 6.065381 loss_att 8.329550 loss_ctc 9.721149 loss_rnnt 4.992220 hw_loss 0.249171 lr 0.00037020 rank 4
2023-02-25 01:25:07,472 DEBUG TRAIN Batch 21/7300 loss 15.729316 loss_att 19.123556 loss_ctc 19.551975 loss_rnnt 14.475538 hw_loss 0.122330 lr 0.00037019 rank 7
2023-02-25 01:25:07,474 DEBUG TRAIN Batch 21/7300 loss 20.481298 loss_att 24.185089 loss_ctc 34.596962 loss_rnnt 17.736374 hw_loss 0.228896 lr 0.00037018 rank 5
2023-02-25 01:25:07,512 DEBUG TRAIN Batch 21/7300 loss 4.044653 loss_att 7.333642 loss_ctc 5.986963 loss_rnnt 3.007034 hw_loss 0.226588 lr 0.00037019 rank 6
2023-02-25 01:26:20,691 DEBUG TRAIN Batch 21/7400 loss 14.927741 loss_att 17.687897 loss_ctc 19.330328 loss_rnnt 13.707436 hw_loss 0.152365 lr 0.00037008 rank 2
2023-02-25 01:26:20,692 DEBUG TRAIN Batch 21/7400 loss 10.850095 loss_att 14.867585 loss_ctc 13.186927 loss_rnnt 9.651416 hw_loss 0.156755 lr 0.00037010 rank 4
2023-02-25 01:26:20,693 DEBUG TRAIN Batch 21/7400 loss 17.981884 loss_att 17.678461 loss_ctc 23.672657 loss_rnnt 17.100761 hw_loss 0.343195 lr 0.00037009 rank 6
2023-02-25 01:26:20,693 DEBUG TRAIN Batch 21/7400 loss 6.945739 loss_att 9.111293 loss_ctc 10.740414 loss_rnnt 5.917536 hw_loss 0.167130 lr 0.00037007 rank 0
2023-02-25 01:26:20,693 DEBUG TRAIN Batch 21/7400 loss 12.078493 loss_att 17.143076 loss_ctc 13.955927 loss_rnnt 10.685125 hw_loss 0.243989 lr 0.00037006 rank 1
2023-02-25 01:26:20,695 DEBUG TRAIN Batch 21/7400 loss 14.169334 loss_att 15.955234 loss_ctc 17.294027 loss_rnnt 13.265835 hw_loss 0.243177 lr 0.00037008 rank 5
2023-02-25 01:26:20,697 DEBUG TRAIN Batch 21/7400 loss 8.509690 loss_att 10.244173 loss_ctc 8.291884 loss_rnnt 8.076870 hw_loss 0.215558 lr 0.00037009 rank 7
2023-02-25 01:26:20,718 DEBUG TRAIN Batch 21/7400 loss 6.567253 loss_att 8.510771 loss_ctc 8.275479 loss_rnnt 5.866043 hw_loss 0.158893 lr 0.00037013 rank 3
2023-02-25 01:27:35,662 DEBUG TRAIN Batch 21/7500 loss 6.403687 loss_att 12.122754 loss_ctc 10.829718 loss_rnnt 4.587110 hw_loss 0.154925 lr 0.00036997 rank 0
2023-02-25 01:27:35,666 DEBUG TRAIN Batch 21/7500 loss 6.347308 loss_att 8.093122 loss_ctc 8.714911 loss_rnnt 5.576006 hw_loss 0.199611 lr 0.00036999 rank 6
2023-02-25 01:27:35,667 DEBUG TRAIN Batch 21/7500 loss 11.208694 loss_att 13.654175 loss_ctc 14.958586 loss_rnnt 10.088211 hw_loss 0.246379 lr 0.00036999 rank 7
2023-02-25 01:27:35,667 DEBUG TRAIN Batch 21/7500 loss 7.557877 loss_att 9.730404 loss_ctc 10.583336 loss_rnnt 6.623620 hw_loss 0.180668 lr 0.00036998 rank 2
2023-02-25 01:27:35,668 DEBUG TRAIN Batch 21/7500 loss 11.716744 loss_att 17.193064 loss_ctc 21.214043 loss_rnnt 9.300774 hw_loss 0.102001 lr 0.00036996 rank 1
2023-02-25 01:27:35,671 DEBUG TRAIN Batch 21/7500 loss 9.909512 loss_att 12.026419 loss_ctc 11.890848 loss_rnnt 9.113547 hw_loss 0.203259 lr 0.00037003 rank 3
2023-02-25 01:27:35,677 DEBUG TRAIN Batch 21/7500 loss 12.447966 loss_att 14.796490 loss_ctc 19.097427 loss_rnnt 11.029275 hw_loss 0.116983 lr 0.00037000 rank 4
2023-02-25 01:27:35,679 DEBUG TRAIN Batch 21/7500 loss 10.360107 loss_att 12.965780 loss_ctc 15.966141 loss_rnnt 8.977381 hw_loss 0.213976 lr 0.00036998 rank 5
2023-02-25 01:28:48,034 DEBUG TRAIN Batch 21/7600 loss 4.457994 loss_att 6.648303 loss_ctc 6.869409 loss_rnnt 3.602601 hw_loss 0.179642 lr 0.00036987 rank 0
2023-02-25 01:28:48,038 DEBUG TRAIN Batch 21/7600 loss 6.376633 loss_att 9.026708 loss_ctc 12.079868 loss_rnnt 4.996624 hw_loss 0.167930 lr 0.00036986 rank 1
2023-02-25 01:28:48,040 DEBUG TRAIN Batch 21/7600 loss 10.779778 loss_att 12.788520 loss_ctc 17.628851 loss_rnnt 9.321759 hw_loss 0.268241 lr 0.00036992 rank 3
2023-02-25 01:28:48,041 DEBUG TRAIN Batch 21/7600 loss 4.371211 loss_att 6.162187 loss_ctc 7.232635 loss_rnnt 3.505233 hw_loss 0.236738 lr 0.00036989 rank 6
2023-02-25 01:28:48,044 DEBUG TRAIN Batch 21/7600 loss 6.941231 loss_att 8.321877 loss_ctc 9.785324 loss_rnnt 6.127595 hw_loss 0.296802 lr 0.00036988 rank 7
2023-02-25 01:28:48,047 DEBUG TRAIN Batch 21/7600 loss 25.039551 loss_att 29.538109 loss_ctc 33.746239 loss_rnnt 22.952383 hw_loss 0.049811 lr 0.00036988 rank 2
2023-02-25 01:28:48,049 DEBUG TRAIN Batch 21/7600 loss 11.435916 loss_att 13.434682 loss_ctc 19.377796 loss_rnnt 9.875476 hw_loss 0.190818 lr 0.00036989 rank 4
2023-02-25 01:28:48,048 DEBUG TRAIN Batch 21/7600 loss 6.199435 loss_att 9.385906 loss_ctc 8.907522 loss_rnnt 5.116862 hw_loss 0.157875 lr 0.00036988 rank 5
2023-02-25 01:29:59,962 DEBUG TRAIN Batch 21/7700 loss 7.949132 loss_att 14.451138 loss_ctc 11.211855 loss_rnnt 6.084691 hw_loss 0.241895 lr 0.00036979 rank 6
2023-02-25 01:29:59,964 DEBUG TRAIN Batch 21/7700 loss 11.374804 loss_att 12.176713 loss_ctc 16.560698 loss_rnnt 10.408788 hw_loss 0.214091 lr 0.00036982 rank 3
2023-02-25 01:29:59,964 DEBUG TRAIN Batch 21/7700 loss 10.457256 loss_att 11.108471 loss_ctc 12.603542 loss_rnnt 9.947752 hw_loss 0.174542 lr 0.00036977 rank 0
2023-02-25 01:29:59,967 DEBUG TRAIN Batch 21/7700 loss 5.108194 loss_att 9.344128 loss_ctc 7.039043 loss_rnnt 3.916381 hw_loss 0.163461 lr 0.00036978 rank 7
2023-02-25 01:29:59,967 DEBUG TRAIN Batch 21/7700 loss 8.565523 loss_att 10.457169 loss_ctc 11.483833 loss_rnnt 7.694638 hw_loss 0.193964 lr 0.00036979 rank 4
2023-02-25 01:29:59,968 DEBUG TRAIN Batch 21/7700 loss 9.692264 loss_att 12.384941 loss_ctc 16.581123 loss_rnnt 8.075210 hw_loss 0.300006 lr 0.00036976 rank 1
2023-02-25 01:29:59,969 DEBUG TRAIN Batch 21/7700 loss 7.341214 loss_att 8.726450 loss_ctc 9.052954 loss_rnnt 6.753021 hw_loss 0.155463 lr 0.00036978 rank 2
2023-02-25 01:29:59,970 DEBUG TRAIN Batch 21/7700 loss 9.851531 loss_att 13.521775 loss_ctc 17.258980 loss_rnnt 8.084925 hw_loss 0.084183 lr 0.00036978 rank 5
2023-02-25 01:31:14,609 DEBUG TRAIN Batch 21/7800 loss 5.362868 loss_att 8.882044 loss_ctc 10.046564 loss_rnnt 3.930000 hw_loss 0.196013 lr 0.00036972 rank 3
2023-02-25 01:31:14,622 DEBUG TRAIN Batch 21/7800 loss 6.232201 loss_att 11.794804 loss_ctc 11.974502 loss_rnnt 4.250238 hw_loss 0.194630 lr 0.00036967 rank 0
2023-02-25 01:31:14,624 DEBUG TRAIN Batch 21/7800 loss 5.722301 loss_att 8.623734 loss_ctc 11.185928 loss_rnnt 4.265335 hw_loss 0.277868 lr 0.00036965 rank 1
2023-02-25 01:31:14,625 DEBUG TRAIN Batch 21/7800 loss 6.386127 loss_att 10.879618 loss_ctc 9.715000 loss_rnnt 4.983415 hw_loss 0.112809 lr 0.00036969 rank 4
2023-02-25 01:31:14,626 DEBUG TRAIN Batch 21/7800 loss 4.462913 loss_att 7.759617 loss_ctc 4.072619 loss_rnnt 3.776703 hw_loss 0.147950 lr 0.00036968 rank 7
2023-02-25 01:31:14,627 DEBUG TRAIN Batch 21/7800 loss 16.427940 loss_att 16.961052 loss_ctc 18.847015 loss_rnnt 15.915668 hw_loss 0.155828 lr 0.00036969 rank 6
2023-02-25 01:31:14,646 DEBUG TRAIN Batch 21/7800 loss 5.663586 loss_att 10.019684 loss_ctc 14.908058 loss_rnnt 3.404404 hw_loss 0.291310 lr 0.00036968 rank 5
2023-02-25 01:31:14,654 DEBUG TRAIN Batch 21/7800 loss 10.226897 loss_att 15.881423 loss_ctc 13.399357 loss_rnnt 8.548491 hw_loss 0.233447 lr 0.00036968 rank 2
2023-02-25 01:32:27,763 DEBUG TRAIN Batch 21/7900 loss 9.370022 loss_att 10.974460 loss_ctc 11.983130 loss_rnnt 8.608742 hw_loss 0.172459 lr 0.00036957 rank 0
2023-02-25 01:32:27,766 DEBUG TRAIN Batch 21/7900 loss 12.205556 loss_att 13.374172 loss_ctc 16.057274 loss_rnnt 11.377995 hw_loss 0.150516 lr 0.00036958 rank 7
2023-02-25 01:32:27,768 DEBUG TRAIN Batch 21/7900 loss 6.673014 loss_att 7.981326 loss_ctc 11.466162 loss_rnnt 5.696563 hw_loss 0.141944 lr 0.00036958 rank 5
2023-02-25 01:32:27,769 DEBUG TRAIN Batch 21/7900 loss 7.813423 loss_att 10.734149 loss_ctc 9.425320 loss_rnnt 6.863173 hw_loss 0.283472 lr 0.00036957 rank 2
2023-02-25 01:32:27,771 DEBUG TRAIN Batch 21/7900 loss 14.444558 loss_att 16.996613 loss_ctc 15.790176 loss_rnnt 13.594806 hw_loss 0.299862 lr 0.00036959 rank 4
2023-02-25 01:32:27,772 DEBUG TRAIN Batch 21/7900 loss 11.753868 loss_att 15.867865 loss_ctc 17.062136 loss_rnnt 10.129704 hw_loss 0.175490 lr 0.00036962 rank 3
2023-02-25 01:32:27,773 DEBUG TRAIN Batch 21/7900 loss 10.161922 loss_att 14.384375 loss_ctc 13.069115 loss_rnnt 8.855036 hw_loss 0.140193 lr 0.00036955 rank 1
2023-02-25 01:32:27,813 DEBUG TRAIN Batch 21/7900 loss 8.518426 loss_att 10.861872 loss_ctc 10.701536 loss_rnnt 7.655647 hw_loss 0.193141 lr 0.00036959 rank 6
2023-02-25 01:33:40,753 DEBUG TRAIN Batch 21/8000 loss 8.165577 loss_att 10.631919 loss_ctc 12.418242 loss_rnnt 7.004271 hw_loss 0.189404 lr 0.00036947 rank 0
2023-02-25 01:33:40,769 DEBUG TRAIN Batch 21/8000 loss 13.519840 loss_att 15.739918 loss_ctc 21.978769 loss_rnnt 11.784874 hw_loss 0.305800 lr 0.00036948 rank 7
2023-02-25 01:33:40,774 DEBUG TRAIN Batch 21/8000 loss 6.328106 loss_att 9.254616 loss_ctc 10.665639 loss_rnnt 5.036635 hw_loss 0.239682 lr 0.00036952 rank 3
2023-02-25 01:33:40,776 DEBUG TRAIN Batch 21/8000 loss 12.028944 loss_att 15.280401 loss_ctc 13.441685 loss_rnnt 11.090353 hw_loss 0.187378 lr 0.00036948 rank 6
2023-02-25 01:33:40,779 DEBUG TRAIN Batch 21/8000 loss 7.460084 loss_att 11.119705 loss_ctc 8.452858 loss_rnnt 6.474530 hw_loss 0.227362 lr 0.00036947 rank 5
2023-02-25 01:33:40,778 DEBUG TRAIN Batch 21/8000 loss 6.271596 loss_att 10.208288 loss_ctc 9.201778 loss_rnnt 4.995728 hw_loss 0.183447 lr 0.00036949 rank 4
2023-02-25 01:33:40,779 DEBUG TRAIN Batch 21/8000 loss 13.796562 loss_att 16.326576 loss_ctc 15.260316 loss_rnnt 12.958946 hw_loss 0.255838 lr 0.00036947 rank 2
2023-02-25 01:33:40,779 DEBUG TRAIN Batch 21/8000 loss 7.080742 loss_att 9.213828 loss_ctc 7.188665 loss_rnnt 6.521832 hw_loss 0.221068 lr 0.00036945 rank 1
2023-02-25 01:34:54,767 DEBUG TRAIN Batch 21/8100 loss 6.974899 loss_att 9.577527 loss_ctc 11.232760 loss_rnnt 5.816430 hw_loss 0.131677 lr 0.00036938 rank 6
2023-02-25 01:34:54,767 DEBUG TRAIN Batch 21/8100 loss 6.889431 loss_att 10.023758 loss_ctc 9.195771 loss_rnnt 5.891851 hw_loss 0.118504 lr 0.00036942 rank 3
2023-02-25 01:34:54,775 DEBUG TRAIN Batch 21/8100 loss 8.969707 loss_att 11.323758 loss_ctc 11.486440 loss_rnnt 8.023261 hw_loss 0.262632 lr 0.00036938 rank 7
2023-02-25 01:34:54,776 DEBUG TRAIN Batch 21/8100 loss 6.386592 loss_att 9.447208 loss_ctc 7.711099 loss_rnnt 5.480699 hw_loss 0.219690 lr 0.00036937 rank 5
2023-02-25 01:34:54,777 DEBUG TRAIN Batch 21/8100 loss 9.415669 loss_att 11.120276 loss_ctc 13.047158 loss_rnnt 8.497643 hw_loss 0.174197 lr 0.00036937 rank 0
2023-02-25 01:34:54,784 DEBUG TRAIN Batch 21/8100 loss 15.345618 loss_att 20.032482 loss_ctc 22.117516 loss_rnnt 13.405310 hw_loss 0.187531 lr 0.00036935 rank 1
2023-02-25 01:34:54,791 DEBUG TRAIN Batch 21/8100 loss 22.225332 loss_att 21.842665 loss_ctc 26.393751 loss_rnnt 21.646935 hw_loss 0.185891 lr 0.00036939 rank 4
2023-02-25 01:34:54,806 DEBUG TRAIN Batch 21/8100 loss 9.239906 loss_att 11.271069 loss_ctc 11.978897 loss_rnnt 8.326465 hw_loss 0.266270 lr 0.00036937 rank 2
2023-02-25 01:36:08,771 DEBUG TRAIN Batch 21/8200 loss 11.708697 loss_att 11.624321 loss_ctc 13.876419 loss_rnnt 11.324607 hw_loss 0.209879 lr 0.00036927 rank 0
2023-02-25 01:36:08,774 DEBUG TRAIN Batch 21/8200 loss 11.009948 loss_att 13.904730 loss_ctc 15.558722 loss_rnnt 9.717983 hw_loss 0.199697 lr 0.00036925 rank 1
2023-02-25 01:36:08,775 DEBUG TRAIN Batch 21/8200 loss 5.144733 loss_att 8.424225 loss_ctc 7.884405 loss_rnnt 4.027262 hw_loss 0.180533 lr 0.00036929 rank 4
2023-02-25 01:36:08,777 DEBUG TRAIN Batch 21/8200 loss 10.221630 loss_att 10.176708 loss_ctc 13.392328 loss_rnnt 9.683195 hw_loss 0.233736 lr 0.00036928 rank 6
2023-02-25 01:36:08,778 DEBUG TRAIN Batch 21/8200 loss 13.570802 loss_att 13.550755 loss_ctc 20.049381 loss_rnnt 12.564651 hw_loss 0.274402 lr 0.00036927 rank 2
2023-02-25 01:36:08,782 DEBUG TRAIN Batch 21/8200 loss 7.737804 loss_att 9.407821 loss_ctc 13.664202 loss_rnnt 6.475448 hw_loss 0.259063 lr 0.00036932 rank 3
2023-02-25 01:36:08,783 DEBUG TRAIN Batch 21/8200 loss 14.305496 loss_att 15.004396 loss_ctc 21.737537 loss_rnnt 13.018433 hw_loss 0.293145 lr 0.00036928 rank 7
2023-02-25 01:36:08,787 DEBUG TRAIN Batch 21/8200 loss 8.896194 loss_att 11.300321 loss_ctc 11.465881 loss_rnnt 8.003112 hw_loss 0.130557 lr 0.00036927 rank 5
2023-02-25 01:37:21,373 DEBUG TRAIN Batch 21/8300 loss 9.454104 loss_att 13.320925 loss_ctc 12.525457 loss_rnnt 8.160402 hw_loss 0.207796 lr 0.00036917 rank 0
2023-02-25 01:37:21,380 DEBUG TRAIN Batch 21/8300 loss 15.086121 loss_att 17.114561 loss_ctc 20.375854 loss_rnnt 13.905269 hw_loss 0.130998 lr 0.00036915 rank 1
2023-02-25 01:37:21,381 DEBUG TRAIN Batch 21/8300 loss 6.456584 loss_att 9.615896 loss_ctc 10.771536 loss_rnnt 5.133339 hw_loss 0.217602 lr 0.00036918 rank 7
2023-02-25 01:37:21,382 DEBUG TRAIN Batch 21/8300 loss 3.861998 loss_att 7.023042 loss_ctc 9.840363 loss_rnnt 2.356397 hw_loss 0.143019 lr 0.00036917 rank 2
2023-02-25 01:37:21,385 DEBUG TRAIN Batch 21/8300 loss 4.928686 loss_att 9.692502 loss_ctc 4.963045 loss_rnnt 3.878221 hw_loss 0.174601 lr 0.00036918 rank 6
2023-02-25 01:37:21,386 DEBUG TRAIN Batch 21/8300 loss 9.952259 loss_att 12.577116 loss_ctc 13.885398 loss_rnnt 8.807844 hw_loss 0.178172 lr 0.00036917 rank 5
2023-02-25 01:37:21,386 DEBUG TRAIN Batch 21/8300 loss 12.789884 loss_att 14.210378 loss_ctc 14.283324 loss_rnnt 12.204323 hw_loss 0.191881 lr 0.00036922 rank 3
2023-02-25 01:37:21,387 DEBUG TRAIN Batch 21/8300 loss 12.079309 loss_att 17.528345 loss_ctc 19.215542 loss_rnnt 9.880661 hw_loss 0.295018 lr 0.00036919 rank 4
2023-02-25 01:38:08,636 DEBUG CV Batch 21/0 loss 1.155971 loss_att 1.462121 loss_ctc 1.803024 loss_rnnt 0.872808 hw_loss 0.254361 history loss 1.113157 rank 2
2023-02-25 01:38:08,636 DEBUG CV Batch 21/0 loss 1.155971 loss_att 1.462121 loss_ctc 1.803024 loss_rnnt 0.872808 hw_loss 0.254361 history loss 1.113157 rank 1
2023-02-25 01:38:08,640 DEBUG CV Batch 21/0 loss 1.155971 loss_att 1.462121 loss_ctc 1.803024 loss_rnnt 0.872808 hw_loss 0.254361 history loss 1.113157 rank 0
2023-02-25 01:38:08,642 DEBUG CV Batch 21/0 loss 1.155971 loss_att 1.462121 loss_ctc 1.803024 loss_rnnt 0.872808 hw_loss 0.254361 history loss 1.113157 rank 4
2023-02-25 01:38:08,645 DEBUG CV Batch 21/0 loss 1.155971 loss_att 1.462121 loss_ctc 1.803024 loss_rnnt 0.872808 hw_loss 0.254361 history loss 1.113157 rank 3
2023-02-25 01:38:08,651 DEBUG CV Batch 21/0 loss 1.155971 loss_att 1.462121 loss_ctc 1.803024 loss_rnnt 0.872808 hw_loss 0.254361 history loss 1.113157 rank 5
2023-02-25 01:38:08,653 DEBUG CV Batch 21/0 loss 1.155971 loss_att 1.462121 loss_ctc 1.803024 loss_rnnt 0.872808 hw_loss 0.254361 history loss 1.113157 rank 7
2023-02-25 01:38:08,656 DEBUG CV Batch 21/0 loss 1.155971 loss_att 1.462121 loss_ctc 1.803024 loss_rnnt 0.872808 hw_loss 0.254361 history loss 1.113157 rank 6
2023-02-25 01:38:20,049 DEBUG CV Batch 21/100 loss 6.960515 loss_att 6.992019 loss_ctc 10.216463 loss_rnnt 6.445278 hw_loss 0.140270 history loss 3.361076 rank 3
2023-02-25 01:38:20,068 DEBUG CV Batch 21/100 loss 6.960515 loss_att 6.992019 loss_ctc 10.216463 loss_rnnt 6.445278 hw_loss 0.140270 history loss 3.361076 rank 1
2023-02-25 01:38:20,092 DEBUG CV Batch 21/100 loss 6.960515 loss_att 6.992019 loss_ctc 10.216463 loss_rnnt 6.445278 hw_loss 0.140270 history loss 3.361076 rank 4
2023-02-25 01:38:20,107 DEBUG CV Batch 21/100 loss 6.960515 loss_att 6.992019 loss_ctc 10.216463 loss_rnnt 6.445278 hw_loss 0.140270 history loss 3.361076 rank 2
2023-02-25 01:38:20,226 DEBUG CV Batch 21/100 loss 6.960515 loss_att 6.992019 loss_ctc 10.216463 loss_rnnt 6.445278 hw_loss 0.140270 history loss 3.361076 rank 5
2023-02-25 01:38:20,280 DEBUG CV Batch 21/100 loss 6.960515 loss_att 6.992019 loss_ctc 10.216463 loss_rnnt 6.445278 hw_loss 0.140270 history loss 3.361076 rank 6
2023-02-25 01:38:20,454 DEBUG CV Batch 21/100 loss 6.960515 loss_att 6.992019 loss_ctc 10.216463 loss_rnnt 6.445278 hw_loss 0.140270 history loss 3.361076 rank 0
2023-02-25 01:38:20,608 DEBUG CV Batch 21/100 loss 6.960515 loss_att 6.992019 loss_ctc 10.216463 loss_rnnt 6.445278 hw_loss 0.140270 history loss 3.361076 rank 7
2023-02-25 01:38:33,695 DEBUG CV Batch 21/200 loss 6.178635 loss_att 13.164171 loss_ctc 7.382113 loss_rnnt 4.524310 hw_loss 0.181412 history loss 4.008183 rank 3
2023-02-25 01:38:33,712 DEBUG CV Batch 21/200 loss 6.178635 loss_att 13.164171 loss_ctc 7.382113 loss_rnnt 4.524310 hw_loss 0.181412 history loss 4.008183 rank 1
2023-02-25 01:38:33,759 DEBUG CV Batch 21/200 loss 6.178635 loss_att 13.164171 loss_ctc 7.382113 loss_rnnt 4.524310 hw_loss 0.181412 history loss 4.008183 rank 2
2023-02-25 01:38:33,876 DEBUG CV Batch 21/200 loss 6.178635 loss_att 13.164171 loss_ctc 7.382113 loss_rnnt 4.524310 hw_loss 0.181412 history loss 4.008183 rank 5
2023-02-25 01:38:34,160 DEBUG CV Batch 21/200 loss 6.178635 loss_att 13.164171 loss_ctc 7.382113 loss_rnnt 4.524310 hw_loss 0.181412 history loss 4.008183 rank 4
2023-02-25 01:38:34,281 DEBUG CV Batch 21/200 loss 6.178635 loss_att 13.164171 loss_ctc 7.382113 loss_rnnt 4.524310 hw_loss 0.181412 history loss 4.008183 rank 6
2023-02-25 01:38:34,317 DEBUG CV Batch 21/200 loss 6.178635 loss_att 13.164171 loss_ctc 7.382113 loss_rnnt 4.524310 hw_loss 0.181412 history loss 4.008183 rank 0
2023-02-25 01:38:34,651 DEBUG CV Batch 21/200 loss 6.178635 loss_att 13.164171 loss_ctc 7.382113 loss_rnnt 4.524310 hw_loss 0.181412 history loss 4.008183 rank 7
2023-02-25 01:38:45,571 DEBUG CV Batch 21/300 loss 5.263337 loss_att 5.313741 loss_ctc 7.728860 loss_rnnt 4.811581 hw_loss 0.211763 history loss 4.155188 rank 3
2023-02-25 01:38:45,738 DEBUG CV Batch 21/300 loss 5.263337 loss_att 5.313741 loss_ctc 7.728860 loss_rnnt 4.811581 hw_loss 0.211763 history loss 4.155188 rank 1
2023-02-25 01:38:45,887 DEBUG CV Batch 21/300 loss 5.263337 loss_att 5.313741 loss_ctc 7.728860 loss_rnnt 4.811581 hw_loss 0.211763 history loss 4.155188 rank 2
2023-02-25 01:38:46,101 DEBUG CV Batch 21/300 loss 5.263337 loss_att 5.313741 loss_ctc 7.728860 loss_rnnt 4.811581 hw_loss 0.211763 history loss 4.155188 rank 4
2023-02-25 01:38:46,132 DEBUG CV Batch 21/300 loss 5.263337 loss_att 5.313741 loss_ctc 7.728860 loss_rnnt 4.811581 hw_loss 0.211763 history loss 4.155188 rank 5
2023-02-25 01:38:46,365 DEBUG CV Batch 21/300 loss 5.263337 loss_att 5.313741 loss_ctc 7.728860 loss_rnnt 4.811581 hw_loss 0.211763 history loss 4.155188 rank 6
2023-02-25 01:38:46,925 DEBUG CV Batch 21/300 loss 5.263337 loss_att 5.313741 loss_ctc 7.728860 loss_rnnt 4.811581 hw_loss 0.211763 history loss 4.155188 rank 0
2023-02-25 01:38:47,250 DEBUG CV Batch 21/300 loss 5.263337 loss_att 5.313741 loss_ctc 7.728860 loss_rnnt 4.811581 hw_loss 0.211763 history loss 4.155188 rank 7
2023-02-25 01:38:57,550 DEBUG CV Batch 21/400 loss 19.768078 loss_att 86.861763 loss_ctc 9.742896 loss_rnnt 7.627201 hw_loss 0.110305 history loss 5.070857 rank 3
2023-02-25 01:38:57,806 DEBUG CV Batch 21/400 loss 19.768078 loss_att 86.861763 loss_ctc 9.742896 loss_rnnt 7.627201 hw_loss 0.110305 history loss 5.070857 rank 1
2023-02-25 01:38:57,931 DEBUG CV Batch 21/400 loss 19.768078 loss_att 86.861763 loss_ctc 9.742896 loss_rnnt 7.627201 hw_loss 0.110305 history loss 5.070857 rank 4
2023-02-25 01:38:58,133 DEBUG CV Batch 21/400 loss 19.768078 loss_att 86.861763 loss_ctc 9.742896 loss_rnnt 7.627201 hw_loss 0.110305 history loss 5.070857 rank 2
2023-02-25 01:38:58,373 DEBUG CV Batch 21/400 loss 19.768078 loss_att 86.861763 loss_ctc 9.742896 loss_rnnt 7.627201 hw_loss 0.110305 history loss 5.070857 rank 5
2023-02-25 01:38:58,452 DEBUG CV Batch 21/400 loss 19.768078 loss_att 86.861763 loss_ctc 9.742896 loss_rnnt 7.627201 hw_loss 0.110305 history loss 5.070857 rank 6
2023-02-25 01:38:59,598 DEBUG CV Batch 21/400 loss 19.768078 loss_att 86.861763 loss_ctc 9.742896 loss_rnnt 7.627201 hw_loss 0.110305 history loss 5.070857 rank 0
2023-02-25 01:38:59,912 DEBUG CV Batch 21/400 loss 19.768078 loss_att 86.861763 loss_ctc 9.742896 loss_rnnt 7.627201 hw_loss 0.110305 history loss 5.070857 rank 7
2023-02-25 01:39:07,881 DEBUG CV Batch 21/500 loss 5.522604 loss_att 5.692528 loss_ctc 6.791567 loss_rnnt 5.197538 hw_loss 0.228536 history loss 5.784365 rank 3
2023-02-25 01:39:08,401 DEBUG CV Batch 21/500 loss 5.522604 loss_att 5.692528 loss_ctc 6.791567 loss_rnnt 5.197538 hw_loss 0.228536 history loss 5.784365 rank 4
2023-02-25 01:39:08,555 DEBUG CV Batch 21/500 loss 5.522604 loss_att 5.692528 loss_ctc 6.791567 loss_rnnt 5.197538 hw_loss 0.228536 history loss 5.784365 rank 2
2023-02-25 01:39:08,742 DEBUG CV Batch 21/500 loss 5.522604 loss_att 5.692528 loss_ctc 6.791567 loss_rnnt 5.197538 hw_loss 0.228536 history loss 5.784365 rank 5
2023-02-25 01:39:08,891 DEBUG CV Batch 21/500 loss 5.522604 loss_att 5.692528 loss_ctc 6.791567 loss_rnnt 5.197538 hw_loss 0.228536 history loss 5.784365 rank 6
2023-02-25 01:39:09,108 DEBUG CV Batch 21/500 loss 5.522604 loss_att 5.692528 loss_ctc 6.791567 loss_rnnt 5.197538 hw_loss 0.228536 history loss 5.784365 rank 1
2023-02-25 01:39:10,680 DEBUG CV Batch 21/500 loss 5.522604 loss_att 5.692528 loss_ctc 6.791567 loss_rnnt 5.197538 hw_loss 0.228536 history loss 5.784365 rank 0
2023-02-25 01:39:11,092 DEBUG CV Batch 21/500 loss 5.522604 loss_att 5.692528 loss_ctc 6.791567 loss_rnnt 5.197538 hw_loss 0.228536 history loss 5.784365 rank 7
2023-02-25 01:39:20,106 DEBUG CV Batch 21/600 loss 7.268199 loss_att 6.692890 loss_ctc 9.293177 loss_rnnt 6.930167 hw_loss 0.343307 history loss 6.727823 rank 3
2023-02-25 01:39:20,803 DEBUG CV Batch 21/600 loss 7.268199 loss_att 6.692890 loss_ctc 9.293177 loss_rnnt 6.930167 hw_loss 0.343307 history loss 6.727823 rank 2
2023-02-25 01:39:20,849 DEBUG CV Batch 21/600 loss 7.268199 loss_att 6.692890 loss_ctc 9.293177 loss_rnnt 6.930167 hw_loss 0.343307 history loss 6.727823 rank 5
2023-02-25 01:39:20,855 DEBUG CV Batch 21/600 loss 7.268199 loss_att 6.692890 loss_ctc 9.293177 loss_rnnt 6.930167 hw_loss 0.343307 history loss 6.727823 rank 4
2023-02-25 01:39:20,960 DEBUG CV Batch 21/600 loss 7.268199 loss_att 6.692890 loss_ctc 9.293177 loss_rnnt 6.930167 hw_loss 0.343307 history loss 6.727823 rank 6
2023-02-25 01:39:21,165 DEBUG CV Batch 21/600 loss 7.268199 loss_att 6.692890 loss_ctc 9.293177 loss_rnnt 6.930167 hw_loss 0.343307 history loss 6.727823 rank 1
2023-02-25 01:39:23,271 DEBUG CV Batch 21/600 loss 7.268199 loss_att 6.692890 loss_ctc 9.293177 loss_rnnt 6.930167 hw_loss 0.343307 history loss 6.727823 rank 0
2023-02-25 01:39:23,754 DEBUG CV Batch 21/600 loss 7.268199 loss_att 6.692890 loss_ctc 9.293177 loss_rnnt 6.930167 hw_loss 0.343307 history loss 6.727823 rank 7
2023-02-25 01:39:32,071 DEBUG CV Batch 21/700 loss 14.160478 loss_att 32.997284 loss_ctc 17.622375 loss_rnnt 9.901404 hw_loss 0.056487 history loss 7.357608 rank 3
2023-02-25 01:39:32,180 DEBUG CV Batch 21/700 loss 14.160478 loss_att 32.997284 loss_ctc 17.622375 loss_rnnt 9.901404 hw_loss 0.056487 history loss 7.357608 rank 5
2023-02-25 01:39:32,285 DEBUG CV Batch 21/700 loss 14.160478 loss_att 32.997284 loss_ctc 17.622375 loss_rnnt 9.901404 hw_loss 0.056487 history loss 7.357608 rank 2
2023-02-25 01:39:32,303 DEBUG CV Batch 21/700 loss 14.160478 loss_att 32.997284 loss_ctc 17.622375 loss_rnnt 9.901404 hw_loss 0.056487 history loss 7.357608 rank 6
2023-02-25 01:39:32,309 DEBUG CV Batch 21/700 loss 14.160478 loss_att 32.997284 loss_ctc 17.622375 loss_rnnt 9.901404 hw_loss 0.056487 history loss 7.357608 rank 1
2023-02-25 01:39:33,346 DEBUG CV Batch 21/700 loss 14.160478 loss_att 32.997284 loss_ctc 17.622375 loss_rnnt 9.901404 hw_loss 0.056487 history loss 7.357608 rank 4
2023-02-25 01:39:35,163 DEBUG CV Batch 21/700 loss 14.160478 loss_att 32.997284 loss_ctc 17.622375 loss_rnnt 9.901404 hw_loss 0.056487 history loss 7.357608 rank 0
2023-02-25 01:39:35,691 DEBUG CV Batch 21/700 loss 14.160478 loss_att 32.997284 loss_ctc 17.622375 loss_rnnt 9.901404 hw_loss 0.056487 history loss 7.357608 rank 7
2023-02-25 01:39:43,422 DEBUG CV Batch 21/800 loss 11.837532 loss_att 10.728547 loss_ctc 16.861111 loss_rnnt 11.253731 hw_loss 0.254600 history loss 6.815353 rank 1
2023-02-25 01:39:43,469 DEBUG CV Batch 21/800 loss 11.837532 loss_att 10.728547 loss_ctc 16.861111 loss_rnnt 11.253731 hw_loss 0.254600 history loss 6.815353 rank 5
2023-02-25 01:39:43,529 DEBUG CV Batch 21/800 loss 11.837532 loss_att 10.728547 loss_ctc 16.861111 loss_rnnt 11.253731 hw_loss 0.254600 history loss 6.815353 rank 2
2023-02-25 01:39:43,880 DEBUG CV Batch 21/800 loss 11.837532 loss_att 10.728547 loss_ctc 16.861111 loss_rnnt 11.253731 hw_loss 0.254600 history loss 6.815353 rank 3
2023-02-25 01:39:44,016 DEBUG CV Batch 21/800 loss 11.837532 loss_att 10.728547 loss_ctc 16.861111 loss_rnnt 11.253731 hw_loss 0.254600 history loss 6.815353 rank 6
2023-02-25 01:39:45,530 DEBUG CV Batch 21/800 loss 11.837532 loss_att 10.728547 loss_ctc 16.861111 loss_rnnt 11.253731 hw_loss 0.254600 history loss 6.815353 rank 4
2023-02-25 01:39:46,983 DEBUG CV Batch 21/800 loss 11.837532 loss_att 10.728547 loss_ctc 16.861111 loss_rnnt 11.253731 hw_loss 0.254600 history loss 6.815353 rank 0
2023-02-25 01:39:47,557 DEBUG CV Batch 21/800 loss 11.837532 loss_att 10.728547 loss_ctc 16.861111 loss_rnnt 11.253731 hw_loss 0.254600 history loss 6.815353 rank 7
2023-02-25 01:39:56,750 DEBUG CV Batch 21/900 loss 15.796710 loss_att 15.920161 loss_ctc 23.864937 loss_rnnt 14.568744 hw_loss 0.239087 history loss 6.630067 rank 1
2023-02-25 01:39:56,985 DEBUG CV Batch 21/900 loss 15.796710 loss_att 15.920161 loss_ctc 23.864937 loss_rnnt 14.568744 hw_loss 0.239087 history loss 6.630067 rank 5
2023-02-25 01:39:57,020 DEBUG CV Batch 21/900 loss 15.796710 loss_att 15.920161 loss_ctc 23.864937 loss_rnnt 14.568744 hw_loss 0.239087 history loss 6.630067 rank 2
2023-02-25 01:39:57,564 DEBUG CV Batch 21/900 loss 15.796710 loss_att 15.920161 loss_ctc 23.864937 loss_rnnt 14.568744 hw_loss 0.239087 history loss 6.630067 rank 6
2023-02-25 01:39:57,857 DEBUG CV Batch 21/900 loss 15.796710 loss_att 15.920161 loss_ctc 23.864937 loss_rnnt 14.568744 hw_loss 0.239087 history loss 6.630067 rank 3
2023-02-25 01:39:59,550 DEBUG CV Batch 21/900 loss 15.796710 loss_att 15.920161 loss_ctc 23.864937 loss_rnnt 14.568744 hw_loss 0.239087 history loss 6.630067 rank 4
2023-02-25 01:40:00,877 DEBUG CV Batch 21/900 loss 15.796710 loss_att 15.920161 loss_ctc 23.864937 loss_rnnt 14.568744 hw_loss 0.239087 history loss 6.630067 rank 0
2023-02-25 01:40:01,567 DEBUG CV Batch 21/900 loss 15.796710 loss_att 15.920161 loss_ctc 23.864937 loss_rnnt 14.568744 hw_loss 0.239087 history loss 6.630067 rank 7
2023-02-25 01:40:08,748 DEBUG CV Batch 21/1000 loss 4.784246 loss_att 4.809702 loss_ctc 5.069936 loss_rnnt 4.596720 hw_loss 0.270644 history loss 6.413414 rank 1
2023-02-25 01:40:09,076 DEBUG CV Batch 21/1000 loss 4.784246 loss_att 4.809702 loss_ctc 5.069936 loss_rnnt 4.596720 hw_loss 0.270644 history loss 6.413414 rank 5
2023-02-25 01:40:09,173 DEBUG CV Batch 21/1000 loss 4.784246 loss_att 4.809702 loss_ctc 5.069936 loss_rnnt 4.596720 hw_loss 0.270644 history loss 6.413414 rank 2
2023-02-25 01:40:09,707 DEBUG CV Batch 21/1000 loss 4.784246 loss_att 4.809702 loss_ctc 5.069936 loss_rnnt 4.596720 hw_loss 0.270644 history loss 6.413414 rank 6
2023-02-25 01:40:10,040 DEBUG CV Batch 21/1000 loss 4.784246 loss_att 4.809702 loss_ctc 5.069936 loss_rnnt 4.596720 hw_loss 0.270644 history loss 6.413414 rank 3
2023-02-25 01:40:11,662 DEBUG CV Batch 21/1000 loss 4.784246 loss_att 4.809702 loss_ctc 5.069936 loss_rnnt 4.596720 hw_loss 0.270644 history loss 6.413414 rank 4
2023-02-25 01:40:13,696 DEBUG CV Batch 21/1000 loss 4.784246 loss_att 4.809702 loss_ctc 5.069936 loss_rnnt 4.596720 hw_loss 0.270644 history loss 6.413414 rank 0
2023-02-25 01:40:14,590 DEBUG CV Batch 21/1000 loss 4.784246 loss_att 4.809702 loss_ctc 5.069936 loss_rnnt 4.596720 hw_loss 0.270644 history loss 6.413414 rank 7
2023-02-25 01:40:20,521 DEBUG CV Batch 21/1100 loss 5.776106 loss_att 5.227777 loss_ctc 8.111126 loss_rnnt 5.468668 hw_loss 0.198317 history loss 6.389728 rank 1
2023-02-25 01:40:21,112 DEBUG CV Batch 21/1100 loss 5.776106 loss_att 5.227777 loss_ctc 8.111126 loss_rnnt 5.468668 hw_loss 0.198317 history loss 6.389728 rank 5
2023-02-25 01:40:21,499 DEBUG CV Batch 21/1100 loss 5.776106 loss_att 5.227777 loss_ctc 8.111126 loss_rnnt 5.468668 hw_loss 0.198317 history loss 6.389728 rank 2
2023-02-25 01:40:21,931 DEBUG CV Batch 21/1100 loss 5.776106 loss_att 5.227777 loss_ctc 8.111126 loss_rnnt 5.468668 hw_loss 0.198317 history loss 6.389728 rank 6
2023-02-25 01:40:22,207 DEBUG CV Batch 21/1100 loss 5.776106 loss_att 5.227777 loss_ctc 8.111126 loss_rnnt 5.468668 hw_loss 0.198317 history loss 6.389728 rank 3
2023-02-25 01:40:23,408 DEBUG CV Batch 21/1100 loss 5.776106 loss_att 5.227777 loss_ctc 8.111126 loss_rnnt 5.468668 hw_loss 0.198317 history loss 6.389728 rank 4
2023-02-25 01:40:26,371 DEBUG CV Batch 21/1100 loss 5.776106 loss_att 5.227777 loss_ctc 8.111126 loss_rnnt 5.468668 hw_loss 0.198317 history loss 6.389728 rank 0
2023-02-25 01:40:27,219 DEBUG CV Batch 21/1100 loss 5.776106 loss_att 5.227777 loss_ctc 8.111126 loss_rnnt 5.468668 hw_loss 0.198317 history loss 6.389728 rank 7
2023-02-25 01:40:30,767 DEBUG CV Batch 21/1200 loss 8.176617 loss_att 9.151423 loss_ctc 9.407695 loss_rnnt 7.682756 hw_loss 0.252666 history loss 6.701779 rank 1
2023-02-25 01:40:31,579 DEBUG CV Batch 21/1200 loss 8.176617 loss_att 9.151423 loss_ctc 9.407695 loss_rnnt 7.682756 hw_loss 0.252666 history loss 6.701779 rank 5
2023-02-25 01:40:32,015 DEBUG CV Batch 21/1200 loss 8.176617 loss_att 9.151423 loss_ctc 9.407695 loss_rnnt 7.682756 hw_loss 0.252666 history loss 6.701779 rank 2
2023-02-25 01:40:32,375 DEBUG CV Batch 21/1200 loss 8.176617 loss_att 9.151423 loss_ctc 9.407695 loss_rnnt 7.682756 hw_loss 0.252666 history loss 6.701779 rank 6
2023-02-25 01:40:32,912 DEBUG CV Batch 21/1200 loss 8.176617 loss_att 9.151423 loss_ctc 9.407695 loss_rnnt 7.682756 hw_loss 0.252666 history loss 6.701779 rank 3
2023-02-25 01:40:34,140 DEBUG CV Batch 21/1200 loss 8.176617 loss_att 9.151423 loss_ctc 9.407695 loss_rnnt 7.682756 hw_loss 0.252666 history loss 6.701779 rank 4
2023-02-25 01:40:37,736 DEBUG CV Batch 21/1200 loss 8.176617 loss_att 9.151423 loss_ctc 9.407695 loss_rnnt 7.682756 hw_loss 0.252666 history loss 6.701779 rank 0
2023-02-25 01:40:38,807 DEBUG CV Batch 21/1200 loss 8.176617 loss_att 9.151423 loss_ctc 9.407695 loss_rnnt 7.682756 hw_loss 0.252666 history loss 6.701779 rank 7
2023-02-25 01:40:42,655 DEBUG CV Batch 21/1300 loss 5.085329 loss_att 5.409057 loss_ctc 7.279716 loss_rnnt 4.609587 hw_loss 0.222021 history loss 7.026893 rank 1
2023-02-25 01:40:43,789 DEBUG CV Batch 21/1300 loss 5.085329 loss_att 5.409057 loss_ctc 7.279716 loss_rnnt 4.609587 hw_loss 0.222021 history loss 7.026893 rank 5
2023-02-25 01:40:44,272 DEBUG CV Batch 21/1300 loss 5.085329 loss_att 5.409057 loss_ctc 7.279716 loss_rnnt 4.609587 hw_loss 0.222021 history loss 7.026893 rank 2
2023-02-25 01:40:44,527 DEBUG CV Batch 21/1300 loss 5.085329 loss_att 5.409057 loss_ctc 7.279716 loss_rnnt 4.609587 hw_loss 0.222021 history loss 7.026893 rank 6
2023-02-25 01:40:44,905 DEBUG CV Batch 21/1300 loss 5.085329 loss_att 5.409057 loss_ctc 7.279716 loss_rnnt 4.609587 hw_loss 0.222021 history loss 7.026893 rank 3
2023-02-25 01:40:46,351 DEBUG CV Batch 21/1300 loss 5.085329 loss_att 5.409057 loss_ctc 7.279716 loss_rnnt 4.609587 hw_loss 0.222021 history loss 7.026893 rank 4
2023-02-25 01:40:50,374 DEBUG CV Batch 21/1300 loss 5.085329 loss_att 5.409057 loss_ctc 7.279716 loss_rnnt 4.609587 hw_loss 0.222021 history loss 7.026893 rank 0
2023-02-25 01:40:51,595 DEBUG CV Batch 21/1300 loss 5.085329 loss_att 5.409057 loss_ctc 7.279716 loss_rnnt 4.609587 hw_loss 0.222021 history loss 7.026893 rank 7
2023-02-25 01:40:54,019 DEBUG CV Batch 21/1400 loss 6.659594 loss_att 16.597456 loss_ctc 6.951748 loss_rnnt 4.529387 hw_loss 0.194401 history loss 7.331173 rank 1
2023-02-25 01:40:54,963 DEBUG CV Batch 21/1400 loss 6.659594 loss_att 16.597456 loss_ctc 6.951748 loss_rnnt 4.529387 hw_loss 0.194401 history loss 7.331173 rank 5
2023-02-25 01:40:55,598 DEBUG CV Batch 21/1400 loss 6.659594 loss_att 16.597456 loss_ctc 6.951748 loss_rnnt 4.529387 hw_loss 0.194401 history loss 7.331173 rank 2
2023-02-25 01:40:55,776 DEBUG CV Batch 21/1400 loss 6.659594 loss_att 16.597456 loss_ctc 6.951748 loss_rnnt 4.529387 hw_loss 0.194401 history loss 7.331173 rank 6
2023-02-25 01:40:56,286 DEBUG CV Batch 21/1400 loss 6.659594 loss_att 16.597456 loss_ctc 6.951748 loss_rnnt 4.529387 hw_loss 0.194401 history loss 7.331173 rank 3
2023-02-25 01:40:59,169 DEBUG CV Batch 21/1400 loss 6.659594 loss_att 16.597456 loss_ctc 6.951748 loss_rnnt 4.529387 hw_loss 0.194401 history loss 7.331173 rank 4
2023-02-25 01:41:02,386 DEBUG CV Batch 21/1400 loss 6.659594 loss_att 16.597456 loss_ctc 6.951748 loss_rnnt 4.529387 hw_loss 0.194401 history loss 7.331173 rank 0
2023-02-25 01:41:03,576 DEBUG CV Batch 21/1400 loss 6.659594 loss_att 16.597456 loss_ctc 6.951748 loss_rnnt 4.529387 hw_loss 0.194401 history loss 7.331173 rank 7
2023-02-25 01:41:05,824 DEBUG CV Batch 21/1500 loss 7.267971 loss_att 8.050904 loss_ctc 5.894339 loss_rnnt 7.193279 hw_loss 0.189854 history loss 7.160403 rank 1
2023-02-25 01:41:06,517 DEBUG CV Batch 21/1500 loss 7.267971 loss_att 8.050904 loss_ctc 5.894339 loss_rnnt 7.193279 hw_loss 0.189854 history loss 7.160403 rank 5
2023-02-25 01:41:06,911 DEBUG CV Batch 21/1500 loss 7.267971 loss_att 8.050904 loss_ctc 5.894339 loss_rnnt 7.193279 hw_loss 0.189854 history loss 7.160403 rank 2
2023-02-25 01:41:07,092 DEBUG CV Batch 21/1500 loss 7.267971 loss_att 8.050904 loss_ctc 5.894339 loss_rnnt 7.193279 hw_loss 0.189854 history loss 7.160403 rank 6
2023-02-25 01:41:08,751 DEBUG CV Batch 21/1500 loss 7.267971 loss_att 8.050904 loss_ctc 5.894339 loss_rnnt 7.193279 hw_loss 0.189854 history loss 7.160403 rank 3
2023-02-25 01:41:11,946 DEBUG CV Batch 21/1500 loss 7.267971 loss_att 8.050904 loss_ctc 5.894339 loss_rnnt 7.193279 hw_loss 0.189854 history loss 7.160403 rank 4
2023-02-25 01:41:14,519 DEBUG CV Batch 21/1500 loss 7.267971 loss_att 8.050904 loss_ctc 5.894339 loss_rnnt 7.193279 hw_loss 0.189854 history loss 7.160403 rank 0
2023-02-25 01:41:15,885 DEBUG CV Batch 21/1500 loss 7.267971 loss_att 8.050904 loss_ctc 5.894339 loss_rnnt 7.193279 hw_loss 0.189854 history loss 7.160403 rank 7
2023-02-25 01:41:18,904 DEBUG CV Batch 21/1600 loss 9.548522 loss_att 12.856101 loss_ctc 6.935523 loss_rnnt 9.140276 hw_loss 0.178367 history loss 7.099956 rank 1
2023-02-25 01:41:19,814 DEBUG CV Batch 21/1600 loss 9.548522 loss_att 12.856101 loss_ctc 6.935523 loss_rnnt 9.140276 hw_loss 0.178367 history loss 7.099956 rank 5
2023-02-25 01:41:20,187 DEBUG CV Batch 21/1600 loss 9.548522 loss_att 12.856101 loss_ctc 6.935523 loss_rnnt 9.140276 hw_loss 0.178367 history loss 7.099956 rank 2
2023-02-25 01:41:20,326 DEBUG CV Batch 21/1600 loss 9.548522 loss_att 12.856101 loss_ctc 6.935523 loss_rnnt 9.140276 hw_loss 0.178367 history loss 7.099956 rank 6
2023-02-25 01:41:22,669 DEBUG CV Batch 21/1600 loss 9.548522 loss_att 12.856101 loss_ctc 6.935523 loss_rnnt 9.140276 hw_loss 0.178367 history loss 7.099956 rank 3
2023-02-25 01:41:25,654 DEBUG CV Batch 21/1600 loss 9.548522 loss_att 12.856101 loss_ctc 6.935523 loss_rnnt 9.140276 hw_loss 0.178367 history loss 7.099956 rank 4
2023-02-25 01:41:28,271 DEBUG CV Batch 21/1600 loss 9.548522 loss_att 12.856101 loss_ctc 6.935523 loss_rnnt 9.140276 hw_loss 0.178367 history loss 7.099956 rank 0
2023-02-25 01:41:29,603 DEBUG CV Batch 21/1600 loss 9.548522 loss_att 12.856101 loss_ctc 6.935523 loss_rnnt 9.140276 hw_loss 0.178367 history loss 7.099956 rank 7
2023-02-25 01:41:31,306 DEBUG CV Batch 21/1700 loss 13.043280 loss_att 10.666796 loss_ctc 17.911741 loss_rnnt 12.730742 hw_loss 0.260077 history loss 7.017646 rank 1
2023-02-25 01:41:32,561 DEBUG CV Batch 21/1700 loss 13.043280 loss_att 10.666796 loss_ctc 17.911741 loss_rnnt 12.730742 hw_loss 0.260077 history loss 7.017646 rank 5
2023-02-25 01:41:32,976 DEBUG CV Batch 21/1700 loss 13.043280 loss_att 10.666796 loss_ctc 17.911741 loss_rnnt 12.730742 hw_loss 0.260077 history loss 7.017646 rank 2
2023-02-25 01:41:33,159 DEBUG CV Batch 21/1700 loss 13.043280 loss_att 10.666796 loss_ctc 17.911741 loss_rnnt 12.730742 hw_loss 0.260077 history loss 7.017646 rank 6
2023-02-25 01:41:35,847 DEBUG CV Batch 21/1700 loss 13.043280 loss_att 10.666796 loss_ctc 17.911741 loss_rnnt 12.730742 hw_loss 0.260077 history loss 7.017646 rank 3
2023-02-25 01:41:37,953 DEBUG CV Batch 21/1700 loss 13.043280 loss_att 10.666796 loss_ctc 17.911741 loss_rnnt 12.730742 hw_loss 0.260077 history loss 7.017646 rank 4
2023-02-25 01:41:40,812 INFO Epoch 21 CV info cv_loss 6.991431628956514
2023-02-25 01:41:40,813 INFO Epoch 22 TRAIN info lr 0.0003691338932512392
2023-02-25 01:41:40,815 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 01:41:41,098 DEBUG CV Batch 21/1700 loss 13.043280 loss_att 10.666796 loss_ctc 17.911741 loss_rnnt 12.730742 hw_loss 0.260077 history loss 7.017646 rank 0
2023-02-25 01:41:42,147 INFO Epoch 21 CV info cv_loss 6.991431627853841
2023-02-25 01:41:42,147 INFO Epoch 22 TRAIN info lr 0.00036911578725956536
2023-02-25 01:41:42,153 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 01:41:42,195 DEBUG CV Batch 21/1700 loss 13.043280 loss_att 10.666796 loss_ctc 17.911741 loss_rnnt 12.730742 hw_loss 0.260077 history loss 7.017646 rank 7
2023-02-25 01:41:42,497 INFO Epoch 21 CV info cv_loss 6.991431629378631
2023-02-25 01:41:42,498 INFO Epoch 22 TRAIN info lr 0.00036912182229408756
2023-02-25 01:41:42,500 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 01:41:42,629 INFO Epoch 21 CV info cv_loss 6.991431628922055
2023-02-25 01:41:42,630 INFO Epoch 22 TRAIN info lr 0.00036912986946727546
2023-02-25 01:41:42,635 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 01:41:45,311 INFO Epoch 21 CV info cv_loss 6.991431628999587
2023-02-25 01:41:45,312 INFO Epoch 22 TRAIN info lr 0.0003691862144200123
2023-02-25 01:41:45,317 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 01:41:47,621 INFO Epoch 21 CV info cv_loss 6.991431629326944
2023-02-25 01:41:47,622 INFO Epoch 22 TRAIN info lr 0.0003691630696207679
2023-02-25 01:41:47,627 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 01:41:50,677 INFO Epoch 21 CV info cv_loss 6.99143162835349
2023-02-25 01:41:50,677 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/21.pt
2023-02-25 01:41:51,299 INFO Epoch 22 TRAIN info lr 0.0003691479775314256
2023-02-25 01:41:51,304 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 01:41:51,790 INFO Epoch 21 CV info cv_loss 6.9914316282070414
2023-02-25 01:41:51,790 INFO Epoch 22 TRAIN info lr 0.0003691359051925666
2023-02-25 01:41:51,792 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 01:42:51,491 DEBUG TRAIN Batch 22/0 loss 11.141158 loss_att 11.417547 loss_ctc 13.051683 loss_rnnt 10.695610 hw_loss 0.254125 lr 0.00036913 rank 1
2023-02-25 01:42:51,491 DEBUG TRAIN Batch 22/0 loss 9.275624 loss_att 8.841849 loss_ctc 12.420327 loss_rnnt 8.793699 hw_loss 0.280098 lr 0.00036919 rank 3
2023-02-25 01:42:51,496 DEBUG TRAIN Batch 22/0 loss 8.531341 loss_att 9.052355 loss_ctc 13.119680 loss_rnnt 7.681965 hw_loss 0.250116 lr 0.00036913 rank 6
2023-02-25 01:42:51,508 DEBUG TRAIN Batch 22/0 loss 9.432386 loss_att 9.283841 loss_ctc 13.686215 loss_rnnt 8.766715 hw_loss 0.240381 lr 0.00036915 rank 0
2023-02-25 01:42:51,516 DEBUG TRAIN Batch 22/0 loss 9.066904 loss_att 8.692739 loss_ctc 12.364445 loss_rnnt 8.570127 hw_loss 0.247384 lr 0.00036916 rank 4
2023-02-25 01:42:51,525 DEBUG TRAIN Batch 22/0 loss 12.540816 loss_att 11.584770 loss_ctc 16.228691 loss_rnnt 12.116834 hw_loss 0.231515 lr 0.00036913 rank 7
2023-02-25 01:42:51,543 DEBUG TRAIN Batch 22/0 loss 9.543788 loss_att 8.934016 loss_ctc 13.076429 loss_rnnt 9.060963 hw_loss 0.250802 lr 0.00036912 rank 2
2023-02-25 01:42:51,570 DEBUG TRAIN Batch 22/0 loss 6.839398 loss_att 7.273916 loss_ctc 9.419455 loss_rnnt 6.275364 hw_loss 0.249607 lr 0.00036911 rank 5
2023-02-25 01:44:04,044 DEBUG TRAIN Batch 22/100 loss 17.435389 loss_att 22.132631 loss_ctc 23.522079 loss_rnnt 15.608953 hw_loss 0.141424 lr 0.00036905 rank 0
2023-02-25 01:44:04,047 DEBUG TRAIN Batch 22/100 loss 1.723977 loss_att 3.577872 loss_ctc 1.739992 loss_rnnt 1.179223 hw_loss 0.322198 lr 0.00036903 rank 7
2023-02-25 01:44:04,047 DEBUG TRAIN Batch 22/100 loss 3.499190 loss_att 5.937304 loss_ctc 5.196457 loss_rnnt 2.707521 hw_loss 0.145770 lr 0.00036908 rank 3
2023-02-25 01:44:04,049 DEBUG TRAIN Batch 22/100 loss 8.524695 loss_att 10.282180 loss_ctc 10.854769 loss_rnnt 7.725094 hw_loss 0.257677 lr 0.00036903 rank 6
2023-02-25 01:44:04,051 DEBUG TRAIN Batch 22/100 loss 17.193884 loss_att 17.401192 loss_ctc 20.973202 loss_rnnt 16.530022 hw_loss 0.222171 lr 0.00036901 rank 5
2023-02-25 01:44:04,053 DEBUG TRAIN Batch 22/100 loss 8.992551 loss_att 11.286102 loss_ctc 10.056295 loss_rnnt 8.299673 hw_loss 0.173127 lr 0.00036906 rank 4
2023-02-25 01:44:04,081 DEBUG TRAIN Batch 22/100 loss 6.194510 loss_att 10.019786 loss_ctc 7.311247 loss_rnnt 5.173977 hw_loss 0.199835 lr 0.00036903 rank 1
2023-02-25 01:44:04,094 DEBUG TRAIN Batch 22/100 loss 4.574610 loss_att 8.759817 loss_ctc 5.834854 loss_rnnt 3.410648 hw_loss 0.297914 lr 0.00036902 rank 2
2023-02-25 01:45:16,128 DEBUG TRAIN Batch 22/200 loss 7.386223 loss_att 10.516373 loss_ctc 14.895857 loss_rnnt 5.684139 hw_loss 0.140193 lr 0.00036898 rank 3
2023-02-25 01:45:16,129 DEBUG TRAIN Batch 22/200 loss 2.625820 loss_att 5.355409 loss_ctc 3.848909 loss_rnnt 1.740237 hw_loss 0.331098 lr 0.00036895 rank 0
2023-02-25 01:45:16,133 DEBUG TRAIN Batch 22/200 loss 8.926118 loss_att 8.820134 loss_ctc 8.178950 loss_rnnt 8.929853 hw_loss 0.219532 lr 0.00036891 rank 5
2023-02-25 01:45:16,135 DEBUG TRAIN Batch 22/200 loss 17.755026 loss_att 21.498165 loss_ctc 20.630535 loss_rnnt 16.519545 hw_loss 0.193972 lr 0.00036893 rank 6
2023-02-25 01:45:16,137 DEBUG TRAIN Batch 22/200 loss 5.387617 loss_att 9.503688 loss_ctc 8.919518 loss_rnnt 3.966829 hw_loss 0.237475 lr 0.00036893 rank 1
2023-02-25 01:45:16,137 DEBUG TRAIN Batch 22/200 loss 11.690211 loss_att 11.934351 loss_ctc 12.187283 loss_rnnt 11.468561 hw_loss 0.199773 lr 0.00036892 rank 2
2023-02-25 01:45:16,137 DEBUG TRAIN Batch 22/200 loss 2.648711 loss_att 4.563272 loss_ctc 4.049471 loss_rnnt 1.997008 hw_loss 0.153794 lr 0.00036893 rank 7
2023-02-25 01:45:16,189 DEBUG TRAIN Batch 22/200 loss 14.402804 loss_att 16.621004 loss_ctc 19.136263 loss_rnnt 13.220351 hw_loss 0.201910 lr 0.00036896 rank 4
2023-02-25 01:46:29,830 DEBUG TRAIN Batch 22/300 loss 7.972158 loss_att 10.635465 loss_ctc 11.605802 loss_rnnt 6.846432 hw_loss 0.203584 lr 0.00036882 rank 2
2023-02-25 01:46:29,830 DEBUG TRAIN Batch 22/300 loss 7.444340 loss_att 9.357908 loss_ctc 9.847224 loss_rnnt 6.583916 hw_loss 0.294987 lr 0.00036881 rank 5
2023-02-25 01:46:29,837 DEBUG TRAIN Batch 22/300 loss 7.026227 loss_att 9.669482 loss_ctc 9.810476 loss_rnnt 6.008788 hw_loss 0.220417 lr 0.00036885 rank 0
2023-02-25 01:46:29,840 DEBUG TRAIN Batch 22/300 loss 7.151744 loss_att 10.594606 loss_ctc 8.170727 loss_rnnt 6.210417 hw_loss 0.219168 lr 0.00036883 rank 6
2023-02-25 01:46:29,843 DEBUG TRAIN Batch 22/300 loss 7.989436 loss_att 10.221081 loss_ctc 13.541069 loss_rnnt 6.683154 hw_loss 0.224503 lr 0.00036883 rank 7
2023-02-25 01:46:29,845 DEBUG TRAIN Batch 22/300 loss 9.627423 loss_att 14.133925 loss_ctc 11.427099 loss_rnnt 8.407832 hw_loss 0.146878 lr 0.00036886 rank 4
2023-02-25 01:46:29,878 DEBUG TRAIN Batch 22/300 loss 10.533740 loss_att 14.001299 loss_ctc 15.123783 loss_rnnt 9.090348 hw_loss 0.258511 lr 0.00036883 rank 1
2023-02-25 01:46:29,879 DEBUG TRAIN Batch 22/300 loss 5.101575 loss_att 6.881501 loss_ctc 6.267079 loss_rnnt 4.505377 hw_loss 0.159022 lr 0.00036888 rank 3
2023-02-25 01:47:43,767 DEBUG TRAIN Batch 22/400 loss 3.663892 loss_att 7.189217 loss_ctc 4.689873 loss_rnnt 2.708567 hw_loss 0.212742 lr 0.00036873 rank 7
2023-02-25 01:47:43,772 DEBUG TRAIN Batch 22/400 loss 3.379763 loss_att 5.163967 loss_ctc 3.189060 loss_rnnt 2.959219 hw_loss 0.167119 lr 0.00036873 rank 6
2023-02-25 01:47:43,773 DEBUG TRAIN Batch 22/400 loss 13.194501 loss_att 15.705456 loss_ctc 17.820906 loss_rnnt 11.935110 hw_loss 0.263149 lr 0.00036878 rank 3
2023-02-25 01:47:43,774 DEBUG TRAIN Batch 22/400 loss 7.896999 loss_att 12.439413 loss_ctc 12.827949 loss_rnnt 6.200761 hw_loss 0.244304 lr 0.00036875 rank 0
2023-02-25 01:47:43,774 DEBUG TRAIN Batch 22/400 loss 5.027117 loss_att 8.354784 loss_ctc 10.190868 loss_rnnt 3.478638 hw_loss 0.364585 lr 0.00036873 rank 1
2023-02-25 01:47:43,777 DEBUG TRAIN Batch 22/400 loss 8.893838 loss_att 10.167330 loss_ctc 12.051515 loss_rnnt 8.095507 hw_loss 0.229892 lr 0.00036876 rank 4
2023-02-25 01:47:43,798 DEBUG TRAIN Batch 22/400 loss 6.737509 loss_att 9.284388 loss_ctc 9.030868 loss_rnnt 5.782324 hw_loss 0.262551 lr 0.00036872 rank 2
2023-02-25 01:47:43,816 DEBUG TRAIN Batch 22/400 loss 5.714828 loss_att 7.854430 loss_ctc 7.214978 loss_rnnt 4.951906 hw_loss 0.253092 lr 0.00036871 rank 5
2023-02-25 01:48:56,311 DEBUG TRAIN Batch 22/500 loss 9.128491 loss_att 12.435379 loss_ctc 15.010071 loss_rnnt 7.601143 hw_loss 0.153301 lr 0.00036864 rank 0
2023-02-25 01:48:56,317 DEBUG TRAIN Batch 22/500 loss 16.463623 loss_att 18.037701 loss_ctc 22.572111 loss_rnnt 15.156104 hw_loss 0.334197 lr 0.00036863 rank 6
2023-02-25 01:48:56,318 DEBUG TRAIN Batch 22/500 loss 4.332852 loss_att 6.211687 loss_ctc 8.659027 loss_rnnt 3.218111 hw_loss 0.304035 lr 0.00036863 rank 7
2023-02-25 01:48:56,319 DEBUG TRAIN Batch 22/500 loss 9.085610 loss_att 9.608867 loss_ctc 9.180093 loss_rnnt 8.888389 hw_loss 0.149949 lr 0.00036862 rank 2
2023-02-25 01:48:56,319 DEBUG TRAIN Batch 22/500 loss 18.681616 loss_att 20.818901 loss_ctc 24.885096 loss_rnnt 17.320545 hw_loss 0.199657 lr 0.00036863 rank 1
2023-02-25 01:48:56,322 DEBUG TRAIN Batch 22/500 loss 5.660326 loss_att 8.122378 loss_ctc 11.597815 loss_rnnt 4.221760 hw_loss 0.289668 lr 0.00036861 rank 5
2023-02-25 01:48:56,326 DEBUG TRAIN Batch 22/500 loss 9.314494 loss_att 10.093325 loss_ctc 9.915735 loss_rnnt 8.941907 hw_loss 0.256229 lr 0.00036868 rank 3
2023-02-25 01:48:56,328 DEBUG TRAIN Batch 22/500 loss 6.529668 loss_att 6.989927 loss_ctc 9.677092 loss_rnnt 5.900119 hw_loss 0.220953 lr 0.00036866 rank 4
2023-02-25 01:50:09,473 DEBUG TRAIN Batch 22/600 loss 12.656642 loss_att 12.355036 loss_ctc 16.240112 loss_rnnt 12.126825 hw_loss 0.210641 lr 0.00036853 rank 1
2023-02-25 01:50:09,478 DEBUG TRAIN Batch 22/600 loss 8.652738 loss_att 10.255583 loss_ctc 8.856661 loss_rnnt 8.187338 hw_loss 0.220578 lr 0.00036852 rank 2
2023-02-25 01:50:09,479 DEBUG TRAIN Batch 22/600 loss 4.374999 loss_att 5.324695 loss_ctc 5.532717 loss_rnnt 3.883385 hw_loss 0.276211 lr 0.00036854 rank 0
2023-02-25 01:50:09,479 DEBUG TRAIN Batch 22/600 loss 5.973752 loss_att 7.602167 loss_ctc 7.170650 loss_rnnt 5.386281 hw_loss 0.191627 lr 0.00036858 rank 3
2023-02-25 01:50:09,483 DEBUG TRAIN Batch 22/600 loss 6.720752 loss_att 7.621247 loss_ctc 7.725669 loss_rnnt 6.300545 hw_loss 0.198974 lr 0.00036856 rank 4
2023-02-25 01:50:09,483 DEBUG TRAIN Batch 22/600 loss 9.001369 loss_att 10.467708 loss_ctc 13.042557 loss_rnnt 8.047788 hw_loss 0.227793 lr 0.00036853 rank 7
2023-02-25 01:50:09,489 DEBUG TRAIN Batch 22/600 loss 11.684901 loss_att 12.164492 loss_ctc 12.828453 loss_rnnt 11.282482 hw_loss 0.288802 lr 0.00036851 rank 5
2023-02-25 01:50:09,492 DEBUG TRAIN Batch 22/600 loss 5.855221 loss_att 6.575614 loss_ctc 8.538343 loss_rnnt 5.193010 hw_loss 0.300719 lr 0.00036853 rank 6
2023-02-25 01:51:23,853 DEBUG TRAIN Batch 22/700 loss 5.655736 loss_att 9.603146 loss_ctc 12.678986 loss_rnnt 3.810366 hw_loss 0.223977 lr 0.00036844 rank 0
2023-02-25 01:51:23,854 DEBUG TRAIN Batch 22/700 loss 7.263546 loss_att 10.345408 loss_ctc 11.300659 loss_rnnt 6.015725 hw_loss 0.174687 lr 0.00036843 rank 6
2023-02-25 01:51:23,855 DEBUG TRAIN Batch 22/700 loss 7.424050 loss_att 9.560799 loss_ctc 10.158840 loss_rnnt 6.532598 hw_loss 0.186495 lr 0.00036843 rank 7
2023-02-25 01:51:23,857 DEBUG TRAIN Batch 22/700 loss 3.314059 loss_att 6.355428 loss_ctc 6.945133 loss_rnnt 2.122832 hw_loss 0.185269 lr 0.00036843 rank 1
2023-02-25 01:51:23,858 DEBUG TRAIN Batch 22/700 loss 9.689425 loss_att 14.124324 loss_ctc 18.780167 loss_rnnt 7.522111 hw_loss 0.127940 lr 0.00036846 rank 4
2023-02-25 01:51:23,863 DEBUG TRAIN Batch 22/700 loss 8.346563 loss_att 12.202662 loss_ctc 15.836676 loss_rnnt 6.468421 hw_loss 0.202951 lr 0.00036842 rank 2
2023-02-25 01:51:23,869 DEBUG TRAIN Batch 22/700 loss 5.217221 loss_att 7.785804 loss_ctc 7.816568 loss_rnnt 4.273450 hw_loss 0.156515 lr 0.00036848 rank 3
2023-02-25 01:51:23,892 DEBUG TRAIN Batch 22/700 loss 1.497179 loss_att 3.561291 loss_ctc 1.784280 loss_rnnt 0.983554 hw_loss 0.117228 lr 0.00036841 rank 5
2023-02-25 01:52:36,209 DEBUG TRAIN Batch 22/800 loss 4.181222 loss_att 6.781663 loss_ctc 5.361133 loss_rnnt 3.404526 hw_loss 0.186161 lr 0.00036834 rank 0
2023-02-25 01:52:36,212 DEBUG TRAIN Batch 22/800 loss 5.895852 loss_att 7.723157 loss_ctc 10.904688 loss_rnnt 4.795918 hw_loss 0.124927 lr 0.00036832 rank 2
2023-02-25 01:52:36,215 DEBUG TRAIN Batch 22/800 loss 4.112573 loss_att 6.271042 loss_ctc 5.258307 loss_rnnt 3.410100 hw_loss 0.221279 lr 0.00036838 rank 3
2023-02-25 01:52:36,216 DEBUG TRAIN Batch 22/800 loss 4.360194 loss_att 7.326706 loss_ctc 8.536594 loss_rnnt 3.070243 hw_loss 0.262115 lr 0.00036833 rank 6
2023-02-25 01:52:36,216 DEBUG TRAIN Batch 22/800 loss 1.223489 loss_att 3.345825 loss_ctc 1.034217 loss_rnnt 0.743666 hw_loss 0.151111 lr 0.00036836 rank 4
2023-02-25 01:52:36,219 DEBUG TRAIN Batch 22/800 loss 13.425200 loss_att 15.925165 loss_ctc 14.629070 loss_rnnt 12.701984 hw_loss 0.117576 lr 0.00036833 rank 1
2023-02-25 01:52:36,221 DEBUG TRAIN Batch 22/800 loss 5.592505 loss_att 6.884456 loss_ctc 6.505439 loss_rnnt 5.143317 hw_loss 0.129510 lr 0.00036833 rank 7
2023-02-25 01:52:36,262 DEBUG TRAIN Batch 22/800 loss 3.957115 loss_att 6.346056 loss_ctc 5.038730 loss_rnnt 3.231055 hw_loss 0.195106 lr 0.00036831 rank 5
2023-02-25 01:53:47,965 DEBUG TRAIN Batch 22/900 loss 9.808593 loss_att 14.078133 loss_ctc 14.199689 loss_rnnt 8.304134 hw_loss 0.122006 lr 0.00036821 rank 5
2023-02-25 01:53:47,974 DEBUG TRAIN Batch 22/900 loss 10.633181 loss_att 12.520710 loss_ctc 17.289364 loss_rnnt 9.268019 hw_loss 0.187806 lr 0.00036823 rank 6
2023-02-25 01:53:47,976 DEBUG TRAIN Batch 22/900 loss 10.340644 loss_att 13.206580 loss_ctc 12.888823 loss_rnnt 9.355406 hw_loss 0.135549 lr 0.00036824 rank 0
2023-02-25 01:53:47,977 DEBUG TRAIN Batch 22/900 loss 11.924246 loss_att 14.673021 loss_ctc 14.700892 loss_rnnt 10.908031 hw_loss 0.180452 lr 0.00036822 rank 2
2023-02-25 01:53:47,981 DEBUG TRAIN Batch 22/900 loss 10.777530 loss_att 12.820251 loss_ctc 16.785843 loss_rnnt 9.483280 hw_loss 0.158619 lr 0.00036826 rank 4
2023-02-25 01:53:47,981 DEBUG TRAIN Batch 22/900 loss 4.176242 loss_att 7.255894 loss_ctc 5.477584 loss_rnnt 3.281563 hw_loss 0.197318 lr 0.00036828 rank 3
2023-02-25 01:53:47,984 DEBUG TRAIN Batch 22/900 loss 7.626063 loss_att 14.303006 loss_ctc 11.835619 loss_rnnt 5.636523 hw_loss 0.174145 lr 0.00036823 rank 7
2023-02-25 01:53:48,033 DEBUG TRAIN Batch 22/900 loss 3.721709 loss_att 5.120983 loss_ctc 4.400958 loss_rnnt 3.262833 hw_loss 0.165852 lr 0.00036823 rank 1
2023-02-25 01:55:00,730 DEBUG TRAIN Batch 22/1000 loss 6.303195 loss_att 6.842921 loss_ctc 8.265197 loss_rnnt 5.888011 hw_loss 0.085572 lr 0.00036818 rank 3
2023-02-25 01:55:00,741 DEBUG TRAIN Batch 22/1000 loss 6.795463 loss_att 8.943008 loss_ctc 9.292542 loss_rnnt 5.941339 hw_loss 0.171883 lr 0.00036814 rank 0
2023-02-25 01:55:00,746 DEBUG TRAIN Batch 22/1000 loss 8.194674 loss_att 11.779031 loss_ctc 13.053153 loss_rnnt 6.710960 hw_loss 0.223213 lr 0.00036813 rank 6
2023-02-25 01:55:00,750 DEBUG TRAIN Batch 22/1000 loss 8.415813 loss_att 12.590252 loss_ctc 13.304300 loss_rnnt 6.794689 hw_loss 0.252070 lr 0.00036813 rank 7
2023-02-25 01:55:00,751 DEBUG TRAIN Batch 22/1000 loss 4.028345 loss_att 6.519346 loss_ctc 11.060387 loss_rnnt 2.515535 hw_loss 0.144381 lr 0.00036813 rank 1
2023-02-25 01:55:00,777 DEBUG TRAIN Batch 22/1000 loss 15.724229 loss_att 28.029449 loss_ctc 20.866323 loss_rnnt 12.480285 hw_loss 0.182413 lr 0.00036816 rank 4
2023-02-25 01:55:00,778 DEBUG TRAIN Batch 22/1000 loss 5.734077 loss_att 8.446186 loss_ctc 9.443128 loss_rnnt 4.616360 hw_loss 0.151416 lr 0.00036812 rank 2
2023-02-25 01:55:00,781 DEBUG TRAIN Batch 22/1000 loss 9.485490 loss_att 13.119192 loss_ctc 14.840591 loss_rnnt 7.976303 hw_loss 0.128312 lr 0.00036811 rank 5
2023-02-25 01:56:14,763 DEBUG TRAIN Batch 22/1100 loss 7.366145 loss_att 10.125817 loss_ctc 11.202902 loss_rnnt 6.233688 hw_loss 0.129291 lr 0.00036802 rank 2
2023-02-25 01:56:14,777 DEBUG TRAIN Batch 22/1100 loss 9.315704 loss_att 11.308092 loss_ctc 11.542166 loss_rnnt 8.507483 hw_loss 0.211654 lr 0.00036808 rank 3
2023-02-25 01:56:14,777 DEBUG TRAIN Batch 22/1100 loss 14.794006 loss_att 15.809372 loss_ctc 20.896290 loss_rnnt 13.670842 hw_loss 0.199599 lr 0.00036805 rank 0
2023-02-25 01:56:14,780 DEBUG TRAIN Batch 22/1100 loss 10.870256 loss_att 11.850858 loss_ctc 15.344774 loss_rnnt 9.972271 hw_loss 0.197367 lr 0.00036803 rank 6
2023-02-25 01:56:14,785 DEBUG TRAIN Batch 22/1100 loss 11.457716 loss_att 13.904018 loss_ctc 17.035177 loss_rnnt 10.044453 hw_loss 0.338140 lr 0.00036803 rank 7
2023-02-25 01:56:14,785 DEBUG TRAIN Batch 22/1100 loss 8.935501 loss_att 11.131962 loss_ctc 12.201094 loss_rnnt 7.968503 hw_loss 0.173049 lr 0.00036801 rank 5
2023-02-25 01:56:14,786 DEBUG TRAIN Batch 22/1100 loss 8.310906 loss_att 9.982023 loss_ctc 10.484276 loss_rnnt 7.608576 hw_loss 0.146858 lr 0.00036803 rank 1
2023-02-25 01:56:14,790 DEBUG TRAIN Batch 22/1100 loss 5.043796 loss_att 6.486021 loss_ctc 5.103634 loss_rnnt 4.673723 hw_loss 0.138093 lr 0.00036806 rank 4
2023-02-25 01:57:26,760 DEBUG TRAIN Batch 22/1200 loss 2.725773 loss_att 4.313165 loss_ctc 3.811549 loss_rnnt 2.141143 hw_loss 0.229464 lr 0.00036795 rank 0
2023-02-25 01:57:26,766 DEBUG TRAIN Batch 22/1200 loss 5.613751 loss_att 7.267500 loss_ctc 7.300756 loss_rnnt 4.928813 hw_loss 0.242351 lr 0.00036793 rank 7
2023-02-25 01:57:26,769 DEBUG TRAIN Batch 22/1200 loss 4.926083 loss_att 7.471242 loss_ctc 6.304385 loss_rnnt 4.108150 hw_loss 0.234613 lr 0.00036793 rank 1
2023-02-25 01:57:26,770 DEBUG TRAIN Batch 22/1200 loss 9.176093 loss_att 11.048248 loss_ctc 12.499632 loss_rnnt 8.258503 hw_loss 0.187539 lr 0.00036793 rank 6
2023-02-25 01:57:26,771 DEBUG TRAIN Batch 22/1200 loss 6.023662 loss_att 8.403670 loss_ctc 8.243150 loss_rnnt 5.103201 hw_loss 0.278488 lr 0.00036792 rank 2
2023-02-25 01:57:26,770 DEBUG TRAIN Batch 22/1200 loss 15.111468 loss_att 16.655979 loss_ctc 19.929132 loss_rnnt 14.036674 hw_loss 0.231630 lr 0.00036798 rank 3
2023-02-25 01:57:26,772 DEBUG TRAIN Batch 22/1200 loss 13.465926 loss_att 12.898262 loss_ctc 20.403614 loss_rnnt 12.499712 hw_loss 0.290102 lr 0.00036796 rank 4
2023-02-25 01:57:26,772 DEBUG TRAIN Batch 22/1200 loss 12.839137 loss_att 13.625186 loss_ctc 17.625280 loss_rnnt 11.913539 hw_loss 0.244190 lr 0.00036791 rank 5
2023-02-25 01:58:38,554 DEBUG TRAIN Batch 22/1300 loss 2.598197 loss_att 6.129622 loss_ctc 7.489234 loss_rnnt 1.160269 hw_loss 0.149074 lr 0.00036783 rank 6
2023-02-25 01:58:38,556 DEBUG TRAIN Batch 22/1300 loss 5.192063 loss_att 7.737450 loss_ctc 7.408954 loss_rnnt 4.325091 hw_loss 0.116829 lr 0.00036783 rank 7
2023-02-25 01:58:38,557 DEBUG TRAIN Batch 22/1300 loss 10.325309 loss_att 15.312208 loss_ctc 13.911460 loss_rnnt 8.753231 hw_loss 0.181020 lr 0.00036782 rank 2
2023-02-25 01:58:38,558 DEBUG TRAIN Batch 22/1300 loss 4.360654 loss_att 9.299475 loss_ctc 6.294097 loss_rnnt 2.962385 hw_loss 0.286336 lr 0.00036781 rank 5
2023-02-25 01:58:38,558 DEBUG TRAIN Batch 22/1300 loss 7.308039 loss_att 9.261248 loss_ctc 12.414238 loss_rnnt 6.187063 hw_loss 0.092824 lr 0.00036785 rank 0
2023-02-25 01:58:38,560 DEBUG TRAIN Batch 22/1300 loss 4.362445 loss_att 6.596115 loss_ctc 6.357292 loss_rnnt 3.533821 hw_loss 0.217331 lr 0.00036786 rank 4
2023-02-25 01:58:38,566 DEBUG TRAIN Batch 22/1300 loss 7.323163 loss_att 6.462733 loss_ctc 10.779169 loss_rnnt 6.864851 hw_loss 0.317993 lr 0.00036783 rank 1
2023-02-25 01:58:38,606 DEBUG TRAIN Batch 22/1300 loss 9.016448 loss_att 8.688635 loss_ctc 11.439059 loss_rnnt 8.606811 hw_loss 0.285346 lr 0.00036788 rank 3
2023-02-25 01:59:52,331 DEBUG TRAIN Batch 22/1400 loss 8.070728 loss_att 10.217663 loss_ctc 11.212461 loss_rnnt 7.093320 hw_loss 0.242105 lr 0.00036773 rank 6
2023-02-25 01:59:52,332 DEBUG TRAIN Batch 22/1400 loss 6.612106 loss_att 8.737538 loss_ctc 6.638139 loss_rnnt 6.067785 hw_loss 0.217057 lr 0.00036773 rank 7
2023-02-25 01:59:52,333 DEBUG TRAIN Batch 22/1400 loss 4.644137 loss_att 7.378467 loss_ctc 6.573822 loss_rnnt 3.751232 hw_loss 0.166401 lr 0.00036772 rank 2
2023-02-25 01:59:52,333 DEBUG TRAIN Batch 22/1400 loss 9.329123 loss_att 12.202131 loss_ctc 11.295261 loss_rnnt 8.404002 hw_loss 0.165690 lr 0.00036773 rank 1
2023-02-25 01:59:52,333 DEBUG TRAIN Batch 22/1400 loss 5.458200 loss_att 8.119546 loss_ctc 8.516535 loss_rnnt 4.417859 hw_loss 0.188052 lr 0.00036775 rank 0
2023-02-25 01:59:52,334 DEBUG TRAIN Batch 22/1400 loss 6.258035 loss_att 10.374289 loss_ctc 10.237710 loss_rnnt 4.792032 hw_loss 0.210241 lr 0.00036776 rank 4
2023-02-25 01:59:52,335 DEBUG TRAIN Batch 22/1400 loss 12.230452 loss_att 16.809744 loss_ctc 17.975876 loss_rnnt 10.453109 hw_loss 0.178929 lr 0.00036771 rank 5
2023-02-25 01:59:52,357 DEBUG TRAIN Batch 22/1400 loss 7.110621 loss_att 11.159637 loss_ctc 14.593975 loss_rnnt 5.184423 hw_loss 0.222400 lr 0.00036778 rank 3
2023-02-25 02:01:05,450 DEBUG TRAIN Batch 22/1500 loss 8.109541 loss_att 12.539711 loss_ctc 11.729015 loss_rnnt 6.611387 hw_loss 0.242856 lr 0.00036768 rank 3
2023-02-25 02:01:05,454 DEBUG TRAIN Batch 22/1500 loss 4.392453 loss_att 7.752306 loss_ctc 8.488838 loss_rnnt 3.047462 hw_loss 0.237816 lr 0.00036762 rank 2
2023-02-25 02:01:05,454 DEBUG TRAIN Batch 22/1500 loss 9.865517 loss_att 11.392957 loss_ctc 14.119251 loss_rnnt 8.898600 hw_loss 0.176746 lr 0.00036763 rank 6
2023-02-25 02:01:05,457 DEBUG TRAIN Batch 22/1500 loss 9.654469 loss_att 12.248365 loss_ctc 16.749931 loss_rnnt 8.117411 hw_loss 0.135407 lr 0.00036765 rank 0
2023-02-25 02:01:05,459 DEBUG TRAIN Batch 22/1500 loss 5.810270 loss_att 10.146781 loss_ctc 10.062490 loss_rnnt 4.268968 hw_loss 0.200696 lr 0.00036766 rank 4
2023-02-25 02:01:05,458 DEBUG TRAIN Batch 22/1500 loss 9.197335 loss_att 10.116488 loss_ctc 13.578291 loss_rnnt 8.295275 hw_loss 0.251440 lr 0.00036763 rank 1
2023-02-25 02:01:05,462 DEBUG TRAIN Batch 22/1500 loss 23.709663 loss_att 22.457117 loss_ctc 36.097649 loss_rnnt 22.235394 hw_loss 0.136966 lr 0.00036762 rank 5
2023-02-25 02:01:05,463 DEBUG TRAIN Batch 22/1500 loss 10.540443 loss_att 13.041832 loss_ctc 15.410694 loss_rnnt 9.317673 hw_loss 0.137113 lr 0.00036764 rank 7
2023-02-25 02:02:17,129 DEBUG TRAIN Batch 22/1600 loss 15.562486 loss_att 17.674818 loss_ctc 19.522709 loss_rnnt 14.518085 hw_loss 0.176076 lr 0.00036755 rank 0
2023-02-25 02:02:17,134 DEBUG TRAIN Batch 22/1600 loss 6.091197 loss_att 6.715138 loss_ctc 7.570486 loss_rnnt 5.663392 hw_loss 0.198333 lr 0.00036756 rank 4
2023-02-25 02:02:17,135 DEBUG TRAIN Batch 22/1600 loss 7.765029 loss_att 10.213112 loss_ctc 15.577202 loss_rnnt 6.146559 hw_loss 0.163556 lr 0.00036753 rank 1
2023-02-25 02:02:17,135 DEBUG TRAIN Batch 22/1600 loss 7.465272 loss_att 10.180725 loss_ctc 9.354084 loss_rnnt 6.525156 hw_loss 0.272218 lr 0.00036759 rank 3
2023-02-25 02:02:17,140 DEBUG TRAIN Batch 22/1600 loss 3.627391 loss_att 6.136006 loss_ctc 5.031589 loss_rnnt 2.844378 hw_loss 0.176369 lr 0.00036753 rank 6
2023-02-25 02:02:17,141 DEBUG TRAIN Batch 22/1600 loss 5.973616 loss_att 9.056461 loss_ctc 8.114079 loss_rnnt 4.987944 hw_loss 0.156951 lr 0.00036754 rank 7
2023-02-25 02:02:17,142 DEBUG TRAIN Batch 22/1600 loss 11.058454 loss_att 14.807355 loss_ctc 16.817699 loss_rnnt 9.456789 hw_loss 0.157470 lr 0.00036752 rank 2
2023-02-25 02:02:17,142 DEBUG TRAIN Batch 22/1600 loss 12.480928 loss_att 16.420647 loss_ctc 20.084606 loss_rnnt 10.571710 hw_loss 0.201468 lr 0.00036752 rank 5
2023-02-25 02:03:29,264 DEBUG TRAIN Batch 22/1700 loss 4.461945 loss_att 6.612032 loss_ctc 6.609039 loss_rnnt 3.641690 hw_loss 0.194921 lr 0.00036742 rank 2
2023-02-25 02:03:29,273 DEBUG TRAIN Batch 22/1700 loss 3.498114 loss_att 6.005264 loss_ctc 6.276927 loss_rnnt 2.514415 hw_loss 0.209550 lr 0.00036742 rank 5
2023-02-25 02:03:29,280 DEBUG TRAIN Batch 22/1700 loss 6.421513 loss_att 10.550113 loss_ctc 9.016333 loss_rnnt 5.150116 hw_loss 0.186938 lr 0.00036745 rank 0
2023-02-25 02:03:29,280 DEBUG TRAIN Batch 22/1700 loss 5.087871 loss_att 7.898207 loss_ctc 8.436059 loss_rnnt 4.012115 hw_loss 0.126118 lr 0.00036743 rank 1
2023-02-25 02:03:29,281 DEBUG TRAIN Batch 22/1700 loss 14.571607 loss_att 17.059025 loss_ctc 20.087811 loss_rnnt 13.226944 hw_loss 0.209407 lr 0.00036743 rank 6
2023-02-25 02:03:29,283 DEBUG TRAIN Batch 22/1700 loss 22.691710 loss_att 23.905050 loss_ctc 31.513697 loss_rnnt 21.159086 hw_loss 0.213169 lr 0.00036746 rank 4
2023-02-25 02:03:29,284 DEBUG TRAIN Batch 22/1700 loss 6.160029 loss_att 8.785298 loss_ctc 7.153589 loss_rnnt 5.410559 hw_loss 0.172391 lr 0.00036744 rank 7
2023-02-25 02:03:29,291 DEBUG TRAIN Batch 22/1700 loss 4.792363 loss_att 7.947968 loss_ctc 8.141131 loss_rnnt 3.581524 hw_loss 0.249779 lr 0.00036749 rank 3
2023-02-25 02:04:44,016 DEBUG TRAIN Batch 22/1800 loss 2.707331 loss_att 5.186756 loss_ctc 4.032261 loss_rnnt 1.950042 hw_loss 0.158899 lr 0.00036735 rank 0
2023-02-25 02:04:44,023 DEBUG TRAIN Batch 22/1800 loss 4.681762 loss_att 7.014367 loss_ctc 6.871539 loss_rnnt 3.770217 hw_loss 0.286976 lr 0.00036734 rank 1
2023-02-25 02:04:44,023 DEBUG TRAIN Batch 22/1800 loss 9.925276 loss_att 10.298564 loss_ctc 11.027807 loss_rnnt 9.573086 hw_loss 0.244742 lr 0.00036739 rank 3
2023-02-25 02:04:44,027 DEBUG TRAIN Batch 22/1800 loss 8.994331 loss_att 11.356793 loss_ctc 9.895891 loss_rnnt 8.251492 hw_loss 0.281511 lr 0.00036734 rank 7
2023-02-25 02:04:44,028 DEBUG TRAIN Batch 22/1800 loss 3.592688 loss_att 5.917522 loss_ctc 6.568192 loss_rnnt 2.618621 hw_loss 0.210687 lr 0.00036733 rank 6
2023-02-25 02:04:44,029 DEBUG TRAIN Batch 22/1800 loss 6.737752 loss_att 7.952896 loss_ctc 9.475108 loss_rnnt 6.004863 hw_loss 0.234150 lr 0.00036732 rank 2
2023-02-25 02:04:44,034 DEBUG TRAIN Batch 22/1800 loss 6.314864 loss_att 8.366576 loss_ctc 9.092941 loss_rnnt 5.390755 hw_loss 0.268792 lr 0.00036732 rank 5
2023-02-25 02:04:44,084 DEBUG TRAIN Batch 22/1800 loss 6.043924 loss_att 6.186221 loss_ctc 6.953195 loss_rnnt 5.745483 hw_loss 0.278897 lr 0.00036736 rank 4
2023-02-25 02:05:57,193 DEBUG TRAIN Batch 22/1900 loss 7.991712 loss_att 10.368575 loss_ctc 12.047461 loss_rnnt 6.814118 hw_loss 0.302724 lr 0.00036725 rank 0
2023-02-25 02:05:57,194 DEBUG TRAIN Batch 22/1900 loss 10.546155 loss_att 13.262653 loss_ctc 13.656492 loss_rnnt 9.419421 hw_loss 0.316355 lr 0.00036729 rank 3
2023-02-25 02:05:57,197 DEBUG TRAIN Batch 22/1900 loss 8.503052 loss_att 8.621874 loss_ctc 10.654302 loss_rnnt 8.011424 hw_loss 0.339431 lr 0.00036724 rank 7
2023-02-25 02:05:57,196 DEBUG TRAIN Batch 22/1900 loss 2.815853 loss_att 5.323175 loss_ctc 6.236301 loss_rnnt 1.748028 hw_loss 0.206814 lr 0.00036727 rank 4
2023-02-25 02:05:57,197 DEBUG TRAIN Batch 22/1900 loss 19.503838 loss_att 28.904182 loss_ctc 36.744884 loss_rnnt 15.282008 hw_loss 0.080536 lr 0.00036722 rank 2
2023-02-25 02:05:57,199 DEBUG TRAIN Batch 22/1900 loss 12.197403 loss_att 18.626143 loss_ctc 23.801565 loss_rnnt 9.260861 hw_loss 0.194196 lr 0.00036723 rank 6
2023-02-25 02:05:57,200 DEBUG TRAIN Batch 22/1900 loss 5.976912 loss_att 9.649628 loss_ctc 9.113497 loss_rnnt 4.727502 hw_loss 0.181227 lr 0.00036722 rank 5
2023-02-25 02:05:57,248 DEBUG TRAIN Batch 22/1900 loss 8.742811 loss_att 9.356956 loss_ctc 11.302218 loss_rnnt 8.135698 hw_loss 0.268181 lr 0.00036724 rank 1
2023-02-25 02:07:11,133 DEBUG TRAIN Batch 22/2000 loss 14.617286 loss_att 18.021608 loss_ctc 23.954176 loss_rnnt 12.616371 hw_loss 0.140870 lr 0.00036715 rank 0
2023-02-25 02:07:11,136 DEBUG TRAIN Batch 22/2000 loss 22.297445 loss_att 25.246220 loss_ctc 36.669739 loss_rnnt 19.655807 hw_loss 0.254206 lr 0.00036719 rank 3
2023-02-25 02:07:11,137 DEBUG TRAIN Batch 22/2000 loss 8.273273 loss_att 10.884164 loss_ctc 11.195292 loss_rnnt 7.301828 hw_loss 0.111867 lr 0.00036714 rank 7
2023-02-25 02:07:11,142 DEBUG TRAIN Batch 22/2000 loss 7.363410 loss_att 11.926246 loss_ctc 8.596344 loss_rnnt 6.209517 hw_loss 0.144250 lr 0.00036714 rank 1
2023-02-25 02:07:11,144 DEBUG TRAIN Batch 22/2000 loss 5.619597 loss_att 8.084202 loss_ctc 7.771214 loss_rnnt 4.736036 hw_loss 0.194545 lr 0.00036717 rank 4
2023-02-25 02:07:11,144 DEBUG TRAIN Batch 22/2000 loss 3.663840 loss_att 6.351577 loss_ctc 6.053275 loss_rnnt 2.693700 hw_loss 0.213753 lr 0.00036713 rank 2
2023-02-25 02:07:11,148 DEBUG TRAIN Batch 22/2000 loss 4.419018 loss_att 6.866266 loss_ctc 6.032313 loss_rnnt 3.597355 hw_loss 0.219574 lr 0.00036712 rank 5
2023-02-25 02:07:11,193 DEBUG TRAIN Batch 22/2000 loss 14.131043 loss_att 16.546640 loss_ctc 18.718712 loss_rnnt 12.975241 hw_loss 0.114367 lr 0.00036713 rank 6
2023-02-25 02:08:25,986 DEBUG TRAIN Batch 22/2100 loss 5.465039 loss_att 6.347675 loss_ctc 6.686947 loss_rnnt 5.011958 hw_loss 0.213062 lr 0.00036704 rank 1
2023-02-25 02:08:25,987 DEBUG TRAIN Batch 22/2100 loss 5.489252 loss_att 8.092382 loss_ctc 6.737371 loss_rnnt 4.695298 hw_loss 0.200459 lr 0.00036709 rank 3
2023-02-25 02:08:25,997 DEBUG TRAIN Batch 22/2100 loss 10.329943 loss_att 12.559673 loss_ctc 13.784744 loss_rnnt 9.340468 hw_loss 0.155414 lr 0.00036705 rank 0
2023-02-25 02:08:26,002 DEBUG TRAIN Batch 22/2100 loss 6.440739 loss_att 8.980936 loss_ctc 9.490477 loss_rnnt 5.413570 hw_loss 0.210933 lr 0.00036704 rank 7
2023-02-25 02:08:26,002 DEBUG TRAIN Batch 22/2100 loss 8.331807 loss_att 11.091595 loss_ctc 12.356755 loss_rnnt 7.136416 hw_loss 0.200201 lr 0.00036707 rank 4
2023-02-25 02:08:26,007 DEBUG TRAIN Batch 22/2100 loss 7.079529 loss_att 11.627198 loss_ctc 11.498516 loss_rnnt 5.454450 hw_loss 0.236901 lr 0.00036703 rank 2
2023-02-25 02:08:26,008 DEBUG TRAIN Batch 22/2100 loss 12.055639 loss_att 13.606926 loss_ctc 13.059708 loss_rnnt 11.503597 hw_loss 0.202326 lr 0.00036702 rank 5
2023-02-25 02:08:26,048 DEBUG TRAIN Batch 22/2100 loss 4.639791 loss_att 8.212206 loss_ctc 7.530535 loss_rnnt 3.403530 hw_loss 0.255648 lr 0.00036703 rank 6
2023-02-25 02:09:40,043 DEBUG TRAIN Batch 22/2200 loss 11.356956 loss_att 16.583391 loss_ctc 15.057828 loss_rnnt 9.691957 hw_loss 0.236741 lr 0.00036694 rank 1
2023-02-25 02:09:40,045 DEBUG TRAIN Batch 22/2200 loss 7.339803 loss_att 11.696929 loss_ctc 12.227607 loss_rnnt 5.663629 hw_loss 0.286953 lr 0.00036695 rank 0
2023-02-25 02:09:40,045 DEBUG TRAIN Batch 22/2200 loss 8.990419 loss_att 11.973309 loss_ctc 11.070913 loss_rnnt 7.988672 hw_loss 0.239571 lr 0.00036697 rank 4
2023-02-25 02:09:40,051 DEBUG TRAIN Batch 22/2200 loss 6.228287 loss_att 9.311335 loss_ctc 8.188832 loss_rnnt 5.263156 hw_loss 0.163342 lr 0.00036694 rank 6
2023-02-25 02:09:40,052 DEBUG TRAIN Batch 22/2200 loss 10.853200 loss_att 12.357219 loss_ctc 13.333769 loss_rnnt 10.065933 hw_loss 0.291974 lr 0.00036694 rank 7
2023-02-25 02:09:40,054 DEBUG TRAIN Batch 22/2200 loss 13.411044 loss_att 16.281097 loss_ctc 19.213348 loss_rnnt 11.978821 hw_loss 0.158572 lr 0.00036693 rank 2
2023-02-25 02:09:40,055 DEBUG TRAIN Batch 22/2200 loss 7.709954 loss_att 10.290903 loss_ctc 11.347160 loss_rnnt 6.578405 hw_loss 0.244499 lr 0.00036699 rank 3
2023-02-25 02:09:40,095 DEBUG TRAIN Batch 22/2200 loss 12.707701 loss_att 14.330613 loss_ctc 15.220699 loss_rnnt 11.912994 hw_loss 0.253231 lr 0.00036692 rank 5
2023-02-25 02:10:52,714 DEBUG TRAIN Batch 22/2300 loss 9.758558 loss_att 13.615405 loss_ctc 9.456194 loss_rnnt 8.904951 hw_loss 0.229786 lr 0.00036684 rank 7
2023-02-25 02:10:52,727 DEBUG TRAIN Batch 22/2300 loss 13.463444 loss_att 16.736937 loss_ctc 18.885054 loss_rnnt 11.949130 hw_loss 0.256376 lr 0.00036685 rank 0
2023-02-25 02:10:52,731 DEBUG TRAIN Batch 22/2300 loss 9.599418 loss_att 12.425673 loss_ctc 13.331690 loss_rnnt 8.434653 hw_loss 0.191020 lr 0.00036689 rank 3
2023-02-25 02:10:52,732 DEBUG TRAIN Batch 22/2300 loss 11.337088 loss_att 16.225639 loss_ctc 17.178936 loss_rnnt 9.467077 hw_loss 0.212601 lr 0.00036682 rank 5
2023-02-25 02:10:52,733 DEBUG TRAIN Batch 22/2300 loss 8.559570 loss_att 11.531053 loss_ctc 14.461460 loss_rnnt 7.079537 hw_loss 0.185284 lr 0.00036683 rank 2
2023-02-25 02:10:52,734 DEBUG TRAIN Batch 22/2300 loss 8.293415 loss_att 11.420343 loss_ctc 13.266708 loss_rnnt 6.870444 hw_loss 0.252149 lr 0.00036684 rank 1
2023-02-25 02:10:52,735 DEBUG TRAIN Batch 22/2300 loss 7.059179 loss_att 9.672803 loss_ctc 11.204281 loss_rnnt 5.878180 hw_loss 0.197989 lr 0.00036687 rank 4
2023-02-25 02:10:52,735 DEBUG TRAIN Batch 22/2300 loss 3.669096 loss_att 7.805519 loss_ctc 4.729652 loss_rnnt 2.529522 hw_loss 0.320403 lr 0.00036684 rank 6
2023-02-25 02:12:05,628 DEBUG TRAIN Batch 22/2400 loss 6.229363 loss_att 8.886713 loss_ctc 8.295319 loss_rnnt 5.286894 hw_loss 0.254135 lr 0.00036674 rank 1
2023-02-25 02:12:05,632 DEBUG TRAIN Batch 22/2400 loss 5.333283 loss_att 7.810958 loss_ctc 11.595313 loss_rnnt 3.890083 hw_loss 0.211365 lr 0.00036676 rank 0
2023-02-25 02:12:05,637 DEBUG TRAIN Batch 22/2400 loss 9.758220 loss_att 13.266121 loss_ctc 12.448681 loss_rnnt 8.582472 hw_loss 0.216448 lr 0.00036679 rank 3
2023-02-25 02:12:05,638 DEBUG TRAIN Batch 22/2400 loss 10.820155 loss_att 12.618305 loss_ctc 13.061471 loss_rnnt 10.023112 hw_loss 0.259822 lr 0.00036674 rank 7
2023-02-25 02:12:05,641 DEBUG TRAIN Batch 22/2400 loss 13.408010 loss_att 16.044672 loss_ctc 25.095045 loss_rnnt 11.237638 hw_loss 0.158942 lr 0.00036673 rank 2
2023-02-25 02:12:05,675 DEBUG TRAIN Batch 22/2400 loss 6.278014 loss_att 7.713476 loss_ctc 8.042483 loss_rnnt 5.645801 hw_loss 0.205983 lr 0.00036677 rank 4
2023-02-25 02:12:05,682 DEBUG TRAIN Batch 22/2400 loss 12.211400 loss_att 13.214039 loss_ctc 17.197201 loss_rnnt 11.235970 hw_loss 0.206493 lr 0.00036674 rank 6
2023-02-25 02:12:05,684 DEBUG TRAIN Batch 22/2400 loss 9.432400 loss_att 11.816166 loss_ctc 14.514522 loss_rnnt 8.135202 hw_loss 0.267801 lr 0.00036672 rank 5
2023-02-25 02:13:20,384 DEBUG TRAIN Batch 22/2500 loss 11.262022 loss_att 12.609081 loss_ctc 15.843088 loss_rnnt 10.262481 hw_loss 0.223727 lr 0.00036666 rank 0
2023-02-25 02:13:20,386 DEBUG TRAIN Batch 22/2500 loss 7.283289 loss_att 10.039237 loss_ctc 11.229875 loss_rnnt 6.021866 hw_loss 0.345041 lr 0.00036665 rank 7
2023-02-25 02:13:20,386 DEBUG TRAIN Batch 22/2500 loss 7.072618 loss_att 7.418455 loss_ctc 9.951278 loss_rnnt 6.500814 hw_loss 0.222779 lr 0.00036663 rank 2
2023-02-25 02:13:20,390 DEBUG TRAIN Batch 22/2500 loss 4.918385 loss_att 7.942892 loss_ctc 10.587207 loss_rnnt 3.409927 hw_loss 0.276962 lr 0.00036664 rank 6
2023-02-25 02:13:20,400 DEBUG TRAIN Batch 22/2500 loss 4.952776 loss_att 7.700352 loss_ctc 7.131339 loss_rnnt 3.989966 hw_loss 0.230288 lr 0.00036669 rank 3
2023-02-25 02:13:20,415 DEBUG TRAIN Batch 22/2500 loss 5.040103 loss_att 8.930447 loss_ctc 7.376495 loss_rnnt 3.827204 hw_loss 0.231210 lr 0.00036667 rank 4
2023-02-25 02:13:20,418 DEBUG TRAIN Batch 22/2500 loss 3.798680 loss_att 5.972504 loss_ctc 5.309395 loss_rnnt 3.026329 hw_loss 0.255295 lr 0.00036663 rank 5
2023-02-25 02:13:20,436 DEBUG TRAIN Batch 22/2500 loss 14.910682 loss_att 15.824831 loss_ctc 24.142445 loss_rnnt 13.386637 hw_loss 0.206835 lr 0.00036664 rank 1
2023-02-25 02:14:33,193 DEBUG TRAIN Batch 22/2600 loss 8.821198 loss_att 9.212986 loss_ctc 14.037145 loss_rnnt 7.916223 hw_loss 0.245919 lr 0.00036656 rank 0
2023-02-25 02:14:33,198 DEBUG TRAIN Batch 22/2600 loss 9.507558 loss_att 12.608316 loss_ctc 10.347747 loss_rnnt 8.678271 hw_loss 0.182080 lr 0.00036655 rank 7
2023-02-25 02:14:33,202 DEBUG TRAIN Batch 22/2600 loss 5.286545 loss_att 9.770042 loss_ctc 8.565664 loss_rnnt 3.861545 hw_loss 0.170783 lr 0.00036657 rank 4
2023-02-25 02:14:33,203 DEBUG TRAIN Batch 22/2600 loss 5.597085 loss_att 6.457495 loss_ctc 9.599627 loss_rnnt 4.751352 hw_loss 0.262462 lr 0.00036660 rank 3
2023-02-25 02:14:33,206 DEBUG TRAIN Batch 22/2600 loss 18.736280 loss_att 19.880852 loss_ctc 22.013294 loss_rnnt 17.970655 hw_loss 0.187081 lr 0.00036653 rank 5
2023-02-25 02:14:33,207 DEBUG TRAIN Batch 22/2600 loss 17.705473 loss_att 20.823013 loss_ctc 24.768227 loss_rnnt 16.043659 hw_loss 0.181137 lr 0.00036653 rank 2
2023-02-25 02:14:33,210 DEBUG TRAIN Batch 22/2600 loss 5.091310 loss_att 7.622099 loss_ctc 9.058302 loss_rnnt 3.991671 hw_loss 0.121029 lr 0.00036654 rank 6
2023-02-25 02:14:33,254 DEBUG TRAIN Batch 22/2600 loss 2.469316 loss_att 6.090609 loss_ctc 3.893091 loss_rnnt 1.457528 hw_loss 0.183175 lr 0.00036654 rank 1
2023-02-25 02:15:46,406 DEBUG TRAIN Batch 22/2700 loss 2.751691 loss_att 5.393505 loss_ctc 5.183195 loss_rnnt 1.784923 hw_loss 0.214132 lr 0.00036646 rank 0
2023-02-25 02:15:46,408 DEBUG TRAIN Batch 22/2700 loss 6.652409 loss_att 8.868805 loss_ctc 10.146883 loss_rnnt 5.615604 hw_loss 0.239239 lr 0.00036643 rank 2
2023-02-25 02:15:46,410 DEBUG TRAIN Batch 22/2700 loss 11.010069 loss_att 15.421052 loss_ctc 17.087305 loss_rnnt 9.171009 hw_loss 0.274807 lr 0.00036645 rank 1
2023-02-25 02:15:46,410 DEBUG TRAIN Batch 22/2700 loss 8.680355 loss_att 11.409060 loss_ctc 13.886252 loss_rnnt 7.347036 hw_loss 0.175236 lr 0.00036645 rank 7
2023-02-25 02:15:46,412 DEBUG TRAIN Batch 22/2700 loss 17.026079 loss_att 19.870783 loss_ctc 22.683445 loss_rnnt 15.627487 hw_loss 0.141253 lr 0.00036647 rank 4
2023-02-25 02:15:46,412 DEBUG TRAIN Batch 22/2700 loss 9.572757 loss_att 11.181440 loss_ctc 13.995628 loss_rnnt 8.608742 hw_loss 0.098554 lr 0.00036650 rank 3
2023-02-25 02:15:46,415 DEBUG TRAIN Batch 22/2700 loss 13.455240 loss_att 15.468462 loss_ctc 17.383108 loss_rnnt 12.486387 hw_loss 0.079675 lr 0.00036644 rank 6
2023-02-25 02:15:46,422 DEBUG TRAIN Batch 22/2700 loss 5.845078 loss_att 7.608100 loss_ctc 9.045276 loss_rnnt 4.980324 hw_loss 0.160230 lr 0.00036643 rank 5
2023-02-25 02:17:00,254 DEBUG TRAIN Batch 22/2800 loss 4.877896 loss_att 7.291115 loss_ctc 6.419351 loss_rnnt 4.035660 hw_loss 0.288873 lr 0.00036635 rank 1
2023-02-25 02:17:00,263 DEBUG TRAIN Batch 22/2800 loss 5.379119 loss_att 8.343356 loss_ctc 7.747640 loss_rnnt 4.365410 hw_loss 0.196985 lr 0.00036636 rank 0
2023-02-25 02:17:00,269 DEBUG TRAIN Batch 22/2800 loss 11.920149 loss_att 13.090737 loss_ctc 16.542389 loss_rnnt 10.911570 hw_loss 0.296554 lr 0.00036634 rank 2
2023-02-25 02:17:00,269 DEBUG TRAIN Batch 22/2800 loss 5.297829 loss_att 9.391909 loss_ctc 9.512360 loss_rnnt 3.761099 hw_loss 0.292456 lr 0.00036635 rank 7
2023-02-25 02:17:00,270 DEBUG TRAIN Batch 22/2800 loss 8.143801 loss_att 10.561234 loss_ctc 12.409335 loss_rnnt 6.962990 hw_loss 0.241097 lr 0.00036638 rank 4
2023-02-25 02:17:00,271 DEBUG TRAIN Batch 22/2800 loss 9.901392 loss_att 13.358562 loss_ctc 11.993082 loss_rnnt 8.798130 hw_loss 0.249256 lr 0.00036633 rank 5
2023-02-25 02:17:00,277 DEBUG TRAIN Batch 22/2800 loss 2.089366 loss_att 4.140385 loss_ctc 3.286414 loss_rnnt 1.394292 hw_loss 0.234870 lr 0.00036634 rank 6
2023-02-25 02:17:00,287 DEBUG TRAIN Batch 22/2800 loss 5.077521 loss_att 7.213866 loss_ctc 7.952144 loss_rnnt 4.171074 hw_loss 0.179803 lr 0.00036640 rank 3
2023-02-25 02:18:12,643 DEBUG TRAIN Batch 22/2900 loss 5.669668 loss_att 8.197363 loss_ctc 6.538131 loss_rnnt 4.960052 hw_loss 0.165528 lr 0.00036625 rank 1
2023-02-25 02:18:12,653 DEBUG TRAIN Batch 22/2900 loss 11.851870 loss_att 15.119968 loss_ctc 22.106327 loss_rnnt 9.720655 hw_loss 0.206876 lr 0.00036626 rank 0
2023-02-25 02:18:12,656 DEBUG TRAIN Batch 22/2900 loss 5.387061 loss_att 8.133891 loss_ctc 7.612009 loss_rnnt 4.423355 hw_loss 0.220651 lr 0.00036630 rank 3
2023-02-25 02:18:12,657 DEBUG TRAIN Batch 22/2900 loss 11.548494 loss_att 14.514670 loss_ctc 16.423437 loss_rnnt 10.177582 hw_loss 0.239410 lr 0.00036623 rank 5
2023-02-25 02:18:12,662 DEBUG TRAIN Batch 22/2900 loss 15.400041 loss_att 16.094908 loss_ctc 26.806385 loss_rnnt 13.629122 hw_loss 0.208311 lr 0.00036625 rank 7
2023-02-25 02:18:12,662 DEBUG TRAIN Batch 22/2900 loss 8.123181 loss_att 12.363416 loss_ctc 12.319094 loss_rnnt 6.655124 hw_loss 0.113542 lr 0.00036624 rank 2
2023-02-25 02:18:12,664 DEBUG TRAIN Batch 22/2900 loss 12.097519 loss_att 16.273003 loss_ctc 14.365846 loss_rnnt 10.891191 hw_loss 0.128976 lr 0.00036628 rank 4
2023-02-25 02:18:12,703 DEBUG TRAIN Batch 22/2900 loss 5.981279 loss_att 9.954386 loss_ctc 11.201575 loss_rnnt 4.370748 hw_loss 0.224757 lr 0.00036625 rank 6
2023-02-25 02:19:23,674 DEBUG TRAIN Batch 22/3000 loss 6.538929 loss_att 9.985684 loss_ctc 10.822430 loss_rnnt 5.156218 hw_loss 0.229175 lr 0.00036614 rank 2
2023-02-25 02:19:23,690 DEBUG TRAIN Batch 22/3000 loss 14.758096 loss_att 17.117039 loss_ctc 20.272804 loss_rnnt 13.458495 hw_loss 0.173471 lr 0.00036620 rank 3
2023-02-25 02:19:23,690 DEBUG TRAIN Batch 22/3000 loss 19.089468 loss_att 21.866976 loss_ctc 26.609623 loss_rnnt 17.469826 hw_loss 0.115228 lr 0.00036615 rank 1
2023-02-25 02:19:23,695 DEBUG TRAIN Batch 22/3000 loss 7.494700 loss_att 10.379290 loss_ctc 12.929012 loss_rnnt 6.097623 hw_loss 0.179220 lr 0.00036617 rank 0
2023-02-25 02:19:23,696 DEBUG TRAIN Batch 22/3000 loss 17.694485 loss_att 16.215195 loss_ctc 18.682232 loss_rnnt 17.786568 hw_loss 0.135143 lr 0.00036613 rank 5
2023-02-25 02:19:23,696 DEBUG TRAIN Batch 22/3000 loss 13.888300 loss_att 13.283003 loss_ctc 15.896834 loss_rnnt 13.614136 hw_loss 0.238909 lr 0.00036615 rank 6
2023-02-25 02:19:23,698 DEBUG TRAIN Batch 22/3000 loss 5.105451 loss_att 8.606754 loss_ctc 9.388567 loss_rnnt 3.762292 hw_loss 0.134654 lr 0.00036618 rank 4
2023-02-25 02:19:23,699 DEBUG TRAIN Batch 22/3000 loss 5.822925 loss_att 7.527878 loss_ctc 8.967011 loss_rnnt 4.989774 hw_loss 0.136778 lr 0.00036615 rank 7
2023-02-25 02:20:34,514 DEBUG TRAIN Batch 22/3100 loss 11.522249 loss_att 12.539953 loss_ctc 15.900409 loss_rnnt 10.605759 hw_loss 0.242240 lr 0.00036607 rank 0
2023-02-25 02:20:34,515 DEBUG TRAIN Batch 22/3100 loss 12.198681 loss_att 15.076601 loss_ctc 18.201712 loss_rnnt 10.707079 hw_loss 0.216774 lr 0.00036610 rank 3
2023-02-25 02:20:34,518 DEBUG TRAIN Batch 22/3100 loss 9.600506 loss_att 10.743129 loss_ctc 13.735496 loss_rnnt 8.714675 hw_loss 0.198703 lr 0.00036605 rank 6
2023-02-25 02:20:34,521 DEBUG TRAIN Batch 22/3100 loss 11.680355 loss_att 12.993021 loss_ctc 13.712010 loss_rnnt 11.027554 hw_loss 0.223836 lr 0.00036605 rank 1
2023-02-25 02:20:34,523 DEBUG TRAIN Batch 22/3100 loss 8.718410 loss_att 9.938137 loss_ctc 9.551326 loss_rnnt 8.224845 hw_loss 0.259810 lr 0.00036606 rank 7
2023-02-25 02:20:34,524 DEBUG TRAIN Batch 22/3100 loss 10.474944 loss_att 13.183239 loss_ctc 15.440763 loss_rnnt 9.135279 hw_loss 0.254806 lr 0.00036604 rank 5
2023-02-25 02:20:34,525 DEBUG TRAIN Batch 22/3100 loss 7.217450 loss_att 8.976192 loss_ctc 7.405600 loss_rnnt 6.740132 hw_loss 0.188404 lr 0.00036604 rank 2
2023-02-25 02:20:34,577 DEBUG TRAIN Batch 22/3100 loss 2.290807 loss_att 5.400381 loss_ctc 2.707286 loss_rnnt 1.480077 hw_loss 0.249910 lr 0.00036608 rank 4
2023-02-25 02:21:48,695 DEBUG TRAIN Batch 22/3200 loss 10.408651 loss_att 10.317713 loss_ctc 12.460902 loss_rnnt 10.050472 hw_loss 0.192626 lr 0.00036597 rank 0
2023-02-25 02:21:48,696 DEBUG TRAIN Batch 22/3200 loss 8.278650 loss_att 7.908416 loss_ctc 8.633033 loss_rnnt 8.177794 hw_loss 0.239348 lr 0.00036601 rank 3
2023-02-25 02:21:48,697 DEBUG TRAIN Batch 22/3200 loss 13.771824 loss_att 16.707523 loss_ctc 18.481565 loss_rnnt 12.459152 hw_loss 0.182936 lr 0.00036596 rank 7
2023-02-25 02:21:48,699 DEBUG TRAIN Batch 22/3200 loss 3.937528 loss_att 4.682673 loss_ctc 6.052998 loss_rnnt 3.358362 hw_loss 0.277639 lr 0.00036594 rank 2
2023-02-25 02:21:48,716 DEBUG TRAIN Batch 22/3200 loss 9.723942 loss_att 12.043525 loss_ctc 13.398972 loss_rnnt 8.682949 hw_loss 0.163259 lr 0.00036598 rank 4
2023-02-25 02:21:48,717 DEBUG TRAIN Batch 22/3200 loss 5.008950 loss_att 8.682056 loss_ctc 7.000614 loss_rnnt 3.950762 hw_loss 0.108770 lr 0.00036594 rank 5
2023-02-25 02:21:48,719 DEBUG TRAIN Batch 22/3200 loss 8.369246 loss_att 11.440559 loss_ctc 15.589636 loss_rnnt 6.628942 hw_loss 0.306230 lr 0.00036595 rank 6
2023-02-25 02:21:48,726 DEBUG TRAIN Batch 22/3200 loss 3.434949 loss_att 8.229630 loss_ctc 5.518962 loss_rnnt 2.103767 hw_loss 0.176958 lr 0.00036596 rank 1
2023-02-25 02:22:59,345 DEBUG TRAIN Batch 22/3300 loss 9.046792 loss_att 13.614826 loss_ctc 13.777325 loss_rnnt 7.405510 hw_loss 0.181757 lr 0.00036587 rank 0
2023-02-25 02:22:59,347 DEBUG TRAIN Batch 22/3300 loss 7.052605 loss_att 9.208422 loss_ctc 9.997437 loss_rnnt 6.189856 hw_loss 0.073016 lr 0.00036591 rank 3
2023-02-25 02:22:59,349 DEBUG TRAIN Batch 22/3300 loss 10.045923 loss_att 10.787666 loss_ctc 11.926456 loss_rnnt 9.490185 hw_loss 0.293721 lr 0.00036585 rank 6
2023-02-25 02:22:59,349 DEBUG TRAIN Batch 22/3300 loss 13.645523 loss_att 17.887486 loss_ctc 23.056011 loss_rnnt 11.417780 hw_loss 0.233660 lr 0.00036586 rank 7
2023-02-25 02:22:59,350 DEBUG TRAIN Batch 22/3300 loss 3.619423 loss_att 7.595645 loss_ctc 7.832036 loss_rnnt 2.169665 hw_loss 0.174059 lr 0.00036589 rank 4
2023-02-25 02:22:59,352 DEBUG TRAIN Batch 22/3300 loss 9.875267 loss_att 12.519053 loss_ctc 12.616679 loss_rnnt 8.867622 hw_loss 0.212561 lr 0.00036586 rank 1
2023-02-25 02:22:59,355 DEBUG TRAIN Batch 22/3300 loss 11.384446 loss_att 13.519189 loss_ctc 12.052977 loss_rnnt 10.674670 hw_loss 0.363169 lr 0.00036585 rank 2
2023-02-25 02:22:59,356 DEBUG TRAIN Batch 22/3300 loss 6.142497 loss_att 9.859310 loss_ctc 10.170138 loss_rnnt 4.751763 hw_loss 0.206910 lr 0.00036584 rank 5
2023-02-25 02:24:09,577 DEBUG TRAIN Batch 22/3400 loss 5.926552 loss_att 9.899845 loss_ctc 12.022090 loss_rnnt 4.235225 hw_loss 0.157369 lr 0.00036577 rank 0
2023-02-25 02:24:09,576 DEBUG TRAIN Batch 22/3400 loss 8.022305 loss_att 14.489081 loss_ctc 10.715827 loss_rnnt 6.298006 hw_loss 0.134641 lr 0.00036581 rank 3
2023-02-25 02:24:09,580 DEBUG TRAIN Batch 22/3400 loss 10.716214 loss_att 14.079592 loss_ctc 16.446499 loss_rnnt 9.167463 hw_loss 0.210068 lr 0.00036579 rank 4
2023-02-25 02:24:09,582 DEBUG TRAIN Batch 22/3400 loss 11.489305 loss_att 12.733677 loss_ctc 15.795265 loss_rnnt 10.602233 hw_loss 0.120129 lr 0.00036574 rank 5
2023-02-25 02:24:09,582 DEBUG TRAIN Batch 22/3400 loss 16.393888 loss_att 23.547878 loss_ctc 26.308674 loss_rnnt 13.545463 hw_loss 0.179354 lr 0.00036576 rank 7
2023-02-25 02:24:09,584 DEBUG TRAIN Batch 22/3400 loss 6.643812 loss_att 7.508667 loss_ctc 11.081053 loss_rnnt 5.789964 hw_loss 0.167334 lr 0.00036576 rank 1
2023-02-25 02:24:09,585 DEBUG TRAIN Batch 22/3400 loss 7.396611 loss_att 10.211139 loss_ctc 11.119780 loss_rnnt 6.247643 hw_loss 0.168074 lr 0.00036575 rank 2
2023-02-25 02:24:09,587 DEBUG TRAIN Batch 22/3400 loss 14.593576 loss_att 19.540529 loss_ctc 21.496984 loss_rnnt 12.523073 hw_loss 0.301231 lr 0.00036576 rank 6
2023-02-25 02:25:20,643 DEBUG TRAIN Batch 22/3500 loss 5.290142 loss_att 9.099086 loss_ctc 9.128447 loss_rnnt 3.912016 hw_loss 0.196057 lr 0.00036571 rank 3
2023-02-25 02:25:20,645 DEBUG TRAIN Batch 22/3500 loss 11.499802 loss_att 13.226674 loss_ctc 17.403282 loss_rnnt 10.233368 hw_loss 0.251116 lr 0.00036566 rank 6
2023-02-25 02:25:20,648 DEBUG TRAIN Batch 22/3500 loss 24.964949 loss_att 29.279230 loss_ctc 33.301456 loss_rnnt 22.871643 hw_loss 0.222963 lr 0.00036569 rank 4
2023-02-25 02:25:20,649 DEBUG TRAIN Batch 22/3500 loss 12.376445 loss_att 14.632495 loss_ctc 20.965912 loss_rnnt 10.643495 hw_loss 0.255897 lr 0.00036568 rank 0
2023-02-25 02:25:20,651 DEBUG TRAIN Batch 22/3500 loss 5.672089 loss_att 9.062260 loss_ctc 7.871759 loss_rnnt 4.604073 hw_loss 0.181298 lr 0.00036566 rank 1
2023-02-25 02:25:20,652 DEBUG TRAIN Batch 22/3500 loss 14.279487 loss_att 16.100969 loss_ctc 21.944292 loss_rnnt 12.778065 hw_loss 0.215909 lr 0.00036565 rank 2
2023-02-25 02:25:20,653 DEBUG TRAIN Batch 22/3500 loss 15.282380 loss_att 18.112959 loss_ctc 22.165476 loss_rnnt 13.693014 hw_loss 0.197820 lr 0.00036566 rank 7
2023-02-25 02:25:20,665 DEBUG TRAIN Batch 22/3500 loss 8.432579 loss_att 14.071934 loss_ctc 12.969991 loss_rnnt 6.569950 hw_loss 0.243319 lr 0.00036564 rank 5
2023-02-25 02:26:33,680 DEBUG TRAIN Batch 22/3600 loss 4.071850 loss_att 7.417522 loss_ctc 7.881026 loss_rnnt 2.799664 hw_loss 0.178428 lr 0.00036556 rank 1
2023-02-25 02:26:33,680 DEBUG TRAIN Batch 22/3600 loss 8.690283 loss_att 13.366484 loss_ctc 14.464987 loss_rnnt 6.821561 hw_loss 0.306601 lr 0.00036561 rank 3
2023-02-25 02:26:33,684 DEBUG TRAIN Batch 22/3600 loss 8.418817 loss_att 9.643043 loss_ctc 10.398108 loss_rnnt 7.825130 hw_loss 0.159252 lr 0.00036559 rank 4
2023-02-25 02:26:33,684 DEBUG TRAIN Batch 22/3600 loss 7.458960 loss_att 8.933181 loss_ctc 8.524910 loss_rnnt 6.901539 hw_loss 0.225843 lr 0.00036556 rank 6
2023-02-25 02:26:33,686 DEBUG TRAIN Batch 22/3600 loss 11.847901 loss_att 13.718413 loss_ctc 20.389988 loss_rnnt 10.246841 hw_loss 0.165023 lr 0.00036557 rank 7
2023-02-25 02:26:33,686 DEBUG TRAIN Batch 22/3600 loss 10.578155 loss_att 14.191025 loss_ctc 16.218283 loss_rnnt 9.010972 hw_loss 0.173610 lr 0.00036558 rank 0
2023-02-25 02:26:33,689 DEBUG TRAIN Batch 22/3600 loss 9.168878 loss_att 8.321997 loss_ctc 10.618198 loss_rnnt 9.073891 hw_loss 0.133349 lr 0.00036555 rank 2
2023-02-25 02:26:33,691 DEBUG TRAIN Batch 22/3600 loss 1.295971 loss_att 4.843460 loss_ctc 1.286149 loss_rnnt 0.468694 hw_loss 0.223293 lr 0.00036555 rank 5
2023-02-25 02:27:43,680 DEBUG TRAIN Batch 22/3700 loss 8.028900 loss_att 10.598851 loss_ctc 10.570045 loss_rnnt 7.051180 hw_loss 0.234208 lr 0.00036552 rank 3
2023-02-25 02:27:43,680 DEBUG TRAIN Batch 22/3700 loss 9.203307 loss_att 11.397942 loss_ctc 13.940054 loss_rnnt 8.048376 hw_loss 0.158322 lr 0.00036548 rank 0
2023-02-25 02:27:43,682 DEBUG TRAIN Batch 22/3700 loss 16.932896 loss_att 16.512035 loss_ctc 22.655455 loss_rnnt 16.122507 hw_loss 0.246657 lr 0.00036546 rank 6
2023-02-25 02:27:43,686 DEBUG TRAIN Batch 22/3700 loss 3.484612 loss_att 5.798450 loss_ctc 5.377098 loss_rnnt 2.650262 hw_loss 0.223596 lr 0.00036547 rank 1
2023-02-25 02:27:43,688 DEBUG TRAIN Batch 22/3700 loss 11.991866 loss_att 15.006227 loss_ctc 17.909576 loss_rnnt 10.514417 hw_loss 0.160405 lr 0.00036547 rank 7
2023-02-25 02:27:43,687 DEBUG TRAIN Batch 22/3700 loss 10.842000 loss_att 13.302650 loss_ctc 14.443970 loss_rnnt 9.708153 hw_loss 0.302726 lr 0.00036545 rank 5
2023-02-25 02:27:43,689 DEBUG TRAIN Batch 22/3700 loss 7.197651 loss_att 10.914395 loss_ctc 10.511343 loss_rnnt 5.926177 hw_loss 0.161814 lr 0.00036545 rank 2
2023-02-25 02:27:43,734 DEBUG TRAIN Batch 22/3700 loss 9.234403 loss_att 12.013790 loss_ctc 14.545115 loss_rnnt 7.807260 hw_loss 0.305943 lr 0.00036549 rank 4
2023-02-25 02:28:54,065 DEBUG TRAIN Batch 22/3800 loss 7.521431 loss_att 8.559000 loss_ctc 10.268654 loss_rnnt 6.813435 hw_loss 0.251598 lr 0.00036538 rank 0
2023-02-25 02:28:54,068 DEBUG TRAIN Batch 22/3800 loss 11.779218 loss_att 12.088190 loss_ctc 15.036364 loss_rnnt 11.146336 hw_loss 0.256504 lr 0.00036542 rank 3
2023-02-25 02:28:54,068 DEBUG TRAIN Batch 22/3800 loss 7.775655 loss_att 10.671438 loss_ctc 12.151179 loss_rnnt 6.446653 hw_loss 0.312079 lr 0.00036536 rank 6
2023-02-25 02:28:54,071 DEBUG TRAIN Batch 22/3800 loss 12.762663 loss_att 12.599684 loss_ctc 15.147136 loss_rnnt 12.337148 hw_loss 0.262841 lr 0.00036537 rank 1
2023-02-25 02:28:54,073 DEBUG TRAIN Batch 22/3800 loss 6.414649 loss_att 7.333488 loss_ctc 9.991732 loss_rnnt 5.578220 hw_loss 0.329469 lr 0.00036535 rank 5
2023-02-25 02:28:54,073 DEBUG TRAIN Batch 22/3800 loss 6.773973 loss_att 8.369558 loss_ctc 9.083655 loss_rnnt 5.988091 hw_loss 0.297761 lr 0.00036537 rank 7
2023-02-25 02:28:54,074 DEBUG TRAIN Batch 22/3800 loss 5.640342 loss_att 7.199025 loss_ctc 6.134151 loss_rnnt 5.174674 hw_loss 0.165170 lr 0.00036540 rank 4
2023-02-25 02:28:54,075 DEBUG TRAIN Batch 22/3800 loss 6.727587 loss_att 9.072824 loss_ctc 9.716302 loss_rnnt 5.654109 hw_loss 0.386130 lr 0.00036536 rank 2
2023-02-25 02:30:07,524 DEBUG TRAIN Batch 22/3900 loss 14.156276 loss_att 17.748598 loss_ctc 17.023506 loss_rnnt 12.957927 hw_loss 0.182976 lr 0.00036530 rank 4
2023-02-25 02:30:07,525 DEBUG TRAIN Batch 22/3900 loss 16.361109 loss_att 23.844067 loss_ctc 24.515518 loss_rnnt 13.686714 hw_loss 0.169777 lr 0.00036527 rank 7
2023-02-25 02:30:07,528 DEBUG TRAIN Batch 22/3900 loss 2.807564 loss_att 4.943095 loss_ctc 4.801277 loss_rnnt 1.927312 hw_loss 0.351220 lr 0.00036526 rank 2
2023-02-25 02:30:07,529 DEBUG TRAIN Batch 22/3900 loss 15.400028 loss_att 16.462690 loss_ctc 19.748058 loss_rnnt 14.546268 hw_loss 0.115295 lr 0.00036528 rank 0
2023-02-25 02:30:07,532 DEBUG TRAIN Batch 22/3900 loss 6.221230 loss_att 7.244037 loss_ctc 9.968801 loss_rnnt 5.462433 hw_loss 0.102298 lr 0.00036527 rank 6
2023-02-25 02:30:07,533 DEBUG TRAIN Batch 22/3900 loss 5.469030 loss_att 7.592497 loss_ctc 12.712943 loss_rnnt 4.038321 hw_loss 0.075302 lr 0.00036532 rank 3
2023-02-25 02:30:07,533 DEBUG TRAIN Batch 22/3900 loss 9.710977 loss_att 14.486258 loss_ctc 14.700129 loss_rnnt 7.969245 hw_loss 0.227731 lr 0.00036527 rank 1
2023-02-25 02:30:07,580 DEBUG TRAIN Batch 22/3900 loss 8.739339 loss_att 12.547325 loss_ctc 12.411117 loss_rnnt 7.358809 hw_loss 0.242552 lr 0.00036525 rank 5
2023-02-25 02:31:19,692 DEBUG TRAIN Batch 22/4000 loss 6.090690 loss_att 11.678575 loss_ctc 10.806570 loss_rnnt 4.254438 hw_loss 0.168544 lr 0.00036517 rank 1
2023-02-25 02:31:19,695 DEBUG TRAIN Batch 22/4000 loss 6.874638 loss_att 11.107324 loss_ctc 9.016024 loss_rnnt 5.605462 hw_loss 0.257101 lr 0.00036520 rank 4
2023-02-25 02:31:19,695 DEBUG TRAIN Batch 22/4000 loss 8.316207 loss_att 11.067911 loss_ctc 11.399237 loss_rnnt 7.234419 hw_loss 0.225706 lr 0.00036517 rank 6
2023-02-25 02:31:19,695 DEBUG TRAIN Batch 22/4000 loss 6.012801 loss_att 8.048798 loss_ctc 9.416239 loss_rnnt 5.099081 hw_loss 0.098868 lr 0.00036519 rank 0
2023-02-25 02:31:19,696 DEBUG TRAIN Batch 22/4000 loss 7.141583 loss_att 8.524530 loss_ctc 9.636548 loss_rnnt 6.419881 hw_loss 0.210844 lr 0.00036516 rank 2
2023-02-25 02:31:19,696 DEBUG TRAIN Batch 22/4000 loss 3.404703 loss_att 6.341706 loss_ctc 5.247925 loss_rnnt 2.510414 hw_loss 0.114612 lr 0.00036522 rank 3
2023-02-25 02:31:19,702 DEBUG TRAIN Batch 22/4000 loss 10.934703 loss_att 13.308911 loss_ctc 16.085789 loss_rnnt 9.674429 hw_loss 0.184912 lr 0.00036518 rank 7
2023-02-25 02:31:19,702 DEBUG TRAIN Batch 22/4000 loss 28.238031 loss_att 33.415836 loss_ctc 35.201645 loss_rnnt 26.165268 hw_loss 0.203852 lr 0.00036516 rank 5
2023-02-25 02:32:29,667 DEBUG TRAIN Batch 22/4100 loss 4.629319 loss_att 7.463422 loss_ctc 4.688890 loss_rnnt 3.983371 hw_loss 0.133473 lr 0.00036508 rank 1
2023-02-25 02:32:29,672 DEBUG TRAIN Batch 22/4100 loss 6.772419 loss_att 9.823605 loss_ctc 10.630117 loss_rnnt 5.522579 hw_loss 0.234831 lr 0.00036507 rank 6
2023-02-25 02:32:29,674 DEBUG TRAIN Batch 22/4100 loss 6.816055 loss_att 11.124319 loss_ctc 11.214157 loss_rnnt 5.266834 hw_loss 0.189664 lr 0.00036509 rank 0
2023-02-25 02:32:29,676 DEBUG TRAIN Batch 22/4100 loss 15.193792 loss_att 14.922617 loss_ctc 17.016850 loss_rnnt 14.891696 hw_loss 0.212357 lr 0.00036506 rank 2
2023-02-25 02:32:29,679 DEBUG TRAIN Batch 22/4100 loss 5.124142 loss_att 6.216748 loss_ctc 6.513337 loss_rnnt 4.624276 hw_loss 0.180223 lr 0.00036513 rank 3
2023-02-25 02:32:29,680 DEBUG TRAIN Batch 22/4100 loss 10.249724 loss_att 13.940714 loss_ctc 14.289232 loss_rnnt 8.891425 hw_loss 0.152811 lr 0.00036510 rank 4
2023-02-25 02:32:29,682 DEBUG TRAIN Batch 22/4100 loss 9.278979 loss_att 11.438869 loss_ctc 15.784903 loss_rnnt 7.880944 hw_loss 0.184876 lr 0.00036506 rank 5
2023-02-25 02:32:29,683 DEBUG TRAIN Batch 22/4100 loss 11.699670 loss_att 13.282139 loss_ctc 18.081034 loss_rnnt 10.436079 hw_loss 0.180467 lr 0.00036508 rank 7
2023-02-25 02:33:41,397 DEBUG TRAIN Batch 22/4200 loss 5.252301 loss_att 7.171156 loss_ctc 7.298127 loss_rnnt 4.459080 hw_loss 0.256262 lr 0.00036503 rank 3
2023-02-25 02:33:41,410 DEBUG TRAIN Batch 22/4200 loss 7.629533 loss_att 12.163411 loss_ctc 13.055800 loss_rnnt 5.893209 hw_loss 0.198834 lr 0.00036499 rank 0
2023-02-25 02:33:41,410 DEBUG TRAIN Batch 22/4200 loss 19.660555 loss_att 20.310064 loss_ctc 31.231916 loss_rnnt 17.889679 hw_loss 0.183986 lr 0.00036498 rank 6
2023-02-25 02:33:41,412 DEBUG TRAIN Batch 22/4200 loss 4.654707 loss_att 7.841395 loss_ctc 7.336408 loss_rnnt 3.568254 hw_loss 0.171666 lr 0.00036498 rank 7
2023-02-25 02:33:41,412 DEBUG TRAIN Batch 22/4200 loss 13.090714 loss_att 13.674747 loss_ctc 15.935359 loss_rnnt 12.478785 hw_loss 0.217190 lr 0.00036498 rank 1
2023-02-25 02:33:41,414 DEBUG TRAIN Batch 22/4200 loss 9.419173 loss_att 10.887419 loss_ctc 16.194933 loss_rnnt 8.056286 hw_loss 0.310881 lr 0.00036497 rank 2
2023-02-25 02:33:41,415 DEBUG TRAIN Batch 22/4200 loss 5.746382 loss_att 9.060250 loss_ctc 7.745863 loss_rnnt 4.706283 hw_loss 0.207616 lr 0.00036496 rank 5
2023-02-25 02:33:41,416 DEBUG TRAIN Batch 22/4200 loss 10.613122 loss_att 11.830106 loss_ctc 14.260953 loss_rnnt 9.763266 hw_loss 0.225153 lr 0.00036501 rank 4
2023-02-25 02:34:54,529 DEBUG TRAIN Batch 22/4300 loss 5.579116 loss_att 9.006035 loss_ctc 9.508397 loss_rnnt 4.235423 hw_loss 0.252009 lr 0.00036488 rank 6
2023-02-25 02:34:54,530 DEBUG TRAIN Batch 22/4300 loss 9.681832 loss_att 13.175072 loss_ctc 14.077343 loss_rnnt 8.338930 hw_loss 0.109098 lr 0.00036493 rank 3
2023-02-25 02:34:54,532 DEBUG TRAIN Batch 22/4300 loss 10.087889 loss_att 11.970578 loss_ctc 13.468647 loss_rnnt 9.171814 hw_loss 0.166443 lr 0.00036488 rank 7
2023-02-25 02:34:54,534 DEBUG TRAIN Batch 22/4300 loss 7.806251 loss_att 10.556965 loss_ctc 12.034102 loss_rnnt 6.527573 hw_loss 0.309040 lr 0.00036487 rank 2
2023-02-25 02:34:54,534 DEBUG TRAIN Batch 22/4300 loss 7.270275 loss_att 8.382583 loss_ctc 11.365266 loss_rnnt 6.386575 hw_loss 0.216074 lr 0.00036488 rank 1
2023-02-25 02:34:54,536 DEBUG TRAIN Batch 22/4300 loss 20.391174 loss_att 23.534525 loss_ctc 30.505428 loss_rnnt 18.340378 hw_loss 0.137924 lr 0.00036490 rank 0
2023-02-25 02:34:54,536 DEBUG TRAIN Batch 22/4300 loss 15.373845 loss_att 16.893948 loss_ctc 19.767311 loss_rnnt 14.400517 hw_loss 0.156587 lr 0.00036491 rank 4
2023-02-25 02:34:54,538 DEBUG TRAIN Batch 22/4300 loss 17.124594 loss_att 17.743183 loss_ctc 24.285000 loss_rnnt 15.985134 hw_loss 0.114411 lr 0.00036486 rank 5
2023-02-25 02:36:05,706 DEBUG TRAIN Batch 22/4400 loss 6.877659 loss_att 9.819403 loss_ctc 12.018519 loss_rnnt 5.490291 hw_loss 0.212949 lr 0.00036484 rank 3
2023-02-25 02:36:05,706 DEBUG TRAIN Batch 22/4400 loss 10.582520 loss_att 12.328554 loss_ctc 16.208588 loss_rnnt 9.408243 hw_loss 0.140490 lr 0.00036478 rank 6
2023-02-25 02:36:05,713 DEBUG TRAIN Batch 22/4400 loss 11.552339 loss_att 14.196008 loss_ctc 19.548805 loss_rnnt 9.899641 hw_loss 0.108316 lr 0.00036477 rank 2
2023-02-25 02:36:05,713 DEBUG TRAIN Batch 22/4400 loss 8.016559 loss_att 10.363338 loss_ctc 15.345763 loss_rnnt 6.446594 hw_loss 0.231339 lr 0.00036479 rank 7
2023-02-25 02:36:05,723 DEBUG TRAIN Batch 22/4400 loss 8.037581 loss_att 7.586133 loss_ctc 10.865780 loss_rnnt 7.622339 hw_loss 0.240825 lr 0.00036481 rank 4
2023-02-25 02:36:05,729 DEBUG TRAIN Batch 22/4400 loss 11.018250 loss_att 13.934826 loss_ctc 16.322557 loss_rnnt 9.612614 hw_loss 0.215776 lr 0.00036480 rank 0
2023-02-25 02:36:05,738 DEBUG TRAIN Batch 22/4400 loss 10.807150 loss_att 11.770139 loss_ctc 14.203938 loss_rnnt 10.042167 hw_loss 0.224025 lr 0.00036478 rank 1
2023-02-25 02:36:05,762 DEBUG TRAIN Batch 22/4400 loss 6.508920 loss_att 9.230739 loss_ctc 10.120454 loss_rnnt 5.400481 hw_loss 0.154757 lr 0.00036477 rank 5
2023-02-25 02:37:16,300 DEBUG TRAIN Batch 22/4500 loss 14.772489 loss_att 17.805786 loss_ctc 25.323185 loss_rnnt 12.650176 hw_loss 0.204175 lr 0.00036468 rank 6
2023-02-25 02:37:16,302 DEBUG TRAIN Batch 22/4500 loss 8.507104 loss_att 10.325182 loss_ctc 7.490149 loss_rnnt 8.209616 hw_loss 0.130252 lr 0.00036472 rank 4
2023-02-25 02:37:16,303 DEBUG TRAIN Batch 22/4500 loss 9.499623 loss_att 9.168092 loss_ctc 12.144979 loss_rnnt 9.089984 hw_loss 0.231060 lr 0.00036470 rank 0
2023-02-25 02:37:16,303 DEBUG TRAIN Batch 22/4500 loss 8.545526 loss_att 9.206456 loss_ctc 10.930439 loss_rnnt 7.985837 hw_loss 0.205337 lr 0.00036474 rank 3
2023-02-25 02:37:16,304 DEBUG TRAIN Batch 22/4500 loss 5.249077 loss_att 10.672523 loss_ctc 10.536463 loss_rnnt 3.347225 hw_loss 0.210333 lr 0.00036469 rank 7
2023-02-25 02:37:16,307 DEBUG TRAIN Batch 22/4500 loss 7.983585 loss_att 10.829237 loss_ctc 11.545519 loss_rnnt 6.844317 hw_loss 0.178525 lr 0.00036469 rank 1
2023-02-25 02:37:16,307 DEBUG TRAIN Batch 22/4500 loss 13.124429 loss_att 18.834063 loss_ctc 18.393187 loss_rnnt 11.177668 hw_loss 0.191874 lr 0.00036468 rank 2
2023-02-25 02:37:16,311 DEBUG TRAIN Batch 22/4500 loss 8.059748 loss_att 11.829247 loss_ctc 17.199064 loss_rnnt 5.979752 hw_loss 0.201600 lr 0.00036467 rank 5
2023-02-25 02:38:28,755 DEBUG TRAIN Batch 22/4600 loss 9.008335 loss_att 11.868363 loss_ctc 9.264655 loss_rnnt 8.211742 hw_loss 0.357020 lr 0.00036459 rank 6
2023-02-25 02:38:28,768 DEBUG TRAIN Batch 22/4600 loss 10.526033 loss_att 15.578523 loss_ctc 12.496150 loss_rnnt 9.190767 hw_loss 0.116410 lr 0.00036460 rank 0
2023-02-25 02:38:28,773 DEBUG TRAIN Batch 22/4600 loss 6.498332 loss_att 7.863170 loss_ctc 8.609526 loss_rnnt 5.865040 hw_loss 0.147809 lr 0.00036458 rank 2
2023-02-25 02:38:28,775 DEBUG TRAIN Batch 22/4600 loss 1.393787 loss_att 5.461052 loss_ctc 1.566650 loss_rnnt 0.413836 hw_loss 0.268967 lr 0.00036464 rank 3
2023-02-25 02:38:28,777 DEBUG TRAIN Batch 22/4600 loss 14.564334 loss_att 17.348301 loss_ctc 17.094259 loss_rnnt 13.558640 hw_loss 0.209207 lr 0.00036459 rank 1
2023-02-25 02:38:28,781 DEBUG TRAIN Batch 22/4600 loss 12.959690 loss_att 16.455359 loss_ctc 18.183285 loss_rnnt 11.491833 hw_loss 0.135459 lr 0.00036462 rank 4
2023-02-25 02:38:28,784 DEBUG TRAIN Batch 22/4600 loss 11.746616 loss_att 19.087481 loss_ctc 17.878979 loss_rnnt 9.327100 hw_loss 0.250678 lr 0.00036457 rank 5
2023-02-25 02:38:28,808 DEBUG TRAIN Batch 22/4600 loss 12.141742 loss_att 13.983141 loss_ctc 17.101622 loss_rnnt 11.049843 hw_loss 0.116816 lr 0.00036459 rank 7
2023-02-25 02:39:40,202 DEBUG TRAIN Batch 22/4700 loss 5.788844 loss_att 7.547761 loss_ctc 6.902871 loss_rnnt 5.171709 hw_loss 0.219028 lr 0.00036451 rank 0
2023-02-25 02:39:40,205 DEBUG TRAIN Batch 22/4700 loss 3.744658 loss_att 6.016922 loss_ctc 3.693668 loss_rnnt 3.175966 hw_loss 0.226945 lr 0.00036449 rank 1
2023-02-25 02:39:40,205 DEBUG TRAIN Batch 22/4700 loss 7.317979 loss_att 12.865572 loss_ctc 10.035801 loss_rnnt 5.749649 hw_loss 0.180816 lr 0.00036448 rank 5
2023-02-25 02:39:40,205 DEBUG TRAIN Batch 22/4700 loss 5.534991 loss_att 6.063093 loss_ctc 6.148964 loss_rnnt 5.229835 hw_loss 0.220636 lr 0.00036454 rank 3
2023-02-25 02:39:40,208 DEBUG TRAIN Batch 22/4700 loss 7.847300 loss_att 9.153334 loss_ctc 10.614390 loss_rnnt 7.068760 hw_loss 0.278224 lr 0.00036452 rank 4
2023-02-25 02:39:40,209 DEBUG TRAIN Batch 22/4700 loss 7.610366 loss_att 9.407283 loss_ctc 10.488047 loss_rnnt 6.734657 hw_loss 0.248689 lr 0.00036450 rank 7
2023-02-25 02:39:40,210 DEBUG TRAIN Batch 22/4700 loss 13.040935 loss_att 16.738880 loss_ctc 13.838333 loss_rnnt 12.113804 hw_loss 0.152293 lr 0.00036448 rank 2
2023-02-25 02:39:40,254 DEBUG TRAIN Batch 22/4700 loss 7.183185 loss_att 8.567061 loss_ctc 7.558185 loss_rnnt 6.737915 hw_loss 0.222177 lr 0.00036449 rank 6
2023-02-25 02:40:50,167 DEBUG TRAIN Batch 22/4800 loss 6.760797 loss_att 8.249157 loss_ctc 10.491932 loss_rnnt 5.907219 hw_loss 0.109540 lr 0.00036438 rank 5
2023-02-25 02:40:50,173 DEBUG TRAIN Batch 22/4800 loss 10.745215 loss_att 14.861956 loss_ctc 17.126226 loss_rnnt 8.902622 hw_loss 0.315832 lr 0.00036441 rank 0
2023-02-25 02:40:50,182 DEBUG TRAIN Batch 22/4800 loss 11.961728 loss_att 12.641468 loss_ctc 20.430347 loss_rnnt 10.581318 hw_loss 0.216212 lr 0.00036439 rank 6
2023-02-25 02:40:50,183 DEBUG TRAIN Batch 22/4800 loss 5.253158 loss_att 6.524149 loss_ctc 6.256267 loss_rnnt 4.724605 hw_loss 0.263637 lr 0.00036440 rank 7
2023-02-25 02:40:50,183 DEBUG TRAIN Batch 22/4800 loss 5.210027 loss_att 8.549614 loss_ctc 6.888824 loss_rnnt 4.238148 hw_loss 0.150230 lr 0.00036440 rank 1
2023-02-25 02:40:50,185 DEBUG TRAIN Batch 22/4800 loss 10.310808 loss_att 12.158167 loss_ctc 13.241070 loss_rnnt 9.447338 hw_loss 0.193682 lr 0.00036439 rank 2
2023-02-25 02:40:50,221 DEBUG TRAIN Batch 22/4800 loss 10.018148 loss_att 12.651554 loss_ctc 15.401213 loss_rnnt 8.697769 hw_loss 0.142416 lr 0.00036443 rank 4
2023-02-25 02:40:50,252 DEBUG TRAIN Batch 22/4800 loss 7.910847 loss_att 11.707829 loss_ctc 11.761980 loss_rnnt 6.534269 hw_loss 0.194433 lr 0.00036445 rank 3
2023-02-25 02:42:01,459 DEBUG TRAIN Batch 22/4900 loss 13.916533 loss_att 16.742039 loss_ctc 23.406078 loss_rnnt 11.960139 hw_loss 0.236290 lr 0.00036430 rank 6
2023-02-25 02:42:01,468 DEBUG TRAIN Batch 22/4900 loss 23.041790 loss_att 25.269253 loss_ctc 33.044155 loss_rnnt 21.179714 hw_loss 0.155503 lr 0.00036431 rank 0
2023-02-25 02:42:01,472 DEBUG TRAIN Batch 22/4900 loss 10.405740 loss_att 12.671499 loss_ctc 14.260636 loss_rnnt 9.345392 hw_loss 0.174764 lr 0.00036429 rank 2
2023-02-25 02:42:01,474 DEBUG TRAIN Batch 22/4900 loss 12.895818 loss_att 15.985511 loss_ctc 18.358732 loss_rnnt 11.400178 hw_loss 0.279957 lr 0.00036435 rank 3
2023-02-25 02:42:01,477 DEBUG TRAIN Batch 22/4900 loss 7.618158 loss_att 11.896867 loss_ctc 11.102395 loss_rnnt 6.257384 hw_loss 0.075876 lr 0.00036430 rank 7
2023-02-25 02:42:01,479 DEBUG TRAIN Batch 22/4900 loss 4.933436 loss_att 5.753437 loss_ctc 5.802008 loss_rnnt 4.529401 hw_loss 0.232922 lr 0.00036430 rank 1
2023-02-25 02:42:01,479 DEBUG TRAIN Batch 22/4900 loss 8.975169 loss_att 10.039534 loss_ctc 13.239231 loss_rnnt 8.079209 hw_loss 0.214773 lr 0.00036428 rank 5
2023-02-25 02:42:01,500 DEBUG TRAIN Batch 22/4900 loss 8.873217 loss_att 11.933220 loss_ctc 11.431562 loss_rnnt 7.780856 hw_loss 0.261090 lr 0.00036433 rank 4
2023-02-25 02:43:15,111 DEBUG TRAIN Batch 22/5000 loss 9.698406 loss_att 12.870647 loss_ctc 14.720934 loss_rnnt 8.274795 hw_loss 0.224048 lr 0.00036422 rank 0
2023-02-25 02:43:15,113 DEBUG TRAIN Batch 22/5000 loss 13.581076 loss_att 14.522871 loss_ctc 18.833481 loss_rnnt 12.608975 hw_loss 0.156413 lr 0.00036425 rank 3
2023-02-25 02:43:15,113 DEBUG TRAIN Batch 22/5000 loss 4.828959 loss_att 8.958996 loss_ctc 8.221527 loss_rnnt 3.423296 hw_loss 0.238710 lr 0.00036420 rank 6
2023-02-25 02:43:15,113 DEBUG TRAIN Batch 22/5000 loss 4.163854 loss_att 6.719840 loss_ctc 7.334448 loss_rnnt 3.083311 hw_loss 0.274875 lr 0.00036419 rank 2
2023-02-25 02:43:15,114 DEBUG TRAIN Batch 22/5000 loss 5.806444 loss_att 7.830505 loss_ctc 9.422384 loss_rnnt 4.807571 hw_loss 0.209878 lr 0.00036423 rank 4
2023-02-25 02:43:15,114 DEBUG TRAIN Batch 22/5000 loss 12.401713 loss_att 13.125740 loss_ctc 20.972328 loss_rnnt 10.990511 hw_loss 0.231841 lr 0.00036420 rank 1
2023-02-25 02:43:15,120 DEBUG TRAIN Batch 22/5000 loss 4.083797 loss_att 6.281548 loss_ctc 6.528285 loss_rnnt 3.200879 hw_loss 0.220191 lr 0.00036421 rank 7
2023-02-25 02:43:15,123 DEBUG TRAIN Batch 22/5000 loss 10.916162 loss_att 12.761242 loss_ctc 14.486667 loss_rnnt 9.934478 hw_loss 0.256129 lr 0.00036419 rank 5
2023-02-25 02:44:25,913 DEBUG TRAIN Batch 22/5100 loss 6.197279 loss_att 7.270626 loss_ctc 9.793819 loss_rnnt 5.319682 hw_loss 0.343855 lr 0.00036412 rank 0
2023-02-25 02:44:25,916 DEBUG TRAIN Batch 22/5100 loss 8.899959 loss_att 11.895088 loss_ctc 13.087416 loss_rnnt 7.624305 hw_loss 0.221813 lr 0.00036411 rank 1
2023-02-25 02:44:25,919 DEBUG TRAIN Batch 22/5100 loss 7.544831 loss_att 11.568348 loss_ctc 10.250196 loss_rnnt 6.256629 hw_loss 0.230218 lr 0.00036414 rank 4
2023-02-25 02:44:25,920 DEBUG TRAIN Batch 22/5100 loss 5.691523 loss_att 9.156812 loss_ctc 8.554943 loss_rnnt 4.492888 hw_loss 0.232102 lr 0.00036410 rank 6
2023-02-25 02:44:25,922 DEBUG TRAIN Batch 22/5100 loss 3.808950 loss_att 5.350176 loss_ctc 5.865998 loss_rnnt 3.083696 hw_loss 0.267631 lr 0.00036416 rank 3
2023-02-25 02:44:25,924 DEBUG TRAIN Batch 22/5100 loss 7.729056 loss_att 11.515499 loss_ctc 11.271746 loss_rnnt 6.425194 hw_loss 0.139155 lr 0.00036411 rank 7
2023-02-25 02:44:25,924 DEBUG TRAIN Batch 22/5100 loss 7.772230 loss_att 10.280037 loss_ctc 10.930851 loss_rnnt 6.701280 hw_loss 0.277947 lr 0.00036409 rank 5
2023-02-25 02:44:25,926 DEBUG TRAIN Batch 22/5100 loss 11.020514 loss_att 12.094529 loss_ctc 15.016138 loss_rnnt 10.137428 hw_loss 0.254123 lr 0.00036410 rank 2
2023-02-25 02:45:36,680 DEBUG TRAIN Batch 22/5200 loss 10.689878 loss_att 13.750530 loss_ctc 19.099781 loss_rnnt 8.759647 hw_loss 0.368960 lr 0.00036401 rank 6
2023-02-25 02:45:36,685 DEBUG TRAIN Batch 22/5200 loss 15.434643 loss_att 17.341606 loss_ctc 20.157654 loss_rnnt 14.264323 hw_loss 0.298486 lr 0.00036404 rank 4
2023-02-25 02:45:36,685 DEBUG TRAIN Batch 22/5200 loss 4.410983 loss_att 7.332759 loss_ctc 6.171864 loss_rnnt 3.433366 hw_loss 0.297146 lr 0.00036400 rank 2
2023-02-25 02:45:36,686 DEBUG TRAIN Batch 22/5200 loss 7.789436 loss_att 10.458220 loss_ctc 11.409969 loss_rnnt 6.650747 hw_loss 0.229115 lr 0.00036402 rank 0
2023-02-25 02:45:36,686 DEBUG TRAIN Batch 22/5200 loss 5.580526 loss_att 6.630157 loss_ctc 7.419443 loss_rnnt 5.012521 hw_loss 0.211668 lr 0.00036401 rank 7
2023-02-25 02:45:36,686 DEBUG TRAIN Batch 22/5200 loss 10.018879 loss_att 12.082674 loss_ctc 19.869930 loss_rnnt 8.178749 hw_loss 0.213558 lr 0.00036401 rank 1
2023-02-25 02:45:36,689 DEBUG TRAIN Batch 22/5200 loss 5.358683 loss_att 7.876176 loss_ctc 8.285400 loss_rnnt 4.424137 hw_loss 0.076534 lr 0.00036399 rank 5
2023-02-25 02:45:36,736 DEBUG TRAIN Batch 22/5200 loss 7.365520 loss_att 13.342200 loss_ctc 10.566721 loss_rnnt 5.686119 hw_loss 0.107319 lr 0.00036406 rank 3
2023-02-25 02:46:49,913 DEBUG TRAIN Batch 22/5300 loss 14.214797 loss_att 18.978783 loss_ctc 27.856247 loss_rnnt 11.242729 hw_loss 0.375769 lr 0.00036390 rank 5
2023-02-25 02:46:49,919 DEBUG TRAIN Batch 22/5300 loss 6.275358 loss_att 8.636406 loss_ctc 9.165447 loss_rnnt 5.334379 hw_loss 0.156420 lr 0.00036393 rank 0
2023-02-25 02:46:49,924 DEBUG TRAIN Batch 22/5300 loss 6.428395 loss_att 9.947954 loss_ctc 7.185705 loss_rnnt 5.495516 hw_loss 0.239987 lr 0.00036391 rank 1
2023-02-25 02:46:49,926 DEBUG TRAIN Batch 22/5300 loss 8.428953 loss_att 11.549335 loss_ctc 13.760752 loss_rnnt 7.013668 hw_loss 0.150567 lr 0.00036390 rank 2
2023-02-25 02:46:49,926 DEBUG TRAIN Batch 22/5300 loss 8.169632 loss_att 11.634760 loss_ctc 12.480907 loss_rnnt 6.784072 hw_loss 0.220683 lr 0.00036392 rank 7
2023-02-25 02:46:49,929 DEBUG TRAIN Batch 22/5300 loss 8.001271 loss_att 9.428708 loss_ctc 9.254972 loss_rnnt 7.413453 hw_loss 0.253445 lr 0.00036394 rank 4
2023-02-25 02:46:49,946 DEBUG TRAIN Batch 22/5300 loss 8.049461 loss_att 12.869211 loss_ctc 13.783819 loss_rnnt 6.204365 hw_loss 0.218560 lr 0.00036391 rank 6
2023-02-25 02:46:49,950 DEBUG TRAIN Batch 22/5300 loss 37.675995 loss_att 38.616173 loss_ctc 49.001366 loss_rnnt 35.815636 hw_loss 0.304264 lr 0.00036396 rank 3
2023-02-25 02:48:00,660 DEBUG TRAIN Batch 22/5400 loss 12.643566 loss_att 14.969311 loss_ctc 16.520760 loss_rnnt 11.594588 hw_loss 0.125382 lr 0.00036382 rank 1
2023-02-25 02:48:00,660 DEBUG TRAIN Batch 22/5400 loss 5.613446 loss_att 7.393389 loss_ctc 6.515504 loss_rnnt 5.027019 hw_loss 0.206559 lr 0.00036382 rank 7
2023-02-25 02:48:00,660 DEBUG TRAIN Batch 22/5400 loss 3.505086 loss_att 6.367227 loss_ctc 4.821194 loss_rnnt 2.635577 hw_loss 0.228000 lr 0.00036380 rank 5
2023-02-25 02:48:00,660 DEBUG TRAIN Batch 22/5400 loss 9.971223 loss_att 12.760949 loss_ctc 15.191351 loss_rnnt 8.612938 hw_loss 0.195607 lr 0.00036387 rank 3
2023-02-25 02:48:00,663 DEBUG TRAIN Batch 22/5400 loss 5.192718 loss_att 7.894227 loss_ctc 10.312473 loss_rnnt 3.821275 hw_loss 0.278449 lr 0.00036381 rank 6
2023-02-25 02:48:00,663 DEBUG TRAIN Batch 22/5400 loss 9.694485 loss_att 12.133787 loss_ctc 16.707876 loss_rnnt 8.185949 hw_loss 0.160417 lr 0.00036385 rank 4
2023-02-25 02:48:00,666 DEBUG TRAIN Batch 22/5400 loss 8.398538 loss_att 11.095337 loss_ctc 11.489271 loss_rnnt 7.348954 hw_loss 0.183986 lr 0.00036383 rank 0
2023-02-25 02:48:00,671 DEBUG TRAIN Batch 22/5400 loss 8.639154 loss_att 10.614184 loss_ctc 11.219623 loss_rnnt 7.767825 hw_loss 0.247988 lr 0.00036381 rank 2
2023-02-25 02:49:10,842 DEBUG TRAIN Batch 22/5500 loss 13.483031 loss_att 14.486778 loss_ctc 19.359032 loss_rnnt 12.412066 hw_loss 0.162653 lr 0.00036373 rank 0
2023-02-25 02:49:10,843 DEBUG TRAIN Batch 22/5500 loss 11.784313 loss_att 13.802079 loss_ctc 22.317883 loss_rnnt 9.745037 hw_loss 0.433588 lr 0.00036372 rank 6
2023-02-25 02:49:10,847 DEBUG TRAIN Batch 22/5500 loss 9.714775 loss_att 10.819908 loss_ctc 16.436005 loss_rnnt 8.484496 hw_loss 0.212040 lr 0.00036375 rank 4
2023-02-25 02:49:10,846 DEBUG TRAIN Batch 22/5500 loss 4.707484 loss_att 7.653088 loss_ctc 7.796372 loss_rnnt 3.638075 hw_loss 0.128319 lr 0.00036371 rank 2
2023-02-25 02:49:10,846 DEBUG TRAIN Batch 22/5500 loss 11.658839 loss_att 12.304228 loss_ctc 14.559828 loss_rnnt 10.982386 hw_loss 0.301080 lr 0.00036372 rank 1
2023-02-25 02:49:10,848 DEBUG TRAIN Batch 22/5500 loss 8.128867 loss_att 10.091866 loss_ctc 8.821926 loss_rnnt 7.554542 hw_loss 0.167473 lr 0.00036377 rank 3
2023-02-25 02:49:10,850 DEBUG TRAIN Batch 22/5500 loss 4.739305 loss_att 6.008007 loss_ctc 6.667175 loss_rnnt 4.117211 hw_loss 0.208695 lr 0.00036372 rank 7
2023-02-25 02:49:10,851 DEBUG TRAIN Batch 22/5500 loss 8.577868 loss_att 10.021685 loss_ctc 12.789303 loss_rnnt 7.608530 hw_loss 0.223216 lr 0.00036370 rank 5
2023-02-25 02:50:21,729 DEBUG TRAIN Batch 22/5600 loss 7.847800 loss_att 9.921295 loss_ctc 9.965528 loss_rnnt 7.017751 hw_loss 0.249349 lr 0.00036364 rank 0
2023-02-25 02:50:21,735 DEBUG TRAIN Batch 22/5600 loss 5.720252 loss_att 7.333291 loss_ctc 9.130262 loss_rnnt 4.836202 hw_loss 0.200203 lr 0.00036363 rank 1
2023-02-25 02:50:21,737 DEBUG TRAIN Batch 22/5600 loss 20.123482 loss_att 21.272135 loss_ctc 29.865538 loss_rnnt 18.467466 hw_loss 0.238772 lr 0.00036365 rank 4
2023-02-25 02:50:21,738 DEBUG TRAIN Batch 22/5600 loss 1.587208 loss_att 4.307743 loss_ctc 1.792875 loss_rnnt 0.903774 hw_loss 0.209820 lr 0.00036361 rank 2
2023-02-25 02:50:21,741 DEBUG TRAIN Batch 22/5600 loss 4.501345 loss_att 6.572148 loss_ctc 6.118270 loss_rnnt 3.769624 hw_loss 0.191196 lr 0.00036363 rank 7
2023-02-25 02:50:21,740 DEBUG TRAIN Batch 22/5600 loss 26.877071 loss_att 29.791220 loss_ctc 40.695019 loss_rnnt 24.287916 hw_loss 0.307376 lr 0.00036362 rank 6
2023-02-25 02:50:21,755 DEBUG TRAIN Batch 22/5600 loss 4.536069 loss_att 6.727923 loss_ctc 7.466159 loss_rnnt 3.639167 hw_loss 0.127225 lr 0.00036361 rank 5
2023-02-25 02:50:21,757 DEBUG TRAIN Batch 22/5600 loss 14.602890 loss_att 18.168699 loss_ctc 17.678612 loss_rnnt 13.381671 hw_loss 0.183677 lr 0.00036368 rank 3
2023-02-25 02:51:35,459 DEBUG TRAIN Batch 22/5700 loss 9.819070 loss_att 11.188277 loss_ctc 15.179759 loss_rnnt 8.625940 hw_loss 0.383495 lr 0.00036353 rank 7
2023-02-25 02:51:35,459 DEBUG TRAIN Batch 22/5700 loss 5.653759 loss_att 7.994702 loss_ctc 9.566547 loss_rnnt 4.541078 hw_loss 0.230227 lr 0.00036358 rank 3
2023-02-25 02:51:35,462 DEBUG TRAIN Batch 22/5700 loss 13.069970 loss_att 14.849158 loss_ctc 17.656464 loss_rnnt 11.956087 hw_loss 0.274710 lr 0.00036353 rank 1
2023-02-25 02:51:35,462 DEBUG TRAIN Batch 22/5700 loss 6.652893 loss_att 7.299556 loss_ctc 6.744444 loss_rnnt 6.393239 hw_loss 0.221466 lr 0.00036354 rank 0
2023-02-25 02:51:35,463 DEBUG TRAIN Batch 22/5700 loss 13.413584 loss_att 15.385757 loss_ctc 18.874063 loss_rnnt 12.194913 hw_loss 0.180320 lr 0.00036351 rank 5
2023-02-25 02:51:35,463 DEBUG TRAIN Batch 22/5700 loss 5.964372 loss_att 6.549724 loss_ctc 6.916336 loss_rnnt 5.610118 hw_loss 0.206730 lr 0.00036353 rank 6
2023-02-25 02:51:35,467 DEBUG TRAIN Batch 22/5700 loss 10.915889 loss_att 12.716087 loss_ctc 14.108141 loss_rnnt 9.958348 hw_loss 0.322250 lr 0.00036352 rank 2
2023-02-25 02:51:35,478 DEBUG TRAIN Batch 22/5700 loss 21.378950 loss_att 26.976156 loss_ctc 28.569679 loss_rnnt 19.190544 hw_loss 0.206626 lr 0.00036356 rank 4
2023-02-25 02:52:46,632 DEBUG TRAIN Batch 22/5800 loss 20.099735 loss_att 20.111225 loss_ctc 34.869514 loss_rnnt 18.002445 hw_loss 0.235668 lr 0.00036348 rank 3
2023-02-25 02:52:46,634 DEBUG TRAIN Batch 22/5800 loss 5.898494 loss_att 8.315145 loss_ctc 7.133491 loss_rnnt 5.114725 hw_loss 0.254574 lr 0.00036345 rank 0
2023-02-25 02:52:46,642 DEBUG TRAIN Batch 22/5800 loss 9.576511 loss_att 13.256107 loss_ctc 19.153835 loss_rnnt 7.455370 hw_loss 0.202962 lr 0.00036343 rank 6
2023-02-25 02:52:46,643 DEBUG TRAIN Batch 22/5800 loss 8.464221 loss_att 13.606115 loss_ctc 12.923332 loss_rnnt 6.735726 hw_loss 0.197938 lr 0.00036344 rank 7
2023-02-25 02:52:46,647 DEBUG TRAIN Batch 22/5800 loss 7.984008 loss_att 10.989885 loss_ctc 11.293615 loss_rnnt 6.810255 hw_loss 0.246182 lr 0.00036343 rank 1
2023-02-25 02:52:46,649 DEBUG TRAIN Batch 22/5800 loss 15.592222 loss_att 21.676819 loss_ctc 22.139030 loss_rnnt 13.391672 hw_loss 0.207603 lr 0.00036342 rank 2
2023-02-25 02:52:46,659 DEBUG TRAIN Batch 22/5800 loss 8.659563 loss_att 9.855357 loss_ctc 7.790439 loss_rnnt 8.423439 hw_loss 0.211591 lr 0.00036342 rank 5
2023-02-25 02:52:46,667 DEBUG TRAIN Batch 22/5800 loss 6.544282 loss_att 10.306104 loss_ctc 12.360357 loss_rnnt 4.927586 hw_loss 0.166604 lr 0.00036346 rank 4
2023-02-25 02:53:58,186 DEBUG TRAIN Batch 22/5900 loss 10.683512 loss_att 13.863911 loss_ctc 15.939524 loss_rnnt 9.215907 hw_loss 0.245106 lr 0.00036335 rank 0
2023-02-25 02:53:58,189 DEBUG TRAIN Batch 22/5900 loss 12.260756 loss_att 17.977592 loss_ctc 16.967644 loss_rnnt 10.402735 hw_loss 0.163253 lr 0.00036334 rank 1
2023-02-25 02:53:58,189 DEBUG TRAIN Batch 22/5900 loss 13.901261 loss_att 17.624302 loss_ctc 16.060791 loss_rnnt 12.757564 hw_loss 0.208412 lr 0.00036333 rank 6
2023-02-25 02:53:58,191 DEBUG TRAIN Batch 22/5900 loss 4.874887 loss_att 12.265144 loss_ctc 7.991416 loss_rnnt 2.891217 hw_loss 0.168901 lr 0.00036339 rank 3
2023-02-25 02:53:58,191 DEBUG TRAIN Batch 22/5900 loss 8.851943 loss_att 13.833846 loss_ctc 13.128852 loss_rnnt 7.133703 hw_loss 0.284260 lr 0.00036334 rank 7
2023-02-25 02:53:58,194 DEBUG TRAIN Batch 22/5900 loss 4.766263 loss_att 11.515188 loss_ctc 6.775005 loss_rnnt 3.009009 hw_loss 0.261819 lr 0.00036332 rank 5
2023-02-25 02:53:58,196 DEBUG TRAIN Batch 22/5900 loss 13.136140 loss_att 14.864662 loss_ctc 15.775032 loss_rnnt 12.372583 hw_loss 0.123750 lr 0.00036336 rank 4
2023-02-25 02:53:58,198 DEBUG TRAIN Batch 22/5900 loss 12.564590 loss_att 15.715101 loss_ctc 20.917717 loss_rnnt 10.754931 hw_loss 0.123384 lr 0.00036333 rank 2
2023-02-25 02:55:10,368 DEBUG TRAIN Batch 22/6000 loss 3.917807 loss_att 6.088387 loss_ctc 3.931023 loss_rnnt 3.396696 hw_loss 0.159811 lr 0.00036329 rank 3
2023-02-25 02:55:10,377 DEBUG TRAIN Batch 22/6000 loss 19.560877 loss_att 24.376644 loss_ctc 26.877441 loss_rnnt 17.532005 hw_loss 0.169082 lr 0.00036325 rank 0
2023-02-25 02:55:10,382 DEBUG TRAIN Batch 22/6000 loss 7.419610 loss_att 9.832881 loss_ctc 13.842392 loss_rnnt 5.995728 hw_loss 0.159106 lr 0.00036324 rank 6
2023-02-25 02:55:10,383 DEBUG TRAIN Batch 22/6000 loss 13.008705 loss_att 17.995403 loss_ctc 19.207188 loss_rnnt 11.134192 hw_loss 0.095082 lr 0.00036327 rank 4
2023-02-25 02:55:10,384 DEBUG TRAIN Batch 22/6000 loss 10.597741 loss_att 15.783625 loss_ctc 17.244040 loss_rnnt 8.541637 hw_loss 0.248912 lr 0.00036323 rank 2
2023-02-25 02:55:10,385 DEBUG TRAIN Batch 22/6000 loss 10.522846 loss_att 15.119076 loss_ctc 16.020554 loss_rnnt 8.768891 hw_loss 0.190652 lr 0.00036324 rank 1
2023-02-25 02:55:10,390 DEBUG TRAIN Batch 22/6000 loss 9.573462 loss_att 13.552883 loss_ctc 15.209510 loss_rnnt 7.891934 hw_loss 0.251566 lr 0.00036324 rank 7
2023-02-25 02:55:10,409 DEBUG TRAIN Batch 22/6000 loss 5.350899 loss_att 8.195433 loss_ctc 7.641558 loss_rnnt 4.425420 hw_loss 0.095909 lr 0.00036322 rank 5
2023-02-25 02:56:22,929 DEBUG TRAIN Batch 22/6100 loss 6.132073 loss_att 9.137553 loss_ctc 9.862303 loss_rnnt 4.951943 hw_loss 0.153131 lr 0.00036316 rank 0
2023-02-25 02:56:22,932 DEBUG TRAIN Batch 22/6100 loss 5.514607 loss_att 9.550565 loss_ctc 8.159440 loss_rnnt 4.254200 hw_loss 0.188571 lr 0.00036317 rank 4
2023-02-25 02:56:22,933 DEBUG TRAIN Batch 22/6100 loss 18.324549 loss_att 21.870121 loss_ctc 24.399174 loss_rnnt 16.650127 hw_loss 0.291294 lr 0.00036320 rank 3
2023-02-25 02:56:22,935 DEBUG TRAIN Batch 22/6100 loss 3.239242 loss_att 5.784255 loss_ctc 6.254377 loss_rnnt 2.215956 hw_loss 0.210497 lr 0.00036314 rank 6
2023-02-25 02:56:22,935 DEBUG TRAIN Batch 22/6100 loss 10.970904 loss_att 12.627798 loss_ctc 14.800162 loss_rnnt 10.063450 hw_loss 0.122829 lr 0.00036315 rank 1
2023-02-25 02:56:22,938 DEBUG TRAIN Batch 22/6100 loss 6.573522 loss_att 8.715714 loss_ctc 9.505490 loss_rnnt 5.656153 hw_loss 0.183753 lr 0.00036313 rank 2
2023-02-25 02:56:22,950 DEBUG TRAIN Batch 22/6100 loss 11.087057 loss_att 15.812718 loss_ctc 18.224396 loss_rnnt 9.084039 hw_loss 0.199202 lr 0.00036315 rank 7
2023-02-25 02:56:22,980 DEBUG TRAIN Batch 22/6100 loss 12.673788 loss_att 13.763208 loss_ctc 20.279882 loss_rnnt 11.350149 hw_loss 0.171766 lr 0.00036313 rank 5
2023-02-25 02:57:34,405 DEBUG TRAIN Batch 22/6200 loss 13.845624 loss_att 15.804977 loss_ctc 19.211103 loss_rnnt 12.668979 hw_loss 0.130084 lr 0.00036308 rank 4
2023-02-25 02:57:34,415 DEBUG TRAIN Batch 22/6200 loss 8.417598 loss_att 11.513000 loss_ctc 14.035936 loss_rnnt 6.926997 hw_loss 0.229517 lr 0.00036305 rank 1
2023-02-25 02:57:34,415 DEBUG TRAIN Batch 22/6200 loss 13.334372 loss_att 14.442063 loss_ctc 24.011139 loss_rnnt 11.567139 hw_loss 0.228988 lr 0.00036306 rank 0
2023-02-25 02:57:34,415 DEBUG TRAIN Batch 22/6200 loss 13.147456 loss_att 14.925285 loss_ctc 14.703060 loss_rnnt 12.439963 hw_loss 0.270961 lr 0.00036310 rank 3
2023-02-25 02:57:34,419 DEBUG TRAIN Batch 22/6200 loss 11.398163 loss_att 11.758419 loss_ctc 15.207644 loss_rnnt 10.744902 hw_loss 0.137400 lr 0.00036304 rank 2
2023-02-25 02:57:34,421 DEBUG TRAIN Batch 22/6200 loss 8.268941 loss_att 12.540063 loss_ctc 9.828818 loss_rnnt 7.028041 hw_loss 0.335046 lr 0.00036305 rank 7
2023-02-25 02:57:34,422 DEBUG TRAIN Batch 22/6200 loss 12.535564 loss_att 17.507374 loss_ctc 16.608295 loss_rnnt 10.851771 hw_loss 0.274501 lr 0.00036305 rank 6
2023-02-25 02:57:34,422 DEBUG TRAIN Batch 22/6200 loss 9.579823 loss_att 9.609518 loss_ctc 15.679350 loss_rnnt 8.621212 hw_loss 0.261380 lr 0.00036303 rank 5
2023-02-25 02:58:44,966 DEBUG TRAIN Batch 22/6300 loss 8.059016 loss_att 9.829559 loss_ctc 10.650867 loss_rnnt 7.246494 hw_loss 0.211564 lr 0.00036297 rank 0
2023-02-25 02:58:44,973 DEBUG TRAIN Batch 22/6300 loss 10.586701 loss_att 12.304605 loss_ctc 15.886146 loss_rnnt 9.442199 hw_loss 0.176868 lr 0.00036295 rank 6
2023-02-25 02:58:44,975 DEBUG TRAIN Batch 22/6300 loss 5.259240 loss_att 7.216391 loss_ctc 7.282889 loss_rnnt 4.512986 hw_loss 0.159382 lr 0.00036296 rank 7
2023-02-25 02:58:44,976 DEBUG TRAIN Batch 22/6300 loss 6.498777 loss_att 8.062457 loss_ctc 8.590976 loss_rnnt 5.717317 hw_loss 0.355808 lr 0.00036298 rank 4
2023-02-25 02:58:44,976 DEBUG TRAIN Batch 22/6300 loss 4.192153 loss_att 5.673986 loss_ctc 6.438544 loss_rnnt 3.497653 hw_loss 0.184902 lr 0.00036294 rank 5
2023-02-25 02:58:44,979 DEBUG TRAIN Batch 22/6300 loss 9.072969 loss_att 10.162622 loss_ctc 11.329839 loss_rnnt 8.406714 hw_loss 0.276391 lr 0.00036295 rank 1
2023-02-25 02:58:44,983 DEBUG TRAIN Batch 22/6300 loss 2.322633 loss_att 5.220160 loss_ctc 2.802068 loss_rnnt 1.549358 hw_loss 0.243459 lr 0.00036294 rank 2
2023-02-25 02:58:44,982 DEBUG TRAIN Batch 22/6300 loss 5.345149 loss_att 7.826383 loss_ctc 8.864269 loss_rnnt 4.252118 hw_loss 0.239192 lr 0.00036300 rank 3
2023-02-25 02:59:58,525 DEBUG TRAIN Batch 22/6400 loss 4.505790 loss_att 4.954334 loss_ctc 7.228834 loss_rnnt 3.912915 hw_loss 0.262675 lr 0.00036291 rank 3
2023-02-25 02:59:58,533 DEBUG TRAIN Batch 22/6400 loss 3.345568 loss_att 4.968619 loss_ctc 7.717827 loss_rnnt 2.345633 hw_loss 0.173170 lr 0.00036284 rank 5
2023-02-25 02:59:58,532 DEBUG TRAIN Batch 22/6400 loss 2.470435 loss_att 5.248620 loss_ctc 2.517114 loss_rnnt 1.850175 hw_loss 0.109499 lr 0.00036289 rank 4
2023-02-25 02:59:58,533 DEBUG TRAIN Batch 22/6400 loss 9.710748 loss_att 10.274682 loss_ctc 12.320906 loss_rnnt 9.077068 hw_loss 0.324131 lr 0.00036286 rank 1
2023-02-25 02:59:58,533 DEBUG TRAIN Batch 22/6400 loss 7.959246 loss_att 8.855585 loss_ctc 11.391825 loss_rnnt 7.217543 hw_loss 0.196421 lr 0.00036285 rank 2
2023-02-25 02:59:58,558 DEBUG TRAIN Batch 22/6400 loss 5.367844 loss_att 10.328375 loss_ctc 8.057069 loss_rnnt 3.921337 hw_loss 0.179695 lr 0.00036287 rank 0
2023-02-25 02:59:58,559 DEBUG TRAIN Batch 22/6400 loss 2.285673 loss_att 5.312034 loss_ctc 5.105245 loss_rnnt 1.189244 hw_loss 0.216026 lr 0.00036285 rank 6
2023-02-25 02:59:58,585 DEBUG TRAIN Batch 22/6400 loss 9.312572 loss_att 12.898021 loss_ctc 13.574992 loss_rnnt 7.956089 hw_loss 0.133258 lr 0.00036286 rank 7
2023-02-25 03:01:09,408 DEBUG TRAIN Batch 22/6500 loss 13.829499 loss_att 16.216875 loss_ctc 17.146591 loss_rnnt 12.832670 hw_loss 0.144516 lr 0.00036278 rank 0
2023-02-25 03:01:09,409 DEBUG TRAIN Batch 22/6500 loss 12.641536 loss_att 14.764320 loss_ctc 18.305962 loss_rnnt 11.324225 hw_loss 0.257807 lr 0.00036279 rank 4
2023-02-25 03:01:09,411 DEBUG TRAIN Batch 22/6500 loss 5.191648 loss_att 7.451108 loss_ctc 6.691613 loss_rnnt 4.478433 hw_loss 0.114991 lr 0.00036276 rank 1
2023-02-25 03:01:09,411 DEBUG TRAIN Batch 22/6500 loss 5.831788 loss_att 8.036953 loss_ctc 6.229312 loss_rnnt 5.227712 hw_loss 0.206323 lr 0.00036281 rank 3
2023-02-25 03:01:09,414 DEBUG TRAIN Batch 22/6500 loss 3.377867 loss_att 5.319285 loss_ctc 4.313059 loss_rnnt 2.748951 hw_loss 0.217388 lr 0.00036275 rank 2
2023-02-25 03:01:09,415 DEBUG TRAIN Batch 22/6500 loss 8.380667 loss_att 12.101025 loss_ctc 14.129820 loss_rnnt 6.794796 hw_loss 0.141084 lr 0.00036276 rank 7
2023-02-25 03:01:09,418 DEBUG TRAIN Batch 22/6500 loss 7.731116 loss_att 11.454529 loss_ctc 9.941592 loss_rnnt 6.609998 hw_loss 0.153199 lr 0.00036275 rank 5
2023-02-25 03:01:09,466 DEBUG TRAIN Batch 22/6500 loss 7.808528 loss_att 11.141582 loss_ctc 13.057665 loss_rnnt 6.315796 hw_loss 0.236694 lr 0.00036276 rank 6
2023-02-25 03:02:20,349 DEBUG TRAIN Batch 22/6600 loss 13.378158 loss_att 15.567689 loss_ctc 24.356823 loss_rnnt 11.385216 hw_loss 0.171024 lr 0.00036268 rank 0
2023-02-25 03:02:20,350 DEBUG TRAIN Batch 22/6600 loss 4.818934 loss_att 5.718702 loss_ctc 4.641178 loss_rnnt 4.532388 hw_loss 0.244300 lr 0.00036267 rank 1
2023-02-25 03:02:20,350 DEBUG TRAIN Batch 22/6600 loss 6.343265 loss_att 11.590149 loss_ctc 14.129200 loss_rnnt 4.136888 hw_loss 0.222892 lr 0.00036272 rank 3
2023-02-25 03:02:20,352 DEBUG TRAIN Batch 22/6600 loss 15.976753 loss_att 17.883747 loss_ctc 20.149033 loss_rnnt 14.943206 hw_loss 0.179710 lr 0.00036266 rank 6
2023-02-25 03:02:20,353 DEBUG TRAIN Batch 22/6600 loss 17.229246 loss_att 21.136145 loss_ctc 20.479780 loss_rnnt 15.917673 hw_loss 0.181473 lr 0.00036267 rank 7
2023-02-25 03:02:20,360 DEBUG TRAIN Batch 22/6600 loss 15.468179 loss_att 17.868155 loss_ctc 20.760054 loss_rnnt 14.204238 hw_loss 0.146931 lr 0.00036266 rank 2
2023-02-25 03:02:20,363 DEBUG TRAIN Batch 22/6600 loss 8.421547 loss_att 9.970043 loss_ctc 13.613170 loss_rnnt 7.292283 hw_loss 0.238776 lr 0.00036270 rank 4
2023-02-25 03:02:20,403 DEBUG TRAIN Batch 22/6600 loss 8.967237 loss_att 10.443006 loss_ctc 9.891366 loss_rnnt 8.503702 hw_loss 0.084685 lr 0.00036265 rank 5
2023-02-25 03:03:32,611 DEBUG TRAIN Batch 22/6700 loss 9.177138 loss_att 11.116520 loss_ctc 14.943161 loss_rnnt 7.944400 hw_loss 0.142610 lr 0.00036257 rank 6
2023-02-25 03:03:32,618 DEBUG TRAIN Batch 22/6700 loss 9.844463 loss_att 13.772941 loss_ctc 16.974613 loss_rnnt 8.011707 hw_loss 0.180702 lr 0.00036257 rank 1
2023-02-25 03:03:32,620 DEBUG TRAIN Batch 22/6700 loss 14.817890 loss_att 19.259335 loss_ctc 25.733562 loss_rnnt 12.392417 hw_loss 0.153302 lr 0.00036259 rank 0
2023-02-25 03:03:32,621 DEBUG TRAIN Batch 22/6700 loss 12.977885 loss_att 14.764892 loss_ctc 18.095848 loss_rnnt 11.855169 hw_loss 0.155475 lr 0.00036255 rank 5
2023-02-25 03:03:32,622 DEBUG TRAIN Batch 22/6700 loss 6.053832 loss_att 8.584040 loss_ctc 8.511028 loss_rnnt 5.090972 hw_loss 0.242236 lr 0.00036256 rank 2
2023-02-25 03:03:32,625 DEBUG TRAIN Batch 22/6700 loss 4.192540 loss_att 6.669695 loss_ctc 5.661339 loss_rnnt 3.393707 hw_loss 0.201678 lr 0.00036262 rank 3
2023-02-25 03:03:32,627 DEBUG TRAIN Batch 22/6700 loss 11.455171 loss_att 14.475018 loss_ctc 16.229738 loss_rnnt 10.101667 hw_loss 0.211733 lr 0.00036257 rank 7
2023-02-25 03:03:32,645 DEBUG TRAIN Batch 22/6700 loss 5.519952 loss_att 8.745214 loss_ctc 7.245019 loss_rnnt 4.526178 hw_loss 0.222587 lr 0.00036260 rank 4
2023-02-25 03:04:45,559 DEBUG TRAIN Batch 22/6800 loss 6.752472 loss_att 8.778265 loss_ctc 9.856352 loss_rnnt 5.779860 hw_loss 0.288005 lr 0.00036248 rank 1
2023-02-25 03:04:45,559 DEBUG TRAIN Batch 22/6800 loss 18.026676 loss_att 22.550915 loss_ctc 24.125416 loss_rnnt 16.199680 hw_loss 0.204343 lr 0.00036249 rank 0
2023-02-25 03:04:45,562 DEBUG TRAIN Batch 22/6800 loss 18.322004 loss_att 19.979839 loss_ctc 25.137505 loss_rnnt 16.948414 hw_loss 0.249914 lr 0.00036253 rank 3
2023-02-25 03:04:45,564 DEBUG TRAIN Batch 22/6800 loss 4.340889 loss_att 7.522125 loss_ctc 7.629945 loss_rnnt 3.151654 hw_loss 0.214588 lr 0.00036248 rank 7
2023-02-25 03:04:45,563 DEBUG TRAIN Batch 22/6800 loss 15.170712 loss_att 19.425976 loss_ctc 22.730930 loss_rnnt 13.189241 hw_loss 0.229476 lr 0.00036247 rank 2
2023-02-25 03:04:45,567 DEBUG TRAIN Batch 22/6800 loss 7.279176 loss_att 10.284109 loss_ctc 10.417669 loss_rnnt 6.172309 hw_loss 0.163901 lr 0.00036250 rank 4
2023-02-25 03:04:45,567 DEBUG TRAIN Batch 22/6800 loss 6.231218 loss_att 9.665194 loss_ctc 10.794468 loss_rnnt 4.793970 hw_loss 0.266289 lr 0.00036247 rank 6
2023-02-25 03:04:45,570 DEBUG TRAIN Batch 22/6800 loss 4.301739 loss_att 6.502949 loss_ctc 6.941590 loss_rnnt 3.409782 hw_loss 0.187002 lr 0.00036246 rank 5
2023-02-25 03:05:56,692 DEBUG TRAIN Batch 22/6900 loss 7.706596 loss_att 10.272153 loss_ctc 10.827973 loss_rnnt 6.694973 hw_loss 0.154365 lr 0.00036239 rank 0
2023-02-25 03:05:56,694 DEBUG TRAIN Batch 22/6900 loss 12.671801 loss_att 15.509722 loss_ctc 20.227051 loss_rnnt 10.989551 hw_loss 0.201186 lr 0.00036237 rank 2
2023-02-25 03:05:56,695 DEBUG TRAIN Batch 22/6900 loss 14.723621 loss_att 15.320253 loss_ctc 17.313763 loss_rnnt 14.101897 hw_loss 0.294462 lr 0.00036238 rank 1
2023-02-25 03:05:56,698 DEBUG TRAIN Batch 22/6900 loss 3.645663 loss_att 6.385793 loss_ctc 5.419160 loss_rnnt 2.716598 hw_loss 0.271074 lr 0.00036243 rank 3
2023-02-25 03:05:56,698 DEBUG TRAIN Batch 22/6900 loss 6.461051 loss_att 7.950222 loss_ctc 9.752294 loss_rnnt 5.636880 hw_loss 0.164071 lr 0.00036238 rank 6
2023-02-25 03:05:56,700 DEBUG TRAIN Batch 22/6900 loss 5.924451 loss_att 8.156296 loss_ctc 6.632410 loss_rnnt 5.254069 hw_loss 0.243034 lr 0.00036241 rank 4
2023-02-25 03:05:56,700 DEBUG TRAIN Batch 22/6900 loss 8.622034 loss_att 12.212957 loss_ctc 12.752658 loss_rnnt 7.224859 hw_loss 0.240453 lr 0.00036238 rank 7
2023-02-25 03:05:56,704 DEBUG TRAIN Batch 22/6900 loss 6.971719 loss_att 9.334678 loss_ctc 10.480461 loss_rnnt 5.922511 hw_loss 0.203970 lr 0.00036236 rank 5
2023-02-25 03:07:07,297 DEBUG TRAIN Batch 22/7000 loss 8.101724 loss_att 8.547529 loss_ctc 12.738281 loss_rnnt 7.178582 hw_loss 0.404573 lr 0.00036234 rank 3
2023-02-25 03:07:07,298 DEBUG TRAIN Batch 22/7000 loss 7.674104 loss_att 9.199955 loss_ctc 10.895035 loss_rnnt 6.824575 hw_loss 0.215441 lr 0.00036230 rank 0
2023-02-25 03:07:07,303 DEBUG TRAIN Batch 22/7000 loss 9.431400 loss_att 13.590995 loss_ctc 14.683266 loss_rnnt 7.725827 hw_loss 0.325134 lr 0.00036228 rank 2
2023-02-25 03:07:07,303 DEBUG TRAIN Batch 22/7000 loss 10.644821 loss_att 14.099370 loss_ctc 14.161125 loss_rnnt 9.380661 hw_loss 0.195768 lr 0.00036231 rank 4
2023-02-25 03:07:07,307 DEBUG TRAIN Batch 22/7000 loss 7.912086 loss_att 10.050225 loss_ctc 15.070244 loss_rnnt 6.385862 hw_loss 0.270329 lr 0.00036229 rank 1
2023-02-25 03:07:07,309 DEBUG TRAIN Batch 22/7000 loss 5.410408 loss_att 8.159070 loss_ctc 11.029058 loss_rnnt 3.987562 hw_loss 0.232425 lr 0.00036227 rank 5
2023-02-25 03:07:07,314 DEBUG TRAIN Batch 22/7000 loss 3.583693 loss_att 6.708271 loss_ctc 5.389894 loss_rnnt 2.624139 hw_loss 0.175897 lr 0.00036229 rank 7
2023-02-25 03:07:07,354 DEBUG TRAIN Batch 22/7000 loss 12.384996 loss_att 15.410300 loss_ctc 14.631370 loss_rnnt 11.375027 hw_loss 0.197610 lr 0.00036228 rank 6
2023-02-25 03:08:20,673 DEBUG TRAIN Batch 22/7100 loss 10.999484 loss_att 13.988591 loss_ctc 15.472587 loss_rnnt 9.718337 hw_loss 0.162961 lr 0.00036217 rank 5
2023-02-25 03:08:20,680 DEBUG TRAIN Batch 22/7100 loss 7.953524 loss_att 11.660906 loss_ctc 19.719746 loss_rnnt 5.498613 hw_loss 0.271136 lr 0.00036220 rank 0
2023-02-25 03:08:20,681 DEBUG TRAIN Batch 22/7100 loss 6.207729 loss_att 8.441267 loss_ctc 7.434760 loss_rnnt 5.463666 hw_loss 0.250783 lr 0.00036224 rank 3
2023-02-25 03:08:20,686 DEBUG TRAIN Batch 22/7100 loss 5.356201 loss_att 7.487270 loss_ctc 7.686724 loss_rnnt 4.506655 hw_loss 0.211116 lr 0.00036222 rank 4
2023-02-25 03:08:20,688 DEBUG TRAIN Batch 22/7100 loss 6.681017 loss_att 9.166561 loss_ctc 8.343876 loss_rnnt 5.835256 hw_loss 0.238008 lr 0.00036219 rank 7
2023-02-25 03:08:20,687 DEBUG TRAIN Batch 22/7100 loss 7.900849 loss_att 10.357802 loss_ctc 10.364798 loss_rnnt 7.021182 hw_loss 0.112031 lr 0.00036219 rank 6
2023-02-25 03:08:20,689 DEBUG TRAIN Batch 22/7100 loss 4.858777 loss_att 8.977809 loss_ctc 9.995550 loss_rnnt 3.258500 hw_loss 0.171690 lr 0.00036219 rank 1
2023-02-25 03:08:20,691 DEBUG TRAIN Batch 22/7100 loss 5.543915 loss_att 6.906902 loss_ctc 7.622602 loss_rnnt 4.928941 hw_loss 0.122282 lr 0.00036218 rank 2
2023-02-25 03:09:32,976 DEBUG TRAIN Batch 22/7200 loss 10.499632 loss_att 13.499631 loss_ctc 10.111803 loss_rnnt 9.858240 hw_loss 0.174566 lr 0.00036211 rank 0
2023-02-25 03:09:32,984 DEBUG TRAIN Batch 22/7200 loss 10.997241 loss_att 12.920504 loss_ctc 14.722023 loss_rnnt 10.037130 hw_loss 0.147787 lr 0.00036210 rank 1
2023-02-25 03:09:32,985 DEBUG TRAIN Batch 22/7200 loss 10.721918 loss_att 14.398464 loss_ctc 12.818817 loss_rnnt 9.541550 hw_loss 0.310261 lr 0.00036212 rank 4
2023-02-25 03:09:32,985 DEBUG TRAIN Batch 22/7200 loss 6.323296 loss_att 8.486785 loss_ctc 5.999091 loss_rnnt 5.820202 hw_loss 0.213043 lr 0.00036209 rank 6
2023-02-25 03:09:32,987 DEBUG TRAIN Batch 22/7200 loss 7.737076 loss_att 9.177421 loss_ctc 8.822374 loss_rnnt 7.209248 hw_loss 0.178223 lr 0.00036210 rank 7
2023-02-25 03:09:32,989 DEBUG TRAIN Batch 22/7200 loss 3.593803 loss_att 6.041843 loss_ctc 4.439839 loss_rnnt 2.893806 hw_loss 0.182972 lr 0.00036209 rank 2
2023-02-25 03:09:33,021 DEBUG TRAIN Batch 22/7200 loss 9.809399 loss_att 10.974339 loss_ctc 12.043540 loss_rnnt 9.183655 hw_loss 0.177882 lr 0.00036208 rank 5
2023-02-25 03:09:33,028 DEBUG TRAIN Batch 22/7200 loss 15.146474 loss_att 20.408125 loss_ctc 30.436275 loss_rnnt 11.957561 hw_loss 0.183641 lr 0.00036215 rank 3
2023-02-25 03:10:43,644 DEBUG TRAIN Batch 22/7300 loss 9.686357 loss_att 12.324607 loss_ctc 10.567459 loss_rnnt 8.934932 hw_loss 0.199305 lr 0.00036203 rank 4
2023-02-25 03:10:43,661 DEBUG TRAIN Batch 22/7300 loss 12.900189 loss_att 14.181925 loss_ctc 17.367748 loss_rnnt 11.948057 hw_loss 0.187710 lr 0.00036200 rank 6
2023-02-25 03:10:43,667 DEBUG TRAIN Batch 22/7300 loss 6.450455 loss_att 8.785275 loss_ctc 8.801635 loss_rnnt 5.582836 hw_loss 0.163431 lr 0.00036201 rank 0
2023-02-25 03:10:43,670 DEBUG TRAIN Batch 22/7300 loss 4.429591 loss_att 8.061790 loss_ctc 8.900272 loss_rnnt 3.010336 hw_loss 0.181358 lr 0.00036200 rank 1
2023-02-25 03:10:43,671 DEBUG TRAIN Batch 22/7300 loss 8.766516 loss_att 12.374340 loss_ctc 12.999147 loss_rnnt 7.394059 hw_loss 0.162264 lr 0.00036205 rank 3
2023-02-25 03:10:43,671 DEBUG TRAIN Batch 22/7300 loss 6.385884 loss_att 8.692120 loss_ctc 10.005969 loss_rnnt 5.311002 hw_loss 0.245544 lr 0.00036199 rank 2
2023-02-25 03:10:43,671 DEBUG TRAIN Batch 22/7300 loss 11.606389 loss_att 17.705761 loss_ctc 14.916805 loss_rnnt 9.844905 hw_loss 0.187914 lr 0.00036198 rank 5
2023-02-25 03:10:43,675 DEBUG TRAIN Batch 22/7300 loss 9.616040 loss_att 11.148608 loss_ctc 13.306652 loss_rnnt 8.711649 hw_loss 0.198368 lr 0.00036200 rank 7
2023-02-25 03:11:54,767 DEBUG TRAIN Batch 22/7400 loss 15.526433 loss_att 16.021427 loss_ctc 26.759695 loss_rnnt 13.840494 hw_loss 0.167193 lr 0.00036196 rank 3
2023-02-25 03:11:54,772 DEBUG TRAIN Batch 22/7400 loss 12.615923 loss_att 15.007540 loss_ctc 16.458902 loss_rnnt 11.539701 hw_loss 0.160315 lr 0.00036190 rank 6
2023-02-25 03:11:54,787 DEBUG TRAIN Batch 22/7400 loss 7.184130 loss_att 13.030862 loss_ctc 9.885211 loss_rnnt 5.491892 hw_loss 0.305153 lr 0.00036191 rank 1
2023-02-25 03:11:54,787 DEBUG TRAIN Batch 22/7400 loss 3.502096 loss_att 6.795407 loss_ctc 4.484311 loss_rnnt 2.624950 hw_loss 0.164104 lr 0.00036189 rank 5
2023-02-25 03:11:54,790 DEBUG TRAIN Batch 22/7400 loss 4.111440 loss_att 7.422423 loss_ctc 6.196861 loss_rnnt 3.081578 hw_loss 0.168017 lr 0.00036190 rank 2
2023-02-25 03:11:54,789 DEBUG TRAIN Batch 22/7400 loss 9.960354 loss_att 13.218533 loss_ctc 14.259146 loss_rnnt 8.616962 hw_loss 0.222344 lr 0.00036191 rank 7
2023-02-25 03:11:54,810 DEBUG TRAIN Batch 22/7400 loss 13.630110 loss_att 15.160421 loss_ctc 16.455414 loss_rnnt 12.775579 hw_loss 0.322053 lr 0.00036192 rank 0
2023-02-25 03:11:54,818 DEBUG TRAIN Batch 22/7400 loss 13.665294 loss_att 17.040642 loss_ctc 19.374828 loss_rnnt 12.154598 hw_loss 0.139416 lr 0.00036193 rank 4
2023-02-25 03:13:08,216 DEBUG TRAIN Batch 22/7500 loss 12.958878 loss_att 14.995930 loss_ctc 19.322348 loss_rnnt 11.600723 hw_loss 0.191778 lr 0.00036184 rank 4
2023-02-25 03:13:08,216 DEBUG TRAIN Batch 22/7500 loss 9.774467 loss_att 14.406236 loss_ctc 16.546261 loss_rnnt 7.841692 hw_loss 0.194092 lr 0.00036183 rank 0
2023-02-25 03:13:08,217 DEBUG TRAIN Batch 22/7500 loss 7.180953 loss_att 8.634252 loss_ctc 7.119408 loss_rnnt 6.769461 hw_loss 0.241946 lr 0.00036181 rank 6
2023-02-25 03:13:08,219 DEBUG TRAIN Batch 22/7500 loss 9.612106 loss_att 11.006425 loss_ctc 12.002616 loss_rnnt 8.905556 hw_loss 0.204286 lr 0.00036181 rank 7
2023-02-25 03:13:08,218 DEBUG TRAIN Batch 22/7500 loss 6.808000 loss_att 8.169773 loss_ctc 13.167144 loss_rnnt 5.586026 hw_loss 0.190750 lr 0.00036186 rank 3
2023-02-25 03:13:08,219 DEBUG TRAIN Batch 22/7500 loss 12.050010 loss_att 12.758533 loss_ctc 14.485033 loss_rnnt 11.469860 hw_loss 0.213328 lr 0.00036181 rank 1
2023-02-25 03:13:08,221 DEBUG TRAIN Batch 22/7500 loss 3.720035 loss_att 5.940377 loss_ctc 4.449141 loss_rnnt 3.040890 hw_loss 0.258492 lr 0.00036180 rank 2
2023-02-25 03:13:08,226 DEBUG TRAIN Batch 22/7500 loss 15.891088 loss_att 18.637318 loss_ctc 20.936895 loss_rnnt 14.514890 hw_loss 0.289081 lr 0.00036179 rank 5
2023-02-25 03:14:19,509 DEBUG TRAIN Batch 22/7600 loss 6.953851 loss_att 9.269170 loss_ctc 12.217712 loss_rnnt 5.677912 hw_loss 0.208175 lr 0.00036177 rank 3
2023-02-25 03:14:19,511 DEBUG TRAIN Batch 22/7600 loss 5.457750 loss_att 5.637033 loss_ctc 6.676641 loss_rnnt 5.135736 hw_loss 0.231823 lr 0.00036171 rank 6
2023-02-25 03:14:19,512 DEBUG TRAIN Batch 22/7600 loss 13.896263 loss_att 16.539661 loss_ctc 21.498247 loss_rnnt 12.246959 hw_loss 0.200674 lr 0.00036173 rank 0
2023-02-25 03:14:19,513 DEBUG TRAIN Batch 22/7600 loss 6.768021 loss_att 10.031326 loss_ctc 9.088263 loss_rnnt 5.724409 hw_loss 0.152973 lr 0.00036171 rank 2
2023-02-25 03:14:19,515 DEBUG TRAIN Batch 22/7600 loss 6.023253 loss_att 9.535406 loss_ctc 11.539798 loss_rnnt 4.491568 hw_loss 0.175716 lr 0.00036172 rank 1
2023-02-25 03:14:19,516 DEBUG TRAIN Batch 22/7600 loss 5.773295 loss_att 9.232869 loss_ctc 9.485972 loss_rnnt 4.520677 hw_loss 0.123150 lr 0.00036170 rank 5
2023-02-25 03:14:19,517 DEBUG TRAIN Batch 22/7600 loss 10.944290 loss_att 12.218310 loss_ctc 15.810476 loss_rnnt 9.914327 hw_loss 0.236876 lr 0.00036174 rank 4
2023-02-25 03:14:19,521 DEBUG TRAIN Batch 22/7600 loss 2.660712 loss_att 5.984996 loss_ctc 4.949432 loss_rnnt 1.558179 hw_loss 0.248464 lr 0.00036172 rank 7
2023-02-25 03:15:29,939 DEBUG TRAIN Batch 22/7700 loss 9.447580 loss_att 8.733865 loss_ctc 12.679385 loss_rnnt 8.995414 hw_loss 0.307506 lr 0.00036162 rank 1
2023-02-25 03:15:29,941 DEBUG TRAIN Batch 22/7700 loss 3.006237 loss_att 8.110569 loss_ctc 5.638487 loss_rnnt 1.525818 hw_loss 0.203600 lr 0.00036162 rank 6
2023-02-25 03:15:29,941 DEBUG TRAIN Batch 22/7700 loss 10.564737 loss_att 15.031397 loss_ctc 17.922680 loss_rnnt 8.582076 hw_loss 0.203009 lr 0.00036162 rank 7
2023-02-25 03:15:29,944 DEBUG TRAIN Batch 22/7700 loss 11.286166 loss_att 14.198121 loss_ctc 16.044384 loss_rnnt 10.002250 hw_loss 0.125807 lr 0.00036165 rank 4
2023-02-25 03:15:29,945 DEBUG TRAIN Batch 22/7700 loss 6.137105 loss_att 9.519275 loss_ctc 7.013655 loss_rnnt 5.229970 hw_loss 0.213425 lr 0.00036161 rank 5
2023-02-25 03:15:29,949 DEBUG TRAIN Batch 22/7700 loss 8.137419 loss_att 11.882617 loss_ctc 14.347298 loss_rnnt 6.428177 hw_loss 0.247910 lr 0.00036164 rank 0
2023-02-25 03:15:29,953 DEBUG TRAIN Batch 22/7700 loss 16.758041 loss_att 17.216831 loss_ctc 24.569988 loss_rnnt 15.519992 hw_loss 0.196310 lr 0.00036161 rank 2
2023-02-25 03:15:29,996 DEBUG TRAIN Batch 22/7700 loss 4.805651 loss_att 7.810928 loss_ctc 7.113939 loss_rnnt 3.824560 hw_loss 0.135495 lr 0.00036167 rank 3
2023-02-25 03:16:42,798 DEBUG TRAIN Batch 22/7800 loss 4.240548 loss_att 7.776173 loss_ctc 5.985297 loss_rnnt 3.223484 hw_loss 0.144948 lr 0.00036158 rank 3
2023-02-25 03:16:42,801 DEBUG TRAIN Batch 22/7800 loss 2.654797 loss_att 5.391257 loss_ctc 2.424404 loss_rnnt 2.018509 hw_loss 0.224464 lr 0.00036153 rank 1
2023-02-25 03:16:42,805 DEBUG TRAIN Batch 22/7800 loss 12.871709 loss_att 15.491566 loss_ctc 17.597174 loss_rnnt 11.653944 hw_loss 0.119499 lr 0.00036153 rank 7
2023-02-25 03:16:42,807 DEBUG TRAIN Batch 22/7800 loss 11.199422 loss_att 15.062083 loss_ctc 18.014626 loss_rnnt 9.436848 hw_loss 0.152528 lr 0.00036154 rank 0
2023-02-25 03:16:42,813 DEBUG TRAIN Batch 22/7800 loss 3.522080 loss_att 7.154749 loss_ctc 6.380171 loss_rnnt 2.290988 hw_loss 0.231525 lr 0.00036152 rank 6
2023-02-25 03:16:42,825 DEBUG TRAIN Batch 22/7800 loss 10.760581 loss_att 14.952330 loss_ctc 18.955921 loss_rnnt 8.704252 hw_loss 0.234875 lr 0.00036151 rank 5
2023-02-25 03:16:42,828 DEBUG TRAIN Batch 22/7800 loss 17.201883 loss_att 22.362518 loss_ctc 22.917425 loss_rnnt 15.342524 hw_loss 0.122177 lr 0.00036156 rank 4
2023-02-25 03:16:42,857 DEBUG TRAIN Batch 22/7800 loss 2.988897 loss_att 6.667150 loss_ctc 3.052691 loss_rnnt 2.158240 hw_loss 0.162189 lr 0.00036152 rank 2
2023-02-25 03:17:54,607 DEBUG TRAIN Batch 22/7900 loss 4.765621 loss_att 9.157186 loss_ctc 7.324657 loss_rnnt 3.415993 hw_loss 0.243955 lr 0.00036145 rank 0
2023-02-25 03:17:54,611 DEBUG TRAIN Batch 22/7900 loss 20.846678 loss_att 21.130184 loss_ctc 26.954193 loss_rnnt 19.879116 hw_loss 0.180988 lr 0.00036143 rank 1
2023-02-25 03:17:54,619 DEBUG TRAIN Batch 22/7900 loss 8.584704 loss_att 12.155869 loss_ctc 13.116653 loss_rnnt 7.191556 hw_loss 0.139978 lr 0.00036142 rank 5
2023-02-25 03:17:54,617 DEBUG TRAIN Batch 22/7900 loss 9.095671 loss_att 12.064832 loss_ctc 11.443948 loss_rnnt 8.043716 hw_loss 0.271910 lr 0.00036143 rank 6
2023-02-25 03:17:54,619 DEBUG TRAIN Batch 22/7900 loss 17.341131 loss_att 22.168257 loss_ctc 24.403210 loss_rnnt 15.335648 hw_loss 0.184590 lr 0.00036146 rank 4
2023-02-25 03:17:54,618 DEBUG TRAIN Batch 22/7900 loss 5.014175 loss_att 7.396115 loss_ctc 9.355947 loss_rnnt 3.808022 hw_loss 0.282865 lr 0.00036142 rank 2
2023-02-25 03:17:54,619 DEBUG TRAIN Batch 22/7900 loss 3.311130 loss_att 5.989967 loss_ctc 5.526607 loss_rnnt 2.369630 hw_loss 0.206878 lr 0.00036144 rank 7
2023-02-25 03:17:54,672 DEBUG TRAIN Batch 22/7900 loss 7.847387 loss_att 8.785336 loss_ctc 9.181987 loss_rnnt 7.419437 hw_loss 0.117025 lr 0.00036148 rank 3
2023-02-25 03:19:04,534 DEBUG TRAIN Batch 22/8000 loss 13.930734 loss_att 17.995289 loss_ctc 19.585382 loss_rnnt 12.259552 hw_loss 0.195597 lr 0.00036135 rank 0
2023-02-25 03:19:04,536 DEBUG TRAIN Batch 22/8000 loss 7.792890 loss_att 12.361139 loss_ctc 13.858511 loss_rnnt 5.974835 hw_loss 0.179352 lr 0.00036139 rank 3
2023-02-25 03:19:04,537 DEBUG TRAIN Batch 22/8000 loss 10.337454 loss_att 14.349757 loss_ctc 15.836287 loss_rnnt 8.692603 hw_loss 0.204774 lr 0.00036134 rank 7
2023-02-25 03:19:04,536 DEBUG TRAIN Batch 22/8000 loss 15.732651 loss_att 16.439354 loss_ctc 22.240644 loss_rnnt 14.621841 hw_loss 0.190754 lr 0.00036133 rank 2
2023-02-25 03:19:04,540 DEBUG TRAIN Batch 22/8000 loss 8.501569 loss_att 11.993824 loss_ctc 13.180556 loss_rnnt 7.086590 hw_loss 0.173743 lr 0.00036137 rank 4
2023-02-25 03:19:04,542 DEBUG TRAIN Batch 22/8000 loss 12.359876 loss_att 14.917504 loss_ctc 18.388893 loss_rnnt 10.886325 hw_loss 0.296544 lr 0.00036132 rank 5
2023-02-25 03:19:04,571 DEBUG TRAIN Batch 22/8000 loss 9.174904 loss_att 10.205398 loss_ctc 9.517522 loss_rnnt 8.773818 hw_loss 0.279947 lr 0.00036134 rank 6
2023-02-25 03:19:04,583 DEBUG TRAIN Batch 22/8000 loss 8.465945 loss_att 12.234135 loss_ctc 18.924772 loss_rnnt 6.209609 hw_loss 0.202853 lr 0.00036134 rank 1
2023-02-25 03:20:16,146 DEBUG TRAIN Batch 22/8100 loss 8.413157 loss_att 9.992840 loss_ctc 9.468412 loss_rnnt 7.892282 hw_loss 0.120446 lr 0.00036126 rank 0
2023-02-25 03:20:16,151 DEBUG TRAIN Batch 22/8100 loss 12.981970 loss_att 17.005764 loss_ctc 17.751095 loss_rnnt 11.421309 hw_loss 0.225037 lr 0.00036124 rank 1
2023-02-25 03:20:16,153 DEBUG TRAIN Batch 22/8100 loss 8.381507 loss_att 11.000023 loss_ctc 11.921995 loss_rnnt 7.218560 hw_loss 0.313458 lr 0.00036125 rank 7
2023-02-25 03:20:16,155 DEBUG TRAIN Batch 22/8100 loss 4.153522 loss_att 6.510527 loss_ctc 6.867678 loss_rnnt 3.180053 hw_loss 0.262839 lr 0.00036127 rank 4
2023-02-25 03:20:16,155 DEBUG TRAIN Batch 22/8100 loss 7.990060 loss_att 10.136030 loss_ctc 11.321287 loss_rnnt 6.999850 hw_loss 0.219098 lr 0.00036124 rank 6
2023-02-25 03:20:16,156 DEBUG TRAIN Batch 22/8100 loss 12.827007 loss_att 18.239742 loss_ctc 19.173855 loss_rnnt 10.788453 hw_loss 0.205802 lr 0.00036123 rank 2
2023-02-25 03:20:16,174 DEBUG TRAIN Batch 22/8100 loss 13.651585 loss_att 15.686298 loss_ctc 19.475903 loss_rnnt 12.361890 hw_loss 0.199079 lr 0.00036129 rank 3
2023-02-25 03:20:16,184 DEBUG TRAIN Batch 22/8100 loss 13.069480 loss_att 14.544361 loss_ctc 17.189260 loss_rnnt 12.083130 hw_loss 0.266381 lr 0.00036123 rank 5
2023-02-25 03:21:27,971 DEBUG TRAIN Batch 22/8200 loss 13.294541 loss_att 16.230152 loss_ctc 19.830557 loss_rnnt 11.774942 hw_loss 0.114388 lr 0.00036115 rank 6
2023-02-25 03:21:27,974 DEBUG TRAIN Batch 22/8200 loss 9.499945 loss_att 12.567108 loss_ctc 13.530952 loss_rnnt 8.272904 hw_loss 0.142763 lr 0.00036114 rank 2
2023-02-25 03:21:27,975 DEBUG TRAIN Batch 22/8200 loss 13.513959 loss_att 14.530245 loss_ctc 16.057888 loss_rnnt 12.820221 hw_loss 0.283669 lr 0.00036116 rank 0
2023-02-25 03:21:27,975 DEBUG TRAIN Batch 22/8200 loss 9.429252 loss_att 13.388942 loss_ctc 16.573673 loss_rnnt 7.578500 hw_loss 0.199171 lr 0.00036120 rank 3
2023-02-25 03:21:27,976 DEBUG TRAIN Batch 22/8200 loss 10.974853 loss_att 11.926883 loss_ctc 13.962517 loss_rnnt 10.287334 hw_loss 0.185168 lr 0.00036115 rank 1
2023-02-25 03:21:27,982 DEBUG TRAIN Batch 22/8200 loss 11.778895 loss_att 11.511789 loss_ctc 15.783146 loss_rnnt 11.180087 hw_loss 0.221868 lr 0.00036115 rank 7
2023-02-25 03:21:27,983 DEBUG TRAIN Batch 22/8200 loss 13.222657 loss_att 13.247725 loss_ctc 17.857719 loss_rnnt 12.463166 hw_loss 0.255878 lr 0.00036113 rank 5
2023-02-25 03:21:28,037 DEBUG TRAIN Batch 22/8200 loss 10.243266 loss_att 12.071171 loss_ctc 17.631220 loss_rnnt 8.743766 hw_loss 0.279112 lr 0.00036118 rank 4
2023-02-25 03:22:38,236 DEBUG TRAIN Batch 22/8300 loss 4.800660 loss_att 6.414164 loss_ctc 7.596864 loss_rnnt 3.994955 hw_loss 0.206582 lr 0.00036111 rank 3
2023-02-25 03:22:38,241 DEBUG TRAIN Batch 22/8300 loss 7.590137 loss_att 9.993896 loss_ctc 12.212816 loss_rnnt 6.347103 hw_loss 0.273611 lr 0.00036107 rank 0
2023-02-25 03:22:38,248 DEBUG TRAIN Batch 22/8300 loss 13.107642 loss_att 18.194845 loss_ctc 17.963415 loss_rnnt 11.326138 hw_loss 0.218675 lr 0.00036106 rank 1
2023-02-25 03:22:38,248 DEBUG TRAIN Batch 22/8300 loss 4.102043 loss_att 7.264663 loss_ctc 8.070816 loss_rnnt 2.857741 hw_loss 0.154889 lr 0.00036108 rank 4
2023-02-25 03:22:38,249 DEBUG TRAIN Batch 22/8300 loss 8.425359 loss_att 13.110251 loss_ctc 12.183612 loss_rnnt 6.872888 hw_loss 0.214483 lr 0.00036105 rank 6
2023-02-25 03:22:38,249 DEBUG TRAIN Batch 22/8300 loss 15.374673 loss_att 20.466801 loss_ctc 32.391060 loss_rnnt 11.983152 hw_loss 0.195457 lr 0.00036104 rank 5
2023-02-25 03:22:38,266 DEBUG TRAIN Batch 22/8300 loss 1.990805 loss_att 5.009163 loss_ctc 3.064558 loss_rnnt 1.165746 hw_loss 0.146662 lr 0.00036105 rank 2
2023-02-25 03:22:38,278 DEBUG TRAIN Batch 22/8300 loss 15.704923 loss_att 19.555286 loss_ctc 22.865150 loss_rnnt 13.876728 hw_loss 0.193919 lr 0.00036106 rank 7
2023-02-25 03:23:22,660 DEBUG CV Batch 22/0 loss 2.166943 loss_att 2.051645 loss_ctc 2.549410 loss_rnnt 2.022609 hw_loss 0.218248 history loss 2.086686 rank 4
2023-02-25 03:23:22,661 DEBUG CV Batch 22/0 loss 2.166943 loss_att 2.051645 loss_ctc 2.549410 loss_rnnt 2.022609 hw_loss 0.218248 history loss 2.086686 rank 5
2023-02-25 03:23:22,664 DEBUG CV Batch 22/0 loss 2.166943 loss_att 2.051645 loss_ctc 2.549410 loss_rnnt 2.022609 hw_loss 0.218248 history loss 2.086686 rank 2
2023-02-25 03:23:22,665 DEBUG CV Batch 22/0 loss 2.166943 loss_att 2.051645 loss_ctc 2.549410 loss_rnnt 2.022609 hw_loss 0.218248 history loss 2.086686 rank 7
2023-02-25 03:23:22,670 DEBUG CV Batch 22/0 loss 2.166943 loss_att 2.051645 loss_ctc 2.549410 loss_rnnt 2.022609 hw_loss 0.218248 history loss 2.086686 rank 0
2023-02-25 03:23:22,672 DEBUG CV Batch 22/0 loss 2.166943 loss_att 2.051645 loss_ctc 2.549410 loss_rnnt 2.022609 hw_loss 0.218248 history loss 2.086686 rank 6
2023-02-25 03:23:22,676 DEBUG CV Batch 22/0 loss 2.166943 loss_att 2.051645 loss_ctc 2.549410 loss_rnnt 2.022609 hw_loss 0.218248 history loss 2.086686 rank 1
2023-02-25 03:23:22,679 DEBUG CV Batch 22/0 loss 2.166943 loss_att 2.051645 loss_ctc 2.549410 loss_rnnt 2.022609 hw_loss 0.218248 history loss 2.086686 rank 3
2023-02-25 03:23:33,928 DEBUG CV Batch 22/100 loss 6.983399 loss_att 7.229407 loss_ctc 10.905609 loss_rnnt 6.281778 hw_loss 0.242733 history loss 3.375330 rank 3
2023-02-25 03:23:34,147 DEBUG CV Batch 22/100 loss 6.983399 loss_att 7.229407 loss_ctc 10.905609 loss_rnnt 6.281778 hw_loss 0.242733 history loss 3.375330 rank 5
2023-02-25 03:23:34,184 DEBUG CV Batch 22/100 loss 6.983399 loss_att 7.229407 loss_ctc 10.905609 loss_rnnt 6.281778 hw_loss 0.242733 history loss 3.375330 rank 4
2023-02-25 03:23:34,284 DEBUG CV Batch 22/100 loss 6.983399 loss_att 7.229407 loss_ctc 10.905609 loss_rnnt 6.281778 hw_loss 0.242733 history loss 3.375330 rank 1
2023-02-25 03:23:34,349 DEBUG CV Batch 22/100 loss 6.983399 loss_att 7.229407 loss_ctc 10.905609 loss_rnnt 6.281778 hw_loss 0.242733 history loss 3.375330 rank 7
2023-02-25 03:23:34,475 DEBUG CV Batch 22/100 loss 6.983399 loss_att 7.229407 loss_ctc 10.905609 loss_rnnt 6.281778 hw_loss 0.242733 history loss 3.375330 rank 6
2023-02-25 03:23:34,553 DEBUG CV Batch 22/100 loss 6.983399 loss_att 7.229407 loss_ctc 10.905609 loss_rnnt 6.281778 hw_loss 0.242733 history loss 3.375330 rank 2
2023-02-25 03:23:34,560 DEBUG CV Batch 22/100 loss 6.983399 loss_att 7.229407 loss_ctc 10.905609 loss_rnnt 6.281778 hw_loss 0.242733 history loss 3.375330 rank 0
2023-02-25 03:23:47,718 DEBUG CV Batch 22/200 loss 5.089416 loss_att 10.884123 loss_ctc 5.659428 loss_rnnt 3.784664 hw_loss 0.130892 history loss 3.941951 rank 3
2023-02-25 03:23:47,902 DEBUG CV Batch 22/200 loss 5.089416 loss_att 10.884123 loss_ctc 5.659428 loss_rnnt 3.784664 hw_loss 0.130892 history loss 3.941951 rank 5
2023-02-25 03:23:48,016 DEBUG CV Batch 22/200 loss 5.089416 loss_att 10.884123 loss_ctc 5.659428 loss_rnnt 3.784664 hw_loss 0.130892 history loss 3.941951 rank 1
2023-02-25 03:23:48,077 DEBUG CV Batch 22/200 loss 5.089416 loss_att 10.884123 loss_ctc 5.659428 loss_rnnt 3.784664 hw_loss 0.130892 history loss 3.941951 rank 7
2023-02-25 03:23:48,315 DEBUG CV Batch 22/200 loss 5.089416 loss_att 10.884123 loss_ctc 5.659428 loss_rnnt 3.784664 hw_loss 0.130892 history loss 3.941951 rank 6
2023-02-25 03:23:48,340 DEBUG CV Batch 22/200 loss 5.089416 loss_att 10.884123 loss_ctc 5.659428 loss_rnnt 3.784664 hw_loss 0.130892 history loss 3.941951 rank 0
2023-02-25 03:23:48,409 DEBUG CV Batch 22/200 loss 5.089416 loss_att 10.884123 loss_ctc 5.659428 loss_rnnt 3.784664 hw_loss 0.130892 history loss 3.941951 rank 2
2023-02-25 03:23:48,681 DEBUG CV Batch 22/200 loss 5.089416 loss_att 10.884123 loss_ctc 5.659428 loss_rnnt 3.784664 hw_loss 0.130892 history loss 3.941951 rank 4
2023-02-25 03:24:00,116 DEBUG CV Batch 22/300 loss 4.810001 loss_att 4.982256 loss_ctc 7.473796 loss_rnnt 4.244884 hw_loss 0.329050 history loss 4.062827 rank 5
2023-02-25 03:24:00,143 DEBUG CV Batch 22/300 loss 4.810001 loss_att 4.982256 loss_ctc 7.473796 loss_rnnt 4.244884 hw_loss 0.329050 history loss 4.062827 rank 3
2023-02-25 03:24:00,315 DEBUG CV Batch 22/300 loss 4.810001 loss_att 4.982256 loss_ctc 7.473796 loss_rnnt 4.244884 hw_loss 0.329050 history loss 4.062827 rank 1
2023-02-25 03:24:00,520 DEBUG CV Batch 22/300 loss 4.810001 loss_att 4.982256 loss_ctc 7.473796 loss_rnnt 4.244884 hw_loss 0.329050 history loss 4.062827 rank 7
2023-02-25 03:24:00,763 DEBUG CV Batch 22/300 loss 4.810001 loss_att 4.982256 loss_ctc 7.473796 loss_rnnt 4.244884 hw_loss 0.329050 history loss 4.062827 rank 6
2023-02-25 03:24:00,818 DEBUG CV Batch 22/300 loss 4.810001 loss_att 4.982256 loss_ctc 7.473796 loss_rnnt 4.244884 hw_loss 0.329050 history loss 4.062827 rank 0
2023-02-25 03:24:00,843 DEBUG CV Batch 22/300 loss 4.810001 loss_att 4.982256 loss_ctc 7.473796 loss_rnnt 4.244884 hw_loss 0.329050 history loss 4.062827 rank 2
2023-02-25 03:24:00,853 DEBUG CV Batch 22/300 loss 4.810001 loss_att 4.982256 loss_ctc 7.473796 loss_rnnt 4.244884 hw_loss 0.329050 history loss 4.062827 rank 4
2023-02-25 03:24:12,200 DEBUG CV Batch 22/400 loss 19.089663 loss_att 87.544182 loss_ctc 13.502042 loss_rnnt 6.055277 hw_loss 0.165931 history loss 4.938921 rank 3
2023-02-25 03:24:12,264 DEBUG CV Batch 22/400 loss 19.089663 loss_att 87.544182 loss_ctc 13.502042 loss_rnnt 6.055277 hw_loss 0.165931 history loss 4.938921 rank 5
2023-02-25 03:24:12,770 DEBUG CV Batch 22/400 loss 19.089663 loss_att 87.544182 loss_ctc 13.502042 loss_rnnt 6.055277 hw_loss 0.165931 history loss 4.938921 rank 4
2023-02-25 03:24:12,782 DEBUG CV Batch 22/400 loss 19.089663 loss_att 87.544182 loss_ctc 13.502042 loss_rnnt 6.055277 hw_loss 0.165931 history loss 4.938921 rank 1
2023-02-25 03:24:13,024 DEBUG CV Batch 22/400 loss 19.089663 loss_att 87.544182 loss_ctc 13.502042 loss_rnnt 6.055277 hw_loss 0.165931 history loss 4.938921 rank 7
2023-02-25 03:24:13,184 DEBUG CV Batch 22/400 loss 19.089663 loss_att 87.544182 loss_ctc 13.502042 loss_rnnt 6.055277 hw_loss 0.165931 history loss 4.938921 rank 6
2023-02-25 03:24:13,365 DEBUG CV Batch 22/400 loss 19.089663 loss_att 87.544182 loss_ctc 13.502042 loss_rnnt 6.055277 hw_loss 0.165931 history loss 4.938921 rank 0
2023-02-25 03:24:13,406 DEBUG CV Batch 22/400 loss 19.089663 loss_att 87.544182 loss_ctc 13.502042 loss_rnnt 6.055277 hw_loss 0.165931 history loss 4.938921 rank 2
2023-02-25 03:24:22,797 DEBUG CV Batch 22/500 loss 5.470354 loss_att 5.371447 loss_ctc 6.686519 loss_rnnt 5.246205 hw_loss 0.153328 history loss 5.701341 rank 3
2023-02-25 03:24:22,854 DEBUG CV Batch 22/500 loss 5.470354 loss_att 5.371447 loss_ctc 6.686519 loss_rnnt 5.246205 hw_loss 0.153328 history loss 5.701341 rank 5
2023-02-25 03:24:23,240 DEBUG CV Batch 22/500 loss 5.470354 loss_att 5.371447 loss_ctc 6.686519 loss_rnnt 5.246205 hw_loss 0.153328 history loss 5.701341 rank 4
2023-02-25 03:24:23,528 DEBUG CV Batch 22/500 loss 5.470354 loss_att 5.371447 loss_ctc 6.686519 loss_rnnt 5.246205 hw_loss 0.153328 history loss 5.701341 rank 1
2023-02-25 03:24:23,846 DEBUG CV Batch 22/500 loss 5.470354 loss_att 5.371447 loss_ctc 6.686519 loss_rnnt 5.246205 hw_loss 0.153328 history loss 5.701341 rank 6
2023-02-25 03:24:24,017 DEBUG CV Batch 22/500 loss 5.470354 loss_att 5.371447 loss_ctc 6.686519 loss_rnnt 5.246205 hw_loss 0.153328 history loss 5.701341 rank 7
2023-02-25 03:24:24,140 DEBUG CV Batch 22/500 loss 5.470354 loss_att 5.371447 loss_ctc 6.686519 loss_rnnt 5.246205 hw_loss 0.153328 history loss 5.701341 rank 0
2023-02-25 03:24:24,464 DEBUG CV Batch 22/500 loss 5.470354 loss_att 5.371447 loss_ctc 6.686519 loss_rnnt 5.246205 hw_loss 0.153328 history loss 5.701341 rank 2
2023-02-25 03:24:35,000 DEBUG CV Batch 22/600 loss 6.710008 loss_att 7.299208 loss_ctc 8.862802 loss_rnnt 6.192873 hw_loss 0.210481 history loss 6.699089 rank 5
2023-02-25 03:24:35,065 DEBUG CV Batch 22/600 loss 6.710008 loss_att 7.299208 loss_ctc 8.862802 loss_rnnt 6.192873 hw_loss 0.210481 history loss 6.699089 rank 3
2023-02-25 03:24:35,396 DEBUG CV Batch 22/600 loss 6.710008 loss_att 7.299208 loss_ctc 8.862802 loss_rnnt 6.192873 hw_loss 0.210481 history loss 6.699089 rank 4
2023-02-25 03:24:35,880 DEBUG CV Batch 22/600 loss 6.710008 loss_att 7.299208 loss_ctc 8.862802 loss_rnnt 6.192873 hw_loss 0.210481 history loss 6.699089 rank 1
2023-02-25 03:24:36,267 DEBUG CV Batch 22/600 loss 6.710008 loss_att 7.299208 loss_ctc 8.862802 loss_rnnt 6.192873 hw_loss 0.210481 history loss 6.699089 rank 6
2023-02-25 03:24:36,457 DEBUG CV Batch 22/600 loss 6.710008 loss_att 7.299208 loss_ctc 8.862802 loss_rnnt 6.192873 hw_loss 0.210481 history loss 6.699089 rank 7
2023-02-25 03:24:36,685 DEBUG CV Batch 22/600 loss 6.710008 loss_att 7.299208 loss_ctc 8.862802 loss_rnnt 6.192873 hw_loss 0.210481 history loss 6.699089 rank 0
2023-02-25 03:24:36,966 DEBUG CV Batch 22/600 loss 6.710008 loss_att 7.299208 loss_ctc 8.862802 loss_rnnt 6.192873 hw_loss 0.210481 history loss 6.699089 rank 2
2023-02-25 03:24:46,367 DEBUG CV Batch 22/700 loss 11.251256 loss_att 35.848637 loss_ctc 13.926031 loss_rnnt 5.850908 hw_loss 0.232943 history loss 7.347876 rank 5
2023-02-25 03:24:46,669 DEBUG CV Batch 22/700 loss 11.251256 loss_att 35.848637 loss_ctc 13.926031 loss_rnnt 5.850908 hw_loss 0.232943 history loss 7.347876 rank 3
2023-02-25 03:24:47,459 DEBUG CV Batch 22/700 loss 11.251256 loss_att 35.848637 loss_ctc 13.926031 loss_rnnt 5.850908 hw_loss 0.232943 history loss 7.347876 rank 1
2023-02-25 03:24:47,564 DEBUG CV Batch 22/700 loss 11.251256 loss_att 35.848637 loss_ctc 13.926031 loss_rnnt 5.850908 hw_loss 0.232943 history loss 7.347876 rank 4
2023-02-25 03:24:47,824 DEBUG CV Batch 22/700 loss 11.251256 loss_att 35.848637 loss_ctc 13.926031 loss_rnnt 5.850908 hw_loss 0.232943 history loss 7.347876 rank 6
2023-02-25 03:24:48,300 DEBUG CV Batch 22/700 loss 11.251256 loss_att 35.848637 loss_ctc 13.926031 loss_rnnt 5.850908 hw_loss 0.232943 history loss 7.347876 rank 7
2023-02-25 03:24:48,499 DEBUG CV Batch 22/700 loss 11.251256 loss_att 35.848637 loss_ctc 13.926031 loss_rnnt 5.850908 hw_loss 0.232943 history loss 7.347876 rank 0
2023-02-25 03:24:48,839 DEBUG CV Batch 22/700 loss 11.251256 loss_att 35.848637 loss_ctc 13.926031 loss_rnnt 5.850908 hw_loss 0.232943 history loss 7.347876 rank 2
2023-02-25 03:24:57,717 DEBUG CV Batch 22/800 loss 8.436466 loss_att 8.618712 loss_ctc 13.124563 loss_rnnt 7.616218 hw_loss 0.297598 history loss 6.813835 rank 5
2023-02-25 03:24:58,612 DEBUG CV Batch 22/800 loss 8.436466 loss_att 8.618712 loss_ctc 13.124563 loss_rnnt 7.616218 hw_loss 0.297598 history loss 6.813835 rank 3
2023-02-25 03:24:58,831 DEBUG CV Batch 22/800 loss 8.436466 loss_att 8.618712 loss_ctc 13.124563 loss_rnnt 7.616218 hw_loss 0.297598 history loss 6.813835 rank 1
2023-02-25 03:24:59,313 DEBUG CV Batch 22/800 loss 8.436466 loss_att 8.618712 loss_ctc 13.124563 loss_rnnt 7.616218 hw_loss 0.297598 history loss 6.813835 rank 6
2023-02-25 03:24:59,811 DEBUG CV Batch 22/800 loss 8.436466 loss_att 8.618712 loss_ctc 13.124563 loss_rnnt 7.616218 hw_loss 0.297598 history loss 6.813835 rank 4
2023-02-25 03:24:59,866 DEBUG CV Batch 22/800 loss 8.436466 loss_att 8.618712 loss_ctc 13.124563 loss_rnnt 7.616218 hw_loss 0.297598 history loss 6.813835 rank 7
2023-02-25 03:25:00,453 DEBUG CV Batch 22/800 loss 8.436466 loss_att 8.618712 loss_ctc 13.124563 loss_rnnt 7.616218 hw_loss 0.297598 history loss 6.813835 rank 2
2023-02-25 03:25:00,631 DEBUG CV Batch 22/800 loss 8.436466 loss_att 8.618712 loss_ctc 13.124563 loss_rnnt 7.616218 hw_loss 0.297598 history loss 6.813835 rank 0
2023-02-25 03:25:11,226 DEBUG CV Batch 22/900 loss 9.704398 loss_att 13.804535 loss_ctc 20.917418 loss_rnnt 7.279117 hw_loss 0.206597 history loss 6.614546 rank 5
2023-02-25 03:25:12,552 DEBUG CV Batch 22/900 loss 9.704398 loss_att 13.804535 loss_ctc 20.917418 loss_rnnt 7.279117 hw_loss 0.206597 history loss 6.614546 rank 1
2023-02-25 03:25:12,805 DEBUG CV Batch 22/900 loss 9.704398 loss_att 13.804535 loss_ctc 20.917418 loss_rnnt 7.279117 hw_loss 0.206597 history loss 6.614546 rank 3
2023-02-25 03:25:13,011 DEBUG CV Batch 22/900 loss 9.704398 loss_att 13.804535 loss_ctc 20.917418 loss_rnnt 7.279117 hw_loss 0.206597 history loss 6.614546 rank 6
2023-02-25 03:25:13,547 DEBUG CV Batch 22/900 loss 9.704398 loss_att 13.804535 loss_ctc 20.917418 loss_rnnt 7.279117 hw_loss 0.206597 history loss 6.614546 rank 4
2023-02-25 03:25:13,824 DEBUG CV Batch 22/900 loss 9.704398 loss_att 13.804535 loss_ctc 20.917418 loss_rnnt 7.279117 hw_loss 0.206597 history loss 6.614546 rank 7
2023-02-25 03:25:14,254 DEBUG CV Batch 22/900 loss 9.704398 loss_att 13.804535 loss_ctc 20.917418 loss_rnnt 7.279117 hw_loss 0.206597 history loss 6.614546 rank 2
2023-02-25 03:25:14,576 DEBUG CV Batch 22/900 loss 9.704398 loss_att 13.804535 loss_ctc 20.917418 loss_rnnt 7.279117 hw_loss 0.206597 history loss 6.614546 rank 0
2023-02-25 03:25:23,820 DEBUG CV Batch 22/1000 loss 4.404554 loss_att 4.834566 loss_ctc 5.074054 loss_rnnt 4.082958 hw_loss 0.274363 history loss 6.376915 rank 5
2023-02-25 03:25:24,966 DEBUG CV Batch 22/1000 loss 4.404554 loss_att 4.834566 loss_ctc 5.074054 loss_rnnt 4.082958 hw_loss 0.274363 history loss 6.376915 rank 3
2023-02-25 03:25:25,239 DEBUG CV Batch 22/1000 loss 4.404554 loss_att 4.834566 loss_ctc 5.074054 loss_rnnt 4.082958 hw_loss 0.274363 history loss 6.376915 rank 1
2023-02-25 03:25:25,613 DEBUG CV Batch 22/1000 loss 4.404554 loss_att 4.834566 loss_ctc 5.074054 loss_rnnt 4.082958 hw_loss 0.274363 history loss 6.376915 rank 4
2023-02-25 03:25:25,652 DEBUG CV Batch 22/1000 loss 4.404554 loss_att 4.834566 loss_ctc 5.074054 loss_rnnt 4.082958 hw_loss 0.274363 history loss 6.376915 rank 6
2023-02-25 03:25:26,528 DEBUG CV Batch 22/1000 loss 4.404554 loss_att 4.834566 loss_ctc 5.074054 loss_rnnt 4.082958 hw_loss 0.274363 history loss 6.376915 rank 7
2023-02-25 03:25:27,038 DEBUG CV Batch 22/1000 loss 4.404554 loss_att 4.834566 loss_ctc 5.074054 loss_rnnt 4.082958 hw_loss 0.274363 history loss 6.376915 rank 2
2023-02-25 03:25:27,128 DEBUG CV Batch 22/1000 loss 4.404554 loss_att 4.834566 loss_ctc 5.074054 loss_rnnt 4.082958 hw_loss 0.274363 history loss 6.376915 rank 0
2023-02-25 03:25:35,988 DEBUG CV Batch 22/1100 loss 5.948802 loss_att 5.325007 loss_ctc 7.646541 loss_rnnt 5.689590 hw_loss 0.295511 history loss 6.346471 rank 5
2023-02-25 03:25:36,910 DEBUG CV Batch 22/1100 loss 5.948802 loss_att 5.325007 loss_ctc 7.646541 loss_rnnt 5.689590 hw_loss 0.295511 history loss 6.346471 rank 3
2023-02-25 03:25:37,434 DEBUG CV Batch 22/1100 loss 5.948802 loss_att 5.325007 loss_ctc 7.646541 loss_rnnt 5.689590 hw_loss 0.295511 history loss 6.346471 rank 1
2023-02-25 03:25:37,453 DEBUG CV Batch 22/1100 loss 5.948802 loss_att 5.325007 loss_ctc 7.646541 loss_rnnt 5.689590 hw_loss 0.295511 history loss 6.346471 rank 4
2023-02-25 03:25:37,933 DEBUG CV Batch 22/1100 loss 5.948802 loss_att 5.325007 loss_ctc 7.646541 loss_rnnt 5.689590 hw_loss 0.295511 history loss 6.346471 rank 6
2023-02-25 03:25:38,847 DEBUG CV Batch 22/1100 loss 5.948802 loss_att 5.325007 loss_ctc 7.646541 loss_rnnt 5.689590 hw_loss 0.295511 history loss 6.346471 rank 7
2023-02-25 03:25:39,456 DEBUG CV Batch 22/1100 loss 5.948802 loss_att 5.325007 loss_ctc 7.646541 loss_rnnt 5.689590 hw_loss 0.295511 history loss 6.346471 rank 2
2023-02-25 03:25:39,553 DEBUG CV Batch 22/1100 loss 5.948802 loss_att 5.325007 loss_ctc 7.646541 loss_rnnt 5.689590 hw_loss 0.295511 history loss 6.346471 rank 0
2023-02-25 03:25:46,402 DEBUG CV Batch 22/1200 loss 7.528278 loss_att 8.380072 loss_ctc 8.702400 loss_rnnt 7.061276 hw_loss 0.262672 history loss 6.656919 rank 5
2023-02-25 03:25:47,490 DEBUG CV Batch 22/1200 loss 7.528278 loss_att 8.380072 loss_ctc 8.702400 loss_rnnt 7.061276 hw_loss 0.262672 history loss 6.656919 rank 3
2023-02-25 03:25:48,057 DEBUG CV Batch 22/1200 loss 7.528278 loss_att 8.380072 loss_ctc 8.702400 loss_rnnt 7.061276 hw_loss 0.262672 history loss 6.656919 rank 4
2023-02-25 03:25:48,447 DEBUG CV Batch 22/1200 loss 7.528278 loss_att 8.380072 loss_ctc 8.702400 loss_rnnt 7.061276 hw_loss 0.262672 history loss 6.656919 rank 1
2023-02-25 03:25:49,078 DEBUG CV Batch 22/1200 loss 7.528278 loss_att 8.380072 loss_ctc 8.702400 loss_rnnt 7.061276 hw_loss 0.262672 history loss 6.656919 rank 6
2023-02-25 03:25:50,062 DEBUG CV Batch 22/1200 loss 7.528278 loss_att 8.380072 loss_ctc 8.702400 loss_rnnt 7.061276 hw_loss 0.262672 history loss 6.656919 rank 7
2023-02-25 03:25:50,442 DEBUG CV Batch 22/1200 loss 7.528278 loss_att 8.380072 loss_ctc 8.702400 loss_rnnt 7.061276 hw_loss 0.262672 history loss 6.656919 rank 0
2023-02-25 03:25:50,644 DEBUG CV Batch 22/1200 loss 7.528278 loss_att 8.380072 loss_ctc 8.702400 loss_rnnt 7.061276 hw_loss 0.262672 history loss 6.656919 rank 2
2023-02-25 03:25:58,584 DEBUG CV Batch 22/1300 loss 5.155920 loss_att 5.923638 loss_ctc 7.592619 loss_rnnt 4.561725 hw_loss 0.217046 history loss 6.994919 rank 5
2023-02-25 03:25:59,975 DEBUG CV Batch 22/1300 loss 5.155920 loss_att 5.923638 loss_ctc 7.592619 loss_rnnt 4.561725 hw_loss 0.217046 history loss 6.994919 rank 3
2023-02-25 03:26:00,007 DEBUG CV Batch 22/1300 loss 5.155920 loss_att 5.923638 loss_ctc 7.592619 loss_rnnt 4.561725 hw_loss 0.217046 history loss 6.994919 rank 4
2023-02-25 03:26:00,747 DEBUG CV Batch 22/1300 loss 5.155920 loss_att 5.923638 loss_ctc 7.592619 loss_rnnt 4.561725 hw_loss 0.217046 history loss 6.994919 rank 1
2023-02-25 03:26:01,389 DEBUG CV Batch 22/1300 loss 5.155920 loss_att 5.923638 loss_ctc 7.592619 loss_rnnt 4.561725 hw_loss 0.217046 history loss 6.994919 rank 6
2023-02-25 03:26:02,413 DEBUG CV Batch 22/1300 loss 5.155920 loss_att 5.923638 loss_ctc 7.592619 loss_rnnt 4.561725 hw_loss 0.217046 history loss 6.994919 rank 7
2023-02-25 03:26:02,961 DEBUG CV Batch 22/1300 loss 5.155920 loss_att 5.923638 loss_ctc 7.592619 loss_rnnt 4.561725 hw_loss 0.217046 history loss 6.994919 rank 0
2023-02-25 03:26:03,256 DEBUG CV Batch 22/1300 loss 5.155920 loss_att 5.923638 loss_ctc 7.592619 loss_rnnt 4.561725 hw_loss 0.217046 history loss 6.994919 rank 2
2023-02-25 03:26:09,785 DEBUG CV Batch 22/1400 loss 5.719529 loss_att 12.382109 loss_ctc 5.374144 loss_rnnt 4.334770 hw_loss 0.184301 history loss 7.319902 rank 5
2023-02-25 03:26:11,710 DEBUG CV Batch 22/1400 loss 5.719529 loss_att 12.382109 loss_ctc 5.374144 loss_rnnt 4.334770 hw_loss 0.184301 history loss 7.319902 rank 3
2023-02-25 03:26:12,355 DEBUG CV Batch 22/1400 loss 5.719529 loss_att 12.382109 loss_ctc 5.374144 loss_rnnt 4.334770 hw_loss 0.184301 history loss 7.319902 rank 1
2023-02-25 03:26:12,634 DEBUG CV Batch 22/1400 loss 5.719529 loss_att 12.382109 loss_ctc 5.374144 loss_rnnt 4.334770 hw_loss 0.184301 history loss 7.319902 rank 4
2023-02-25 03:26:13,056 DEBUG CV Batch 22/1400 loss 5.719529 loss_att 12.382109 loss_ctc 5.374144 loss_rnnt 4.334770 hw_loss 0.184301 history loss 7.319902 rank 6
2023-02-25 03:26:14,314 DEBUG CV Batch 22/1400 loss 5.719529 loss_att 12.382109 loss_ctc 5.374144 loss_rnnt 4.334770 hw_loss 0.184301 history loss 7.319902 rank 7
2023-02-25 03:26:14,680 DEBUG CV Batch 22/1400 loss 5.719529 loss_att 12.382109 loss_ctc 5.374144 loss_rnnt 4.334770 hw_loss 0.184301 history loss 7.319902 rank 0
2023-02-25 03:26:15,052 DEBUG CV Batch 22/1400 loss 5.719529 loss_att 12.382109 loss_ctc 5.374144 loss_rnnt 4.334770 hw_loss 0.184301 history loss 7.319902 rank 2
2023-02-25 03:26:21,674 DEBUG CV Batch 22/1500 loss 7.117037 loss_att 7.527094 loss_ctc 7.717579 loss_rnnt 6.846542 hw_loss 0.203270 history loss 7.144474 rank 5
2023-02-25 03:26:24,147 DEBUG CV Batch 22/1500 loss 7.117037 loss_att 7.527094 loss_ctc 7.717579 loss_rnnt 6.846542 hw_loss 0.203270 history loss 7.144474 rank 1
2023-02-25 03:26:24,312 DEBUG CV Batch 22/1500 loss 7.117037 loss_att 7.527094 loss_ctc 7.717579 loss_rnnt 6.846542 hw_loss 0.203270 history loss 7.144474 rank 3
2023-02-25 03:26:24,834 DEBUG CV Batch 22/1500 loss 7.117037 loss_att 7.527094 loss_ctc 7.717579 loss_rnnt 6.846542 hw_loss 0.203270 history loss 7.144474 rank 6
2023-02-25 03:26:25,157 DEBUG CV Batch 22/1500 loss 7.117037 loss_att 7.527094 loss_ctc 7.717579 loss_rnnt 6.846542 hw_loss 0.203270 history loss 7.144474 rank 4
2023-02-25 03:26:26,196 DEBUG CV Batch 22/1500 loss 7.117037 loss_att 7.527094 loss_ctc 7.717579 loss_rnnt 6.846542 hw_loss 0.203270 history loss 7.144474 rank 7
2023-02-25 03:26:26,779 DEBUG CV Batch 22/1500 loss 7.117037 loss_att 7.527094 loss_ctc 7.717579 loss_rnnt 6.846542 hw_loss 0.203270 history loss 7.144474 rank 0
2023-02-25 03:26:27,052 DEBUG CV Batch 22/1500 loss 7.117037 loss_att 7.527094 loss_ctc 7.717579 loss_rnnt 6.846542 hw_loss 0.203270 history loss 7.144474 rank 2
2023-02-25 03:26:35,738 DEBUG CV Batch 22/1600 loss 8.676634 loss_att 11.230018 loss_ctc 9.263988 loss_rnnt 7.938475 hw_loss 0.279689 history loss 7.078242 rank 5
2023-02-25 03:26:37,569 DEBUG CV Batch 22/1600 loss 8.676634 loss_att 11.230018 loss_ctc 9.263988 loss_rnnt 7.938475 hw_loss 0.279689 history loss 7.078242 rank 1
2023-02-25 03:26:38,418 DEBUG CV Batch 22/1600 loss 8.676634 loss_att 11.230018 loss_ctc 9.263988 loss_rnnt 7.938475 hw_loss 0.279689 history loss 7.078242 rank 6
2023-02-25 03:26:38,452 DEBUG CV Batch 22/1600 loss 8.676634 loss_att 11.230018 loss_ctc 9.263988 loss_rnnt 7.938475 hw_loss 0.279689 history loss 7.078242 rank 3
2023-02-25 03:26:38,947 DEBUG CV Batch 22/1600 loss 8.676634 loss_att 11.230018 loss_ctc 9.263988 loss_rnnt 7.938475 hw_loss 0.279689 history loss 7.078242 rank 4
2023-02-25 03:26:39,754 DEBUG CV Batch 22/1600 loss 8.676634 loss_att 11.230018 loss_ctc 9.263988 loss_rnnt 7.938475 hw_loss 0.279689 history loss 7.078242 rank 7
2023-02-25 03:26:40,592 DEBUG CV Batch 22/1600 loss 8.676634 loss_att 11.230018 loss_ctc 9.263988 loss_rnnt 7.938475 hw_loss 0.279689 history loss 7.078242 rank 0
2023-02-25 03:26:40,645 DEBUG CV Batch 22/1600 loss 8.676634 loss_att 11.230018 loss_ctc 9.263988 loss_rnnt 7.938475 hw_loss 0.279689 history loss 7.078242 rank 2
2023-02-25 03:26:48,747 DEBUG CV Batch 22/1700 loss 9.533232 loss_att 8.183863 loss_ctc 13.755672 loss_rnnt 9.085199 hw_loss 0.290465 history loss 6.977515 rank 5
2023-02-25 03:26:50,168 DEBUG CV Batch 22/1700 loss 9.533232 loss_att 8.183863 loss_ctc 13.755672 loss_rnnt 9.085199 hw_loss 0.290465 history loss 6.977515 rank 1
2023-02-25 03:26:50,914 DEBUG CV Batch 22/1700 loss 9.533232 loss_att 8.183863 loss_ctc 13.755672 loss_rnnt 9.085199 hw_loss 0.290465 history loss 6.977515 rank 3
2023-02-25 03:26:51,050 DEBUG CV Batch 22/1700 loss 9.533232 loss_att 8.183863 loss_ctc 13.755672 loss_rnnt 9.085199 hw_loss 0.290465 history loss 6.977515 rank 6
2023-02-25 03:26:51,390 DEBUG CV Batch 22/1700 loss 9.533232 loss_att 8.183863 loss_ctc 13.755672 loss_rnnt 9.085199 hw_loss 0.290465 history loss 6.977515 rank 4
2023-02-25 03:26:52,443 DEBUG CV Batch 22/1700 loss 9.533232 loss_att 8.183863 loss_ctc 13.755672 loss_rnnt 9.085199 hw_loss 0.290465 history loss 6.977515 rank 7
2023-02-25 03:26:53,229 DEBUG CV Batch 22/1700 loss 9.533232 loss_att 8.183863 loss_ctc 13.755672 loss_rnnt 9.085199 hw_loss 0.290465 history loss 6.977515 rank 0
2023-02-25 03:26:53,440 DEBUG CV Batch 22/1700 loss 9.533232 loss_att 8.183863 loss_ctc 13.755672 loss_rnnt 9.085199 hw_loss 0.290465 history loss 6.977515 rank 2
2023-02-25 03:26:58,106 INFO Epoch 22 CV info cv_loss 6.941393848949558
2023-02-25 03:26:58,107 INFO Epoch 23 TRAIN info lr 0.0003609906004758424
2023-02-25 03:26:58,109 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 03:26:59,169 INFO Epoch 22 CV info cv_loss 6.941393847547526
2023-02-25 03:26:59,170 INFO Epoch 23 TRAIN info lr 0.0003610301224185349
2023-02-25 03:26:59,171 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 03:27:00,184 INFO Epoch 22 CV info cv_loss 6.94139384655469
2023-02-25 03:27:00,185 INFO Epoch 23 TRAIN info lr 0.00036101600594890015
2023-02-25 03:27:00,190 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 03:27:00,244 INFO Epoch 22 CV info cv_loss 6.941393847493685
2023-02-25 03:27:00,245 INFO Epoch 23 TRAIN info lr 0.00036107718927970477
2023-02-25 03:27:00,250 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 03:27:01,153 INFO Epoch 22 CV info cv_loss 6.941393847019881
2023-02-25 03:27:01,153 INFO Epoch 23 TRAIN info lr 0.00036104706436814897
2023-02-25 03:27:01,158 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 03:27:01,678 INFO Epoch 22 CV info cv_loss 6.941393847872729
2023-02-25 03:27:01,678 INFO Epoch 23 TRAIN info lr 0.0003610122418366495
2023-02-25 03:27:01,680 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 03:27:02,674 INFO Epoch 22 CV info cv_loss 6.94139384916277
2023-02-25 03:27:02,674 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/22.pt
2023-02-25 03:27:02,772 INFO Epoch 22 CV info cv_loss 6.941393847855499
2023-02-25 03:27:02,772 INFO Epoch 23 TRAIN info lr 0.0003610254167446639
2023-02-25 03:27:02,774 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 03:27:03,309 INFO Epoch 23 TRAIN info lr 0.0003610357694700727
2023-02-25 03:27:03,312 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 03:28:07,727 DEBUG TRAIN Batch 23/0 loss 5.629430 loss_att 5.646269 loss_ctc 7.786952 loss_rnnt 5.184261 hw_loss 0.288997 lr 0.00036102 rank 6
2023-02-25 03:28:07,731 DEBUG TRAIN Batch 23/0 loss 9.058535 loss_att 8.731035 loss_ctc 11.057080 loss_rnnt 8.762852 hw_loss 0.177580 lr 0.00036108 rank 3
2023-02-25 03:28:07,735 DEBUG TRAIN Batch 23/0 loss 6.591059 loss_att 6.717309 loss_ctc 9.158327 loss_rnnt 6.080977 hw_loss 0.267242 lr 0.00036105 rank 4
2023-02-25 03:28:07,736 DEBUG TRAIN Batch 23/0 loss 8.117115 loss_att 7.590738 loss_ctc 10.079375 loss_rnnt 7.864984 hw_loss 0.179572 lr 0.00036099 rank 5
2023-02-25 03:28:07,738 DEBUG TRAIN Batch 23/0 loss 9.490949 loss_att 8.889174 loss_ctc 11.030982 loss_rnnt 9.269143 hw_loss 0.256542 lr 0.00036103 rank 0
2023-02-25 03:28:07,745 DEBUG TRAIN Batch 23/0 loss 8.574303 loss_att 8.475492 loss_ctc 11.512795 loss_rnnt 8.113188 hw_loss 0.167022 lr 0.00036103 rank 1
2023-02-25 03:28:07,746 DEBUG TRAIN Batch 23/0 loss 8.494085 loss_att 8.160876 loss_ctc 9.715624 loss_rnnt 8.245759 hw_loss 0.285179 lr 0.00036102 rank 2
2023-02-25 03:28:07,768 DEBUG TRAIN Batch 23/0 loss 8.388830 loss_att 7.990606 loss_ctc 11.640544 loss_rnnt 7.891987 hw_loss 0.267985 lr 0.00036101 rank 7
2023-02-25 03:29:18,391 DEBUG TRAIN Batch 23/100 loss 8.760053 loss_att 11.211385 loss_ctc 10.431730 loss_rnnt 8.006300 hw_loss 0.076118 lr 0.00036094 rank 0
2023-02-25 03:29:18,394 DEBUG TRAIN Batch 23/100 loss 9.079965 loss_att 9.338691 loss_ctc 13.405783 loss_rnnt 8.300956 hw_loss 0.282167 lr 0.00036092 rank 6
2023-02-25 03:29:18,397 DEBUG TRAIN Batch 23/100 loss 11.371541 loss_att 14.531558 loss_ctc 12.679072 loss_rnnt 10.486038 hw_loss 0.148428 lr 0.00036098 rank 3
2023-02-25 03:29:18,399 DEBUG TRAIN Batch 23/100 loss 8.764436 loss_att 13.025957 loss_ctc 10.119541 loss_rnnt 7.630394 hw_loss 0.189480 lr 0.00036094 rank 1
2023-02-25 03:29:18,399 DEBUG TRAIN Batch 23/100 loss 3.652486 loss_att 5.961167 loss_ctc 5.080166 loss_rnnt 2.924510 hw_loss 0.142278 lr 0.00036090 rank 5
2023-02-25 03:29:18,399 DEBUG TRAIN Batch 23/100 loss 18.443972 loss_att 17.774124 loss_ctc 25.309952 loss_rnnt 17.548115 hw_loss 0.214430 lr 0.00036095 rank 4
2023-02-25 03:29:18,401 DEBUG TRAIN Batch 23/100 loss 9.448524 loss_att 11.701761 loss_ctc 15.038382 loss_rnnt 8.207163 hw_loss 0.085126 lr 0.00036092 rank 7
2023-02-25 03:29:18,402 DEBUG TRAIN Batch 23/100 loss 7.591997 loss_att 11.326303 loss_ctc 10.227938 loss_rnnt 6.429595 hw_loss 0.120153 lr 0.00036093 rank 2
2023-02-25 03:30:28,979 DEBUG TRAIN Batch 23/200 loss 3.465133 loss_att 5.918789 loss_ctc 5.702826 loss_rnnt 2.544804 hw_loss 0.246074 lr 0.00036086 rank 4
2023-02-25 03:30:28,981 DEBUG TRAIN Batch 23/200 loss 1.973488 loss_att 4.741863 loss_ctc 2.558199 loss_rnnt 1.191241 hw_loss 0.282395 lr 0.00036089 rank 3
2023-02-25 03:30:28,982 DEBUG TRAIN Batch 23/200 loss 4.496286 loss_att 5.852758 loss_ctc 7.212142 loss_rnnt 3.767351 hw_loss 0.179113 lr 0.00036084 rank 1
2023-02-25 03:30:28,981 DEBUG TRAIN Batch 23/200 loss 4.473497 loss_att 5.356384 loss_ctc 4.995758 loss_rnnt 4.173425 hw_loss 0.100987 lr 0.00036080 rank 5
2023-02-25 03:30:28,981 DEBUG TRAIN Batch 23/200 loss 5.747223 loss_att 8.472689 loss_ctc 6.576480 loss_rnnt 4.979361 hw_loss 0.210379 lr 0.00036083 rank 6
2023-02-25 03:30:28,982 DEBUG TRAIN Batch 23/200 loss 13.418405 loss_att 16.055273 loss_ctc 19.950012 loss_rnnt 11.926018 hw_loss 0.176498 lr 0.00036085 rank 0
2023-02-25 03:30:28,983 DEBUG TRAIN Batch 23/200 loss 3.203617 loss_att 6.677557 loss_ctc 4.764269 loss_rnnt 2.171443 hw_loss 0.242433 lr 0.00036082 rank 7
2023-02-25 03:30:28,986 DEBUG TRAIN Batch 23/200 loss 7.560307 loss_att 11.593840 loss_ctc 11.695438 loss_rnnt 6.063505 hw_loss 0.260144 lr 0.00036084 rank 2
2023-02-25 03:31:40,009 DEBUG TRAIN Batch 23/300 loss 5.394730 loss_att 8.287354 loss_ctc 7.071911 loss_rnnt 4.497706 hw_loss 0.177890 lr 0.00036075 rank 0
2023-02-25 03:31:40,019 DEBUG TRAIN Batch 23/300 loss 14.515471 loss_att 16.260727 loss_ctc 19.655622 loss_rnnt 13.361878 hw_loss 0.223478 lr 0.00036079 rank 3
2023-02-25 03:31:40,025 DEBUG TRAIN Batch 23/300 loss 13.076404 loss_att 16.515429 loss_ctc 17.735609 loss_rnnt 11.673037 hw_loss 0.176876 lr 0.00036075 rank 1
2023-02-25 03:31:40,025 DEBUG TRAIN Batch 23/300 loss 4.360015 loss_att 6.278244 loss_ctc 4.601394 loss_rnnt 3.778424 hw_loss 0.310803 lr 0.00036076 rank 4
2023-02-25 03:31:40,026 DEBUG TRAIN Batch 23/300 loss 19.133410 loss_att 26.501881 loss_ctc 29.597855 loss_rnnt 16.142962 hw_loss 0.227800 lr 0.00036074 rank 2
2023-02-25 03:31:40,028 DEBUG TRAIN Batch 23/300 loss 12.220011 loss_att 12.201576 loss_ctc 13.717251 loss_rnnt 11.909829 hw_loss 0.214193 lr 0.00036073 rank 6
2023-02-25 03:31:40,028 DEBUG TRAIN Batch 23/300 loss 9.369439 loss_att 15.020142 loss_ctc 11.310877 loss_rnnt 7.893858 hw_loss 0.162341 lr 0.00036073 rank 7
2023-02-25 03:31:40,050 DEBUG TRAIN Batch 23/300 loss 7.447264 loss_att 8.971354 loss_ctc 8.460459 loss_rnnt 6.894901 hw_loss 0.210847 lr 0.00036071 rank 5
2023-02-25 03:32:54,235 DEBUG TRAIN Batch 23/400 loss 8.829829 loss_att 12.692855 loss_ctc 15.752475 loss_rnnt 6.988120 hw_loss 0.273907 lr 0.00036064 rank 6
2023-02-25 03:32:54,238 DEBUG TRAIN Batch 23/400 loss 3.841278 loss_att 5.615304 loss_ctc 5.294746 loss_rnnt 3.236826 hw_loss 0.104720 lr 0.00036066 rank 0
2023-02-25 03:32:54,238 DEBUG TRAIN Batch 23/400 loss 5.077436 loss_att 6.595454 loss_ctc 6.278365 loss_rnnt 4.502654 hw_loss 0.208227 lr 0.00036064 rank 7
2023-02-25 03:32:54,238 DEBUG TRAIN Batch 23/400 loss 9.514988 loss_att 10.932880 loss_ctc 12.712921 loss_rnnt 8.693883 hw_loss 0.208377 lr 0.00036065 rank 2
2023-02-25 03:32:54,240 DEBUG TRAIN Batch 23/400 loss 7.551570 loss_att 10.092156 loss_ctc 9.682543 loss_rnnt 6.657418 hw_loss 0.191071 lr 0.00036065 rank 1
2023-02-25 03:32:54,242 DEBUG TRAIN Batch 23/400 loss 5.765785 loss_att 8.107773 loss_ctc 8.656619 loss_rnnt 4.810516 hw_loss 0.190174 lr 0.00036067 rank 4
2023-02-25 03:32:54,246 DEBUG TRAIN Batch 23/400 loss 20.412489 loss_att 22.883411 loss_ctc 27.544827 loss_rnnt 18.901234 hw_loss 0.123921 lr 0.00036070 rank 3
2023-02-25 03:32:54,248 DEBUG TRAIN Batch 23/400 loss 7.104519 loss_att 8.574265 loss_ctc 10.229099 loss_rnnt 6.233231 hw_loss 0.301364 lr 0.00036061 rank 5
2023-02-25 03:34:04,361 DEBUG TRAIN Batch 23/500 loss 8.500658 loss_att 10.595776 loss_ctc 11.900200 loss_rnnt 7.516339 hw_loss 0.210042 lr 0.00036057 rank 0
2023-02-25 03:34:04,366 DEBUG TRAIN Batch 23/500 loss 8.889527 loss_att 8.768258 loss_ctc 8.788618 loss_rnnt 8.763098 hw_loss 0.307758 lr 0.00036054 rank 7
2023-02-25 03:34:04,367 DEBUG TRAIN Batch 23/500 loss 5.398870 loss_att 8.402058 loss_ctc 8.094167 loss_rnnt 4.338206 hw_loss 0.188726 lr 0.00036055 rank 6
2023-02-25 03:34:04,367 DEBUG TRAIN Batch 23/500 loss 8.231772 loss_att 10.121832 loss_ctc 10.327158 loss_rnnt 7.473306 hw_loss 0.189505 lr 0.00036058 rank 4
2023-02-25 03:34:04,371 DEBUG TRAIN Batch 23/500 loss 11.557261 loss_att 15.005776 loss_ctc 19.080839 loss_rnnt 9.744150 hw_loss 0.225495 lr 0.00036061 rank 3
2023-02-25 03:34:04,373 DEBUG TRAIN Batch 23/500 loss 16.635746 loss_att 17.048973 loss_ctc 24.728153 loss_rnnt 15.320099 hw_loss 0.288779 lr 0.00036056 rank 1
2023-02-25 03:34:04,377 DEBUG TRAIN Batch 23/500 loss 11.593541 loss_att 13.895824 loss_ctc 17.395010 loss_rnnt 10.261539 hw_loss 0.183779 lr 0.00036052 rank 5
2023-02-25 03:34:04,376 DEBUG TRAIN Batch 23/500 loss 10.150335 loss_att 13.487779 loss_ctc 12.803468 loss_rnnt 9.090882 hw_loss 0.071649 lr 0.00036055 rank 2
2023-02-25 03:35:15,264 DEBUG TRAIN Batch 23/600 loss 11.106689 loss_att 13.434959 loss_ctc 17.075184 loss_rnnt 9.704766 hw_loss 0.263380 lr 0.00036051 rank 3
2023-02-25 03:35:15,265 DEBUG TRAIN Batch 23/600 loss 5.217144 loss_att 6.865729 loss_ctc 8.949318 loss_rnnt 4.275508 hw_loss 0.214304 lr 0.00036047 rank 1
2023-02-25 03:35:15,267 DEBUG TRAIN Batch 23/600 loss 5.461893 loss_att 7.079762 loss_ctc 8.021286 loss_rnnt 4.637359 hw_loss 0.299451 lr 0.00036047 rank 0
2023-02-25 03:35:15,271 DEBUG TRAIN Batch 23/600 loss 13.340316 loss_att 15.730117 loss_ctc 16.600386 loss_rnnt 12.270862 hw_loss 0.294035 lr 0.00036045 rank 7
2023-02-25 03:35:15,273 DEBUG TRAIN Batch 23/600 loss 5.012540 loss_att 6.854392 loss_ctc 9.223221 loss_rnnt 3.937021 hw_loss 0.273233 lr 0.00036045 rank 6
2023-02-25 03:35:15,274 DEBUG TRAIN Batch 23/600 loss 5.034286 loss_att 5.695373 loss_ctc 6.869371 loss_rnnt 4.502835 hw_loss 0.289792 lr 0.00036043 rank 5
2023-02-25 03:35:15,274 DEBUG TRAIN Batch 23/600 loss 13.449389 loss_att 14.003418 loss_ctc 15.067801 loss_rnnt 12.964341 hw_loss 0.297101 lr 0.00036048 rank 4
2023-02-25 03:35:15,274 DEBUG TRAIN Batch 23/600 loss 2.725907 loss_att 4.926908 loss_ctc 4.166764 loss_rnnt 1.984261 hw_loss 0.204996 lr 0.00036046 rank 2
2023-02-25 03:36:28,849 DEBUG TRAIN Batch 23/700 loss 7.743935 loss_att 9.653014 loss_ctc 12.860680 loss_rnnt 6.602035 hw_loss 0.145973 lr 0.00036035 rank 7
2023-02-25 03:36:28,858 DEBUG TRAIN Batch 23/700 loss 14.264514 loss_att 16.648184 loss_ctc 21.182205 loss_rnnt 12.750956 hw_loss 0.214623 lr 0.00036038 rank 0
2023-02-25 03:36:28,859 DEBUG TRAIN Batch 23/700 loss 7.659033 loss_att 13.004442 loss_ctc 12.178859 loss_rnnt 5.917073 hw_loss 0.131688 lr 0.00036042 rank 3
2023-02-25 03:36:28,866 DEBUG TRAIN Batch 23/700 loss 3.393776 loss_att 6.506055 loss_ctc 5.416585 loss_rnnt 2.418667 hw_loss 0.155522 lr 0.00036036 rank 6
2023-02-25 03:36:28,867 DEBUG TRAIN Batch 23/700 loss 5.295300 loss_att 7.959990 loss_ctc 7.682250 loss_rnnt 4.298048 hw_loss 0.273851 lr 0.00036039 rank 4
2023-02-25 03:36:28,873 DEBUG TRAIN Batch 23/700 loss 4.619767 loss_att 7.982165 loss_ctc 6.047562 loss_rnnt 3.678531 hw_loss 0.146970 lr 0.00036037 rank 1
2023-02-25 03:36:28,874 DEBUG TRAIN Batch 23/700 loss 13.059476 loss_att 15.365458 loss_ctc 14.920081 loss_rnnt 12.222918 hw_loss 0.238650 lr 0.00036033 rank 5
2023-02-25 03:36:28,887 DEBUG TRAIN Batch 23/700 loss 8.945374 loss_att 12.378503 loss_ctc 12.538589 loss_rnnt 7.602649 hw_loss 0.331885 lr 0.00036037 rank 2
2023-02-25 03:37:40,247 DEBUG TRAIN Batch 23/800 loss 3.925467 loss_att 7.733535 loss_ctc 7.011676 loss_rnnt 2.626947 hw_loss 0.235148 lr 0.00036028 rank 0
2023-02-25 03:37:40,248 DEBUG TRAIN Batch 23/800 loss 6.218413 loss_att 8.254816 loss_ctc 9.366620 loss_rnnt 5.305140 hw_loss 0.161683 lr 0.00036026 rank 7
2023-02-25 03:37:40,250 DEBUG TRAIN Batch 23/800 loss 6.886123 loss_att 9.655588 loss_ctc 8.246159 loss_rnnt 6.062588 hw_loss 0.165569 lr 0.00036024 rank 5
2023-02-25 03:37:40,250 DEBUG TRAIN Batch 23/800 loss 10.931057 loss_att 14.454443 loss_ctc 16.511154 loss_rnnt 9.376697 hw_loss 0.198130 lr 0.00036033 rank 3
2023-02-25 03:37:40,251 DEBUG TRAIN Batch 23/800 loss 9.545566 loss_att 11.000090 loss_ctc 12.062729 loss_rnnt 8.824282 hw_loss 0.177669 lr 0.00036026 rank 6
2023-02-25 03:37:40,256 DEBUG TRAIN Batch 23/800 loss 4.427168 loss_att 5.997775 loss_ctc 5.822509 loss_rnnt 3.806865 hw_loss 0.225254 lr 0.00036030 rank 4
2023-02-25 03:37:40,258 DEBUG TRAIN Batch 23/800 loss 21.044943 loss_att 25.295162 loss_ctc 25.738804 loss_rnnt 19.489330 hw_loss 0.149473 lr 0.00036027 rank 2
2023-02-25 03:37:40,260 DEBUG TRAIN Batch 23/800 loss 6.972957 loss_att 8.461159 loss_ctc 7.418261 loss_rnnt 6.557789 hw_loss 0.109039 lr 0.00036028 rank 1
2023-02-25 03:38:50,487 DEBUG TRAIN Batch 23/900 loss 7.444337 loss_att 9.357909 loss_ctc 13.448038 loss_rnnt 6.179970 hw_loss 0.152173 lr 0.00036019 rank 0
2023-02-25 03:38:50,488 DEBUG TRAIN Batch 23/900 loss 7.786452 loss_att 9.870507 loss_ctc 10.204113 loss_rnnt 6.970209 hw_loss 0.144519 lr 0.00036023 rank 3
2023-02-25 03:38:50,488 DEBUG TRAIN Batch 23/900 loss 12.741894 loss_att 15.506935 loss_ctc 19.107319 loss_rnnt 11.188814 hw_loss 0.283776 lr 0.00036019 rank 1
2023-02-25 03:38:50,487 DEBUG TRAIN Batch 23/900 loss 13.986670 loss_att 14.933743 loss_ctc 22.287231 loss_rnnt 12.598804 hw_loss 0.171955 lr 0.00036020 rank 4
2023-02-25 03:38:50,488 DEBUG TRAIN Batch 23/900 loss 6.576481 loss_att 7.396272 loss_ctc 9.703315 loss_rnnt 5.917722 hw_loss 0.146042 lr 0.00036015 rank 5
2023-02-25 03:38:50,491 DEBUG TRAIN Batch 23/900 loss 4.370966 loss_att 5.668462 loss_ctc 5.110968 loss_rnnt 3.944071 hw_loss 0.128867 lr 0.00036017 rank 7
2023-02-25 03:38:50,492 DEBUG TRAIN Batch 23/900 loss 10.806730 loss_att 15.801905 loss_ctc 16.946415 loss_rnnt 8.817047 hw_loss 0.322545 lr 0.00036017 rank 6
2023-02-25 03:38:50,493 DEBUG TRAIN Batch 23/900 loss 8.090157 loss_att 10.567883 loss_ctc 11.086527 loss_rnnt 7.068062 hw_loss 0.238187 lr 0.00036018 rank 2
2023-02-25 03:40:02,893 DEBUG TRAIN Batch 23/1000 loss 11.817858 loss_att 14.402455 loss_ctc 18.638952 loss_rnnt 10.285499 hw_loss 0.198673 lr 0.00036010 rank 0
2023-02-25 03:40:02,893 DEBUG TRAIN Batch 23/1000 loss 6.583827 loss_att 10.190032 loss_ctc 8.330806 loss_rnnt 5.547965 hw_loss 0.153170 lr 0.00036007 rank 7
2023-02-25 03:40:02,899 DEBUG TRAIN Batch 23/1000 loss 7.122209 loss_att 11.915052 loss_ctc 10.867702 loss_rnnt 5.599439 hw_loss 0.121504 lr 0.00036011 rank 4
2023-02-25 03:40:02,900 DEBUG TRAIN Batch 23/1000 loss 10.993905 loss_att 15.590002 loss_ctc 18.019131 loss_rnnt 9.070106 hw_loss 0.127281 lr 0.00036009 rank 1
2023-02-25 03:40:02,900 DEBUG TRAIN Batch 23/1000 loss 18.616428 loss_att 19.738325 loss_ctc 21.409111 loss_rnnt 17.927666 hw_loss 0.172550 lr 0.00036009 rank 2
2023-02-25 03:40:02,900 DEBUG TRAIN Batch 23/1000 loss 9.855241 loss_att 13.372438 loss_ctc 15.329056 loss_rnnt 8.343870 hw_loss 0.146417 lr 0.00036008 rank 6
2023-02-25 03:40:02,908 DEBUG TRAIN Batch 23/1000 loss 5.773504 loss_att 9.474711 loss_ctc 10.013802 loss_rnnt 4.362887 hw_loss 0.196880 lr 0.00036014 rank 3
2023-02-25 03:40:02,912 DEBUG TRAIN Batch 23/1000 loss 5.646809 loss_att 8.146889 loss_ctc 8.263460 loss_rnnt 4.680440 hw_loss 0.220248 lr 0.00036005 rank 5
2023-02-25 03:41:15,700 DEBUG TRAIN Batch 23/1100 loss 7.032233 loss_att 7.678709 loss_ctc 10.520082 loss_rnnt 6.313008 hw_loss 0.234157 lr 0.00036000 rank 0
2023-02-25 03:41:15,703 DEBUG TRAIN Batch 23/1100 loss 4.309908 loss_att 7.369592 loss_ctc 6.562487 loss_rnnt 3.294501 hw_loss 0.193363 lr 0.00035998 rank 7
2023-02-25 03:41:15,703 DEBUG TRAIN Batch 23/1100 loss 14.498143 loss_att 13.989829 loss_ctc 22.096003 loss_rnnt 13.466101 hw_loss 0.226233 lr 0.00036005 rank 3
2023-02-25 03:41:15,705 DEBUG TRAIN Batch 23/1100 loss 8.458970 loss_att 12.276293 loss_ctc 13.312330 loss_rnnt 6.950776 hw_loss 0.183027 lr 0.00035998 rank 6
2023-02-25 03:41:15,707 DEBUG TRAIN Batch 23/1100 loss 9.812272 loss_att 13.618238 loss_ctc 15.328761 loss_rnnt 8.245943 hw_loss 0.130508 lr 0.00035996 rank 5
2023-02-25 03:41:15,710 DEBUG TRAIN Batch 23/1100 loss 7.852960 loss_att 11.920979 loss_ctc 11.055549 loss_rnnt 6.528864 hw_loss 0.156523 lr 0.00036002 rank 4
2023-02-25 03:41:15,711 DEBUG TRAIN Batch 23/1100 loss 8.411386 loss_att 9.976971 loss_ctc 10.874508 loss_rnnt 7.670767 hw_loss 0.185789 lr 0.00036000 rank 1
2023-02-25 03:41:15,712 DEBUG TRAIN Batch 23/1100 loss 9.945122 loss_att 13.069088 loss_ctc 15.400588 loss_rnnt 8.456952 hw_loss 0.254963 lr 0.00035999 rank 2
2023-02-25 03:42:25,879 DEBUG TRAIN Batch 23/1200 loss 7.205904 loss_att 10.797802 loss_ctc 10.633379 loss_rnnt 5.942774 hw_loss 0.164538 lr 0.00035995 rank 3
2023-02-25 03:42:25,881 DEBUG TRAIN Batch 23/1200 loss 7.440780 loss_att 9.195557 loss_ctc 10.359165 loss_rnnt 6.533647 hw_loss 0.313237 lr 0.00035991 rank 0
2023-02-25 03:42:25,881 DEBUG TRAIN Batch 23/1200 loss 8.875651 loss_att 12.168303 loss_ctc 13.960944 loss_rnnt 7.412682 hw_loss 0.236999 lr 0.00035989 rank 6
2023-02-25 03:42:25,882 DEBUG TRAIN Batch 23/1200 loss 11.833607 loss_att 13.303472 loss_ctc 13.491536 loss_rnnt 11.235639 hw_loss 0.155508 lr 0.00035991 rank 1
2023-02-25 03:42:25,883 DEBUG TRAIN Batch 23/1200 loss 6.913018 loss_att 10.303963 loss_ctc 12.101936 loss_rnnt 5.444406 hw_loss 0.184815 lr 0.00035992 rank 4
2023-02-25 03:42:25,888 DEBUG TRAIN Batch 23/1200 loss 6.054410 loss_att 7.970764 loss_ctc 11.692072 loss_rnnt 4.765715 hw_loss 0.288257 lr 0.00035987 rank 5
2023-02-25 03:42:25,888 DEBUG TRAIN Batch 23/1200 loss 7.875011 loss_att 9.662460 loss_ctc 11.885848 loss_rnnt 6.864059 hw_loss 0.222533 lr 0.00035990 rank 2
2023-02-25 03:42:25,889 DEBUG TRAIN Batch 23/1200 loss 10.386418 loss_att 13.597438 loss_ctc 12.000603 loss_rnnt 9.408259 hw_loss 0.226372 lr 0.00035989 rank 7
2023-02-25 03:43:38,439 DEBUG TRAIN Batch 23/1300 loss 4.588518 loss_att 7.040035 loss_ctc 6.687730 loss_rnnt 3.706514 hw_loss 0.209635 lr 0.00035986 rank 3
2023-02-25 03:43:38,441 DEBUG TRAIN Batch 23/1300 loss 6.407428 loss_att 5.736976 loss_ctc 9.477933 loss_rnnt 5.928115 hw_loss 0.382504 lr 0.00035979 rank 7
2023-02-25 03:43:38,441 DEBUG TRAIN Batch 23/1300 loss 9.789025 loss_att 9.749231 loss_ctc 12.565761 loss_rnnt 9.308337 hw_loss 0.222027 lr 0.00035977 rank 5
2023-02-25 03:43:38,442 DEBUG TRAIN Batch 23/1300 loss 2.176041 loss_att 5.119356 loss_ctc 3.846139 loss_rnnt 1.239252 hw_loss 0.235212 lr 0.00035980 rank 6
2023-02-25 03:43:38,443 DEBUG TRAIN Batch 23/1300 loss 9.063663 loss_att 10.417821 loss_ctc 9.807302 loss_rnnt 8.611605 hw_loss 0.153893 lr 0.00035982 rank 0
2023-02-25 03:43:38,443 DEBUG TRAIN Batch 23/1300 loss 5.249722 loss_att 5.735454 loss_ctc 6.092470 loss_rnnt 4.827783 hw_loss 0.398300 lr 0.00035983 rank 4
2023-02-25 03:43:38,445 DEBUG TRAIN Batch 23/1300 loss 7.594803 loss_att 7.557724 loss_ctc 9.545325 loss_rnnt 7.165642 hw_loss 0.330950 lr 0.00035981 rank 1
2023-02-25 03:43:38,445 DEBUG TRAIN Batch 23/1300 loss 6.441764 loss_att 7.548939 loss_ctc 9.060092 loss_rnnt 5.796810 hw_loss 0.139517 lr 0.00035981 rank 2
2023-02-25 03:44:52,167 DEBUG TRAIN Batch 23/1400 loss 7.284913 loss_att 10.967770 loss_ctc 7.706327 loss_rnnt 6.405767 hw_loss 0.161975 lr 0.00035971 rank 2
2023-02-25 03:44:52,168 DEBUG TRAIN Batch 23/1400 loss 7.580334 loss_att 10.553748 loss_ctc 11.727043 loss_rnnt 6.354519 hw_loss 0.146697 lr 0.00035977 rank 3
2023-02-25 03:44:52,175 DEBUG TRAIN Batch 23/1400 loss 4.471062 loss_att 6.100584 loss_ctc 6.562968 loss_rnnt 3.775876 hw_loss 0.169427 lr 0.00035972 rank 0
2023-02-25 03:44:52,180 DEBUG TRAIN Batch 23/1400 loss 9.989021 loss_att 11.531195 loss_ctc 10.106035 loss_rnnt 9.522268 hw_loss 0.267594 lr 0.00035974 rank 4
2023-02-25 03:44:52,180 DEBUG TRAIN Batch 23/1400 loss 14.875011 loss_att 21.333862 loss_ctc 29.581749 loss_rnnt 11.463177 hw_loss 0.298437 lr 0.00035970 rank 7
2023-02-25 03:44:52,182 DEBUG TRAIN Batch 23/1400 loss 25.724354 loss_att 25.175388 loss_ctc 35.051266 loss_rnnt 24.510199 hw_loss 0.150676 lr 0.00035972 rank 1
2023-02-25 03:44:52,184 DEBUG TRAIN Batch 23/1400 loss 13.233944 loss_att 15.005131 loss_ctc 15.406311 loss_rnnt 12.438393 hw_loss 0.284370 lr 0.00035970 rank 6
2023-02-25 03:44:52,185 DEBUG TRAIN Batch 23/1400 loss 6.013327 loss_att 7.169239 loss_ctc 7.802026 loss_rnnt 5.421844 hw_loss 0.228387 lr 0.00035968 rank 5
2023-02-25 03:46:04,567 DEBUG TRAIN Batch 23/1500 loss 10.317568 loss_att 17.899471 loss_ctc 18.153967 loss_rnnt 7.632973 hw_loss 0.231303 lr 0.00035963 rank 0
2023-02-25 03:46:04,569 DEBUG TRAIN Batch 23/1500 loss 6.826796 loss_att 8.430826 loss_ctc 8.873388 loss_rnnt 6.136257 hw_loss 0.181601 lr 0.00035963 rank 1
2023-02-25 03:46:04,572 DEBUG TRAIN Batch 23/1500 loss 2.910655 loss_att 5.691094 loss_ctc 4.675389 loss_rnnt 1.975756 hw_loss 0.269087 lr 0.00035961 rank 7
2023-02-25 03:46:04,574 DEBUG TRAIN Batch 23/1500 loss 15.256598 loss_att 14.936928 loss_ctc 19.076321 loss_rnnt 14.741272 hw_loss 0.131182 lr 0.00035967 rank 3
2023-02-25 03:46:04,576 DEBUG TRAIN Batch 23/1500 loss 5.132360 loss_att 9.712959 loss_ctc 6.862925 loss_rnnt 3.820655 hw_loss 0.309083 lr 0.00035962 rank 2
2023-02-25 03:46:04,578 DEBUG TRAIN Batch 23/1500 loss 7.352469 loss_att 8.337292 loss_ctc 9.341942 loss_rnnt 6.772608 hw_loss 0.220564 lr 0.00035961 rank 6
2023-02-25 03:46:04,582 DEBUG TRAIN Batch 23/1500 loss 5.120436 loss_att 7.580746 loss_ctc 7.864309 loss_rnnt 4.120156 hw_loss 0.266939 lr 0.00035959 rank 5
2023-02-25 03:46:04,623 DEBUG TRAIN Batch 23/1500 loss 2.693216 loss_att 5.151677 loss_ctc 3.078843 loss_rnnt 1.998035 hw_loss 0.285134 lr 0.00035964 rank 4
2023-02-25 03:47:13,954 DEBUG TRAIN Batch 23/1600 loss 12.192744 loss_att 15.736446 loss_ctc 16.692245 loss_rnnt 10.803115 hw_loss 0.151791 lr 0.00035954 rank 0
2023-02-25 03:47:13,961 DEBUG TRAIN Batch 23/1600 loss 14.932205 loss_att 19.118008 loss_ctc 20.899950 loss_rnnt 13.187691 hw_loss 0.209351 lr 0.00035953 rank 1
2023-02-25 03:47:13,961 DEBUG TRAIN Batch 23/1600 loss 10.195929 loss_att 12.967793 loss_ctc 13.616697 loss_rnnt 9.068436 hw_loss 0.219406 lr 0.00035958 rank 3
2023-02-25 03:47:13,962 DEBUG TRAIN Batch 23/1600 loss 11.380545 loss_att 13.769093 loss_ctc 13.687677 loss_rnnt 10.455553 hw_loss 0.261872 lr 0.00035955 rank 4
2023-02-25 03:47:13,962 DEBUG TRAIN Batch 23/1600 loss 8.426754 loss_att 11.704136 loss_ctc 14.602051 loss_rnnt 6.875759 hw_loss 0.135274 lr 0.00035952 rank 6
2023-02-25 03:47:13,965 DEBUG TRAIN Batch 23/1600 loss 9.678734 loss_att 10.819203 loss_ctc 13.460640 loss_rnnt 8.868164 hw_loss 0.146665 lr 0.00035953 rank 2
2023-02-25 03:47:13,966 DEBUG TRAIN Batch 23/1600 loss 9.141492 loss_att 11.610164 loss_ctc 10.901881 loss_rnnt 8.340435 hw_loss 0.136132 lr 0.00035952 rank 7
2023-02-25 03:47:14,009 DEBUG TRAIN Batch 23/1600 loss 9.377216 loss_att 10.770728 loss_ctc 13.954350 loss_rnnt 8.387650 hw_loss 0.188584 lr 0.00035949 rank 5
2023-02-25 03:48:24,510 DEBUG TRAIN Batch 23/1700 loss 7.990673 loss_att 9.997278 loss_ctc 12.220505 loss_rnnt 6.896813 hw_loss 0.241051 lr 0.00035942 rank 7
2023-02-25 03:48:24,519 DEBUG TRAIN Batch 23/1700 loss 18.755295 loss_att 20.910864 loss_ctc 26.053923 loss_rnnt 17.228979 hw_loss 0.228848 lr 0.00035949 rank 3
2023-02-25 03:48:24,521 DEBUG TRAIN Batch 23/1700 loss 8.775705 loss_att 9.159994 loss_ctc 13.104460 loss_rnnt 7.940784 hw_loss 0.339179 lr 0.00035944 rank 2
2023-02-25 03:48:24,527 DEBUG TRAIN Batch 23/1700 loss 11.658093 loss_att 15.453073 loss_ctc 14.410259 loss_rnnt 10.479163 hw_loss 0.099337 lr 0.00035945 rank 0
2023-02-25 03:48:24,527 DEBUG TRAIN Batch 23/1700 loss 16.077629 loss_att 15.006956 loss_ctc 22.762636 loss_rnnt 15.303939 hw_loss 0.180924 lr 0.00035940 rank 5
2023-02-25 03:48:24,528 DEBUG TRAIN Batch 23/1700 loss 6.519079 loss_att 8.631210 loss_ctc 9.348755 loss_rnnt 5.582589 hw_loss 0.256451 lr 0.00035944 rank 1
2023-02-25 03:48:24,529 DEBUG TRAIN Batch 23/1700 loss 15.809149 loss_att 20.147894 loss_ctc 24.184204 loss_rnnt 13.715639 hw_loss 0.204535 lr 0.00035943 rank 6
2023-02-25 03:48:24,568 DEBUG TRAIN Batch 23/1700 loss 10.258097 loss_att 14.469837 loss_ctc 13.700507 loss_rnnt 8.877207 hw_loss 0.149162 lr 0.00035946 rank 4
2023-02-25 03:49:37,709 DEBUG TRAIN Batch 23/1800 loss 7.366303 loss_att 9.974693 loss_ctc 11.132895 loss_rnnt 6.226664 hw_loss 0.217031 lr 0.00035935 rank 0
2023-02-25 03:49:37,711 DEBUG TRAIN Batch 23/1800 loss 18.380785 loss_att 19.171925 loss_ctc 28.361826 loss_rnnt 16.790434 hw_loss 0.189972 lr 0.00035939 rank 3
2023-02-25 03:49:37,712 DEBUG TRAIN Batch 23/1800 loss 6.116583 loss_att 7.804789 loss_ctc 7.886693 loss_rnnt 5.422051 hw_loss 0.226644 lr 0.00035935 rank 1
2023-02-25 03:49:37,714 DEBUG TRAIN Batch 23/1800 loss 11.355244 loss_att 15.611805 loss_ctc 18.445343 loss_rnnt 9.431336 hw_loss 0.238590 lr 0.00035933 rank 7
2023-02-25 03:49:37,715 DEBUG TRAIN Batch 23/1800 loss 8.944824 loss_att 12.285856 loss_ctc 15.957026 loss_rnnt 7.230899 hw_loss 0.207672 lr 0.00035933 rank 6
2023-02-25 03:49:37,719 DEBUG TRAIN Batch 23/1800 loss 14.997507 loss_att 17.481365 loss_ctc 20.928465 loss_rnnt 13.601546 hw_loss 0.203240 lr 0.00035934 rank 2
2023-02-25 03:49:37,720 DEBUG TRAIN Batch 23/1800 loss 14.052848 loss_att 16.267708 loss_ctc 23.324095 loss_rnnt 12.260018 hw_loss 0.213169 lr 0.00035931 rank 5
2023-02-25 03:49:37,720 DEBUG TRAIN Batch 23/1800 loss 11.454793 loss_att 12.992990 loss_ctc 15.906547 loss_rnnt 10.458357 hw_loss 0.178554 lr 0.00035936 rank 4
2023-02-25 03:50:48,298 DEBUG TRAIN Batch 23/1900 loss 8.367809 loss_att 8.529448 loss_ctc 11.677463 loss_rnnt 7.786247 hw_loss 0.202403 lr 0.00035924 rank 6
2023-02-25 03:50:48,301 DEBUG TRAIN Batch 23/1900 loss 6.685342 loss_att 8.714439 loss_ctc 8.512373 loss_rnnt 5.926707 hw_loss 0.204771 lr 0.00035924 rank 7
2023-02-25 03:50:48,302 DEBUG TRAIN Batch 23/1900 loss 11.151980 loss_att 12.468007 loss_ctc 12.621541 loss_rnnt 10.570752 hw_loss 0.228903 lr 0.00035927 rank 4
2023-02-25 03:50:48,303 DEBUG TRAIN Batch 23/1900 loss 8.484520 loss_att 9.337085 loss_ctc 10.207829 loss_rnnt 7.955230 hw_loss 0.241880 lr 0.00035925 rank 1
2023-02-25 03:50:48,304 DEBUG TRAIN Batch 23/1900 loss 4.971027 loss_att 6.224861 loss_ctc 6.689008 loss_rnnt 4.394089 hw_loss 0.182077 lr 0.00035926 rank 0
2023-02-25 03:50:48,306 DEBUG TRAIN Batch 23/1900 loss 8.506211 loss_att 10.636999 loss_ctc 11.947763 loss_rnnt 7.483774 hw_loss 0.257638 lr 0.00035925 rank 2
2023-02-25 03:50:48,310 DEBUG TRAIN Batch 23/1900 loss 6.662490 loss_att 9.240883 loss_ctc 10.315775 loss_rnnt 5.552102 hw_loss 0.201759 lr 0.00035930 rank 3
2023-02-25 03:50:48,317 DEBUG TRAIN Batch 23/1900 loss 5.812277 loss_att 9.734227 loss_ctc 8.419759 loss_rnnt 4.611035 hw_loss 0.129727 lr 0.00035922 rank 5
2023-02-25 03:51:59,165 DEBUG TRAIN Batch 23/2000 loss 9.146678 loss_att 15.024591 loss_ctc 15.266201 loss_rnnt 7.049099 hw_loss 0.198863 lr 0.00035921 rank 3
2023-02-25 03:51:59,169 DEBUG TRAIN Batch 23/2000 loss 4.951093 loss_att 6.855730 loss_ctc 8.347363 loss_rnnt 4.035579 hw_loss 0.153281 lr 0.00035916 rank 2
2023-02-25 03:51:59,169 DEBUG TRAIN Batch 23/2000 loss 6.367516 loss_att 8.736740 loss_ctc 10.808205 loss_rnnt 5.250542 hw_loss 0.095694 lr 0.00035917 rank 0
2023-02-25 03:51:59,171 DEBUG TRAIN Batch 23/2000 loss 11.373674 loss_att 15.571442 loss_ctc 17.271946 loss_rnnt 9.673358 hw_loss 0.139362 lr 0.00035916 rank 1
2023-02-25 03:51:59,171 DEBUG TRAIN Batch 23/2000 loss 15.313355 loss_att 16.061167 loss_ctc 18.111755 loss_rnnt 14.687564 hw_loss 0.193332 lr 0.00035915 rank 6
2023-02-25 03:51:59,172 DEBUG TRAIN Batch 23/2000 loss 13.358147 loss_att 15.516798 loss_ctc 18.070992 loss_rnnt 12.192478 hw_loss 0.197920 lr 0.00035918 rank 4
2023-02-25 03:51:59,176 DEBUG TRAIN Batch 23/2000 loss 12.046177 loss_att 13.464027 loss_ctc 18.387735 loss_rnnt 10.852827 hw_loss 0.120449 lr 0.00035914 rank 7
2023-02-25 03:51:59,179 DEBUG TRAIN Batch 23/2000 loss 6.291809 loss_att 10.258820 loss_ctc 9.498475 loss_rnnt 4.978058 hw_loss 0.173987 lr 0.00035912 rank 5
2023-02-25 03:53:11,562 DEBUG TRAIN Batch 23/2100 loss 2.721531 loss_att 6.055687 loss_ctc 4.951524 loss_rnnt 1.711833 hw_loss 0.085376 lr 0.00035907 rank 0
2023-02-25 03:53:11,564 DEBUG TRAIN Batch 23/2100 loss 9.133864 loss_att 12.685242 loss_ctc 11.488873 loss_rnnt 8.003016 hw_loss 0.199824 lr 0.00035903 rank 5
2023-02-25 03:53:11,565 DEBUG TRAIN Batch 23/2100 loss 7.931428 loss_att 9.459494 loss_ctc 8.233006 loss_rnnt 7.499082 hw_loss 0.162230 lr 0.00035907 rank 1
2023-02-25 03:53:11,565 DEBUG TRAIN Batch 23/2100 loss 9.473663 loss_att 11.775128 loss_ctc 13.862033 loss_rnnt 8.349177 hw_loss 0.148271 lr 0.00035905 rank 6
2023-02-25 03:53:11,571 DEBUG TRAIN Batch 23/2100 loss 10.523892 loss_att 13.654604 loss_ctc 11.354930 loss_rnnt 9.740256 hw_loss 0.087540 lr 0.00035906 rank 2
2023-02-25 03:53:11,571 DEBUG TRAIN Batch 23/2100 loss 8.912139 loss_att 13.583486 loss_ctc 13.564990 loss_rnnt 7.325462 hw_loss 0.060052 lr 0.00035912 rank 3
2023-02-25 03:53:11,571 DEBUG TRAIN Batch 23/2100 loss 7.673159 loss_att 9.674715 loss_ctc 10.485750 loss_rnnt 6.786951 hw_loss 0.207910 lr 0.00035909 rank 4
2023-02-25 03:53:11,577 DEBUG TRAIN Batch 23/2100 loss 10.376870 loss_att 14.451113 loss_ctc 10.950885 loss_rnnt 9.424822 hw_loss 0.113746 lr 0.00035905 rank 7
2023-02-25 03:54:22,660 DEBUG TRAIN Batch 23/2200 loss 8.642632 loss_att 9.470240 loss_ctc 11.474197 loss_rnnt 7.967349 hw_loss 0.247912 lr 0.00035902 rank 3
2023-02-25 03:54:22,664 DEBUG TRAIN Batch 23/2200 loss 13.631036 loss_att 18.360870 loss_ctc 21.326448 loss_rnnt 11.546045 hw_loss 0.211815 lr 0.00035896 rank 6
2023-02-25 03:54:22,666 DEBUG TRAIN Batch 23/2200 loss 8.303843 loss_att 11.396713 loss_ctc 11.412416 loss_rnnt 7.187371 hw_loss 0.156415 lr 0.00035896 rank 7
2023-02-25 03:54:22,667 DEBUG TRAIN Batch 23/2200 loss 5.535094 loss_att 10.427737 loss_ctc 13.413170 loss_rnnt 3.355289 hw_loss 0.282872 lr 0.00035898 rank 1
2023-02-25 03:54:22,668 DEBUG TRAIN Batch 23/2200 loss 9.217845 loss_att 15.343918 loss_ctc 15.379031 loss_rnnt 7.054775 hw_loss 0.218181 lr 0.00035897 rank 2
2023-02-25 03:54:22,669 DEBUG TRAIN Batch 23/2200 loss 4.403551 loss_att 6.981090 loss_ctc 6.486018 loss_rnnt 3.486762 hw_loss 0.231786 lr 0.00035898 rank 0
2023-02-25 03:54:22,670 DEBUG TRAIN Batch 23/2200 loss 9.380899 loss_att 10.888798 loss_ctc 12.837094 loss_rnnt 8.519547 hw_loss 0.185526 lr 0.00035899 rank 4
2023-02-25 03:54:22,671 DEBUG TRAIN Batch 23/2200 loss 4.809819 loss_att 8.606651 loss_ctc 7.627008 loss_rnnt 3.566756 hw_loss 0.202632 lr 0.00035894 rank 5
2023-02-25 03:55:32,255 DEBUG TRAIN Batch 23/2300 loss 4.647209 loss_att 8.718858 loss_ctc 6.995374 loss_rnnt 3.376225 hw_loss 0.269184 lr 0.00035887 rank 6
2023-02-25 03:55:32,258 DEBUG TRAIN Batch 23/2300 loss 12.073594 loss_att 16.075382 loss_ctc 16.940598 loss_rnnt 10.513241 hw_loss 0.208242 lr 0.00035887 rank 7
2023-02-25 03:55:32,258 DEBUG TRAIN Batch 23/2300 loss 9.655424 loss_att 13.489731 loss_ctc 15.798363 loss_rnnt 7.947066 hw_loss 0.229570 lr 0.00035888 rank 1
2023-02-25 03:55:32,259 DEBUG TRAIN Batch 23/2300 loss 5.024764 loss_att 7.652705 loss_ctc 7.472577 loss_rnnt 4.082555 hw_loss 0.169210 lr 0.00035889 rank 0
2023-02-25 03:55:32,260 DEBUG TRAIN Batch 23/2300 loss 6.232379 loss_att 9.246422 loss_ctc 10.432823 loss_rnnt 4.995320 hw_loss 0.139108 lr 0.00035890 rank 4
2023-02-25 03:55:32,263 DEBUG TRAIN Batch 23/2300 loss 16.547831 loss_att 16.617069 loss_ctc 25.659624 loss_rnnt 15.208090 hw_loss 0.208102 lr 0.00035888 rank 2
2023-02-25 03:55:32,264 DEBUG TRAIN Batch 23/2300 loss 14.205612 loss_att 13.771988 loss_ctc 19.834034 loss_rnnt 13.391426 hw_loss 0.282101 lr 0.00035884 rank 5
2023-02-25 03:55:32,265 DEBUG TRAIN Batch 23/2300 loss 6.479703 loss_att 9.115844 loss_ctc 8.559475 loss_rnnt 5.525738 hw_loss 0.280189 lr 0.00035893 rank 3
2023-02-25 03:56:43,022 DEBUG TRAIN Batch 23/2400 loss 3.277965 loss_att 5.792782 loss_ctc 6.300916 loss_rnnt 2.253814 hw_loss 0.221490 lr 0.00035875 rank 5
2023-02-25 03:56:43,023 DEBUG TRAIN Batch 23/2400 loss 5.325295 loss_att 6.180511 loss_ctc 5.832785 loss_rnnt 4.946793 hw_loss 0.262112 lr 0.00035881 rank 4
2023-02-25 03:56:43,035 DEBUG TRAIN Batch 23/2400 loss 9.493952 loss_att 11.032817 loss_ctc 12.084503 loss_rnnt 8.746836 hw_loss 0.176130 lr 0.00035880 rank 0
2023-02-25 03:56:43,036 DEBUG TRAIN Batch 23/2400 loss 14.237964 loss_att 15.989424 loss_ctc 17.196600 loss_rnnt 13.419251 hw_loss 0.138629 lr 0.00035877 rank 7
2023-02-25 03:56:43,037 DEBUG TRAIN Batch 23/2400 loss 5.765583 loss_att 9.448088 loss_ctc 11.789871 loss_rnnt 4.061957 hw_loss 0.307286 lr 0.00035878 rank 6
2023-02-25 03:56:43,040 DEBUG TRAIN Batch 23/2400 loss 13.631897 loss_att 15.858854 loss_ctc 19.818771 loss_rnnt 12.270476 hw_loss 0.170837 lr 0.00035879 rank 1
2023-02-25 03:56:43,043 DEBUG TRAIN Batch 23/2400 loss 7.407180 loss_att 9.494007 loss_ctc 13.434911 loss_rnnt 6.079581 hw_loss 0.199756 lr 0.00035884 rank 3
2023-02-25 03:56:43,043 DEBUG TRAIN Batch 23/2400 loss 11.793632 loss_att 14.349190 loss_ctc 15.587461 loss_rnnt 10.727736 hw_loss 0.091760 lr 0.00035879 rank 2
2023-02-25 03:57:57,198 DEBUG TRAIN Batch 23/2500 loss 6.850901 loss_att 7.449956 loss_ctc 9.506558 loss_rnnt 6.277109 hw_loss 0.187300 lr 0.00035870 rank 0
2023-02-25 03:57:57,198 DEBUG TRAIN Batch 23/2500 loss 9.920948 loss_att 13.300880 loss_ctc 15.012105 loss_rnnt 8.473660 hw_loss 0.173400 lr 0.00035869 rank 2
2023-02-25 03:57:57,201 DEBUG TRAIN Batch 23/2500 loss 14.416093 loss_att 16.274082 loss_ctc 19.590691 loss_rnnt 13.259996 hw_loss 0.177284 lr 0.00035866 rank 5
2023-02-25 03:57:57,202 DEBUG TRAIN Batch 23/2500 loss 12.254675 loss_att 13.100747 loss_ctc 15.543683 loss_rnnt 11.513236 hw_loss 0.250671 lr 0.00035872 rank 4
2023-02-25 03:57:57,202 DEBUG TRAIN Batch 23/2500 loss 16.351585 loss_att 16.978842 loss_ctc 25.898600 loss_rnnt 14.839757 hw_loss 0.212702 lr 0.00035869 rank 6
2023-02-25 03:57:57,203 DEBUG TRAIN Batch 23/2500 loss 10.418888 loss_att 11.394923 loss_ctc 12.019909 loss_rnnt 9.865385 hw_loss 0.271550 lr 0.00035868 rank 7
2023-02-25 03:57:57,204 DEBUG TRAIN Batch 23/2500 loss 5.746267 loss_att 8.367487 loss_ctc 9.821901 loss_rnnt 4.577552 hw_loss 0.189475 lr 0.00035875 rank 3
2023-02-25 03:57:57,208 DEBUG TRAIN Batch 23/2500 loss 9.342031 loss_att 9.856082 loss_ctc 9.958135 loss_rnnt 9.013044 hw_loss 0.270055 lr 0.00035870 rank 1
2023-02-25 03:59:07,799 DEBUG TRAIN Batch 23/2600 loss 5.487584 loss_att 6.088068 loss_ctc 8.095436 loss_rnnt 4.863658 hw_loss 0.292715 lr 0.00035861 rank 0
2023-02-25 03:59:07,799 DEBUG TRAIN Batch 23/2600 loss 2.432177 loss_att 4.595803 loss_ctc 3.998240 loss_rnnt 1.650591 hw_loss 0.262599 lr 0.00035859 rank 7
2023-02-25 03:59:07,801 DEBUG TRAIN Batch 23/2600 loss 9.809480 loss_att 12.745941 loss_ctc 13.455017 loss_rnnt 8.645775 hw_loss 0.169390 lr 0.00035859 rank 6
2023-02-25 03:59:07,802 DEBUG TRAIN Batch 23/2600 loss 9.746985 loss_att 10.398505 loss_ctc 12.585012 loss_rnnt 9.107168 hw_loss 0.245832 lr 0.00035860 rank 2
2023-02-25 03:59:07,804 DEBUG TRAIN Batch 23/2600 loss 8.463785 loss_att 9.168484 loss_ctc 9.406764 loss_rnnt 8.074741 hw_loss 0.229451 lr 0.00035857 rank 5
2023-02-25 03:59:07,805 DEBUG TRAIN Batch 23/2600 loss 6.776692 loss_att 6.365892 loss_ctc 9.280377 loss_rnnt 6.401858 hw_loss 0.230942 lr 0.00035861 rank 1
2023-02-25 03:59:07,805 DEBUG TRAIN Batch 23/2600 loss 17.485830 loss_att 24.077770 loss_ctc 24.015381 loss_rnnt 15.172546 hw_loss 0.233043 lr 0.00035862 rank 4
2023-02-25 03:59:07,809 DEBUG TRAIN Batch 23/2600 loss 14.675263 loss_att 18.360317 loss_ctc 21.713711 loss_rnnt 12.901770 hw_loss 0.183795 lr 0.00035865 rank 3
2023-02-25 04:00:18,057 DEBUG TRAIN Batch 23/2700 loss 5.182931 loss_att 9.950609 loss_ctc 7.968902 loss_rnnt 3.816181 hw_loss 0.078284 lr 0.00035848 rank 5
2023-02-25 04:00:18,057 DEBUG TRAIN Batch 23/2700 loss 9.060834 loss_att 11.194752 loss_ctc 10.620428 loss_rnnt 8.347033 hw_loss 0.148260 lr 0.00035851 rank 2
2023-02-25 04:00:18,072 DEBUG TRAIN Batch 23/2700 loss 1.796265 loss_att 4.076347 loss_ctc 3.159447 loss_rnnt 1.001359 hw_loss 0.294623 lr 0.00035856 rank 3
2023-02-25 04:00:18,075 DEBUG TRAIN Batch 23/2700 loss 11.494129 loss_att 16.270712 loss_ctc 21.413414 loss_rnnt 9.096966 hw_loss 0.223641 lr 0.00035850 rank 7
2023-02-25 04:00:18,075 DEBUG TRAIN Batch 23/2700 loss 2.617436 loss_att 6.754105 loss_ctc 5.268975 loss_rnnt 1.338368 hw_loss 0.184116 lr 0.00035852 rank 0
2023-02-25 04:00:18,075 DEBUG TRAIN Batch 23/2700 loss 4.287067 loss_att 7.541928 loss_ctc 7.109595 loss_rnnt 3.203040 hw_loss 0.106346 lr 0.00035850 rank 6
2023-02-25 04:00:18,076 DEBUG TRAIN Batch 23/2700 loss 4.814817 loss_att 6.544488 loss_ctc 8.156279 loss_rnnt 3.947515 hw_loss 0.142200 lr 0.00035851 rank 1
2023-02-25 04:00:18,121 DEBUG TRAIN Batch 23/2700 loss 19.639320 loss_att 18.411581 loss_ctc 19.176123 loss_rnnt 19.816635 hw_loss 0.243735 lr 0.00035853 rank 4
2023-02-25 04:01:29,712 DEBUG TRAIN Batch 23/2800 loss 1.736948 loss_att 4.358592 loss_ctc 5.201053 loss_rnnt 0.626617 hw_loss 0.232729 lr 0.00035843 rank 0
2023-02-25 04:01:29,717 DEBUG TRAIN Batch 23/2800 loss 12.212027 loss_att 12.321676 loss_ctc 13.297791 loss_rnnt 11.910908 hw_loss 0.252039 lr 0.00035842 rank 1
2023-02-25 04:01:29,721 DEBUG TRAIN Batch 23/2800 loss 18.560688 loss_att 20.035118 loss_ctc 21.890320 loss_rnnt 17.764267 hw_loss 0.107971 lr 0.00035840 rank 7
2023-02-25 04:01:29,724 DEBUG TRAIN Batch 23/2800 loss 11.576498 loss_att 16.412615 loss_ctc 15.197874 loss_rnnt 9.996545 hw_loss 0.243523 lr 0.00035842 rank 2
2023-02-25 04:01:29,734 DEBUG TRAIN Batch 23/2800 loss 9.754274 loss_att 12.873450 loss_ctc 14.978232 loss_rnnt 8.335854 hw_loss 0.183859 lr 0.00035841 rank 6
2023-02-25 04:01:29,739 DEBUG TRAIN Batch 23/2800 loss 5.607426 loss_att 7.939394 loss_ctc 6.345048 loss_rnnt 4.923848 hw_loss 0.222815 lr 0.00035838 rank 5
2023-02-25 04:01:29,750 DEBUG TRAIN Batch 23/2800 loss 8.101202 loss_att 11.175218 loss_ctc 9.722986 loss_rnnt 7.192957 hw_loss 0.144758 lr 0.00035844 rank 4
2023-02-25 04:01:29,769 DEBUG TRAIN Batch 23/2800 loss 8.868550 loss_att 12.775323 loss_ctc 16.067602 loss_rnnt 7.059521 hw_loss 0.127124 lr 0.00035847 rank 3
2023-02-25 04:02:41,098 DEBUG TRAIN Batch 23/2900 loss 10.546898 loss_att 19.257523 loss_ctc 15.389086 loss_rnnt 8.098560 hw_loss 0.113602 lr 0.00035833 rank 1
2023-02-25 04:02:41,098 DEBUG TRAIN Batch 23/2900 loss 7.543283 loss_att 9.655770 loss_ctc 10.802324 loss_rnnt 6.571813 hw_loss 0.214561 lr 0.00035834 rank 0
2023-02-25 04:02:41,101 DEBUG TRAIN Batch 23/2900 loss 9.530731 loss_att 12.288211 loss_ctc 9.811540 loss_rnnt 8.803852 hw_loss 0.258642 lr 0.00035838 rank 3
2023-02-25 04:02:41,100 DEBUG TRAIN Batch 23/2900 loss 10.068067 loss_att 12.704189 loss_ctc 15.864157 loss_rnnt 8.690035 hw_loss 0.146240 lr 0.00035832 rank 6
2023-02-25 04:02:41,102 DEBUG TRAIN Batch 23/2900 loss 7.181146 loss_att 9.030636 loss_ctc 8.994562 loss_rnnt 6.517925 hw_loss 0.096627 lr 0.00035831 rank 7
2023-02-25 04:02:41,102 DEBUG TRAIN Batch 23/2900 loss 14.646524 loss_att 14.454371 loss_ctc 16.789608 loss_rnnt 14.294827 hw_loss 0.195718 lr 0.00035833 rank 2
2023-02-25 04:02:41,102 DEBUG TRAIN Batch 23/2900 loss 9.774689 loss_att 14.180563 loss_ctc 17.082245 loss_rnnt 7.829360 hw_loss 0.168398 lr 0.00035835 rank 4
2023-02-25 04:02:41,107 DEBUG TRAIN Batch 23/2900 loss 7.240589 loss_att 8.876238 loss_ctc 9.418065 loss_rnnt 6.479996 hw_loss 0.268375 lr 0.00035829 rank 5
2023-02-25 04:03:50,184 DEBUG TRAIN Batch 23/3000 loss 5.277479 loss_att 7.347259 loss_ctc 7.828221 loss_rnnt 4.430885 hw_loss 0.173510 lr 0.00035824 rank 0
2023-02-25 04:03:50,191 DEBUG TRAIN Batch 23/3000 loss 10.670263 loss_att 13.552605 loss_ctc 14.972330 loss_rnnt 9.446780 hw_loss 0.137635 lr 0.00035828 rank 3
2023-02-25 04:03:50,190 DEBUG TRAIN Batch 23/3000 loss 8.364004 loss_att 12.614883 loss_ctc 12.466271 loss_rnnt 6.864064 hw_loss 0.192740 lr 0.00035824 rank 1
2023-02-25 04:03:50,192 DEBUG TRAIN Batch 23/3000 loss 5.202085 loss_att 7.369427 loss_ctc 9.744963 loss_rnnt 4.070145 hw_loss 0.173914 lr 0.00035822 rank 7
2023-02-25 04:03:50,194 DEBUG TRAIN Batch 23/3000 loss 6.671279 loss_att 9.594563 loss_ctc 7.645846 loss_rnnt 5.886626 hw_loss 0.131352 lr 0.00035820 rank 5
2023-02-25 04:03:50,195 DEBUG TRAIN Batch 23/3000 loss 6.282561 loss_att 9.093119 loss_ctc 11.475311 loss_rnnt 4.916152 hw_loss 0.209869 lr 0.00035822 rank 6
2023-02-25 04:03:50,196 DEBUG TRAIN Batch 23/3000 loss 12.090947 loss_att 13.137120 loss_ctc 12.899582 loss_rnnt 11.661268 hw_loss 0.211177 lr 0.00035823 rank 2
2023-02-25 04:03:50,201 DEBUG TRAIN Batch 23/3000 loss 12.486897 loss_att 15.270933 loss_ctc 12.404490 loss_rnnt 11.830195 hw_loss 0.207906 lr 0.00035825 rank 4
2023-02-25 04:05:01,479 DEBUG TRAIN Batch 23/3100 loss 9.888083 loss_att 10.609758 loss_ctc 13.777111 loss_rnnt 9.112513 hw_loss 0.211308 lr 0.00035813 rank 6
2023-02-25 04:05:01,484 DEBUG TRAIN Batch 23/3100 loss 13.241692 loss_att 11.418190 loss_ctc 13.705819 loss_rnnt 13.483683 hw_loss 0.114047 lr 0.00035815 rank 0
2023-02-25 04:05:01,484 DEBUG TRAIN Batch 23/3100 loss 6.406072 loss_att 8.677843 loss_ctc 7.989008 loss_rnnt 5.661307 hw_loss 0.148785 lr 0.00035819 rank 3
2023-02-25 04:05:01,490 DEBUG TRAIN Batch 23/3100 loss 6.041637 loss_att 6.872052 loss_ctc 8.080889 loss_rnnt 5.513484 hw_loss 0.169068 lr 0.00035815 rank 1
2023-02-25 04:05:01,492 DEBUG TRAIN Batch 23/3100 loss 6.419776 loss_att 8.412043 loss_ctc 7.447895 loss_rnnt 5.746211 hw_loss 0.258805 lr 0.00035814 rank 2
2023-02-25 04:05:01,496 DEBUG TRAIN Batch 23/3100 loss 8.253074 loss_att 9.338901 loss_ctc 11.260390 loss_rnnt 7.510931 hw_loss 0.232500 lr 0.00035813 rank 7
2023-02-25 04:05:01,497 DEBUG TRAIN Batch 23/3100 loss 4.961885 loss_att 6.890302 loss_ctc 8.401311 loss_rnnt 3.975574 hw_loss 0.266321 lr 0.00035811 rank 5
2023-02-25 04:05:01,503 DEBUG TRAIN Batch 23/3100 loss 8.341364 loss_att 9.991539 loss_ctc 10.959946 loss_rnnt 7.567888 hw_loss 0.176806 lr 0.00035816 rank 4
2023-02-25 04:06:17,669 DEBUG TRAIN Batch 23/3200 loss 8.252651 loss_att 9.135057 loss_ctc 9.022614 loss_rnnt 7.863378 hw_loss 0.206494 lr 0.00035805 rank 1
2023-02-25 04:06:17,669 DEBUG TRAIN Batch 23/3200 loss 10.087955 loss_att 10.929697 loss_ctc 12.088803 loss_rnnt 9.515963 hw_loss 0.256622 lr 0.00035804 rank 7
2023-02-25 04:06:17,671 DEBUG TRAIN Batch 23/3200 loss 12.770462 loss_att 14.106737 loss_ctc 15.720474 loss_rnnt 11.978636 hw_loss 0.246070 lr 0.00035806 rank 0
2023-02-25 04:06:17,672 DEBUG TRAIN Batch 23/3200 loss 11.074023 loss_att 11.857914 loss_ctc 14.297997 loss_rnnt 10.356714 hw_loss 0.245002 lr 0.00035807 rank 4
2023-02-25 04:06:17,671 DEBUG TRAIN Batch 23/3200 loss 7.500957 loss_att 10.801939 loss_ctc 12.441643 loss_rnnt 6.064622 hw_loss 0.220089 lr 0.00035804 rank 6
2023-02-25 04:06:17,671 DEBUG TRAIN Batch 23/3200 loss 8.440775 loss_att 12.273550 loss_ctc 10.178919 loss_rnnt 7.311706 hw_loss 0.245178 lr 0.00035802 rank 5
2023-02-25 04:06:17,676 DEBUG TRAIN Batch 23/3200 loss 11.041002 loss_att 10.663624 loss_ctc 15.783113 loss_rnnt 10.320756 hw_loss 0.306450 lr 0.00035805 rank 2
2023-02-25 04:06:17,678 DEBUG TRAIN Batch 23/3200 loss 7.466194 loss_att 10.726083 loss_ctc 10.836997 loss_rnnt 6.255413 hw_loss 0.205057 lr 0.00035810 rank 3
2023-02-25 04:07:28,526 DEBUG TRAIN Batch 23/3300 loss 7.552815 loss_att 11.060747 loss_ctc 10.354942 loss_rnnt 6.417905 hw_loss 0.111950 lr 0.00035796 rank 1
2023-02-25 04:07:28,527 DEBUG TRAIN Batch 23/3300 loss 2.701956 loss_att 5.987833 loss_ctc 5.580870 loss_rnnt 1.571278 hw_loss 0.168090 lr 0.00035797 rank 0
2023-02-25 04:07:28,530 DEBUG TRAIN Batch 23/3300 loss 1.911972 loss_att 5.477778 loss_ctc 3.407997 loss_rnnt 0.925883 hw_loss 0.137733 lr 0.00035792 rank 5
2023-02-25 04:07:28,530 DEBUG TRAIN Batch 23/3300 loss 9.307045 loss_att 14.039179 loss_ctc 17.931549 loss_rnnt 7.091503 hw_loss 0.223464 lr 0.00035795 rank 6
2023-02-25 04:07:28,534 DEBUG TRAIN Batch 23/3300 loss 9.097633 loss_att 11.174707 loss_ctc 12.496971 loss_rnnt 8.109004 hw_loss 0.224942 lr 0.00035801 rank 3
2023-02-25 04:07:28,535 DEBUG TRAIN Batch 23/3300 loss 12.102864 loss_att 11.502052 loss_ctc 19.595249 loss_rnnt 11.139956 hw_loss 0.157660 lr 0.00035796 rank 2
2023-02-25 04:07:28,535 DEBUG TRAIN Batch 23/3300 loss 6.953305 loss_att 7.763802 loss_ctc 9.853386 loss_rnnt 6.329089 hw_loss 0.141449 lr 0.00035795 rank 7
2023-02-25 04:07:28,537 DEBUG TRAIN Batch 23/3300 loss 5.473282 loss_att 8.288834 loss_ctc 7.028631 loss_rnnt 4.609752 hw_loss 0.174448 lr 0.00035798 rank 4
2023-02-25 04:08:38,764 DEBUG TRAIN Batch 23/3400 loss 3.363014 loss_att 6.441277 loss_ctc 4.738725 loss_rnnt 2.439201 hw_loss 0.233874 lr 0.00035788 rank 0
2023-02-25 04:08:38,767 DEBUG TRAIN Batch 23/3400 loss 6.528682 loss_att 10.128095 loss_ctc 9.197709 loss_rnnt 5.374683 hw_loss 0.146713 lr 0.00035787 rank 1
2023-02-25 04:08:38,767 DEBUG TRAIN Batch 23/3400 loss 7.112246 loss_att 12.617702 loss_ctc 11.562532 loss_rnnt 5.336388 hw_loss 0.152615 lr 0.00035786 rank 6
2023-02-25 04:08:38,769 DEBUG TRAIN Batch 23/3400 loss 8.650094 loss_att 15.079243 loss_ctc 18.028517 loss_rnnt 6.026572 hw_loss 0.163567 lr 0.00035787 rank 2
2023-02-25 04:08:38,769 DEBUG TRAIN Batch 23/3400 loss 11.905347 loss_att 11.669401 loss_ctc 14.000743 loss_rnnt 11.611792 hw_loss 0.115050 lr 0.00035792 rank 3
2023-02-25 04:08:38,769 DEBUG TRAIN Batch 23/3400 loss 6.100686 loss_att 10.045594 loss_ctc 9.123878 loss_rnnt 4.794744 hw_loss 0.213501 lr 0.00035783 rank 5
2023-02-25 04:08:38,770 DEBUG TRAIN Batch 23/3400 loss 5.285032 loss_att 7.272912 loss_ctc 9.772875 loss_rnnt 4.195258 hw_loss 0.175909 lr 0.00035789 rank 4
2023-02-25 04:08:38,770 DEBUG TRAIN Batch 23/3400 loss 5.133748 loss_att 7.847415 loss_ctc 9.190568 loss_rnnt 3.925747 hw_loss 0.233170 lr 0.00035785 rank 7
2023-02-25 04:09:50,531 DEBUG TRAIN Batch 23/3500 loss 10.299932 loss_att 13.714188 loss_ctc 15.636015 loss_rnnt 8.808161 hw_loss 0.182708 lr 0.00035777 rank 6
2023-02-25 04:09:50,545 DEBUG TRAIN Batch 23/3500 loss 4.670069 loss_att 6.950162 loss_ctc 4.649081 loss_rnnt 4.095268 hw_loss 0.227965 lr 0.00035783 rank 3
2023-02-25 04:09:50,546 DEBUG TRAIN Batch 23/3500 loss 8.854740 loss_att 12.916271 loss_ctc 11.785122 loss_rnnt 7.458325 hw_loss 0.362608 lr 0.00035779 rank 0
2023-02-25 04:09:50,546 DEBUG TRAIN Batch 23/3500 loss 3.438596 loss_att 5.912717 loss_ctc 5.982377 loss_rnnt 2.488509 hw_loss 0.217671 lr 0.00035776 rank 7
2023-02-25 04:09:50,549 DEBUG TRAIN Batch 23/3500 loss 5.330274 loss_att 10.251524 loss_ctc 5.742093 loss_rnnt 4.158909 hw_loss 0.247887 lr 0.00035780 rank 4
2023-02-25 04:09:50,553 DEBUG TRAIN Batch 23/3500 loss 12.490430 loss_att 14.867813 loss_ctc 17.172213 loss_rnnt 11.283332 hw_loss 0.201344 lr 0.00035774 rank 5
2023-02-25 04:09:50,552 DEBUG TRAIN Batch 23/3500 loss 24.843817 loss_att 24.784241 loss_ctc 43.237358 loss_rnnt 22.324364 hw_loss 0.147929 lr 0.00035778 rank 1
2023-02-25 04:09:50,553 DEBUG TRAIN Batch 23/3500 loss 10.376658 loss_att 12.621794 loss_ctc 15.871996 loss_rnnt 9.071444 hw_loss 0.231517 lr 0.00035777 rank 2
2023-02-25 04:11:03,276 DEBUG TRAIN Batch 23/3600 loss 8.031842 loss_att 9.954327 loss_ctc 12.974190 loss_rnnt 6.880953 hw_loss 0.201399 lr 0.00035773 rank 3
2023-02-25 04:11:03,278 DEBUG TRAIN Batch 23/3600 loss 6.715765 loss_att 9.792881 loss_ctc 8.039592 loss_rnnt 5.805926 hw_loss 0.221073 lr 0.00035767 rank 6
2023-02-25 04:11:03,279 DEBUG TRAIN Batch 23/3600 loss 10.627614 loss_att 11.159264 loss_ctc 12.685081 loss_rnnt 10.156364 hw_loss 0.169857 lr 0.00035769 rank 0
2023-02-25 04:11:03,279 DEBUG TRAIN Batch 23/3600 loss 10.796945 loss_att 15.161390 loss_ctc 18.046564 loss_rnnt 8.842623 hw_loss 0.215279 lr 0.00035769 rank 1
2023-02-25 04:11:03,284 DEBUG TRAIN Batch 23/3600 loss 12.974918 loss_att 17.057081 loss_ctc 17.855715 loss_rnnt 11.408159 hw_loss 0.186664 lr 0.00035768 rank 2
2023-02-25 04:11:03,285 DEBUG TRAIN Batch 23/3600 loss 15.015285 loss_att 16.570850 loss_ctc 25.038866 loss_rnnt 13.273643 hw_loss 0.176349 lr 0.00035767 rank 7
2023-02-25 04:11:03,286 DEBUG TRAIN Batch 23/3600 loss 5.156518 loss_att 8.677517 loss_ctc 7.665756 loss_rnnt 4.026264 hw_loss 0.171540 lr 0.00035770 rank 4
2023-02-25 04:11:03,292 DEBUG TRAIN Batch 23/3600 loss 4.611370 loss_att 8.524481 loss_ctc 10.323829 loss_rnnt 2.941241 hw_loss 0.235960 lr 0.00035765 rank 5
2023-02-25 04:12:12,775 DEBUG TRAIN Batch 23/3700 loss 8.318495 loss_att 10.746246 loss_ctc 11.922393 loss_rnnt 7.240090 hw_loss 0.210625 lr 0.00035761 rank 4
2023-02-25 04:12:12,776 DEBUG TRAIN Batch 23/3700 loss 9.237196 loss_att 10.105871 loss_ctc 12.105240 loss_rnnt 8.641075 hw_loss 0.074962 lr 0.00035764 rank 3
2023-02-25 04:12:12,780 DEBUG TRAIN Batch 23/3700 loss 10.789540 loss_att 13.830171 loss_ctc 13.449169 loss_rnnt 9.701539 hw_loss 0.234856 lr 0.00035759 rank 2
2023-02-25 04:12:12,782 DEBUG TRAIN Batch 23/3700 loss 3.691187 loss_att 5.645447 loss_ctc 6.615814 loss_rnnt 2.801174 hw_loss 0.204772 lr 0.00035760 rank 0
2023-02-25 04:12:12,782 DEBUG TRAIN Batch 23/3700 loss 5.516436 loss_att 9.619883 loss_ctc 7.362176 loss_rnnt 4.354233 hw_loss 0.178903 lr 0.00035760 rank 1
2023-02-25 04:12:12,783 DEBUG TRAIN Batch 23/3700 loss 11.505338 loss_att 14.466601 loss_ctc 16.068533 loss_rnnt 10.208342 hw_loss 0.180595 lr 0.00035758 rank 6
2023-02-25 04:12:12,785 DEBUG TRAIN Batch 23/3700 loss 5.303655 loss_att 9.284212 loss_ctc 9.675013 loss_rnnt 3.789667 hw_loss 0.253177 lr 0.00035756 rank 5
2023-02-25 04:12:12,785 DEBUG TRAIN Batch 23/3700 loss 13.206909 loss_att 14.477545 loss_ctc 13.369008 loss_rnnt 12.860237 hw_loss 0.132996 lr 0.00035758 rank 7
2023-02-25 04:13:23,738 DEBUG TRAIN Batch 23/3800 loss 10.803526 loss_att 12.056106 loss_ctc 15.177435 loss_rnnt 9.838614 hw_loss 0.246013 lr 0.00035749 rank 7
2023-02-25 04:13:23,741 DEBUG TRAIN Batch 23/3800 loss 9.716116 loss_att 10.147038 loss_ctc 12.006644 loss_rnnt 9.219728 hw_loss 0.196499 lr 0.00035752 rank 4
2023-02-25 04:13:23,742 DEBUG TRAIN Batch 23/3800 loss 8.665478 loss_att 8.920926 loss_ctc 11.140193 loss_rnnt 8.170058 hw_loss 0.214440 lr 0.00035751 rank 0
2023-02-25 04:13:23,743 DEBUG TRAIN Batch 23/3800 loss 9.486568 loss_att 14.847023 loss_ctc 13.746051 loss_rnnt 7.748481 hw_loss 0.183873 lr 0.00035747 rank 5
2023-02-25 04:13:23,742 DEBUG TRAIN Batch 23/3800 loss 9.629354 loss_att 10.060730 loss_ctc 13.255696 loss_rnnt 8.916884 hw_loss 0.267530 lr 0.00035755 rank 3
2023-02-25 04:13:23,745 DEBUG TRAIN Batch 23/3800 loss 7.997183 loss_att 12.236448 loss_ctc 13.451826 loss_rnnt 6.277962 hw_loss 0.270153 lr 0.00035751 rank 1
2023-02-25 04:13:23,745 DEBUG TRAIN Batch 23/3800 loss 6.839733 loss_att 7.647539 loss_ctc 8.940983 loss_rnnt 6.251929 hw_loss 0.273892 lr 0.00035750 rank 2
2023-02-25 04:13:23,775 DEBUG TRAIN Batch 23/3800 loss 9.749607 loss_att 10.130840 loss_ctc 14.152144 loss_rnnt 8.858521 hw_loss 0.427189 lr 0.00035749 rank 6
2023-02-25 04:14:37,190 DEBUG TRAIN Batch 23/3900 loss 7.025778 loss_att 10.922712 loss_ctc 10.399593 loss_rnnt 5.717805 hw_loss 0.147646 lr 0.00035740 rank 6
2023-02-25 04:14:37,195 DEBUG TRAIN Batch 23/3900 loss 7.444771 loss_att 13.688861 loss_ctc 8.389255 loss_rnnt 5.977319 hw_loss 0.173818 lr 0.00035742 rank 0
2023-02-25 04:14:37,203 DEBUG TRAIN Batch 23/3900 loss 11.978194 loss_att 15.210709 loss_ctc 15.923620 loss_rnnt 10.656168 hw_loss 0.280249 lr 0.00035746 rank 3
2023-02-25 04:14:37,206 DEBUG TRAIN Batch 23/3900 loss 6.806974 loss_att 8.756872 loss_ctc 11.071976 loss_rnnt 5.673223 hw_loss 0.328319 lr 0.00035741 rank 1
2023-02-25 04:14:37,206 DEBUG TRAIN Batch 23/3900 loss 8.580020 loss_att 11.919680 loss_ctc 10.090721 loss_rnnt 7.599336 hw_loss 0.208733 lr 0.00035741 rank 2
2023-02-25 04:14:37,215 DEBUG TRAIN Batch 23/3900 loss 10.899348 loss_att 9.938301 loss_ctc 12.547994 loss_rnnt 10.734319 hw_loss 0.257661 lr 0.00035743 rank 4
2023-02-25 04:14:37,216 DEBUG TRAIN Batch 23/3900 loss 7.989339 loss_att 11.160608 loss_ctc 8.668300 loss_rnnt 7.147473 hw_loss 0.219532 lr 0.00035738 rank 5
2023-02-25 04:14:37,236 DEBUG TRAIN Batch 23/3900 loss 7.752080 loss_att 12.508951 loss_ctc 12.976318 loss_rnnt 6.035227 hw_loss 0.129213 lr 0.00035740 rank 7
2023-02-25 04:15:47,955 DEBUG TRAIN Batch 23/4000 loss 4.635570 loss_att 6.748789 loss_ctc 5.462482 loss_rnnt 4.016953 hw_loss 0.160723 lr 0.00035731 rank 6
2023-02-25 04:15:47,956 DEBUG TRAIN Batch 23/4000 loss 2.353631 loss_att 7.400115 loss_ctc 3.050576 loss_rnnt 1.145197 hw_loss 0.199145 lr 0.00035733 rank 0
2023-02-25 04:15:47,956 DEBUG TRAIN Batch 23/4000 loss 14.130115 loss_att 14.742407 loss_ctc 21.501598 loss_rnnt 12.952801 hw_loss 0.134983 lr 0.00035728 rank 5
2023-02-25 04:15:47,957 DEBUG TRAIN Batch 23/4000 loss 18.252106 loss_att 19.986813 loss_ctc 24.960745 loss_rnnt 16.907850 hw_loss 0.192803 lr 0.00035737 rank 3
2023-02-25 04:15:47,957 DEBUG TRAIN Batch 23/4000 loss 10.055641 loss_att 11.898636 loss_ctc 11.436408 loss_rnnt 9.379815 hw_loss 0.230857 lr 0.00035734 rank 4
2023-02-25 04:15:47,964 DEBUG TRAIN Batch 23/4000 loss 8.234720 loss_att 11.226203 loss_ctc 17.053911 loss_rnnt 6.375758 hw_loss 0.158952 lr 0.00035732 rank 1
2023-02-25 04:15:47,964 DEBUG TRAIN Batch 23/4000 loss 11.965534 loss_att 13.438572 loss_ctc 15.547145 loss_rnnt 11.075857 hw_loss 0.220353 lr 0.00035731 rank 7
2023-02-25 04:15:47,966 DEBUG TRAIN Batch 23/4000 loss 4.794988 loss_att 7.714628 loss_ctc 10.007589 loss_rnnt 3.442845 hw_loss 0.137252 lr 0.00035732 rank 2
2023-02-25 04:16:58,248 DEBUG TRAIN Batch 23/4100 loss 5.639532 loss_att 9.384571 loss_ctc 9.994351 loss_rnnt 4.159812 hw_loss 0.281380 lr 0.00035724 rank 0
2023-02-25 04:16:58,250 DEBUG TRAIN Batch 23/4100 loss 2.962176 loss_att 5.571891 loss_ctc 4.825797 loss_rnnt 2.090162 hw_loss 0.190477 lr 0.00035722 rank 6
2023-02-25 04:16:58,252 DEBUG TRAIN Batch 23/4100 loss 10.137075 loss_att 12.060337 loss_ctc 15.580874 loss_rnnt 8.858371 hw_loss 0.315398 lr 0.00035723 rank 2
2023-02-25 04:16:58,254 DEBUG TRAIN Batch 23/4100 loss 15.294368 loss_att 19.772690 loss_ctc 20.653254 loss_rnnt 13.578003 hw_loss 0.199091 lr 0.00035719 rank 5
2023-02-25 04:16:58,256 DEBUG TRAIN Batch 23/4100 loss 14.341140 loss_att 13.031666 loss_ctc 16.692371 loss_rnnt 14.146673 hw_loss 0.267869 lr 0.00035723 rank 1
2023-02-25 04:16:58,260 DEBUG TRAIN Batch 23/4100 loss 6.197437 loss_att 9.995097 loss_ctc 10.908249 loss_rnnt 4.702373 hw_loss 0.201420 lr 0.00035728 rank 3
2023-02-25 04:16:58,261 DEBUG TRAIN Batch 23/4100 loss 6.859875 loss_att 8.395205 loss_ctc 7.569398 loss_rnnt 6.369844 hw_loss 0.165680 lr 0.00035721 rank 7
2023-02-25 04:16:58,275 DEBUG TRAIN Batch 23/4100 loss 8.729524 loss_att 10.021828 loss_ctc 12.397833 loss_rnnt 7.831110 hw_loss 0.282837 lr 0.00035725 rank 4
2023-02-25 04:18:09,042 DEBUG TRAIN Batch 23/4200 loss 6.605092 loss_att 9.050887 loss_ctc 8.428613 loss_rnnt 5.718704 hw_loss 0.288925 lr 0.00035712 rank 7
2023-02-25 04:18:09,043 DEBUG TRAIN Batch 23/4200 loss 5.751339 loss_att 8.008130 loss_ctc 8.039608 loss_rnnt 4.886001 hw_loss 0.204145 lr 0.00035715 rank 0
2023-02-25 04:18:09,045 DEBUG TRAIN Batch 23/4200 loss 5.892322 loss_att 6.990624 loss_ctc 7.410182 loss_rnnt 5.370957 hw_loss 0.186229 lr 0.00035710 rank 5
2023-02-25 04:18:09,045 DEBUG TRAIN Batch 23/4200 loss 6.243309 loss_att 9.535867 loss_ctc 9.462296 loss_rnnt 5.019880 hw_loss 0.254474 lr 0.00035716 rank 4
2023-02-25 04:18:09,047 DEBUG TRAIN Batch 23/4200 loss 14.591894 loss_att 17.375603 loss_ctc 18.930788 loss_rnnt 13.378342 hw_loss 0.146797 lr 0.00035719 rank 3
2023-02-25 04:18:09,050 DEBUG TRAIN Batch 23/4200 loss 7.208975 loss_att 8.924423 loss_ctc 9.859265 loss_rnnt 6.393579 hw_loss 0.223002 lr 0.00035714 rank 1
2023-02-25 04:18:09,049 DEBUG TRAIN Batch 23/4200 loss 5.613970 loss_att 8.115641 loss_ctc 8.782160 loss_rnnt 4.591475 hw_loss 0.187003 lr 0.00035714 rank 2
2023-02-25 04:18:09,057 DEBUG TRAIN Batch 23/4200 loss 9.206511 loss_att 10.989384 loss_ctc 12.482910 loss_rnnt 8.278501 hw_loss 0.252341 lr 0.00035713 rank 6
2023-02-25 04:19:21,987 DEBUG TRAIN Batch 23/4300 loss 8.912099 loss_att 12.602271 loss_ctc 11.646654 loss_rnnt 7.678051 hw_loss 0.246386 lr 0.00035705 rank 0
2023-02-25 04:19:21,990 DEBUG TRAIN Batch 23/4300 loss 18.251587 loss_att 21.398680 loss_ctc 23.599922 loss_rnnt 16.778034 hw_loss 0.245668 lr 0.00035705 rank 1
2023-02-25 04:19:21,991 DEBUG TRAIN Batch 23/4300 loss 8.432162 loss_att 10.726650 loss_ctc 13.300442 loss_rnnt 7.280768 hw_loss 0.081361 lr 0.00035704 rank 2
2023-02-25 04:19:21,994 DEBUG TRAIN Batch 23/4300 loss 4.991405 loss_att 7.109948 loss_ctc 8.316215 loss_rnnt 3.969766 hw_loss 0.289916 lr 0.00035703 rank 7
2023-02-25 04:19:21,994 DEBUG TRAIN Batch 23/4300 loss 2.601531 loss_att 4.594179 loss_ctc 5.003859 loss_rnnt 1.760575 hw_loss 0.228968 lr 0.00035701 rank 5
2023-02-25 04:19:21,994 DEBUG TRAIN Batch 23/4300 loss 4.762609 loss_att 7.504908 loss_ctc 6.877538 loss_rnnt 3.799316 hw_loss 0.249081 lr 0.00035704 rank 6
2023-02-25 04:19:21,997 DEBUG TRAIN Batch 23/4300 loss 6.317234 loss_att 8.059470 loss_ctc 10.768034 loss_rnnt 5.265750 hw_loss 0.205492 lr 0.00035707 rank 4
2023-02-25 04:19:21,999 DEBUG TRAIN Batch 23/4300 loss 5.339430 loss_att 7.440581 loss_ctc 7.518980 loss_rnnt 4.551052 hw_loss 0.145390 lr 0.00035709 rank 3
2023-02-25 04:20:32,014 DEBUG TRAIN Batch 23/4400 loss 9.906261 loss_att 10.642139 loss_ctc 14.165928 loss_rnnt 9.021095 hw_loss 0.318815 lr 0.00035700 rank 3
2023-02-25 04:20:32,016 DEBUG TRAIN Batch 23/4400 loss 10.492822 loss_att 12.293784 loss_ctc 17.352243 loss_rnnt 9.029427 hw_loss 0.353650 lr 0.00035696 rank 0
2023-02-25 04:20:32,017 DEBUG TRAIN Batch 23/4400 loss 5.916496 loss_att 9.100547 loss_ctc 10.765746 loss_rnnt 4.526779 hw_loss 0.199388 lr 0.00035694 rank 7
2023-02-25 04:20:32,019 DEBUG TRAIN Batch 23/4400 loss 18.227409 loss_att 19.840731 loss_ctc 23.202415 loss_rnnt 17.087439 hw_loss 0.288696 lr 0.00035695 rank 2
2023-02-25 04:20:32,019 DEBUG TRAIN Batch 23/4400 loss 10.194524 loss_att 9.907616 loss_ctc 12.982370 loss_rnnt 9.722929 hw_loss 0.294869 lr 0.00035694 rank 6
2023-02-25 04:20:32,020 DEBUG TRAIN Batch 23/4400 loss 14.094541 loss_att 17.972134 loss_ctc 18.902351 loss_rnnt 12.483425 hw_loss 0.364791 lr 0.00035697 rank 4
2023-02-25 04:20:32,021 DEBUG TRAIN Batch 23/4400 loss 5.497921 loss_att 7.210426 loss_ctc 10.922688 loss_rnnt 4.290458 hw_loss 0.265613 lr 0.00035696 rank 1
2023-02-25 04:20:32,021 DEBUG TRAIN Batch 23/4400 loss 8.969504 loss_att 10.521865 loss_ctc 13.012920 loss_rnnt 7.976003 hw_loss 0.269825 lr 0.00035692 rank 5
2023-02-25 04:21:41,946 DEBUG TRAIN Batch 23/4500 loss 17.342640 loss_att 17.510328 loss_ctc 25.440811 loss_rnnt 16.192364 hw_loss 0.069340 lr 0.00035687 rank 0
2023-02-25 04:21:41,947 DEBUG TRAIN Batch 23/4500 loss 7.147625 loss_att 11.474081 loss_ctc 9.184999 loss_rnnt 5.934671 hw_loss 0.142525 lr 0.00035685 rank 6
2023-02-25 04:21:41,950 DEBUG TRAIN Batch 23/4500 loss 5.899987 loss_att 5.549518 loss_ctc 8.966098 loss_rnnt 5.401012 hw_loss 0.300477 lr 0.00035686 rank 2
2023-02-25 04:21:41,952 DEBUG TRAIN Batch 23/4500 loss 8.321127 loss_att 9.321911 loss_ctc 11.414279 loss_rnnt 7.617510 hw_loss 0.170697 lr 0.00035687 rank 1
2023-02-25 04:21:41,952 DEBUG TRAIN Batch 23/4500 loss 12.702387 loss_att 15.123705 loss_ctc 17.824780 loss_rnnt 11.476343 hw_loss 0.110237 lr 0.00035691 rank 3
2023-02-25 04:21:41,953 DEBUG TRAIN Batch 23/4500 loss 4.396255 loss_att 7.289635 loss_ctc 6.393436 loss_rnnt 3.467154 hw_loss 0.157749 lr 0.00035685 rank 7
2023-02-25 04:21:41,959 DEBUG TRAIN Batch 23/4500 loss 10.819123 loss_att 14.140514 loss_ctc 23.934927 loss_rnnt 8.351108 hw_loss 0.103057 lr 0.00035683 rank 5
2023-02-25 04:21:41,980 DEBUG TRAIN Batch 23/4500 loss 8.492551 loss_att 8.912981 loss_ctc 10.370085 loss_rnnt 8.026591 hw_loss 0.246629 lr 0.00035688 rank 4
2023-02-25 04:22:55,099 DEBUG TRAIN Batch 23/4600 loss 11.851151 loss_att 14.682343 loss_ctc 23.175896 loss_rnnt 9.648796 hw_loss 0.236530 lr 0.00035679 rank 4
2023-02-25 04:22:55,111 DEBUG TRAIN Batch 23/4600 loss 8.896399 loss_att 12.638454 loss_ctc 13.892962 loss_rnnt 7.366966 hw_loss 0.215275 lr 0.00035677 rank 2
2023-02-25 04:22:55,112 DEBUG TRAIN Batch 23/4600 loss 6.435324 loss_att 9.998659 loss_ctc 8.824085 loss_rnnt 5.302883 hw_loss 0.189887 lr 0.00035678 rank 0
2023-02-25 04:22:55,114 DEBUG TRAIN Batch 23/4600 loss 6.226524 loss_att 8.275053 loss_ctc 11.453823 loss_rnnt 4.972650 hw_loss 0.275990 lr 0.00035678 rank 1
2023-02-25 04:22:55,115 DEBUG TRAIN Batch 23/4600 loss 7.138474 loss_att 9.179933 loss_ctc 14.288168 loss_rnnt 5.687670 hw_loss 0.167286 lr 0.00035676 rank 7
2023-02-25 04:22:55,116 DEBUG TRAIN Batch 23/4600 loss 16.326456 loss_att 19.345451 loss_ctc 25.430374 loss_rnnt 14.377087 hw_loss 0.246963 lr 0.00035682 rank 3
2023-02-25 04:22:55,131 DEBUG TRAIN Batch 23/4600 loss 5.295104 loss_att 6.091664 loss_ctc 5.786594 loss_rnnt 4.981183 hw_loss 0.167018 lr 0.00035676 rank 6
2023-02-25 04:22:55,133 DEBUG TRAIN Batch 23/4600 loss 6.423400 loss_att 10.454927 loss_ctc 8.825927 loss_rnnt 5.228862 hw_loss 0.127304 lr 0.00035674 rank 5
2023-02-25 04:24:07,342 DEBUG TRAIN Batch 23/4700 loss 7.460470 loss_att 11.789546 loss_ctc 10.713642 loss_rnnt 6.097833 hw_loss 0.118246 lr 0.00035669 rank 0
2023-02-25 04:24:07,346 DEBUG TRAIN Batch 23/4700 loss 17.579847 loss_att 21.153694 loss_ctc 23.162655 loss_rnnt 16.007971 hw_loss 0.211376 lr 0.00035673 rank 3
2023-02-25 04:24:07,348 DEBUG TRAIN Batch 23/4700 loss 14.956573 loss_att 15.254063 loss_ctc 17.378531 loss_rnnt 14.492835 hw_loss 0.152461 lr 0.00035667 rank 7
2023-02-25 04:24:07,350 DEBUG TRAIN Batch 23/4700 loss 5.120276 loss_att 11.171871 loss_ctc 9.014346 loss_rnnt 3.316632 hw_loss 0.138967 lr 0.00035669 rank 1
2023-02-25 04:24:07,352 DEBUG TRAIN Batch 23/4700 loss 11.346724 loss_att 14.369642 loss_ctc 14.501643 loss_rnnt 10.244233 hw_loss 0.144843 lr 0.00035667 rank 6
2023-02-25 04:24:07,353 DEBUG TRAIN Batch 23/4700 loss 7.029135 loss_att 9.412308 loss_ctc 12.252950 loss_rnnt 5.757987 hw_loss 0.183759 lr 0.00035665 rank 5
2023-02-25 04:24:07,353 DEBUG TRAIN Batch 23/4700 loss 6.367530 loss_att 7.715895 loss_ctc 10.899918 loss_rnnt 5.401149 hw_loss 0.173232 lr 0.00035668 rank 2
2023-02-25 04:24:07,355 DEBUG TRAIN Batch 23/4700 loss 4.788333 loss_att 7.382438 loss_ctc 5.371803 loss_rnnt 4.123859 hw_loss 0.127232 lr 0.00035670 rank 4
2023-02-25 04:25:16,947 DEBUG TRAIN Batch 23/4800 loss 8.622783 loss_att 9.939893 loss_ctc 9.975487 loss_rnnt 8.080690 hw_loss 0.184331 lr 0.00035658 rank 7
2023-02-25 04:25:16,951 DEBUG TRAIN Batch 23/4800 loss 11.379261 loss_att 14.519608 loss_ctc 15.732508 loss_rnnt 10.069859 hw_loss 0.189189 lr 0.00035656 rank 5
2023-02-25 04:25:16,956 DEBUG TRAIN Batch 23/4800 loss 8.854575 loss_att 10.626574 loss_ctc 13.604526 loss_rnnt 7.775185 hw_loss 0.171869 lr 0.00035660 rank 0
2023-02-25 04:25:16,961 DEBUG TRAIN Batch 23/4800 loss 19.898144 loss_att 27.252432 loss_ctc 28.907528 loss_rnnt 17.154779 hw_loss 0.133601 lr 0.00035659 rank 1
2023-02-25 04:25:16,964 DEBUG TRAIN Batch 23/4800 loss 4.535846 loss_att 7.566223 loss_ctc 7.759618 loss_rnnt 3.396258 hw_loss 0.194394 lr 0.00035658 rank 6
2023-02-25 04:25:16,970 DEBUG TRAIN Batch 23/4800 loss 13.738524 loss_att 15.373886 loss_ctc 18.771208 loss_rnnt 12.622150 hw_loss 0.221770 lr 0.00035659 rank 2
2023-02-25 04:25:16,973 DEBUG TRAIN Batch 23/4800 loss 7.741240 loss_att 9.011975 loss_ctc 10.292192 loss_rnnt 7.038844 hw_loss 0.202727 lr 0.00035664 rank 3
2023-02-25 04:25:17,014 DEBUG TRAIN Batch 23/4800 loss 9.097597 loss_att 10.718677 loss_ctc 16.898273 loss_rnnt 7.619631 hw_loss 0.213114 lr 0.00035661 rank 4
2023-02-25 04:26:28,925 DEBUG TRAIN Batch 23/4900 loss 2.518858 loss_att 4.896223 loss_ctc 3.632797 loss_rnnt 1.764281 hw_loss 0.244837 lr 0.00035651 rank 0
2023-02-25 04:26:28,929 DEBUG TRAIN Batch 23/4900 loss 16.042898 loss_att 22.168213 loss_ctc 23.554321 loss_rnnt 13.724236 hw_loss 0.172644 lr 0.00035655 rank 3
2023-02-25 04:26:28,929 DEBUG TRAIN Batch 23/4900 loss 9.137292 loss_att 11.751322 loss_ctc 11.871799 loss_rnnt 8.062329 hw_loss 0.351666 lr 0.00035650 rank 2
2023-02-25 04:26:28,932 DEBUG TRAIN Batch 23/4900 loss 9.613599 loss_att 11.467302 loss_ctc 13.063539 loss_rnnt 8.640012 hw_loss 0.267852 lr 0.00035647 rank 5
2023-02-25 04:26:28,931 DEBUG TRAIN Batch 23/4900 loss 6.691795 loss_att 10.185712 loss_ctc 7.297226 loss_rnnt 5.791859 hw_loss 0.225805 lr 0.00035650 rank 1
2023-02-25 04:26:28,936 DEBUG TRAIN Batch 23/4900 loss 10.887083 loss_att 11.515921 loss_ctc 13.673923 loss_rnnt 10.301549 hw_loss 0.165350 lr 0.00035649 rank 7
2023-02-25 04:26:28,937 DEBUG TRAIN Batch 23/4900 loss 9.566189 loss_att 12.642750 loss_ctc 13.801938 loss_rnnt 8.266322 hw_loss 0.224603 lr 0.00035652 rank 4
2023-02-25 04:26:28,939 DEBUG TRAIN Batch 23/4900 loss 7.110135 loss_att 9.654994 loss_ctc 8.371922 loss_rnnt 6.330966 hw_loss 0.191173 lr 0.00035649 rank 6
2023-02-25 04:27:44,257 DEBUG TRAIN Batch 23/5000 loss 10.765466 loss_att 11.746903 loss_ctc 17.699499 loss_rnnt 9.501123 hw_loss 0.269097 lr 0.00035643 rank 4
2023-02-25 04:27:44,259 DEBUG TRAIN Batch 23/5000 loss 6.036818 loss_att 6.621478 loss_ctc 9.174093 loss_rnnt 5.394059 hw_loss 0.201605 lr 0.00035646 rank 3
2023-02-25 04:27:44,259 DEBUG TRAIN Batch 23/5000 loss 9.101575 loss_att 10.103443 loss_ctc 13.276949 loss_rnnt 8.207853 hw_loss 0.256184 lr 0.00035641 rank 2
2023-02-25 04:27:44,261 DEBUG TRAIN Batch 23/5000 loss 9.680883 loss_att 12.941938 loss_ctc 14.732404 loss_rnnt 8.221786 hw_loss 0.250030 lr 0.00035640 rank 7
2023-02-25 04:27:44,261 DEBUG TRAIN Batch 23/5000 loss 14.707151 loss_att 13.485606 loss_ctc 14.337040 loss_rnnt 14.914413 hw_loss 0.161992 lr 0.00035641 rank 1
2023-02-25 04:27:44,264 DEBUG TRAIN Batch 23/5000 loss 5.616396 loss_att 6.278346 loss_ctc 7.856830 loss_rnnt 5.041757 hw_loss 0.269108 lr 0.00035642 rank 0
2023-02-25 04:27:44,265 DEBUG TRAIN Batch 23/5000 loss 12.902494 loss_att 15.063697 loss_ctc 16.602407 loss_rnnt 11.843071 hw_loss 0.250989 lr 0.00035638 rank 5
2023-02-25 04:27:44,265 DEBUG TRAIN Batch 23/5000 loss 13.422619 loss_att 16.031734 loss_ctc 15.261380 loss_rnnt 12.541292 hw_loss 0.214376 lr 0.00035640 rank 6
2023-02-25 04:28:54,535 DEBUG TRAIN Batch 23/5100 loss 5.753973 loss_att 8.843623 loss_ctc 8.850927 loss_rnnt 4.637414 hw_loss 0.160691 lr 0.00035633 rank 0
2023-02-25 04:28:54,536 DEBUG TRAIN Batch 23/5100 loss 8.013330 loss_att 10.319305 loss_ctc 10.179749 loss_rnnt 7.196777 hw_loss 0.124692 lr 0.00035637 rank 3
2023-02-25 04:28:54,538 DEBUG TRAIN Batch 23/5100 loss 9.400644 loss_att 12.148773 loss_ctc 11.327059 loss_rnnt 8.521862 hw_loss 0.135565 lr 0.00035631 rank 6
2023-02-25 04:28:54,538 DEBUG TRAIN Batch 23/5100 loss 10.020330 loss_att 10.702991 loss_ctc 12.551083 loss_rnnt 9.446424 hw_loss 0.187390 lr 0.00035632 rank 1
2023-02-25 04:28:54,539 DEBUG TRAIN Batch 23/5100 loss 9.044360 loss_att 9.942009 loss_ctc 11.159114 loss_rnnt 8.420309 hw_loss 0.304790 lr 0.00035632 rank 2
2023-02-25 04:28:54,541 DEBUG TRAIN Batch 23/5100 loss 2.188326 loss_att 5.023934 loss_ctc 3.599494 loss_rnnt 1.330659 hw_loss 0.191982 lr 0.00035631 rank 7
2023-02-25 04:28:54,543 DEBUG TRAIN Batch 23/5100 loss 12.105542 loss_att 13.572848 loss_ctc 15.305287 loss_rnnt 11.265266 hw_loss 0.225342 lr 0.00035634 rank 4
2023-02-25 04:28:54,549 DEBUG TRAIN Batch 23/5100 loss 5.468499 loss_att 5.834345 loss_ctc 8.670938 loss_rnnt 4.853273 hw_loss 0.215746 lr 0.00035628 rank 5
2023-02-25 04:30:06,205 DEBUG TRAIN Batch 23/5200 loss 6.062137 loss_att 10.550125 loss_ctc 7.861536 loss_rnnt 4.793880 hw_loss 0.245135 lr 0.00035628 rank 3
2023-02-25 04:30:06,207 DEBUG TRAIN Batch 23/5200 loss 12.549236 loss_att 13.663247 loss_ctc 16.247974 loss_rnnt 11.687713 hw_loss 0.272919 lr 0.00035623 rank 1
2023-02-25 04:30:06,208 DEBUG TRAIN Batch 23/5200 loss 3.978980 loss_att 8.218490 loss_ctc 5.814247 loss_rnnt 2.762747 hw_loss 0.231804 lr 0.00035624 rank 0
2023-02-25 04:30:06,209 DEBUG TRAIN Batch 23/5200 loss 12.429457 loss_att 13.975065 loss_ctc 21.668514 loss_rnnt 10.785170 hw_loss 0.193672 lr 0.00035622 rank 7
2023-02-25 04:30:06,210 DEBUG TRAIN Batch 23/5200 loss 9.262371 loss_att 10.691827 loss_ctc 12.444199 loss_rnnt 8.422647 hw_loss 0.242977 lr 0.00035622 rank 6
2023-02-25 04:30:06,212 DEBUG TRAIN Batch 23/5200 loss 7.249197 loss_att 13.772570 loss_ctc 14.555679 loss_rnnt 4.819472 hw_loss 0.282849 lr 0.00035625 rank 4
2023-02-25 04:30:06,214 DEBUG TRAIN Batch 23/5200 loss 11.959841 loss_att 14.854478 loss_ctc 16.295734 loss_rnnt 10.724812 hw_loss 0.146217 lr 0.00035619 rank 5
2023-02-25 04:30:06,214 DEBUG TRAIN Batch 23/5200 loss 12.083141 loss_att 15.383240 loss_ctc 15.193867 loss_rnnt 10.907166 hw_loss 0.189738 lr 0.00035623 rank 2
2023-02-25 04:31:19,739 DEBUG TRAIN Batch 23/5300 loss 4.412212 loss_att 5.173435 loss_ctc 5.685701 loss_rnnt 4.015125 hw_loss 0.140708 lr 0.00035616 rank 4
2023-02-25 04:31:19,742 DEBUG TRAIN Batch 23/5300 loss 6.381468 loss_att 9.978686 loss_ctc 9.662230 loss_rnnt 5.121926 hw_loss 0.192493 lr 0.00035615 rank 0
2023-02-25 04:31:19,744 DEBUG TRAIN Batch 23/5300 loss 4.449005 loss_att 6.740674 loss_ctc 5.773143 loss_rnnt 3.748381 hw_loss 0.123259 lr 0.00035612 rank 7
2023-02-25 04:31:19,745 DEBUG TRAIN Batch 23/5300 loss 7.594237 loss_att 11.582582 loss_ctc 10.577375 loss_rnnt 6.293728 hw_loss 0.197041 lr 0.00035614 rank 2
2023-02-25 04:31:19,748 DEBUG TRAIN Batch 23/5300 loss 10.497249 loss_att 10.985882 loss_ctc 12.923180 loss_rnnt 9.977876 hw_loss 0.184103 lr 0.00035610 rank 5
2023-02-25 04:31:19,749 DEBUG TRAIN Batch 23/5300 loss 4.780267 loss_att 10.076471 loss_ctc 8.239911 loss_rnnt 3.143511 hw_loss 0.217930 lr 0.00035613 rank 6
2023-02-25 04:31:19,752 DEBUG TRAIN Batch 23/5300 loss 2.116699 loss_att 4.214653 loss_ctc 1.653513 loss_rnnt 1.644643 hw_loss 0.214168 lr 0.00035614 rank 1
2023-02-25 04:31:20,082 DEBUG TRAIN Batch 23/5300 loss 3.306955 loss_att 7.924843 loss_ctc 4.199571 loss_rnnt 2.146698 hw_loss 0.220621 lr 0.00035619 rank 3
2023-02-25 04:32:31,804 DEBUG TRAIN Batch 23/5400 loss 7.350951 loss_att 9.761189 loss_ctc 9.451876 loss_rnnt 6.452982 hw_loss 0.254621 lr 0.00035610 rank 3
2023-02-25 04:32:31,809 DEBUG TRAIN Batch 23/5400 loss 7.070348 loss_att 11.772339 loss_ctc 13.664030 loss_rnnt 5.178436 hw_loss 0.135668 lr 0.00035605 rank 1
2023-02-25 04:32:31,811 DEBUG TRAIN Batch 23/5400 loss 9.158444 loss_att 12.624001 loss_ctc 13.900297 loss_rnnt 7.700238 hw_loss 0.249088 lr 0.00035606 rank 0
2023-02-25 04:32:31,810 DEBUG TRAIN Batch 23/5400 loss 4.539980 loss_att 7.947413 loss_ctc 7.072973 loss_rnnt 3.416788 hw_loss 0.194950 lr 0.00035607 rank 4
2023-02-25 04:32:31,813 DEBUG TRAIN Batch 23/5400 loss 5.339014 loss_att 9.176858 loss_ctc 9.084069 loss_rnnt 3.915091 hw_loss 0.294401 lr 0.00035601 rank 5
2023-02-25 04:32:31,814 DEBUG TRAIN Batch 23/5400 loss 6.955546 loss_att 11.883573 loss_ctc 10.621259 loss_rnnt 5.388824 hw_loss 0.173166 lr 0.00035605 rank 2
2023-02-25 04:32:31,814 DEBUG TRAIN Batch 23/5400 loss 12.614803 loss_att 15.293770 loss_ctc 17.286211 loss_rnnt 11.321794 hw_loss 0.251928 lr 0.00035604 rank 6
2023-02-25 04:32:31,816 DEBUG TRAIN Batch 23/5400 loss 7.872458 loss_att 11.800997 loss_ctc 12.310627 loss_rnnt 6.413412 hw_loss 0.152965 lr 0.00035603 rank 7
2023-02-25 04:33:41,939 DEBUG TRAIN Batch 23/5500 loss 7.005988 loss_att 9.264198 loss_ctc 9.853265 loss_rnnt 6.125633 hw_loss 0.092018 lr 0.00035597 rank 0
2023-02-25 04:33:41,952 DEBUG TRAIN Batch 23/5500 loss 6.112541 loss_att 8.503188 loss_ctc 7.926817 loss_rnnt 5.279398 hw_loss 0.212082 lr 0.00035596 rank 2
2023-02-25 04:33:41,952 DEBUG TRAIN Batch 23/5500 loss 8.734482 loss_att 13.445330 loss_ctc 13.334026 loss_rnnt 7.063428 hw_loss 0.216771 lr 0.00035596 rank 1
2023-02-25 04:33:41,953 DEBUG TRAIN Batch 23/5500 loss 15.256149 loss_att 18.916105 loss_ctc 21.683704 loss_rnnt 13.583193 hw_loss 0.157420 lr 0.00035594 rank 7
2023-02-25 04:33:41,954 DEBUG TRAIN Batch 23/5500 loss 14.221083 loss_att 16.426191 loss_ctc 17.931759 loss_rnnt 13.210884 hw_loss 0.139538 lr 0.00035598 rank 4
2023-02-25 04:33:41,960 DEBUG TRAIN Batch 23/5500 loss 5.584943 loss_att 9.664049 loss_ctc 6.876173 loss_rnnt 4.494592 hw_loss 0.191936 lr 0.00035592 rank 5
2023-02-25 04:33:41,964 DEBUG TRAIN Batch 23/5500 loss 9.584014 loss_att 12.032296 loss_ctc 13.361135 loss_rnnt 8.460659 hw_loss 0.243905 lr 0.00035595 rank 6
2023-02-25 04:33:42,001 DEBUG TRAIN Batch 23/5500 loss 9.982889 loss_att 13.788407 loss_ctc 14.333833 loss_rnnt 8.519969 hw_loss 0.228172 lr 0.00035601 rank 3
2023-02-25 04:34:53,497 DEBUG TRAIN Batch 23/5600 loss 20.825453 loss_att 21.472233 loss_ctc 24.342815 loss_rnnt 20.133945 hw_loss 0.174694 lr 0.00035588 rank 0
2023-02-25 04:34:53,501 DEBUG TRAIN Batch 23/5600 loss 8.044117 loss_att 10.069131 loss_ctc 13.380209 loss_rnnt 6.824527 hw_loss 0.193328 lr 0.00035587 rank 2
2023-02-25 04:34:53,501 DEBUG TRAIN Batch 23/5600 loss 8.884424 loss_att 10.200625 loss_ctc 13.499800 loss_rnnt 7.886327 hw_loss 0.224014 lr 0.00035586 rank 6
2023-02-25 04:34:53,502 DEBUG TRAIN Batch 23/5600 loss 7.144300 loss_att 10.466432 loss_ctc 11.205620 loss_rnnt 5.809252 hw_loss 0.242086 lr 0.00035589 rank 4
2023-02-25 04:34:53,504 DEBUG TRAIN Batch 23/5600 loss 9.030087 loss_att 12.080394 loss_ctc 14.687502 loss_rnnt 7.597308 hw_loss 0.128241 lr 0.00035587 rank 1
2023-02-25 04:34:53,507 DEBUG TRAIN Batch 23/5600 loss 10.845547 loss_att 13.321401 loss_ctc 13.713988 loss_rnnt 9.814688 hw_loss 0.287305 lr 0.00035585 rank 7
2023-02-25 04:34:53,508 DEBUG TRAIN Batch 23/5600 loss 11.240839 loss_att 12.877609 loss_ctc 16.304010 loss_rnnt 10.129674 hw_loss 0.203851 lr 0.00035583 rank 5
2023-02-25 04:34:53,553 DEBUG TRAIN Batch 23/5600 loss 7.505190 loss_att 8.377499 loss_ctc 9.387452 loss_rnnt 6.950101 hw_loss 0.243112 lr 0.00035592 rank 3
2023-02-25 04:36:08,993 DEBUG TRAIN Batch 23/5700 loss 8.765354 loss_att 11.505112 loss_ctc 15.475459 loss_rnnt 7.183931 hw_loss 0.260233 lr 0.00035577 rank 6
2023-02-25 04:36:08,993 DEBUG TRAIN Batch 23/5700 loss 11.411585 loss_att 12.055953 loss_ctc 15.061251 loss_rnnt 10.645633 hw_loss 0.282106 lr 0.00035583 rank 3
2023-02-25 04:36:08,994 DEBUG TRAIN Batch 23/5700 loss 2.053208 loss_att 4.250745 loss_ctc 3.284518 loss_rnnt 1.334072 hw_loss 0.216476 lr 0.00035578 rank 1
2023-02-25 04:36:08,998 DEBUG TRAIN Batch 23/5700 loss 7.624438 loss_att 7.669575 loss_ctc 9.529822 loss_rnnt 7.195230 hw_loss 0.311492 lr 0.00035579 rank 0
2023-02-25 04:36:08,999 DEBUG TRAIN Batch 23/5700 loss 10.205098 loss_att 9.962464 loss_ctc 11.837554 loss_rnnt 9.929112 hw_loss 0.200345 lr 0.00035576 rank 7
2023-02-25 04:36:08,999 DEBUG TRAIN Batch 23/5700 loss 11.640499 loss_att 12.119734 loss_ctc 14.211430 loss_rnnt 11.066474 hw_loss 0.253854 lr 0.00035578 rank 2
2023-02-25 04:36:08,999 DEBUG TRAIN Batch 23/5700 loss 15.838118 loss_att 16.620018 loss_ctc 20.058317 loss_rnnt 14.986625 hw_loss 0.248287 lr 0.00035580 rank 4
2023-02-25 04:36:09,004 DEBUG TRAIN Batch 23/5700 loss 6.621645 loss_att 7.954476 loss_ctc 8.296405 loss_rnnt 5.960569 hw_loss 0.321015 lr 0.00035574 rank 5
2023-02-25 04:37:19,334 DEBUG TRAIN Batch 23/5800 loss 11.418308 loss_att 15.648660 loss_ctc 18.465137 loss_rnnt 9.518841 hw_loss 0.213412 lr 0.00035569 rank 1
2023-02-25 04:37:19,338 DEBUG TRAIN Batch 23/5800 loss 18.945961 loss_att 19.676540 loss_ctc 25.904867 loss_rnnt 17.749781 hw_loss 0.229146 lr 0.00035569 rank 2
2023-02-25 04:37:19,338 DEBUG TRAIN Batch 23/5800 loss 7.589561 loss_att 10.148411 loss_ctc 10.346304 loss_rnnt 6.615247 hw_loss 0.178086 lr 0.00035570 rank 0
2023-02-25 04:37:19,338 DEBUG TRAIN Batch 23/5800 loss 8.708616 loss_att 13.612927 loss_ctc 12.522601 loss_rnnt 7.125007 hw_loss 0.176653 lr 0.00035574 rank 3
2023-02-25 04:37:19,339 DEBUG TRAIN Batch 23/5800 loss 5.089949 loss_att 7.738012 loss_ctc 7.620836 loss_rnnt 4.147264 hw_loss 0.141790 lr 0.00035568 rank 6
2023-02-25 04:37:19,342 DEBUG TRAIN Batch 23/5800 loss 5.040689 loss_att 6.441775 loss_ctc 7.333304 loss_rnnt 4.332638 hw_loss 0.229034 lr 0.00035571 rank 4
2023-02-25 04:37:19,344 DEBUG TRAIN Batch 23/5800 loss 3.957757 loss_att 8.220055 loss_ctc 7.285552 loss_rnnt 2.525788 hw_loss 0.254630 lr 0.00035567 rank 7
2023-02-25 04:37:19,345 DEBUG TRAIN Batch 23/5800 loss 7.360106 loss_att 8.794006 loss_ctc 10.738436 loss_rnnt 6.512812 hw_loss 0.206382 lr 0.00035565 rank 5
2023-02-25 04:38:29,374 DEBUG TRAIN Batch 23/5900 loss 4.994677 loss_att 8.534427 loss_ctc 10.798285 loss_rnnt 3.371011 hw_loss 0.266067 lr 0.00035562 rank 4
2023-02-25 04:38:29,387 DEBUG TRAIN Batch 23/5900 loss 10.355931 loss_att 12.841568 loss_ctc 13.921097 loss_rnnt 9.280312 hw_loss 0.193381 lr 0.00035565 rank 3
2023-02-25 04:38:29,393 DEBUG TRAIN Batch 23/5900 loss 6.932830 loss_att 9.336785 loss_ctc 13.637992 loss_rnnt 5.424244 hw_loss 0.250825 lr 0.00035556 rank 5
2023-02-25 04:38:29,393 DEBUG TRAIN Batch 23/5900 loss 9.581715 loss_att 11.266232 loss_ctc 14.570686 loss_rnnt 8.461129 hw_loss 0.222161 lr 0.00035559 rank 6
2023-02-25 04:38:29,394 DEBUG TRAIN Batch 23/5900 loss 5.083634 loss_att 7.803263 loss_ctc 6.758859 loss_rnnt 4.223347 hw_loss 0.174371 lr 0.00035561 rank 0
2023-02-25 04:38:29,394 DEBUG TRAIN Batch 23/5900 loss 13.192009 loss_att 16.719810 loss_ctc 21.178608 loss_rnnt 11.297073 hw_loss 0.233428 lr 0.00035558 rank 7
2023-02-25 04:38:29,395 DEBUG TRAIN Batch 23/5900 loss 6.752417 loss_att 9.342699 loss_ctc 10.447143 loss_rnnt 5.619256 hw_loss 0.229638 lr 0.00035560 rank 2
2023-02-25 04:38:29,396 DEBUG TRAIN Batch 23/5900 loss 5.894729 loss_att 8.130830 loss_ctc 8.617466 loss_rnnt 5.001776 hw_loss 0.155064 lr 0.00035560 rank 1
2023-02-25 04:39:41,860 DEBUG TRAIN Batch 23/6000 loss 7.130381 loss_att 9.635118 loss_ctc 8.124739 loss_rnnt 6.415830 hw_loss 0.151916 lr 0.00035550 rank 6
2023-02-25 04:39:41,872 DEBUG TRAIN Batch 23/6000 loss 7.812235 loss_att 10.651434 loss_ctc 13.153374 loss_rnnt 6.443342 hw_loss 0.166692 lr 0.00035556 rank 3
2023-02-25 04:39:41,874 DEBUG TRAIN Batch 23/6000 loss 6.304262 loss_att 8.791689 loss_ctc 10.022700 loss_rnnt 5.253874 hw_loss 0.107082 lr 0.00035552 rank 0
2023-02-25 04:39:41,875 DEBUG TRAIN Batch 23/6000 loss 5.066802 loss_att 10.027655 loss_ctc 8.350027 loss_rnnt 3.542605 hw_loss 0.176742 lr 0.00035551 rank 1
2023-02-25 04:39:41,876 DEBUG TRAIN Batch 23/6000 loss 9.415450 loss_att 14.492351 loss_ctc 15.780528 loss_rnnt 7.436812 hw_loss 0.214840 lr 0.00035549 rank 7
2023-02-25 04:39:41,876 DEBUG TRAIN Batch 23/6000 loss 4.373308 loss_att 10.896431 loss_ctc 4.710924 loss_rnnt 2.927437 hw_loss 0.180433 lr 0.00035551 rank 2
2023-02-25 04:39:41,878 DEBUG TRAIN Batch 23/6000 loss 14.291447 loss_att 14.756471 loss_ctc 20.486538 loss_rnnt 13.247145 hw_loss 0.234912 lr 0.00035547 rank 5
2023-02-25 04:39:41,879 DEBUG TRAIN Batch 23/6000 loss 3.327665 loss_att 7.084355 loss_ctc 7.489047 loss_rnnt 1.864157 hw_loss 0.294972 lr 0.00035553 rank 4
2023-02-25 04:40:54,077 DEBUG TRAIN Batch 23/6100 loss 6.151642 loss_att 10.032141 loss_ctc 8.836796 loss_rnnt 4.906518 hw_loss 0.208133 lr 0.00035543 rank 0
2023-02-25 04:40:54,079 DEBUG TRAIN Batch 23/6100 loss 9.113782 loss_att 11.651543 loss_ctc 14.670732 loss_rnnt 7.752984 hw_loss 0.210597 lr 0.00035541 rank 6
2023-02-25 04:40:54,081 DEBUG TRAIN Batch 23/6100 loss 6.805413 loss_att 11.488579 loss_ctc 8.914556 loss_rnnt 5.467262 hw_loss 0.225560 lr 0.00035547 rank 3
2023-02-25 04:40:54,083 DEBUG TRAIN Batch 23/6100 loss 6.348889 loss_att 9.768332 loss_ctc 12.639197 loss_rnnt 4.690432 hw_loss 0.254738 lr 0.00035538 rank 5
2023-02-25 04:40:54,084 DEBUG TRAIN Batch 23/6100 loss 10.827805 loss_att 13.357166 loss_ctc 16.177795 loss_rnnt 9.525767 hw_loss 0.155311 lr 0.00035540 rank 7
2023-02-25 04:40:54,089 DEBUG TRAIN Batch 23/6100 loss 4.807633 loss_att 8.951180 loss_ctc 8.006752 loss_rnnt 3.374224 hw_loss 0.334031 lr 0.00035542 rank 1
2023-02-25 04:40:54,093 DEBUG TRAIN Batch 23/6100 loss 7.997705 loss_att 11.672011 loss_ctc 10.555491 loss_rnnt 6.801548 hw_loss 0.225484 lr 0.00035544 rank 4
2023-02-25 04:40:54,096 DEBUG TRAIN Batch 23/6100 loss 5.591869 loss_att 7.569182 loss_ctc 5.445510 loss_rnnt 5.090019 hw_loss 0.236066 lr 0.00035542 rank 2
2023-02-25 04:42:04,141 DEBUG TRAIN Batch 23/6200 loss 18.857929 loss_att 23.914387 loss_ctc 24.329523 loss_rnnt 17.016277 hw_loss 0.189028 lr 0.00035534 rank 0
2023-02-25 04:42:04,141 DEBUG TRAIN Batch 23/6200 loss 9.012932 loss_att 10.889187 loss_ctc 9.965412 loss_rnnt 8.408265 hw_loss 0.192035 lr 0.00035535 rank 4
2023-02-25 04:42:04,143 DEBUG TRAIN Batch 23/6200 loss 6.573583 loss_att 8.665793 loss_ctc 9.617995 loss_rnnt 5.627209 hw_loss 0.228768 lr 0.00035538 rank 3
2023-02-25 04:42:04,143 DEBUG TRAIN Batch 23/6200 loss 10.938393 loss_att 12.852738 loss_ctc 15.296377 loss_rnnt 9.802706 hw_loss 0.322038 lr 0.00035531 rank 7
2023-02-25 04:42:04,144 DEBUG TRAIN Batch 23/6200 loss 4.261452 loss_att 6.344011 loss_ctc 6.347003 loss_rnnt 3.435267 hw_loss 0.246749 lr 0.00035529 rank 5
2023-02-25 04:42:04,146 DEBUG TRAIN Batch 23/6200 loss 10.355280 loss_att 15.301462 loss_ctc 17.373581 loss_rnnt 8.273855 hw_loss 0.293278 lr 0.00035533 rank 1
2023-02-25 04:42:04,149 DEBUG TRAIN Batch 23/6200 loss 12.296159 loss_att 13.108656 loss_ctc 18.533134 loss_rnnt 11.218771 hw_loss 0.156174 lr 0.00035533 rank 2
2023-02-25 04:42:04,151 DEBUG TRAIN Batch 23/6200 loss 5.545034 loss_att 8.536703 loss_ctc 6.164440 loss_rnnt 4.779290 hw_loss 0.159043 lr 0.00035532 rank 6
2023-02-25 04:43:15,080 DEBUG TRAIN Batch 23/6300 loss 8.154909 loss_att 10.192917 loss_ctc 12.824423 loss_rnnt 7.016808 hw_loss 0.202310 lr 0.00035525 rank 0
2023-02-25 04:43:15,086 DEBUG TRAIN Batch 23/6300 loss 9.963788 loss_att 11.502527 loss_ctc 14.446968 loss_rnnt 8.919408 hw_loss 0.260392 lr 0.00035524 rank 1
2023-02-25 04:43:15,086 DEBUG TRAIN Batch 23/6300 loss 11.020291 loss_att 13.584293 loss_ctc 15.626784 loss_rnnt 9.774665 hw_loss 0.222426 lr 0.00035529 rank 3
2023-02-25 04:43:15,087 DEBUG TRAIN Batch 23/6300 loss 8.307340 loss_att 9.717247 loss_ctc 13.841892 loss_rnnt 7.149746 hw_loss 0.258134 lr 0.00035523 rank 7
2023-02-25 04:43:15,087 DEBUG TRAIN Batch 23/6300 loss 3.628610 loss_att 5.785034 loss_ctc 5.825484 loss_rnnt 2.789771 hw_loss 0.214946 lr 0.00035526 rank 4
2023-02-25 04:43:15,088 DEBUG TRAIN Batch 23/6300 loss 6.945340 loss_att 8.835886 loss_ctc 9.832685 loss_rnnt 6.017937 hw_loss 0.308092 lr 0.00035524 rank 2
2023-02-25 04:43:15,096 DEBUG TRAIN Batch 23/6300 loss 9.970427 loss_att 12.673639 loss_ctc 13.261423 loss_rnnt 8.881592 hw_loss 0.205112 lr 0.00035520 rank 5
2023-02-25 04:43:15,097 DEBUG TRAIN Batch 23/6300 loss 11.298154 loss_att 15.451528 loss_ctc 15.856711 loss_rnnt 9.733949 hw_loss 0.235728 lr 0.00035523 rank 6
2023-02-25 04:44:28,568 DEBUG TRAIN Batch 23/6400 loss 6.476260 loss_att 8.059421 loss_ctc 10.692962 loss_rnnt 5.455431 hw_loss 0.266191 lr 0.00035515 rank 1
2023-02-25 04:44:28,574 DEBUG TRAIN Batch 23/6400 loss 7.893179 loss_att 10.667482 loss_ctc 8.174636 loss_rnnt 7.185618 hw_loss 0.215949 lr 0.00035516 rank 0
2023-02-25 04:44:28,573 DEBUG TRAIN Batch 23/6400 loss 11.195802 loss_att 11.528131 loss_ctc 14.151097 loss_rnnt 10.588575 hw_loss 0.275102 lr 0.00035517 rank 4
2023-02-25 04:44:28,574 DEBUG TRAIN Batch 23/6400 loss 11.358264 loss_att 12.223436 loss_ctc 13.646297 loss_rnnt 10.740722 hw_loss 0.261445 lr 0.00035515 rank 2
2023-02-25 04:44:28,575 DEBUG TRAIN Batch 23/6400 loss 16.217304 loss_att 16.073196 loss_ctc 23.105871 loss_rnnt 15.198916 hw_loss 0.241375 lr 0.00035514 rank 7
2023-02-25 04:44:28,580 DEBUG TRAIN Batch 23/6400 loss 17.296303 loss_att 15.800838 loss_ctc 20.640722 loss_rnnt 17.069660 hw_loss 0.149651 lr 0.00035514 rank 6
2023-02-25 04:44:28,597 DEBUG TRAIN Batch 23/6400 loss 6.439225 loss_att 12.838388 loss_ctc 11.533512 loss_rnnt 4.364545 hw_loss 0.216766 lr 0.00035511 rank 5
2023-02-25 04:44:28,604 DEBUG TRAIN Batch 23/6400 loss 9.608656 loss_att 13.182348 loss_ctc 13.310051 loss_rnnt 8.274849 hw_loss 0.235405 lr 0.00035520 rank 3
2023-02-25 04:45:38,966 DEBUG TRAIN Batch 23/6500 loss 10.478903 loss_att 11.716965 loss_ctc 12.594845 loss_rnnt 9.859299 hw_loss 0.168500 lr 0.00035505 rank 6
2023-02-25 04:45:38,970 DEBUG TRAIN Batch 23/6500 loss 9.611055 loss_att 13.458071 loss_ctc 12.755511 loss_rnnt 8.351015 hw_loss 0.133829 lr 0.00035507 rank 0
2023-02-25 04:45:38,970 DEBUG TRAIN Batch 23/6500 loss 7.221561 loss_att 13.459555 loss_ctc 11.513736 loss_rnnt 5.290829 hw_loss 0.207831 lr 0.00035511 rank 3
2023-02-25 04:45:38,973 DEBUG TRAIN Batch 23/6500 loss 16.423914 loss_att 21.006639 loss_ctc 26.038595 loss_rnnt 14.124319 hw_loss 0.189546 lr 0.00035505 rank 7
2023-02-25 04:45:38,976 DEBUG TRAIN Batch 23/6500 loss 8.579239 loss_att 13.775901 loss_ctc 16.439459 loss_rnnt 6.429630 hw_loss 0.116713 lr 0.00035506 rank 2
2023-02-25 04:45:38,980 DEBUG TRAIN Batch 23/6500 loss 8.571642 loss_att 8.059033 loss_ctc 10.853675 loss_rnnt 8.265188 hw_loss 0.196322 lr 0.00035506 rank 1
2023-02-25 04:45:38,983 DEBUG TRAIN Batch 23/6500 loss 19.390842 loss_att 22.082359 loss_ctc 29.095259 loss_rnnt 17.487494 hw_loss 0.133351 lr 0.00035508 rank 4
2023-02-25 04:45:39,019 DEBUG TRAIN Batch 23/6500 loss 6.037043 loss_att 9.743905 loss_ctc 9.151542 loss_rnnt 4.782328 hw_loss 0.183892 lr 0.00035503 rank 5
2023-02-25 04:46:49,253 DEBUG TRAIN Batch 23/6600 loss 6.376789 loss_att 7.624385 loss_ctc 9.619006 loss_rnnt 5.583066 hw_loss 0.209827 lr 0.00035498 rank 0
2023-02-25 04:46:49,261 DEBUG TRAIN Batch 23/6600 loss 7.051137 loss_att 9.543257 loss_ctc 8.816862 loss_rnnt 6.272405 hw_loss 0.084148 lr 0.00035499 rank 4
2023-02-25 04:46:49,262 DEBUG TRAIN Batch 23/6600 loss 2.428949 loss_att 4.949268 loss_ctc 5.748741 loss_rnnt 1.405714 hw_loss 0.143500 lr 0.00035496 rank 6
2023-02-25 04:46:49,265 DEBUG TRAIN Batch 23/6600 loss 9.013928 loss_att 12.835590 loss_ctc 13.984356 loss_rnnt 7.539266 hw_loss 0.089261 lr 0.00035497 rank 2
2023-02-25 04:46:49,265 DEBUG TRAIN Batch 23/6600 loss 7.417335 loss_att 10.139526 loss_ctc 13.613844 loss_rnnt 5.918508 hw_loss 0.240351 lr 0.00035496 rank 7
2023-02-25 04:46:49,265 DEBUG TRAIN Batch 23/6600 loss 7.131135 loss_att 12.644719 loss_ctc 11.508784 loss_rnnt 5.344253 hw_loss 0.188400 lr 0.00035502 rank 3
2023-02-25 04:46:49,267 DEBUG TRAIN Batch 23/6600 loss 6.631033 loss_att 10.010124 loss_ctc 6.127910 loss_rnnt 5.900839 hw_loss 0.227736 lr 0.00035497 rank 1
2023-02-25 04:46:49,270 DEBUG TRAIN Batch 23/6600 loss 13.963099 loss_att 17.288017 loss_ctc 20.737232 loss_rnnt 12.316559 hw_loss 0.146888 lr 0.00035494 rank 5
2023-02-25 04:48:00,447 DEBUG TRAIN Batch 23/6700 loss 6.363167 loss_att 9.880184 loss_ctc 11.114247 loss_rnnt 4.974516 hw_loss 0.097069 lr 0.00035489 rank 0
2023-02-25 04:48:00,449 DEBUG TRAIN Batch 23/6700 loss 6.181556 loss_att 10.204568 loss_ctc 9.734367 loss_rnnt 4.850615 hw_loss 0.098681 lr 0.00035488 rank 1
2023-02-25 04:48:00,451 DEBUG TRAIN Batch 23/6700 loss 9.816667 loss_att 12.291727 loss_ctc 13.118751 loss_rnnt 8.779612 hw_loss 0.190809 lr 0.00035487 rank 6
2023-02-25 04:48:00,452 DEBUG TRAIN Batch 23/6700 loss 4.156430 loss_att 5.376332 loss_ctc 7.252274 loss_rnnt 3.364146 hw_loss 0.254109 lr 0.00035490 rank 4
2023-02-25 04:48:00,453 DEBUG TRAIN Batch 23/6700 loss 10.905453 loss_att 13.689504 loss_ctc 17.929081 loss_rnnt 9.331841 hw_loss 0.150595 lr 0.00035487 rank 7
2023-02-25 04:48:00,455 DEBUG TRAIN Batch 23/6700 loss 6.893945 loss_att 9.676044 loss_ctc 9.054321 loss_rnnt 5.943285 hw_loss 0.199105 lr 0.00035493 rank 3
2023-02-25 04:48:00,472 DEBUG TRAIN Batch 23/6700 loss 7.435579 loss_att 8.699384 loss_ctc 9.669363 loss_rnnt 6.752004 hw_loss 0.249331 lr 0.00035488 rank 2
2023-02-25 04:48:00,498 DEBUG TRAIN Batch 23/6700 loss 7.138046 loss_att 10.447048 loss_ctc 10.797120 loss_rnnt 5.834076 hw_loss 0.289299 lr 0.00035485 rank 5
2023-02-25 04:49:12,747 DEBUG TRAIN Batch 23/6800 loss 10.117514 loss_att 11.025607 loss_ctc 15.207684 loss_rnnt 9.141916 hw_loss 0.216166 lr 0.00035478 rank 7
2023-02-25 04:49:12,748 DEBUG TRAIN Batch 23/6800 loss 7.560035 loss_att 10.902439 loss_ctc 14.614780 loss_rnnt 5.829697 hw_loss 0.227296 lr 0.00035476 rank 5
2023-02-25 04:49:12,749 DEBUG TRAIN Batch 23/6800 loss 6.675403 loss_att 8.903129 loss_ctc 10.075712 loss_rnnt 5.663262 hw_loss 0.212288 lr 0.00035480 rank 0
2023-02-25 04:49:12,748 DEBUG TRAIN Batch 23/6800 loss 4.959942 loss_att 6.285378 loss_ctc 6.795005 loss_rnnt 4.416996 hw_loss 0.062219 lr 0.00035481 rank 4
2023-02-25 04:49:12,751 DEBUG TRAIN Batch 23/6800 loss 4.412650 loss_att 5.200171 loss_ctc 5.060988 loss_rnnt 4.084146 hw_loss 0.158540 lr 0.00035478 rank 6
2023-02-25 04:49:12,753 DEBUG TRAIN Batch 23/6800 loss 8.100141 loss_att 11.560356 loss_ctc 12.192175 loss_rnnt 6.768399 hw_loss 0.176424 lr 0.00035479 rank 1
2023-02-25 04:49:12,753 DEBUG TRAIN Batch 23/6800 loss 6.577621 loss_att 10.022293 loss_ctc 11.719661 loss_rnnt 5.073477 hw_loss 0.243008 lr 0.00035484 rank 3
2023-02-25 04:49:12,754 DEBUG TRAIN Batch 23/6800 loss 5.052728 loss_att 9.061249 loss_ctc 6.250264 loss_rnnt 4.013885 hw_loss 0.145251 lr 0.00035479 rank 2
2023-02-25 04:50:23,757 DEBUG TRAIN Batch 23/6900 loss 3.784009 loss_att 6.894005 loss_ctc 5.059526 loss_rnnt 2.866394 hw_loss 0.235401 lr 0.00035471 rank 1
2023-02-25 04:50:23,759 DEBUG TRAIN Batch 23/6900 loss 8.937588 loss_att 11.465418 loss_ctc 12.253452 loss_rnnt 7.895117 hw_loss 0.177728 lr 0.00035471 rank 0
2023-02-25 04:50:23,758 DEBUG TRAIN Batch 23/6900 loss 7.412876 loss_att 10.954417 loss_ctc 11.506254 loss_rnnt 6.041182 hw_loss 0.220503 lr 0.00035470 rank 2
2023-02-25 04:50:23,759 DEBUG TRAIN Batch 23/6900 loss 11.083639 loss_att 13.879153 loss_ctc 16.298361 loss_rnnt 9.699436 hw_loss 0.243382 lr 0.00035469 rank 6
2023-02-25 04:50:23,761 DEBUG TRAIN Batch 23/6900 loss 6.905627 loss_att 9.581387 loss_ctc 9.178237 loss_rnnt 5.942399 hw_loss 0.234491 lr 0.00035469 rank 7
2023-02-25 04:50:23,764 DEBUG TRAIN Batch 23/6900 loss 13.210812 loss_att 16.050789 loss_ctc 22.135002 loss_rnnt 11.397242 hw_loss 0.104404 lr 0.00035475 rank 3
2023-02-25 04:50:23,765 DEBUG TRAIN Batch 23/6900 loss 6.328068 loss_att 9.217371 loss_ctc 11.924153 loss_rnnt 4.861911 hw_loss 0.266535 lr 0.00035467 rank 5
2023-02-25 04:50:23,768 DEBUG TRAIN Batch 23/6900 loss 7.601628 loss_att 9.155085 loss_ctc 10.293001 loss_rnnt 6.845549 hw_loss 0.162258 lr 0.00035472 rank 4
2023-02-25 04:51:34,693 DEBUG TRAIN Batch 23/7000 loss 7.587657 loss_att 7.346857 loss_ctc 10.867266 loss_rnnt 7.044506 hw_loss 0.288808 lr 0.00035462 rank 0
2023-02-25 04:51:34,699 DEBUG TRAIN Batch 23/7000 loss 8.292119 loss_att 8.619280 loss_ctc 9.682119 loss_rnnt 8.004449 hw_loss 0.069196 lr 0.00035462 rank 1
2023-02-25 04:51:34,700 DEBUG TRAIN Batch 23/7000 loss 4.361707 loss_att 5.500032 loss_ctc 5.728726 loss_rnnt 3.828483 hw_loss 0.231167 lr 0.00035461 rank 2
2023-02-25 04:51:34,701 DEBUG TRAIN Batch 23/7000 loss 15.190660 loss_att 16.431625 loss_ctc 20.922327 loss_rnnt 14.029386 hw_loss 0.279111 lr 0.00035458 rank 5
2023-02-25 04:51:34,702 DEBUG TRAIN Batch 23/7000 loss 10.244695 loss_att 13.034613 loss_ctc 14.746473 loss_rnnt 8.973125 hw_loss 0.212529 lr 0.00035460 rank 6
2023-02-25 04:51:34,704 DEBUG TRAIN Batch 23/7000 loss 9.969341 loss_att 13.988781 loss_ctc 21.288025 loss_rnnt 7.570257 hw_loss 0.161324 lr 0.00035460 rank 7
2023-02-25 04:51:34,708 DEBUG TRAIN Batch 23/7000 loss 10.371382 loss_att 9.900517 loss_ctc 13.524916 loss_rnnt 9.899715 hw_loss 0.272566 lr 0.00035466 rank 3
2023-02-25 04:51:34,715 DEBUG TRAIN Batch 23/7000 loss 5.390523 loss_att 7.629346 loss_ctc 7.594321 loss_rnnt 4.473743 hw_loss 0.328454 lr 0.00035463 rank 4
2023-02-25 04:52:48,644 DEBUG TRAIN Batch 23/7100 loss 14.974771 loss_att 14.966005 loss_ctc 15.955719 loss_rnnt 14.739734 hw_loss 0.198749 lr 0.00035453 rank 0
2023-02-25 04:52:48,661 DEBUG TRAIN Batch 23/7100 loss 8.094302 loss_att 8.150463 loss_ctc 9.334762 loss_rnnt 7.874589 hw_loss 0.080787 lr 0.00035457 rank 3
2023-02-25 04:52:48,663 DEBUG TRAIN Batch 23/7100 loss 14.091631 loss_att 14.726587 loss_ctc 16.123158 loss_rnnt 13.553621 hw_loss 0.262775 lr 0.00035453 rank 1
2023-02-25 04:52:48,662 DEBUG TRAIN Batch 23/7100 loss 5.860561 loss_att 8.139137 loss_ctc 9.638250 loss_rnnt 4.802605 hw_loss 0.184779 lr 0.00035454 rank 4
2023-02-25 04:52:48,666 DEBUG TRAIN Batch 23/7100 loss 9.326005 loss_att 14.104303 loss_ctc 18.716942 loss_rnnt 7.052658 hw_loss 0.122928 lr 0.00035451 rank 6
2023-02-25 04:52:48,666 DEBUG TRAIN Batch 23/7100 loss 10.475465 loss_att 12.791401 loss_ctc 16.268604 loss_rnnt 9.159750 hw_loss 0.150207 lr 0.00035449 rank 5
2023-02-25 04:52:48,667 DEBUG TRAIN Batch 23/7100 loss 6.644800 loss_att 8.157816 loss_ctc 9.130517 loss_rnnt 5.912406 hw_loss 0.184428 lr 0.00035452 rank 2
2023-02-25 04:52:48,669 DEBUG TRAIN Batch 23/7100 loss 5.892642 loss_att 10.987435 loss_ctc 8.702843 loss_rnnt 4.346255 hw_loss 0.286378 lr 0.00035451 rank 7
2023-02-25 04:53:59,721 DEBUG TRAIN Batch 23/7200 loss 10.465348 loss_att 12.750544 loss_ctc 16.537149 loss_rnnt 9.109013 hw_loss 0.168230 lr 0.00035444 rank 1
2023-02-25 04:53:59,721 DEBUG TRAIN Batch 23/7200 loss 16.785946 loss_att 19.118093 loss_ctc 26.161060 loss_rnnt 14.910666 hw_loss 0.297820 lr 0.00035444 rank 0
2023-02-25 04:53:59,725 DEBUG TRAIN Batch 23/7200 loss 2.631535 loss_att 5.558922 loss_ctc 4.268125 loss_rnnt 1.675472 hw_loss 0.285700 lr 0.00035448 rank 3
2023-02-25 04:53:59,725 DEBUG TRAIN Batch 23/7200 loss 10.670095 loss_att 14.901203 loss_ctc 17.718731 loss_rnnt 8.731569 hw_loss 0.285912 lr 0.00035445 rank 4
2023-02-25 04:53:59,725 DEBUG TRAIN Batch 23/7200 loss 4.284001 loss_att 7.133238 loss_ctc 7.457691 loss_rnnt 3.178072 hw_loss 0.211731 lr 0.00035442 rank 7
2023-02-25 04:53:59,727 DEBUG TRAIN Batch 23/7200 loss 5.259267 loss_att 7.493393 loss_ctc 7.784600 loss_rnnt 4.332434 hw_loss 0.268681 lr 0.00035443 rank 2
2023-02-25 04:53:59,772 DEBUG TRAIN Batch 23/7200 loss 5.698080 loss_att 7.697605 loss_ctc 7.006259 loss_rnnt 4.975112 hw_loss 0.278698 lr 0.00035442 rank 6
2023-02-25 04:53:59,801 DEBUG TRAIN Batch 23/7200 loss 8.125979 loss_att 10.499433 loss_ctc 10.985565 loss_rnnt 7.215739 hw_loss 0.101759 lr 0.00035440 rank 5
2023-02-25 04:55:10,903 DEBUG TRAIN Batch 23/7300 loss 6.834064 loss_att 9.787175 loss_ctc 10.705173 loss_rnnt 5.627745 hw_loss 0.186656 lr 0.00035435 rank 0
2023-02-25 04:55:10,905 DEBUG TRAIN Batch 23/7300 loss 11.087264 loss_att 12.854959 loss_ctc 11.443678 loss_rnnt 10.541599 hw_loss 0.271133 lr 0.00035435 rank 1
2023-02-25 04:55:10,907 DEBUG TRAIN Batch 23/7300 loss 17.372906 loss_att 21.338810 loss_ctc 21.277412 loss_rnnt 15.951837 hw_loss 0.201162 lr 0.00035434 rank 2
2023-02-25 04:55:10,908 DEBUG TRAIN Batch 23/7300 loss 10.397206 loss_att 12.757992 loss_ctc 15.605976 loss_rnnt 9.102896 hw_loss 0.239345 lr 0.00035433 rank 7
2023-02-25 04:55:10,911 DEBUG TRAIN Batch 23/7300 loss 9.931133 loss_att 10.743466 loss_ctc 14.313136 loss_rnnt 9.097435 hw_loss 0.163058 lr 0.00035434 rank 6
2023-02-25 04:55:10,911 DEBUG TRAIN Batch 23/7300 loss 10.973025 loss_att 11.977545 loss_ctc 12.412991 loss_rnnt 10.486277 hw_loss 0.175968 lr 0.00035431 rank 5
2023-02-25 04:55:10,911 DEBUG TRAIN Batch 23/7300 loss 5.747268 loss_att 8.640369 loss_ctc 7.151308 loss_rnnt 4.878583 hw_loss 0.192862 lr 0.00035436 rank 4
2023-02-25 04:55:10,953 DEBUG TRAIN Batch 23/7300 loss 10.606923 loss_att 11.618999 loss_ctc 13.396916 loss_rnnt 9.880300 hw_loss 0.285395 lr 0.00035439 rank 3
2023-02-25 04:56:21,860 DEBUG TRAIN Batch 23/7400 loss 4.501643 loss_att 7.283984 loss_ctc 4.908121 loss_rnnt 3.775120 hw_loss 0.217232 lr 0.00035424 rank 7
2023-02-25 04:56:21,859 DEBUG TRAIN Batch 23/7400 loss 11.027378 loss_att 13.188852 loss_ctc 16.078051 loss_rnnt 9.819099 hw_loss 0.192300 lr 0.00035428 rank 4
2023-02-25 04:56:21,861 DEBUG TRAIN Batch 23/7400 loss 10.263400 loss_att 11.961571 loss_ctc 14.691921 loss_rnnt 9.217484 hw_loss 0.217148 lr 0.00035427 rank 0
2023-02-25 04:56:21,861 DEBUG TRAIN Batch 23/7400 loss 6.227553 loss_att 9.766723 loss_ctc 7.888196 loss_rnnt 5.162368 hw_loss 0.254874 lr 0.00035426 rank 1
2023-02-25 04:56:21,862 DEBUG TRAIN Batch 23/7400 loss 16.304039 loss_att 20.010023 loss_ctc 25.552719 loss_rnnt 14.231159 hw_loss 0.184734 lr 0.00035422 rank 5
2023-02-25 04:56:21,863 DEBUG TRAIN Batch 23/7400 loss 8.476228 loss_att 9.728739 loss_ctc 15.011171 loss_rnnt 7.261660 hw_loss 0.173888 lr 0.00035426 rank 2
2023-02-25 04:56:21,870 DEBUG TRAIN Batch 23/7400 loss 7.999514 loss_att 12.459476 loss_ctc 17.090050 loss_rnnt 5.785987 hw_loss 0.205242 lr 0.00035425 rank 6
2023-02-25 04:56:21,893 DEBUG TRAIN Batch 23/7400 loss 2.441896 loss_att 4.820069 loss_ctc 4.972939 loss_rnnt 1.532578 hw_loss 0.180395 lr 0.00035430 rank 3
2023-02-25 04:57:36,788 DEBUG TRAIN Batch 23/7500 loss 10.697224 loss_att 13.056018 loss_ctc 13.888250 loss_rnnt 9.734632 hw_loss 0.122553 lr 0.00035418 rank 0
2023-02-25 04:57:36,792 DEBUG TRAIN Batch 23/7500 loss 12.984963 loss_att 15.582129 loss_ctc 16.347347 loss_rnnt 11.876162 hw_loss 0.264468 lr 0.00035422 rank 3
2023-02-25 04:57:36,796 DEBUG TRAIN Batch 23/7500 loss 7.500627 loss_att 15.501520 loss_ctc 9.923493 loss_rnnt 5.455625 hw_loss 0.228326 lr 0.00035417 rank 1
2023-02-25 04:57:36,796 DEBUG TRAIN Batch 23/7500 loss 15.902020 loss_att 18.586199 loss_ctc 23.963936 loss_rnnt 14.144405 hw_loss 0.273483 lr 0.00035416 rank 6
2023-02-25 04:57:36,798 DEBUG TRAIN Batch 23/7500 loss 10.233335 loss_att 12.579207 loss_ctc 15.269417 loss_rnnt 8.954675 hw_loss 0.258765 lr 0.00035417 rank 2
2023-02-25 04:57:36,801 DEBUG TRAIN Batch 23/7500 loss 11.866186 loss_att 15.555874 loss_ctc 16.840376 loss_rnnt 10.342939 hw_loss 0.228905 lr 0.00035415 rank 7
2023-02-25 04:57:36,802 DEBUG TRAIN Batch 23/7500 loss 7.180682 loss_att 9.464375 loss_ctc 7.425871 loss_rnnt 6.580639 hw_loss 0.207399 lr 0.00035413 rank 5
2023-02-25 04:57:36,804 DEBUG TRAIN Batch 23/7500 loss 10.116483 loss_att 12.840890 loss_ctc 13.651838 loss_rnnt 9.048048 hw_loss 0.097822 lr 0.00035419 rank 4
2023-02-25 04:58:47,622 DEBUG TRAIN Batch 23/7600 loss 14.167018 loss_att 16.798365 loss_ctc 22.025259 loss_rnnt 12.517985 hw_loss 0.140622 lr 0.00035407 rank 7
2023-02-25 04:58:47,629 DEBUG TRAIN Batch 23/7600 loss 7.395983 loss_att 8.972406 loss_ctc 10.554205 loss_rnnt 6.511046 hw_loss 0.278543 lr 0.00035409 rank 0
2023-02-25 04:58:47,632 DEBUG TRAIN Batch 23/7600 loss 7.247161 loss_att 9.159331 loss_ctc 9.398716 loss_rnnt 6.483804 hw_loss 0.176343 lr 0.00035413 rank 3
2023-02-25 04:58:47,637 DEBUG TRAIN Batch 23/7600 loss 4.830876 loss_att 5.917710 loss_ctc 4.963164 loss_rnnt 4.478806 hw_loss 0.219495 lr 0.00035407 rank 6
2023-02-25 04:58:47,639 DEBUG TRAIN Batch 23/7600 loss 3.890741 loss_att 5.429485 loss_ctc 4.885003 loss_rnnt 3.354024 hw_loss 0.180750 lr 0.00035404 rank 5
2023-02-25 04:58:47,639 DEBUG TRAIN Batch 23/7600 loss 10.621200 loss_att 11.584887 loss_ctc 14.287834 loss_rnnt 9.838260 hw_loss 0.189972 lr 0.00035408 rank 2
2023-02-25 04:58:47,643 DEBUG TRAIN Batch 23/7600 loss 4.939094 loss_att 7.196956 loss_ctc 5.878102 loss_rnnt 4.270516 hw_loss 0.172132 lr 0.00035408 rank 1
2023-02-25 04:58:47,694 DEBUG TRAIN Batch 23/7600 loss 8.600995 loss_att 12.064559 loss_ctc 11.220478 loss_rnnt 7.459543 hw_loss 0.186514 lr 0.00035410 rank 4
2023-02-25 04:59:57,837 DEBUG TRAIN Batch 23/7700 loss 8.083007 loss_att 11.027832 loss_ctc 9.269306 loss_rnnt 7.259395 hw_loss 0.143390 lr 0.00035400 rank 0
2023-02-25 04:59:57,837 DEBUG TRAIN Batch 23/7700 loss 4.307983 loss_att 11.490993 loss_ctc 9.487953 loss_rnnt 2.051519 hw_loss 0.242248 lr 0.00035398 rank 6
2023-02-25 04:59:57,839 DEBUG TRAIN Batch 23/7700 loss 7.777148 loss_att 10.447780 loss_ctc 11.972907 loss_rnnt 6.559158 hw_loss 0.233303 lr 0.00035399 rank 1
2023-02-25 04:59:57,840 DEBUG TRAIN Batch 23/7700 loss 8.764450 loss_att 11.594077 loss_ctc 11.141804 loss_rnnt 7.742218 hw_loss 0.261239 lr 0.00035404 rank 3
2023-02-25 04:59:57,841 DEBUG TRAIN Batch 23/7700 loss 2.572257 loss_att 4.850110 loss_ctc 6.929276 loss_rnnt 1.446949 hw_loss 0.166502 lr 0.00035396 rank 5
2023-02-25 04:59:57,841 DEBUG TRAIN Batch 23/7700 loss 12.535447 loss_att 14.692604 loss_ctc 19.392973 loss_rnnt 11.047850 hw_loss 0.265930 lr 0.00035401 rank 4
2023-02-25 04:59:57,843 DEBUG TRAIN Batch 23/7700 loss 9.830346 loss_att 14.551762 loss_ctc 21.253925 loss_rnnt 7.199630 hw_loss 0.306167 lr 0.00035399 rank 2
2023-02-25 04:59:57,847 DEBUG TRAIN Batch 23/7700 loss 16.789696 loss_att 20.072138 loss_ctc 21.262733 loss_rnnt 15.447257 hw_loss 0.167902 lr 0.00035398 rank 7
2023-02-25 05:01:10,481 DEBUG TRAIN Batch 23/7800 loss 5.532351 loss_att 8.127158 loss_ctc 12.003775 loss_rnnt 4.038485 hw_loss 0.210092 lr 0.00035390 rank 2
2023-02-25 05:01:10,486 DEBUG TRAIN Batch 23/7800 loss 7.844076 loss_att 10.088608 loss_ctc 8.742618 loss_rnnt 7.169748 hw_loss 0.198029 lr 0.00035389 rank 7
2023-02-25 05:01:10,489 DEBUG TRAIN Batch 23/7800 loss 11.306803 loss_att 14.061561 loss_ctc 16.563961 loss_rnnt 9.926061 hw_loss 0.241569 lr 0.00035391 rank 0
2023-02-25 05:01:10,489 DEBUG TRAIN Batch 23/7800 loss 7.335504 loss_att 11.557470 loss_ctc 12.555218 loss_rnnt 5.689588 hw_loss 0.197927 lr 0.00035389 rank 6
2023-02-25 05:01:10,498 DEBUG TRAIN Batch 23/7800 loss 5.968894 loss_att 8.964400 loss_ctc 9.028632 loss_rnnt 4.866766 hw_loss 0.178240 lr 0.00035392 rank 4
2023-02-25 05:01:10,510 DEBUG TRAIN Batch 23/7800 loss 9.032446 loss_att 10.026632 loss_ctc 8.315447 loss_rnnt 8.862131 hw_loss 0.125771 lr 0.00035390 rank 1
2023-02-25 05:01:10,518 DEBUG TRAIN Batch 23/7800 loss 5.675212 loss_att 8.362370 loss_ctc 12.188540 loss_rnnt 4.198191 hw_loss 0.133399 lr 0.00035395 rank 3
2023-02-25 05:01:10,523 DEBUG TRAIN Batch 23/7800 loss 11.108791 loss_att 15.957008 loss_ctc 15.574333 loss_rnnt 9.485764 hw_loss 0.108710 lr 0.00035387 rank 5
2023-02-25 05:02:21,341 DEBUG TRAIN Batch 23/7900 loss 8.207871 loss_att 12.507819 loss_ctc 15.059288 loss_rnnt 6.353300 hw_loss 0.151987 lr 0.00035382 rank 0
2023-02-25 05:02:21,343 DEBUG TRAIN Batch 23/7900 loss 12.369959 loss_att 15.024298 loss_ctc 17.168945 loss_rnnt 11.125987 hw_loss 0.137325 lr 0.00035382 rank 1
2023-02-25 05:02:21,350 DEBUG TRAIN Batch 23/7900 loss 5.373274 loss_att 9.886636 loss_ctc 9.678638 loss_rnnt 3.804160 hw_loss 0.173237 lr 0.00035386 rank 3
2023-02-25 05:02:21,351 DEBUG TRAIN Batch 23/7900 loss 4.286845 loss_att 7.292646 loss_ctc 6.481330 loss_rnnt 3.306657 hw_loss 0.162057 lr 0.00035380 rank 7
2023-02-25 05:02:21,352 DEBUG TRAIN Batch 23/7900 loss 4.251266 loss_att 8.338923 loss_ctc 5.354862 loss_rnnt 3.142108 hw_loss 0.270899 lr 0.00035380 rank 6
2023-02-25 05:02:21,353 DEBUG TRAIN Batch 23/7900 loss 10.062629 loss_att 13.162363 loss_ctc 12.616520 loss_rnnt 9.002370 hw_loss 0.187111 lr 0.00035381 rank 2
2023-02-25 05:02:21,354 DEBUG TRAIN Batch 23/7900 loss 16.225571 loss_att 18.428619 loss_ctc 21.870604 loss_rnnt 14.873599 hw_loss 0.297545 lr 0.00035383 rank 4
2023-02-25 05:02:21,356 DEBUG TRAIN Batch 23/7900 loss 18.344770 loss_att 19.128017 loss_ctc 26.855536 loss_rnnt 16.959381 hw_loss 0.176195 lr 0.00035378 rank 5
2023-02-25 05:03:31,198 DEBUG TRAIN Batch 23/8000 loss 13.797381 loss_att 16.303783 loss_ctc 21.388500 loss_rnnt 12.142492 hw_loss 0.265237 lr 0.00035371 rank 6
2023-02-25 05:03:31,201 DEBUG TRAIN Batch 23/8000 loss 8.389243 loss_att 12.140669 loss_ctc 12.517000 loss_rnnt 6.954997 hw_loss 0.250486 lr 0.00035373 rank 0
2023-02-25 05:03:31,203 DEBUG TRAIN Batch 23/8000 loss 6.863036 loss_att 9.554524 loss_ctc 10.969288 loss_rnnt 5.675931 hw_loss 0.189949 lr 0.00035377 rank 3
2023-02-25 05:03:31,204 DEBUG TRAIN Batch 23/8000 loss 13.138733 loss_att 15.647995 loss_ctc 16.298611 loss_rnnt 12.107971 hw_loss 0.201737 lr 0.00035372 rank 2
2023-02-25 05:03:31,206 DEBUG TRAIN Batch 23/8000 loss 5.635076 loss_att 9.922696 loss_ctc 11.115059 loss_rnnt 3.987376 hw_loss 0.111583 lr 0.00035373 rank 1
2023-02-25 05:03:31,206 DEBUG TRAIN Batch 23/8000 loss 12.955435 loss_att 17.030470 loss_ctc 16.022104 loss_rnnt 11.627523 hw_loss 0.195031 lr 0.00035371 rank 7
2023-02-25 05:03:31,207 DEBUG TRAIN Batch 23/8000 loss 5.206347 loss_att 8.814026 loss_ctc 9.022800 loss_rnnt 3.843014 hw_loss 0.249258 lr 0.00035374 rank 4
2023-02-25 05:03:31,210 DEBUG TRAIN Batch 23/8000 loss 4.201171 loss_att 6.516804 loss_ctc 5.741403 loss_rnnt 3.445991 hw_loss 0.162542 lr 0.00035369 rank 5
2023-02-25 05:04:41,956 DEBUG TRAIN Batch 23/8100 loss 13.360119 loss_att 17.151667 loss_ctc 16.296976 loss_rnnt 12.090273 hw_loss 0.224913 lr 0.00035368 rank 3
2023-02-25 05:04:41,965 DEBUG TRAIN Batch 23/8100 loss 14.093534 loss_att 18.047237 loss_ctc 20.086685 loss_rnnt 12.362389 hw_loss 0.264969 lr 0.00035364 rank 0
2023-02-25 05:04:41,968 DEBUG TRAIN Batch 23/8100 loss 6.469069 loss_att 9.019108 loss_ctc 6.729943 loss_rnnt 5.812198 hw_loss 0.210151 lr 0.00035364 rank 1
2023-02-25 05:04:41,969 DEBUG TRAIN Batch 23/8100 loss 13.352673 loss_att 17.056541 loss_ctc 18.649588 loss_rnnt 11.801945 hw_loss 0.194435 lr 0.00035360 rank 5
2023-02-25 05:04:41,968 DEBUG TRAIN Batch 23/8100 loss 10.891983 loss_att 14.786278 loss_ctc 15.075055 loss_rnnt 9.493876 hw_loss 0.115324 lr 0.00035362 rank 7
2023-02-25 05:04:41,969 DEBUG TRAIN Batch 23/8100 loss 10.266523 loss_att 11.659399 loss_ctc 19.060160 loss_rnnt 8.740904 hw_loss 0.139800 lr 0.00035366 rank 4
2023-02-25 05:04:41,972 DEBUG TRAIN Batch 23/8100 loss 6.109311 loss_att 8.849339 loss_ctc 11.353943 loss_rnnt 4.791582 hw_loss 0.132073 lr 0.00035363 rank 6
2023-02-25 05:04:41,975 DEBUG TRAIN Batch 23/8100 loss 11.223600 loss_att 14.743670 loss_ctc 12.348734 loss_rnnt 10.249519 hw_loss 0.225090 lr 0.00035363 rank 2
2023-02-25 05:05:53,022 DEBUG TRAIN Batch 23/8200 loss 11.565138 loss_att 11.313398 loss_ctc 14.586601 loss_rnnt 11.059356 hw_loss 0.287377 lr 0.00035359 rank 3
2023-02-25 05:05:53,025 DEBUG TRAIN Batch 23/8200 loss 10.968566 loss_att 12.644725 loss_ctc 15.181643 loss_rnnt 9.938552 hw_loss 0.249449 lr 0.00035354 rank 6
2023-02-25 05:05:53,027 DEBUG TRAIN Batch 23/8200 loss 7.346725 loss_att 9.852414 loss_ctc 10.866632 loss_rnnt 6.291836 hw_loss 0.158305 lr 0.00035356 rank 0
2023-02-25 05:05:53,027 DEBUG TRAIN Batch 23/8200 loss 8.446970 loss_att 8.970938 loss_ctc 11.713347 loss_rnnt 7.807752 hw_loss 0.185452 lr 0.00035355 rank 2
2023-02-25 05:05:53,029 DEBUG TRAIN Batch 23/8200 loss 20.208176 loss_att 22.845726 loss_ctc 34.777515 loss_rnnt 17.615864 hw_loss 0.229170 lr 0.00035355 rank 1
2023-02-25 05:05:53,033 DEBUG TRAIN Batch 23/8200 loss 10.016581 loss_att 9.986524 loss_ctc 13.802066 loss_rnnt 9.349629 hw_loss 0.315433 lr 0.00035353 rank 7
2023-02-25 05:05:53,034 DEBUG TRAIN Batch 23/8200 loss 13.269111 loss_att 15.467559 loss_ctc 17.227463 loss_rnnt 12.221008 hw_loss 0.151187 lr 0.00035357 rank 4
2023-02-25 05:05:53,080 DEBUG TRAIN Batch 23/8200 loss 12.448924 loss_att 13.394072 loss_ctc 15.922012 loss_rnnt 11.658851 hw_loss 0.258688 lr 0.00035351 rank 5
2023-02-25 05:07:03,434 DEBUG TRAIN Batch 23/8300 loss 8.782053 loss_att 13.025524 loss_ctc 13.636549 loss_rnnt 7.210971 hw_loss 0.140853 lr 0.00035347 rank 0
2023-02-25 05:07:03,436 DEBUG TRAIN Batch 23/8300 loss 13.137938 loss_att 14.410340 loss_ctc 17.643934 loss_rnnt 12.144418 hw_loss 0.259196 lr 0.00035345 rank 6
2023-02-25 05:07:03,438 DEBUG TRAIN Batch 23/8300 loss 7.127341 loss_att 9.128498 loss_ctc 11.708232 loss_rnnt 6.020919 hw_loss 0.178886 lr 0.00035351 rank 3
2023-02-25 05:07:03,440 DEBUG TRAIN Batch 23/8300 loss 3.331679 loss_att 8.725273 loss_ctc 8.235798 loss_rnnt 1.503123 hw_loss 0.179917 lr 0.00035345 rank 7
2023-02-25 05:07:03,444 DEBUG TRAIN Batch 23/8300 loss 5.138648 loss_att 6.163357 loss_ctc 6.342593 loss_rnnt 4.715057 hw_loss 0.108979 lr 0.00035346 rank 1
2023-02-25 05:07:03,446 DEBUG TRAIN Batch 23/8300 loss 3.973635 loss_att 7.069987 loss_ctc 4.915634 loss_rnnt 3.138585 hw_loss 0.169087 lr 0.00035343 rank 5
2023-02-25 05:07:03,449 DEBUG TRAIN Batch 23/8300 loss 14.141327 loss_att 18.093662 loss_ctc 23.989727 loss_rnnt 11.950542 hw_loss 0.163496 lr 0.00035348 rank 4
2023-02-25 05:07:03,451 DEBUG TRAIN Batch 23/8300 loss 6.213908 loss_att 9.014514 loss_ctc 7.080227 loss_rnnt 5.379782 hw_loss 0.297180 lr 0.00035346 rank 2
2023-02-25 05:07:45,201 DEBUG CV Batch 23/0 loss 1.429807 loss_att 1.822138 loss_ctc 2.516355 loss_rnnt 1.085010 hw_loss 0.227735 history loss 1.376852 rank 7
2023-02-25 05:07:45,211 DEBUG CV Batch 23/0 loss 1.429807 loss_att 1.822138 loss_ctc 2.516355 loss_rnnt 1.085010 hw_loss 0.227735 history loss 1.376852 rank 2
2023-02-25 05:07:45,212 DEBUG CV Batch 23/0 loss 1.429807 loss_att 1.822138 loss_ctc 2.516355 loss_rnnt 1.085010 hw_loss 0.227735 history loss 1.376852 rank 6
2023-02-25 05:07:45,212 DEBUG CV Batch 23/0 loss 1.429807 loss_att 1.822138 loss_ctc 2.516355 loss_rnnt 1.085010 hw_loss 0.227735 history loss 1.376852 rank 1
2023-02-25 05:07:45,213 DEBUG CV Batch 23/0 loss 1.429807 loss_att 1.822138 loss_ctc 2.516355 loss_rnnt 1.085010 hw_loss 0.227735 history loss 1.376852 rank 0
2023-02-25 05:07:45,220 DEBUG CV Batch 23/0 loss 1.429807 loss_att 1.822138 loss_ctc 2.516355 loss_rnnt 1.085010 hw_loss 0.227735 history loss 1.376852 rank 3
2023-02-25 05:07:45,220 DEBUG CV Batch 23/0 loss 1.429807 loss_att 1.822138 loss_ctc 2.516355 loss_rnnt 1.085010 hw_loss 0.227735 history loss 1.376852 rank 4
2023-02-25 05:07:45,226 DEBUG CV Batch 23/0 loss 1.429807 loss_att 1.822138 loss_ctc 2.516355 loss_rnnt 1.085010 hw_loss 0.227735 history loss 1.376852 rank 5
2023-02-25 05:07:56,723 DEBUG CV Batch 23/100 loss 5.410347 loss_att 6.042955 loss_ctc 9.340571 loss_rnnt 4.606073 hw_loss 0.288228 history loss 3.189582 rank 3
2023-02-25 05:07:56,760 DEBUG CV Batch 23/100 loss 5.410347 loss_att 6.042955 loss_ctc 9.340571 loss_rnnt 4.606073 hw_loss 0.288228 history loss 3.189582 rank 5
2023-02-25 05:07:56,801 DEBUG CV Batch 23/100 loss 5.410347 loss_att 6.042955 loss_ctc 9.340571 loss_rnnt 4.606073 hw_loss 0.288228 history loss 3.189582 rank 6
2023-02-25 05:07:56,823 DEBUG CV Batch 23/100 loss 5.410347 loss_att 6.042955 loss_ctc 9.340571 loss_rnnt 4.606073 hw_loss 0.288228 history loss 3.189582 rank 4
2023-02-25 05:07:56,827 DEBUG CV Batch 23/100 loss 5.410347 loss_att 6.042955 loss_ctc 9.340571 loss_rnnt 4.606073 hw_loss 0.288228 history loss 3.189582 rank 0
2023-02-25 05:07:56,928 DEBUG CV Batch 23/100 loss 5.410347 loss_att 6.042955 loss_ctc 9.340571 loss_rnnt 4.606073 hw_loss 0.288228 history loss 3.189582 rank 7
2023-02-25 05:07:56,976 DEBUG CV Batch 23/100 loss 5.410347 loss_att 6.042955 loss_ctc 9.340571 loss_rnnt 4.606073 hw_loss 0.288228 history loss 3.189582 rank 1
2023-02-25 05:07:57,013 DEBUG CV Batch 23/100 loss 5.410347 loss_att 6.042955 loss_ctc 9.340571 loss_rnnt 4.606073 hw_loss 0.288228 history loss 3.189582 rank 2
2023-02-25 05:08:10,334 DEBUG CV Batch 23/200 loss 6.738226 loss_att 15.294609 loss_ctc 6.581569 loss_rnnt 4.956528 hw_loss 0.171205 history loss 3.809122 rank 6
2023-02-25 05:08:10,536 DEBUG CV Batch 23/200 loss 6.738226 loss_att 15.294609 loss_ctc 6.581569 loss_rnnt 4.956528 hw_loss 0.171205 history loss 3.809122 rank 3
2023-02-25 05:08:10,570 DEBUG CV Batch 23/200 loss 6.738226 loss_att 15.294609 loss_ctc 6.581569 loss_rnnt 4.956528 hw_loss 0.171205 history loss 3.809122 rank 5
2023-02-25 05:08:10,604 DEBUG CV Batch 23/200 loss 6.738226 loss_att 15.294609 loss_ctc 6.581569 loss_rnnt 4.956528 hw_loss 0.171205 history loss 3.809122 rank 0
2023-02-25 05:08:10,770 DEBUG CV Batch 23/200 loss 6.738226 loss_att 15.294609 loss_ctc 6.581569 loss_rnnt 4.956528 hw_loss 0.171205 history loss 3.809122 rank 7
2023-02-25 05:08:10,778 DEBUG CV Batch 23/200 loss 6.738226 loss_att 15.294609 loss_ctc 6.581569 loss_rnnt 4.956528 hw_loss 0.171205 history loss 3.809122 rank 4
2023-02-25 05:08:10,889 DEBUG CV Batch 23/200 loss 6.738226 loss_att 15.294609 loss_ctc 6.581569 loss_rnnt 4.956528 hw_loss 0.171205 history loss 3.809122 rank 1
2023-02-25 05:08:10,940 DEBUG CV Batch 23/200 loss 6.738226 loss_att 15.294609 loss_ctc 6.581569 loss_rnnt 4.956528 hw_loss 0.171205 history loss 3.809122 rank 2
2023-02-25 05:08:22,401 DEBUG CV Batch 23/300 loss 4.951423 loss_att 5.525004 loss_ctc 7.578086 loss_rnnt 4.342091 hw_loss 0.270737 history loss 3.985698 rank 6
2023-02-25 05:08:22,575 DEBUG CV Batch 23/300 loss 4.951423 loss_att 5.525004 loss_ctc 7.578086 loss_rnnt 4.342091 hw_loss 0.270737 history loss 3.985698 rank 3
2023-02-25 05:08:22,680 DEBUG CV Batch 23/300 loss 4.951423 loss_att 5.525004 loss_ctc 7.578086 loss_rnnt 4.342091 hw_loss 0.270737 history loss 3.985698 rank 5
2023-02-25 05:08:23,074 DEBUG CV Batch 23/300 loss 4.951423 loss_att 5.525004 loss_ctc 7.578086 loss_rnnt 4.342091 hw_loss 0.270737 history loss 3.985698 rank 0
2023-02-25 05:08:23,236 DEBUG CV Batch 23/300 loss 4.951423 loss_att 5.525004 loss_ctc 7.578086 loss_rnnt 4.342091 hw_loss 0.270737 history loss 3.985698 rank 1
2023-02-25 05:08:23,239 DEBUG CV Batch 23/300 loss 4.951423 loss_att 5.525004 loss_ctc 7.578086 loss_rnnt 4.342091 hw_loss 0.270737 history loss 3.985698 rank 7
2023-02-25 05:08:23,382 DEBUG CV Batch 23/300 loss 4.951423 loss_att 5.525004 loss_ctc 7.578086 loss_rnnt 4.342091 hw_loss 0.270737 history loss 3.985698 rank 4
2023-02-25 05:08:23,516 DEBUG CV Batch 23/300 loss 4.951423 loss_att 5.525004 loss_ctc 7.578086 loss_rnnt 4.342091 hw_loss 0.270737 history loss 3.985698 rank 2
2023-02-25 05:08:34,413 DEBUG CV Batch 23/400 loss 16.245560 loss_att 74.962364 loss_ctc 8.930134 loss_rnnt 5.427534 hw_loss 0.093850 history loss 4.847963 rank 6
2023-02-25 05:08:34,507 DEBUG CV Batch 23/400 loss 16.245560 loss_att 74.962364 loss_ctc 8.930134 loss_rnnt 5.427534 hw_loss 0.093850 history loss 4.847963 rank 3
2023-02-25 05:08:34,725 DEBUG CV Batch 23/400 loss 16.245560 loss_att 74.962364 loss_ctc 8.930134 loss_rnnt 5.427534 hw_loss 0.093850 history loss 4.847963 rank 5
2023-02-25 05:08:35,294 DEBUG CV Batch 23/400 loss 16.245560 loss_att 74.962364 loss_ctc 8.930134 loss_rnnt 5.427534 hw_loss 0.093850 history loss 4.847963 rank 4
2023-02-25 05:08:35,438 DEBUG CV Batch 23/400 loss 16.245560 loss_att 74.962364 loss_ctc 8.930134 loss_rnnt 5.427534 hw_loss 0.093850 history loss 4.847963 rank 0
2023-02-25 05:08:35,571 DEBUG CV Batch 23/400 loss 16.245560 loss_att 74.962364 loss_ctc 8.930134 loss_rnnt 5.427534 hw_loss 0.093850 history loss 4.847963 rank 1
2023-02-25 05:08:35,834 DEBUG CV Batch 23/400 loss 16.245560 loss_att 74.962364 loss_ctc 8.930134 loss_rnnt 5.427534 hw_loss 0.093850 history loss 4.847963 rank 7
2023-02-25 05:08:36,048 DEBUG CV Batch 23/400 loss 16.245560 loss_att 74.962364 loss_ctc 8.930134 loss_rnnt 5.427534 hw_loss 0.093850 history loss 4.847963 rank 2
2023-02-25 05:08:44,732 DEBUG CV Batch 23/500 loss 5.247025 loss_att 5.651425 loss_ctc 7.533621 loss_rnnt 4.773065 hw_loss 0.165375 history loss 5.528281 rank 6
2023-02-25 05:08:45,009 DEBUG CV Batch 23/500 loss 5.247025 loss_att 5.651425 loss_ctc 7.533621 loss_rnnt 4.773065 hw_loss 0.165375 history loss 5.528281 rank 3
2023-02-25 05:08:45,258 DEBUG CV Batch 23/500 loss 5.247025 loss_att 5.651425 loss_ctc 7.533621 loss_rnnt 4.773065 hw_loss 0.165375 history loss 5.528281 rank 5
2023-02-25 05:08:45,650 DEBUG CV Batch 23/500 loss 5.247025 loss_att 5.651425 loss_ctc 7.533621 loss_rnnt 4.773065 hw_loss 0.165375 history loss 5.528281 rank 4
2023-02-25 05:08:46,207 DEBUG CV Batch 23/500 loss 5.247025 loss_att 5.651425 loss_ctc 7.533621 loss_rnnt 4.773065 hw_loss 0.165375 history loss 5.528281 rank 0
2023-02-25 05:08:46,276 DEBUG CV Batch 23/500 loss 5.247025 loss_att 5.651425 loss_ctc 7.533621 loss_rnnt 4.773065 hw_loss 0.165375 history loss 5.528281 rank 1
2023-02-25 05:08:46,673 DEBUG CV Batch 23/500 loss 5.247025 loss_att 5.651425 loss_ctc 7.533621 loss_rnnt 4.773065 hw_loss 0.165375 history loss 5.528281 rank 7
2023-02-25 05:08:46,932 DEBUG CV Batch 23/500 loss 5.247025 loss_att 5.651425 loss_ctc 7.533621 loss_rnnt 4.773065 hw_loss 0.165375 history loss 5.528281 rank 2
2023-02-25 05:08:56,989 DEBUG CV Batch 23/600 loss 7.905575 loss_att 7.455757 loss_ctc 9.884046 loss_rnnt 7.574670 hw_loss 0.294510 history loss 6.433175 rank 6
2023-02-25 05:08:57,200 DEBUG CV Batch 23/600 loss 7.905575 loss_att 7.455757 loss_ctc 9.884046 loss_rnnt 7.574670 hw_loss 0.294510 history loss 6.433175 rank 3
2023-02-25 05:08:57,440 DEBUG CV Batch 23/600 loss 7.905575 loss_att 7.455757 loss_ctc 9.884046 loss_rnnt 7.574670 hw_loss 0.294510 history loss 6.433175 rank 5
2023-02-25 05:08:57,715 DEBUG CV Batch 23/600 loss 7.905575 loss_att 7.455757 loss_ctc 9.884046 loss_rnnt 7.574670 hw_loss 0.294510 history loss 6.433175 rank 4
2023-02-25 05:08:58,507 DEBUG CV Batch 23/600 loss 7.905575 loss_att 7.455757 loss_ctc 9.884046 loss_rnnt 7.574670 hw_loss 0.294510 history loss 6.433175 rank 1
2023-02-25 05:08:58,625 DEBUG CV Batch 23/600 loss 7.905575 loss_att 7.455757 loss_ctc 9.884046 loss_rnnt 7.574670 hw_loss 0.294510 history loss 6.433175 rank 0
2023-02-25 05:08:59,095 DEBUG CV Batch 23/600 loss 7.905575 loss_att 7.455757 loss_ctc 9.884046 loss_rnnt 7.574670 hw_loss 0.294510 history loss 6.433175 rank 7
2023-02-25 05:08:59,421 DEBUG CV Batch 23/600 loss 7.905575 loss_att 7.455757 loss_ctc 9.884046 loss_rnnt 7.574670 hw_loss 0.294510 history loss 6.433175 rank 2
2023-02-25 05:09:08,270 DEBUG CV Batch 23/700 loss 15.017895 loss_att 27.891626 loss_ctc 18.607647 loss_rnnt 11.911106 hw_loss 0.100140 history loss 7.052152 rank 6
2023-02-25 05:09:08,913 DEBUG CV Batch 23/700 loss 15.017895 loss_att 27.891626 loss_ctc 18.607647 loss_rnnt 11.911106 hw_loss 0.100140 history loss 7.052152 rank 5
2023-02-25 05:09:09,087 DEBUG CV Batch 23/700 loss 15.017895 loss_att 27.891626 loss_ctc 18.607647 loss_rnnt 11.911106 hw_loss 0.100140 history loss 7.052152 rank 4
2023-02-25 05:09:09,234 DEBUG CV Batch 23/700 loss 15.017895 loss_att 27.891626 loss_ctc 18.607647 loss_rnnt 11.911106 hw_loss 0.100140 history loss 7.052152 rank 3
2023-02-25 05:09:10,037 DEBUG CV Batch 23/700 loss 15.017895 loss_att 27.891626 loss_ctc 18.607647 loss_rnnt 11.911106 hw_loss 0.100140 history loss 7.052152 rank 1
2023-02-25 05:09:10,329 DEBUG CV Batch 23/700 loss 15.017895 loss_att 27.891626 loss_ctc 18.607647 loss_rnnt 11.911106 hw_loss 0.100140 history loss 7.052152 rank 0
2023-02-25 05:09:10,796 DEBUG CV Batch 23/700 loss 15.017895 loss_att 27.891626 loss_ctc 18.607647 loss_rnnt 11.911106 hw_loss 0.100140 history loss 7.052152 rank 7
2023-02-25 05:09:11,328 DEBUG CV Batch 23/700 loss 15.017895 loss_att 27.891626 loss_ctc 18.607647 loss_rnnt 11.911106 hw_loss 0.100140 history loss 7.052152 rank 2
2023-02-25 05:09:19,511 DEBUG CV Batch 23/800 loss 9.459885 loss_att 9.051414 loss_ctc 14.824995 loss_rnnt 8.658888 hw_loss 0.313769 history loss 6.546151 rank 6
2023-02-25 05:09:20,953 DEBUG CV Batch 23/800 loss 9.459885 loss_att 9.051414 loss_ctc 14.824995 loss_rnnt 8.658888 hw_loss 0.313769 history loss 6.546151 rank 4
2023-02-25 05:09:21,245 DEBUG CV Batch 23/800 loss 9.459885 loss_att 9.051414 loss_ctc 14.824995 loss_rnnt 8.658888 hw_loss 0.313769 history loss 6.546151 rank 3
2023-02-25 05:09:21,329 DEBUG CV Batch 23/800 loss 9.459885 loss_att 9.051414 loss_ctc 14.824995 loss_rnnt 8.658888 hw_loss 0.313769 history loss 6.546151 rank 5
2023-02-25 05:09:21,383 DEBUG CV Batch 23/800 loss 9.459885 loss_att 9.051414 loss_ctc 14.824995 loss_rnnt 8.658888 hw_loss 0.313769 history loss 6.546151 rank 1
2023-02-25 05:09:21,838 DEBUG CV Batch 23/800 loss 9.459885 loss_att 9.051414 loss_ctc 14.824995 loss_rnnt 8.658888 hw_loss 0.313769 history loss 6.546151 rank 0
2023-02-25 05:09:22,283 DEBUG CV Batch 23/800 loss 9.459885 loss_att 9.051414 loss_ctc 14.824995 loss_rnnt 8.658888 hw_loss 0.313769 history loss 6.546151 rank 7
2023-02-25 05:09:22,876 DEBUG CV Batch 23/800 loss 9.459885 loss_att 9.051414 loss_ctc 14.824995 loss_rnnt 8.658888 hw_loss 0.313769 history loss 6.546151 rank 2
2023-02-25 05:09:32,949 DEBUG CV Batch 23/900 loss 12.732267 loss_att 16.119633 loss_ctc 24.230665 loss_rnnt 10.471448 hw_loss 0.094174 history loss 6.368494 rank 6
2023-02-25 05:09:34,633 DEBUG CV Batch 23/900 loss 12.732267 loss_att 16.119633 loss_ctc 24.230665 loss_rnnt 10.471448 hw_loss 0.094174 history loss 6.368494 rank 4
2023-02-25 05:09:34,770 DEBUG CV Batch 23/900 loss 12.732267 loss_att 16.119633 loss_ctc 24.230665 loss_rnnt 10.471448 hw_loss 0.094174 history loss 6.368494 rank 3
2023-02-25 05:09:35,098 DEBUG CV Batch 23/900 loss 12.732267 loss_att 16.119633 loss_ctc 24.230665 loss_rnnt 10.471448 hw_loss 0.094174 history loss 6.368494 rank 5
2023-02-25 05:09:35,212 DEBUG CV Batch 23/900 loss 12.732267 loss_att 16.119633 loss_ctc 24.230665 loss_rnnt 10.471448 hw_loss 0.094174 history loss 6.368494 rank 1
2023-02-25 05:09:35,573 DEBUG CV Batch 23/900 loss 12.732267 loss_att 16.119633 loss_ctc 24.230665 loss_rnnt 10.471448 hw_loss 0.094174 history loss 6.368494 rank 0
2023-02-25 05:09:35,969 DEBUG CV Batch 23/900 loss 12.732267 loss_att 16.119633 loss_ctc 24.230665 loss_rnnt 10.471448 hw_loss 0.094174 history loss 6.368494 rank 7
2023-02-25 05:09:36,810 DEBUG CV Batch 23/900 loss 12.732267 loss_att 16.119633 loss_ctc 24.230665 loss_rnnt 10.471448 hw_loss 0.094174 history loss 6.368494 rank 2
2023-02-25 05:09:44,987 DEBUG CV Batch 23/1000 loss 5.642112 loss_att 5.006322 loss_ctc 5.438365 loss_rnnt 5.662507 hw_loss 0.251116 history loss 6.157661 rank 6
2023-02-25 05:09:46,982 DEBUG CV Batch 23/1000 loss 5.642112 loss_att 5.006322 loss_ctc 5.438365 loss_rnnt 5.662507 hw_loss 0.251116 history loss 6.157661 rank 3
2023-02-25 05:09:47,355 DEBUG CV Batch 23/1000 loss 5.642112 loss_att 5.006322 loss_ctc 5.438365 loss_rnnt 5.662507 hw_loss 0.251116 history loss 6.157661 rank 5
2023-02-25 05:09:47,396 DEBUG CV Batch 23/1000 loss 5.642112 loss_att 5.006322 loss_ctc 5.438365 loss_rnnt 5.662507 hw_loss 0.251116 history loss 6.157661 rank 4
2023-02-25 05:09:47,840 DEBUG CV Batch 23/1000 loss 5.642112 loss_att 5.006322 loss_ctc 5.438365 loss_rnnt 5.662507 hw_loss 0.251116 history loss 6.157661 rank 1
2023-02-25 05:09:48,341 DEBUG CV Batch 23/1000 loss 5.642112 loss_att 5.006322 loss_ctc 5.438365 loss_rnnt 5.662507 hw_loss 0.251116 history loss 6.157661 rank 0
2023-02-25 05:09:48,689 DEBUG CV Batch 23/1000 loss 5.642112 loss_att 5.006322 loss_ctc 5.438365 loss_rnnt 5.662507 hw_loss 0.251116 history loss 6.157661 rank 7
2023-02-25 05:09:49,676 DEBUG CV Batch 23/1000 loss 5.642112 loss_att 5.006322 loss_ctc 5.438365 loss_rnnt 5.662507 hw_loss 0.251116 history loss 6.157661 rank 2
2023-02-25 05:09:57,143 DEBUG CV Batch 23/1100 loss 6.058879 loss_att 5.409338 loss_ctc 8.443992 loss_rnnt 5.737310 hw_loss 0.250241 history loss 6.133483 rank 6
2023-02-25 05:09:58,898 DEBUG CV Batch 23/1100 loss 6.058879 loss_att 5.409338 loss_ctc 8.443992 loss_rnnt 5.737310 hw_loss 0.250241 history loss 6.133483 rank 3
2023-02-25 05:09:59,228 DEBUG CV Batch 23/1100 loss 6.058879 loss_att 5.409338 loss_ctc 8.443992 loss_rnnt 5.737310 hw_loss 0.250241 history loss 6.133483 rank 5
2023-02-25 05:09:59,255 DEBUG CV Batch 23/1100 loss 6.058879 loss_att 5.409338 loss_ctc 8.443992 loss_rnnt 5.737310 hw_loss 0.250241 history loss 6.133483 rank 4
2023-02-25 05:10:00,130 DEBUG CV Batch 23/1100 loss 6.058879 loss_att 5.409338 loss_ctc 8.443992 loss_rnnt 5.737310 hw_loss 0.250241 history loss 6.133483 rank 1
2023-02-25 05:10:00,900 DEBUG CV Batch 23/1100 loss 6.058879 loss_att 5.409338 loss_ctc 8.443992 loss_rnnt 5.737310 hw_loss 0.250241 history loss 6.133483 rank 0
2023-02-25 05:10:01,064 DEBUG CV Batch 23/1100 loss 6.058879 loss_att 5.409338 loss_ctc 8.443992 loss_rnnt 5.737310 hw_loss 0.250241 history loss 6.133483 rank 7
2023-02-25 05:10:02,147 DEBUG CV Batch 23/1100 loss 6.058879 loss_att 5.409338 loss_ctc 8.443992 loss_rnnt 5.737310 hw_loss 0.250241 history loss 6.133483 rank 2
2023-02-25 05:10:07,704 DEBUG CV Batch 23/1200 loss 8.166163 loss_att 8.489920 loss_ctc 10.427440 loss_rnnt 7.681561 hw_loss 0.221898 history loss 6.427006 rank 6
2023-02-25 05:10:09,656 DEBUG CV Batch 23/1200 loss 8.166163 loss_att 8.489920 loss_ctc 10.427440 loss_rnnt 7.681561 hw_loss 0.221898 history loss 6.427006 rank 4
2023-02-25 05:10:09,749 DEBUG CV Batch 23/1200 loss 8.166163 loss_att 8.489920 loss_ctc 10.427440 loss_rnnt 7.681561 hw_loss 0.221898 history loss 6.427006 rank 5
2023-02-25 05:10:09,813 DEBUG CV Batch 23/1200 loss 8.166163 loss_att 8.489920 loss_ctc 10.427440 loss_rnnt 7.681561 hw_loss 0.221898 history loss 6.427006 rank 3
2023-02-25 05:10:10,934 DEBUG CV Batch 23/1200 loss 8.166163 loss_att 8.489920 loss_ctc 10.427440 loss_rnnt 7.681561 hw_loss 0.221898 history loss 6.427006 rank 1
2023-02-25 05:10:12,034 DEBUG CV Batch 23/1200 loss 8.166163 loss_att 8.489920 loss_ctc 10.427440 loss_rnnt 7.681561 hw_loss 0.221898 history loss 6.427006 rank 0
2023-02-25 05:10:12,125 DEBUG CV Batch 23/1200 loss 8.166163 loss_att 8.489920 loss_ctc 10.427440 loss_rnnt 7.681561 hw_loss 0.221898 history loss 6.427006 rank 7
2023-02-25 05:10:13,351 DEBUG CV Batch 23/1200 loss 8.166163 loss_att 8.489920 loss_ctc 10.427440 loss_rnnt 7.681561 hw_loss 0.221898 history loss 6.427006 rank 2
2023-02-25 05:10:19,673 DEBUG CV Batch 23/1300 loss 5.387419 loss_att 6.120177 loss_ctc 8.309517 loss_rnnt 4.743626 hw_loss 0.201803 history loss 6.725732 rank 6
2023-02-25 05:10:21,636 DEBUG CV Batch 23/1300 loss 5.387419 loss_att 6.120177 loss_ctc 8.309517 loss_rnnt 4.743626 hw_loss 0.201803 history loss 6.725732 rank 4
2023-02-25 05:10:21,770 DEBUG CV Batch 23/1300 loss 5.387419 loss_att 6.120177 loss_ctc 8.309517 loss_rnnt 4.743626 hw_loss 0.201803 history loss 6.725732 rank 5
2023-02-25 05:10:21,894 DEBUG CV Batch 23/1300 loss 5.387419 loss_att 6.120177 loss_ctc 8.309517 loss_rnnt 4.743626 hw_loss 0.201803 history loss 6.725732 rank 3
2023-02-25 05:10:23,117 DEBUG CV Batch 23/1300 loss 5.387419 loss_att 6.120177 loss_ctc 8.309517 loss_rnnt 4.743626 hw_loss 0.201803 history loss 6.725732 rank 1
2023-02-25 05:10:24,505 DEBUG CV Batch 23/1300 loss 5.387419 loss_att 6.120177 loss_ctc 8.309517 loss_rnnt 4.743626 hw_loss 0.201803 history loss 6.725732 rank 0
2023-02-25 05:10:24,612 DEBUG CV Batch 23/1300 loss 5.387419 loss_att 6.120177 loss_ctc 8.309517 loss_rnnt 4.743626 hw_loss 0.201803 history loss 6.725732 rank 7
2023-02-25 05:10:25,925 DEBUG CV Batch 23/1300 loss 5.387419 loss_att 6.120177 loss_ctc 8.309517 loss_rnnt 4.743626 hw_loss 0.201803 history loss 6.725732 rank 2
2023-02-25 05:10:30,847 DEBUG CV Batch 23/1400 loss 4.276207 loss_att 12.481184 loss_ctc 6.603450 loss_rnnt 2.221879 hw_loss 0.193187 history loss 7.023228 rank 6
2023-02-25 05:10:32,807 DEBUG CV Batch 23/1400 loss 4.276207 loss_att 12.481184 loss_ctc 6.603450 loss_rnnt 2.221879 hw_loss 0.193187 history loss 7.023228 rank 4
2023-02-25 05:10:33,040 DEBUG CV Batch 23/1400 loss 4.276207 loss_att 12.481184 loss_ctc 6.603450 loss_rnnt 2.221879 hw_loss 0.193187 history loss 7.023228 rank 5
2023-02-25 05:10:33,103 DEBUG CV Batch 23/1400 loss 4.276207 loss_att 12.481184 loss_ctc 6.603450 loss_rnnt 2.221879 hw_loss 0.193187 history loss 7.023228 rank 3
2023-02-25 05:10:34,671 DEBUG CV Batch 23/1400 loss 4.276207 loss_att 12.481184 loss_ctc 6.603450 loss_rnnt 2.221879 hw_loss 0.193187 history loss 7.023228 rank 1
2023-02-25 05:10:36,290 DEBUG CV Batch 23/1400 loss 4.276207 loss_att 12.481184 loss_ctc 6.603450 loss_rnnt 2.221879 hw_loss 0.193187 history loss 7.023228 rank 0
2023-02-25 05:10:36,316 DEBUG CV Batch 23/1400 loss 4.276207 loss_att 12.481184 loss_ctc 6.603450 loss_rnnt 2.221879 hw_loss 0.193187 history loss 7.023228 rank 7
2023-02-25 05:10:37,792 DEBUG CV Batch 23/1400 loss 4.276207 loss_att 12.481184 loss_ctc 6.603450 loss_rnnt 2.221879 hw_loss 0.193187 history loss 7.023228 rank 2
2023-02-25 05:10:42,140 DEBUG CV Batch 23/1500 loss 5.454906 loss_att 6.867502 loss_ctc 4.512491 loss_rnnt 5.198026 hw_loss 0.187528 history loss 6.852463 rank 6
2023-02-25 05:10:44,762 DEBUG CV Batch 23/1500 loss 5.454906 loss_att 6.867502 loss_ctc 4.512491 loss_rnnt 5.198026 hw_loss 0.187528 history loss 6.852463 rank 3
2023-02-25 05:10:45,191 DEBUG CV Batch 23/1500 loss 5.454906 loss_att 6.867502 loss_ctc 4.512491 loss_rnnt 5.198026 hw_loss 0.187528 history loss 6.852463 rank 5
2023-02-25 05:10:45,250 DEBUG CV Batch 23/1500 loss 5.454906 loss_att 6.867502 loss_ctc 4.512491 loss_rnnt 5.198026 hw_loss 0.187528 history loss 6.852463 rank 4
2023-02-25 05:10:46,473 DEBUG CV Batch 23/1500 loss 5.454906 loss_att 6.867502 loss_ctc 4.512491 loss_rnnt 5.198026 hw_loss 0.187528 history loss 6.852463 rank 1
2023-02-25 05:10:48,185 DEBUG CV Batch 23/1500 loss 5.454906 loss_att 6.867502 loss_ctc 4.512491 loss_rnnt 5.198026 hw_loss 0.187528 history loss 6.852463 rank 7
2023-02-25 05:10:48,292 DEBUG CV Batch 23/1500 loss 5.454906 loss_att 6.867502 loss_ctc 4.512491 loss_rnnt 5.198026 hw_loss 0.187528 history loss 6.852463 rank 0
2023-02-25 05:10:49,975 DEBUG CV Batch 23/1500 loss 5.454906 loss_att 6.867502 loss_ctc 4.512491 loss_rnnt 5.198026 hw_loss 0.187528 history loss 6.852463 rank 2
2023-02-25 05:10:55,386 DEBUG CV Batch 23/1600 loss 7.720115 loss_att 11.849812 loss_ctc 10.096124 loss_rnnt 6.481333 hw_loss 0.180077 history loss 6.787321 rank 6
2023-02-25 05:10:58,005 DEBUG CV Batch 23/1600 loss 7.720115 loss_att 11.849812 loss_ctc 10.096124 loss_rnnt 6.481333 hw_loss 0.180077 history loss 6.787321 rank 3
2023-02-25 05:10:58,740 DEBUG CV Batch 23/1600 loss 7.720115 loss_att 11.849812 loss_ctc 10.096124 loss_rnnt 6.481333 hw_loss 0.180077 history loss 6.787321 rank 4
2023-02-25 05:10:58,801 DEBUG CV Batch 23/1600 loss 7.720115 loss_att 11.849812 loss_ctc 10.096124 loss_rnnt 6.481333 hw_loss 0.180077 history loss 6.787321 rank 5
2023-02-25 05:10:59,896 DEBUG CV Batch 23/1600 loss 7.720115 loss_att 11.849812 loss_ctc 10.096124 loss_rnnt 6.481333 hw_loss 0.180077 history loss 6.787321 rank 1
2023-02-25 05:11:01,683 DEBUG CV Batch 23/1600 loss 7.720115 loss_att 11.849812 loss_ctc 10.096124 loss_rnnt 6.481333 hw_loss 0.180077 history loss 6.787321 rank 7
2023-02-25 05:11:01,839 DEBUG CV Batch 23/1600 loss 7.720115 loss_att 11.849812 loss_ctc 10.096124 loss_rnnt 6.481333 hw_loss 0.180077 history loss 6.787321 rank 0
2023-02-25 05:11:03,656 DEBUG CV Batch 23/1600 loss 7.720115 loss_att 11.849812 loss_ctc 10.096124 loss_rnnt 6.481333 hw_loss 0.180077 history loss 6.787321 rank 2
2023-02-25 05:11:08,182 DEBUG CV Batch 23/1700 loss 10.654221 loss_att 9.237776 loss_ctc 15.176310 loss_rnnt 10.229146 hw_loss 0.197660 history loss 6.698774 rank 6
2023-02-25 05:11:10,604 DEBUG CV Batch 23/1700 loss 10.654221 loss_att 9.237776 loss_ctc 15.176310 loss_rnnt 10.229146 hw_loss 0.197660 history loss 6.698774 rank 3
2023-02-25 05:11:11,332 DEBUG CV Batch 23/1700 loss 10.654221 loss_att 9.237776 loss_ctc 15.176310 loss_rnnt 10.229146 hw_loss 0.197660 history loss 6.698774 rank 4
2023-02-25 05:11:11,395 DEBUG CV Batch 23/1700 loss 10.654221 loss_att 9.237776 loss_ctc 15.176310 loss_rnnt 10.229146 hw_loss 0.197660 history loss 6.698774 rank 5
2023-02-25 05:11:12,471 DEBUG CV Batch 23/1700 loss 10.654221 loss_att 9.237776 loss_ctc 15.176310 loss_rnnt 10.229146 hw_loss 0.197660 history loss 6.698774 rank 1
2023-02-25 05:11:14,378 DEBUG CV Batch 23/1700 loss 10.654221 loss_att 9.237776 loss_ctc 15.176310 loss_rnnt 10.229146 hw_loss 0.197660 history loss 6.698774 rank 7
2023-02-25 05:11:14,489 DEBUG CV Batch 23/1700 loss 10.654221 loss_att 9.237776 loss_ctc 15.176310 loss_rnnt 10.229146 hw_loss 0.197660 history loss 6.698774 rank 0
2023-02-25 05:11:16,291 DEBUG CV Batch 23/1700 loss 10.654221 loss_att 9.237776 loss_ctc 15.176310 loss_rnnt 10.229146 hw_loss 0.197660 history loss 6.698774 rank 2
2023-02-25 05:11:17,514 INFO Epoch 23 CV info cv_loss 6.667951810726508
2023-02-25 05:11:17,514 INFO Epoch 24 TRAIN info lr 0.00035341911683604453
2023-02-25 05:11:17,516 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 05:11:20,121 INFO Epoch 23 CV info cv_loss 6.667951813724399
2023-02-25 05:11:20,121 INFO Epoch 24 TRAIN info lr 0.000353472101357149
2023-02-25 05:11:20,123 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 05:11:20,570 INFO Epoch 23 CV info cv_loss 6.667951813584412
2023-02-25 05:11:20,571 INFO Epoch 24 TRAIN info lr 0.00035345620349870024
2023-02-25 05:11:20,575 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 05:11:20,677 INFO Epoch 23 CV info cv_loss 6.66795181341858
2023-02-25 05:11:20,677 INFO Epoch 24 TRAIN info lr 0.00035338998550367103
2023-02-25 05:11:20,683 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 05:11:21,424 INFO Epoch 23 CV info cv_loss 6.6679518147452335
2023-02-25 05:11:21,424 INFO Epoch 24 TRAIN info lr 0.00035345090468921585
2023-02-25 05:11:21,429 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 05:11:23,625 INFO Epoch 23 CV info cv_loss 6.6679518124192825
2023-02-25 05:11:23,626 INFO Epoch 24 TRAIN info lr 0.00035340146062312947
2023-02-25 05:11:23,631 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 05:11:23,749 INFO Epoch 23 CV info cv_loss 6.667951812139307
2023-02-25 05:11:23,749 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/23.pt
2023-02-25 05:11:24,311 INFO Epoch 24 TRAIN info lr 0.000353431477759784
2023-02-25 05:11:24,315 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 05:11:25,607 INFO Epoch 23 CV info cv_loss 6.667951812453741
2023-02-25 05:11:25,608 INFO Epoch 24 TRAIN info lr 0.00035341999971615854
2023-02-25 05:11:25,613 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 05:12:27,000 DEBUG TRAIN Batch 24/0 loss 8.686586 loss_att 8.944370 loss_ctc 10.241546 loss_rnnt 8.289348 hw_loss 0.259413 lr 0.00035343 rank 0
2023-02-25 05:12:27,001 DEBUG TRAIN Batch 24/0 loss 7.894583 loss_att 7.844760 loss_ctc 10.193192 loss_rnnt 7.441094 hw_loss 0.294321 lr 0.00035347 rank 3
2023-02-25 05:12:26,999 DEBUG TRAIN Batch 24/0 loss 8.317049 loss_att 7.512868 loss_ctc 10.957202 loss_rnnt 7.977255 hw_loss 0.278641 lr 0.00035346 rank 4
2023-02-25 05:12:27,004 DEBUG TRAIN Batch 24/0 loss 9.346293 loss_att 8.361198 loss_ctc 11.086033 loss_rnnt 9.122202 hw_loss 0.354647 lr 0.00035345 rank 1
2023-02-25 05:12:27,005 DEBUG TRAIN Batch 24/0 loss 8.696939 loss_att 9.209349 loss_ctc 11.688744 loss_rnnt 8.076057 hw_loss 0.224047 lr 0.00035342 rank 6
2023-02-25 05:12:27,007 DEBUG TRAIN Batch 24/0 loss 11.695642 loss_att 11.676046 loss_ctc 14.471087 loss_rnnt 11.181071 hw_loss 0.278307 lr 0.00035339 rank 5
2023-02-25 05:12:27,011 DEBUG TRAIN Batch 24/0 loss 8.019257 loss_att 7.927652 loss_ctc 10.492795 loss_rnnt 7.541567 hw_loss 0.311634 lr 0.00035342 rank 2
2023-02-25 05:12:27,025 DEBUG TRAIN Batch 24/0 loss 10.230107 loss_att 9.863525 loss_ctc 12.316116 loss_rnnt 9.861460 hw_loss 0.307181 lr 0.00035340 rank 7
2023-02-25 05:13:36,374 DEBUG TRAIN Batch 24/100 loss 8.649632 loss_att 12.670335 loss_ctc 11.281673 loss_rnnt 7.397102 hw_loss 0.182718 lr 0.00035334 rank 0
2023-02-25 05:13:36,374 DEBUG TRAIN Batch 24/100 loss 2.451566 loss_att 6.453434 loss_ctc 5.861320 loss_rnnt 1.131792 hw_loss 0.121437 lr 0.00035333 rank 2
2023-02-25 05:13:36,376 DEBUG TRAIN Batch 24/100 loss 5.605183 loss_att 8.608648 loss_ctc 10.300650 loss_rnnt 4.308891 hw_loss 0.130382 lr 0.00035336 rank 1
2023-02-25 05:13:36,377 DEBUG TRAIN Batch 24/100 loss 15.922102 loss_att 19.193995 loss_ctc 22.775566 loss_rnnt 14.268360 hw_loss 0.160443 lr 0.00035338 rank 3
2023-02-25 05:13:36,378 DEBUG TRAIN Batch 24/100 loss 6.203700 loss_att 6.739275 loss_ctc 7.816972 loss_rnnt 5.775786 hw_loss 0.198179 lr 0.00035333 rank 6
2023-02-25 05:13:36,380 DEBUG TRAIN Batch 24/100 loss 6.566348 loss_att 8.849903 loss_ctc 8.399624 loss_rnnt 5.752811 hw_loss 0.210730 lr 0.00035331 rank 7
2023-02-25 05:13:36,382 DEBUG TRAIN Batch 24/100 loss 6.789909 loss_att 9.155115 loss_ctc 8.979532 loss_rnnt 5.935146 hw_loss 0.168323 lr 0.00035330 rank 5
2023-02-25 05:13:36,428 DEBUG TRAIN Batch 24/100 loss 8.068069 loss_att 8.897502 loss_ctc 10.460042 loss_rnnt 7.521379 hw_loss 0.116011 lr 0.00035337 rank 4
2023-02-25 05:14:46,239 DEBUG TRAIN Batch 24/200 loss 10.558765 loss_att 12.461402 loss_ctc 14.711169 loss_rnnt 9.488304 hw_loss 0.255524 lr 0.00035325 rank 0
2023-02-25 05:14:46,242 DEBUG TRAIN Batch 24/200 loss 11.964871 loss_att 12.996915 loss_ctc 13.115475 loss_rnnt 11.459010 hw_loss 0.273824 lr 0.00035322 rank 7
2023-02-25 05:14:46,243 DEBUG TRAIN Batch 24/200 loss 3.624695 loss_att 6.072271 loss_ctc 5.178782 loss_rnnt 2.833520 hw_loss 0.177089 lr 0.00035328 rank 4
2023-02-25 05:14:46,244 DEBUG TRAIN Batch 24/200 loss 8.177526 loss_att 12.769585 loss_ctc 15.863930 loss_rnnt 6.084811 hw_loss 0.280218 lr 0.00035329 rank 3
2023-02-25 05:14:46,245 DEBUG TRAIN Batch 24/200 loss 5.718934 loss_att 8.204611 loss_ctc 8.832792 loss_rnnt 4.651938 hw_loss 0.290023 lr 0.00035321 rank 5
2023-02-25 05:14:46,246 DEBUG TRAIN Batch 24/200 loss 5.900895 loss_att 9.185830 loss_ctc 8.282775 loss_rnnt 4.776062 hw_loss 0.281741 lr 0.00035327 rank 1
2023-02-25 05:14:46,246 DEBUG TRAIN Batch 24/200 loss 12.159584 loss_att 12.072130 loss_ctc 18.420324 loss_rnnt 11.233381 hw_loss 0.204241 lr 0.00035324 rank 2
2023-02-25 05:14:46,248 DEBUG TRAIN Batch 24/200 loss 9.446401 loss_att 11.307089 loss_ctc 11.410734 loss_rnnt 8.704039 hw_loss 0.203087 lr 0.00035324 rank 6
2023-02-25 05:15:57,469 DEBUG TRAIN Batch 24/300 loss 8.888251 loss_att 15.866343 loss_ctc 14.589964 loss_rnnt 6.647647 hw_loss 0.158919 lr 0.00035315 rank 6
2023-02-25 05:15:57,470 DEBUG TRAIN Batch 24/300 loss 10.702045 loss_att 13.375236 loss_ctc 12.620100 loss_rnnt 9.825114 hw_loss 0.162286 lr 0.00035315 rank 2
2023-02-25 05:15:57,487 DEBUG TRAIN Batch 24/300 loss 6.436040 loss_att 8.750661 loss_ctc 7.532673 loss_rnnt 5.707171 hw_loss 0.224488 lr 0.00035321 rank 3
2023-02-25 05:15:57,490 DEBUG TRAIN Batch 24/300 loss 5.807861 loss_att 8.257998 loss_ctc 8.941861 loss_rnnt 4.835747 hw_loss 0.120412 lr 0.00035319 rank 1
2023-02-25 05:15:57,491 DEBUG TRAIN Batch 24/300 loss 12.701026 loss_att 14.470697 loss_ctc 20.161007 loss_rnnt 11.256792 hw_loss 0.179315 lr 0.00035312 rank 5
2023-02-25 05:15:57,493 DEBUG TRAIN Batch 24/300 loss 11.903440 loss_att 13.061253 loss_ctc 12.566745 loss_rnnt 11.480688 hw_loss 0.192657 lr 0.00035314 rank 7
2023-02-25 05:15:57,502 DEBUG TRAIN Batch 24/300 loss 7.331196 loss_att 9.746401 loss_ctc 9.724678 loss_rnnt 6.457906 hw_loss 0.133346 lr 0.00035317 rank 0
2023-02-25 05:15:57,534 DEBUG TRAIN Batch 24/300 loss 11.441607 loss_att 15.814848 loss_ctc 16.382553 loss_rnnt 9.811741 hw_loss 0.180800 lr 0.00035319 rank 4
2023-02-25 05:17:09,099 DEBUG TRAIN Batch 24/400 loss 9.452139 loss_att 11.581923 loss_ctc 11.244705 loss_rnnt 8.696568 hw_loss 0.169884 lr 0.00035308 rank 0
2023-02-25 05:17:09,105 DEBUG TRAIN Batch 24/400 loss 12.409303 loss_att 13.006241 loss_ctc 18.463596 loss_rnnt 11.372890 hw_loss 0.205848 lr 0.00035304 rank 5
2023-02-25 05:17:09,105 DEBUG TRAIN Batch 24/400 loss 4.112000 loss_att 5.886536 loss_ctc 8.740270 loss_rnnt 2.992862 hw_loss 0.275866 lr 0.00035312 rank 3
2023-02-25 05:17:09,106 DEBUG TRAIN Batch 24/400 loss 4.277455 loss_att 8.870493 loss_ctc 5.568205 loss_rnnt 3.064630 hw_loss 0.228971 lr 0.00035310 rank 1
2023-02-25 05:17:09,106 DEBUG TRAIN Batch 24/400 loss 15.454443 loss_att 17.155575 loss_ctc 21.586721 loss_rnnt 14.188795 hw_loss 0.202096 lr 0.00035307 rank 6
2023-02-25 05:17:09,106 DEBUG TRAIN Batch 24/400 loss 9.014785 loss_att 10.476078 loss_ctc 12.964384 loss_rnnt 8.045725 hw_loss 0.281600 lr 0.00035305 rank 7
2023-02-25 05:17:09,107 DEBUG TRAIN Batch 24/400 loss 5.672092 loss_att 8.698694 loss_ctc 6.479790 loss_rnnt 4.824454 hw_loss 0.252421 lr 0.00035307 rank 2
2023-02-25 05:17:09,166 DEBUG TRAIN Batch 24/400 loss 7.721833 loss_att 8.738431 loss_ctc 10.666725 loss_rnnt 6.977005 hw_loss 0.279106 lr 0.00035310 rank 4
2023-02-25 05:18:19,424 DEBUG TRAIN Batch 24/500 loss 5.502443 loss_att 9.191628 loss_ctc 8.510588 loss_rnnt 4.259812 hw_loss 0.194453 lr 0.00035299 rank 0
2023-02-25 05:18:19,426 DEBUG TRAIN Batch 24/500 loss 8.750281 loss_att 10.013601 loss_ctc 11.678928 loss_rnnt 7.979150 hw_loss 0.239963 lr 0.00035301 rank 1
2023-02-25 05:18:19,430 DEBUG TRAIN Batch 24/500 loss 7.037759 loss_att 9.176601 loss_ctc 9.243261 loss_rnnt 6.218392 hw_loss 0.182870 lr 0.00035298 rank 6
2023-02-25 05:18:19,430 DEBUG TRAIN Batch 24/500 loss 12.450050 loss_att 14.414070 loss_ctc 18.124424 loss_rnnt 11.199910 hw_loss 0.188912 lr 0.00035298 rank 2
2023-02-25 05:18:19,433 DEBUG TRAIN Batch 24/500 loss 9.132617 loss_att 12.511457 loss_ctc 12.941421 loss_rnnt 7.862461 hw_loss 0.162275 lr 0.00035301 rank 4
2023-02-25 05:18:19,436 DEBUG TRAIN Batch 24/500 loss 8.156652 loss_att 12.720150 loss_ctc 13.459888 loss_rnnt 6.419237 hw_loss 0.220533 lr 0.00035303 rank 3
2023-02-25 05:18:19,438 DEBUG TRAIN Batch 24/500 loss 7.947803 loss_att 8.578977 loss_ctc 9.268315 loss_rnnt 7.466814 hw_loss 0.335035 lr 0.00035295 rank 5
2023-02-25 05:18:19,438 DEBUG TRAIN Batch 24/500 loss 3.535706 loss_att 6.169716 loss_ctc 5.242163 loss_rnnt 2.670777 hw_loss 0.207373 lr 0.00035296 rank 7
2023-02-25 05:19:30,103 DEBUG TRAIN Batch 24/600 loss 5.110372 loss_att 5.708737 loss_ctc 6.855585 loss_rnnt 4.653381 hw_loss 0.196168 lr 0.00035292 rank 1
2023-02-25 05:19:30,104 DEBUG TRAIN Batch 24/600 loss 4.548846 loss_att 5.471521 loss_ctc 5.575162 loss_rnnt 4.114907 hw_loss 0.211054 lr 0.00035293 rank 4
2023-02-25 05:19:30,104 DEBUG TRAIN Batch 24/600 loss 5.093609 loss_att 6.614082 loss_ctc 7.596842 loss_rnnt 4.347078 hw_loss 0.203761 lr 0.00035294 rank 3
2023-02-25 05:19:30,105 DEBUG TRAIN Batch 24/600 loss 9.559712 loss_att 11.444655 loss_ctc 13.118876 loss_rnnt 8.564987 hw_loss 0.268463 lr 0.00035290 rank 0
2023-02-25 05:19:30,108 DEBUG TRAIN Batch 24/600 loss 6.674553 loss_att 7.857518 loss_ctc 8.689157 loss_rnnt 6.060909 hw_loss 0.203319 lr 0.00035289 rank 2
2023-02-25 05:19:30,111 DEBUG TRAIN Batch 24/600 loss 3.264462 loss_att 4.580204 loss_ctc 5.182845 loss_rnnt 2.583222 hw_loss 0.304328 lr 0.00035286 rank 5
2023-02-25 05:19:30,110 DEBUG TRAIN Batch 24/600 loss 5.580019 loss_att 8.691946 loss_ctc 8.578976 loss_rnnt 4.369822 hw_loss 0.352407 lr 0.00035289 rank 6
2023-02-25 05:19:30,112 DEBUG TRAIN Batch 24/600 loss 7.258167 loss_att 8.166292 loss_ctc 9.874043 loss_rnnt 6.588030 hw_loss 0.261991 lr 0.00035287 rank 7
2023-02-25 05:20:43,863 DEBUG TRAIN Batch 24/700 loss 12.653100 loss_att 16.504612 loss_ctc 16.110588 loss_rnnt 11.301138 hw_loss 0.226241 lr 0.00035281 rank 0
2023-02-25 05:20:43,869 DEBUG TRAIN Batch 24/700 loss 4.425471 loss_att 8.555801 loss_ctc 7.473907 loss_rnnt 3.067585 hw_loss 0.235052 lr 0.00035278 rank 7
2023-02-25 05:20:43,871 DEBUG TRAIN Batch 24/700 loss 3.514385 loss_att 5.612517 loss_ctc 3.760743 loss_rnnt 2.950998 hw_loss 0.207959 lr 0.00035285 rank 3
2023-02-25 05:20:43,874 DEBUG TRAIN Batch 24/700 loss 9.903612 loss_att 11.427136 loss_ctc 8.982889 loss_rnnt 9.601975 hw_loss 0.224428 lr 0.00035284 rank 4
2023-02-25 05:20:43,877 DEBUG TRAIN Batch 24/700 loss 4.243599 loss_att 5.986047 loss_ctc 4.847988 loss_rnnt 3.748200 hw_loss 0.124357 lr 0.00035280 rank 2
2023-02-25 05:20:43,880 DEBUG TRAIN Batch 24/700 loss 5.498546 loss_att 9.798002 loss_ctc 10.373369 loss_rnnt 3.869929 hw_loss 0.222655 lr 0.00035277 rank 5
2023-02-25 05:20:43,880 DEBUG TRAIN Batch 24/700 loss 19.231234 loss_att 18.606110 loss_ctc 27.344986 loss_rnnt 18.172832 hw_loss 0.190486 lr 0.00035280 rank 6
2023-02-25 05:20:43,913 DEBUG TRAIN Batch 24/700 loss 6.768098 loss_att 9.664795 loss_ctc 10.784147 loss_rnnt 5.597125 hw_loss 0.105301 lr 0.00035283 rank 1
2023-02-25 05:21:55,529 DEBUG TRAIN Batch 24/800 loss 7.991129 loss_att 8.835436 loss_ctc 10.270871 loss_rnnt 7.422561 hw_loss 0.179516 lr 0.00035277 rank 3
2023-02-25 05:21:55,531 DEBUG TRAIN Batch 24/800 loss 2.306573 loss_att 6.569084 loss_ctc 4.227942 loss_rnnt 1.159349 hw_loss 0.072262 lr 0.00035269 rank 5
2023-02-25 05:21:55,532 DEBUG TRAIN Batch 24/800 loss 7.908117 loss_att 10.527871 loss_ctc 12.212543 loss_rnnt 6.673459 hw_loss 0.256467 lr 0.00035273 rank 0
2023-02-25 05:21:55,532 DEBUG TRAIN Batch 24/800 loss 11.896569 loss_att 13.637753 loss_ctc 13.286326 loss_rnnt 11.257195 hw_loss 0.198445 lr 0.00035271 rank 6
2023-02-25 05:21:55,533 DEBUG TRAIN Batch 24/800 loss 4.112482 loss_att 6.820928 loss_ctc 6.552397 loss_rnnt 3.171097 hw_loss 0.139451 lr 0.00035270 rank 7
2023-02-25 05:21:55,537 DEBUG TRAIN Batch 24/800 loss 5.149943 loss_att 7.796810 loss_ctc 11.273513 loss_rnnt 3.668475 hw_loss 0.254287 lr 0.00035271 rank 2
2023-02-25 05:21:55,537 DEBUG TRAIN Batch 24/800 loss 3.725722 loss_att 6.094970 loss_ctc 5.836219 loss_rnnt 2.853291 hw_loss 0.219715 lr 0.00035275 rank 4
2023-02-25 05:21:55,544 DEBUG TRAIN Batch 24/800 loss 10.958858 loss_att 13.410259 loss_ctc 15.030990 loss_rnnt 9.795733 hw_loss 0.243548 lr 0.00035275 rank 1
2023-02-25 05:23:06,152 DEBUG TRAIN Batch 24/900 loss 13.504052 loss_att 14.375853 loss_ctc 20.504478 loss_rnnt 12.334505 hw_loss 0.115868 lr 0.00035263 rank 6
2023-02-25 05:23:06,163 DEBUG TRAIN Batch 24/900 loss 6.797563 loss_att 11.284657 loss_ctc 10.931432 loss_rnnt 5.261577 hw_loss 0.163846 lr 0.00035263 rank 2
2023-02-25 05:23:06,163 DEBUG TRAIN Batch 24/900 loss 11.771181 loss_att 14.445009 loss_ctc 15.383604 loss_rnnt 10.655850 hw_loss 0.185452 lr 0.00035268 rank 3
2023-02-25 05:23:06,163 DEBUG TRAIN Batch 24/900 loss 7.020593 loss_att 10.527311 loss_ctc 8.103245 loss_rnnt 6.067769 hw_loss 0.200861 lr 0.00035264 rank 0
2023-02-25 05:23:06,169 DEBUG TRAIN Batch 24/900 loss 3.816624 loss_att 8.256210 loss_ctc 4.987462 loss_rnnt 2.677185 hw_loss 0.178893 lr 0.00035260 rank 5
2023-02-25 05:23:06,171 DEBUG TRAIN Batch 24/900 loss 10.143734 loss_att 14.197250 loss_ctc 19.223516 loss_rnnt 8.016913 hw_loss 0.197774 lr 0.00035261 rank 7
2023-02-25 05:23:06,173 DEBUG TRAIN Batch 24/900 loss 6.613406 loss_att 9.989281 loss_ctc 9.693657 loss_rnnt 5.458669 hw_loss 0.129116 lr 0.00035266 rank 1
2023-02-25 05:23:06,177 DEBUG TRAIN Batch 24/900 loss 3.944284 loss_att 7.224353 loss_ctc 4.481892 loss_rnnt 3.126075 hw_loss 0.169714 lr 0.00035266 rank 4
2023-02-25 05:24:17,089 DEBUG TRAIN Batch 24/1000 loss 9.361370 loss_att 12.537134 loss_ctc 14.145597 loss_rnnt 7.987571 hw_loss 0.188905 lr 0.00035252 rank 7
2023-02-25 05:24:17,092 DEBUG TRAIN Batch 24/1000 loss 5.405125 loss_att 10.499247 loss_ctc 7.401831 loss_rnnt 4.037085 hw_loss 0.155602 lr 0.00035254 rank 2
2023-02-25 05:24:17,095 DEBUG TRAIN Batch 24/1000 loss 14.168472 loss_att 17.482410 loss_ctc 22.833229 loss_rnnt 12.232735 hw_loss 0.220589 lr 0.00035255 rank 0
2023-02-25 05:24:17,096 DEBUG TRAIN Batch 24/1000 loss 11.979397 loss_att 12.657608 loss_ctc 15.506126 loss_rnnt 11.241642 hw_loss 0.247278 lr 0.00035257 rank 1
2023-02-25 05:24:17,098 DEBUG TRAIN Batch 24/1000 loss 17.656837 loss_att 20.984373 loss_ctc 21.949017 loss_rnnt 16.292374 hw_loss 0.237499 lr 0.00035259 rank 3
2023-02-25 05:24:17,101 DEBUG TRAIN Batch 24/1000 loss 3.435547 loss_att 5.846234 loss_ctc 3.553819 loss_rnnt 2.846717 hw_loss 0.170481 lr 0.00035258 rank 4
2023-02-25 05:24:17,101 DEBUG TRAIN Batch 24/1000 loss 5.703058 loss_att 9.055439 loss_ctc 9.047914 loss_rnnt 4.447604 hw_loss 0.260620 lr 0.00035254 rank 6
2023-02-25 05:24:17,106 DEBUG TRAIN Batch 24/1000 loss 13.061620 loss_att 15.013206 loss_ctc 18.194048 loss_rnnt 11.847718 hw_loss 0.261113 lr 0.00035251 rank 5
2023-02-25 05:25:29,436 DEBUG TRAIN Batch 24/1100 loss 5.961064 loss_att 9.116048 loss_ctc 6.797283 loss_rnnt 5.125028 hw_loss 0.175395 lr 0.00035246 rank 0
2023-02-25 05:25:29,437 DEBUG TRAIN Batch 24/1100 loss 5.162517 loss_att 7.597980 loss_ctc 8.440720 loss_rnnt 4.150651 hw_loss 0.164399 lr 0.00035250 rank 3
2023-02-25 05:25:29,437 DEBUG TRAIN Batch 24/1100 loss 9.085681 loss_att 11.565240 loss_ctc 11.666774 loss_rnnt 8.169729 hw_loss 0.142301 lr 0.00035248 rank 1
2023-02-25 05:25:29,441 DEBUG TRAIN Batch 24/1100 loss 4.119239 loss_att 6.819577 loss_ctc 7.072537 loss_rnnt 3.030590 hw_loss 0.290267 lr 0.00035249 rank 4
2023-02-25 05:25:29,442 DEBUG TRAIN Batch 24/1100 loss 6.407843 loss_att 8.363226 loss_ctc 8.694556 loss_rnnt 5.640787 hw_loss 0.133281 lr 0.00035243 rank 7
2023-02-25 05:25:29,448 DEBUG TRAIN Batch 24/1100 loss 13.532730 loss_att 13.453255 loss_ctc 18.144211 loss_rnnt 12.827377 hw_loss 0.199470 lr 0.00035245 rank 2
2023-02-25 05:25:29,450 DEBUG TRAIN Batch 24/1100 loss 5.636825 loss_att 6.859976 loss_ctc 8.015638 loss_rnnt 4.955467 hw_loss 0.224162 lr 0.00035242 rank 5
2023-02-25 05:25:29,493 DEBUG TRAIN Batch 24/1100 loss 11.879328 loss_att 13.401148 loss_ctc 14.035308 loss_rnnt 11.195149 hw_loss 0.173155 lr 0.00035245 rank 6
2023-02-25 05:26:39,562 DEBUG TRAIN Batch 24/1200 loss 3.746945 loss_att 6.873526 loss_ctc 6.671627 loss_rnnt 2.628857 hw_loss 0.192776 lr 0.00035242 rank 3
2023-02-25 05:26:39,565 DEBUG TRAIN Batch 24/1200 loss 10.574623 loss_att 11.689785 loss_ctc 16.038830 loss_rnnt 9.500093 hw_loss 0.230505 lr 0.00035236 rank 6
2023-02-25 05:26:39,566 DEBUG TRAIN Batch 24/1200 loss 8.974926 loss_att 11.143851 loss_ctc 10.095425 loss_rnnt 8.242133 hw_loss 0.280515 lr 0.00035240 rank 4
2023-02-25 05:26:39,566 DEBUG TRAIN Batch 24/1200 loss 5.665785 loss_att 8.484551 loss_ctc 6.928063 loss_rnnt 4.789410 hw_loss 0.270595 lr 0.00035238 rank 0
2023-02-25 05:26:39,567 DEBUG TRAIN Batch 24/1200 loss 3.325103 loss_att 5.113754 loss_ctc 4.153344 loss_rnnt 2.725391 hw_loss 0.246655 lr 0.00035236 rank 2
2023-02-25 05:26:39,569 DEBUG TRAIN Batch 24/1200 loss 7.788852 loss_att 10.279869 loss_ctc 9.915302 loss_rnnt 6.843157 hw_loss 0.307433 lr 0.00035240 rank 1
2023-02-25 05:26:39,573 DEBUG TRAIN Batch 24/1200 loss 10.229490 loss_att 12.386212 loss_ctc 16.132603 loss_rnnt 8.856435 hw_loss 0.289928 lr 0.00035233 rank 5
2023-02-25 05:26:39,575 DEBUG TRAIN Batch 24/1200 loss 8.677200 loss_att 10.407374 loss_ctc 13.349742 loss_rnnt 7.578165 hw_loss 0.243740 lr 0.00035235 rank 7
2023-02-25 05:27:50,008 DEBUG TRAIN Batch 24/1300 loss 27.072563 loss_att 31.629707 loss_ctc 42.683956 loss_rnnt 23.949278 hw_loss 0.244380 lr 0.00035226 rank 7
2023-02-25 05:27:50,011 DEBUG TRAIN Batch 24/1300 loss 14.360017 loss_att 13.375587 loss_ctc 18.646494 loss_rnnt 13.864410 hw_loss 0.226803 lr 0.00035231 rank 4
2023-02-25 05:27:50,012 DEBUG TRAIN Batch 24/1300 loss 5.572940 loss_att 5.961960 loss_ctc 7.653021 loss_rnnt 5.085161 hw_loss 0.248684 lr 0.00035225 rank 5
2023-02-25 05:27:50,013 DEBUG TRAIN Batch 24/1300 loss 11.182277 loss_att 12.430756 loss_ctc 15.105519 loss_rnnt 10.277323 hw_loss 0.247799 lr 0.00035233 rank 3
2023-02-25 05:27:50,013 DEBUG TRAIN Batch 24/1300 loss 14.014524 loss_att 17.577896 loss_ctc 22.404972 loss_rnnt 12.125881 hw_loss 0.107326 lr 0.00035231 rank 1
2023-02-25 05:27:50,013 DEBUG TRAIN Batch 24/1300 loss 1.032616 loss_att 3.943422 loss_ctc 1.236198 loss_rnnt 0.346394 hw_loss 0.144218 lr 0.00035229 rank 0
2023-02-25 05:27:50,022 DEBUG TRAIN Batch 24/1300 loss 7.880404 loss_att 8.532187 loss_ctc 9.711553 loss_rnnt 7.382683 hw_loss 0.231022 lr 0.00035228 rank 2
2023-02-25 05:27:50,062 DEBUG TRAIN Batch 24/1300 loss 1.887386 loss_att 5.863198 loss_ctc 4.615851 loss_rnnt 0.617725 hw_loss 0.207568 lr 0.00035228 rank 6
2023-02-25 05:29:03,289 DEBUG TRAIN Batch 24/1400 loss 6.571472 loss_att 8.800125 loss_ctc 9.419226 loss_rnnt 5.618373 hw_loss 0.239377 lr 0.00035224 rank 3
2023-02-25 05:29:03,306 DEBUG TRAIN Batch 24/1400 loss 7.110093 loss_att 8.779587 loss_ctc 10.305472 loss_rnnt 6.288518 hw_loss 0.115548 lr 0.00035219 rank 6
2023-02-25 05:29:03,306 DEBUG TRAIN Batch 24/1400 loss 7.548402 loss_att 10.177017 loss_ctc 11.054281 loss_rnnt 6.483102 hw_loss 0.135237 lr 0.00035222 rank 1
2023-02-25 05:29:03,308 DEBUG TRAIN Batch 24/1400 loss 3.519248 loss_att 6.817128 loss_ctc 4.635860 loss_rnnt 2.637540 hw_loss 0.137343 lr 0.00035219 rank 2
2023-02-25 05:29:03,309 DEBUG TRAIN Batch 24/1400 loss 6.099348 loss_att 9.146599 loss_ctc 6.698386 loss_rnnt 5.299468 hw_loss 0.207297 lr 0.00035220 rank 0
2023-02-25 05:29:03,310 DEBUG TRAIN Batch 24/1400 loss 8.952586 loss_att 12.245296 loss_ctc 17.088163 loss_rnnt 7.161271 hw_loss 0.090054 lr 0.00035217 rank 7
2023-02-25 05:29:03,312 DEBUG TRAIN Batch 24/1400 loss 10.922799 loss_att 13.774672 loss_ctc 13.704948 loss_rnnt 9.873841 hw_loss 0.201804 lr 0.00035223 rank 4
2023-02-25 05:29:03,324 DEBUG TRAIN Batch 24/1400 loss 11.808105 loss_att 14.866508 loss_ctc 17.477364 loss_rnnt 10.315362 hw_loss 0.234679 lr 0.00035216 rank 5
2023-02-25 05:30:14,183 DEBUG TRAIN Batch 24/1500 loss 7.438837 loss_att 10.674820 loss_ctc 11.908766 loss_rnnt 6.105994 hw_loss 0.168104 lr 0.00035211 rank 0
2023-02-25 05:30:14,184 DEBUG TRAIN Batch 24/1500 loss 2.299327 loss_att 4.289993 loss_ctc 2.549468 loss_rnnt 1.747679 hw_loss 0.225303 lr 0.00035215 rank 3
2023-02-25 05:30:14,190 DEBUG TRAIN Batch 24/1500 loss 16.632061 loss_att 18.214294 loss_ctc 22.679739 loss_rnnt 15.425404 hw_loss 0.157224 lr 0.00035210 rank 6
2023-02-25 05:30:14,191 DEBUG TRAIN Batch 24/1500 loss 8.944011 loss_att 11.626005 loss_ctc 13.492760 loss_rnnt 7.718021 hw_loss 0.155795 lr 0.00035208 rank 7
2023-02-25 05:30:14,191 DEBUG TRAIN Batch 24/1500 loss 6.792471 loss_att 9.189432 loss_ctc 8.366639 loss_rnnt 6.016830 hw_loss 0.161924 lr 0.00035214 rank 4
2023-02-25 05:30:14,191 DEBUG TRAIN Batch 24/1500 loss 16.263538 loss_att 19.026007 loss_ctc 23.603806 loss_rnnt 14.670609 hw_loss 0.115751 lr 0.00035213 rank 1
2023-02-25 05:30:14,193 DEBUG TRAIN Batch 24/1500 loss 7.961385 loss_att 12.580206 loss_ctc 12.262684 loss_rnnt 6.365612 hw_loss 0.184691 lr 0.00035207 rank 5
2023-02-25 05:30:14,197 DEBUG TRAIN Batch 24/1500 loss 7.613874 loss_att 10.143748 loss_ctc 8.634439 loss_rnnt 6.888113 hw_loss 0.156958 lr 0.00035210 rank 2
2023-02-25 05:31:23,499 DEBUG TRAIN Batch 24/1600 loss 5.026160 loss_att 8.830446 loss_ctc 5.768154 loss_rnnt 4.082159 hw_loss 0.157895 lr 0.00035201 rank 6
2023-02-25 05:31:23,503 DEBUG TRAIN Batch 24/1600 loss 6.346766 loss_att 9.695826 loss_ctc 8.958445 loss_rnnt 5.243742 hw_loss 0.159353 lr 0.00035200 rank 7
2023-02-25 05:31:23,503 DEBUG TRAIN Batch 24/1600 loss 10.233978 loss_att 12.771973 loss_ctc 14.601351 loss_rnnt 9.068777 hw_loss 0.141161 lr 0.00035205 rank 1
2023-02-25 05:31:23,503 DEBUG TRAIN Batch 24/1600 loss 5.795529 loss_att 9.814541 loss_ctc 9.262664 loss_rnnt 4.438790 hw_loss 0.169973 lr 0.00035207 rank 3
2023-02-25 05:31:23,503 DEBUG TRAIN Batch 24/1600 loss 4.968541 loss_att 8.020106 loss_ctc 7.999076 loss_rnnt 3.877410 hw_loss 0.143900 lr 0.00035203 rank 0
2023-02-25 05:31:23,505 DEBUG TRAIN Batch 24/1600 loss 10.367737 loss_att 12.941728 loss_ctc 16.569489 loss_rnnt 8.909407 hw_loss 0.218685 lr 0.00035201 rank 2
2023-02-25 05:31:23,506 DEBUG TRAIN Batch 24/1600 loss 9.440539 loss_att 12.322894 loss_ctc 16.732271 loss_rnnt 7.767302 hw_loss 0.233506 lr 0.00035205 rank 4
2023-02-25 05:31:23,508 DEBUG TRAIN Batch 24/1600 loss 4.673737 loss_att 8.846643 loss_ctc 5.373393 loss_rnnt 3.624280 hw_loss 0.227978 lr 0.00035199 rank 5
2023-02-25 05:32:34,014 DEBUG TRAIN Batch 24/1700 loss 15.406719 loss_att 16.089378 loss_ctc 18.099072 loss_rnnt 14.818832 hw_loss 0.173203 lr 0.00035194 rank 0
2023-02-25 05:32:34,021 DEBUG TRAIN Batch 24/1700 loss 6.435648 loss_att 9.952145 loss_ctc 9.804229 loss_rnnt 5.206333 hw_loss 0.144135 lr 0.00035193 rank 6
2023-02-25 05:32:34,023 DEBUG TRAIN Batch 24/1700 loss 7.418365 loss_att 8.133558 loss_ctc 10.283352 loss_rnnt 6.747430 hw_loss 0.273557 lr 0.00035198 rank 3
2023-02-25 05:32:34,023 DEBUG TRAIN Batch 24/1700 loss 8.725647 loss_att 13.780842 loss_ctc 12.329351 loss_rnnt 7.068090 hw_loss 0.311295 lr 0.00035193 rank 2
2023-02-25 05:32:34,027 DEBUG TRAIN Batch 24/1700 loss 4.739045 loss_att 9.802265 loss_ctc 7.408829 loss_rnnt 3.274058 hw_loss 0.180696 lr 0.00035196 rank 4
2023-02-25 05:32:34,049 DEBUG TRAIN Batch 24/1700 loss 9.607294 loss_att 12.710531 loss_ctc 12.645571 loss_rnnt 8.518934 hw_loss 0.117390 lr 0.00035196 rank 1
2023-02-25 05:32:34,052 DEBUG TRAIN Batch 24/1700 loss 5.036705 loss_att 7.633390 loss_ctc 7.645089 loss_rnnt 4.052882 hw_loss 0.218814 lr 0.00035191 rank 7
2023-02-25 05:32:34,071 DEBUG TRAIN Batch 24/1700 loss 9.555910 loss_att 11.266899 loss_ctc 12.606266 loss_rnnt 8.724864 hw_loss 0.154002 lr 0.00035190 rank 5
2023-02-25 05:33:46,976 DEBUG TRAIN Batch 24/1800 loss 7.743210 loss_att 10.208365 loss_ctc 13.440202 loss_rnnt 6.419148 hw_loss 0.133932 lr 0.00035189 rank 3
2023-02-25 05:33:46,978 DEBUG TRAIN Batch 24/1800 loss 8.732287 loss_att 10.130375 loss_ctc 13.449130 loss_rnnt 7.690568 hw_loss 0.249729 lr 0.00035184 rank 6
2023-02-25 05:33:46,979 DEBUG TRAIN Batch 24/1800 loss 9.406304 loss_att 11.722768 loss_ctc 12.767410 loss_rnnt 8.431533 hw_loss 0.118748 lr 0.00035185 rank 0
2023-02-25 05:33:46,984 DEBUG TRAIN Batch 24/1800 loss 9.524090 loss_att 10.155631 loss_ctc 12.939185 loss_rnnt 8.812628 hw_loss 0.243389 lr 0.00035187 rank 1
2023-02-25 05:33:46,986 DEBUG TRAIN Batch 24/1800 loss 8.945436 loss_att 11.819746 loss_ctc 13.261222 loss_rnnt 7.710505 hw_loss 0.158684 lr 0.00035184 rank 2
2023-02-25 05:33:46,986 DEBUG TRAIN Batch 24/1800 loss 7.229007 loss_att 9.994333 loss_ctc 10.647261 loss_rnnt 6.106987 hw_loss 0.212226 lr 0.00035181 rank 5
2023-02-25 05:33:46,985 DEBUG TRAIN Batch 24/1800 loss 13.213527 loss_att 19.533226 loss_ctc 22.669239 loss_rnnt 10.547035 hw_loss 0.265856 lr 0.00035182 rank 7
2023-02-25 05:33:46,986 DEBUG TRAIN Batch 24/1800 loss 10.713810 loss_att 13.428787 loss_ctc 12.791660 loss_rnnt 9.791158 hw_loss 0.192391 lr 0.00035188 rank 4
2023-02-25 05:34:58,290 DEBUG TRAIN Batch 24/1900 loss 7.159506 loss_att 9.927887 loss_ctc 10.511408 loss_rnnt 6.051602 hw_loss 0.201201 lr 0.00035180 rank 3
2023-02-25 05:34:58,292 DEBUG TRAIN Batch 24/1900 loss 3.720558 loss_att 5.616611 loss_ctc 7.569221 loss_rnnt 2.719371 hw_loss 0.204041 lr 0.00035178 rank 1
2023-02-25 05:34:58,293 DEBUG TRAIN Batch 24/1900 loss 6.575159 loss_att 7.406660 loss_ctc 10.617467 loss_rnnt 5.701621 hw_loss 0.315494 lr 0.00035175 rank 6
2023-02-25 05:34:58,294 DEBUG TRAIN Batch 24/1900 loss 8.625901 loss_att 9.217570 loss_ctc 12.086980 loss_rnnt 7.924073 hw_loss 0.228778 lr 0.00035172 rank 5
2023-02-25 05:34:58,294 DEBUG TRAIN Batch 24/1900 loss 13.099793 loss_att 13.927738 loss_ctc 17.713787 loss_rnnt 12.212669 hw_loss 0.199379 lr 0.00035176 rank 0
2023-02-25 05:34:58,297 DEBUG TRAIN Batch 24/1900 loss 12.415612 loss_att 12.713988 loss_ctc 16.956640 loss_rnnt 11.578230 hw_loss 0.322945 lr 0.00035179 rank 4
2023-02-25 05:34:58,299 DEBUG TRAIN Batch 24/1900 loss 10.824412 loss_att 10.135461 loss_ctc 12.880080 loss_rnnt 10.565393 hw_loss 0.230101 lr 0.00035175 rank 2
2023-02-25 05:34:58,300 DEBUG TRAIN Batch 24/1900 loss 8.125841 loss_att 12.537313 loss_ctc 15.128212 loss_rnnt 6.245511 hw_loss 0.120724 lr 0.00035174 rank 7
2023-02-25 05:36:07,875 DEBUG TRAIN Batch 24/2000 loss 3.140092 loss_att 6.688994 loss_ctc 4.318164 loss_rnnt 2.187745 hw_loss 0.160294 lr 0.00035170 rank 1
2023-02-25 05:36:07,886 DEBUG TRAIN Batch 24/2000 loss 8.886048 loss_att 11.174478 loss_ctc 11.661329 loss_rnnt 8.003246 hw_loss 0.103273 lr 0.00035168 rank 0
2023-02-25 05:36:07,888 DEBUG TRAIN Batch 24/2000 loss 6.027438 loss_att 8.303202 loss_ctc 9.728469 loss_rnnt 4.985818 hw_loss 0.174367 lr 0.00035167 rank 6
2023-02-25 05:36:07,893 DEBUG TRAIN Batch 24/2000 loss 6.402330 loss_att 11.424229 loss_ctc 10.366993 loss_rnnt 4.730356 hw_loss 0.260574 lr 0.00035172 rank 3
2023-02-25 05:36:07,894 DEBUG TRAIN Batch 24/2000 loss 6.635321 loss_att 8.844862 loss_ctc 7.781505 loss_rnnt 5.917243 hw_loss 0.231270 lr 0.00035167 rank 2
2023-02-25 05:36:07,894 DEBUG TRAIN Batch 24/2000 loss 10.747014 loss_att 13.674019 loss_ctc 17.197968 loss_rnnt 9.239297 hw_loss 0.116603 lr 0.00035170 rank 4
2023-02-25 05:36:07,896 DEBUG TRAIN Batch 24/2000 loss 12.136509 loss_att 15.879821 loss_ctc 15.801186 loss_rnnt 10.803326 hw_loss 0.179809 lr 0.00035165 rank 7
2023-02-25 05:36:07,897 DEBUG TRAIN Batch 24/2000 loss 12.987044 loss_att 13.235594 loss_ctc 16.638897 loss_rnnt 12.339206 hw_loss 0.208528 lr 0.00035164 rank 5
2023-02-25 05:37:20,327 DEBUG TRAIN Batch 24/2100 loss 6.541799 loss_att 9.367302 loss_ctc 8.302393 loss_rnnt 5.676798 hw_loss 0.122164 lr 0.00035159 rank 0
2023-02-25 05:37:20,334 DEBUG TRAIN Batch 24/2100 loss 3.665567 loss_att 8.791577 loss_ctc 5.509655 loss_rnnt 2.221620 hw_loss 0.324123 lr 0.00035156 rank 7
2023-02-25 05:37:20,334 DEBUG TRAIN Batch 24/2100 loss 23.716629 loss_att 28.458395 loss_ctc 27.413513 loss_rnnt 22.177267 hw_loss 0.183919 lr 0.00035162 rank 4
2023-02-25 05:37:20,335 DEBUG TRAIN Batch 24/2100 loss 3.494530 loss_att 6.880336 loss_ctc 6.044641 loss_rnnt 2.407806 hw_loss 0.130402 lr 0.00035161 rank 1
2023-02-25 05:37:20,335 DEBUG TRAIN Batch 24/2100 loss 4.911987 loss_att 6.707603 loss_ctc 4.812753 loss_rnnt 4.514732 hw_loss 0.096304 lr 0.00035158 rank 6
2023-02-25 05:37:20,336 DEBUG TRAIN Batch 24/2100 loss 4.044806 loss_att 8.049166 loss_ctc 5.680893 loss_rnnt 2.912667 hw_loss 0.212106 lr 0.00035158 rank 2
2023-02-25 05:37:20,339 DEBUG TRAIN Batch 24/2100 loss 2.920959 loss_att 4.121897 loss_ctc 3.985614 loss_rnnt 2.397650 hw_loss 0.264690 lr 0.00035155 rank 5
2023-02-25 05:37:20,379 DEBUG TRAIN Batch 24/2100 loss 8.959466 loss_att 11.030138 loss_ctc 10.418303 loss_rnnt 8.242782 hw_loss 0.202572 lr 0.00035163 rank 3
2023-02-25 05:38:32,244 DEBUG TRAIN Batch 24/2200 loss 5.694488 loss_att 11.493369 loss_ctc 7.413266 loss_rnnt 4.197652 hw_loss 0.202293 lr 0.00035152 rank 1
2023-02-25 05:38:32,251 DEBUG TRAIN Batch 24/2200 loss 8.388499 loss_att 9.625705 loss_ctc 10.710020 loss_rnnt 7.701480 hw_loss 0.243829 lr 0.00035153 rank 4
2023-02-25 05:38:32,254 DEBUG TRAIN Batch 24/2200 loss 4.732279 loss_att 8.192790 loss_ctc 7.559669 loss_rnnt 3.527151 hw_loss 0.255078 lr 0.00035150 rank 0
2023-02-25 05:38:32,257 DEBUG TRAIN Batch 24/2200 loss 11.106823 loss_att 15.099812 loss_ctc 17.316193 loss_rnnt 9.383484 hw_loss 0.181550 lr 0.00035146 rank 5
2023-02-25 05:38:32,261 DEBUG TRAIN Batch 24/2200 loss 10.370107 loss_att 14.724251 loss_ctc 12.178194 loss_rnnt 9.165005 hw_loss 0.174739 lr 0.00035149 rank 6
2023-02-25 05:38:32,263 DEBUG TRAIN Batch 24/2200 loss 7.759476 loss_att 10.862444 loss_ctc 13.230431 loss_rnnt 6.333229 hw_loss 0.142862 lr 0.00035149 rank 2
2023-02-25 05:38:32,264 DEBUG TRAIN Batch 24/2200 loss 4.723750 loss_att 8.187112 loss_ctc 6.209233 loss_rnnt 3.735247 hw_loss 0.183313 lr 0.00035147 rank 7
2023-02-25 05:38:32,318 DEBUG TRAIN Batch 24/2200 loss 5.769693 loss_att 9.679577 loss_ctc 7.979150 loss_rnnt 4.544490 hw_loss 0.278685 lr 0.00035154 rank 3
2023-02-25 05:39:41,761 DEBUG TRAIN Batch 24/2300 loss 9.109743 loss_att 10.889178 loss_ctc 14.944696 loss_rnnt 7.838535 hw_loss 0.257487 lr 0.00035144 rank 4
2023-02-25 05:39:41,765 DEBUG TRAIN Batch 24/2300 loss 7.547965 loss_att 9.891951 loss_ctc 13.165397 loss_rnnt 6.235422 hw_loss 0.177665 lr 0.00035146 rank 3
2023-02-25 05:39:41,766 DEBUG TRAIN Batch 24/2300 loss 11.967629 loss_att 16.156441 loss_ctc 16.014013 loss_rnnt 10.484509 hw_loss 0.198453 lr 0.00035144 rank 1
2023-02-25 05:39:41,766 DEBUG TRAIN Batch 24/2300 loss 14.042375 loss_att 16.000174 loss_ctc 17.734268 loss_rnnt 13.103183 hw_loss 0.103837 lr 0.00035142 rank 0
2023-02-25 05:39:41,766 DEBUG TRAIN Batch 24/2300 loss 8.159866 loss_att 11.601348 loss_ctc 9.812377 loss_rnnt 7.170847 hw_loss 0.150728 lr 0.00035138 rank 5
2023-02-25 05:39:41,770 DEBUG TRAIN Batch 24/2300 loss 12.181817 loss_att 16.328777 loss_ctc 18.632404 loss_rnnt 10.377285 hw_loss 0.215740 lr 0.00035141 rank 2
2023-02-25 05:39:41,772 DEBUG TRAIN Batch 24/2300 loss 12.424730 loss_att 20.717991 loss_ctc 22.075460 loss_rnnt 9.395884 hw_loss 0.156435 lr 0.00035139 rank 7
2023-02-25 05:39:41,813 DEBUG TRAIN Batch 24/2300 loss 13.912073 loss_att 17.879019 loss_ctc 23.983988 loss_rnnt 11.683348 hw_loss 0.173277 lr 0.00035140 rank 6
2023-02-25 05:40:51,543 DEBUG TRAIN Batch 24/2400 loss 8.564445 loss_att 11.054902 loss_ctc 13.032977 loss_rnnt 7.356039 hw_loss 0.214707 lr 0.00035135 rank 4
2023-02-25 05:40:51,546 DEBUG TRAIN Batch 24/2400 loss 9.629315 loss_att 12.406624 loss_ctc 14.244670 loss_rnnt 8.343987 hw_loss 0.214661 lr 0.00035137 rank 3
2023-02-25 05:40:51,549 DEBUG TRAIN Batch 24/2400 loss 13.515627 loss_att 17.624884 loss_ctc 23.404404 loss_rnnt 11.273197 hw_loss 0.191388 lr 0.00035135 rank 1
2023-02-25 05:40:51,550 DEBUG TRAIN Batch 24/2400 loss 5.997048 loss_att 6.711521 loss_ctc 7.405136 loss_rnnt 5.544382 hw_loss 0.228801 lr 0.00035132 rank 6
2023-02-25 05:40:51,551 DEBUG TRAIN Batch 24/2400 loss 6.193268 loss_att 9.695152 loss_ctc 9.899327 loss_rnnt 4.916197 hw_loss 0.154787 lr 0.00035133 rank 0
2023-02-25 05:40:51,553 DEBUG TRAIN Batch 24/2400 loss 6.363076 loss_att 8.614715 loss_ctc 7.918981 loss_rnnt 5.617565 hw_loss 0.164492 lr 0.00035129 rank 5
2023-02-25 05:40:51,554 DEBUG TRAIN Batch 24/2400 loss 16.180244 loss_att 19.742636 loss_ctc 20.940836 loss_rnnt 14.750943 hw_loss 0.153896 lr 0.00035132 rank 2
2023-02-25 05:40:51,565 DEBUG TRAIN Batch 24/2400 loss 11.588181 loss_att 12.858660 loss_ctc 16.035187 loss_rnnt 10.597684 hw_loss 0.269001 lr 0.00035130 rank 7
2023-02-25 05:42:04,766 DEBUG TRAIN Batch 24/2500 loss 22.770092 loss_att 22.115742 loss_ctc 30.295410 loss_rnnt 21.814318 hw_loss 0.156128 lr 0.00035124 rank 0
2023-02-25 05:42:04,773 DEBUG TRAIN Batch 24/2500 loss 6.954412 loss_att 10.317286 loss_ctc 8.032834 loss_rnnt 6.023191 hw_loss 0.215358 lr 0.00035120 rank 5
2023-02-25 05:42:04,775 DEBUG TRAIN Batch 24/2500 loss 6.998355 loss_att 8.243107 loss_ctc 8.419111 loss_rnnt 6.441066 hw_loss 0.222946 lr 0.00035121 rank 7
2023-02-25 05:42:04,776 DEBUG TRAIN Batch 24/2500 loss 5.878738 loss_att 7.735487 loss_ctc 9.185337 loss_rnnt 4.955881 hw_loss 0.207426 lr 0.00035126 rank 1
2023-02-25 05:42:04,777 DEBUG TRAIN Batch 24/2500 loss 6.113025 loss_att 6.892302 loss_ctc 9.409559 loss_rnnt 5.436100 hw_loss 0.152872 lr 0.00035128 rank 3
2023-02-25 05:42:04,777 DEBUG TRAIN Batch 24/2500 loss 8.832284 loss_att 9.543455 loss_ctc 9.283303 loss_rnnt 8.482483 hw_loss 0.276432 lr 0.00035123 rank 2
2023-02-25 05:42:04,778 DEBUG TRAIN Batch 24/2500 loss 8.425579 loss_att 8.827714 loss_ctc 10.494184 loss_rnnt 7.903532 hw_loss 0.310886 lr 0.00035123 rank 6
2023-02-25 05:42:04,826 DEBUG TRAIN Batch 24/2500 loss 18.935970 loss_att 20.827465 loss_ctc 24.847076 loss_rnnt 17.618153 hw_loss 0.283817 lr 0.00035127 rank 4
2023-02-25 05:43:15,132 DEBUG TRAIN Batch 24/2600 loss 17.623150 loss_att 17.690973 loss_ctc 20.820793 loss_rnnt 17.071745 hw_loss 0.209041 lr 0.00035113 rank 7
2023-02-25 05:43:15,145 DEBUG TRAIN Batch 24/2600 loss 12.392480 loss_att 12.717521 loss_ctc 15.738534 loss_rnnt 11.751581 hw_loss 0.243279 lr 0.00035116 rank 0
2023-02-25 05:43:15,146 DEBUG TRAIN Batch 24/2600 loss 4.490363 loss_att 9.376008 loss_ctc 7.687782 loss_rnnt 2.999635 hw_loss 0.163644 lr 0.00035118 rank 4
2023-02-25 05:43:15,147 DEBUG TRAIN Batch 24/2600 loss 5.871232 loss_att 10.936367 loss_ctc 10.217781 loss_rnnt 4.195561 hw_loss 0.155819 lr 0.00035112 rank 5
2023-02-25 05:43:15,149 DEBUG TRAIN Batch 24/2600 loss 6.243413 loss_att 6.216955 loss_ctc 7.436021 loss_rnnt 5.943358 hw_loss 0.274373 lr 0.00035118 rank 1
2023-02-25 05:43:15,150 DEBUG TRAIN Batch 24/2600 loss 11.013505 loss_att 11.714878 loss_ctc 19.199991 loss_rnnt 9.757210 hw_loss 0.045916 lr 0.00035115 rank 2
2023-02-25 05:43:15,152 DEBUG TRAIN Batch 24/2600 loss 11.357859 loss_att 16.477825 loss_ctc 20.772856 loss_rnnt 8.942508 hw_loss 0.255045 lr 0.00035114 rank 6
2023-02-25 05:43:15,200 DEBUG TRAIN Batch 24/2600 loss 5.269075 loss_att 7.750793 loss_ctc 5.124311 loss_rnnt 4.669899 hw_loss 0.229002 lr 0.00035120 rank 3
2023-02-25 05:44:25,022 DEBUG TRAIN Batch 24/2700 loss 13.436375 loss_att 15.744339 loss_ctc 22.612217 loss_rnnt 11.667053 hw_loss 0.158027 lr 0.00035107 rank 0
2023-02-25 05:44:25,024 DEBUG TRAIN Batch 24/2700 loss 8.869772 loss_att 11.560677 loss_ctc 13.671406 loss_rnnt 7.569671 hw_loss 0.228194 lr 0.00035111 rank 3
2023-02-25 05:44:25,025 DEBUG TRAIN Batch 24/2700 loss 7.893293 loss_att 10.022538 loss_ctc 13.221186 loss_rnnt 6.660035 hw_loss 0.181916 lr 0.00035109 rank 1
2023-02-25 05:44:25,029 DEBUG TRAIN Batch 24/2700 loss 9.139094 loss_att 11.446930 loss_ctc 13.002960 loss_rnnt 8.053521 hw_loss 0.204045 lr 0.00035104 rank 7
2023-02-25 05:44:25,031 DEBUG TRAIN Batch 24/2700 loss 9.157334 loss_att 10.767828 loss_ctc 12.206927 loss_rnnt 8.343674 hw_loss 0.159279 lr 0.00035106 rank 6
2023-02-25 05:44:25,032 DEBUG TRAIN Batch 24/2700 loss 5.359211 loss_att 7.370981 loss_ctc 7.466212 loss_rnnt 4.586218 hw_loss 0.168198 lr 0.00035106 rank 2
2023-02-25 05:44:25,037 DEBUG TRAIN Batch 24/2700 loss 14.108652 loss_att 16.607836 loss_ctc 17.877636 loss_rnnt 13.004347 hw_loss 0.191133 lr 0.00035103 rank 5
2023-02-25 05:44:25,087 DEBUG TRAIN Batch 24/2700 loss 5.001642 loss_att 7.200589 loss_ctc 6.607696 loss_rnnt 4.276160 hw_loss 0.134161 lr 0.00035109 rank 4
2023-02-25 05:45:37,327 DEBUG TRAIN Batch 24/2800 loss 12.129193 loss_att 15.593128 loss_ctc 21.056654 loss_rnnt 10.158972 hw_loss 0.163324 lr 0.00035098 rank 0
2023-02-25 05:45:37,342 DEBUG TRAIN Batch 24/2800 loss 3.339063 loss_att 6.790148 loss_ctc 6.212546 loss_rnnt 2.206673 hw_loss 0.110703 lr 0.00035102 rank 3
2023-02-25 05:45:37,345 DEBUG TRAIN Batch 24/2800 loss 8.025670 loss_att 9.888382 loss_ctc 8.207159 loss_rnnt 7.518536 hw_loss 0.206987 lr 0.00035101 rank 4
2023-02-25 05:45:37,346 DEBUG TRAIN Batch 24/2800 loss 5.131706 loss_att 7.190917 loss_ctc 6.105160 loss_rnnt 4.451056 hw_loss 0.260653 lr 0.00035095 rank 7
2023-02-25 05:45:37,348 DEBUG TRAIN Batch 24/2800 loss 5.445668 loss_att 8.420257 loss_ctc 10.507738 loss_rnnt 4.053979 hw_loss 0.228428 lr 0.00035100 rank 1
2023-02-25 05:45:37,347 DEBUG TRAIN Batch 24/2800 loss 7.709036 loss_att 11.405310 loss_ctc 12.576668 loss_rnnt 6.207490 hw_loss 0.212388 lr 0.00035094 rank 5
2023-02-25 05:45:37,349 DEBUG TRAIN Batch 24/2800 loss 11.503758 loss_att 16.680117 loss_ctc 17.186800 loss_rnnt 9.579422 hw_loss 0.246236 lr 0.00035097 rank 6
2023-02-25 05:45:37,349 DEBUG TRAIN Batch 24/2800 loss 5.935184 loss_att 8.485960 loss_ctc 10.962379 loss_rnnt 4.644815 hw_loss 0.206101 lr 0.00035097 rank 2
2023-02-25 05:46:49,237 DEBUG TRAIN Batch 24/2900 loss 6.567464 loss_att 8.592093 loss_ctc 13.596413 loss_rnnt 5.086165 hw_loss 0.260962 lr 0.00035090 rank 0
2023-02-25 05:46:49,244 DEBUG TRAIN Batch 24/2900 loss 9.400297 loss_att 14.766581 loss_ctc 13.115072 loss_rnnt 7.707412 hw_loss 0.233112 lr 0.00035092 rank 1
2023-02-25 05:46:49,244 DEBUG TRAIN Batch 24/2900 loss 14.304309 loss_att 18.137310 loss_ctc 25.333862 loss_rnnt 11.943726 hw_loss 0.231330 lr 0.00035089 rank 2
2023-02-25 05:46:49,245 DEBUG TRAIN Batch 24/2900 loss 6.851971 loss_att 10.556662 loss_ctc 12.394902 loss_rnnt 5.268207 hw_loss 0.194566 lr 0.00035087 rank 7
2023-02-25 05:46:49,246 DEBUG TRAIN Batch 24/2900 loss 11.204588 loss_att 13.266940 loss_ctc 13.975947 loss_rnnt 10.304141 hw_loss 0.222118 lr 0.00035094 rank 3
2023-02-25 05:46:49,248 DEBUG TRAIN Batch 24/2900 loss 4.711921 loss_att 7.424144 loss_ctc 5.558486 loss_rnnt 3.934675 hw_loss 0.228611 lr 0.00035092 rank 4
2023-02-25 05:46:49,250 DEBUG TRAIN Batch 24/2900 loss 5.910914 loss_att 7.491617 loss_ctc 7.302747 loss_rnnt 5.327336 hw_loss 0.153486 lr 0.00035089 rank 6
2023-02-25 05:46:49,252 DEBUG TRAIN Batch 24/2900 loss 5.613053 loss_att 9.920329 loss_ctc 8.645285 loss_rnnt 4.220618 hw_loss 0.237529 lr 0.00035086 rank 5
2023-02-25 05:47:59,480 DEBUG TRAIN Batch 24/3000 loss 12.548233 loss_att 15.491734 loss_ctc 15.447014 loss_rnnt 11.503528 hw_loss 0.130315 lr 0.00035084 rank 4
2023-02-25 05:47:59,481 DEBUG TRAIN Batch 24/3000 loss 6.650143 loss_att 10.712659 loss_ctc 7.755026 loss_rnnt 5.589250 hw_loss 0.189509 lr 0.00035085 rank 3
2023-02-25 05:47:59,482 DEBUG TRAIN Batch 24/3000 loss 7.500165 loss_att 8.653867 loss_ctc 9.473091 loss_rnnt 6.929969 hw_loss 0.143248 lr 0.00035078 rank 7
2023-02-25 05:47:59,483 DEBUG TRAIN Batch 24/3000 loss 7.987003 loss_att 11.447689 loss_ctc 13.567371 loss_rnnt 6.428700 hw_loss 0.228967 lr 0.00035077 rank 5
2023-02-25 05:47:59,486 DEBUG TRAIN Batch 24/3000 loss 15.798877 loss_att 17.843407 loss_ctc 23.510015 loss_rnnt 14.281860 hw_loss 0.149922 lr 0.00035083 rank 1
2023-02-25 05:47:59,485 DEBUG TRAIN Batch 24/3000 loss 13.498548 loss_att 14.332798 loss_ctc 14.299962 loss_rnnt 13.138499 hw_loss 0.161893 lr 0.00035081 rank 0
2023-02-25 05:47:59,485 DEBUG TRAIN Batch 24/3000 loss 12.193431 loss_att 13.144056 loss_ctc 13.436352 loss_rnnt 11.759676 hw_loss 0.146074 lr 0.00035080 rank 6
2023-02-25 05:47:59,487 DEBUG TRAIN Batch 24/3000 loss 11.640319 loss_att 16.125179 loss_ctc 24.941616 loss_rnnt 8.858144 hw_loss 0.209430 lr 0.00035080 rank 2
2023-02-25 05:49:09,503 DEBUG TRAIN Batch 24/3100 loss 7.017737 loss_att 8.086079 loss_ctc 9.972200 loss_rnnt 6.324905 hw_loss 0.159816 lr 0.00035071 rank 6
2023-02-25 05:49:09,507 DEBUG TRAIN Batch 24/3100 loss 8.083147 loss_att 10.571419 loss_ctc 7.656662 loss_rnnt 7.533333 hw_loss 0.204420 lr 0.00035076 rank 3
2023-02-25 05:49:09,508 DEBUG TRAIN Batch 24/3100 loss 11.762288 loss_att 14.052623 loss_ctc 16.530674 loss_rnnt 10.554790 hw_loss 0.213086 lr 0.00035072 rank 0
2023-02-25 05:49:09,508 DEBUG TRAIN Batch 24/3100 loss 9.595077 loss_att 11.690178 loss_ctc 16.739811 loss_rnnt 8.098064 hw_loss 0.235049 lr 0.00035075 rank 4
2023-02-25 05:49:09,509 DEBUG TRAIN Batch 24/3100 loss 8.468173 loss_att 9.393110 loss_ctc 12.122233 loss_rnnt 7.623799 hw_loss 0.322836 lr 0.00035070 rank 7
2023-02-25 05:49:09,510 DEBUG TRAIN Batch 24/3100 loss 7.929654 loss_att 10.866722 loss_ctc 10.451805 loss_rnnt 6.917953 hw_loss 0.165003 lr 0.00035074 rank 1
2023-02-25 05:49:09,513 DEBUG TRAIN Batch 24/3100 loss 8.066920 loss_att 10.701055 loss_ctc 13.232780 loss_rnnt 6.700402 hw_loss 0.282956 lr 0.00035068 rank 5
2023-02-25 05:49:09,512 DEBUG TRAIN Batch 24/3100 loss 7.881524 loss_att 8.532841 loss_ctc 9.509906 loss_rnnt 7.394159 hw_loss 0.262471 lr 0.00035071 rank 2
2023-02-25 05:50:23,247 DEBUG TRAIN Batch 24/3200 loss 3.749558 loss_att 5.457958 loss_ctc 5.031336 loss_rnnt 3.083300 hw_loss 0.288139 lr 0.00035064 rank 0
2023-02-25 05:50:23,248 DEBUG TRAIN Batch 24/3200 loss 9.794899 loss_att 10.129017 loss_ctc 12.799838 loss_rnnt 9.116013 hw_loss 0.396384 lr 0.00035066 rank 4
2023-02-25 05:50:23,250 DEBUG TRAIN Batch 24/3200 loss 5.284585 loss_att 10.144257 loss_ctc 8.635592 loss_rnnt 3.753045 hw_loss 0.211508 lr 0.00035066 rank 1
2023-02-25 05:50:23,251 DEBUG TRAIN Batch 24/3200 loss 20.084616 loss_att 19.816757 loss_ctc 25.397530 loss_rnnt 19.261927 hw_loss 0.314759 lr 0.00035063 rank 2
2023-02-25 05:50:23,253 DEBUG TRAIN Batch 24/3200 loss 5.241536 loss_att 8.114735 loss_ctc 8.146738 loss_rnnt 4.207581 hw_loss 0.134915 lr 0.00035061 rank 7
2023-02-25 05:50:23,261 DEBUG TRAIN Batch 24/3200 loss 6.059809 loss_att 10.530599 loss_ctc 8.251906 loss_rnnt 4.789522 hw_loss 0.157219 lr 0.00035063 rank 6
2023-02-25 05:50:23,272 DEBUG TRAIN Batch 24/3200 loss 4.903054 loss_att 9.645508 loss_ctc 8.561194 loss_rnnt 3.401015 hw_loss 0.123367 lr 0.00035068 rank 3
2023-02-25 05:50:23,309 DEBUG TRAIN Batch 24/3200 loss 14.469477 loss_att 15.877644 loss_ctc 14.996974 loss_rnnt 13.971725 hw_loss 0.273347 lr 0.00035060 rank 5
2023-02-25 05:51:33,910 DEBUG TRAIN Batch 24/3300 loss 8.077891 loss_att 10.714256 loss_ctc 11.037071 loss_rnnt 6.997810 hw_loss 0.296719 lr 0.00035055 rank 0
2023-02-25 05:51:33,917 DEBUG TRAIN Batch 24/3300 loss 6.582013 loss_att 9.757881 loss_ctc 7.010040 loss_rnnt 5.748052 hw_loss 0.265719 lr 0.00035054 rank 6
2023-02-25 05:51:33,916 DEBUG TRAIN Batch 24/3300 loss 4.177562 loss_att 8.467347 loss_ctc 9.216039 loss_rnnt 2.524919 hw_loss 0.230416 lr 0.00035058 rank 4
2023-02-25 05:51:33,917 DEBUG TRAIN Batch 24/3300 loss 9.293394 loss_att 11.345735 loss_ctc 10.138214 loss_rnnt 8.662919 hw_loss 0.201307 lr 0.00035052 rank 7
2023-02-25 05:51:33,918 DEBUG TRAIN Batch 24/3300 loss 4.153955 loss_att 6.826782 loss_ctc 6.025014 loss_rnnt 3.309107 hw_loss 0.114016 lr 0.00035059 rank 3
2023-02-25 05:51:33,919 DEBUG TRAIN Batch 24/3300 loss 10.080569 loss_att 9.813540 loss_ctc 12.075497 loss_rnnt 9.714949 hw_loss 0.286943 lr 0.00035057 rank 1
2023-02-25 05:51:33,919 DEBUG TRAIN Batch 24/3300 loss 5.913525 loss_att 9.491751 loss_ctc 8.055210 loss_rnnt 4.787241 hw_loss 0.234525 lr 0.00035054 rank 2
2023-02-25 05:51:33,923 DEBUG TRAIN Batch 24/3300 loss 3.483731 loss_att 6.656224 loss_ctc 4.160038 loss_rnnt 2.659322 hw_loss 0.187006 lr 0.00035051 rank 5
2023-02-25 05:52:44,359 DEBUG TRAIN Batch 24/3400 loss 4.125457 loss_att 7.782037 loss_ctc 8.206623 loss_rnnt 2.795356 hw_loss 0.102432 lr 0.00035047 rank 0
2023-02-25 05:52:44,361 DEBUG TRAIN Batch 24/3400 loss 4.997437 loss_att 7.920495 loss_ctc 10.460971 loss_rnnt 3.597993 hw_loss 0.161929 lr 0.00035049 rank 4
2023-02-25 05:52:44,362 DEBUG TRAIN Batch 24/3400 loss 7.697870 loss_att 10.022331 loss_ctc 10.230808 loss_rnnt 6.782218 hw_loss 0.211941 lr 0.00035049 rank 1
2023-02-25 05:52:44,364 DEBUG TRAIN Batch 24/3400 loss 5.470887 loss_att 8.778181 loss_ctc 7.098480 loss_rnnt 4.470710 hw_loss 0.228198 lr 0.00035045 rank 6
2023-02-25 05:52:44,366 DEBUG TRAIN Batch 24/3400 loss 7.860670 loss_att 9.916390 loss_ctc 12.546805 loss_rnnt 6.715040 hw_loss 0.205626 lr 0.00035044 rank 7
2023-02-25 05:52:44,367 DEBUG TRAIN Batch 24/3400 loss 18.553911 loss_att 19.062918 loss_ctc 27.707737 loss_rnnt 17.170979 hw_loss 0.113664 lr 0.00035051 rank 3
2023-02-25 05:52:44,370 DEBUG TRAIN Batch 24/3400 loss 5.309820 loss_att 10.575291 loss_ctc 7.630781 loss_rnnt 3.837710 hw_loss 0.205415 lr 0.00035043 rank 5
2023-02-25 05:52:44,370 DEBUG TRAIN Batch 24/3400 loss 3.485359 loss_att 5.410680 loss_ctc 5.389264 loss_rnnt 2.720202 hw_loss 0.236699 lr 0.00035046 rank 2
2023-02-25 05:53:56,330 DEBUG TRAIN Batch 24/3500 loss 5.684301 loss_att 7.885488 loss_ctc 8.204386 loss_rnnt 4.790534 hw_loss 0.220346 lr 0.00035038 rank 0
2023-02-25 05:53:56,331 DEBUG TRAIN Batch 24/3500 loss 10.667157 loss_att 13.667154 loss_ctc 13.160362 loss_rnnt 9.621752 hw_loss 0.211833 lr 0.00035042 rank 3
2023-02-25 05:53:56,332 DEBUG TRAIN Batch 24/3500 loss 14.050270 loss_att 18.815485 loss_ctc 25.259068 loss_rnnt 11.492537 hw_loss 0.206597 lr 0.00035040 rank 1
2023-02-25 05:53:56,335 DEBUG TRAIN Batch 24/3500 loss 12.256479 loss_att 15.301628 loss_ctc 22.534740 loss_rnnt 10.144526 hw_loss 0.248419 lr 0.00035035 rank 7
2023-02-25 05:53:56,337 DEBUG TRAIN Batch 24/3500 loss 16.203840 loss_att 19.328775 loss_ctc 21.163265 loss_rnnt 14.778877 hw_loss 0.260101 lr 0.00035037 rank 6
2023-02-25 05:53:56,336 DEBUG TRAIN Batch 24/3500 loss 9.334219 loss_att 11.398312 loss_ctc 11.343019 loss_rnnt 8.580185 hw_loss 0.137576 lr 0.00035037 rank 2
2023-02-25 05:53:56,338 DEBUG TRAIN Batch 24/3500 loss 5.255581 loss_att 8.963744 loss_ctc 7.837315 loss_rnnt 4.064145 hw_loss 0.197947 lr 0.00035040 rank 4
2023-02-25 05:53:56,338 DEBUG TRAIN Batch 24/3500 loss 6.627274 loss_att 9.677163 loss_ctc 8.657349 loss_rnnt 5.634744 hw_loss 0.209766 lr 0.00035034 rank 5
2023-02-25 05:55:08,624 DEBUG TRAIN Batch 24/3600 loss 8.267203 loss_att 10.899818 loss_ctc 13.063669 loss_rnnt 7.018740 hw_loss 0.154522 lr 0.00035028 rank 2
2023-02-25 05:55:08,645 DEBUG TRAIN Batch 24/3600 loss 8.895097 loss_att 12.286267 loss_ctc 13.161094 loss_rnnt 7.534133 hw_loss 0.213619 lr 0.00035031 rank 1
2023-02-25 05:55:08,646 DEBUG TRAIN Batch 24/3600 loss 3.396573 loss_att 6.006148 loss_ctc 4.565275 loss_rnnt 2.619765 hw_loss 0.185750 lr 0.00035029 rank 0
2023-02-25 05:55:08,648 DEBUG TRAIN Batch 24/3600 loss 7.973940 loss_att 10.511208 loss_ctc 12.903152 loss_rnnt 6.766701 hw_loss 0.079795 lr 0.00035028 rank 6
2023-02-25 05:55:08,649 DEBUG TRAIN Batch 24/3600 loss 11.519219 loss_att 15.796782 loss_ctc 21.244820 loss_rnnt 9.287579 hw_loss 0.148840 lr 0.00035033 rank 3
2023-02-25 05:55:08,651 DEBUG TRAIN Batch 24/3600 loss 11.203782 loss_att 13.968374 loss_ctc 18.589710 loss_rnnt 9.574675 hw_loss 0.171372 lr 0.00035025 rank 5
2023-02-25 05:55:08,660 DEBUG TRAIN Batch 24/3600 loss 4.197231 loss_att 5.560485 loss_ctc 3.753996 loss_rnnt 3.834116 hw_loss 0.280432 lr 0.00035026 rank 7
2023-02-25 05:55:08,694 DEBUG TRAIN Batch 24/3600 loss 8.256536 loss_att 9.327612 loss_ctc 9.644356 loss_rnnt 7.758547 hw_loss 0.185120 lr 0.00035032 rank 4
2023-02-25 05:56:19,478 DEBUG TRAIN Batch 24/3700 loss 5.448321 loss_att 8.283571 loss_ctc 8.168941 loss_rnnt 4.380304 hw_loss 0.259159 lr 0.00035020 rank 6
2023-02-25 05:56:19,479 DEBUG TRAIN Batch 24/3700 loss 10.422308 loss_att 12.721077 loss_ctc 12.369259 loss_rnnt 9.562619 hw_loss 0.263139 lr 0.00035020 rank 2
2023-02-25 05:56:19,480 DEBUG TRAIN Batch 24/3700 loss 15.878698 loss_att 16.446884 loss_ctc 23.276255 loss_rnnt 14.642722 hw_loss 0.254999 lr 0.00035025 rank 3
2023-02-25 05:56:19,483 DEBUG TRAIN Batch 24/3700 loss 12.126985 loss_att 14.250380 loss_ctc 16.392481 loss_rnnt 11.025834 hw_loss 0.202011 lr 0.00035021 rank 0
2023-02-25 05:56:19,484 DEBUG TRAIN Batch 24/3700 loss 11.432517 loss_att 15.320269 loss_ctc 20.773098 loss_rnnt 9.327584 hw_loss 0.153696 lr 0.00035017 rank 5
2023-02-25 05:56:19,484 DEBUG TRAIN Batch 24/3700 loss 5.822495 loss_att 8.487453 loss_ctc 8.520102 loss_rnnt 4.838233 hw_loss 0.171732 lr 0.00035018 rank 7
2023-02-25 05:56:19,485 DEBUG TRAIN Batch 24/3700 loss 9.815128 loss_att 10.171719 loss_ctc 14.195671 loss_rnnt 9.071348 hw_loss 0.165732 lr 0.00035023 rank 4
2023-02-25 05:56:19,488 DEBUG TRAIN Batch 24/3700 loss 6.397256 loss_att 11.456972 loss_ctc 12.776204 loss_rnnt 4.419452 hw_loss 0.216253 lr 0.00035023 rank 1
2023-02-25 05:57:29,643 DEBUG TRAIN Batch 24/3800 loss 10.403002 loss_att 12.508070 loss_ctc 14.025378 loss_rnnt 9.375969 hw_loss 0.230692 lr 0.00035012 rank 0
2023-02-25 05:57:29,644 DEBUG TRAIN Batch 24/3800 loss 8.673953 loss_att 11.230244 loss_ctc 10.265584 loss_rnnt 7.834614 hw_loss 0.217241 lr 0.00035014 rank 1
2023-02-25 05:57:29,649 DEBUG TRAIN Batch 24/3800 loss 5.724200 loss_att 9.094935 loss_ctc 11.058597 loss_rnnt 4.240746 hw_loss 0.183851 lr 0.00035008 rank 5
2023-02-25 05:57:29,649 DEBUG TRAIN Batch 24/3800 loss 10.418107 loss_att 10.838715 loss_ctc 14.487215 loss_rnnt 9.630078 hw_loss 0.302546 lr 0.00035011 rank 6
2023-02-25 05:57:29,649 DEBUG TRAIN Batch 24/3800 loss 10.771411 loss_att 11.565761 loss_ctc 14.468599 loss_rnnt 9.985605 hw_loss 0.251208 lr 0.00035011 rank 2
2023-02-25 05:57:29,650 DEBUG TRAIN Batch 24/3800 loss 10.827838 loss_att 10.459226 loss_ctc 13.760350 loss_rnnt 10.379691 hw_loss 0.245378 lr 0.00035016 rank 3
2023-02-25 05:57:29,651 DEBUG TRAIN Batch 24/3800 loss 7.567277 loss_att 12.966320 loss_ctc 15.420216 loss_rnnt 5.321301 hw_loss 0.223329 lr 0.00035009 rank 7
2023-02-25 05:57:29,651 DEBUG TRAIN Batch 24/3800 loss 5.335500 loss_att 7.574063 loss_ctc 7.940988 loss_rnnt 4.407444 hw_loss 0.249271 lr 0.00035015 rank 4
2023-02-25 05:58:42,960 DEBUG TRAIN Batch 24/3900 loss 8.895424 loss_att 12.365412 loss_ctc 12.517474 loss_rnnt 7.610796 hw_loss 0.201918 lr 0.00035004 rank 0
2023-02-25 05:58:42,962 DEBUG TRAIN Batch 24/3900 loss 10.400128 loss_att 11.923173 loss_ctc 16.058712 loss_rnnt 9.170809 hw_loss 0.319186 lr 0.00035006 rank 1
2023-02-25 05:58:42,962 DEBUG TRAIN Batch 24/3900 loss 9.392731 loss_att 11.267426 loss_ctc 11.114203 loss_rnnt 8.650335 hw_loss 0.258612 lr 0.00035006 rank 4
2023-02-25 05:58:42,967 DEBUG TRAIN Batch 24/3900 loss 12.077237 loss_att 15.672531 loss_ctc 15.573729 loss_rnnt 10.771284 hw_loss 0.226303 lr 0.00035002 rank 6
2023-02-25 05:58:42,968 DEBUG TRAIN Batch 24/3900 loss 5.497869 loss_att 8.620851 loss_ctc 7.288742 loss_rnnt 4.544733 hw_loss 0.168293 lr 0.00035001 rank 7
2023-02-25 05:58:42,973 DEBUG TRAIN Batch 24/3900 loss 6.264888 loss_att 11.042822 loss_ctc 8.653963 loss_rnnt 4.826846 hw_loss 0.307334 lr 0.00035008 rank 3
2023-02-25 05:58:42,995 DEBUG TRAIN Batch 24/3900 loss 3.374167 loss_att 5.539820 loss_ctc 5.548856 loss_rnnt 2.529238 hw_loss 0.228450 lr 0.00035003 rank 2
2023-02-25 05:58:43,001 DEBUG TRAIN Batch 24/3900 loss 7.893915 loss_att 7.494195 loss_ctc 10.254303 loss_rnnt 7.551138 hw_loss 0.202504 lr 0.00035000 rank 5
2023-02-25 05:59:53,407 DEBUG TRAIN Batch 24/4000 loss 2.532892 loss_att 5.590203 loss_ctc 2.665317 loss_rnnt 1.866443 hw_loss 0.069994 lr 0.00034997 rank 1
2023-02-25 05:59:53,408 DEBUG TRAIN Batch 24/4000 loss 12.965185 loss_att 15.815855 loss_ctc 13.346209 loss_rnnt 12.255923 hw_loss 0.165609 lr 0.00034994 rank 6
2023-02-25 05:59:53,408 DEBUG TRAIN Batch 24/4000 loss 7.440308 loss_att 13.808065 loss_ctc 10.239552 loss_rnnt 5.700111 hw_loss 0.175147 lr 0.00034995 rank 0
2023-02-25 05:59:53,410 DEBUG TRAIN Batch 24/4000 loss 4.553119 loss_att 7.838565 loss_ctc 6.902190 loss_rnnt 3.507973 hw_loss 0.140339 lr 0.00034992 rank 7
2023-02-25 05:59:53,410 DEBUG TRAIN Batch 24/4000 loss 10.876187 loss_att 9.962992 loss_ctc 12.671432 loss_rnnt 10.724968 hw_loss 0.177173 lr 0.00034994 rank 2
2023-02-25 05:59:53,414 DEBUG TRAIN Batch 24/4000 loss 5.738482 loss_att 9.610834 loss_ctc 10.987030 loss_rnnt 4.161205 hw_loss 0.193126 lr 0.00034997 rank 4
2023-02-25 05:59:53,414 DEBUG TRAIN Batch 24/4000 loss 9.608282 loss_att 12.329600 loss_ctc 13.593407 loss_rnnt 8.467937 hw_loss 0.121370 lr 0.00034999 rank 3
2023-02-25 05:59:53,420 DEBUG TRAIN Batch 24/4000 loss 7.457135 loss_att 9.783790 loss_ctc 10.519463 loss_rnnt 6.470645 hw_loss 0.211592 lr 0.00034991 rank 5
2023-02-25 06:01:03,999 DEBUG TRAIN Batch 24/4100 loss 5.667595 loss_att 9.049301 loss_ctc 7.946594 loss_rnnt 4.629098 hw_loss 0.109292 lr 0.00034987 rank 0
2023-02-25 06:01:04,002 DEBUG TRAIN Batch 24/4100 loss 3.790013 loss_att 4.501029 loss_ctc 2.198904 loss_rnnt 3.775710 hw_loss 0.157965 lr 0.00034988 rank 1
2023-02-25 06:01:04,006 DEBUG TRAIN Batch 24/4100 loss 1.285270 loss_att 4.290582 loss_ctc 2.883942 loss_rnnt 0.363210 hw_loss 0.202203 lr 0.00034984 rank 7
2023-02-25 06:01:04,007 DEBUG TRAIN Batch 24/4100 loss 4.242692 loss_att 8.820431 loss_ctc 5.240952 loss_rnnt 3.171512 hw_loss 0.042245 lr 0.00034982 rank 5
2023-02-25 06:01:04,008 DEBUG TRAIN Batch 24/4100 loss 5.682621 loss_att 8.628748 loss_ctc 9.881681 loss_rnnt 4.470890 hw_loss 0.117433 lr 0.00034989 rank 4
2023-02-25 06:01:04,009 DEBUG TRAIN Batch 24/4100 loss 15.334164 loss_att 15.970451 loss_ctc 22.254889 loss_rnnt 14.179989 hw_loss 0.195289 lr 0.00034985 rank 2
2023-02-25 06:01:04,033 DEBUG TRAIN Batch 24/4100 loss 13.690046 loss_att 18.526556 loss_ctc 23.515949 loss_rnnt 11.257912 hw_loss 0.290084 lr 0.00034990 rank 3
2023-02-25 06:01:04,037 DEBUG TRAIN Batch 24/4100 loss 6.473154 loss_att 7.905553 loss_ctc 7.364038 loss_rnnt 5.988923 hw_loss 0.148062 lr 0.00034985 rank 6
2023-02-25 06:02:14,868 DEBUG TRAIN Batch 24/4200 loss 6.135091 loss_att 8.844351 loss_ctc 13.167148 loss_rnnt 4.576823 hw_loss 0.147765 lr 0.00034977 rank 6
2023-02-25 06:02:14,868 DEBUG TRAIN Batch 24/4200 loss 7.069287 loss_att 9.786682 loss_ctc 11.811167 loss_rnnt 5.789011 hw_loss 0.196024 lr 0.00034982 rank 3
2023-02-25 06:02:14,876 DEBUG TRAIN Batch 24/4200 loss 4.664559 loss_att 7.826562 loss_ctc 6.502017 loss_rnnt 3.685045 hw_loss 0.191473 lr 0.00034980 rank 1
2023-02-25 06:02:14,880 DEBUG TRAIN Batch 24/4200 loss 5.637215 loss_att 7.347709 loss_ctc 7.352822 loss_rnnt 4.983530 hw_loss 0.155321 lr 0.00034974 rank 5
2023-02-25 06:02:14,880 DEBUG TRAIN Batch 24/4200 loss 11.963151 loss_att 12.933064 loss_ctc 14.013504 loss_rnnt 11.397070 hw_loss 0.185098 lr 0.00034977 rank 2
2023-02-25 06:02:14,881 DEBUG TRAIN Batch 24/4200 loss 9.437174 loss_att 14.024055 loss_ctc 16.654676 loss_rnnt 7.495212 hw_loss 0.116722 lr 0.00034978 rank 0
2023-02-25 06:02:14,884 DEBUG TRAIN Batch 24/4200 loss 5.464062 loss_att 7.293887 loss_ctc 6.739145 loss_rnnt 4.807783 hw_loss 0.225568 lr 0.00034975 rank 7
2023-02-25 06:02:14,896 DEBUG TRAIN Batch 24/4200 loss 8.903222 loss_att 11.093494 loss_ctc 11.652946 loss_rnnt 8.007224 hw_loss 0.171212 lr 0.00034980 rank 4
2023-02-25 06:03:27,815 DEBUG TRAIN Batch 24/4300 loss 9.345977 loss_att 9.261966 loss_ctc 10.428174 loss_rnnt 9.091270 hw_loss 0.238528 lr 0.00034966 rank 7
2023-02-25 06:03:27,816 DEBUG TRAIN Batch 24/4300 loss 6.778175 loss_att 10.364926 loss_ctc 9.334681 loss_rnnt 5.614442 hw_loss 0.197842 lr 0.00034972 rank 4
2023-02-25 06:03:27,816 DEBUG TRAIN Batch 24/4300 loss 19.483994 loss_att 24.743233 loss_ctc 33.458485 loss_rnnt 16.429100 hw_loss 0.262089 lr 0.00034973 rank 3
2023-02-25 06:03:27,816 DEBUG TRAIN Batch 24/4300 loss 5.850412 loss_att 8.558515 loss_ctc 6.036829 loss_rnnt 5.190733 hw_loss 0.174755 lr 0.00034968 rank 6
2023-02-25 06:03:27,820 DEBUG TRAIN Batch 24/4300 loss 16.390656 loss_att 17.577026 loss_ctc 20.735334 loss_rnnt 15.449003 hw_loss 0.234538 lr 0.00034969 rank 0
2023-02-25 06:03:27,822 DEBUG TRAIN Batch 24/4300 loss 6.876147 loss_att 10.157423 loss_ctc 7.839006 loss_rnnt 6.013969 hw_loss 0.145389 lr 0.00034971 rank 1
2023-02-25 06:03:27,825 DEBUG TRAIN Batch 24/4300 loss 8.136738 loss_att 11.238952 loss_ctc 11.895897 loss_rnnt 6.978420 hw_loss 0.068727 lr 0.00034968 rank 2
2023-02-25 06:03:27,876 DEBUG TRAIN Batch 24/4300 loss 1.977952 loss_att 5.415707 loss_ctc 3.161535 loss_rnnt 1.009354 hw_loss 0.231069 lr 0.00034965 rank 5
2023-02-25 06:04:39,296 DEBUG TRAIN Batch 24/4400 loss 9.298850 loss_att 9.234443 loss_ctc 10.721856 loss_rnnt 8.953499 hw_loss 0.315936 lr 0.00034965 rank 3
2023-02-25 06:04:39,298 DEBUG TRAIN Batch 24/4400 loss 7.204292 loss_att 8.172602 loss_ctc 11.635473 loss_rnnt 6.305149 hw_loss 0.214982 lr 0.00034960 rank 6
2023-02-25 06:04:39,297 DEBUG TRAIN Batch 24/4400 loss 10.922500 loss_att 12.855883 loss_ctc 15.299914 loss_rnnt 9.866454 hw_loss 0.160714 lr 0.00034961 rank 0
2023-02-25 06:04:39,300 DEBUG TRAIN Batch 24/4400 loss 7.172766 loss_att 8.230553 loss_ctc 9.197331 loss_rnnt 6.531295 hw_loss 0.299948 lr 0.00034958 rank 7
2023-02-25 06:04:39,302 DEBUG TRAIN Batch 24/4400 loss 7.634472 loss_att 9.881270 loss_ctc 10.884954 loss_rnnt 6.595808 hw_loss 0.292327 lr 0.00034960 rank 2
2023-02-25 06:04:39,303 DEBUG TRAIN Batch 24/4400 loss 7.383815 loss_att 9.596167 loss_ctc 9.856513 loss_rnnt 6.540806 hw_loss 0.132835 lr 0.00034963 rank 1
2023-02-25 06:04:39,305 DEBUG TRAIN Batch 24/4400 loss 8.333947 loss_att 11.128098 loss_ctc 13.623789 loss_rnnt 6.979553 hw_loss 0.169223 lr 0.00034957 rank 5
2023-02-25 06:04:39,306 DEBUG TRAIN Batch 24/4400 loss 12.939740 loss_att 14.466644 loss_ctc 20.996485 loss_rnnt 11.419318 hw_loss 0.264017 lr 0.00034963 rank 4
2023-02-25 06:05:49,504 DEBUG TRAIN Batch 24/4500 loss 7.925260 loss_att 10.308135 loss_ctc 10.565207 loss_rnnt 7.013870 hw_loss 0.155291 lr 0.00034954 rank 1
2023-02-25 06:05:49,504 DEBUG TRAIN Batch 24/4500 loss 11.552003 loss_att 11.501118 loss_ctc 15.344563 loss_rnnt 10.867193 hw_loss 0.354960 lr 0.00034952 rank 0
2023-02-25 06:05:49,506 DEBUG TRAIN Batch 24/4500 loss 4.837633 loss_att 8.579668 loss_ctc 10.812469 loss_rnnt 3.171251 hw_loss 0.227494 lr 0.00034956 rank 3
2023-02-25 06:05:49,507 DEBUG TRAIN Batch 24/4500 loss 7.887970 loss_att 10.842179 loss_ctc 13.623702 loss_rnnt 6.446283 hw_loss 0.161402 lr 0.00034951 rank 6
2023-02-25 06:05:49,509 DEBUG TRAIN Batch 24/4500 loss 6.388827 loss_att 7.031994 loss_ctc 9.321200 loss_rnnt 5.713872 hw_loss 0.291261 lr 0.00034948 rank 5
2023-02-25 06:05:49,511 DEBUG TRAIN Batch 24/4500 loss 6.815442 loss_att 8.882614 loss_ctc 7.043857 loss_rnnt 6.263894 hw_loss 0.201858 lr 0.00034951 rank 2
2023-02-25 06:05:49,514 DEBUG TRAIN Batch 24/4500 loss 3.777552 loss_att 5.939716 loss_ctc 5.486293 loss_rnnt 2.943311 hw_loss 0.326206 lr 0.00034955 rank 4
2023-02-25 06:05:49,515 DEBUG TRAIN Batch 24/4500 loss 7.383747 loss_att 10.296945 loss_ctc 9.450201 loss_rnnt 6.390087 hw_loss 0.254048 lr 0.00034949 rank 7
2023-02-25 06:07:02,302 DEBUG TRAIN Batch 24/4600 loss 8.774146 loss_att 10.691487 loss_ctc 11.563017 loss_rnnt 7.918978 hw_loss 0.187218 lr 0.00034944 rank 0
2023-02-25 06:07:02,311 DEBUG TRAIN Batch 24/4600 loss 9.521772 loss_att 12.478190 loss_ctc 13.157214 loss_rnnt 8.381524 hw_loss 0.120446 lr 0.00034948 rank 3
2023-02-25 06:07:02,319 DEBUG TRAIN Batch 24/4600 loss 6.430783 loss_att 11.396379 loss_ctc 9.478121 loss_rnnt 4.986388 hw_loss 0.084308 lr 0.00034941 rank 7
2023-02-25 06:07:02,323 DEBUG TRAIN Batch 24/4600 loss 8.471330 loss_att 10.166067 loss_ctc 10.337365 loss_rnnt 7.802000 hw_loss 0.152958 lr 0.00034946 rank 4
2023-02-25 06:07:02,324 DEBUG TRAIN Batch 24/4600 loss 6.391596 loss_att 8.181713 loss_ctc 8.679679 loss_rnnt 5.625050 hw_loss 0.193960 lr 0.00034943 rank 2
2023-02-25 06:07:02,326 DEBUG TRAIN Batch 24/4600 loss 6.789968 loss_att 10.500675 loss_ctc 8.847022 loss_rnnt 5.682628 hw_loss 0.170485 lr 0.00034940 rank 5
2023-02-25 06:07:02,327 DEBUG TRAIN Batch 24/4600 loss 4.381464 loss_att 8.376604 loss_ctc 7.303874 loss_rnnt 3.087737 hw_loss 0.196959 lr 0.00034946 rank 1
2023-02-25 06:07:02,341 DEBUG TRAIN Batch 24/4600 loss 6.844126 loss_att 10.415594 loss_ctc 8.671356 loss_rnnt 5.805049 hw_loss 0.152161 lr 0.00034943 rank 6
2023-02-25 06:08:14,282 DEBUG TRAIN Batch 24/4700 loss 4.580248 loss_att 6.671769 loss_ctc 8.142889 loss_rnnt 3.586523 hw_loss 0.188252 lr 0.00034935 rank 0
2023-02-25 06:08:14,284 DEBUG TRAIN Batch 24/4700 loss 5.835558 loss_att 9.404140 loss_ctc 6.772218 loss_rnnt 4.904297 hw_loss 0.173730 lr 0.00034937 rank 1
2023-02-25 06:08:14,285 DEBUG TRAIN Batch 24/4700 loss 8.456193 loss_att 11.861863 loss_ctc 10.587873 loss_rnnt 7.375558 hw_loss 0.216144 lr 0.00034934 rank 6
2023-02-25 06:08:14,287 DEBUG TRAIN Batch 24/4700 loss 7.029655 loss_att 11.564382 loss_ctc 12.586384 loss_rnnt 5.256648 hw_loss 0.234683 lr 0.00034932 rank 7
2023-02-25 06:08:14,291 DEBUG TRAIN Batch 24/4700 loss 4.656626 loss_att 8.319656 loss_ctc 7.290732 loss_rnnt 3.451649 hw_loss 0.227169 lr 0.00034938 rank 4
2023-02-25 06:08:14,293 DEBUG TRAIN Batch 24/4700 loss 4.066764 loss_att 6.652073 loss_ctc 7.531257 loss_rnnt 2.979506 hw_loss 0.202994 lr 0.00034939 rank 3
2023-02-25 06:08:14,292 DEBUG TRAIN Batch 24/4700 loss 5.827845 loss_att 8.742255 loss_ctc 8.416411 loss_rnnt 4.834690 hw_loss 0.122120 lr 0.00034931 rank 5
2023-02-25 06:08:14,293 DEBUG TRAIN Batch 24/4700 loss 9.295847 loss_att 11.379749 loss_ctc 14.746952 loss_rnnt 8.032679 hw_loss 0.224202 lr 0.00034934 rank 2
2023-02-25 06:09:25,590 DEBUG TRAIN Batch 24/4800 loss 6.073543 loss_att 8.436772 loss_ctc 7.873311 loss_rnnt 5.301832 hw_loss 0.110804 lr 0.00034927 rank 0
2023-02-25 06:09:25,591 DEBUG TRAIN Batch 24/4800 loss 9.976710 loss_att 12.545954 loss_ctc 14.146360 loss_rnnt 8.793720 hw_loss 0.212229 lr 0.00034924 rank 7
2023-02-25 06:09:25,594 DEBUG TRAIN Batch 24/4800 loss 9.354097 loss_att 11.460041 loss_ctc 11.397064 loss_rnnt 8.524579 hw_loss 0.254875 lr 0.00034926 rank 2
2023-02-25 06:09:25,594 DEBUG TRAIN Batch 24/4800 loss 3.202963 loss_att 5.352702 loss_ctc 3.339012 loss_rnnt 2.653514 hw_loss 0.190053 lr 0.00034929 rank 1
2023-02-25 06:09:25,595 DEBUG TRAIN Batch 24/4800 loss 9.410207 loss_att 13.800207 loss_ctc 14.216558 loss_rnnt 7.741343 hw_loss 0.281281 lr 0.00034931 rank 3
2023-02-25 06:09:25,595 DEBUG TRAIN Batch 24/4800 loss 4.607108 loss_att 7.166847 loss_ctc 5.356399 loss_rnnt 3.919419 hw_loss 0.142191 lr 0.00034929 rank 4
2023-02-25 06:09:25,600 DEBUG TRAIN Batch 24/4800 loss 10.224476 loss_att 12.608415 loss_ctc 14.008844 loss_rnnt 9.108553 hw_loss 0.252287 lr 0.00034926 rank 6
2023-02-25 06:09:25,648 DEBUG TRAIN Batch 24/4800 loss 7.062924 loss_att 9.815015 loss_ctc 10.787521 loss_rnnt 5.873350 hw_loss 0.267267 lr 0.00034923 rank 5
2023-02-25 06:10:36,999 DEBUG TRAIN Batch 24/4900 loss 8.387756 loss_att 12.171680 loss_ctc 10.676268 loss_rnnt 7.215908 hw_loss 0.206118 lr 0.00034918 rank 0
2023-02-25 06:10:37,004 DEBUG TRAIN Batch 24/4900 loss 3.346734 loss_att 6.409639 loss_ctc 5.697022 loss_rnnt 2.330204 hw_loss 0.169832 lr 0.00034915 rank 7
2023-02-25 06:10:37,006 DEBUG TRAIN Batch 24/4900 loss 10.938956 loss_att 13.476825 loss_ctc 15.847881 loss_rnnt 9.648235 hw_loss 0.241170 lr 0.00034921 rank 4
2023-02-25 06:10:37,006 DEBUG TRAIN Batch 24/4900 loss 6.873632 loss_att 7.487782 loss_ctc 10.435850 loss_rnnt 6.122799 hw_loss 0.286954 lr 0.00034922 rank 3
2023-02-25 06:10:37,007 DEBUG TRAIN Batch 24/4900 loss 7.169570 loss_att 9.585610 loss_ctc 9.918414 loss_rnnt 6.212856 hw_loss 0.200614 lr 0.00034917 rank 6
2023-02-25 06:10:37,008 DEBUG TRAIN Batch 24/4900 loss 6.593214 loss_att 8.388854 loss_ctc 10.282965 loss_rnnt 5.641369 hw_loss 0.188906 lr 0.00034920 rank 1
2023-02-25 06:10:37,011 DEBUG TRAIN Batch 24/4900 loss 4.957420 loss_att 6.644896 loss_ctc 9.617058 loss_rnnt 3.870966 hw_loss 0.239389 lr 0.00034917 rank 2
2023-02-25 06:10:37,021 DEBUG TRAIN Batch 24/4900 loss 11.203169 loss_att 13.943233 loss_ctc 15.006943 loss_rnnt 10.074202 hw_loss 0.138346 lr 0.00034914 rank 5
2023-02-25 06:11:50,475 DEBUG TRAIN Batch 24/5000 loss 7.207129 loss_att 9.226984 loss_ctc 10.104639 loss_rnnt 6.297346 hw_loss 0.224020 lr 0.00034908 rank 6
2023-02-25 06:11:50,476 DEBUG TRAIN Batch 24/5000 loss 8.490790 loss_att 10.117601 loss_ctc 11.398471 loss_rnnt 7.712257 hw_loss 0.122774 lr 0.00034909 rank 2
2023-02-25 06:11:50,479 DEBUG TRAIN Batch 24/5000 loss 6.609168 loss_att 8.001884 loss_ctc 9.823131 loss_rnnt 5.799037 hw_loss 0.193235 lr 0.00034912 rank 1
2023-02-25 06:11:50,479 DEBUG TRAIN Batch 24/5000 loss 9.047821 loss_att 9.222754 loss_ctc 11.104190 loss_rnnt 8.572412 hw_loss 0.311698 lr 0.00034914 rank 3
2023-02-25 06:11:50,479 DEBUG TRAIN Batch 24/5000 loss 3.608430 loss_att 5.575350 loss_ctc 5.515746 loss_rnnt 2.791319 hw_loss 0.317660 lr 0.00034910 rank 0
2023-02-25 06:11:50,480 DEBUG TRAIN Batch 24/5000 loss 8.253853 loss_att 9.702340 loss_ctc 11.460564 loss_rnnt 7.387161 hw_loss 0.280187 lr 0.00034907 rank 7
2023-02-25 06:11:50,483 DEBUG TRAIN Batch 24/5000 loss 7.825387 loss_att 11.300884 loss_ctc 11.429512 loss_rnnt 6.539556 hw_loss 0.206590 lr 0.00034912 rank 4
2023-02-25 06:11:50,488 DEBUG TRAIN Batch 24/5000 loss 9.103424 loss_att 11.502705 loss_ctc 14.378153 loss_rnnt 7.813340 hw_loss 0.200493 lr 0.00034906 rank 5
2023-02-25 06:13:00,668 DEBUG TRAIN Batch 24/5100 loss 5.723538 loss_att 9.314665 loss_ctc 9.627374 loss_rnnt 4.381235 hw_loss 0.194187 lr 0.00034900 rank 6
2023-02-25 06:13:00,671 DEBUG TRAIN Batch 24/5100 loss 18.436022 loss_att 21.596287 loss_ctc 26.670395 loss_rnnt 16.598576 hw_loss 0.201515 lr 0.00034905 rank 3
2023-02-25 06:13:00,672 DEBUG TRAIN Batch 24/5100 loss 10.642212 loss_att 12.964313 loss_ctc 16.313042 loss_rnnt 9.288336 hw_loss 0.250023 lr 0.00034901 rank 0
2023-02-25 06:13:00,673 DEBUG TRAIN Batch 24/5100 loss 5.932254 loss_att 6.437253 loss_ctc 7.956797 loss_rnnt 5.449643 hw_loss 0.209386 lr 0.00034900 rank 2
2023-02-25 06:13:00,674 DEBUG TRAIN Batch 24/5100 loss 10.981202 loss_att 13.594419 loss_ctc 18.169228 loss_rnnt 9.334370 hw_loss 0.310847 lr 0.00034898 rank 7
2023-02-25 06:13:00,674 DEBUG TRAIN Batch 24/5100 loss 4.786727 loss_att 6.783347 loss_ctc 7.167739 loss_rnnt 3.965431 hw_loss 0.195944 lr 0.00034903 rank 1
2023-02-25 06:13:00,676 DEBUG TRAIN Batch 24/5100 loss 7.216412 loss_att 8.512774 loss_ctc 11.456860 loss_rnnt 6.231063 hw_loss 0.301282 lr 0.00034897 rank 5
2023-02-25 06:13:00,726 DEBUG TRAIN Batch 24/5100 loss 7.658498 loss_att 9.467633 loss_ctc 11.650105 loss_rnnt 6.718118 hw_loss 0.086884 lr 0.00034904 rank 4
2023-02-25 06:14:10,592 DEBUG TRAIN Batch 24/5200 loss 8.132336 loss_att 11.110524 loss_ctc 10.864136 loss_rnnt 7.087265 hw_loss 0.159739 lr 0.00034892 rank 2
2023-02-25 06:14:10,593 DEBUG TRAIN Batch 24/5200 loss 11.446950 loss_att 13.532703 loss_ctc 15.358836 loss_rnnt 10.384018 hw_loss 0.232868 lr 0.00034897 rank 3
2023-02-25 06:14:10,595 DEBUG TRAIN Batch 24/5200 loss 5.059032 loss_att 8.929504 loss_ctc 5.667872 loss_rnnt 4.109966 hw_loss 0.175863 lr 0.00034893 rank 0
2023-02-25 06:14:10,595 DEBUG TRAIN Batch 24/5200 loss 9.859750 loss_att 11.974700 loss_ctc 12.091388 loss_rnnt 9.074924 hw_loss 0.120534 lr 0.00034890 rank 7
2023-02-25 06:14:10,596 DEBUG TRAIN Batch 24/5200 loss 5.140485 loss_att 6.939871 loss_ctc 8.470551 loss_rnnt 4.214475 hw_loss 0.228982 lr 0.00034895 rank 1
2023-02-25 06:14:10,597 DEBUG TRAIN Batch 24/5200 loss 5.955100 loss_att 9.376975 loss_ctc 8.965157 loss_rnnt 4.756638 hw_loss 0.211398 lr 0.00034891 rank 6
2023-02-25 06:14:10,601 DEBUG TRAIN Batch 24/5200 loss 6.261364 loss_att 9.375484 loss_ctc 7.078826 loss_rnnt 5.393755 hw_loss 0.254606 lr 0.00034895 rank 4
2023-02-25 06:14:10,606 DEBUG TRAIN Batch 24/5200 loss 11.590817 loss_att 14.182697 loss_ctc 15.564960 loss_rnnt 10.449239 hw_loss 0.174969 lr 0.00034889 rank 5
2023-02-25 06:15:23,180 DEBUG TRAIN Batch 24/5300 loss 5.994442 loss_att 8.157108 loss_ctc 7.463951 loss_rnnt 5.251373 hw_loss 0.214877 lr 0.00034884 rank 0
2023-02-25 06:15:23,185 DEBUG TRAIN Batch 24/5300 loss 9.584901 loss_att 10.495083 loss_ctc 15.736613 loss_rnnt 8.491881 hw_loss 0.170167 lr 0.00034887 rank 4
2023-02-25 06:15:23,186 DEBUG TRAIN Batch 24/5300 loss 4.881527 loss_att 7.424087 loss_ctc 5.801735 loss_rnnt 4.140352 hw_loss 0.206192 lr 0.00034888 rank 3
2023-02-25 06:15:23,186 DEBUG TRAIN Batch 24/5300 loss 6.831381 loss_att 8.123660 loss_ctc 7.880170 loss_rnnt 6.341245 hw_loss 0.172204 lr 0.00034881 rank 7
2023-02-25 06:15:23,187 DEBUG TRAIN Batch 24/5300 loss 3.215331 loss_att 6.430100 loss_ctc 5.564526 loss_rnnt 2.186617 hw_loss 0.136000 lr 0.00034883 rank 2
2023-02-25 06:15:23,187 DEBUG TRAIN Batch 24/5300 loss 3.270712 loss_att 6.157171 loss_ctc 4.041971 loss_rnnt 2.481178 hw_loss 0.205140 lr 0.00034886 rank 1
2023-02-25 06:15:23,199 DEBUG TRAIN Batch 24/5300 loss 5.738570 loss_att 7.241956 loss_ctc 7.901544 loss_rnnt 5.064901 hw_loss 0.158614 lr 0.00034883 rank 6
2023-02-25 06:15:23,206 DEBUG TRAIN Batch 24/5300 loss 10.571706 loss_att 12.250891 loss_ctc 13.930880 loss_rnnt 9.713342 hw_loss 0.139942 lr 0.00034880 rank 5
2023-02-25 06:16:35,194 DEBUG TRAIN Batch 24/5400 loss 9.732234 loss_att 10.167347 loss_ctc 11.203762 loss_rnnt 9.322860 hw_loss 0.236527 lr 0.00034880 rank 3
2023-02-25 06:16:35,196 DEBUG TRAIN Batch 24/5400 loss 8.550714 loss_att 12.656225 loss_ctc 12.072512 loss_rnnt 7.132895 hw_loss 0.238391 lr 0.00034875 rank 6
2023-02-25 06:16:35,199 DEBUG TRAIN Batch 24/5400 loss 6.656868 loss_att 10.038823 loss_ctc 11.850449 loss_rnnt 5.230179 hw_loss 0.108412 lr 0.00034875 rank 2
2023-02-25 06:16:35,198 DEBUG TRAIN Batch 24/5400 loss 8.725120 loss_att 12.348705 loss_ctc 12.012241 loss_rnnt 7.481793 hw_loss 0.150612 lr 0.00034878 rank 1
2023-02-25 06:16:35,201 DEBUG TRAIN Batch 24/5400 loss 7.699363 loss_att 10.723124 loss_ctc 10.823133 loss_rnnt 6.576350 hw_loss 0.190796 lr 0.00034876 rank 0
2023-02-25 06:16:35,202 DEBUG TRAIN Batch 24/5400 loss 7.339136 loss_att 11.676707 loss_ctc 11.963521 loss_rnnt 5.769145 hw_loss 0.161045 lr 0.00034873 rank 7
2023-02-25 06:16:35,203 DEBUG TRAIN Batch 24/5400 loss 14.636774 loss_att 17.213001 loss_ctc 17.395712 loss_rnnt 13.612517 hw_loss 0.264661 lr 0.00034878 rank 4
2023-02-25 06:16:35,210 DEBUG TRAIN Batch 24/5400 loss 7.951914 loss_att 9.887615 loss_ctc 12.257399 loss_rnnt 6.877849 hw_loss 0.211612 lr 0.00034872 rank 5
2023-02-25 06:17:44,846 DEBUG TRAIN Batch 24/5500 loss 8.663784 loss_att 10.759978 loss_ctc 14.093122 loss_rnnt 7.438925 hw_loss 0.153202 lr 0.00034867 rank 0
2023-02-25 06:17:44,846 DEBUG TRAIN Batch 24/5500 loss 7.470873 loss_att 9.889441 loss_ctc 10.149557 loss_rnnt 6.539047 hw_loss 0.170538 lr 0.00034866 rank 2
2023-02-25 06:17:44,847 DEBUG TRAIN Batch 24/5500 loss 7.975293 loss_att 8.672665 loss_ctc 9.659148 loss_rnnt 7.554121 hw_loss 0.107222 lr 0.00034863 rank 5
2023-02-25 06:17:44,848 DEBUG TRAIN Batch 24/5500 loss 9.074351 loss_att 12.628145 loss_ctc 12.890186 loss_rnnt 7.713028 hw_loss 0.265848 lr 0.00034871 rank 3
2023-02-25 06:17:44,849 DEBUG TRAIN Batch 24/5500 loss 9.586593 loss_att 10.656582 loss_ctc 16.303848 loss_rnnt 8.316374 hw_loss 0.301102 lr 0.00034869 rank 1
2023-02-25 06:17:44,850 DEBUG TRAIN Batch 24/5500 loss 14.409975 loss_att 16.365122 loss_ctc 16.309340 loss_rnnt 13.647512 hw_loss 0.221599 lr 0.00034866 rank 6
2023-02-25 06:17:44,851 DEBUG TRAIN Batch 24/5500 loss 20.247927 loss_att 23.742056 loss_ctc 25.062542 loss_rnnt 18.825119 hw_loss 0.153815 lr 0.00034870 rank 4
2023-02-25 06:17:44,854 DEBUG TRAIN Batch 24/5500 loss 9.232050 loss_att 10.487772 loss_ctc 15.940948 loss_rnnt 7.975868 hw_loss 0.207219 lr 0.00034864 rank 7
2023-02-25 06:18:55,298 DEBUG TRAIN Batch 24/5600 loss 11.468770 loss_att 13.879827 loss_ctc 14.891605 loss_rnnt 10.387668 hw_loss 0.267213 lr 0.00034863 rank 3
2023-02-25 06:18:55,301 DEBUG TRAIN Batch 24/5600 loss 11.130795 loss_att 12.193214 loss_ctc 12.585297 loss_rnnt 10.649184 hw_loss 0.140988 lr 0.00034858 rank 6
2023-02-25 06:18:55,303 DEBUG TRAIN Batch 24/5600 loss 9.981247 loss_att 12.045221 loss_ctc 13.820553 loss_rnnt 8.960850 hw_loss 0.179427 lr 0.00034855 rank 5
2023-02-25 06:18:55,309 DEBUG TRAIN Batch 24/5600 loss 3.688359 loss_att 7.760592 loss_ctc 4.542720 loss_rnnt 2.595842 hw_loss 0.307794 lr 0.00034858 rank 2
2023-02-25 06:18:55,313 DEBUG TRAIN Batch 24/5600 loss 10.119092 loss_att 12.954039 loss_ctc 16.116070 loss_rnnt 8.647123 hw_loss 0.197593 lr 0.00034856 rank 7
2023-02-25 06:18:55,315 DEBUG TRAIN Batch 24/5600 loss 5.430898 loss_att 7.767241 loss_ctc 9.270815 loss_rnnt 4.383471 hw_loss 0.127816 lr 0.00034861 rank 1
2023-02-25 06:18:55,320 DEBUG TRAIN Batch 24/5600 loss 5.371128 loss_att 7.080190 loss_ctc 6.784787 loss_rnnt 4.719167 hw_loss 0.228114 lr 0.00034859 rank 0
2023-02-25 06:18:55,358 DEBUG TRAIN Batch 24/5600 loss 7.200291 loss_att 10.614814 loss_ctc 11.031777 loss_rnnt 5.922696 hw_loss 0.157174 lr 0.00034861 rank 4
2023-02-25 06:20:09,149 DEBUG TRAIN Batch 24/5700 loss 9.456269 loss_att 9.758266 loss_ctc 12.588239 loss_rnnt 8.845687 hw_loss 0.248601 lr 0.00034853 rank 4
2023-02-25 06:20:09,152 DEBUG TRAIN Batch 24/5700 loss 8.315455 loss_att 9.461491 loss_ctc 10.429413 loss_rnnt 7.660408 hw_loss 0.269959 lr 0.00034850 rank 0
2023-02-25 06:20:09,155 DEBUG TRAIN Batch 24/5700 loss 9.587005 loss_att 10.680391 loss_ctc 12.403502 loss_rnnt 8.884566 hw_loss 0.202927 lr 0.00034854 rank 3
2023-02-25 06:20:09,156 DEBUG TRAIN Batch 24/5700 loss 6.822841 loss_att 8.268633 loss_ctc 9.831176 loss_rnnt 6.023038 hw_loss 0.205375 lr 0.00034849 rank 2
2023-02-25 06:20:09,159 DEBUG TRAIN Batch 24/5700 loss 8.369883 loss_att 10.808587 loss_ctc 12.272821 loss_rnnt 7.253960 hw_loss 0.202109 lr 0.00034852 rank 1
2023-02-25 06:20:09,162 DEBUG TRAIN Batch 24/5700 loss 5.414760 loss_att 8.590917 loss_ctc 7.942211 loss_rnnt 4.337570 hw_loss 0.196810 lr 0.00034846 rank 5
2023-02-25 06:20:09,177 DEBUG TRAIN Batch 24/5700 loss 10.313383 loss_att 19.576794 loss_ctc 14.770294 loss_rnnt 7.781149 hw_loss 0.159930 lr 0.00034849 rank 6
2023-02-25 06:20:09,187 DEBUG TRAIN Batch 24/5700 loss 6.143387 loss_att 8.311821 loss_ctc 9.110035 loss_rnnt 5.212751 hw_loss 0.190118 lr 0.00034847 rank 7
2023-02-25 06:21:19,612 DEBUG TRAIN Batch 24/5800 loss 3.510398 loss_att 6.768496 loss_ctc 5.390233 loss_rnnt 2.526066 hw_loss 0.153877 lr 0.00034844 rank 4
2023-02-25 06:21:19,620 DEBUG TRAIN Batch 24/5800 loss 6.514399 loss_att 7.604380 loss_ctc 9.191270 loss_rnnt 5.847816 hw_loss 0.171883 lr 0.00034842 rank 0
2023-02-25 06:21:19,620 DEBUG TRAIN Batch 24/5800 loss 4.217164 loss_att 8.197443 loss_ctc 5.623802 loss_rnnt 3.092480 hw_loss 0.264516 lr 0.00034846 rank 3
2023-02-25 06:21:19,623 DEBUG TRAIN Batch 24/5800 loss 7.595734 loss_att 10.787207 loss_ctc 9.187145 loss_rnnt 6.676414 hw_loss 0.129068 lr 0.00034841 rank 2
2023-02-25 06:21:19,624 DEBUG TRAIN Batch 24/5800 loss 9.420989 loss_att 12.706795 loss_ctc 16.479832 loss_rnnt 7.649319 hw_loss 0.324993 lr 0.00034839 rank 7
2023-02-25 06:21:19,625 DEBUG TRAIN Batch 24/5800 loss 11.652349 loss_att 16.064323 loss_ctc 15.970243 loss_rnnt 10.083416 hw_loss 0.207786 lr 0.00034841 rank 6
2023-02-25 06:21:19,627 DEBUG TRAIN Batch 24/5800 loss 6.558707 loss_att 13.484350 loss_ctc 12.619617 loss_rnnt 4.281705 hw_loss 0.157036 lr 0.00034838 rank 5
2023-02-25 06:21:19,630 DEBUG TRAIN Batch 24/5800 loss 6.114679 loss_att 7.312856 loss_ctc 8.692371 loss_rnnt 5.467720 hw_loss 0.119309 lr 0.00034844 rank 1
2023-02-25 06:22:29,606 DEBUG TRAIN Batch 24/5900 loss 8.252885 loss_att 9.981279 loss_ctc 15.634892 loss_rnnt 6.802356 hw_loss 0.226094 lr 0.00034833 rank 0
2023-02-25 06:22:29,608 DEBUG TRAIN Batch 24/5900 loss 11.449482 loss_att 13.825017 loss_ctc 25.313484 loss_rnnt 9.040003 hw_loss 0.160946 lr 0.00034829 rank 5
2023-02-25 06:22:29,607 DEBUG TRAIN Batch 24/5900 loss 4.971120 loss_att 5.641410 loss_ctc 5.659973 loss_rnnt 4.510141 hw_loss 0.440762 lr 0.00034835 rank 1
2023-02-25 06:22:29,607 DEBUG TRAIN Batch 24/5900 loss 5.344807 loss_att 6.879582 loss_ctc 6.106141 loss_rnnt 4.840358 hw_loss 0.179966 lr 0.00034832 rank 6
2023-02-25 06:22:29,610 DEBUG TRAIN Batch 24/5900 loss 8.083690 loss_att 10.638286 loss_ctc 11.842280 loss_rnnt 6.960496 hw_loss 0.208367 lr 0.00034837 rank 3
2023-02-25 06:22:29,610 DEBUG TRAIN Batch 24/5900 loss 5.340984 loss_att 7.402689 loss_ctc 7.800357 loss_rnnt 4.542023 hw_loss 0.110069 lr 0.00034830 rank 7
2023-02-25 06:22:29,613 DEBUG TRAIN Batch 24/5900 loss 19.416216 loss_att 17.837671 loss_ctc 23.694599 loss_rnnt 19.044506 hw_loss 0.219314 lr 0.00034832 rank 2
2023-02-25 06:22:29,615 DEBUG TRAIN Batch 24/5900 loss 4.927807 loss_att 7.274697 loss_ctc 7.529037 loss_rnnt 4.028029 hw_loss 0.156690 lr 0.00034836 rank 4
2023-02-25 06:23:41,451 DEBUG TRAIN Batch 24/6000 loss 10.013846 loss_att 13.067740 loss_ctc 14.472665 loss_rnnt 8.694655 hw_loss 0.213568 lr 0.00034824 rank 6
2023-02-25 06:23:41,455 DEBUG TRAIN Batch 24/6000 loss 7.308213 loss_att 9.929337 loss_ctc 8.701365 loss_rnnt 6.477648 hw_loss 0.226102 lr 0.00034825 rank 0
2023-02-25 06:23:41,455 DEBUG TRAIN Batch 24/6000 loss 10.636594 loss_att 13.751518 loss_ctc 13.254260 loss_rnnt 9.539762 hw_loss 0.234046 lr 0.00034827 rank 1
2023-02-25 06:23:41,458 DEBUG TRAIN Batch 24/6000 loss 6.792231 loss_att 8.739212 loss_ctc 9.439366 loss_rnnt 5.985068 hw_loss 0.121528 lr 0.00034821 rank 5
2023-02-25 06:23:41,459 DEBUG TRAIN Batch 24/6000 loss 7.920229 loss_att 9.509155 loss_ctc 14.122253 loss_rnnt 6.702025 hw_loss 0.137780 lr 0.00034824 rank 2
2023-02-25 06:23:41,460 DEBUG TRAIN Batch 24/6000 loss 7.709503 loss_att 10.041275 loss_ctc 9.148105 loss_rnnt 6.934279 hw_loss 0.219479 lr 0.00034822 rank 7
2023-02-25 06:23:41,464 DEBUG TRAIN Batch 24/6000 loss 12.230725 loss_att 15.297281 loss_ctc 18.298454 loss_rnnt 10.706491 hw_loss 0.191048 lr 0.00034829 rank 3
2023-02-25 06:23:41,471 DEBUG TRAIN Batch 24/6000 loss 14.002254 loss_att 18.018700 loss_ctc 16.898296 loss_rnnt 12.744713 hw_loss 0.127711 lr 0.00034827 rank 4
2023-02-25 06:24:53,238 DEBUG TRAIN Batch 24/6100 loss 9.494951 loss_att 12.418218 loss_ctc 13.525765 loss_rnnt 8.256680 hw_loss 0.217829 lr 0.00034816 rank 0
2023-02-25 06:24:53,238 DEBUG TRAIN Batch 24/6100 loss 16.661570 loss_att 17.039900 loss_ctc 21.591875 loss_rnnt 15.842030 hw_loss 0.162186 lr 0.00034814 rank 7
2023-02-25 06:24:53,239 DEBUG TRAIN Batch 24/6100 loss 11.099297 loss_att 11.634546 loss_ctc 13.610654 loss_rnnt 10.547601 hw_loss 0.205870 lr 0.00034819 rank 4
2023-02-25 06:24:53,240 DEBUG TRAIN Batch 24/6100 loss 9.998494 loss_att 12.615204 loss_ctc 17.586555 loss_rnnt 8.342984 hw_loss 0.225799 lr 0.00034815 rank 6
2023-02-25 06:24:53,243 DEBUG TRAIN Batch 24/6100 loss 4.325649 loss_att 7.080765 loss_ctc 6.098672 loss_rnnt 3.398476 hw_loss 0.262025 lr 0.00034818 rank 1
2023-02-25 06:24:53,246 DEBUG TRAIN Batch 24/6100 loss 9.542135 loss_att 11.731991 loss_ctc 16.226326 loss_rnnt 8.102701 hw_loss 0.206695 lr 0.00034820 rank 3
2023-02-25 06:24:53,247 DEBUG TRAIN Batch 24/6100 loss 13.858049 loss_att 13.840282 loss_ctc 16.743797 loss_rnnt 13.377253 hw_loss 0.186719 lr 0.00034812 rank 5
2023-02-25 06:24:53,247 DEBUG TRAIN Batch 24/6100 loss 8.928807 loss_att 10.954853 loss_ctc 13.896089 loss_rnnt 7.775217 hw_loss 0.161393 lr 0.00034815 rank 2
2023-02-25 06:26:03,748 DEBUG TRAIN Batch 24/6200 loss 6.528870 loss_att 8.429435 loss_ctc 8.748463 loss_rnnt 5.750916 hw_loss 0.191051 lr 0.00034807 rank 6
2023-02-25 06:26:03,750 DEBUG TRAIN Batch 24/6200 loss 2.660938 loss_att 6.402687 loss_ctc 5.006703 loss_rnnt 1.488066 hw_loss 0.209538 lr 0.00034808 rank 0
2023-02-25 06:26:03,750 DEBUG TRAIN Batch 24/6200 loss 7.835666 loss_att 10.592775 loss_ctc 10.171071 loss_rnnt 6.843158 hw_loss 0.243186 lr 0.00034805 rank 7
2023-02-25 06:26:03,750 DEBUG TRAIN Batch 24/6200 loss 1.460632 loss_att 2.994375 loss_ctc 2.421989 loss_rnnt 0.899201 hw_loss 0.237191 lr 0.00034810 rank 4
2023-02-25 06:26:03,752 DEBUG TRAIN Batch 24/6200 loss 14.474482 loss_att 17.003830 loss_ctc 24.350025 loss_rnnt 12.481685 hw_loss 0.319104 lr 0.00034812 rank 3
2023-02-25 06:26:03,753 DEBUG TRAIN Batch 24/6200 loss 8.759554 loss_att 12.464020 loss_ctc 14.582473 loss_rnnt 7.149351 hw_loss 0.174226 lr 0.00034804 rank 5
2023-02-25 06:26:03,759 DEBUG TRAIN Batch 24/6200 loss 10.010642 loss_att 13.032284 loss_ctc 13.827480 loss_rnnt 8.780586 hw_loss 0.219027 lr 0.00034807 rank 2
2023-02-25 06:26:03,760 DEBUG TRAIN Batch 24/6200 loss 7.562064 loss_att 10.962808 loss_ctc 10.927183 loss_rnnt 6.293727 hw_loss 0.261574 lr 0.00034810 rank 1
2023-02-25 06:27:13,386 DEBUG TRAIN Batch 24/6300 loss 4.305766 loss_att 8.765434 loss_ctc 5.186674 loss_rnnt 3.189134 hw_loss 0.201083 lr 0.00034800 rank 0
2023-02-25 06:27:13,388 DEBUG TRAIN Batch 24/6300 loss 9.824669 loss_att 9.465748 loss_ctc 11.788013 loss_rnnt 9.496216 hw_loss 0.259609 lr 0.00034803 rank 3
2023-02-25 06:27:13,389 DEBUG TRAIN Batch 24/6300 loss 9.039856 loss_att 8.727859 loss_ctc 11.894137 loss_rnnt 8.588891 hw_loss 0.248985 lr 0.00034797 rank 7
2023-02-25 06:27:13,391 DEBUG TRAIN Batch 24/6300 loss 8.419489 loss_att 9.072228 loss_ctc 13.248504 loss_rnnt 7.540341 hw_loss 0.196369 lr 0.00034802 rank 4
2023-02-25 06:27:13,393 DEBUG TRAIN Batch 24/6300 loss 21.797434 loss_att 24.613390 loss_ctc 32.407436 loss_rnnt 19.699879 hw_loss 0.224435 lr 0.00034801 rank 1
2023-02-25 06:27:13,393 DEBUG TRAIN Batch 24/6300 loss 7.547311 loss_att 8.605791 loss_ctc 12.624506 loss_rnnt 6.530900 hw_loss 0.239543 lr 0.00034798 rank 6
2023-02-25 06:27:13,394 DEBUG TRAIN Batch 24/6300 loss 7.111314 loss_att 10.018462 loss_ctc 12.745662 loss_rnnt 5.650432 hw_loss 0.240386 lr 0.00034798 rank 2
2023-02-25 06:27:13,394 DEBUG TRAIN Batch 24/6300 loss 8.264956 loss_att 9.980153 loss_ctc 9.574564 loss_rnnt 7.657819 hw_loss 0.167780 lr 0.00034796 rank 5
2023-02-25 06:28:27,056 DEBUG TRAIN Batch 24/6400 loss 6.558659 loss_att 6.816181 loss_ctc 8.176836 loss_rnnt 6.174856 hw_loss 0.218515 lr 0.00034791 rank 0
2023-02-25 06:28:27,060 DEBUG TRAIN Batch 24/6400 loss 8.446608 loss_att 13.064356 loss_ctc 14.128395 loss_rnnt 6.654273 hw_loss 0.208526 lr 0.00034793 rank 1
2023-02-25 06:28:27,061 DEBUG TRAIN Batch 24/6400 loss 18.432743 loss_att 24.189188 loss_ctc 23.591345 loss_rnnt 16.502296 hw_loss 0.171271 lr 0.00034790 rank 2
2023-02-25 06:28:27,064 DEBUG TRAIN Batch 24/6400 loss 7.539672 loss_att 9.845372 loss_ctc 11.587024 loss_rnnt 6.435696 hw_loss 0.193479 lr 0.00034790 rank 6
2023-02-25 06:28:27,068 DEBUG TRAIN Batch 24/6400 loss 9.030413 loss_att 10.625719 loss_ctc 11.658820 loss_rnnt 8.223892 hw_loss 0.256884 lr 0.00034788 rank 7
2023-02-25 06:28:27,070 DEBUG TRAIN Batch 24/6400 loss 7.028648 loss_att 9.727564 loss_ctc 9.453570 loss_rnnt 6.108531 hw_loss 0.106897 lr 0.00034795 rank 3
2023-02-25 06:28:27,071 DEBUG TRAIN Batch 24/6400 loss 9.407663 loss_att 10.378527 loss_ctc 15.408386 loss_rnnt 8.273129 hw_loss 0.262994 lr 0.00034787 rank 5
2023-02-25 06:28:27,092 DEBUG TRAIN Batch 24/6400 loss 7.473661 loss_att 12.081511 loss_ctc 12.490940 loss_rnnt 5.757824 hw_loss 0.234932 lr 0.00034794 rank 4
2023-02-25 06:29:37,474 DEBUG TRAIN Batch 24/6500 loss 5.999462 loss_att 8.596510 loss_ctc 10.839674 loss_rnnt 4.748885 hw_loss 0.160885 lr 0.00034782 rank 6
2023-02-25 06:29:37,474 DEBUG TRAIN Batch 24/6500 loss 8.623218 loss_att 10.773161 loss_ctc 10.572266 loss_rnnt 7.826034 hw_loss 0.201230 lr 0.00034782 rank 2
2023-02-25 06:29:37,475 DEBUG TRAIN Batch 24/6500 loss 8.374711 loss_att 11.670696 loss_ctc 12.025316 loss_rnnt 7.078752 hw_loss 0.281277 lr 0.00034783 rank 0
2023-02-25 06:29:37,478 DEBUG TRAIN Batch 24/6500 loss 5.600396 loss_att 6.963182 loss_ctc 6.337045 loss_rnnt 5.144629 hw_loss 0.159354 lr 0.00034780 rank 7
2023-02-25 06:29:37,480 DEBUG TRAIN Batch 24/6500 loss 11.578868 loss_att 14.508286 loss_ctc 15.546772 loss_rnnt 10.373882 hw_loss 0.168841 lr 0.00034787 rank 3
2023-02-25 06:29:37,480 DEBUG TRAIN Batch 24/6500 loss 3.791864 loss_att 6.747294 loss_ctc 5.076924 loss_rnnt 2.978173 hw_loss 0.096119 lr 0.00034785 rank 4
2023-02-25 06:29:37,481 DEBUG TRAIN Batch 24/6500 loss 13.466384 loss_att 13.689751 loss_ctc 15.452285 loss_rnnt 13.052261 hw_loss 0.196241 lr 0.00034785 rank 1
2023-02-25 06:29:37,483 DEBUG TRAIN Batch 24/6500 loss 18.911409 loss_att 23.290709 loss_ctc 25.382038 loss_rnnt 17.050747 hw_loss 0.228848 lr 0.00034779 rank 5
2023-02-25 06:30:47,808 DEBUG TRAIN Batch 24/6600 loss 8.326651 loss_att 11.205988 loss_ctc 11.908098 loss_rnnt 7.148031 hw_loss 0.234797 lr 0.00034771 rank 7
2023-02-25 06:30:47,808 DEBUG TRAIN Batch 24/6600 loss 7.818500 loss_att 10.112711 loss_ctc 14.015456 loss_rnnt 6.428318 hw_loss 0.197023 lr 0.00034777 rank 4
2023-02-25 06:30:47,809 DEBUG TRAIN Batch 24/6600 loss 5.802053 loss_att 8.556841 loss_ctc 7.212067 loss_rnnt 4.978956 hw_loss 0.157759 lr 0.00034776 rank 1
2023-02-25 06:30:47,812 DEBUG TRAIN Batch 24/6600 loss 11.444712 loss_att 13.743699 loss_ctc 16.731726 loss_rnnt 10.211855 hw_loss 0.127732 lr 0.00034774 rank 0
2023-02-25 06:30:47,814 DEBUG TRAIN Batch 24/6600 loss 8.544595 loss_att 11.733540 loss_ctc 13.677010 loss_rnnt 7.116863 hw_loss 0.198037 lr 0.00034773 rank 2
2023-02-25 06:30:47,816 DEBUG TRAIN Batch 24/6600 loss 4.685356 loss_att 7.836157 loss_ctc 6.043674 loss_rnnt 3.780467 hw_loss 0.175536 lr 0.00034773 rank 6
2023-02-25 06:30:47,817 DEBUG TRAIN Batch 24/6600 loss 5.532445 loss_att 8.099783 loss_ctc 6.778749 loss_rnnt 4.746536 hw_loss 0.199251 lr 0.00034770 rank 5
2023-02-25 06:30:47,820 DEBUG TRAIN Batch 24/6600 loss 7.588250 loss_att 9.003113 loss_ctc 11.819847 loss_rnnt 6.632619 hw_loss 0.203334 lr 0.00034778 rank 3
2023-02-25 06:31:59,548 DEBUG TRAIN Batch 24/6700 loss 9.938507 loss_att 11.613770 loss_ctc 12.288689 loss_rnnt 9.159191 hw_loss 0.245451 lr 0.00034762 rank 5
2023-02-25 06:31:59,557 DEBUG TRAIN Batch 24/6700 loss 6.276899 loss_att 11.261564 loss_ctc 10.002090 loss_rnnt 4.634363 hw_loss 0.279206 lr 0.00034763 rank 7
2023-02-25 06:31:59,558 DEBUG TRAIN Batch 24/6700 loss 4.158831 loss_att 8.167051 loss_ctc 11.052448 loss_rnnt 2.367583 hw_loss 0.132104 lr 0.00034768 rank 4
2023-02-25 06:31:59,560 DEBUG TRAIN Batch 24/6700 loss 6.134494 loss_att 9.510946 loss_ctc 10.238878 loss_rnnt 4.836166 hw_loss 0.142099 lr 0.00034766 rank 0
2023-02-25 06:31:59,560 DEBUG TRAIN Batch 24/6700 loss 3.031301 loss_att 6.095053 loss_ctc 3.879056 loss_rnnt 2.201690 hw_loss 0.194675 lr 0.00034765 rank 6
2023-02-25 06:31:59,562 DEBUG TRAIN Batch 24/6700 loss 3.057353 loss_att 6.417889 loss_ctc 5.967070 loss_rnnt 1.905974 hw_loss 0.171205 lr 0.00034765 rank 2
2023-02-25 06:31:59,562 DEBUG TRAIN Batch 24/6700 loss 9.391895 loss_att 13.409262 loss_ctc 16.046928 loss_rnnt 7.584077 hw_loss 0.219388 lr 0.00034768 rank 1
2023-02-25 06:31:59,604 DEBUG TRAIN Batch 24/6700 loss 11.341677 loss_att 14.719595 loss_ctc 16.919352 loss_rnnt 9.848627 hw_loss 0.138332 lr 0.00034770 rank 3
2023-02-25 06:33:12,127 DEBUG TRAIN Batch 24/6800 loss 3.033028 loss_att 6.950852 loss_ctc 5.256555 loss_rnnt 1.821251 hw_loss 0.247015 lr 0.00034759 rank 1
2023-02-25 06:33:12,126 DEBUG TRAIN Batch 24/6800 loss 12.272032 loss_att 14.662543 loss_ctc 20.091106 loss_rnnt 10.673063 hw_loss 0.146857 lr 0.00034756 rank 2
2023-02-25 06:33:12,127 DEBUG TRAIN Batch 24/6800 loss 5.916517 loss_att 9.519565 loss_ctc 10.997826 loss_rnnt 4.364301 hw_loss 0.288936 lr 0.00034761 rank 3
2023-02-25 06:33:12,126 DEBUG TRAIN Batch 24/6800 loss 7.806894 loss_att 9.423038 loss_ctc 12.507418 loss_rnnt 6.763833 hw_loss 0.174554 lr 0.00034758 rank 0
2023-02-25 06:33:12,130 DEBUG TRAIN Batch 24/6800 loss 9.914275 loss_att 12.429240 loss_ctc 15.238647 loss_rnnt 8.624115 hw_loss 0.144847 lr 0.00034755 rank 7
2023-02-25 06:33:12,132 DEBUG TRAIN Batch 24/6800 loss 14.377841 loss_att 15.591205 loss_ctc 16.373064 loss_rnnt 13.723481 hw_loss 0.273106 lr 0.00034756 rank 6
2023-02-25 06:33:12,132 DEBUG TRAIN Batch 24/6800 loss 4.436018 loss_att 8.361835 loss_ctc 6.644500 loss_rnnt 3.281132 hw_loss 0.141110 lr 0.00034754 rank 5
2023-02-25 06:33:12,174 DEBUG TRAIN Batch 24/6800 loss 16.897272 loss_att 17.695715 loss_ctc 17.074080 loss_rnnt 16.646168 hw_loss 0.127201 lr 0.00034760 rank 4
2023-02-25 06:34:21,867 DEBUG TRAIN Batch 24/6900 loss 11.242801 loss_att 15.042829 loss_ctc 15.982052 loss_rnnt 9.754784 hw_loss 0.180208 lr 0.00034751 rank 1
2023-02-25 06:34:21,883 DEBUG TRAIN Batch 24/6900 loss 6.794934 loss_att 9.276747 loss_ctc 10.654890 loss_rnnt 5.664267 hw_loss 0.224333 lr 0.00034749 rank 0
2023-02-25 06:34:21,886 DEBUG TRAIN Batch 24/6900 loss 6.661245 loss_att 9.877198 loss_ctc 12.295975 loss_rnnt 5.160002 hw_loss 0.200167 lr 0.00034748 rank 2
2023-02-25 06:34:21,888 DEBUG TRAIN Batch 24/6900 loss 4.862761 loss_att 6.986980 loss_ctc 7.116445 loss_rnnt 4.104981 hw_loss 0.060834 lr 0.00034753 rank 3
2023-02-25 06:34:21,888 DEBUG TRAIN Batch 24/6900 loss 8.593897 loss_att 10.061843 loss_ctc 13.465569 loss_rnnt 7.567052 hw_loss 0.156938 lr 0.00034746 rank 7
2023-02-25 06:34:21,889 DEBUG TRAIN Batch 24/6900 loss 11.313183 loss_att 12.274989 loss_ctc 18.232889 loss_rnnt 10.012621 hw_loss 0.347950 lr 0.00034748 rank 6
2023-02-25 06:34:21,892 DEBUG TRAIN Batch 24/6900 loss 8.504509 loss_att 10.770023 loss_ctc 10.768126 loss_rnnt 7.674431 hw_loss 0.140926 lr 0.00034745 rank 5
2023-02-25 06:34:21,937 DEBUG TRAIN Batch 24/6900 loss 4.424107 loss_att 6.552197 loss_ctc 6.805744 loss_rnnt 3.564851 hw_loss 0.217663 lr 0.00034751 rank 4
2023-02-25 06:35:32,022 DEBUG TRAIN Batch 24/7000 loss 9.860596 loss_att 10.901050 loss_ctc 15.952754 loss_rnnt 8.708823 hw_loss 0.246366 lr 0.00034741 rank 0
2023-02-25 06:35:32,024 DEBUG TRAIN Batch 24/7000 loss 5.475648 loss_att 7.819574 loss_ctc 8.619034 loss_rnnt 4.451848 hw_loss 0.254806 lr 0.00034740 rank 2
2023-02-25 06:35:32,025 DEBUG TRAIN Batch 24/7000 loss 8.795727 loss_att 14.072943 loss_ctc 14.994390 loss_rnnt 6.759842 hw_loss 0.288663 lr 0.00034745 rank 3
2023-02-25 06:35:32,025 DEBUG TRAIN Batch 24/7000 loss 8.783270 loss_att 11.294731 loss_ctc 15.056682 loss_rnnt 7.337560 hw_loss 0.200555 lr 0.00034740 rank 6
2023-02-25 06:35:32,026 DEBUG TRAIN Batch 24/7000 loss 5.662251 loss_att 7.430764 loss_ctc 9.471363 loss_rnnt 4.701380 hw_loss 0.186161 lr 0.00034738 rank 7
2023-02-25 06:35:32,027 DEBUG TRAIN Batch 24/7000 loss 3.975239 loss_att 6.265259 loss_ctc 7.407641 loss_rnnt 2.945062 hw_loss 0.214724 lr 0.00034743 rank 1
2023-02-25 06:35:32,031 DEBUG TRAIN Batch 24/7000 loss 9.906938 loss_att 12.451220 loss_ctc 15.075935 loss_rnnt 8.624098 hw_loss 0.158968 lr 0.00034737 rank 5
2023-02-25 06:35:32,031 DEBUG TRAIN Batch 24/7000 loss 10.303460 loss_att 14.154088 loss_ctc 14.660239 loss_rnnt 8.865416 hw_loss 0.163151 lr 0.00034743 rank 4
2023-02-25 06:36:44,694 DEBUG TRAIN Batch 24/7100 loss 8.524565 loss_att 10.767336 loss_ctc 13.612083 loss_rnnt 7.261161 hw_loss 0.255963 lr 0.00034734 rank 1
2023-02-25 06:36:44,698 DEBUG TRAIN Batch 24/7100 loss 15.412314 loss_att 18.438742 loss_ctc 22.661163 loss_rnnt 13.724787 hw_loss 0.216992 lr 0.00034736 rank 3
2023-02-25 06:36:44,698 DEBUG TRAIN Batch 24/7100 loss 5.671031 loss_att 8.072953 loss_ctc 5.583970 loss_rnnt 5.081507 hw_loss 0.226402 lr 0.00034735 rank 4
2023-02-25 06:36:44,700 DEBUG TRAIN Batch 24/7100 loss 11.778938 loss_att 14.460853 loss_ctc 17.760658 loss_rnnt 10.359615 hw_loss 0.160084 lr 0.00034731 rank 6
2023-02-25 06:36:44,703 DEBUG TRAIN Batch 24/7100 loss 6.660632 loss_att 7.987473 loss_ctc 9.935012 loss_rnnt 5.818871 hw_loss 0.262140 lr 0.00034730 rank 7
2023-02-25 06:36:44,704 DEBUG TRAIN Batch 24/7100 loss 6.599844 loss_att 11.139138 loss_ctc 9.480642 loss_rnnt 5.252033 hw_loss 0.104712 lr 0.00034728 rank 5
2023-02-25 06:36:44,729 DEBUG TRAIN Batch 24/7100 loss 4.536302 loss_att 8.444266 loss_ctc 7.268186 loss_rnnt 3.266955 hw_loss 0.231568 lr 0.00034731 rank 2
2023-02-25 06:36:44,741 DEBUG TRAIN Batch 24/7100 loss 4.923801 loss_att 8.004642 loss_ctc 6.637435 loss_rnnt 3.979191 hw_loss 0.187420 lr 0.00034732 rank 0
2023-02-25 06:37:55,586 DEBUG TRAIN Batch 24/7200 loss 4.755412 loss_att 7.657907 loss_ctc 6.228190 loss_rnnt 3.882442 hw_loss 0.180186 lr 0.00034724 rank 0
2023-02-25 06:37:55,588 DEBUG TRAIN Batch 24/7200 loss 8.005671 loss_att 10.826678 loss_ctc 10.627605 loss_rnnt 7.013891 hw_loss 0.146226 lr 0.00034726 rank 4
2023-02-25 06:37:55,592 DEBUG TRAIN Batch 24/7200 loss 7.911140 loss_att 9.641287 loss_ctc 11.270844 loss_rnnt 6.978913 hw_loss 0.259196 lr 0.00034726 rank 1
2023-02-25 06:37:55,592 DEBUG TRAIN Batch 24/7200 loss 8.395860 loss_att 8.930533 loss_ctc 10.480394 loss_rnnt 7.947655 hw_loss 0.118750 lr 0.00034723 rank 2
2023-02-25 06:37:55,596 DEBUG TRAIN Batch 24/7200 loss 5.565580 loss_att 7.357418 loss_ctc 6.238091 loss_rnnt 4.981724 hw_loss 0.254664 lr 0.00034728 rank 3
2023-02-25 06:37:55,597 DEBUG TRAIN Batch 24/7200 loss 5.871267 loss_att 9.770267 loss_ctc 11.203592 loss_rnnt 4.270204 hw_loss 0.206788 lr 0.00034723 rank 6
2023-02-25 06:37:55,600 DEBUG TRAIN Batch 24/7200 loss 11.131607 loss_att 12.133604 loss_ctc 17.883978 loss_rnnt 9.944663 hw_loss 0.161679 lr 0.00034721 rank 7
2023-02-25 06:37:55,601 DEBUG TRAIN Batch 24/7200 loss 5.329594 loss_att 7.651240 loss_ctc 11.174235 loss_rnnt 3.983945 hw_loss 0.191315 lr 0.00034720 rank 5
2023-02-25 06:39:05,385 DEBUG TRAIN Batch 24/7300 loss 12.365266 loss_att 13.624760 loss_ctc 13.948425 loss_rnnt 11.810339 hw_loss 0.172389 lr 0.00034714 rank 6
2023-02-25 06:39:05,385 DEBUG TRAIN Batch 24/7300 loss 11.303446 loss_att 13.448624 loss_ctc 18.344957 loss_rnnt 9.810356 hw_loss 0.234722 lr 0.00034719 rank 3
2023-02-25 06:39:05,390 DEBUG TRAIN Batch 24/7300 loss 19.197237 loss_att 26.060581 loss_ctc 29.180929 loss_rnnt 16.392061 hw_loss 0.190022 lr 0.00034716 rank 0
2023-02-25 06:39:05,389 DEBUG TRAIN Batch 24/7300 loss 2.689640 loss_att 5.544705 loss_ctc 5.203796 loss_rnnt 1.706867 hw_loss 0.143511 lr 0.00034712 rank 5
2023-02-25 06:39:05,391 DEBUG TRAIN Batch 24/7300 loss 6.017137 loss_att 8.885346 loss_ctc 8.729548 loss_rnnt 4.970393 hw_loss 0.208962 lr 0.00034715 rank 2
2023-02-25 06:39:05,392 DEBUG TRAIN Batch 24/7300 loss 10.033586 loss_att 15.408241 loss_ctc 19.441309 loss_rnnt 7.601516 hw_loss 0.192705 lr 0.00034717 rank 1
2023-02-25 06:39:05,393 DEBUG TRAIN Batch 24/7300 loss 10.929196 loss_att 14.644926 loss_ctc 12.856598 loss_rnnt 9.833302 hw_loss 0.179553 lr 0.00034718 rank 4
2023-02-25 06:39:05,394 DEBUG TRAIN Batch 24/7300 loss 3.823333 loss_att 6.125178 loss_ctc 6.572587 loss_rnnt 2.870257 hw_loss 0.236513 lr 0.00034713 rank 7
2023-02-25 06:40:15,830 DEBUG TRAIN Batch 24/7400 loss 8.572871 loss_att 11.050919 loss_ctc 11.779053 loss_rnnt 7.576468 hw_loss 0.137443 lr 0.00034710 rank 4
2023-02-25 06:40:15,839 DEBUG TRAIN Batch 24/7400 loss 12.578437 loss_att 18.271910 loss_ctc 21.306183 loss_rnnt 10.191780 hw_loss 0.157992 lr 0.00034706 rank 6
2023-02-25 06:40:15,840 DEBUG TRAIN Batch 24/7400 loss 7.916531 loss_att 12.489375 loss_ctc 12.549358 loss_rnnt 6.290718 hw_loss 0.175375 lr 0.00034707 rank 0
2023-02-25 06:40:15,840 DEBUG TRAIN Batch 24/7400 loss 6.230042 loss_att 10.249466 loss_ctc 10.982466 loss_rnnt 4.676535 hw_loss 0.217437 lr 0.00034706 rank 2
2023-02-25 06:40:15,842 DEBUG TRAIN Batch 24/7400 loss 6.731569 loss_att 9.500253 loss_ctc 8.679065 loss_rnnt 5.841007 hw_loss 0.144673 lr 0.00034709 rank 1
2023-02-25 06:40:15,843 DEBUG TRAIN Batch 24/7400 loss 13.223668 loss_att 17.149111 loss_ctc 19.740479 loss_rnnt 11.471426 hw_loss 0.184211 lr 0.00034704 rank 7
2023-02-25 06:40:15,856 DEBUG TRAIN Batch 24/7400 loss 7.736500 loss_att 10.108698 loss_ctc 8.976498 loss_rnnt 6.983397 hw_loss 0.212493 lr 0.00034703 rank 5
2023-02-25 06:40:15,888 DEBUG TRAIN Batch 24/7400 loss 5.513732 loss_att 7.579506 loss_ctc 8.217723 loss_rnnt 4.632514 hw_loss 0.201620 lr 0.00034711 rank 3
2023-02-25 06:41:29,537 DEBUG TRAIN Batch 24/7500 loss 9.953474 loss_att 11.578534 loss_ctc 15.141355 loss_rnnt 8.753557 hw_loss 0.343475 lr 0.00034696 rank 7
2023-02-25 06:41:29,538 DEBUG TRAIN Batch 24/7500 loss 10.389011 loss_att 12.599063 loss_ctc 13.435125 loss_rnnt 9.406034 hw_loss 0.252787 lr 0.00034699 rank 0
2023-02-25 06:41:29,541 DEBUG TRAIN Batch 24/7500 loss 7.923253 loss_att 10.931418 loss_ctc 11.176230 loss_rnnt 6.752160 hw_loss 0.254492 lr 0.00034701 rank 4
2023-02-25 06:41:29,542 DEBUG TRAIN Batch 24/7500 loss 15.364451 loss_att 20.508558 loss_ctc 21.841389 loss_rnnt 13.347950 hw_loss 0.232665 lr 0.00034701 rank 1
2023-02-25 06:41:29,542 DEBUG TRAIN Batch 24/7500 loss 14.256663 loss_att 15.966930 loss_ctc 18.949617 loss_rnnt 13.153583 hw_loss 0.253689 lr 0.00034698 rank 2
2023-02-25 06:41:29,544 DEBUG TRAIN Batch 24/7500 loss 21.928892 loss_att 24.698256 loss_ctc 32.794510 loss_rnnt 19.807148 hw_loss 0.223355 lr 0.00034695 rank 5
2023-02-25 06:41:29,545 DEBUG TRAIN Batch 24/7500 loss 7.127994 loss_att 8.318874 loss_ctc 10.750376 loss_rnnt 6.316389 hw_loss 0.169582 lr 0.00034703 rank 3
2023-02-25 06:41:29,605 DEBUG TRAIN Batch 24/7500 loss 11.470641 loss_att 15.603228 loss_ctc 17.598768 loss_rnnt 9.640460 hw_loss 0.349838 lr 0.00034698 rank 6
2023-02-25 06:42:39,743 DEBUG TRAIN Batch 24/7600 loss 6.317818 loss_att 6.295062 loss_ctc 7.710934 loss_rnnt 5.952170 hw_loss 0.345846 lr 0.00034694 rank 3
2023-02-25 06:42:39,744 DEBUG TRAIN Batch 24/7600 loss 8.145651 loss_att 10.068537 loss_ctc 11.310722 loss_rnnt 7.229587 hw_loss 0.205268 lr 0.00034691 rank 0
2023-02-25 06:42:39,748 DEBUG TRAIN Batch 24/7600 loss 6.775939 loss_att 8.257906 loss_ctc 11.187009 loss_rnnt 5.761792 hw_loss 0.243019 lr 0.00034687 rank 5
2023-02-25 06:42:39,751 DEBUG TRAIN Batch 24/7600 loss 8.255260 loss_att 9.321466 loss_ctc 9.844255 loss_rnnt 7.716017 hw_loss 0.214002 lr 0.00034689 rank 6
2023-02-25 06:42:39,751 DEBUG TRAIN Batch 24/7600 loss 7.222763 loss_att 9.510319 loss_ctc 11.926134 loss_rnnt 6.034868 hw_loss 0.193625 lr 0.00034688 rank 7
2023-02-25 06:42:39,753 DEBUG TRAIN Batch 24/7600 loss 6.844108 loss_att 6.773079 loss_ctc 9.656391 loss_rnnt 6.364890 hw_loss 0.222099 lr 0.00034693 rank 4
2023-02-25 06:42:39,768 DEBUG TRAIN Batch 24/7600 loss 11.312766 loss_att 14.471176 loss_ctc 14.406274 loss_rnnt 10.198344 hw_loss 0.131762 lr 0.00034692 rank 1
2023-02-25 06:42:39,775 DEBUG TRAIN Batch 24/7600 loss 6.927939 loss_att 9.543701 loss_ctc 7.844280 loss_rnnt 6.185712 hw_loss 0.181680 lr 0.00034689 rank 2
2023-02-25 06:43:50,455 DEBUG TRAIN Batch 24/7700 loss 4.014661 loss_att 5.536104 loss_ctc 3.002172 loss_rnnt 3.761285 hw_loss 0.157663 lr 0.00034686 rank 3
2023-02-25 06:43:50,455 DEBUG TRAIN Batch 24/7700 loss 5.993443 loss_att 7.288786 loss_ctc 7.607285 loss_rnnt 5.356400 hw_loss 0.305240 lr 0.00034681 rank 2
2023-02-25 06:43:50,456 DEBUG TRAIN Batch 24/7700 loss 16.883739 loss_att 15.725242 loss_ctc 20.662418 loss_rnnt 16.522909 hw_loss 0.166325 lr 0.00034684 rank 1
2023-02-25 06:43:50,457 DEBUG TRAIN Batch 24/7700 loss 8.029598 loss_att 10.772238 loss_ctc 10.872887 loss_rnnt 7.032753 hw_loss 0.129774 lr 0.00034682 rank 0
2023-02-25 06:43:50,461 DEBUG TRAIN Batch 24/7700 loss 9.537380 loss_att 10.381668 loss_ctc 13.307460 loss_rnnt 8.772455 hw_loss 0.175106 lr 0.00034681 rank 6
2023-02-25 06:43:50,463 DEBUG TRAIN Batch 24/7700 loss 11.266949 loss_att 14.355717 loss_ctc 18.788609 loss_rnnt 9.557783 hw_loss 0.165982 lr 0.00034685 rank 4
2023-02-25 06:43:50,465 DEBUG TRAIN Batch 24/7700 loss 8.956732 loss_att 10.719769 loss_ctc 11.985264 loss_rnnt 8.131463 hw_loss 0.129105 lr 0.00034679 rank 7
2023-02-25 06:43:50,466 DEBUG TRAIN Batch 24/7700 loss 3.569605 loss_att 7.390624 loss_ctc 5.026589 loss_rnnt 2.534744 hw_loss 0.143236 lr 0.00034678 rank 5
2023-02-25 06:45:02,742 DEBUG TRAIN Batch 24/7800 loss 1.914910 loss_att 4.760027 loss_ctc 1.904562 loss_rnnt 1.202564 hw_loss 0.271318 lr 0.00034674 rank 0
2023-02-25 06:45:02,747 DEBUG TRAIN Batch 24/7800 loss 9.274308 loss_att 10.835868 loss_ctc 10.438812 loss_rnnt 8.692814 hw_loss 0.213590 lr 0.00034676 rank 1
2023-02-25 06:45:02,746 DEBUG TRAIN Batch 24/7800 loss 8.182229 loss_att 14.822139 loss_ctc 12.627655 loss_rnnt 6.184980 hw_loss 0.143520 lr 0.00034676 rank 4
2023-02-25 06:45:02,749 DEBUG TRAIN Batch 24/7800 loss 7.989926 loss_att 11.272140 loss_ctc 12.895182 loss_rnnt 6.630686 hw_loss 0.091431 lr 0.00034673 rank 6
2023-02-25 06:45:02,750 DEBUG TRAIN Batch 24/7800 loss 12.150002 loss_att 13.459263 loss_ctc 17.440020 loss_rnnt 11.121506 hw_loss 0.114954 lr 0.00034678 rank 3
2023-02-25 06:45:02,751 DEBUG TRAIN Batch 24/7800 loss 3.078476 loss_att 6.359664 loss_ctc 4.937754 loss_rnnt 2.050024 hw_loss 0.233081 lr 0.00034671 rank 7
2023-02-25 06:45:02,754 DEBUG TRAIN Batch 24/7800 loss 15.621449 loss_att 16.278740 loss_ctc 20.972599 loss_rnnt 14.662077 hw_loss 0.214556 lr 0.00034673 rank 2
2023-02-25 06:45:02,779 DEBUG TRAIN Batch 24/7800 loss 6.486652 loss_att 8.175512 loss_ctc 8.486111 loss_rnnt 5.753579 hw_loss 0.241323 lr 0.00034670 rank 5
2023-02-25 06:46:14,430 DEBUG TRAIN Batch 24/7900 loss 7.095667 loss_att 10.611482 loss_ctc 10.658029 loss_rnnt 5.771142 hw_loss 0.274463 lr 0.00034666 rank 0
2023-02-25 06:46:14,432 DEBUG TRAIN Batch 24/7900 loss 12.297359 loss_att 15.730030 loss_ctc 15.210144 loss_rnnt 11.122683 hw_loss 0.187067 lr 0.00034662 rank 5
2023-02-25 06:46:14,432 DEBUG TRAIN Batch 24/7900 loss 4.195677 loss_att 8.044828 loss_ctc 11.410083 loss_rnnt 2.325138 hw_loss 0.260227 lr 0.00034664 rank 2
2023-02-25 06:46:14,433 DEBUG TRAIN Batch 24/7900 loss 6.028722 loss_att 8.735453 loss_ctc 9.558899 loss_rnnt 4.932171 hw_loss 0.158465 lr 0.00034664 rank 6
2023-02-25 06:46:14,434 DEBUG TRAIN Batch 24/7900 loss 15.491373 loss_att 17.590797 loss_ctc 22.087605 loss_rnnt 14.105133 hw_loss 0.162859 lr 0.00034663 rank 7
2023-02-25 06:46:14,435 DEBUG TRAIN Batch 24/7900 loss 15.356441 loss_att 17.760437 loss_ctc 22.429541 loss_rnnt 13.818728 hw_loss 0.213438 lr 0.00034669 rank 3
2023-02-25 06:46:14,439 DEBUG TRAIN Batch 24/7900 loss 10.513809 loss_att 10.820957 loss_ctc 15.381003 loss_rnnt 9.687508 hw_loss 0.217338 lr 0.00034667 rank 1
2023-02-25 06:46:14,495 DEBUG TRAIN Batch 24/7900 loss 7.095943 loss_att 11.726406 loss_ctc 11.658701 loss_rnnt 5.445968 hw_loss 0.216591 lr 0.00034668 rank 4
2023-02-25 06:47:24,030 DEBUG TRAIN Batch 24/8000 loss 7.355866 loss_att 10.143885 loss_ctc 10.249314 loss_rnnt 6.288274 hw_loss 0.232866 lr 0.00034657 rank 0
2023-02-25 06:47:24,034 DEBUG TRAIN Batch 24/8000 loss 5.108150 loss_att 9.597358 loss_ctc 6.122845 loss_rnnt 3.980954 hw_loss 0.176364 lr 0.00034660 rank 4
2023-02-25 06:47:24,034 DEBUG TRAIN Batch 24/8000 loss 3.735066 loss_att 6.394317 loss_ctc 4.834799 loss_rnnt 2.962708 hw_loss 0.176017 lr 0.00034661 rank 3
2023-02-25 06:47:24,034 DEBUG TRAIN Batch 24/8000 loss 11.937934 loss_att 13.062271 loss_ctc 15.746950 loss_rnnt 11.083790 hw_loss 0.227639 lr 0.00034653 rank 5
2023-02-25 06:47:24,035 DEBUG TRAIN Batch 24/8000 loss 4.280622 loss_att 7.238816 loss_ctc 5.158772 loss_rnnt 3.423604 hw_loss 0.278048 lr 0.00034656 rank 6
2023-02-25 06:47:24,039 DEBUG TRAIN Batch 24/8000 loss 7.846474 loss_att 9.464842 loss_ctc 8.612032 loss_rnnt 7.338451 hw_loss 0.154266 lr 0.00034659 rank 1
2023-02-25 06:47:24,039 DEBUG TRAIN Batch 24/8000 loss 14.034216 loss_att 18.519375 loss_ctc 14.551292 loss_rnnt 12.961894 hw_loss 0.199398 lr 0.00034656 rank 2
2023-02-25 06:47:24,043 DEBUG TRAIN Batch 24/8000 loss 5.104669 loss_att 8.690383 loss_ctc 10.314913 loss_rnnt 3.579273 hw_loss 0.212911 lr 0.00034654 rank 7
2023-02-25 06:48:35,235 DEBUG TRAIN Batch 24/8100 loss 11.123646 loss_att 13.133596 loss_ctc 15.082637 loss_rnnt 10.069821 hw_loss 0.232442 lr 0.00034646 rank 7
2023-02-25 06:48:35,241 DEBUG TRAIN Batch 24/8100 loss 9.661798 loss_att 10.752287 loss_ctc 14.282038 loss_rnnt 8.736197 hw_loss 0.171509 lr 0.00034649 rank 0
2023-02-25 06:48:35,242 DEBUG TRAIN Batch 24/8100 loss 11.962137 loss_att 13.372526 loss_ctc 15.276835 loss_rnnt 11.128736 hw_loss 0.205059 lr 0.00034653 rank 3
2023-02-25 06:48:35,247 DEBUG TRAIN Batch 24/8100 loss 12.578201 loss_att 16.177431 loss_ctc 15.974735 loss_rnnt 11.253152 hw_loss 0.285622 lr 0.00034648 rank 6
2023-02-25 06:48:35,262 DEBUG TRAIN Batch 24/8100 loss 6.837017 loss_att 8.950372 loss_ctc 9.167580 loss_rnnt 6.012610 hw_loss 0.170614 lr 0.00034648 rank 2
2023-02-25 06:48:35,263 DEBUG TRAIN Batch 24/8100 loss 7.299888 loss_att 8.207826 loss_ctc 12.252276 loss_rnnt 6.376363 hw_loss 0.153034 lr 0.00034651 rank 4
2023-02-25 06:48:35,268 DEBUG TRAIN Batch 24/8100 loss 9.951716 loss_att 15.479790 loss_ctc 13.980373 loss_rnnt 8.236958 hw_loss 0.134980 lr 0.00034651 rank 1
2023-02-25 06:48:35,273 DEBUG TRAIN Batch 24/8100 loss 3.480775 loss_att 5.828258 loss_ctc 4.927397 loss_rnnt 2.679493 hw_loss 0.260442 lr 0.00034645 rank 5
2023-02-25 06:49:47,197 DEBUG TRAIN Batch 24/8200 loss 8.500163 loss_att 10.321622 loss_ctc 11.829792 loss_rnnt 7.568404 hw_loss 0.231595 lr 0.00034641 rank 0
2023-02-25 06:49:47,199 DEBUG TRAIN Batch 24/8200 loss 4.392298 loss_att 8.228789 loss_ctc 6.604275 loss_rnnt 3.220885 hw_loss 0.204719 lr 0.00034642 rank 1
2023-02-25 06:49:47,201 DEBUG TRAIN Batch 24/8200 loss 3.013109 loss_att 4.266748 loss_ctc 5.299429 loss_rnnt 2.390615 hw_loss 0.125482 lr 0.00034638 rank 7
2023-02-25 06:49:47,204 DEBUG TRAIN Batch 24/8200 loss 9.426502 loss_att 11.457393 loss_ctc 12.490217 loss_rnnt 8.474723 hw_loss 0.257075 lr 0.00034639 rank 2
2023-02-25 06:49:47,206 DEBUG TRAIN Batch 24/8200 loss 9.913892 loss_att 9.653764 loss_ctc 12.603040 loss_rnnt 9.476692 hw_loss 0.245010 lr 0.00034644 rank 3
2023-02-25 06:49:47,205 DEBUG TRAIN Batch 24/8200 loss 10.965161 loss_att 14.246389 loss_ctc 12.356513 loss_rnnt 10.038301 hw_loss 0.159566 lr 0.00034643 rank 4
2023-02-25 06:49:47,207 DEBUG TRAIN Batch 24/8200 loss 5.745714 loss_att 7.195147 loss_ctc 7.864254 loss_rnnt 5.042904 hw_loss 0.244596 lr 0.00034639 rank 6
2023-02-25 06:49:47,213 DEBUG TRAIN Batch 24/8200 loss 6.532380 loss_att 7.907211 loss_ctc 7.856569 loss_rnnt 5.974498 hw_loss 0.199420 lr 0.00034637 rank 5
2023-02-25 06:50:57,301 DEBUG TRAIN Batch 24/8300 loss 5.979132 loss_att 9.421414 loss_ctc 10.433131 loss_rnnt 4.568881 hw_loss 0.239865 lr 0.00034631 rank 6
2023-02-25 06:50:57,301 DEBUG TRAIN Batch 24/8300 loss 3.833388 loss_att 6.201624 loss_ctc 8.732375 loss_rnnt 2.607314 hw_loss 0.186055 lr 0.00034631 rank 2
2023-02-25 06:50:57,301 DEBUG TRAIN Batch 24/8300 loss 11.572599 loss_att 11.387719 loss_ctc 15.529896 loss_rnnt 10.927121 hw_loss 0.290277 lr 0.00034632 rank 0
2023-02-25 06:50:57,302 DEBUG TRAIN Batch 24/8300 loss 4.990721 loss_att 7.442794 loss_ctc 6.913530 loss_rnnt 4.180112 hw_loss 0.119662 lr 0.00034636 rank 3
2023-02-25 06:50:57,305 DEBUG TRAIN Batch 24/8300 loss 5.313563 loss_att 10.065800 loss_ctc 7.179178 loss_rnnt 4.025994 hw_loss 0.165698 lr 0.00034629 rank 7
2023-02-25 06:50:57,305 DEBUG TRAIN Batch 24/8300 loss 10.215941 loss_att 11.352377 loss_ctc 14.278957 loss_rnnt 9.300005 hw_loss 0.275461 lr 0.00034628 rank 5
2023-02-25 06:50:57,312 DEBUG TRAIN Batch 24/8300 loss 6.814630 loss_att 8.812314 loss_ctc 7.325065 loss_rnnt 6.301069 hw_loss 0.086187 lr 0.00034635 rank 4
2023-02-25 06:51:43,978 DEBUG CV Batch 24/0 loss 1.941045 loss_att 2.018689 loss_ctc 3.170697 loss_rnnt 1.636693 hw_loss 0.234131 history loss 1.869154 rank 4
2023-02-25 06:51:43,980 DEBUG CV Batch 24/0 loss 1.941045 loss_att 2.018689 loss_ctc 3.170697 loss_rnnt 1.636693 hw_loss 0.234131 history loss 1.869154 rank 0
2023-02-25 06:51:43,980 DEBUG CV Batch 24/0 loss 1.941045 loss_att 2.018689 loss_ctc 3.170697 loss_rnnt 1.636693 hw_loss 0.234131 history loss 1.869154 rank 1
2023-02-25 06:51:43,981 DEBUG CV Batch 24/0 loss 1.941045 loss_att 2.018689 loss_ctc 3.170697 loss_rnnt 1.636693 hw_loss 0.234131 history loss 1.869154 rank 3
2023-02-25 06:51:43,982 DEBUG CV Batch 24/0 loss 1.941045 loss_att 2.018689 loss_ctc 3.170697 loss_rnnt 1.636693 hw_loss 0.234131 history loss 1.869154 rank 7
2023-02-25 06:51:43,991 DEBUG CV Batch 24/0 loss 1.941045 loss_att 2.018689 loss_ctc 3.170697 loss_rnnt 1.636693 hw_loss 0.234131 history loss 1.869154 rank 5
2023-02-25 06:51:43,992 DEBUG CV Batch 24/0 loss 1.941045 loss_att 2.018689 loss_ctc 3.170697 loss_rnnt 1.636693 hw_loss 0.234131 history loss 1.869154 rank 2
2023-02-25 06:51:44,003 DEBUG CV Batch 24/0 loss 1.941045 loss_att 2.018689 loss_ctc 3.170697 loss_rnnt 1.636693 hw_loss 0.234131 history loss 1.869154 rank 6
2023-02-25 06:51:55,526 DEBUG CV Batch 24/100 loss 6.342182 loss_att 6.793765 loss_ctc 9.916561 loss_rnnt 5.663289 hw_loss 0.209984 history loss 3.329334 rank 6
2023-02-25 06:51:55,563 DEBUG CV Batch 24/100 loss 6.342182 loss_att 6.793765 loss_ctc 9.916561 loss_rnnt 5.663289 hw_loss 0.209984 history loss 3.329334 rank 4
2023-02-25 06:51:55,566 DEBUG CV Batch 24/100 loss 6.342182 loss_att 6.793765 loss_ctc 9.916561 loss_rnnt 5.663289 hw_loss 0.209984 history loss 3.329334 rank 0
2023-02-25 06:51:55,634 DEBUG CV Batch 24/100 loss 6.342182 loss_att 6.793765 loss_ctc 9.916561 loss_rnnt 5.663289 hw_loss 0.209984 history loss 3.329334 rank 1
2023-02-25 06:51:55,670 DEBUG CV Batch 24/100 loss 6.342182 loss_att 6.793765 loss_ctc 9.916561 loss_rnnt 5.663289 hw_loss 0.209984 history loss 3.329334 rank 5
2023-02-25 06:51:55,737 DEBUG CV Batch 24/100 loss 6.342182 loss_att 6.793765 loss_ctc 9.916561 loss_rnnt 5.663289 hw_loss 0.209984 history loss 3.329334 rank 3
2023-02-25 06:51:55,776 DEBUG CV Batch 24/100 loss 6.342182 loss_att 6.793765 loss_ctc 9.916561 loss_rnnt 5.663289 hw_loss 0.209984 history loss 3.329334 rank 2
2023-02-25 06:51:55,808 DEBUG CV Batch 24/100 loss 6.342182 loss_att 6.793765 loss_ctc 9.916561 loss_rnnt 5.663289 hw_loss 0.209984 history loss 3.329334 rank 7
2023-02-25 06:52:09,234 DEBUG CV Batch 24/200 loss 4.731665 loss_att 14.078711 loss_ctc 5.423096 loss_rnnt 2.705774 hw_loss 0.120545 history loss 3.928576 rank 6
2023-02-25 06:52:09,321 DEBUG CV Batch 24/200 loss 4.731665 loss_att 14.078711 loss_ctc 5.423096 loss_rnnt 2.705774 hw_loss 0.120545 history loss 3.928576 rank 0
2023-02-25 06:52:09,372 DEBUG CV Batch 24/200 loss 4.731665 loss_att 14.078711 loss_ctc 5.423096 loss_rnnt 2.705774 hw_loss 0.120545 history loss 3.928576 rank 1
2023-02-25 06:52:09,426 DEBUG CV Batch 24/200 loss 4.731665 loss_att 14.078711 loss_ctc 5.423096 loss_rnnt 2.705774 hw_loss 0.120545 history loss 3.928576 rank 3
2023-02-25 06:52:09,619 DEBUG CV Batch 24/200 loss 4.731665 loss_att 14.078711 loss_ctc 5.423096 loss_rnnt 2.705774 hw_loss 0.120545 history loss 3.928576 rank 4
2023-02-25 06:52:09,637 DEBUG CV Batch 24/200 loss 4.731665 loss_att 14.078711 loss_ctc 5.423096 loss_rnnt 2.705774 hw_loss 0.120545 history loss 3.928576 rank 2
2023-02-25 06:52:09,760 DEBUG CV Batch 24/200 loss 4.731665 loss_att 14.078711 loss_ctc 5.423096 loss_rnnt 2.705774 hw_loss 0.120545 history loss 3.928576 rank 5
2023-02-25 06:52:09,797 DEBUG CV Batch 24/200 loss 4.731665 loss_att 14.078711 loss_ctc 5.423096 loss_rnnt 2.705774 hw_loss 0.120545 history loss 3.928576 rank 7
2023-02-25 06:52:21,252 DEBUG CV Batch 24/300 loss 5.012992 loss_att 5.201879 loss_ctc 7.581944 loss_rnnt 4.478422 hw_loss 0.289250 history loss 4.044445 rank 6
2023-02-25 06:52:21,493 DEBUG CV Batch 24/300 loss 5.012992 loss_att 5.201879 loss_ctc 7.581944 loss_rnnt 4.478422 hw_loss 0.289250 history loss 4.044445 rank 3
2023-02-25 06:52:21,574 DEBUG CV Batch 24/300 loss 5.012992 loss_att 5.201879 loss_ctc 7.581944 loss_rnnt 4.478422 hw_loss 0.289250 history loss 4.044445 rank 4
2023-02-25 06:52:21,700 DEBUG CV Batch 24/300 loss 5.012992 loss_att 5.201879 loss_ctc 7.581944 loss_rnnt 4.478422 hw_loss 0.289250 history loss 4.044445 rank 0
2023-02-25 06:52:21,715 DEBUG CV Batch 24/300 loss 5.012992 loss_att 5.201879 loss_ctc 7.581944 loss_rnnt 4.478422 hw_loss 0.289250 history loss 4.044445 rank 1
2023-02-25 06:52:21,773 DEBUG CV Batch 24/300 loss 5.012992 loss_att 5.201879 loss_ctc 7.581944 loss_rnnt 4.478422 hw_loss 0.289250 history loss 4.044445 rank 5
2023-02-25 06:52:22,055 DEBUG CV Batch 24/300 loss 5.012992 loss_att 5.201879 loss_ctc 7.581944 loss_rnnt 4.478422 hw_loss 0.289250 history loss 4.044445 rank 2
2023-02-25 06:52:22,435 DEBUG CV Batch 24/300 loss 5.012992 loss_att 5.201879 loss_ctc 7.581944 loss_rnnt 4.478422 hw_loss 0.289250 history loss 4.044445 rank 7
2023-02-25 06:52:33,344 DEBUG CV Batch 24/400 loss 16.349966 loss_att 81.686707 loss_ctc 7.286429 loss_rnnt 4.432966 hw_loss 0.108980 history loss 4.893211 rank 6
2023-02-25 06:52:33,494 DEBUG CV Batch 24/400 loss 16.349966 loss_att 81.686707 loss_ctc 7.286429 loss_rnnt 4.432966 hw_loss 0.108980 history loss 4.893211 rank 3
2023-02-25 06:52:33,712 DEBUG CV Batch 24/400 loss 16.349966 loss_att 81.686707 loss_ctc 7.286429 loss_rnnt 4.432966 hw_loss 0.108980 history loss 4.893211 rank 5
2023-02-25 06:52:33,943 DEBUG CV Batch 24/400 loss 16.349966 loss_att 81.686707 loss_ctc 7.286429 loss_rnnt 4.432966 hw_loss 0.108980 history loss 4.893211 rank 4
2023-02-25 06:52:33,952 DEBUG CV Batch 24/400 loss 16.349966 loss_att 81.686707 loss_ctc 7.286429 loss_rnnt 4.432966 hw_loss 0.108980 history loss 4.893211 rank 1
2023-02-25 06:52:34,071 DEBUG CV Batch 24/400 loss 16.349966 loss_att 81.686707 loss_ctc 7.286429 loss_rnnt 4.432966 hw_loss 0.108980 history loss 4.893211 rank 0
2023-02-25 06:52:34,438 DEBUG CV Batch 24/400 loss 16.349966 loss_att 81.686707 loss_ctc 7.286429 loss_rnnt 4.432966 hw_loss 0.108980 history loss 4.893211 rank 2
2023-02-25 06:52:34,930 DEBUG CV Batch 24/400 loss 16.349966 loss_att 81.686707 loss_ctc 7.286429 loss_rnnt 4.432966 hw_loss 0.108980 history loss 4.893211 rank 7
2023-02-25 06:52:43,849 DEBUG CV Batch 24/500 loss 4.540502 loss_att 5.124698 loss_ctc 6.515812 loss_rnnt 4.078343 hw_loss 0.153647 history loss 5.551135 rank 3
2023-02-25 06:52:43,883 DEBUG CV Batch 24/500 loss 4.540502 loss_att 5.124698 loss_ctc 6.515812 loss_rnnt 4.078343 hw_loss 0.153647 history loss 5.551135 rank 6
2023-02-25 06:52:44,275 DEBUG CV Batch 24/500 loss 4.540502 loss_att 5.124698 loss_ctc 6.515812 loss_rnnt 4.078343 hw_loss 0.153647 history loss 5.551135 rank 5
2023-02-25 06:52:44,314 DEBUG CV Batch 24/500 loss 4.540502 loss_att 5.124698 loss_ctc 6.515812 loss_rnnt 4.078343 hw_loss 0.153647 history loss 5.551135 rank 4
2023-02-25 06:52:44,647 DEBUG CV Batch 24/500 loss 4.540502 loss_att 5.124698 loss_ctc 6.515812 loss_rnnt 4.078343 hw_loss 0.153647 history loss 5.551135 rank 1
2023-02-25 06:52:44,874 DEBUG CV Batch 24/500 loss 4.540502 loss_att 5.124698 loss_ctc 6.515812 loss_rnnt 4.078343 hw_loss 0.153647 history loss 5.551135 rank 0
2023-02-25 06:52:45,241 DEBUG CV Batch 24/500 loss 4.540502 loss_att 5.124698 loss_ctc 6.515812 loss_rnnt 4.078343 hw_loss 0.153647 history loss 5.551135 rank 2
2023-02-25 06:52:45,812 DEBUG CV Batch 24/500 loss 4.540502 loss_att 5.124698 loss_ctc 6.515812 loss_rnnt 4.078343 hw_loss 0.153647 history loss 5.551135 rank 7
2023-02-25 06:52:55,927 DEBUG CV Batch 24/600 loss 6.850599 loss_att 7.343399 loss_ctc 9.068623 loss_rnnt 6.270418 hw_loss 0.348534 history loss 6.471050 rank 6
2023-02-25 06:52:55,977 DEBUG CV Batch 24/600 loss 6.850599 loss_att 7.343399 loss_ctc 9.068623 loss_rnnt 6.270418 hw_loss 0.348534 history loss 6.471050 rank 3
2023-02-25 06:52:56,484 DEBUG CV Batch 24/600 loss 6.850599 loss_att 7.343399 loss_ctc 9.068623 loss_rnnt 6.270418 hw_loss 0.348534 history loss 6.471050 rank 4
2023-02-25 06:52:56,785 DEBUG CV Batch 24/600 loss 6.850599 loss_att 7.343399 loss_ctc 9.068623 loss_rnnt 6.270418 hw_loss 0.348534 history loss 6.471050 rank 5
2023-02-25 06:52:56,926 DEBUG CV Batch 24/600 loss 6.850599 loss_att 7.343399 loss_ctc 9.068623 loss_rnnt 6.270418 hw_loss 0.348534 history loss 6.471050 rank 1
2023-02-25 06:52:57,282 DEBUG CV Batch 24/600 loss 6.850599 loss_att 7.343399 loss_ctc 9.068623 loss_rnnt 6.270418 hw_loss 0.348534 history loss 6.471050 rank 0
2023-02-25 06:52:57,658 DEBUG CV Batch 24/600 loss 6.850599 loss_att 7.343399 loss_ctc 9.068623 loss_rnnt 6.270418 hw_loss 0.348534 history loss 6.471050 rank 2
2023-02-25 06:52:58,258 DEBUG CV Batch 24/600 loss 6.850599 loss_att 7.343399 loss_ctc 9.068623 loss_rnnt 6.270418 hw_loss 0.348534 history loss 6.471050 rank 7
2023-02-25 06:53:07,248 DEBUG CV Batch 24/700 loss 17.192024 loss_att 52.408310 loss_ctc 15.005032 loss_rnnt 10.401084 hw_loss 0.073651 history loss 7.091143 rank 6
2023-02-25 06:53:08,212 DEBUG CV Batch 24/700 loss 17.192024 loss_att 52.408310 loss_ctc 15.005032 loss_rnnt 10.401084 hw_loss 0.073651 history loss 7.091143 rank 3
2023-02-25 06:53:08,454 DEBUG CV Batch 24/700 loss 17.192024 loss_att 52.408310 loss_ctc 15.005032 loss_rnnt 10.401084 hw_loss 0.073651 history loss 7.091143 rank 1
2023-02-25 06:53:08,658 DEBUG CV Batch 24/700 loss 17.192024 loss_att 52.408310 loss_ctc 15.005032 loss_rnnt 10.401084 hw_loss 0.073651 history loss 7.091143 rank 4
2023-02-25 06:53:09,029 DEBUG CV Batch 24/700 loss 17.192024 loss_att 52.408310 loss_ctc 15.005032 loss_rnnt 10.401084 hw_loss 0.073651 history loss 7.091143 rank 5
2023-02-25 06:53:09,053 DEBUG CV Batch 24/700 loss 17.192024 loss_att 52.408310 loss_ctc 15.005032 loss_rnnt 10.401084 hw_loss 0.073651 history loss 7.091143 rank 0
2023-02-25 06:53:09,288 DEBUG CV Batch 24/700 loss 17.192024 loss_att 52.408310 loss_ctc 15.005032 loss_rnnt 10.401084 hw_loss 0.073651 history loss 7.091143 rank 2
2023-02-25 06:53:10,092 DEBUG CV Batch 24/700 loss 17.192024 loss_att 52.408310 loss_ctc 15.005032 loss_rnnt 10.401084 hw_loss 0.073651 history loss 7.091143 rank 7
2023-02-25 06:53:18,579 DEBUG CV Batch 24/800 loss 9.902115 loss_att 9.554442 loss_ctc 15.111061 loss_rnnt 9.134772 hw_loss 0.266908 history loss 6.583872 rank 6
2023-02-25 06:53:19,801 DEBUG CV Batch 24/800 loss 9.902115 loss_att 9.554442 loss_ctc 15.111061 loss_rnnt 9.134772 hw_loss 0.266908 history loss 6.583872 rank 1
2023-02-25 06:53:20,388 DEBUG CV Batch 24/800 loss 9.902115 loss_att 9.554442 loss_ctc 15.111061 loss_rnnt 9.134772 hw_loss 0.266908 history loss 6.583872 rank 3
2023-02-25 06:53:20,588 DEBUG CV Batch 24/800 loss 9.902115 loss_att 9.554442 loss_ctc 15.111061 loss_rnnt 9.134772 hw_loss 0.266908 history loss 6.583872 rank 0
2023-02-25 06:53:20,821 DEBUG CV Batch 24/800 loss 9.902115 loss_att 9.554442 loss_ctc 15.111061 loss_rnnt 9.134772 hw_loss 0.266908 history loss 6.583872 rank 2
2023-02-25 06:53:21,017 DEBUG CV Batch 24/800 loss 9.902115 loss_att 9.554442 loss_ctc 15.111061 loss_rnnt 9.134772 hw_loss 0.266908 history loss 6.583872 rank 4
2023-02-25 06:53:21,050 DEBUG CV Batch 24/800 loss 9.902115 loss_att 9.554442 loss_ctc 15.111061 loss_rnnt 9.134772 hw_loss 0.266908 history loss 6.583872 rank 5
2023-02-25 06:53:21,687 DEBUG CV Batch 24/800 loss 9.902115 loss_att 9.554442 loss_ctc 15.111061 loss_rnnt 9.134772 hw_loss 0.266908 history loss 6.583872 rank 7
2023-02-25 06:53:31,971 DEBUG CV Batch 24/900 loss 13.971222 loss_att 16.959660 loss_ctc 21.815683 loss_rnnt 12.243186 hw_loss 0.158289 history loss 6.404437 rank 6
2023-02-25 06:53:33,374 DEBUG CV Batch 24/900 loss 13.971222 loss_att 16.959660 loss_ctc 21.815683 loss_rnnt 12.243186 hw_loss 0.158289 history loss 6.404437 rank 1
2023-02-25 06:53:34,341 DEBUG CV Batch 24/900 loss 13.971222 loss_att 16.959660 loss_ctc 21.815683 loss_rnnt 12.243186 hw_loss 0.158289 history loss 6.404437 rank 0
2023-02-25 06:53:34,586 DEBUG CV Batch 24/900 loss 13.971222 loss_att 16.959660 loss_ctc 21.815683 loss_rnnt 12.243186 hw_loss 0.158289 history loss 6.404437 rank 3
2023-02-25 06:53:34,635 DEBUG CV Batch 24/900 loss 13.971222 loss_att 16.959660 loss_ctc 21.815683 loss_rnnt 12.243186 hw_loss 0.158289 history loss 6.404437 rank 2
2023-02-25 06:53:34,821 DEBUG CV Batch 24/900 loss 13.971222 loss_att 16.959660 loss_ctc 21.815683 loss_rnnt 12.243186 hw_loss 0.158289 history loss 6.404437 rank 5
2023-02-25 06:53:34,918 DEBUG CV Batch 24/900 loss 13.971222 loss_att 16.959660 loss_ctc 21.815683 loss_rnnt 12.243186 hw_loss 0.158289 history loss 6.404437 rank 4
2023-02-25 06:53:35,547 DEBUG CV Batch 24/900 loss 13.971222 loss_att 16.959660 loss_ctc 21.815683 loss_rnnt 12.243186 hw_loss 0.158289 history loss 6.404437 rank 7
2023-02-25 06:53:44,008 DEBUG CV Batch 24/1000 loss 4.597433 loss_att 4.451381 loss_ctc 3.583987 loss_rnnt 4.675866 hw_loss 0.161069 history loss 6.196132 rank 6
2023-02-25 06:53:45,846 DEBUG CV Batch 24/1000 loss 4.597433 loss_att 4.451381 loss_ctc 3.583987 loss_rnnt 4.675866 hw_loss 0.161069 history loss 6.196132 rank 1
2023-02-25 06:53:46,851 DEBUG CV Batch 24/1000 loss 4.597433 loss_att 4.451381 loss_ctc 3.583987 loss_rnnt 4.675866 hw_loss 0.161069 history loss 6.196132 rank 3
2023-02-25 06:53:47,021 DEBUG CV Batch 24/1000 loss 4.597433 loss_att 4.451381 loss_ctc 3.583987 loss_rnnt 4.675866 hw_loss 0.161069 history loss 6.196132 rank 5
2023-02-25 06:53:47,085 DEBUG CV Batch 24/1000 loss 4.597433 loss_att 4.451381 loss_ctc 3.583987 loss_rnnt 4.675866 hw_loss 0.161069 history loss 6.196132 rank 4
2023-02-25 06:53:47,094 DEBUG CV Batch 24/1000 loss 4.597433 loss_att 4.451381 loss_ctc 3.583987 loss_rnnt 4.675866 hw_loss 0.161069 history loss 6.196132 rank 0
2023-02-25 06:53:47,587 DEBUG CV Batch 24/1000 loss 4.597433 loss_att 4.451381 loss_ctc 3.583987 loss_rnnt 4.675866 hw_loss 0.161069 history loss 6.196132 rank 2
2023-02-25 06:53:48,352 DEBUG CV Batch 24/1000 loss 4.597433 loss_att 4.451381 loss_ctc 3.583987 loss_rnnt 4.675866 hw_loss 0.161069 history loss 6.196132 rank 7
2023-02-25 06:53:55,984 DEBUG CV Batch 24/1100 loss 5.044168 loss_att 4.889799 loss_ctc 7.167362 loss_rnnt 4.662584 hw_loss 0.242559 history loss 6.163368 rank 6
2023-02-25 06:53:58,075 DEBUG CV Batch 24/1100 loss 5.044168 loss_att 4.889799 loss_ctc 7.167362 loss_rnnt 4.662584 hw_loss 0.242559 history loss 6.163368 rank 1
2023-02-25 06:53:58,831 DEBUG CV Batch 24/1100 loss 5.044168 loss_att 4.889799 loss_ctc 7.167362 loss_rnnt 4.662584 hw_loss 0.242559 history loss 6.163368 rank 3
2023-02-25 06:53:58,955 DEBUG CV Batch 24/1100 loss 5.044168 loss_att 4.889799 loss_ctc 7.167362 loss_rnnt 4.662584 hw_loss 0.242559 history loss 6.163368 rank 5
2023-02-25 06:53:58,987 DEBUG CV Batch 24/1100 loss 5.044168 loss_att 4.889799 loss_ctc 7.167362 loss_rnnt 4.662584 hw_loss 0.242559 history loss 6.163368 rank 4
2023-02-25 06:53:59,489 DEBUG CV Batch 24/1100 loss 5.044168 loss_att 4.889799 loss_ctc 7.167362 loss_rnnt 4.662584 hw_loss 0.242559 history loss 6.163368 rank 0
2023-02-25 06:54:00,089 DEBUG CV Batch 24/1100 loss 5.044168 loss_att 4.889799 loss_ctc 7.167362 loss_rnnt 4.662584 hw_loss 0.242559 history loss 6.163368 rank 2
2023-02-25 06:54:00,807 DEBUG CV Batch 24/1100 loss 5.044168 loss_att 4.889799 loss_ctc 7.167362 loss_rnnt 4.662584 hw_loss 0.242559 history loss 6.163368 rank 7
2023-02-25 06:54:06,685 DEBUG CV Batch 24/1200 loss 7.563310 loss_att 8.045764 loss_ctc 9.112871 loss_rnnt 7.115219 hw_loss 0.271859 history loss 6.452902 rank 6
2023-02-25 06:54:08,987 DEBUG CV Batch 24/1200 loss 7.563310 loss_att 8.045764 loss_ctc 9.112871 loss_rnnt 7.115219 hw_loss 0.271859 history loss 6.452902 rank 1
2023-02-25 06:54:09,320 DEBUG CV Batch 24/1200 loss 7.563310 loss_att 8.045764 loss_ctc 9.112871 loss_rnnt 7.115219 hw_loss 0.271859 history loss 6.452902 rank 3
2023-02-25 06:54:09,376 DEBUG CV Batch 24/1200 loss 7.563310 loss_att 8.045764 loss_ctc 9.112871 loss_rnnt 7.115219 hw_loss 0.271859 history loss 6.452902 rank 4
2023-02-25 06:54:09,651 DEBUG CV Batch 24/1200 loss 7.563310 loss_att 8.045764 loss_ctc 9.112871 loss_rnnt 7.115219 hw_loss 0.271859 history loss 6.452902 rank 5
2023-02-25 06:54:10,622 DEBUG CV Batch 24/1200 loss 7.563310 loss_att 8.045764 loss_ctc 9.112871 loss_rnnt 7.115219 hw_loss 0.271859 history loss 6.452902 rank 0
2023-02-25 06:54:11,234 DEBUG CV Batch 24/1200 loss 7.563310 loss_att 8.045764 loss_ctc 9.112871 loss_rnnt 7.115219 hw_loss 0.271859 history loss 6.452902 rank 2
2023-02-25 06:54:12,069 DEBUG CV Batch 24/1200 loss 7.563310 loss_att 8.045764 loss_ctc 9.112871 loss_rnnt 7.115219 hw_loss 0.271859 history loss 6.452902 rank 7
2023-02-25 06:54:18,788 DEBUG CV Batch 24/1300 loss 4.109628 loss_att 5.150009 loss_ctc 6.115380 loss_rnnt 3.523186 hw_loss 0.207998 history loss 6.760631 rank 6
2023-02-25 06:54:21,172 DEBUG CV Batch 24/1300 loss 4.109628 loss_att 5.150009 loss_ctc 6.115380 loss_rnnt 3.523186 hw_loss 0.207998 history loss 6.760631 rank 1
2023-02-25 06:54:21,387 DEBUG CV Batch 24/1300 loss 4.109628 loss_att 5.150009 loss_ctc 6.115380 loss_rnnt 3.523186 hw_loss 0.207998 history loss 6.760631 rank 3
2023-02-25 06:54:21,528 DEBUG CV Batch 24/1300 loss 4.109628 loss_att 5.150009 loss_ctc 6.115380 loss_rnnt 3.523186 hw_loss 0.207998 history loss 6.760631 rank 4
2023-02-25 06:54:21,839 DEBUG CV Batch 24/1300 loss 4.109628 loss_att 5.150009 loss_ctc 6.115380 loss_rnnt 3.523186 hw_loss 0.207998 history loss 6.760631 rank 5
2023-02-25 06:54:22,958 DEBUG CV Batch 24/1300 loss 4.109628 loss_att 5.150009 loss_ctc 6.115380 loss_rnnt 3.523186 hw_loss 0.207998 history loss 6.760631 rank 0
2023-02-25 06:54:23,885 DEBUG CV Batch 24/1300 loss 4.109628 loss_att 5.150009 loss_ctc 6.115380 loss_rnnt 3.523186 hw_loss 0.207998 history loss 6.760631 rank 2
2023-02-25 06:54:24,599 DEBUG CV Batch 24/1300 loss 4.109628 loss_att 5.150009 loss_ctc 6.115380 loss_rnnt 3.523186 hw_loss 0.207998 history loss 6.760631 rank 7
2023-02-25 06:54:29,819 DEBUG CV Batch 24/1400 loss 7.230600 loss_att 12.795951 loss_ctc 6.899555 loss_rnnt 6.144594 hw_loss 0.032016 history loss 7.066101 rank 6
2023-02-25 06:54:32,677 DEBUG CV Batch 24/1400 loss 7.230600 loss_att 12.795951 loss_ctc 6.899555 loss_rnnt 6.144594 hw_loss 0.032016 history loss 7.066101 rank 1
2023-02-25 06:54:32,722 DEBUG CV Batch 24/1400 loss 7.230600 loss_att 12.795951 loss_ctc 6.899555 loss_rnnt 6.144594 hw_loss 0.032016 history loss 7.066101 rank 3
2023-02-25 06:54:33,078 DEBUG CV Batch 24/1400 loss 7.230600 loss_att 12.795951 loss_ctc 6.899555 loss_rnnt 6.144594 hw_loss 0.032016 history loss 7.066101 rank 4
2023-02-25 06:54:33,789 DEBUG CV Batch 24/1400 loss 7.230600 loss_att 12.795951 loss_ctc 6.899555 loss_rnnt 6.144594 hw_loss 0.032016 history loss 7.066101 rank 5
2023-02-25 06:54:34,593 DEBUG CV Batch 24/1400 loss 7.230600 loss_att 12.795951 loss_ctc 6.899555 loss_rnnt 6.144594 hw_loss 0.032016 history loss 7.066101 rank 0
2023-02-25 06:54:35,792 DEBUG CV Batch 24/1400 loss 7.230600 loss_att 12.795951 loss_ctc 6.899555 loss_rnnt 6.144594 hw_loss 0.032016 history loss 7.066101 rank 2
2023-02-25 06:54:36,491 DEBUG CV Batch 24/1400 loss 7.230600 loss_att 12.795951 loss_ctc 6.899555 loss_rnnt 6.144594 hw_loss 0.032016 history loss 7.066101 rank 7
2023-02-25 06:54:41,307 DEBUG CV Batch 24/1500 loss 5.762314 loss_att 7.211764 loss_ctc 6.631986 loss_rnnt 5.189652 hw_loss 0.312780 history loss 6.910054 rank 6
2023-02-25 06:54:44,286 DEBUG CV Batch 24/1500 loss 5.762314 loss_att 7.211764 loss_ctc 6.631986 loss_rnnt 5.189652 hw_loss 0.312780 history loss 6.910054 rank 1
2023-02-25 06:54:45,008 DEBUG CV Batch 24/1500 loss 5.762314 loss_att 7.211764 loss_ctc 6.631986 loss_rnnt 5.189652 hw_loss 0.312780 history loss 6.910054 rank 3
2023-02-25 06:54:45,807 DEBUG CV Batch 24/1500 loss 5.762314 loss_att 7.211764 loss_ctc 6.631986 loss_rnnt 5.189652 hw_loss 0.312780 history loss 6.910054 rank 4
2023-02-25 06:54:46,181 DEBUG CV Batch 24/1500 loss 5.762314 loss_att 7.211764 loss_ctc 6.631986 loss_rnnt 5.189652 hw_loss 0.312780 history loss 6.910054 rank 5
2023-02-25 06:54:46,454 DEBUG CV Batch 24/1500 loss 5.762314 loss_att 7.211764 loss_ctc 6.631986 loss_rnnt 5.189652 hw_loss 0.312780 history loss 6.910054 rank 0
2023-02-25 06:54:47,755 DEBUG CV Batch 24/1500 loss 5.762314 loss_att 7.211764 loss_ctc 6.631986 loss_rnnt 5.189652 hw_loss 0.312780 history loss 6.910054 rank 2
2023-02-25 06:54:48,605 DEBUG CV Batch 24/1500 loss 5.762314 loss_att 7.211764 loss_ctc 6.631986 loss_rnnt 5.189652 hw_loss 0.312780 history loss 6.910054 rank 7
2023-02-25 06:54:54,993 DEBUG CV Batch 24/1600 loss 7.447624 loss_att 11.784731 loss_ctc 7.343042 loss_rnnt 6.496747 hw_loss 0.182623 history loss 6.853784 rank 6
2023-02-25 06:54:57,716 DEBUG CV Batch 24/1600 loss 7.447624 loss_att 11.784731 loss_ctc 7.343042 loss_rnnt 6.496747 hw_loss 0.182623 history loss 6.853784 rank 1
2023-02-25 06:54:58,805 DEBUG CV Batch 24/1600 loss 7.447624 loss_att 11.784731 loss_ctc 7.343042 loss_rnnt 6.496747 hw_loss 0.182623 history loss 6.853784 rank 3
2023-02-25 06:54:59,608 DEBUG CV Batch 24/1600 loss 7.447624 loss_att 11.784731 loss_ctc 7.343042 loss_rnnt 6.496747 hw_loss 0.182623 history loss 6.853784 rank 4
2023-02-25 06:54:59,830 DEBUG CV Batch 24/1600 loss 7.447624 loss_att 11.784731 loss_ctc 7.343042 loss_rnnt 6.496747 hw_loss 0.182623 history loss 6.853784 rank 5
2023-02-25 06:54:59,905 DEBUG CV Batch 24/1600 loss 7.447624 loss_att 11.784731 loss_ctc 7.343042 loss_rnnt 6.496747 hw_loss 0.182623 history loss 6.853784 rank 0
2023-02-25 06:55:01,303 DEBUG CV Batch 24/1600 loss 7.447624 loss_att 11.784731 loss_ctc 7.343042 loss_rnnt 6.496747 hw_loss 0.182623 history loss 6.853784 rank 2
2023-02-25 06:55:02,267 DEBUG CV Batch 24/1600 loss 7.447624 loss_att 11.784731 loss_ctc 7.343042 loss_rnnt 6.496747 hw_loss 0.182623 history loss 6.853784 rank 7
2023-02-25 06:55:07,792 DEBUG CV Batch 24/1700 loss 8.776134 loss_att 7.242415 loss_ctc 10.767004 loss_rnnt 8.650490 hw_loss 0.313007 history loss 6.765587 rank 6
2023-02-25 06:55:10,495 DEBUG CV Batch 24/1700 loss 8.776134 loss_att 7.242415 loss_ctc 10.767004 loss_rnnt 8.650490 hw_loss 0.313007 history loss 6.765587 rank 1
2023-02-25 06:55:11,374 DEBUG CV Batch 24/1700 loss 8.776134 loss_att 7.242415 loss_ctc 10.767004 loss_rnnt 8.650490 hw_loss 0.313007 history loss 6.765587 rank 3
2023-02-25 06:55:12,032 DEBUG CV Batch 24/1700 loss 8.776134 loss_att 7.242415 loss_ctc 10.767004 loss_rnnt 8.650490 hw_loss 0.313007 history loss 6.765587 rank 4
2023-02-25 06:55:12,304 DEBUG CV Batch 24/1700 loss 8.776134 loss_att 7.242415 loss_ctc 10.767004 loss_rnnt 8.650490 hw_loss 0.313007 history loss 6.765587 rank 5
2023-02-25 06:55:12,667 DEBUG CV Batch 24/1700 loss 8.776134 loss_att 7.242415 loss_ctc 10.767004 loss_rnnt 8.650490 hw_loss 0.313007 history loss 6.765587 rank 0
2023-02-25 06:55:13,949 DEBUG CV Batch 24/1700 loss 8.776134 loss_att 7.242415 loss_ctc 10.767004 loss_rnnt 8.650490 hw_loss 0.313007 history loss 6.765587 rank 2
2023-02-25 06:55:14,894 DEBUG CV Batch 24/1700 loss 8.776134 loss_att 7.242415 loss_ctc 10.767004 loss_rnnt 8.650490 hw_loss 0.313007 history loss 6.765587 rank 7
2023-02-25 06:55:17,568 INFO Epoch 24 CV info cv_loss 6.737175214228594
2023-02-25 06:55:17,569 INFO Epoch 25 TRAIN info lr 0.0003462741717222745
2023-02-25 06:55:17,571 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 06:55:19,473 INFO Epoch 24 CV info cv_loss 6.7371752137267915
2023-02-25 06:55:19,474 INFO Epoch 25 TRAIN info lr 0.00034634976338129977
2023-02-25 06:55:19,478 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 06:55:20,711 INFO Epoch 24 CV info cv_loss 6.737175211810035
2023-02-25 06:55:20,712 INFO Epoch 25 TRAIN info lr 0.0003463123767038605
2023-02-25 06:55:20,715 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 06:55:21,304 INFO Epoch 24 CV info cv_loss 6.737175213528655
2023-02-25 06:55:21,305 INFO Epoch 25 TRAIN info lr 0.0003462982559984466
2023-02-25 06:55:21,309 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 06:55:21,708 INFO Epoch 24 CV info cv_loss 6.737175213853857
2023-02-25 06:55:21,708 INFO Epoch 25 TRAIN info lr 0.000346254243707196
2023-02-25 06:55:21,713 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 06:55:21,868 INFO Epoch 24 CV info cv_loss 6.737175213405896
2023-02-25 06:55:21,868 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/24.pt
2023-02-25 06:55:22,475 INFO Epoch 25 TRAIN info lr 0.0003462924420923648
2023-02-25 06:55:22,479 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 06:55:23,241 INFO Epoch 24 CV info cv_loss 6.737175214310432
2023-02-25 06:55:23,241 INFO Epoch 25 TRAIN info lr 0.00034628911999178724
2023-02-25 06:55:23,246 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 06:55:24,197 INFO Epoch 24 CV info cv_loss 6.737175214107989
2023-02-25 06:55:24,198 INFO Epoch 25 TRAIN info lr 0.00034624594138296313
2023-02-25 06:55:24,202 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 06:56:26,095 DEBUG TRAIN Batch 25/0 loss 9.516191 loss_att 9.282191 loss_ctc 12.765612 loss_rnnt 9.000508 hw_loss 0.242299 lr 0.00034629 rank 0
2023-02-25 06:56:26,097 DEBUG TRAIN Batch 25/0 loss 10.051186 loss_att 9.780051 loss_ctc 12.052117 loss_rnnt 9.722985 hw_loss 0.216817 lr 0.00034625 rank 5
2023-02-25 06:56:26,099 DEBUG TRAIN Batch 25/0 loss 6.610075 loss_att 6.937580 loss_ctc 8.976332 loss_rnnt 6.102313 hw_loss 0.237675 lr 0.00034627 rank 6
2023-02-25 06:56:26,099 DEBUG TRAIN Batch 25/0 loss 6.956461 loss_att 6.914271 loss_ctc 8.546240 loss_rnnt 6.625714 hw_loss 0.238529 lr 0.00034630 rank 4
2023-02-25 06:56:26,103 DEBUG TRAIN Batch 25/0 loss 9.686038 loss_att 9.801430 loss_ctc 12.522375 loss_rnnt 9.165780 hw_loss 0.223126 lr 0.00034631 rank 3
2023-02-25 06:56:26,118 DEBUG TRAIN Batch 25/0 loss 10.816884 loss_att 9.800585 loss_ctc 12.630727 loss_rnnt 10.647725 hw_loss 0.244821 lr 0.00034635 rank 1
2023-02-25 06:56:26,118 DEBUG TRAIN Batch 25/0 loss 6.079532 loss_att 5.702904 loss_ctc 7.855536 loss_rnnt 5.775911 hw_loss 0.266525 lr 0.00034625 rank 7
2023-02-25 06:56:26,125 DEBUG TRAIN Batch 25/0 loss 8.597262 loss_att 8.850279 loss_ctc 11.948212 loss_rnnt 7.931880 hw_loss 0.314972 lr 0.00034629 rank 2
2023-02-25 06:57:36,252 DEBUG TRAIN Batch 25/100 loss 9.446044 loss_att 12.470916 loss_ctc 15.715500 loss_rnnt 7.934200 hw_loss 0.133016 lr 0.00034621 rank 0
2023-02-25 06:57:36,253 DEBUG TRAIN Batch 25/100 loss 3.093466 loss_att 7.007638 loss_ctc 5.918661 loss_rnnt 1.834315 hw_loss 0.186795 lr 0.00034621 rank 2
2023-02-25 06:57:36,256 DEBUG TRAIN Batch 25/100 loss 6.702508 loss_att 8.798061 loss_ctc 9.198352 loss_rnnt 5.852224 hw_loss 0.184490 lr 0.00034623 rank 3
2023-02-25 06:57:36,258 DEBUG TRAIN Batch 25/100 loss 3.428472 loss_att 7.154763 loss_ctc 5.637601 loss_rnnt 2.299020 hw_loss 0.168080 lr 0.00034627 rank 1
2023-02-25 06:57:36,259 DEBUG TRAIN Batch 25/100 loss 4.900145 loss_att 10.158147 loss_ctc 8.705513 loss_rnnt 3.237940 hw_loss 0.193541 lr 0.00034616 rank 7
2023-02-25 06:57:36,259 DEBUG TRAIN Batch 25/100 loss 1.508624 loss_att 3.816460 loss_ctc 2.616501 loss_rnnt 0.808410 hw_loss 0.170495 lr 0.00034621 rank 4
2023-02-25 06:57:36,262 DEBUG TRAIN Batch 25/100 loss 1.900652 loss_att 5.692941 loss_ctc 3.819802 loss_rnnt 0.802699 hw_loss 0.156766 lr 0.00034619 rank 6
2023-02-25 06:57:36,306 DEBUG TRAIN Batch 25/100 loss 6.033077 loss_att 10.495433 loss_ctc 11.963835 loss_rnnt 4.211942 hw_loss 0.258554 lr 0.00034617 rank 5
2023-02-25 06:58:47,775 DEBUG TRAIN Batch 25/200 loss 3.646515 loss_att 4.035451 loss_ctc 3.880578 loss_rnnt 3.399688 hw_loss 0.258436 lr 0.00034613 rank 0
2023-02-25 06:58:47,779 DEBUG TRAIN Batch 25/200 loss 12.016490 loss_att 14.446356 loss_ctc 16.740522 loss_rnnt 10.820690 hw_loss 0.149915 lr 0.00034618 rank 1
2023-02-25 06:58:47,782 DEBUG TRAIN Batch 25/200 loss 5.069458 loss_att 8.972926 loss_ctc 8.620905 loss_rnnt 3.709494 hw_loss 0.198270 lr 0.00034615 rank 3
2023-02-25 06:58:47,782 DEBUG TRAIN Batch 25/200 loss 8.322528 loss_att 13.476942 loss_ctc 10.105722 loss_rnnt 6.924900 hw_loss 0.241848 lr 0.00034608 rank 7
2023-02-25 06:58:47,783 DEBUG TRAIN Batch 25/200 loss 10.991734 loss_att 13.232609 loss_ctc 19.859289 loss_rnnt 9.275844 hw_loss 0.160078 lr 0.00034611 rank 6
2023-02-25 06:58:47,785 DEBUG TRAIN Batch 25/200 loss 9.987281 loss_att 11.173252 loss_ctc 11.354889 loss_rnnt 9.494310 hw_loss 0.137676 lr 0.00034612 rank 2
2023-02-25 06:58:47,786 DEBUG TRAIN Batch 25/200 loss 5.805846 loss_att 9.014424 loss_ctc 7.702357 loss_rnnt 4.766601 hw_loss 0.271241 lr 0.00034613 rank 4
2023-02-25 06:58:47,788 DEBUG TRAIN Batch 25/200 loss 5.500478 loss_att 8.129522 loss_ctc 7.414930 loss_rnnt 4.557895 hw_loss 0.302838 lr 0.00034609 rank 5
2023-02-25 07:00:00,164 DEBUG TRAIN Batch 25/300 loss 6.742431 loss_att 11.189981 loss_ctc 14.424063 loss_rnnt 4.718218 hw_loss 0.207159 lr 0.00034606 rank 3
2023-02-25 07:00:00,167 DEBUG TRAIN Batch 25/300 loss 5.887256 loss_att 8.196264 loss_ctc 9.900676 loss_rnnt 4.808765 hw_loss 0.152937 lr 0.00034602 rank 6
2023-02-25 07:00:00,182 DEBUG TRAIN Batch 25/300 loss 17.646191 loss_att 19.201397 loss_ctc 23.850540 loss_rnnt 16.435411 hw_loss 0.135920 lr 0.00034610 rank 1
2023-02-25 07:00:00,182 DEBUG TRAIN Batch 25/300 loss 6.754003 loss_att 8.997442 loss_ctc 7.450727 loss_rnnt 6.130048 hw_loss 0.154444 lr 0.00034604 rank 2
2023-02-25 07:00:00,182 DEBUG TRAIN Batch 25/300 loss 10.156130 loss_att 12.060577 loss_ctc 14.914211 loss_rnnt 9.010893 hw_loss 0.243631 lr 0.00034600 rank 7
2023-02-25 07:00:00,184 DEBUG TRAIN Batch 25/300 loss 11.326436 loss_att 14.357471 loss_ctc 20.529907 loss_rnnt 9.398643 hw_loss 0.177104 lr 0.00034604 rank 0
2023-02-25 07:00:00,185 DEBUG TRAIN Batch 25/300 loss 10.633057 loss_att 12.886724 loss_ctc 17.639811 loss_rnnt 9.173103 hw_loss 0.140598 lr 0.00034600 rank 5
2023-02-25 07:00:00,185 DEBUG TRAIN Batch 25/300 loss 5.694359 loss_att 9.325444 loss_ctc 9.679946 loss_rnnt 4.350126 hw_loss 0.162383 lr 0.00034605 rank 4
2023-02-25 07:01:11,719 DEBUG TRAIN Batch 25/400 loss 6.999600 loss_att 9.592751 loss_ctc 11.171863 loss_rnnt 5.812956 hw_loss 0.209463 lr 0.00034596 rank 0
2023-02-25 07:01:11,721 DEBUG TRAIN Batch 25/400 loss 14.291782 loss_att 17.528431 loss_ctc 22.760868 loss_rnnt 12.415857 hw_loss 0.186342 lr 0.00034596 rank 2
2023-02-25 07:01:11,721 DEBUG TRAIN Batch 25/400 loss 8.934899 loss_att 13.027354 loss_ctc 11.323364 loss_rnnt 7.683436 hw_loss 0.214705 lr 0.00034594 rank 6
2023-02-25 07:01:11,722 DEBUG TRAIN Batch 25/400 loss 6.036322 loss_att 8.999466 loss_ctc 6.828401 loss_rnnt 5.208378 hw_loss 0.243195 lr 0.00034598 rank 3
2023-02-25 07:01:11,724 DEBUG TRAIN Batch 25/400 loss 6.461129 loss_att 8.991503 loss_ctc 8.388022 loss_rnnt 5.578863 hw_loss 0.223636 lr 0.00034602 rank 1
2023-02-25 07:01:11,725 DEBUG TRAIN Batch 25/400 loss 12.208996 loss_att 12.948786 loss_ctc 14.981944 loss_rnnt 11.549259 hw_loss 0.266346 lr 0.00034592 rank 5
2023-02-25 07:01:11,726 DEBUG TRAIN Batch 25/400 loss 14.106118 loss_att 18.959396 loss_ctc 20.607389 loss_rnnt 12.156059 hw_loss 0.211061 lr 0.00034597 rank 4
2023-02-25 07:01:11,727 DEBUG TRAIN Batch 25/400 loss 6.914400 loss_att 9.079100 loss_ctc 9.385729 loss_rnnt 6.065701 hw_loss 0.161717 lr 0.00034591 rank 7
2023-02-25 07:02:21,888 DEBUG TRAIN Batch 25/500 loss 8.290235 loss_att 11.629194 loss_ctc 14.404238 loss_rnnt 6.675314 hw_loss 0.247365 lr 0.00034588 rank 0
2023-02-25 07:02:21,894 DEBUG TRAIN Batch 25/500 loss 5.026393 loss_att 6.910100 loss_ctc 6.382037 loss_rnnt 4.385727 hw_loss 0.155947 lr 0.00034590 rank 3
2023-02-25 07:02:21,894 DEBUG TRAIN Batch 25/500 loss 8.497631 loss_att 10.570851 loss_ctc 12.198787 loss_rnnt 7.494229 hw_loss 0.178631 lr 0.00034593 rank 1
2023-02-25 07:02:21,895 DEBUG TRAIN Batch 25/500 loss 12.029048 loss_att 14.787006 loss_ctc 21.421682 loss_rnnt 10.103445 hw_loss 0.228111 lr 0.00034586 rank 6
2023-02-25 07:02:21,898 DEBUG TRAIN Batch 25/500 loss 9.647588 loss_att 12.976542 loss_ctc 12.126615 loss_rnnt 8.521388 hw_loss 0.243509 lr 0.00034587 rank 2
2023-02-25 07:02:21,898 DEBUG TRAIN Batch 25/500 loss 12.082761 loss_att 16.100985 loss_ctc 16.279045 loss_rnnt 10.595670 hw_loss 0.232390 lr 0.00034583 rank 7
2023-02-25 07:02:21,900 DEBUG TRAIN Batch 25/500 loss 6.932204 loss_att 7.841606 loss_ctc 8.212514 loss_rnnt 6.494086 hw_loss 0.160369 lr 0.00034584 rank 5
2023-02-25 07:02:21,902 DEBUG TRAIN Batch 25/500 loss 5.382187 loss_att 8.090591 loss_ctc 9.780610 loss_rnnt 4.107831 hw_loss 0.274160 lr 0.00034588 rank 4
2023-02-25 07:03:31,790 DEBUG TRAIN Batch 25/600 loss 11.700981 loss_att 12.775850 loss_ctc 15.127782 loss_rnnt 10.872396 hw_loss 0.293820 lr 0.00034579 rank 0
2023-02-25 07:03:31,792 DEBUG TRAIN Batch 25/600 loss 6.236984 loss_att 9.449469 loss_ctc 10.003101 loss_rnnt 4.950483 hw_loss 0.265977 lr 0.00034585 rank 1
2023-02-25 07:03:31,797 DEBUG TRAIN Batch 25/600 loss 7.150976 loss_att 8.613552 loss_ctc 7.993796 loss_rnnt 6.580185 hw_loss 0.311061 lr 0.00034579 rank 2
2023-02-25 07:03:31,801 DEBUG TRAIN Batch 25/600 loss 9.400473 loss_att 10.513579 loss_ctc 12.671718 loss_rnnt 8.610363 hw_loss 0.246229 lr 0.00034580 rank 4
2023-02-25 07:03:31,802 DEBUG TRAIN Batch 25/600 loss 13.991376 loss_att 15.188948 loss_ctc 18.003838 loss_rnnt 13.094150 hw_loss 0.230095 lr 0.00034576 rank 5
2023-02-25 07:03:31,802 DEBUG TRAIN Batch 25/600 loss 6.333394 loss_att 7.772051 loss_ctc 7.408423 loss_rnnt 5.817673 hw_loss 0.158723 lr 0.00034578 rank 6
2023-02-25 07:03:31,803 DEBUG TRAIN Batch 25/600 loss 6.461086 loss_att 6.904508 loss_ctc 9.076152 loss_rnnt 5.882134 hw_loss 0.265485 lr 0.00034581 rank 3
2023-02-25 07:03:31,805 DEBUG TRAIN Batch 25/600 loss 5.662961 loss_att 6.357428 loss_ctc 7.014028 loss_rnnt 5.191602 hw_loss 0.285608 lr 0.00034575 rank 7
2023-02-25 07:04:44,297 DEBUG TRAIN Batch 25/700 loss 4.095734 loss_att 6.474631 loss_ctc 7.340612 loss_rnnt 3.052049 hw_loss 0.253602 lr 0.00034567 rank 5
2023-02-25 07:04:44,307 DEBUG TRAIN Batch 25/700 loss 30.104940 loss_att 32.209503 loss_ctc 53.871788 loss_rnnt 26.409681 hw_loss 0.197692 lr 0.00034572 rank 4
2023-02-25 07:04:44,307 DEBUG TRAIN Batch 25/700 loss 3.940899 loss_att 6.881632 loss_ctc 6.280553 loss_rnnt 2.985385 hw_loss 0.103901 lr 0.00034567 rank 7
2023-02-25 07:04:44,323 DEBUG TRAIN Batch 25/700 loss 4.044414 loss_att 7.808400 loss_ctc 5.930596 loss_rnnt 2.983751 hw_loss 0.105701 lr 0.00034571 rank 2
2023-02-25 07:04:44,329 DEBUG TRAIN Batch 25/700 loss 7.650845 loss_att 9.526951 loss_ctc 9.813040 loss_rnnt 6.845453 hw_loss 0.266019 lr 0.00034569 rank 6
2023-02-25 07:04:44,348 DEBUG TRAIN Batch 25/700 loss 13.821500 loss_att 15.534822 loss_ctc 23.660604 loss_rnnt 12.130596 hw_loss 0.068171 lr 0.00034577 rank 1
2023-02-25 07:04:44,353 DEBUG TRAIN Batch 25/700 loss 9.312807 loss_att 12.843852 loss_ctc 13.830360 loss_rnnt 7.849847 hw_loss 0.289519 lr 0.00034571 rank 0
2023-02-25 07:04:44,357 DEBUG TRAIN Batch 25/700 loss 6.321527 loss_att 11.123605 loss_ctc 10.777857 loss_rnnt 4.635260 hw_loss 0.246889 lr 0.00034573 rank 3
2023-02-25 07:05:54,987 DEBUG TRAIN Batch 25/800 loss 12.961720 loss_att 16.820229 loss_ctc 18.220894 loss_rnnt 11.418742 hw_loss 0.131352 lr 0.00034563 rank 2
2023-02-25 07:05:55,003 DEBUG TRAIN Batch 25/800 loss 9.417364 loss_att 10.112425 loss_ctc 20.640486 loss_rnnt 7.731953 hw_loss 0.093717 lr 0.00034569 rank 1
2023-02-25 07:05:55,003 DEBUG TRAIN Batch 25/800 loss 10.251953 loss_att 14.019442 loss_ctc 12.125999 loss_rnnt 9.145113 hw_loss 0.194006 lr 0.00034561 rank 6
2023-02-25 07:05:55,004 DEBUG TRAIN Batch 25/800 loss 10.037815 loss_att 15.622591 loss_ctc 20.414812 loss_rnnt 7.412224 hw_loss 0.234443 lr 0.00034565 rank 3
2023-02-25 07:05:55,004 DEBUG TRAIN Batch 25/800 loss 4.497645 loss_att 7.504059 loss_ctc 5.044764 loss_rnnt 3.768853 hw_loss 0.102300 lr 0.00034563 rank 0
2023-02-25 07:05:55,004 DEBUG TRAIN Batch 25/800 loss 18.429449 loss_att 16.455006 loss_ctc 22.135382 loss_rnnt 18.206953 hw_loss 0.231113 lr 0.00034563 rank 4
2023-02-25 07:05:55,011 DEBUG TRAIN Batch 25/800 loss 8.276903 loss_att 10.324818 loss_ctc 12.386180 loss_rnnt 7.175206 hw_loss 0.270396 lr 0.00034558 rank 7
2023-02-25 07:05:55,013 DEBUG TRAIN Batch 25/800 loss 14.088269 loss_att 14.731878 loss_ctc 19.518684 loss_rnnt 13.161120 hw_loss 0.139447 lr 0.00034559 rank 5
2023-02-25 07:07:05,172 DEBUG TRAIN Batch 25/900 loss 8.970903 loss_att 11.422709 loss_ctc 12.833847 loss_rnnt 7.901660 hw_loss 0.119669 lr 0.00034555 rank 0
2023-02-25 07:07:05,176 DEBUG TRAIN Batch 25/900 loss 8.230054 loss_att 11.946983 loss_ctc 15.446793 loss_rnnt 6.405118 hw_loss 0.223721 lr 0.00034557 rank 3
2023-02-25 07:07:05,178 DEBUG TRAIN Batch 25/900 loss 7.198824 loss_att 9.874480 loss_ctc 8.929440 loss_rnnt 6.352489 hw_loss 0.150852 lr 0.00034551 rank 5
2023-02-25 07:07:05,178 DEBUG TRAIN Batch 25/900 loss 10.769838 loss_att 11.787321 loss_ctc 12.602688 loss_rnnt 10.198760 hw_loss 0.231004 lr 0.00034555 rank 4
2023-02-25 07:07:05,181 DEBUG TRAIN Batch 25/900 loss 2.285022 loss_att 5.656844 loss_ctc 4.516777 loss_rnnt 1.247619 hw_loss 0.122757 lr 0.00034550 rank 7
2023-02-25 07:07:05,181 DEBUG TRAIN Batch 25/900 loss 19.489342 loss_att 23.425695 loss_ctc 31.957384 loss_rnnt 16.912958 hw_loss 0.237573 lr 0.00034560 rank 1
2023-02-25 07:07:05,181 DEBUG TRAIN Batch 25/900 loss 4.477235 loss_att 8.365872 loss_ctc 8.630866 loss_rnnt 3.056892 hw_loss 0.166496 lr 0.00034553 rank 6
2023-02-25 07:07:05,183 DEBUG TRAIN Batch 25/900 loss 4.315719 loss_att 5.765996 loss_ctc 5.442788 loss_rnnt 3.740468 hw_loss 0.252973 lr 0.00034554 rank 2
2023-02-25 07:08:16,258 DEBUG TRAIN Batch 25/1000 loss 13.223851 loss_att 13.452286 loss_ctc 20.985313 loss_rnnt 12.012905 hw_loss 0.244493 lr 0.00034552 rank 1
2023-02-25 07:08:16,269 DEBUG TRAIN Batch 25/1000 loss 6.969087 loss_att 9.415132 loss_ctc 10.110888 loss_rnnt 5.979762 hw_loss 0.152267 lr 0.00034547 rank 4
2023-02-25 07:08:16,272 DEBUG TRAIN Batch 25/1000 loss 7.166635 loss_att 7.818989 loss_ctc 8.707306 loss_rnnt 6.722297 hw_loss 0.203334 lr 0.00034546 rank 2
2023-02-25 07:08:16,273 DEBUG TRAIN Batch 25/1000 loss 5.381089 loss_att 7.781569 loss_ctc 9.147518 loss_rnnt 4.330035 hw_loss 0.128939 lr 0.00034545 rank 6
2023-02-25 07:08:16,273 DEBUG TRAIN Batch 25/1000 loss 10.886985 loss_att 12.773951 loss_ctc 13.481497 loss_rnnt 10.026390 hw_loss 0.257375 lr 0.00034542 rank 7
2023-02-25 07:08:16,274 DEBUG TRAIN Batch 25/1000 loss 4.552477 loss_att 8.366142 loss_ctc 8.753882 loss_rnnt 3.143474 hw_loss 0.161404 lr 0.00034543 rank 5
2023-02-25 07:08:16,273 DEBUG TRAIN Batch 25/1000 loss 2.419365 loss_att 5.929124 loss_ctc 3.761123 loss_rnnt 1.404943 hw_loss 0.250442 lr 0.00034546 rank 0
2023-02-25 07:08:16,287 DEBUG TRAIN Batch 25/1000 loss 11.396544 loss_att 15.161848 loss_ctc 17.954163 loss_rnnt 9.611273 hw_loss 0.295990 lr 0.00034548 rank 3
2023-02-25 07:09:28,624 DEBUG TRAIN Batch 25/1100 loss 6.345356 loss_att 9.556175 loss_ctc 9.822788 loss_rnnt 5.134418 hw_loss 0.197093 lr 0.00034539 rank 4
2023-02-25 07:09:28,634 DEBUG TRAIN Batch 25/1100 loss 6.761673 loss_att 8.656075 loss_ctc 7.858493 loss_rnnt 6.165198 hw_loss 0.133786 lr 0.00034538 rank 0
2023-02-25 07:09:28,638 DEBUG TRAIN Batch 25/1100 loss 9.094482 loss_att 11.270652 loss_ctc 12.825898 loss_rnnt 8.065226 hw_loss 0.180937 lr 0.00034536 rank 6
2023-02-25 07:09:28,642 DEBUG TRAIN Batch 25/1100 loss 9.809045 loss_att 12.775009 loss_ctc 15.839643 loss_rnnt 8.336796 hw_loss 0.140582 lr 0.00034538 rank 2
2023-02-25 07:09:28,645 DEBUG TRAIN Batch 25/1100 loss 13.508670 loss_att 14.983545 loss_ctc 16.165033 loss_rnnt 12.751774 hw_loss 0.202010 lr 0.00034544 rank 1
2023-02-25 07:09:28,645 DEBUG TRAIN Batch 25/1100 loss 13.018811 loss_att 15.230708 loss_ctc 16.766762 loss_rnnt 11.947134 hw_loss 0.242949 lr 0.00034534 rank 7
2023-02-25 07:09:28,650 DEBUG TRAIN Batch 25/1100 loss 6.636155 loss_att 9.107271 loss_ctc 9.597112 loss_rnnt 5.640955 hw_loss 0.199090 lr 0.00034534 rank 5
2023-02-25 07:09:28,685 DEBUG TRAIN Batch 25/1100 loss 12.879107 loss_att 14.695507 loss_ctc 13.905397 loss_rnnt 12.282508 hw_loss 0.180897 lr 0.00034540 rank 3
2023-02-25 07:10:39,454 DEBUG TRAIN Batch 25/1200 loss 9.408401 loss_att 13.490623 loss_ctc 13.485773 loss_rnnt 7.886328 hw_loss 0.303709 lr 0.00034530 rank 0
2023-02-25 07:10:39,458 DEBUG TRAIN Batch 25/1200 loss 8.632220 loss_att 9.645540 loss_ctc 11.475398 loss_rnnt 7.904871 hw_loss 0.272991 lr 0.00034532 rank 3
2023-02-25 07:10:39,461 DEBUG TRAIN Batch 25/1200 loss 9.574846 loss_att 10.596560 loss_ctc 15.567137 loss_rnnt 8.422553 hw_loss 0.279335 lr 0.00034530 rank 2
2023-02-25 07:10:39,462 DEBUG TRAIN Batch 25/1200 loss 9.371673 loss_att 11.426124 loss_ctc 12.484371 loss_rnnt 8.388160 hw_loss 0.295493 lr 0.00034536 rank 1
2023-02-25 07:10:39,462 DEBUG TRAIN Batch 25/1200 loss 11.658637 loss_att 12.308244 loss_ctc 16.164650 loss_rnnt 10.795355 hw_loss 0.248551 lr 0.00034525 rank 7
2023-02-25 07:10:39,464 DEBUG TRAIN Batch 25/1200 loss 4.679811 loss_att 6.172812 loss_ctc 6.378250 loss_rnnt 4.097868 hw_loss 0.106659 lr 0.00034528 rank 6
2023-02-25 07:10:39,465 DEBUG TRAIN Batch 25/1200 loss 7.231989 loss_att 8.835240 loss_ctc 10.199086 loss_rnnt 6.422997 hw_loss 0.173864 lr 0.00034526 rank 5
2023-02-25 07:10:39,467 DEBUG TRAIN Batch 25/1200 loss 13.745819 loss_att 15.596957 loss_ctc 16.250999 loss_rnnt 12.942641 hw_loss 0.185488 lr 0.00034531 rank 4
2023-02-25 07:11:49,647 DEBUG TRAIN Batch 25/1300 loss 7.363797 loss_att 12.976212 loss_ctc 16.137541 loss_rnnt 4.953437 hw_loss 0.221335 lr 0.00034522 rank 0
2023-02-25 07:11:49,648 DEBUG TRAIN Batch 25/1300 loss 2.267455 loss_att 5.304212 loss_ctc 2.997493 loss_rnnt 1.492139 hw_loss 0.132425 lr 0.00034517 rank 7
2023-02-25 07:11:49,649 DEBUG TRAIN Batch 25/1300 loss 9.746187 loss_att 13.180066 loss_ctc 14.197159 loss_rnnt 8.400991 hw_loss 0.121794 lr 0.00034527 rank 1
2023-02-25 07:11:49,652 DEBUG TRAIN Batch 25/1300 loss 3.242110 loss_att 5.219255 loss_ctc 7.851729 loss_rnnt 2.144021 hw_loss 0.165083 lr 0.00034518 rank 5
2023-02-25 07:11:49,657 DEBUG TRAIN Batch 25/1300 loss 6.846093 loss_att 10.503338 loss_ctc 9.879480 loss_rnnt 5.590122 hw_loss 0.225133 lr 0.00034522 rank 4
2023-02-25 07:11:49,658 DEBUG TRAIN Batch 25/1300 loss 9.819414 loss_att 13.066910 loss_ctc 12.207599 loss_rnnt 8.758975 hw_loss 0.173467 lr 0.00034521 rank 2
2023-02-25 07:11:49,691 DEBUG TRAIN Batch 25/1300 loss 6.424363 loss_att 9.533052 loss_ctc 11.310938 loss_rnnt 4.972727 hw_loss 0.334415 lr 0.00034524 rank 3
2023-02-25 07:11:49,723 DEBUG TRAIN Batch 25/1300 loss 6.696171 loss_att 7.695729 loss_ctc 9.289003 loss_rnnt 5.979121 hw_loss 0.321427 lr 0.00034520 rank 6
2023-02-25 07:13:02,314 DEBUG TRAIN Batch 25/1400 loss 2.985375 loss_att 6.505770 loss_ctc 6.084689 loss_rnnt 1.736239 hw_loss 0.247153 lr 0.00034509 rank 7
2023-02-25 07:13:02,316 DEBUG TRAIN Batch 25/1400 loss 6.766364 loss_att 11.194490 loss_ctc 14.318155 loss_rnnt 4.783670 hw_loss 0.169055 lr 0.00034510 rank 5
2023-02-25 07:13:02,325 DEBUG TRAIN Batch 25/1400 loss 9.933178 loss_att 13.256975 loss_ctc 11.083224 loss_rnnt 8.995438 hw_loss 0.224328 lr 0.00034512 rank 6
2023-02-25 07:13:02,327 DEBUG TRAIN Batch 25/1400 loss 7.908196 loss_att 11.144166 loss_ctc 11.651762 loss_rnnt 6.715431 hw_loss 0.087052 lr 0.00034513 rank 0
2023-02-25 07:13:02,327 DEBUG TRAIN Batch 25/1400 loss 5.422134 loss_att 6.972413 loss_ctc 8.655711 loss_rnnt 4.554978 hw_loss 0.236169 lr 0.00034519 rank 1
2023-02-25 07:13:02,330 DEBUG TRAIN Batch 25/1400 loss 7.439579 loss_att 8.963264 loss_ctc 14.518619 loss_rnnt 6.078157 hw_loss 0.211523 lr 0.00034515 rank 3
2023-02-25 07:13:02,332 DEBUG TRAIN Batch 25/1400 loss 3.800019 loss_att 7.120100 loss_ctc 6.147426 loss_rnnt 2.703227 hw_loss 0.224604 lr 0.00034513 rank 2
2023-02-25 07:13:02,333 DEBUG TRAIN Batch 25/1400 loss 2.528398 loss_att 5.479501 loss_ctc 3.018206 loss_rnnt 1.791989 hw_loss 0.151651 lr 0.00034514 rank 4
2023-02-25 07:14:14,611 DEBUG TRAIN Batch 25/1500 loss 5.960103 loss_att 9.885526 loss_ctc 7.359021 loss_rnnt 4.879565 hw_loss 0.204246 lr 0.00034501 rank 7
2023-02-25 07:14:14,624 DEBUG TRAIN Batch 25/1500 loss 11.857845 loss_att 14.691875 loss_ctc 17.662783 loss_rnnt 10.424822 hw_loss 0.172924 lr 0.00034507 rank 3
2023-02-25 07:14:14,624 DEBUG TRAIN Batch 25/1500 loss 8.452873 loss_att 13.995648 loss_ctc 11.907916 loss_rnnt 6.810453 hw_loss 0.137236 lr 0.00034503 rank 6
2023-02-25 07:14:14,624 DEBUG TRAIN Batch 25/1500 loss 3.037012 loss_att 6.486453 loss_ctc 7.114879 loss_rnnt 1.674905 hw_loss 0.240944 lr 0.00034511 rank 1
2023-02-25 07:14:14,625 DEBUG TRAIN Batch 25/1500 loss 11.650554 loss_att 12.386332 loss_ctc 14.054107 loss_rnnt 11.082847 hw_loss 0.187644 lr 0.00034506 rank 4
2023-02-25 07:14:14,625 DEBUG TRAIN Batch 25/1500 loss 6.689582 loss_att 10.543301 loss_ctc 9.959670 loss_rnnt 5.428722 hw_loss 0.101447 lr 0.00034505 rank 2
2023-02-25 07:14:14,629 DEBUG TRAIN Batch 25/1500 loss 10.909883 loss_att 13.281907 loss_ctc 16.156046 loss_rnnt 9.616570 hw_loss 0.223913 lr 0.00034505 rank 0
2023-02-25 07:14:14,630 DEBUG TRAIN Batch 25/1500 loss 13.713348 loss_att 16.506741 loss_ctc 14.357024 loss_rnnt 12.972822 hw_loss 0.180046 lr 0.00034501 rank 5
2023-02-25 07:15:23,945 DEBUG TRAIN Batch 25/1600 loss 9.473866 loss_att 10.611600 loss_ctc 11.812105 loss_rnnt 8.846919 hw_loss 0.164319 lr 0.00034503 rank 1
2023-02-25 07:15:23,958 DEBUG TRAIN Batch 25/1600 loss 7.579545 loss_att 11.420212 loss_ctc 14.356838 loss_rnnt 5.784833 hw_loss 0.230511 lr 0.00034493 rank 5
2023-02-25 07:15:23,957 DEBUG TRAIN Batch 25/1600 loss 6.087204 loss_att 8.461950 loss_ctc 11.482339 loss_rnnt 4.775619 hw_loss 0.219907 lr 0.00034499 rank 3
2023-02-25 07:15:23,957 DEBUG TRAIN Batch 25/1600 loss 12.403772 loss_att 17.489807 loss_ctc 20.124969 loss_rnnt 10.245785 hw_loss 0.208665 lr 0.00034497 rank 0
2023-02-25 07:15:23,958 DEBUG TRAIN Batch 25/1600 loss 7.354431 loss_att 11.512933 loss_ctc 12.940716 loss_rnnt 5.630407 hw_loss 0.276535 lr 0.00034498 rank 4
2023-02-25 07:15:23,959 DEBUG TRAIN Batch 25/1600 loss 10.318710 loss_att 12.236794 loss_ctc 11.956522 loss_rnnt 9.582294 hw_loss 0.252046 lr 0.00034497 rank 2
2023-02-25 07:15:23,960 DEBUG TRAIN Batch 25/1600 loss 3.814794 loss_att 7.598791 loss_ctc 8.006348 loss_rnnt 2.394851 hw_loss 0.195505 lr 0.00034495 rank 6
2023-02-25 07:15:23,963 DEBUG TRAIN Batch 25/1600 loss 6.175481 loss_att 8.929075 loss_ctc 9.137750 loss_rnnt 5.133516 hw_loss 0.180519 lr 0.00034492 rank 7
2023-02-25 07:16:34,643 DEBUG TRAIN Batch 25/1700 loss 10.525674 loss_att 12.721337 loss_ctc 18.782469 loss_rnnt 8.851852 hw_loss 0.250843 lr 0.00034489 rank 0
2023-02-25 07:16:34,655 DEBUG TRAIN Batch 25/1700 loss 10.499095 loss_att 16.947975 loss_ctc 17.963848 loss_rnnt 8.082000 hw_loss 0.247536 lr 0.00034484 rank 7
2023-02-25 07:16:34,659 DEBUG TRAIN Batch 25/1700 loss 8.302271 loss_att 11.798648 loss_ctc 14.614121 loss_rnnt 6.627501 hw_loss 0.251090 lr 0.00034487 rank 6
2023-02-25 07:16:34,660 DEBUG TRAIN Batch 25/1700 loss 4.543688 loss_att 7.077510 loss_ctc 8.338351 loss_rnnt 3.387909 hw_loss 0.268238 lr 0.00034491 rank 3
2023-02-25 07:16:34,661 DEBUG TRAIN Batch 25/1700 loss 20.949112 loss_att 22.391077 loss_ctc 24.652771 loss_rnnt 20.059099 hw_loss 0.202124 lr 0.00034489 rank 2
2023-02-25 07:16:34,661 DEBUG TRAIN Batch 25/1700 loss 7.369928 loss_att 10.391148 loss_ctc 8.498404 loss_rnnt 6.517960 hw_loss 0.182363 lr 0.00034489 rank 4
2023-02-25 07:16:34,661 DEBUG TRAIN Batch 25/1700 loss 10.502158 loss_att 13.885912 loss_ctc 12.611170 loss_rnnt 9.461589 hw_loss 0.154907 lr 0.00034494 rank 1
2023-02-25 07:16:34,675 DEBUG TRAIN Batch 25/1700 loss 9.477634 loss_att 12.962494 loss_ctc 15.698070 loss_rnnt 7.866034 hw_loss 0.159819 lr 0.00034485 rank 5
2023-02-25 07:17:47,669 DEBUG TRAIN Batch 25/1800 loss 3.044054 loss_att 5.758281 loss_ctc 3.882615 loss_rnnt 2.260948 hw_loss 0.240850 lr 0.00034486 rank 1
2023-02-25 07:17:47,670 DEBUG TRAIN Batch 25/1800 loss 8.576656 loss_att 11.229441 loss_ctc 13.181923 loss_rnnt 7.321198 hw_loss 0.207875 lr 0.00034476 rank 7
2023-02-25 07:17:47,672 DEBUG TRAIN Batch 25/1800 loss 9.408244 loss_att 11.636195 loss_ctc 13.760612 loss_rnnt 8.284334 hw_loss 0.183757 lr 0.00034481 rank 0
2023-02-25 07:17:47,674 DEBUG TRAIN Batch 25/1800 loss 6.353885 loss_att 10.272938 loss_ctc 10.114347 loss_rnnt 4.922087 hw_loss 0.274860 lr 0.00034479 rank 6
2023-02-25 07:17:47,676 DEBUG TRAIN Batch 25/1800 loss 5.262051 loss_att 8.925251 loss_ctc 9.291677 loss_rnnt 3.902967 hw_loss 0.167176 lr 0.00034483 rank 3
2023-02-25 07:17:47,679 DEBUG TRAIN Batch 25/1800 loss 11.714521 loss_att 16.630100 loss_ctc 17.259308 loss_rnnt 9.895864 hw_loss 0.180443 lr 0.00034480 rank 2
2023-02-25 07:17:47,682 DEBUG TRAIN Batch 25/1800 loss 8.583276 loss_att 10.818168 loss_ctc 11.438106 loss_rnnt 7.673198 hw_loss 0.154602 lr 0.00034477 rank 5
2023-02-25 07:17:47,723 DEBUG TRAIN Batch 25/1800 loss 14.342566 loss_att 15.583685 loss_ctc 19.618008 loss_rnnt 13.228519 hw_loss 0.304556 lr 0.00034481 rank 4
2023-02-25 07:18:58,476 DEBUG TRAIN Batch 25/1900 loss 11.495064 loss_att 12.178599 loss_ctc 14.665323 loss_rnnt 10.777319 hw_loss 0.296882 lr 0.00034471 rank 6
2023-02-25 07:18:58,476 DEBUG TRAIN Batch 25/1900 loss 9.916664 loss_att 11.099218 loss_ctc 14.250193 loss_rnnt 8.980501 hw_loss 0.228467 lr 0.00034473 rank 4
2023-02-25 07:18:58,477 DEBUG TRAIN Batch 25/1900 loss 2.951265 loss_att 6.998199 loss_ctc 6.629283 loss_rnnt 1.563555 hw_loss 0.164852 lr 0.00034478 rank 1
2023-02-25 07:18:58,478 DEBUG TRAIN Batch 25/1900 loss 6.750665 loss_att 11.602232 loss_ctc 10.794806 loss_rnnt 5.133540 hw_loss 0.201735 lr 0.00034474 rank 3
2023-02-25 07:18:58,480 DEBUG TRAIN Batch 25/1900 loss 3.775577 loss_att 5.343838 loss_ctc 5.758698 loss_rnnt 3.063096 hw_loss 0.252024 lr 0.00034472 rank 2
2023-02-25 07:18:58,481 DEBUG TRAIN Batch 25/1900 loss 10.482675 loss_att 14.698357 loss_ctc 18.726181 loss_rnnt 8.419561 hw_loss 0.226579 lr 0.00034472 rank 0
2023-02-25 07:18:58,482 DEBUG TRAIN Batch 25/1900 loss 11.303963 loss_att 16.315779 loss_ctc 13.882978 loss_rnnt 9.870042 hw_loss 0.164415 lr 0.00034468 rank 7
2023-02-25 07:18:58,482 DEBUG TRAIN Batch 25/1900 loss 9.496809 loss_att 9.394995 loss_ctc 14.064883 loss_rnnt 8.794372 hw_loss 0.213231 lr 0.00034469 rank 5
2023-02-25 07:20:08,789 DEBUG TRAIN Batch 25/2000 loss 3.436203 loss_att 5.978052 loss_ctc 5.589854 loss_rnnt 2.536093 hw_loss 0.196099 lr 0.00034462 rank 6
2023-02-25 07:20:08,801 DEBUG TRAIN Batch 25/2000 loss 3.431393 loss_att 6.345713 loss_ctc 6.695426 loss_rnnt 2.327668 hw_loss 0.160607 lr 0.00034464 rank 0
2023-02-25 07:20:08,803 DEBUG TRAIN Batch 25/2000 loss 8.404703 loss_att 12.568580 loss_ctc 15.224009 loss_rnnt 6.591810 hw_loss 0.132894 lr 0.00034466 rank 3
2023-02-25 07:20:08,805 DEBUG TRAIN Batch 25/2000 loss 6.771101 loss_att 10.362785 loss_ctc 8.963649 loss_rnnt 5.661640 hw_loss 0.185222 lr 0.00034470 rank 1
2023-02-25 07:20:08,807 DEBUG TRAIN Batch 25/2000 loss 4.554593 loss_att 7.506417 loss_ctc 8.968410 loss_rnnt 3.277307 hw_loss 0.184523 lr 0.00034465 rank 4
2023-02-25 07:20:08,808 DEBUG TRAIN Batch 25/2000 loss 18.669165 loss_att 22.180414 loss_ctc 32.507671 loss_rnnt 16.023750 hw_loss 0.183809 lr 0.00034460 rank 5
2023-02-25 07:20:08,813 DEBUG TRAIN Batch 25/2000 loss 11.471227 loss_att 14.775785 loss_ctc 16.974586 loss_rnnt 9.966915 hw_loss 0.205536 lr 0.00034464 rank 2
2023-02-25 07:20:08,820 DEBUG TRAIN Batch 25/2000 loss 3.722200 loss_att 5.034009 loss_ctc 4.751186 loss_rnnt 3.246944 hw_loss 0.141931 lr 0.00034460 rank 7
2023-02-25 07:21:21,382 DEBUG TRAIN Batch 25/2100 loss 8.250592 loss_att 11.626228 loss_ctc 10.745232 loss_rnnt 7.169556 hw_loss 0.137421 lr 0.00034456 rank 0
2023-02-25 07:21:21,382 DEBUG TRAIN Batch 25/2100 loss 4.212599 loss_att 6.751742 loss_ctc 6.238595 loss_rnnt 3.307523 hw_loss 0.238341 lr 0.00034451 rank 7
2023-02-25 07:21:21,382 DEBUG TRAIN Batch 25/2100 loss 10.232581 loss_att 14.221937 loss_ctc 14.450659 loss_rnnt 8.770029 hw_loss 0.191757 lr 0.00034462 rank 1
2023-02-25 07:21:21,383 DEBUG TRAIN Batch 25/2100 loss 22.023302 loss_att 23.028973 loss_ctc 31.740211 loss_rnnt 20.416096 hw_loss 0.207159 lr 0.00034454 rank 6
2023-02-25 07:21:21,383 DEBUG TRAIN Batch 25/2100 loss 3.234527 loss_att 6.365483 loss_ctc 2.790648 loss_rnnt 2.560426 hw_loss 0.200799 lr 0.00034458 rank 3
2023-02-25 07:21:21,387 DEBUG TRAIN Batch 25/2100 loss 4.403672 loss_att 7.992388 loss_ctc 6.266449 loss_rnnt 3.355478 hw_loss 0.153900 lr 0.00034457 rank 4
2023-02-25 07:21:21,390 DEBUG TRAIN Batch 25/2100 loss 7.651903 loss_att 11.032029 loss_ctc 20.553970 loss_rnnt 5.116823 hw_loss 0.260211 lr 0.00034452 rank 5
2023-02-25 07:21:21,395 DEBUG TRAIN Batch 25/2100 loss 9.576799 loss_att 13.093779 loss_ctc 11.354018 loss_rnnt 8.566391 hw_loss 0.131345 lr 0.00034456 rank 2
2023-02-25 07:22:32,892 DEBUG TRAIN Batch 25/2200 loss 4.392176 loss_att 6.828935 loss_ctc 6.202142 loss_rnnt 3.562669 hw_loss 0.189048 lr 0.00034450 rank 3
2023-02-25 07:22:32,893 DEBUG TRAIN Batch 25/2200 loss 8.265773 loss_att 11.130747 loss_ctc 10.545842 loss_rnnt 7.293797 hw_loss 0.178073 lr 0.00034454 rank 1
2023-02-25 07:22:32,894 DEBUG TRAIN Batch 25/2200 loss 3.204757 loss_att 6.073817 loss_ctc 5.449564 loss_rnnt 2.279573 hw_loss 0.097622 lr 0.00034448 rank 2
2023-02-25 07:22:32,895 DEBUG TRAIN Batch 25/2200 loss 7.487055 loss_att 11.985085 loss_ctc 11.292912 loss_rnnt 5.963623 hw_loss 0.218210 lr 0.00034448 rank 0
2023-02-25 07:22:32,897 DEBUG TRAIN Batch 25/2200 loss 10.352180 loss_att 14.050627 loss_ctc 17.888346 loss_rnnt 8.489706 hw_loss 0.221179 lr 0.00034443 rank 7
2023-02-25 07:22:32,899 DEBUG TRAIN Batch 25/2200 loss 3.971048 loss_att 9.431725 loss_ctc 5.802216 loss_rnnt 2.507240 hw_loss 0.239093 lr 0.00034446 rank 6
2023-02-25 07:22:32,901 DEBUG TRAIN Batch 25/2200 loss 7.812450 loss_att 11.210056 loss_ctc 10.591887 loss_rnnt 6.653233 hw_loss 0.204570 lr 0.00034448 rank 4
2023-02-25 07:22:32,907 DEBUG TRAIN Batch 25/2200 loss 8.996551 loss_att 11.783835 loss_ctc 12.728670 loss_rnnt 7.854572 hw_loss 0.162946 lr 0.00034444 rank 5
2023-02-25 07:23:42,670 DEBUG TRAIN Batch 25/2300 loss 7.391456 loss_att 10.310837 loss_ctc 10.469718 loss_rnnt 6.234010 hw_loss 0.305877 lr 0.00034445 rank 1
2023-02-25 07:23:42,670 DEBUG TRAIN Batch 25/2300 loss 6.786728 loss_att 9.876846 loss_ctc 10.969825 loss_rnnt 5.486918 hw_loss 0.232575 lr 0.00034440 rank 0
2023-02-25 07:23:42,676 DEBUG TRAIN Batch 25/2300 loss 5.245526 loss_att 9.261899 loss_ctc 9.020744 loss_rnnt 3.844090 hw_loss 0.177751 lr 0.00034438 rank 6
2023-02-25 07:23:42,676 DEBUG TRAIN Batch 25/2300 loss 5.425035 loss_att 9.582072 loss_ctc 7.780454 loss_rnnt 4.130541 hw_loss 0.279431 lr 0.00034436 rank 5
2023-02-25 07:23:42,676 DEBUG TRAIN Batch 25/2300 loss 3.293397 loss_att 5.522742 loss_ctc 5.048014 loss_rnnt 2.554986 hw_loss 0.109862 lr 0.00034435 rank 7
2023-02-25 07:23:42,676 DEBUG TRAIN Batch 25/2300 loss 15.962481 loss_att 17.419930 loss_ctc 20.801125 loss_rnnt 14.875286 hw_loss 0.282286 lr 0.00034442 rank 3
2023-02-25 07:23:42,688 DEBUG TRAIN Batch 25/2300 loss 3.453984 loss_att 5.071864 loss_ctc 2.896062 loss_rnnt 3.126767 hw_loss 0.146308 lr 0.00034439 rank 2
2023-02-25 07:23:42,729 DEBUG TRAIN Batch 25/2300 loss 8.543593 loss_att 9.861332 loss_ctc 14.631128 loss_rnnt 7.326851 hw_loss 0.265357 lr 0.00034440 rank 4
2023-02-25 07:24:53,903 DEBUG TRAIN Batch 25/2400 loss 10.843372 loss_att 11.340301 loss_ctc 13.772178 loss_rnnt 10.258354 hw_loss 0.178358 lr 0.00034437 rank 1
2023-02-25 07:24:53,903 DEBUG TRAIN Batch 25/2400 loss 6.084954 loss_att 7.251452 loss_ctc 7.585202 loss_rnnt 5.570028 hw_loss 0.152988 lr 0.00034432 rank 0
2023-02-25 07:24:53,904 DEBUG TRAIN Batch 25/2400 loss 15.196777 loss_att 16.995052 loss_ctc 20.203045 loss_rnnt 14.044157 hw_loss 0.235242 lr 0.00034431 rank 2
2023-02-25 07:24:53,906 DEBUG TRAIN Batch 25/2400 loss 15.813536 loss_att 15.475721 loss_ctc 19.455173 loss_rnnt 15.248218 hw_loss 0.276241 lr 0.00034433 rank 3
2023-02-25 07:24:53,907 DEBUG TRAIN Batch 25/2400 loss 7.972490 loss_att 11.298325 loss_ctc 10.492254 loss_rnnt 6.881685 hw_loss 0.168130 lr 0.00034430 rank 6
2023-02-25 07:24:53,908 DEBUG TRAIN Batch 25/2400 loss 12.280406 loss_att 14.659136 loss_ctc 20.209137 loss_rnnt 10.640779 hw_loss 0.200093 lr 0.00034427 rank 7
2023-02-25 07:24:53,930 DEBUG TRAIN Batch 25/2400 loss 7.672521 loss_att 10.383001 loss_ctc 8.908640 loss_rnnt 6.832496 hw_loss 0.249587 lr 0.00034432 rank 4
2023-02-25 07:24:53,958 DEBUG TRAIN Batch 25/2400 loss 7.663203 loss_att 10.254357 loss_ctc 9.456964 loss_rnnt 6.755733 hw_loss 0.281382 lr 0.00034428 rank 5
2023-02-25 07:26:07,743 DEBUG TRAIN Batch 25/2500 loss 14.217079 loss_att 13.591252 loss_ctc 20.643293 loss_rnnt 13.287335 hw_loss 0.371400 lr 0.00034429 rank 1
2023-02-25 07:26:07,746 DEBUG TRAIN Batch 25/2500 loss 6.095178 loss_att 7.871204 loss_ctc 7.926410 loss_rnnt 5.450392 hw_loss 0.085155 lr 0.00034423 rank 0
2023-02-25 07:26:07,754 DEBUG TRAIN Batch 25/2500 loss 2.860508 loss_att 4.193531 loss_ctc 4.249587 loss_rnnt 2.250074 hw_loss 0.297410 lr 0.00034425 rank 3
2023-02-25 07:26:07,757 DEBUG TRAIN Batch 25/2500 loss 15.558794 loss_att 16.551334 loss_ctc 24.905447 loss_rnnt 14.014009 hw_loss 0.187603 lr 0.00034422 rank 6
2023-02-25 07:26:07,758 DEBUG TRAIN Batch 25/2500 loss 9.418339 loss_att 10.202745 loss_ctc 13.691716 loss_rnnt 8.557008 hw_loss 0.252498 lr 0.00034419 rank 7
2023-02-25 07:26:07,758 DEBUG TRAIN Batch 25/2500 loss 12.801575 loss_att 15.882786 loss_ctc 19.895296 loss_rnnt 11.084568 hw_loss 0.290504 lr 0.00034423 rank 2
2023-02-25 07:26:07,761 DEBUG TRAIN Batch 25/2500 loss 12.153215 loss_att 14.805415 loss_ctc 18.849648 loss_rnnt 10.625487 hw_loss 0.195810 lr 0.00034424 rank 4
2023-02-25 07:26:07,806 DEBUG TRAIN Batch 25/2500 loss 12.169273 loss_att 15.857351 loss_ctc 18.063793 loss_rnnt 10.500789 hw_loss 0.271750 lr 0.00034420 rank 5
2023-02-25 07:27:17,762 DEBUG TRAIN Batch 25/2600 loss 1.364020 loss_att 3.835345 loss_ctc 3.397239 loss_rnnt 0.506094 hw_loss 0.173560 lr 0.00034411 rank 7
2023-02-25 07:27:17,776 DEBUG TRAIN Batch 25/2600 loss 7.100792 loss_att 8.766985 loss_ctc 10.431595 loss_rnnt 6.200501 hw_loss 0.230521 lr 0.00034413 rank 6
2023-02-25 07:27:17,775 DEBUG TRAIN Batch 25/2600 loss 8.043419 loss_att 10.887917 loss_ctc 9.852469 loss_rnnt 7.168043 hw_loss 0.122380 lr 0.00034421 rank 1
2023-02-25 07:27:17,776 DEBUG TRAIN Batch 25/2600 loss 7.220397 loss_att 7.864668 loss_ctc 10.529203 loss_rnnt 6.556089 hw_loss 0.176774 lr 0.00034415 rank 0
2023-02-25 07:27:17,776 DEBUG TRAIN Batch 25/2600 loss 13.451544 loss_att 18.403591 loss_ctc 17.909351 loss_rnnt 11.802245 hw_loss 0.120963 lr 0.00034416 rank 4
2023-02-25 07:27:17,778 DEBUG TRAIN Batch 25/2600 loss 8.840752 loss_att 8.588821 loss_ctc 11.349950 loss_rnnt 8.408285 hw_loss 0.278048 lr 0.00034415 rank 2
2023-02-25 07:27:17,778 DEBUG TRAIN Batch 25/2600 loss 6.592879 loss_att 7.484474 loss_ctc 9.776105 loss_rnnt 5.874574 hw_loss 0.216667 lr 0.00034411 rank 5
2023-02-25 07:27:17,784 DEBUG TRAIN Batch 25/2600 loss 3.298338 loss_att 4.994859 loss_ctc 7.760718 loss_rnnt 2.241908 hw_loss 0.229017 lr 0.00034417 rank 3
2023-02-25 07:28:28,611 DEBUG TRAIN Batch 25/2700 loss 9.428215 loss_att 11.667408 loss_ctc 11.887858 loss_rnnt 8.564066 hw_loss 0.165670 lr 0.00034407 rank 0
2023-02-25 07:28:28,613 DEBUG TRAIN Batch 25/2700 loss 14.620043 loss_att 15.415618 loss_ctc 22.768173 loss_rnnt 13.290488 hw_loss 0.157542 lr 0.00034403 rank 7
2023-02-25 07:28:28,613 DEBUG TRAIN Batch 25/2700 loss 10.679905 loss_att 14.984327 loss_ctc 18.712408 loss_rnnt 8.641703 hw_loss 0.199343 lr 0.00034403 rank 5
2023-02-25 07:28:28,614 DEBUG TRAIN Batch 25/2700 loss 9.240889 loss_att 11.338180 loss_ctc 14.039230 loss_rnnt 8.125108 hw_loss 0.106021 lr 0.00034405 rank 6
2023-02-25 07:28:28,616 DEBUG TRAIN Batch 25/2700 loss 5.204039 loss_att 10.123911 loss_ctc 4.488804 loss_rnnt 4.211280 hw_loss 0.195279 lr 0.00034409 rank 3
2023-02-25 07:28:28,618 DEBUG TRAIN Batch 25/2700 loss 7.572759 loss_att 9.811201 loss_ctc 11.511955 loss_rnnt 6.494089 hw_loss 0.198292 lr 0.00034408 rank 4
2023-02-25 07:28:28,620 DEBUG TRAIN Batch 25/2700 loss 7.388083 loss_att 11.347294 loss_ctc 12.320633 loss_rnnt 5.829246 hw_loss 0.204980 lr 0.00034413 rank 1
2023-02-25 07:28:28,623 DEBUG TRAIN Batch 25/2700 loss 6.615319 loss_att 12.686111 loss_ctc 9.879524 loss_rnnt 4.873076 hw_loss 0.174108 lr 0.00034407 rank 2
2023-02-25 07:29:41,214 DEBUG TRAIN Batch 25/2800 loss 10.783676 loss_att 13.227586 loss_ctc 16.426556 loss_rnnt 9.417719 hw_loss 0.233984 lr 0.00034397 rank 6
2023-02-25 07:29:41,216 DEBUG TRAIN Batch 25/2800 loss 11.998864 loss_att 16.086260 loss_ctc 18.455702 loss_rnnt 10.239258 hw_loss 0.152279 lr 0.00034399 rank 0
2023-02-25 07:29:41,216 DEBUG TRAIN Batch 25/2800 loss 11.367715 loss_att 13.515419 loss_ctc 15.899435 loss_rnnt 10.224764 hw_loss 0.204714 lr 0.00034405 rank 1
2023-02-25 07:29:41,223 DEBUG TRAIN Batch 25/2800 loss 8.145453 loss_att 9.255010 loss_ctc 10.782924 loss_rnnt 7.496002 hw_loss 0.142270 lr 0.00034395 rank 5
2023-02-25 07:29:41,223 DEBUG TRAIN Batch 25/2800 loss 8.906051 loss_att 10.363100 loss_ctc 8.787964 loss_rnnt 8.478625 hw_loss 0.284550 lr 0.00034399 rank 4
2023-02-25 07:29:41,225 DEBUG TRAIN Batch 25/2800 loss 11.522881 loss_att 14.688547 loss_ctc 14.288074 loss_rnnt 10.392029 hw_loss 0.241923 lr 0.00034399 rank 2
2023-02-25 07:29:41,227 DEBUG TRAIN Batch 25/2800 loss 14.473515 loss_att 17.181440 loss_ctc 25.428432 loss_rnnt 12.391597 hw_loss 0.149396 lr 0.00034394 rank 7
2023-02-25 07:29:41,263 DEBUG TRAIN Batch 25/2800 loss 5.395028 loss_att 8.293656 loss_ctc 6.825488 loss_rnnt 4.535658 hw_loss 0.166717 lr 0.00034401 rank 3
2023-02-25 07:30:52,651 DEBUG TRAIN Batch 25/2900 loss 13.472898 loss_att 16.646547 loss_ctc 20.403093 loss_rnnt 11.734704 hw_loss 0.336444 lr 0.00034391 rank 4
2023-02-25 07:30:52,653 DEBUG TRAIN Batch 25/2900 loss 15.133683 loss_att 18.288097 loss_ctc 19.812605 loss_rnnt 13.809028 hw_loss 0.131095 lr 0.00034396 rank 1
2023-02-25 07:30:52,653 DEBUG TRAIN Batch 25/2900 loss 9.235472 loss_att 10.679692 loss_ctc 11.994410 loss_rnnt 8.483265 hw_loss 0.179072 lr 0.00034393 rank 3
2023-02-25 07:30:52,655 DEBUG TRAIN Batch 25/2900 loss 11.861210 loss_att 13.847834 loss_ctc 16.374256 loss_rnnt 10.761114 hw_loss 0.189437 lr 0.00034389 rank 6
2023-02-25 07:30:52,657 DEBUG TRAIN Batch 25/2900 loss 3.843392 loss_att 8.190336 loss_ctc 7.225338 loss_rnnt 2.417760 hw_loss 0.197469 lr 0.00034386 rank 7
2023-02-25 07:30:52,656 DEBUG TRAIN Batch 25/2900 loss 12.695506 loss_att 16.760483 loss_ctc 19.537344 loss_rnnt 10.843479 hw_loss 0.237725 lr 0.00034391 rank 0
2023-02-25 07:30:52,659 DEBUG TRAIN Batch 25/2900 loss 7.621436 loss_att 12.734013 loss_ctc 7.694674 loss_rnnt 6.450810 hw_loss 0.259397 lr 0.00034387 rank 5
2023-02-25 07:30:52,660 DEBUG TRAIN Batch 25/2900 loss 12.741972 loss_att 15.858702 loss_ctc 17.576054 loss_rnnt 11.371401 hw_loss 0.192525 lr 0.00034390 rank 2
2023-02-25 07:32:02,914 DEBUG TRAIN Batch 25/3000 loss 9.409184 loss_att 11.665045 loss_ctc 11.594144 loss_rnnt 8.513267 hw_loss 0.287659 lr 0.00034388 rank 1
2023-02-25 07:32:02,922 DEBUG TRAIN Batch 25/3000 loss 6.286831 loss_att 7.197845 loss_ctc 6.409675 loss_rnnt 5.973104 hw_loss 0.215895 lr 0.00034382 rank 2
2023-02-25 07:32:02,925 DEBUG TRAIN Batch 25/3000 loss 7.714028 loss_att 9.986052 loss_ctc 11.597357 loss_rnnt 6.647777 hw_loss 0.176380 lr 0.00034383 rank 0
2023-02-25 07:32:02,933 DEBUG TRAIN Batch 25/3000 loss 6.625359 loss_att 8.741922 loss_ctc 8.788525 loss_rnnt 5.799404 hw_loss 0.214163 lr 0.00034383 rank 4
2023-02-25 07:32:02,933 DEBUG TRAIN Batch 25/3000 loss 6.555713 loss_att 8.368909 loss_ctc 9.714527 loss_rnnt 5.618155 hw_loss 0.288268 lr 0.00034378 rank 7
2023-02-25 07:32:02,935 DEBUG TRAIN Batch 25/3000 loss 7.171008 loss_att 9.846958 loss_ctc 9.896439 loss_rnnt 6.145500 hw_loss 0.237988 lr 0.00034385 rank 3
2023-02-25 07:32:02,936 DEBUG TRAIN Batch 25/3000 loss 11.086369 loss_att 14.446480 loss_ctc 16.063377 loss_rnnt 9.667278 hw_loss 0.156501 lr 0.00034379 rank 5
2023-02-25 07:32:02,981 DEBUG TRAIN Batch 25/3000 loss 4.459064 loss_att 7.168129 loss_ctc 6.578871 loss_rnnt 3.577800 hw_loss 0.106521 lr 0.00034381 rank 6
2023-02-25 07:33:13,919 DEBUG TRAIN Batch 25/3100 loss 15.700859 loss_att 16.589882 loss_ctc 20.699087 loss_rnnt 14.712183 hw_loss 0.270824 lr 0.00034373 rank 6
2023-02-25 07:33:13,919 DEBUG TRAIN Batch 25/3100 loss 8.769741 loss_att 11.430247 loss_ctc 13.125231 loss_rnnt 7.474901 hw_loss 0.341263 lr 0.00034375 rank 4
2023-02-25 07:33:13,919 DEBUG TRAIN Batch 25/3100 loss 10.160705 loss_att 10.742975 loss_ctc 12.657253 loss_rnnt 9.574233 hw_loss 0.257146 lr 0.00034375 rank 0
2023-02-25 07:33:13,921 DEBUG TRAIN Batch 25/3100 loss 12.874017 loss_att 13.767598 loss_ctc 19.565842 loss_rnnt 11.670591 hw_loss 0.248375 lr 0.00034370 rank 7
2023-02-25 07:33:13,920 DEBUG TRAIN Batch 25/3100 loss 4.280736 loss_att 5.963234 loss_ctc 7.756598 loss_rnnt 3.349196 hw_loss 0.246737 lr 0.00034371 rank 5
2023-02-25 07:33:13,922 DEBUG TRAIN Batch 25/3100 loss 10.062451 loss_att 13.278129 loss_ctc 14.879439 loss_rnnt 8.646850 hw_loss 0.244127 lr 0.00034380 rank 1
2023-02-25 07:33:13,923 DEBUG TRAIN Batch 25/3100 loss 7.017530 loss_att 9.994291 loss_ctc 15.046537 loss_rnnt 5.244206 hw_loss 0.201446 lr 0.00034374 rank 2
2023-02-25 07:33:13,968 DEBUG TRAIN Batch 25/3100 loss 6.933263 loss_att 7.241503 loss_ctc 11.606055 loss_rnnt 6.106438 hw_loss 0.266510 lr 0.00034376 rank 3
2023-02-25 07:34:27,519 DEBUG TRAIN Batch 25/3200 loss 7.152725 loss_att 8.101701 loss_ctc 13.472670 loss_rnnt 5.961946 hw_loss 0.296858 lr 0.00034366 rank 0
2023-02-25 07:34:27,521 DEBUG TRAIN Batch 25/3200 loss 18.023453 loss_att 23.219709 loss_ctc 39.788109 loss_rnnt 13.973114 hw_loss 0.204625 lr 0.00034367 rank 4
2023-02-25 07:34:27,522 DEBUG TRAIN Batch 25/3200 loss 14.743687 loss_att 14.306625 loss_ctc 20.149878 loss_rnnt 13.991562 hw_loss 0.222583 lr 0.00034363 rank 5
2023-02-25 07:34:27,526 DEBUG TRAIN Batch 25/3200 loss 14.565477 loss_att 18.382401 loss_ctc 22.695633 loss_rnnt 12.591369 hw_loss 0.237566 lr 0.00034372 rank 1
2023-02-25 07:34:27,527 DEBUG TRAIN Batch 25/3200 loss 3.660952 loss_att 6.375906 loss_ctc 4.939003 loss_rnnt 2.854020 hw_loss 0.175378 lr 0.00034362 rank 7
2023-02-25 07:34:27,530 DEBUG TRAIN Batch 25/3200 loss 8.137940 loss_att 9.723766 loss_ctc 11.353854 loss_rnnt 7.257010 hw_loss 0.253082 lr 0.00034366 rank 2
2023-02-25 07:34:27,549 DEBUG TRAIN Batch 25/3200 loss 7.108439 loss_att 10.155438 loss_ctc 9.552318 loss_rnnt 6.068768 hw_loss 0.195789 lr 0.00034368 rank 3
2023-02-25 07:34:27,551 DEBUG TRAIN Batch 25/3200 loss 11.093628 loss_att 14.002110 loss_ctc 16.520386 loss_rnnt 9.661095 hw_loss 0.238629 lr 0.00034365 rank 6
2023-02-25 07:35:38,451 DEBUG TRAIN Batch 25/3300 loss 9.674410 loss_att 14.146475 loss_ctc 18.244770 loss_rnnt 7.559550 hw_loss 0.145749 lr 0.00034364 rank 1
2023-02-25 07:35:38,454 DEBUG TRAIN Batch 25/3300 loss 2.250092 loss_att 5.472986 loss_ctc 2.905968 loss_rnnt 1.383808 hw_loss 0.251729 lr 0.00034358 rank 0
2023-02-25 07:35:38,458 DEBUG TRAIN Batch 25/3300 loss 5.419371 loss_att 8.388916 loss_ctc 8.783602 loss_rnnt 4.265170 hw_loss 0.209488 lr 0.00034357 rank 6
2023-02-25 07:35:38,459 DEBUG TRAIN Batch 25/3300 loss 8.526109 loss_att 11.987793 loss_ctc 13.143108 loss_rnnt 7.126578 hw_loss 0.171738 lr 0.00034355 rank 5
2023-02-25 07:35:38,459 DEBUG TRAIN Batch 25/3300 loss 10.464647 loss_att 14.921433 loss_ctc 13.917225 loss_rnnt 8.996138 hw_loss 0.219017 lr 0.00034359 rank 4
2023-02-25 07:35:38,460 DEBUG TRAIN Batch 25/3300 loss 5.023184 loss_att 8.608139 loss_ctc 6.103469 loss_rnnt 4.080164 hw_loss 0.153733 lr 0.00034358 rank 2
2023-02-25 07:35:38,462 DEBUG TRAIN Batch 25/3300 loss 10.813398 loss_att 16.328087 loss_ctc 10.945243 loss_rnnt 9.620646 hw_loss 0.135442 lr 0.00034354 rank 7
2023-02-25 07:35:38,463 DEBUG TRAIN Batch 25/3300 loss 4.108858 loss_att 7.505564 loss_ctc 5.007570 loss_rnnt 3.205340 hw_loss 0.195653 lr 0.00034360 rank 3
2023-02-25 07:36:48,925 DEBUG TRAIN Batch 25/3400 loss 11.352245 loss_att 15.531070 loss_ctc 12.538609 loss_rnnt 10.289091 hw_loss 0.129765 lr 0.00034350 rank 0
2023-02-25 07:36:48,930 DEBUG TRAIN Batch 25/3400 loss 11.854156 loss_att 16.547123 loss_ctc 16.864679 loss_rnnt 10.129250 hw_loss 0.221704 lr 0.00034348 rank 6
2023-02-25 07:36:48,933 DEBUG TRAIN Batch 25/3400 loss 12.715384 loss_att 17.210716 loss_ctc 20.449928 loss_rnnt 10.636938 hw_loss 0.277700 lr 0.00034352 rank 3
2023-02-25 07:36:48,933 DEBUG TRAIN Batch 25/3400 loss 7.024201 loss_att 9.703442 loss_ctc 10.948397 loss_rnnt 5.880368 hw_loss 0.158923 lr 0.00034346 rank 7
2023-02-25 07:36:48,934 DEBUG TRAIN Batch 25/3400 loss 12.339141 loss_att 15.090695 loss_ctc 20.380157 loss_rnnt 10.607932 hw_loss 0.203929 lr 0.00034351 rank 4
2023-02-25 07:36:48,935 DEBUG TRAIN Batch 25/3400 loss 1.446375 loss_att 3.899119 loss_ctc 1.014628 loss_rnnt 0.918990 hw_loss 0.177004 lr 0.00034356 rank 1
2023-02-25 07:36:48,941 DEBUG TRAIN Batch 25/3400 loss 8.289402 loss_att 13.030647 loss_ctc 13.028095 loss_rnnt 6.605004 hw_loss 0.195603 lr 0.00034350 rank 2
2023-02-25 07:36:48,979 DEBUG TRAIN Batch 25/3400 loss 4.292262 loss_att 9.090166 loss_ctc 9.344912 loss_rnnt 2.524787 hw_loss 0.251639 lr 0.00034346 rank 5
2023-02-25 07:37:59,556 DEBUG TRAIN Batch 25/3500 loss 6.965829 loss_att 10.829861 loss_ctc 12.306660 loss_rnnt 5.412782 hw_loss 0.127742 lr 0.00034338 rank 7
2023-02-25 07:37:59,566 DEBUG TRAIN Batch 25/3500 loss 12.081121 loss_att 18.615364 loss_ctc 17.291191 loss_rnnt 9.992582 hw_loss 0.163152 lr 0.00034344 rank 3
2023-02-25 07:37:59,569 DEBUG TRAIN Batch 25/3500 loss 8.832654 loss_att 10.258142 loss_ctc 13.847465 loss_rnnt 7.777879 hw_loss 0.189442 lr 0.00034348 rank 1
2023-02-25 07:37:59,568 DEBUG TRAIN Batch 25/3500 loss 19.383860 loss_att 19.945221 loss_ctc 23.472599 loss_rnnt 18.609951 hw_loss 0.218381 lr 0.00034338 rank 5
2023-02-25 07:37:59,571 DEBUG TRAIN Batch 25/3500 loss 7.199777 loss_att 8.329758 loss_ctc 7.802477 loss_rnnt 6.797740 hw_loss 0.179401 lr 0.00034342 rank 0
2023-02-25 07:37:59,574 DEBUG TRAIN Batch 25/3500 loss 5.376503 loss_att 8.057323 loss_ctc 9.702581 loss_rnnt 4.148858 hw_loss 0.215006 lr 0.00034342 rank 2
2023-02-25 07:37:59,574 DEBUG TRAIN Batch 25/3500 loss 4.277864 loss_att 8.567065 loss_ctc 6.763645 loss_rnnt 3.043380 hw_loss 0.084762 lr 0.00034340 rank 6
2023-02-25 07:37:59,580 DEBUG TRAIN Batch 25/3500 loss 8.948794 loss_att 10.050193 loss_ctc 15.196690 loss_rnnt 7.783668 hw_loss 0.209614 lr 0.00034343 rank 4
2023-02-25 07:39:11,914 DEBUG TRAIN Batch 25/3600 loss 11.865263 loss_att 14.669042 loss_ctc 17.426987 loss_rnnt 10.443596 hw_loss 0.223777 lr 0.00034329 rank 7
2023-02-25 07:39:11,917 DEBUG TRAIN Batch 25/3600 loss 7.180535 loss_att 11.221878 loss_ctc 12.261423 loss_rnnt 5.607108 hw_loss 0.164449 lr 0.00034334 rank 2
2023-02-25 07:39:11,918 DEBUG TRAIN Batch 25/3600 loss 7.737069 loss_att 10.390498 loss_ctc 11.393476 loss_rnnt 6.530226 hw_loss 0.353694 lr 0.00034334 rank 0
2023-02-25 07:39:11,918 DEBUG TRAIN Batch 25/3600 loss 6.607718 loss_att 8.586897 loss_ctc 8.522471 loss_rnnt 5.859923 hw_loss 0.181234 lr 0.00034335 rank 4
2023-02-25 07:39:11,919 DEBUG TRAIN Batch 25/3600 loss 3.126541 loss_att 6.122395 loss_ctc 5.160875 loss_rnnt 2.175670 hw_loss 0.150854 lr 0.00034336 rank 3
2023-02-25 07:39:11,919 DEBUG TRAIN Batch 25/3600 loss 11.684333 loss_att 13.681780 loss_ctc 17.210423 loss_rnnt 10.455397 hw_loss 0.173691 lr 0.00034340 rank 1
2023-02-25 07:39:11,922 DEBUG TRAIN Batch 25/3600 loss 8.773391 loss_att 9.725592 loss_ctc 12.469046 loss_rnnt 8.004345 hw_loss 0.160974 lr 0.00034330 rank 5
2023-02-25 07:39:11,929 DEBUG TRAIN Batch 25/3600 loss 5.765930 loss_att 8.968976 loss_ctc 9.137438 loss_rnnt 4.577569 hw_loss 0.184156 lr 0.00034332 rank 6
2023-02-25 07:40:23,150 DEBUG TRAIN Batch 25/3700 loss 12.790662 loss_att 12.140544 loss_ctc 17.167568 loss_rnnt 12.173347 hw_loss 0.307033 lr 0.00034326 rank 0
2023-02-25 07:40:23,152 DEBUG TRAIN Batch 25/3700 loss 6.186946 loss_att 8.090974 loss_ctc 9.020800 loss_rnnt 5.248829 hw_loss 0.336495 lr 0.00034331 rank 1
2023-02-25 07:40:23,155 DEBUG TRAIN Batch 25/3700 loss 9.818323 loss_att 14.432125 loss_ctc 11.663089 loss_rnnt 8.566893 hw_loss 0.155064 lr 0.00034324 rank 6
2023-02-25 07:40:23,154 DEBUG TRAIN Batch 25/3700 loss 15.521297 loss_att 15.760962 loss_ctc 21.342695 loss_rnnt 14.497705 hw_loss 0.374013 lr 0.00034328 rank 3
2023-02-25 07:40:23,157 DEBUG TRAIN Batch 25/3700 loss 5.808937 loss_att 8.916794 loss_ctc 8.538361 loss_rnnt 4.697656 hw_loss 0.235849 lr 0.00034321 rank 7
2023-02-25 07:40:23,159 DEBUG TRAIN Batch 25/3700 loss 3.174749 loss_att 4.875521 loss_ctc 6.344468 loss_rnnt 2.329529 hw_loss 0.154568 lr 0.00034326 rank 2
2023-02-25 07:40:23,160 DEBUG TRAIN Batch 25/3700 loss 5.654837 loss_att 8.664686 loss_ctc 9.870430 loss_rnnt 4.352988 hw_loss 0.258373 lr 0.00034326 rank 4
2023-02-25 07:40:23,169 DEBUG TRAIN Batch 25/3700 loss 6.075101 loss_att 11.029308 loss_ctc 9.655025 loss_rnnt 4.443626 hw_loss 0.306206 lr 0.00034322 rank 5
2023-02-25 07:41:34,298 DEBUG TRAIN Batch 25/3800 loss 8.182792 loss_att 9.741959 loss_ctc 10.030158 loss_rnnt 7.510533 hw_loss 0.213953 lr 0.00034323 rank 1
2023-02-25 07:41:34,302 DEBUG TRAIN Batch 25/3800 loss 5.058615 loss_att 9.942183 loss_ctc 6.702232 loss_rnnt 3.783512 hw_loss 0.148577 lr 0.00034320 rank 3
2023-02-25 07:41:34,302 DEBUG TRAIN Batch 25/3800 loss 5.447007 loss_att 12.078720 loss_ctc 10.377804 loss_rnnt 3.372586 hw_loss 0.169948 lr 0.00034318 rank 4
2023-02-25 07:41:34,302 DEBUG TRAIN Batch 25/3800 loss 6.280132 loss_att 8.497279 loss_ctc 11.056023 loss_rnnt 5.141334 hw_loss 0.109844 lr 0.00034318 rank 0
2023-02-25 07:41:34,305 DEBUG TRAIN Batch 25/3800 loss 7.004556 loss_att 13.067291 loss_ctc 7.357338 loss_rnnt 5.607667 hw_loss 0.257444 lr 0.00034316 rank 6
2023-02-25 07:41:34,307 DEBUG TRAIN Batch 25/3800 loss 6.632595 loss_att 7.676573 loss_ctc 8.664757 loss_rnnt 5.959339 hw_loss 0.362823 lr 0.00034317 rank 2
2023-02-25 07:41:34,310 DEBUG TRAIN Batch 25/3800 loss 9.044918 loss_att 12.559111 loss_ctc 12.215017 loss_rnnt 7.822747 hw_loss 0.181222 lr 0.00034313 rank 7
2023-02-25 07:41:34,311 DEBUG TRAIN Batch 25/3800 loss 10.436346 loss_att 10.230253 loss_ctc 11.695637 loss_rnnt 10.101318 hw_loss 0.390639 lr 0.00034314 rank 5
2023-02-25 07:42:46,976 DEBUG TRAIN Batch 25/3900 loss 9.488320 loss_att 15.154565 loss_ctc 14.547199 loss_rnnt 7.609726 hw_loss 0.132802 lr 0.00034306 rank 5
2023-02-25 07:42:46,984 DEBUG TRAIN Batch 25/3900 loss 3.448500 loss_att 7.409565 loss_ctc 5.119921 loss_rnnt 2.373691 hw_loss 0.112011 lr 0.00034310 rank 0
2023-02-25 07:42:46,983 DEBUG TRAIN Batch 25/3900 loss 6.201810 loss_att 12.116519 loss_ctc 13.580151 loss_rnnt 3.933078 hw_loss 0.191272 lr 0.00034315 rank 1
2023-02-25 07:42:46,985 DEBUG TRAIN Batch 25/3900 loss 10.062803 loss_att 11.903665 loss_ctc 10.136452 loss_rnnt 9.576748 hw_loss 0.202619 lr 0.00034310 rank 4
2023-02-25 07:42:47,001 DEBUG TRAIN Batch 25/3900 loss 7.252009 loss_att 11.592254 loss_ctc 10.683818 loss_rnnt 5.853592 hw_loss 0.136488 lr 0.00034312 rank 3
2023-02-25 07:42:47,001 DEBUG TRAIN Batch 25/3900 loss 4.394919 loss_att 9.068620 loss_ctc 9.696438 loss_rnnt 2.615070 hw_loss 0.259199 lr 0.00034309 rank 2
2023-02-25 07:42:47,009 DEBUG TRAIN Batch 25/3900 loss 16.782045 loss_att 18.751200 loss_ctc 25.531919 loss_rnnt 15.160923 hw_loss 0.113703 lr 0.00034305 rank 7
2023-02-25 07:42:47,029 DEBUG TRAIN Batch 25/3900 loss 6.742414 loss_att 6.888399 loss_ctc 9.330021 loss_rnnt 6.217270 hw_loss 0.283000 lr 0.00034308 rank 6
2023-02-25 07:43:57,845 DEBUG TRAIN Batch 25/4000 loss 19.170309 loss_att 19.317783 loss_ctc 35.105980 loss_rnnt 16.932144 hw_loss 0.157342 lr 0.00034307 rank 1
2023-02-25 07:43:57,848 DEBUG TRAIN Batch 25/4000 loss 2.916519 loss_att 7.031542 loss_ctc 5.994498 loss_rnnt 1.589478 hw_loss 0.175574 lr 0.00034297 rank 7
2023-02-25 07:43:57,848 DEBUG TRAIN Batch 25/4000 loss 6.377536 loss_att 10.837273 loss_ctc 10.646368 loss_rnnt 4.849194 hw_loss 0.126033 lr 0.00034304 rank 3
2023-02-25 07:43:57,851 DEBUG TRAIN Batch 25/4000 loss 2.717579 loss_att 6.479970 loss_ctc 5.592786 loss_rnnt 1.463356 hw_loss 0.221970 lr 0.00034300 rank 6
2023-02-25 07:43:57,851 DEBUG TRAIN Batch 25/4000 loss 2.970090 loss_att 6.397586 loss_ctc 3.930551 loss_rnnt 2.029307 hw_loss 0.238541 lr 0.00034301 rank 2
2023-02-25 07:43:57,851 DEBUG TRAIN Batch 25/4000 loss 13.108089 loss_att 15.063771 loss_ctc 14.742593 loss_rnnt 12.356047 hw_loss 0.268074 lr 0.00034302 rank 0
2023-02-25 07:43:57,901 DEBUG TRAIN Batch 25/4000 loss 4.980041 loss_att 9.723615 loss_ctc 7.719977 loss_rnnt 3.584952 hw_loss 0.151968 lr 0.00034298 rank 5
2023-02-25 07:43:58,268 DEBUG TRAIN Batch 25/4000 loss 11.301086 loss_att 14.982632 loss_ctc 13.684965 loss_rnnt 10.153671 hw_loss 0.174855 lr 0.00034302 rank 4
2023-02-25 07:45:08,871 DEBUG TRAIN Batch 25/4100 loss 15.248698 loss_att 15.800545 loss_ctc 17.706091 loss_rnnt 14.684284 hw_loss 0.236985 lr 0.00034289 rank 7
2023-02-25 07:45:08,872 DEBUG TRAIN Batch 25/4100 loss 5.436366 loss_att 8.094291 loss_ctc 10.175357 loss_rnnt 4.175062 hw_loss 0.183475 lr 0.00034299 rank 1
2023-02-25 07:45:08,876 DEBUG TRAIN Batch 25/4100 loss 7.261621 loss_att 11.197796 loss_ctc 12.531249 loss_rnnt 5.715660 hw_loss 0.105205 lr 0.00034294 rank 0
2023-02-25 07:45:08,877 DEBUG TRAIN Batch 25/4100 loss 1.941177 loss_att 4.274439 loss_ctc 3.041318 loss_rnnt 1.215854 hw_loss 0.209971 lr 0.00034292 rank 6
2023-02-25 07:45:08,876 DEBUG TRAIN Batch 25/4100 loss 3.376501 loss_att 5.633336 loss_ctc 3.463264 loss_rnnt 2.827958 hw_loss 0.160513 lr 0.00034290 rank 5
2023-02-25 07:45:08,877 DEBUG TRAIN Batch 25/4100 loss 10.084443 loss_att 11.496374 loss_ctc 10.680633 loss_rnnt 9.625822 hw_loss 0.181393 lr 0.00034293 rank 2
2023-02-25 07:45:08,881 DEBUG TRAIN Batch 25/4100 loss 10.095594 loss_att 10.923158 loss_ctc 12.885946 loss_rnnt 9.457536 hw_loss 0.188437 lr 0.00034296 rank 3
2023-02-25 07:45:08,882 DEBUG TRAIN Batch 25/4100 loss 6.533939 loss_att 10.330477 loss_ctc 10.468550 loss_rnnt 5.122324 hw_loss 0.239424 lr 0.00034294 rank 4
2023-02-25 07:46:19,910 DEBUG TRAIN Batch 25/4200 loss 8.948304 loss_att 12.230697 loss_ctc 12.577934 loss_rnnt 7.710298 hw_loss 0.182958 lr 0.00034282 rank 5
2023-02-25 07:46:19,916 DEBUG TRAIN Batch 25/4200 loss 5.996644 loss_att 7.833486 loss_ctc 7.481090 loss_rnnt 5.274713 hw_loss 0.293695 lr 0.00034287 rank 3
2023-02-25 07:46:19,917 DEBUG TRAIN Batch 25/4200 loss 10.828921 loss_att 12.857875 loss_ctc 10.444456 loss_rnnt 10.359233 hw_loss 0.215924 lr 0.00034286 rank 0
2023-02-25 07:46:19,920 DEBUG TRAIN Batch 25/4200 loss 4.132896 loss_att 6.889837 loss_ctc 7.585567 loss_rnnt 3.061839 hw_loss 0.111213 lr 0.00034291 rank 1
2023-02-25 07:46:19,920 DEBUG TRAIN Batch 25/4200 loss 8.857311 loss_att 12.460293 loss_ctc 13.994818 loss_rnnt 7.294032 hw_loss 0.295653 lr 0.00034281 rank 7
2023-02-25 07:46:19,922 DEBUG TRAIN Batch 25/4200 loss 9.571524 loss_att 10.811833 loss_ctc 11.383669 loss_rnnt 9.010221 hw_loss 0.134288 lr 0.00034284 rank 6
2023-02-25 07:46:19,923 DEBUG TRAIN Batch 25/4200 loss 7.602184 loss_att 12.662213 loss_ctc 10.940691 loss_rnnt 6.003544 hw_loss 0.265312 lr 0.00034285 rank 2
2023-02-25 07:46:19,943 DEBUG TRAIN Batch 25/4200 loss 4.811235 loss_att 8.280958 loss_ctc 9.191479 loss_rnnt 3.428109 hw_loss 0.197155 lr 0.00034286 rank 4
2023-02-25 07:47:33,239 DEBUG TRAIN Batch 25/4300 loss 7.630837 loss_att 10.360705 loss_ctc 13.157725 loss_rnnt 6.215321 hw_loss 0.248671 lr 0.00034283 rank 1
2023-02-25 07:47:33,240 DEBUG TRAIN Batch 25/4300 loss 11.648623 loss_att 13.132474 loss_ctc 17.580734 loss_rnnt 10.455580 hw_loss 0.197485 lr 0.00034276 rank 6
2023-02-25 07:47:33,242 DEBUG TRAIN Batch 25/4300 loss 10.275496 loss_att 11.232454 loss_ctc 14.063625 loss_rnnt 9.456360 hw_loss 0.229988 lr 0.00034277 rank 0
2023-02-25 07:47:33,242 DEBUG TRAIN Batch 25/4300 loss 10.036936 loss_att 11.899141 loss_ctc 13.670707 loss_rnnt 9.033515 hw_loss 0.274647 lr 0.00034278 rank 4
2023-02-25 07:47:33,246 DEBUG TRAIN Batch 25/4300 loss 5.403669 loss_att 7.446061 loss_ctc 5.877313 loss_rnnt 4.840741 hw_loss 0.171182 lr 0.00034277 rank 2
2023-02-25 07:47:33,248 DEBUG TRAIN Batch 25/4300 loss 11.227168 loss_att 13.654376 loss_ctc 15.486842 loss_rnnt 10.012950 hw_loss 0.301538 lr 0.00034279 rank 3
2023-02-25 07:47:33,248 DEBUG TRAIN Batch 25/4300 loss 5.392768 loss_att 8.111134 loss_ctc 5.488266 loss_rnnt 4.742067 hw_loss 0.176802 lr 0.00034273 rank 7
2023-02-25 07:47:33,251 DEBUG TRAIN Batch 25/4300 loss 14.271989 loss_att 17.976625 loss_ctc 20.549202 loss_rnnt 12.612911 hw_loss 0.152227 lr 0.00034274 rank 5
2023-02-25 07:48:43,759 DEBUG TRAIN Batch 25/4400 loss 17.128475 loss_att 19.013264 loss_ctc 27.665154 loss_rnnt 15.253290 hw_loss 0.175005 lr 0.00034269 rank 0
2023-02-25 07:48:43,761 DEBUG TRAIN Batch 25/4400 loss 5.269243 loss_att 7.549406 loss_ctc 9.026802 loss_rnnt 4.189366 hw_loss 0.230318 lr 0.00034265 rank 7
2023-02-25 07:48:43,763 DEBUG TRAIN Batch 25/4400 loss 7.875006 loss_att 11.165909 loss_ctc 13.940833 loss_rnnt 6.282193 hw_loss 0.235979 lr 0.00034266 rank 5
2023-02-25 07:48:43,764 DEBUG TRAIN Batch 25/4400 loss 6.813187 loss_att 6.760285 loss_ctc 9.221446 loss_rnnt 6.336219 hw_loss 0.312090 lr 0.00034270 rank 4
2023-02-25 07:48:43,764 DEBUG TRAIN Batch 25/4400 loss 9.030611 loss_att 12.129356 loss_ctc 14.510596 loss_rnnt 7.591710 hw_loss 0.165914 lr 0.00034269 rank 2
2023-02-25 07:48:43,765 DEBUG TRAIN Batch 25/4400 loss 8.773367 loss_att 10.853524 loss_ctc 15.448721 loss_rnnt 7.369575 hw_loss 0.183212 lr 0.00034268 rank 6
2023-02-25 07:48:43,766 DEBUG TRAIN Batch 25/4400 loss 4.628372 loss_att 8.322970 loss_ctc 7.824913 loss_rnnt 3.365811 hw_loss 0.182693 lr 0.00034275 rank 1
2023-02-25 07:48:43,767 DEBUG TRAIN Batch 25/4400 loss 3.264982 loss_att 5.956420 loss_ctc 5.379573 loss_rnnt 2.351863 hw_loss 0.174161 lr 0.00034271 rank 3
2023-02-25 07:49:54,151 DEBUG TRAIN Batch 25/4500 loss 5.934193 loss_att 9.181575 loss_ctc 10.964384 loss_rnnt 4.562735 hw_loss 0.096166 lr 0.00034267 rank 1
2023-02-25 07:49:54,151 DEBUG TRAIN Batch 25/4500 loss 4.622097 loss_att 6.497437 loss_ctc 5.866386 loss_rnnt 3.959578 hw_loss 0.227898 lr 0.00034263 rank 3
2023-02-25 07:49:54,163 DEBUG TRAIN Batch 25/4500 loss 11.018602 loss_att 14.362765 loss_ctc 18.409079 loss_rnnt 9.302477 hw_loss 0.116055 lr 0.00034257 rank 7
2023-02-25 07:49:54,164 DEBUG TRAIN Batch 25/4500 loss 15.048138 loss_att 18.181520 loss_ctc 28.192440 loss_rnnt 12.495128 hw_loss 0.325798 lr 0.00034262 rank 4
2023-02-25 07:49:54,165 DEBUG TRAIN Batch 25/4500 loss 9.065476 loss_att 9.360207 loss_ctc 11.837956 loss_rnnt 8.450963 hw_loss 0.348568 lr 0.00034261 rank 2
2023-02-25 07:49:54,165 DEBUG TRAIN Batch 25/4500 loss 9.295503 loss_att 12.926115 loss_ctc 10.278452 loss_rnnt 8.331404 hw_loss 0.200466 lr 0.00034261 rank 0
2023-02-25 07:49:54,167 DEBUG TRAIN Batch 25/4500 loss 10.698337 loss_att 12.618862 loss_ctc 14.003146 loss_rnnt 9.729268 hw_loss 0.270603 lr 0.00034260 rank 6
2023-02-25 07:49:54,226 DEBUG TRAIN Batch 25/4500 loss 1.525491 loss_att 4.291915 loss_ctc 2.761357 loss_rnnt 0.740654 hw_loss 0.125193 lr 0.00034258 rank 5
2023-02-25 07:51:06,515 DEBUG TRAIN Batch 25/4600 loss 5.573146 loss_att 11.251118 loss_ctc 8.928328 loss_rnnt 3.868492 hw_loss 0.228190 lr 0.00034255 rank 3
2023-02-25 07:51:06,516 DEBUG TRAIN Batch 25/4600 loss 5.882874 loss_att 9.440823 loss_ctc 6.694118 loss_rnnt 4.979350 hw_loss 0.157065 lr 0.00034259 rank 1
2023-02-25 07:51:06,517 DEBUG TRAIN Batch 25/4600 loss 6.997660 loss_att 11.556284 loss_ctc 12.936934 loss_rnnt 5.203317 hw_loss 0.170091 lr 0.00034254 rank 4
2023-02-25 07:51:06,519 DEBUG TRAIN Batch 25/4600 loss 3.989517 loss_att 4.786160 loss_ctc 3.985790 loss_rnnt 3.738075 hw_loss 0.173644 lr 0.00034253 rank 0
2023-02-25 07:51:06,519 DEBUG TRAIN Batch 25/4600 loss 11.990051 loss_att 14.613739 loss_ctc 20.182381 loss_rnnt 10.278690 hw_loss 0.176834 lr 0.00034252 rank 6
2023-02-25 07:51:06,526 DEBUG TRAIN Batch 25/4600 loss 7.979412 loss_att 12.771522 loss_ctc 8.000023 loss_rnnt 6.902109 hw_loss 0.217750 lr 0.00034249 rank 7
2023-02-25 07:51:06,537 DEBUG TRAIN Batch 25/4600 loss 8.364248 loss_att 10.682536 loss_ctc 11.655176 loss_rnnt 7.377225 hw_loss 0.158576 lr 0.00034253 rank 2
2023-02-25 07:51:06,560 DEBUG TRAIN Batch 25/4600 loss 10.620412 loss_att 13.297707 loss_ctc 18.448864 loss_rnnt 8.976927 hw_loss 0.120436 lr 0.00034250 rank 5
2023-02-25 07:52:17,811 DEBUG TRAIN Batch 25/4700 loss 10.383731 loss_att 13.394142 loss_ctc 14.342523 loss_rnnt 9.174992 hw_loss 0.147783 lr 0.00034241 rank 7
2023-02-25 07:52:17,823 DEBUG TRAIN Batch 25/4700 loss 10.228859 loss_att 10.893909 loss_ctc 9.605128 loss_rnnt 10.084208 hw_loss 0.177758 lr 0.00034247 rank 3
2023-02-25 07:52:17,825 DEBUG TRAIN Batch 25/4700 loss 6.831944 loss_att 10.058590 loss_ctc 8.968580 loss_rnnt 5.808917 hw_loss 0.174024 lr 0.00034245 rank 0
2023-02-25 07:52:17,826 DEBUG TRAIN Batch 25/4700 loss 17.511530 loss_att 24.520744 loss_ctc 27.837845 loss_rnnt 14.621222 hw_loss 0.209294 lr 0.00034245 rank 2
2023-02-25 07:52:17,828 DEBUG TRAIN Batch 25/4700 loss 12.034341 loss_att 16.136459 loss_ctc 16.607420 loss_rnnt 10.524821 hw_loss 0.148786 lr 0.00034251 rank 1
2023-02-25 07:52:17,831 DEBUG TRAIN Batch 25/4700 loss 7.670456 loss_att 10.238111 loss_ctc 14.414266 loss_rnnt 6.172332 hw_loss 0.160160 lr 0.00034246 rank 4
2023-02-25 07:52:17,832 DEBUG TRAIN Batch 25/4700 loss 7.864171 loss_att 9.273039 loss_ctc 13.648191 loss_rnnt 6.662327 hw_loss 0.279126 lr 0.00034244 rank 6
2023-02-25 07:52:17,833 DEBUG TRAIN Batch 25/4700 loss 6.553285 loss_att 7.983207 loss_ctc 7.482946 loss_rnnt 6.004091 hw_loss 0.261103 lr 0.00034242 rank 5
2023-02-25 07:53:27,818 DEBUG TRAIN Batch 25/4800 loss 7.780942 loss_att 9.690645 loss_ctc 8.580074 loss_rnnt 7.167850 hw_loss 0.233627 lr 0.00034237 rank 0
2023-02-25 07:53:27,824 DEBUG TRAIN Batch 25/4800 loss 5.334648 loss_att 8.679626 loss_ctc 9.345658 loss_rnnt 4.039937 hw_loss 0.170464 lr 0.00034239 rank 3
2023-02-25 07:53:27,825 DEBUG TRAIN Batch 25/4800 loss 13.424869 loss_att 15.482363 loss_ctc 16.120436 loss_rnnt 12.564093 hw_loss 0.168505 lr 0.00034235 rank 6
2023-02-25 07:53:27,824 DEBUG TRAIN Batch 25/4800 loss 10.898890 loss_att 13.569492 loss_ctc 14.281097 loss_rnnt 9.817585 hw_loss 0.180418 lr 0.00034243 rank 1
2023-02-25 07:53:27,825 DEBUG TRAIN Batch 25/4800 loss 9.451744 loss_att 12.885713 loss_ctc 12.917545 loss_rnnt 8.241879 hw_loss 0.114311 lr 0.00034238 rank 4
2023-02-25 07:53:27,825 DEBUG TRAIN Batch 25/4800 loss 4.761837 loss_att 6.613914 loss_ctc 5.313434 loss_rnnt 4.255061 hw_loss 0.117777 lr 0.00034233 rank 7
2023-02-25 07:53:27,827 DEBUG TRAIN Batch 25/4800 loss 12.605960 loss_att 14.536070 loss_ctc 15.410215 loss_rnnt 11.769100 hw_loss 0.144258 lr 0.00034234 rank 5
2023-02-25 07:53:27,830 DEBUG TRAIN Batch 25/4800 loss 7.802084 loss_att 11.781150 loss_ctc 8.624252 loss_rnnt 6.849632 hw_loss 0.088155 lr 0.00034237 rank 2
2023-02-25 07:54:38,017 DEBUG TRAIN Batch 25/4900 loss 5.832785 loss_att 7.485642 loss_ctc 8.140798 loss_rnnt 5.075035 hw_loss 0.223957 lr 0.00034229 rank 0
2023-02-25 07:54:38,019 DEBUG TRAIN Batch 25/4900 loss 5.492763 loss_att 7.960073 loss_ctc 8.438188 loss_rnnt 4.482117 hw_loss 0.233364 lr 0.00034231 rank 3
2023-02-25 07:54:38,021 DEBUG TRAIN Batch 25/4900 loss 19.318359 loss_att 22.335581 loss_ctc 26.564617 loss_rnnt 17.609667 hw_loss 0.260770 lr 0.00034229 rank 2
2023-02-25 07:54:38,023 DEBUG TRAIN Batch 25/4900 loss 10.443602 loss_att 11.451060 loss_ctc 12.976064 loss_rnnt 9.831571 hw_loss 0.136644 lr 0.00034227 rank 6
2023-02-25 07:54:38,023 DEBUG TRAIN Batch 25/4900 loss 4.769415 loss_att 7.591692 loss_ctc 7.425092 loss_rnnt 3.754559 hw_loss 0.180584 lr 0.00034226 rank 5
2023-02-25 07:54:38,024 DEBUG TRAIN Batch 25/4900 loss 7.259763 loss_att 10.142960 loss_ctc 12.344680 loss_rnnt 5.863589 hw_loss 0.265398 lr 0.00034230 rank 4
2023-02-25 07:54:38,025 DEBUG TRAIN Batch 25/4900 loss 5.325318 loss_att 7.690341 loss_ctc 9.860825 loss_rnnt 4.199323 hw_loss 0.090483 lr 0.00034225 rank 7
2023-02-25 07:54:38,030 DEBUG TRAIN Batch 25/4900 loss 8.375876 loss_att 11.527348 loss_ctc 14.247128 loss_rnnt 6.891428 hw_loss 0.133724 lr 0.00034235 rank 1
2023-02-25 07:55:51,921 DEBUG TRAIN Batch 25/5000 loss 8.909131 loss_att 11.586921 loss_ctc 18.076904 loss_rnnt 7.052957 hw_loss 0.184213 lr 0.00034221 rank 0
2023-02-25 07:55:51,921 DEBUG TRAIN Batch 25/5000 loss 7.508352 loss_att 8.842945 loss_ctc 9.679110 loss_rnnt 6.832487 hw_loss 0.224087 lr 0.00034217 rank 7
2023-02-25 07:55:51,924 DEBUG TRAIN Batch 25/5000 loss 6.073179 loss_att 7.469488 loss_ctc 9.249781 loss_rnnt 5.269073 hw_loss 0.189933 lr 0.00034227 rank 1
2023-02-25 07:55:51,925 DEBUG TRAIN Batch 25/5000 loss 8.949852 loss_att 7.909231 loss_ctc 12.001540 loss_rnnt 8.579217 hw_loss 0.322252 lr 0.00034223 rank 3
2023-02-25 07:55:51,928 DEBUG TRAIN Batch 25/5000 loss 6.214931 loss_att 7.658216 loss_ctc 8.242105 loss_rnnt 5.554158 hw_loss 0.190922 lr 0.00034219 rank 6
2023-02-25 07:55:51,929 DEBUG TRAIN Batch 25/5000 loss 4.887416 loss_att 7.092532 loss_ctc 5.769692 loss_rnnt 4.247441 hw_loss 0.152465 lr 0.00034218 rank 5
2023-02-25 07:55:51,931 DEBUG TRAIN Batch 25/5000 loss 8.849246 loss_att 9.500644 loss_ctc 12.054100 loss_rnnt 8.086388 hw_loss 0.384872 lr 0.00034222 rank 4
2023-02-25 07:55:51,933 DEBUG TRAIN Batch 25/5000 loss 7.961705 loss_att 10.375517 loss_ctc 12.673295 loss_rnnt 6.791137 hw_loss 0.111740 lr 0.00034221 rank 2
2023-02-25 07:57:01,940 DEBUG TRAIN Batch 25/5100 loss 7.808718 loss_att 8.652996 loss_ctc 12.008532 loss_rnnt 6.990438 hw_loss 0.167717 lr 0.00034213 rank 0
2023-02-25 07:57:01,940 DEBUG TRAIN Batch 25/5100 loss 6.113956 loss_att 8.445425 loss_ctc 6.012130 loss_rnnt 5.530382 hw_loss 0.245359 lr 0.00034215 rank 3
2023-02-25 07:57:01,944 DEBUG TRAIN Batch 25/5100 loss 5.649703 loss_att 7.646670 loss_ctc 7.259657 loss_rnnt 4.928164 hw_loss 0.201534 lr 0.00034219 rank 1
2023-02-25 07:57:01,945 DEBUG TRAIN Batch 25/5100 loss 7.702420 loss_att 14.559426 loss_ctc 12.347055 loss_rnnt 5.623239 hw_loss 0.165929 lr 0.00034209 rank 7
2023-02-25 07:57:01,946 DEBUG TRAIN Batch 25/5100 loss 4.499598 loss_att 7.540415 loss_ctc 5.078521 loss_rnnt 3.696785 hw_loss 0.220236 lr 0.00034214 rank 4
2023-02-25 07:57:01,949 DEBUG TRAIN Batch 25/5100 loss 9.148909 loss_att 12.012833 loss_ctc 13.137555 loss_rnnt 7.925810 hw_loss 0.222175 lr 0.00034210 rank 5
2023-02-25 07:57:01,950 DEBUG TRAIN Batch 25/5100 loss 3.933789 loss_att 5.874278 loss_ctc 6.865497 loss_rnnt 3.024210 hw_loss 0.244850 lr 0.00034213 rank 2
2023-02-25 07:57:01,993 DEBUG TRAIN Batch 25/5100 loss 12.731251 loss_att 15.841848 loss_ctc 18.763618 loss_rnnt 11.197967 hw_loss 0.200343 lr 0.00034211 rank 6
2023-02-25 07:58:12,004 DEBUG TRAIN Batch 25/5200 loss 9.862938 loss_att 13.044687 loss_ctc 13.445906 loss_rnnt 8.658634 hw_loss 0.169173 lr 0.00034203 rank 6
2023-02-25 07:58:12,004 DEBUG TRAIN Batch 25/5200 loss 6.909045 loss_att 10.612690 loss_ctc 11.735767 loss_rnnt 5.471334 hw_loss 0.100161 lr 0.00034205 rank 0
2023-02-25 07:58:12,005 DEBUG TRAIN Batch 25/5200 loss 6.932600 loss_att 12.263653 loss_ctc 10.076035 loss_rnnt 5.400644 hw_loss 0.087413 lr 0.00034211 rank 1
2023-02-25 07:58:12,007 DEBUG TRAIN Batch 25/5200 loss 10.198749 loss_att 12.247150 loss_ctc 13.712093 loss_rnnt 9.237371 hw_loss 0.156096 lr 0.00034205 rank 2
2023-02-25 07:58:12,007 DEBUG TRAIN Batch 25/5200 loss 4.185145 loss_att 5.542013 loss_ctc 6.751649 loss_rnnt 3.477464 hw_loss 0.176451 lr 0.00034207 rank 3
2023-02-25 07:58:12,007 DEBUG TRAIN Batch 25/5200 loss 1.385068 loss_att 3.324838 loss_ctc 1.721374 loss_rnnt 0.830098 hw_loss 0.229079 lr 0.00034206 rank 4
2023-02-25 07:58:12,009 DEBUG TRAIN Batch 25/5200 loss 4.267046 loss_att 6.397167 loss_ctc 4.507446 loss_rnnt 3.677912 hw_loss 0.245734 lr 0.00034201 rank 7
2023-02-25 07:58:12,014 DEBUG TRAIN Batch 25/5200 loss 4.206792 loss_att 6.214118 loss_ctc 3.575846 loss_rnnt 3.768085 hw_loss 0.227565 lr 0.00034202 rank 5
2023-02-25 07:59:23,603 DEBUG TRAIN Batch 25/5300 loss 14.541831 loss_att 16.294807 loss_ctc 19.280720 loss_rnnt 13.489168 hw_loss 0.131652 lr 0.00034197 rank 0
2023-02-25 07:59:23,605 DEBUG TRAIN Batch 25/5300 loss 10.724743 loss_att 14.084543 loss_ctc 18.810560 loss_rnnt 8.859529 hw_loss 0.215894 lr 0.00034197 rank 2
2023-02-25 07:59:23,608 DEBUG TRAIN Batch 25/5300 loss 7.170963 loss_att 10.037062 loss_ctc 9.310117 loss_rnnt 6.198470 hw_loss 0.213849 lr 0.00034193 rank 7
2023-02-25 07:59:23,610 DEBUG TRAIN Batch 25/5300 loss 6.374452 loss_att 8.098206 loss_ctc 7.475867 loss_rnnt 5.783803 hw_loss 0.185704 lr 0.00034199 rank 3
2023-02-25 07:59:23,612 DEBUG TRAIN Batch 25/5300 loss 7.419190 loss_att 13.053666 loss_ctc 15.511386 loss_rnnt 5.053144 hw_loss 0.300358 lr 0.00034203 rank 1
2023-02-25 07:59:23,613 DEBUG TRAIN Batch 25/5300 loss 8.991853 loss_att 11.022974 loss_ctc 9.128524 loss_rnnt 8.473927 hw_loss 0.175273 lr 0.00034194 rank 5
2023-02-25 07:59:23,613 DEBUG TRAIN Batch 25/5300 loss 4.278857 loss_att 6.324918 loss_ctc 5.109596 loss_rnnt 3.636095 hw_loss 0.230221 lr 0.00034195 rank 6
2023-02-25 07:59:23,632 DEBUG TRAIN Batch 25/5300 loss 10.334779 loss_att 10.809428 loss_ctc 13.310017 loss_rnnt 9.758141 hw_loss 0.159395 lr 0.00034198 rank 4
2023-02-25 08:00:35,455 DEBUG TRAIN Batch 25/5400 loss 11.019693 loss_att 13.573917 loss_ctc 18.033096 loss_rnnt 9.439442 hw_loss 0.251787 lr 0.00034187 rank 6
2023-02-25 08:00:35,458 DEBUG TRAIN Batch 25/5400 loss 17.214256 loss_att 20.019939 loss_ctc 22.576939 loss_rnnt 15.780123 hw_loss 0.296199 lr 0.00034191 rank 3
2023-02-25 08:00:35,457 DEBUG TRAIN Batch 25/5400 loss 4.583699 loss_att 6.574038 loss_ctc 6.200054 loss_rnnt 3.847835 hw_loss 0.229280 lr 0.00034189 rank 0
2023-02-25 08:00:35,461 DEBUG TRAIN Batch 25/5400 loss 5.624124 loss_att 8.482780 loss_ctc 8.720379 loss_rnnt 4.574743 hw_loss 0.121529 lr 0.00034185 rank 7
2023-02-25 08:00:35,464 DEBUG TRAIN Batch 25/5400 loss 7.985845 loss_att 8.478196 loss_ctc 14.385784 loss_rnnt 6.941391 hw_loss 0.173733 lr 0.00034190 rank 4
2023-02-25 08:00:35,466 DEBUG TRAIN Batch 25/5400 loss 4.795149 loss_att 6.246342 loss_ctc 7.459970 loss_rnnt 4.025782 hw_loss 0.232162 lr 0.00034186 rank 5
2023-02-25 08:00:35,466 DEBUG TRAIN Batch 25/5400 loss 6.778335 loss_att 9.946394 loss_ctc 12.515156 loss_rnnt 5.307674 hw_loss 0.135261 lr 0.00034189 rank 2
2023-02-25 08:00:35,474 DEBUG TRAIN Batch 25/5400 loss 9.770651 loss_att 11.258366 loss_ctc 12.703302 loss_rnnt 9.002439 hw_loss 0.149340 lr 0.00034195 rank 1
2023-02-25 08:01:45,298 DEBUG TRAIN Batch 25/5500 loss 10.622623 loss_att 14.869157 loss_ctc 18.529337 loss_rnnt 8.605018 hw_loss 0.213883 lr 0.00034181 rank 0
2023-02-25 08:01:45,300 DEBUG TRAIN Batch 25/5500 loss 5.945738 loss_att 8.582338 loss_ctc 9.137042 loss_rnnt 4.871175 hw_loss 0.228255 lr 0.00034183 rank 3
2023-02-25 08:01:45,305 DEBUG TRAIN Batch 25/5500 loss 1.019359 loss_att 3.344807 loss_ctc 1.422008 loss_rnnt 0.400338 hw_loss 0.187958 lr 0.00034179 rank 6
2023-02-25 08:01:45,306 DEBUG TRAIN Batch 25/5500 loss 8.552277 loss_att 8.245991 loss_ctc 10.922869 loss_rnnt 8.160210 hw_loss 0.257334 lr 0.00034187 rank 1
2023-02-25 08:01:45,306 DEBUG TRAIN Batch 25/5500 loss 5.514778 loss_att 8.206276 loss_ctc 8.174361 loss_rnnt 4.496980 hw_loss 0.234165 lr 0.00034177 rank 7
2023-02-25 08:01:45,309 DEBUG TRAIN Batch 25/5500 loss 7.905559 loss_att 10.694585 loss_ctc 9.753853 loss_rnnt 6.921782 hw_loss 0.336623 lr 0.00034178 rank 5
2023-02-25 08:01:45,311 DEBUG TRAIN Batch 25/5500 loss 6.020324 loss_att 9.658903 loss_ctc 8.390657 loss_rnnt 4.860292 hw_loss 0.218009 lr 0.00034181 rank 2
2023-02-25 08:01:45,372 DEBUG TRAIN Batch 25/5500 loss 9.568484 loss_att 10.408478 loss_ctc 11.866788 loss_rnnt 8.993678 hw_loss 0.188188 lr 0.00034182 rank 4
2023-02-25 08:02:56,551 DEBUG TRAIN Batch 25/5600 loss 8.185804 loss_att 10.160119 loss_ctc 10.464527 loss_rnnt 7.354158 hw_loss 0.249288 lr 0.00034173 rank 0
2023-02-25 08:02:56,554 DEBUG TRAIN Batch 25/5600 loss 11.576452 loss_att 12.837391 loss_ctc 14.807749 loss_rnnt 10.753867 hw_loss 0.261672 lr 0.00034169 rank 7
2023-02-25 08:02:56,554 DEBUG TRAIN Batch 25/5600 loss 8.554812 loss_att 9.603920 loss_ctc 12.582806 loss_rnnt 7.711181 hw_loss 0.181395 lr 0.00034173 rank 2
2023-02-25 08:02:56,556 DEBUG TRAIN Batch 25/5600 loss 6.721353 loss_att 7.230984 loss_ctc 8.720036 loss_rnnt 6.220584 hw_loss 0.248159 lr 0.00034175 rank 3
2023-02-25 08:02:56,555 DEBUG TRAIN Batch 25/5600 loss 12.194076 loss_att 12.054594 loss_ctc 16.177036 loss_rnnt 11.568746 hw_loss 0.229059 lr 0.00034174 rank 4
2023-02-25 08:02:56,556 DEBUG TRAIN Batch 25/5600 loss 3.521689 loss_att 5.898843 loss_ctc 5.388491 loss_rnnt 2.718571 hw_loss 0.147713 lr 0.00034170 rank 5
2023-02-25 08:02:56,557 DEBUG TRAIN Batch 25/5600 loss 11.770436 loss_att 15.125869 loss_ctc 18.900764 loss_rnnt 10.068531 hw_loss 0.150201 lr 0.00034171 rank 6
2023-02-25 08:02:56,559 DEBUG TRAIN Batch 25/5600 loss 9.909186 loss_att 13.342739 loss_ctc 15.273028 loss_rnnt 8.443599 hw_loss 0.119436 lr 0.00034179 rank 1
2023-02-25 08:04:10,105 DEBUG TRAIN Batch 25/5700 loss 10.066023 loss_att 13.244143 loss_ctc 14.403921 loss_rnnt 8.775154 hw_loss 0.144108 lr 0.00034171 rank 1
2023-02-25 08:04:10,105 DEBUG TRAIN Batch 25/5700 loss 6.218237 loss_att 10.358030 loss_ctc 8.900096 loss_rnnt 4.921678 hw_loss 0.208161 lr 0.00034163 rank 6
2023-02-25 08:04:10,108 DEBUG TRAIN Batch 25/5700 loss 5.216758 loss_att 6.659059 loss_ctc 6.808845 loss_rnnt 4.592233 hw_loss 0.232101 lr 0.00034167 rank 3
2023-02-25 08:04:10,112 DEBUG TRAIN Batch 25/5700 loss 7.979630 loss_att 8.890977 loss_ctc 12.549361 loss_rnnt 7.095729 hw_loss 0.173125 lr 0.00034165 rank 2
2023-02-25 08:04:10,114 DEBUG TRAIN Batch 25/5700 loss 10.508479 loss_att 12.377150 loss_ctc 17.120369 loss_rnnt 9.109561 hw_loss 0.269247 lr 0.00034166 rank 4
2023-02-25 08:04:10,122 DEBUG TRAIN Batch 25/5700 loss 7.355087 loss_att 10.991247 loss_ctc 11.575998 loss_rnnt 5.973232 hw_loss 0.172191 lr 0.00034162 rank 5
2023-02-25 08:04:10,133 DEBUG TRAIN Batch 25/5700 loss 11.971207 loss_att 13.382227 loss_ctc 15.514014 loss_rnnt 11.109611 hw_loss 0.200659 lr 0.00034165 rank 0
2023-02-25 08:04:10,144 DEBUG TRAIN Batch 25/5700 loss 4.289696 loss_att 6.918089 loss_ctc 6.012657 loss_rnnt 3.405130 hw_loss 0.242172 lr 0.00034161 rank 7
2023-02-25 08:05:20,999 DEBUG TRAIN Batch 25/5800 loss 4.684064 loss_att 4.633179 loss_ctc 7.121274 loss_rnnt 4.293472 hw_loss 0.142138 lr 0.00034157 rank 0
2023-02-25 08:05:21,004 DEBUG TRAIN Batch 25/5800 loss 7.845716 loss_att 9.053198 loss_ctc 8.902862 loss_rnnt 7.376174 hw_loss 0.163301 lr 0.00034159 rank 3
2023-02-25 08:05:21,005 DEBUG TRAIN Batch 25/5800 loss 12.055026 loss_att 12.910851 loss_ctc 18.347107 loss_rnnt 10.876065 hw_loss 0.316596 lr 0.00034157 rank 2
2023-02-25 08:05:21,008 DEBUG TRAIN Batch 25/5800 loss 11.739677 loss_att 17.507223 loss_ctc 16.724186 loss_rnnt 9.857949 hw_loss 0.119283 lr 0.00034154 rank 5
2023-02-25 08:05:21,008 DEBUG TRAIN Batch 25/5800 loss 5.049491 loss_att 9.449707 loss_ctc 7.834366 loss_rnnt 3.749980 hw_loss 0.090283 lr 0.00034153 rank 7
2023-02-25 08:05:21,009 DEBUG TRAIN Batch 25/5800 loss 4.471189 loss_att 6.897745 loss_ctc 6.599358 loss_rnnt 3.637955 hw_loss 0.120316 lr 0.00034163 rank 1
2023-02-25 08:05:21,011 DEBUG TRAIN Batch 25/5800 loss 1.479169 loss_att 4.467564 loss_ctc 1.539237 loss_rnnt 0.800803 hw_loss 0.136272 lr 0.00034158 rank 4
2023-02-25 08:05:21,012 DEBUG TRAIN Batch 25/5800 loss 8.898517 loss_att 10.898039 loss_ctc 13.486575 loss_rnnt 7.749339 hw_loss 0.257875 lr 0.00034156 rank 6
2023-02-25 08:06:31,188 DEBUG TRAIN Batch 25/5900 loss 10.491110 loss_att 11.918640 loss_ctc 15.544188 loss_rnnt 9.436831 hw_loss 0.178180 lr 0.00034151 rank 3
2023-02-25 08:06:31,192 DEBUG TRAIN Batch 25/5900 loss 15.512985 loss_att 18.166058 loss_ctc 20.767727 loss_rnnt 14.223848 hw_loss 0.108543 lr 0.00034148 rank 6
2023-02-25 08:06:31,194 DEBUG TRAIN Batch 25/5900 loss 5.207702 loss_att 8.564435 loss_ctc 10.807952 loss_rnnt 3.662848 hw_loss 0.237762 lr 0.00034149 rank 0
2023-02-25 08:06:31,195 DEBUG TRAIN Batch 25/5900 loss 15.332032 loss_att 17.053303 loss_ctc 21.787964 loss_rnnt 14.056938 hw_loss 0.131341 lr 0.00034150 rank 4
2023-02-25 08:06:31,197 DEBUG TRAIN Batch 25/5900 loss 9.095651 loss_att 11.497896 loss_ctc 11.571405 loss_rnnt 8.167582 hw_loss 0.220349 lr 0.00034145 rank 7
2023-02-25 08:06:31,198 DEBUG TRAIN Batch 25/5900 loss 11.562003 loss_att 13.986893 loss_ctc 15.990614 loss_rnnt 10.383294 hw_loss 0.193594 lr 0.00034155 rank 1
2023-02-25 08:06:31,199 DEBUG TRAIN Batch 25/5900 loss 4.190409 loss_att 7.072665 loss_ctc 5.556346 loss_rnnt 3.312429 hw_loss 0.223880 lr 0.00034149 rank 2
2023-02-25 08:06:31,202 DEBUG TRAIN Batch 25/5900 loss 4.766227 loss_att 6.988783 loss_ctc 6.814327 loss_rnnt 3.977672 hw_loss 0.133058 lr 0.00034146 rank 5
2023-02-25 08:07:42,746 DEBUG TRAIN Batch 25/6000 loss 3.862805 loss_att 8.211420 loss_ctc 8.575698 loss_rnnt 2.250512 hw_loss 0.214096 lr 0.00034140 rank 6
2023-02-25 08:07:42,748 DEBUG TRAIN Batch 25/6000 loss 6.608331 loss_att 9.148869 loss_ctc 11.567484 loss_rnnt 5.335074 hw_loss 0.194866 lr 0.00034141 rank 0
2023-02-25 08:07:42,747 DEBUG TRAIN Batch 25/6000 loss 5.865106 loss_att 9.462222 loss_ctc 10.362266 loss_rnnt 4.464675 hw_loss 0.152599 lr 0.00034141 rank 2
2023-02-25 08:07:42,750 DEBUG TRAIN Batch 25/6000 loss 7.177425 loss_att 10.321678 loss_ctc 10.944441 loss_rnnt 5.988104 hw_loss 0.109127 lr 0.00034147 rank 1
2023-02-25 08:07:42,751 DEBUG TRAIN Batch 25/6000 loss 17.503786 loss_att 22.939613 loss_ctc 25.951384 loss_rnnt 15.154564 hw_loss 0.254455 lr 0.00034142 rank 4
2023-02-25 08:07:42,753 DEBUG TRAIN Batch 25/6000 loss 6.530527 loss_att 9.491309 loss_ctc 11.724018 loss_rnnt 5.186161 hw_loss 0.112020 lr 0.00034143 rank 3
2023-02-25 08:07:42,762 DEBUG TRAIN Batch 25/6000 loss 11.067018 loss_att 12.000016 loss_ctc 12.365570 loss_rnnt 10.618953 hw_loss 0.165611 lr 0.00034137 rank 7
2023-02-25 08:07:42,795 DEBUG TRAIN Batch 25/6000 loss 3.290075 loss_att 5.715015 loss_ctc 6.451835 loss_rnnt 2.275385 hw_loss 0.202750 lr 0.00034138 rank 5
2023-02-25 08:08:55,243 DEBUG TRAIN Batch 25/6100 loss 7.193709 loss_att 7.978151 loss_ctc 9.670161 loss_rnnt 6.588409 hw_loss 0.221659 lr 0.00034133 rank 0
2023-02-25 08:08:55,244 DEBUG TRAIN Batch 25/6100 loss 12.166230 loss_att 13.036358 loss_ctc 14.739441 loss_rnnt 11.547023 hw_loss 0.191414 lr 0.00034135 rank 3
2023-02-25 08:08:55,249 DEBUG TRAIN Batch 25/6100 loss 7.002688 loss_att 9.901885 loss_ctc 9.964355 loss_rnnt 5.920576 hw_loss 0.201345 lr 0.00034133 rank 2
2023-02-25 08:08:55,251 DEBUG TRAIN Batch 25/6100 loss 6.601574 loss_att 12.529022 loss_ctc 12.388829 loss_rnnt 4.559243 hw_loss 0.159764 lr 0.00034139 rank 1
2023-02-25 08:08:55,252 DEBUG TRAIN Batch 25/6100 loss 7.431391 loss_att 8.524965 loss_ctc 13.441837 loss_rnnt 6.286669 hw_loss 0.233651 lr 0.00034132 rank 6
2023-02-25 08:08:55,253 DEBUG TRAIN Batch 25/6100 loss 7.747861 loss_att 11.593492 loss_ctc 12.363131 loss_rnnt 6.260324 hw_loss 0.193205 lr 0.00034134 rank 4
2023-02-25 08:08:55,254 DEBUG TRAIN Batch 25/6100 loss 12.492410 loss_att 17.883820 loss_ctc 15.616675 loss_rnnt 10.881176 hw_loss 0.218216 lr 0.00034129 rank 7
2023-02-25 08:08:55,254 DEBUG TRAIN Batch 25/6100 loss 5.276274 loss_att 7.689760 loss_ctc 8.766577 loss_rnnt 4.242652 hw_loss 0.160408 lr 0.00034130 rank 5
2023-02-25 08:10:05,227 DEBUG TRAIN Batch 25/6200 loss 8.479301 loss_att 9.419673 loss_ctc 9.467196 loss_rnnt 8.086095 hw_loss 0.137650 lr 0.00034125 rank 2
2023-02-25 08:10:05,235 DEBUG TRAIN Batch 25/6200 loss 13.047067 loss_att 13.493575 loss_ctc 17.707529 loss_rnnt 12.252686 hw_loss 0.156908 lr 0.00034125 rank 0
2023-02-25 08:10:05,238 DEBUG TRAIN Batch 25/6200 loss 12.853938 loss_att 14.864450 loss_ctc 14.733718 loss_rnnt 12.121461 hw_loss 0.149509 lr 0.00034131 rank 1
2023-02-25 08:10:05,241 DEBUG TRAIN Batch 25/6200 loss 6.204699 loss_att 8.534373 loss_ctc 7.730792 loss_rnnt 5.435607 hw_loss 0.186896 lr 0.00034124 rank 6
2023-02-25 08:10:05,246 DEBUG TRAIN Batch 25/6200 loss 5.884551 loss_att 7.950651 loss_ctc 10.599332 loss_rnnt 4.737583 hw_loss 0.197082 lr 0.00034121 rank 7
2023-02-25 08:10:05,248 DEBUG TRAIN Batch 25/6200 loss 10.119145 loss_att 10.785054 loss_ctc 15.282233 loss_rnnt 9.194814 hw_loss 0.192634 lr 0.00034126 rank 4
2023-02-25 08:10:05,249 DEBUG TRAIN Batch 25/6200 loss 11.544231 loss_att 13.446638 loss_ctc 16.796953 loss_rnnt 10.336674 hw_loss 0.237589 lr 0.00034127 rank 3
2023-02-25 08:10:05,250 DEBUG TRAIN Batch 25/6200 loss 5.655519 loss_att 8.038199 loss_ctc 8.677273 loss_rnnt 4.698327 hw_loss 0.145792 lr 0.00034122 rank 5
2023-02-25 08:11:16,194 DEBUG TRAIN Batch 25/6300 loss 6.947221 loss_att 12.855433 loss_ctc 11.444648 loss_rnnt 5.076897 hw_loss 0.166922 lr 0.00034117 rank 0
2023-02-25 08:11:16,198 DEBUG TRAIN Batch 25/6300 loss 10.144119 loss_att 11.316328 loss_ctc 15.132751 loss_rnnt 9.109016 hw_loss 0.254080 lr 0.00034117 rank 2
2023-02-25 08:11:16,198 DEBUG TRAIN Batch 25/6300 loss 10.961615 loss_att 14.221784 loss_ctc 20.244328 loss_rnnt 8.922697 hw_loss 0.279729 lr 0.00034119 rank 3
2023-02-25 08:11:16,199 DEBUG TRAIN Batch 25/6300 loss 6.820337 loss_att 7.222741 loss_ctc 9.985139 loss_rnnt 6.183805 hw_loss 0.251396 lr 0.00034123 rank 1
2023-02-25 08:11:16,199 DEBUG TRAIN Batch 25/6300 loss 8.003544 loss_att 8.628616 loss_ctc 11.031666 loss_rnnt 7.342645 hw_loss 0.247753 lr 0.00034113 rank 7
2023-02-25 08:11:16,203 DEBUG TRAIN Batch 25/6300 loss 9.217585 loss_att 12.284032 loss_ctc 12.569036 loss_rnnt 8.066631 hw_loss 0.170257 lr 0.00034116 rank 6
2023-02-25 08:11:16,205 DEBUG TRAIN Batch 25/6300 loss 6.871420 loss_att 12.616068 loss_ctc 13.075931 loss_rnnt 4.788786 hw_loss 0.199566 lr 0.00034118 rank 4
2023-02-25 08:11:16,257 DEBUG TRAIN Batch 25/6300 loss 5.767094 loss_att 9.642809 loss_ctc 8.509688 loss_rnnt 4.514387 hw_loss 0.209785 lr 0.00034114 rank 5
2023-02-25 08:12:28,982 DEBUG TRAIN Batch 25/6400 loss 8.580022 loss_att 10.005954 loss_ctc 11.268188 loss_rnnt 7.799660 hw_loss 0.256413 lr 0.00034115 rank 1
2023-02-25 08:12:28,988 DEBUG TRAIN Batch 25/6400 loss 5.668391 loss_att 9.697611 loss_ctc 8.104658 loss_rnnt 4.434953 hw_loss 0.192670 lr 0.00034111 rank 3
2023-02-25 08:12:28,989 DEBUG TRAIN Batch 25/6400 loss 6.250796 loss_att 8.161942 loss_ctc 10.289606 loss_rnnt 5.198210 hw_loss 0.247215 lr 0.00034110 rank 4
2023-02-25 08:12:28,990 DEBUG TRAIN Batch 25/6400 loss 12.975559 loss_att 13.462476 loss_ctc 16.235916 loss_rnnt 12.330038 hw_loss 0.212670 lr 0.00034109 rank 2
2023-02-25 08:12:28,991 DEBUG TRAIN Batch 25/6400 loss 5.501798 loss_att 6.371553 loss_ctc 5.841657 loss_rnnt 5.213688 hw_loss 0.129083 lr 0.00034105 rank 7
2023-02-25 08:12:28,994 DEBUG TRAIN Batch 25/6400 loss 8.558252 loss_att 10.400869 loss_ctc 11.153440 loss_rnnt 7.723369 hw_loss 0.225629 lr 0.00034106 rank 5
2023-02-25 08:12:28,996 DEBUG TRAIN Batch 25/6400 loss 5.980711 loss_att 8.430151 loss_ctc 9.293470 loss_rnnt 4.943407 hw_loss 0.198214 lr 0.00034108 rank 6
2023-02-25 08:12:28,995 DEBUG TRAIN Batch 25/6400 loss 10.626958 loss_att 13.628710 loss_ctc 13.522818 loss_rnnt 9.534185 hw_loss 0.199326 lr 0.00034110 rank 0
2023-02-25 08:13:40,084 DEBUG TRAIN Batch 25/6500 loss 11.432275 loss_att 13.059141 loss_ctc 14.305305 loss_rnnt 10.636841 hw_loss 0.163107 lr 0.00034104 rank 3
2023-02-25 08:13:40,085 DEBUG TRAIN Batch 25/6500 loss 3.028762 loss_att 4.495901 loss_ctc 6.362009 loss_rnnt 2.170906 hw_loss 0.224991 lr 0.00034102 rank 4
2023-02-25 08:13:40,088 DEBUG TRAIN Batch 25/6500 loss 5.184033 loss_att 9.248058 loss_ctc 8.756324 loss_rnnt 3.776117 hw_loss 0.222759 lr 0.00034102 rank 0
2023-02-25 08:13:40,088 DEBUG TRAIN Batch 25/6500 loss 8.325222 loss_att 11.484138 loss_ctc 14.032490 loss_rnnt 6.851916 hw_loss 0.151037 lr 0.00034107 rank 1
2023-02-25 08:13:40,092 DEBUG TRAIN Batch 25/6500 loss 16.496445 loss_att 20.858898 loss_ctc 20.961178 loss_rnnt 14.953310 hw_loss 0.141272 lr 0.00034097 rank 7
2023-02-25 08:13:40,094 DEBUG TRAIN Batch 25/6500 loss 6.238552 loss_att 11.217934 loss_ctc 10.255554 loss_rnnt 4.574316 hw_loss 0.248923 lr 0.00034098 rank 5
2023-02-25 08:13:40,098 DEBUG TRAIN Batch 25/6500 loss 7.251698 loss_att 10.978844 loss_ctc 11.566390 loss_rnnt 5.810186 hw_loss 0.226481 lr 0.00034101 rank 2
2023-02-25 08:13:40,099 DEBUG TRAIN Batch 25/6500 loss 7.782082 loss_att 9.097157 loss_ctc 13.831154 loss_rnnt 6.630076 hw_loss 0.154591 lr 0.00034100 rank 6
2023-02-25 08:14:49,729 DEBUG TRAIN Batch 25/6600 loss 11.942681 loss_att 12.919468 loss_ctc 15.805554 loss_rnnt 11.175746 hw_loss 0.105991 lr 0.00034094 rank 0
2023-02-25 08:14:49,731 DEBUG TRAIN Batch 25/6600 loss 6.378704 loss_att 9.668274 loss_ctc 12.715521 loss_rnnt 4.766376 hw_loss 0.205320 lr 0.00034090 rank 5
2023-02-25 08:14:49,732 DEBUG TRAIN Batch 25/6600 loss 5.516675 loss_att 8.533195 loss_ctc 9.530617 loss_rnnt 4.299511 hw_loss 0.147504 lr 0.00034099 rank 1
2023-02-25 08:14:49,732 DEBUG TRAIN Batch 25/6600 loss 11.580814 loss_att 15.000879 loss_ctc 16.440771 loss_rnnt 10.112841 hw_loss 0.254937 lr 0.00034089 rank 7
2023-02-25 08:14:49,733 DEBUG TRAIN Batch 25/6600 loss 6.678163 loss_att 11.980237 loss_ctc 14.052135 loss_rnnt 4.574807 hw_loss 0.112021 lr 0.00034096 rank 3
2023-02-25 08:14:49,735 DEBUG TRAIN Batch 25/6600 loss 4.954795 loss_att 6.172926 loss_ctc 7.707250 loss_rnnt 4.233152 hw_loss 0.208170 lr 0.00034092 rank 6
2023-02-25 08:14:49,735 DEBUG TRAIN Batch 25/6600 loss 11.396688 loss_att 12.448462 loss_ctc 13.843554 loss_rnnt 10.773223 hw_loss 0.162865 lr 0.00034093 rank 2
2023-02-25 08:14:49,737 DEBUG TRAIN Batch 25/6600 loss 5.254525 loss_att 7.634712 loss_ctc 7.913568 loss_rnnt 4.337005 hw_loss 0.163019 lr 0.00034094 rank 4
2023-02-25 08:16:01,092 DEBUG TRAIN Batch 25/6700 loss 5.289304 loss_att 9.895891 loss_ctc 8.828215 loss_rnnt 3.781729 hw_loss 0.214505 lr 0.00034088 rank 3
2023-02-25 08:16:01,094 DEBUG TRAIN Batch 25/6700 loss 11.684844 loss_att 12.519405 loss_ctc 14.253970 loss_rnnt 11.059417 hw_loss 0.217435 lr 0.00034091 rank 1
2023-02-25 08:16:01,097 DEBUG TRAIN Batch 25/6700 loss 12.384843 loss_att 17.620012 loss_ctc 19.555357 loss_rnnt 10.324072 hw_loss 0.108129 lr 0.00034082 rank 5
2023-02-25 08:16:01,098 DEBUG TRAIN Batch 25/6700 loss 16.414652 loss_att 20.617783 loss_ctc 28.275763 loss_rnnt 13.914967 hw_loss 0.145458 lr 0.00034085 rank 2
2023-02-25 08:16:01,100 DEBUG TRAIN Batch 25/6700 loss 8.433056 loss_att 11.380209 loss_ctc 11.614058 loss_rnnt 7.359138 hw_loss 0.113162 lr 0.00034084 rank 6
2023-02-25 08:16:01,100 DEBUG TRAIN Batch 25/6700 loss 8.253265 loss_att 10.555318 loss_ctc 14.946917 loss_rnnt 6.799029 hw_loss 0.190009 lr 0.00034086 rank 4
2023-02-25 08:16:01,118 DEBUG TRAIN Batch 25/6700 loss 6.728065 loss_att 9.937307 loss_ctc 8.074529 loss_rnnt 5.788182 hw_loss 0.222198 lr 0.00034081 rank 7
2023-02-25 08:16:01,122 DEBUG TRAIN Batch 25/6700 loss 7.717781 loss_att 9.072950 loss_ctc 9.208776 loss_rnnt 7.173652 hw_loss 0.139305 lr 0.00034086 rank 0
2023-02-25 08:17:14,223 DEBUG TRAIN Batch 25/6800 loss 15.059857 loss_att 20.079502 loss_ctc 18.821579 loss_rnnt 13.478380 hw_loss 0.142468 lr 0.00034074 rank 5
2023-02-25 08:17:14,234 DEBUG TRAIN Batch 25/6800 loss 6.364589 loss_att 10.291358 loss_ctc 10.153446 loss_rnnt 4.983025 hw_loss 0.170680 lr 0.00034078 rank 0
2023-02-25 08:17:14,236 DEBUG TRAIN Batch 25/6800 loss 11.236697 loss_att 15.510143 loss_ctc 15.885865 loss_rnnt 9.622386 hw_loss 0.261999 lr 0.00034076 rank 6
2023-02-25 08:17:14,238 DEBUG TRAIN Batch 25/6800 loss 6.215765 loss_att 7.969349 loss_ctc 6.801103 loss_rnnt 5.695999 hw_loss 0.170633 lr 0.00034080 rank 3
2023-02-25 08:17:14,239 DEBUG TRAIN Batch 25/6800 loss 9.075644 loss_att 8.107077 loss_ctc 10.599936 loss_rnnt 8.945986 hw_loss 0.225248 lr 0.00034083 rank 1
2023-02-25 08:17:14,241 DEBUG TRAIN Batch 25/6800 loss 11.630863 loss_att 14.918553 loss_ctc 19.214161 loss_rnnt 9.818134 hw_loss 0.270158 lr 0.00034078 rank 2
2023-02-25 08:17:14,243 DEBUG TRAIN Batch 25/6800 loss 2.653687 loss_att 5.287267 loss_ctc 2.792970 loss_rnnt 2.001120 hw_loss 0.201151 lr 0.00034073 rank 7
2023-02-25 08:17:14,289 DEBUG TRAIN Batch 25/6800 loss 7.107752 loss_att 9.475843 loss_ctc 10.850592 loss_rnnt 6.028717 hw_loss 0.199446 lr 0.00034078 rank 4
2023-02-25 08:18:25,196 DEBUG TRAIN Batch 25/6900 loss 11.537872 loss_att 12.100018 loss_ctc 12.733409 loss_rnnt 11.114567 hw_loss 0.284008 lr 0.00034070 rank 0
2023-02-25 08:18:25,197 DEBUG TRAIN Batch 25/6900 loss 6.025289 loss_att 9.086797 loss_ctc 9.592880 loss_rnnt 4.769839 hw_loss 0.314004 lr 0.00034068 rank 6
2023-02-25 08:18:25,197 DEBUG TRAIN Batch 25/6900 loss 6.878333 loss_att 9.332179 loss_ctc 12.211466 loss_rnnt 5.545999 hw_loss 0.244650 lr 0.00034075 rank 1
2023-02-25 08:18:25,199 DEBUG TRAIN Batch 25/6900 loss 9.513923 loss_att 12.265627 loss_ctc 15.393402 loss_rnnt 8.070471 hw_loss 0.204714 lr 0.00034072 rank 3
2023-02-25 08:18:25,199 DEBUG TRAIN Batch 25/6900 loss 7.375582 loss_att 10.152574 loss_ctc 12.654252 loss_rnnt 5.998306 hw_loss 0.221352 lr 0.00034070 rank 2
2023-02-25 08:18:25,205 DEBUG TRAIN Batch 25/6900 loss 28.700182 loss_att 29.397392 loss_ctc 36.875816 loss_rnnt 27.338236 hw_loss 0.248285 lr 0.00034066 rank 7
2023-02-25 08:18:25,209 DEBUG TRAIN Batch 25/6900 loss 5.101167 loss_att 9.149704 loss_ctc 9.867085 loss_rnnt 3.570208 hw_loss 0.160868 lr 0.00034066 rank 5
2023-02-25 08:18:25,226 DEBUG TRAIN Batch 25/6900 loss 12.399648 loss_att 17.908016 loss_ctc 16.111206 loss_rnnt 10.728667 hw_loss 0.139564 lr 0.00034070 rank 4
2023-02-25 08:19:35,783 DEBUG TRAIN Batch 25/7000 loss 10.419161 loss_att 15.615700 loss_ctc 17.393772 loss_rnnt 8.394260 hw_loss 0.104334 lr 0.00034067 rank 1
2023-02-25 08:19:35,784 DEBUG TRAIN Batch 25/7000 loss 11.606735 loss_att 11.426257 loss_ctc 25.025448 loss_rnnt 9.775982 hw_loss 0.145662 lr 0.00034062 rank 0
2023-02-25 08:19:35,786 DEBUG TRAIN Batch 25/7000 loss 9.836747 loss_att 13.424240 loss_ctc 16.431400 loss_rnnt 8.120419 hw_loss 0.224144 lr 0.00034058 rank 7
2023-02-25 08:19:35,789 DEBUG TRAIN Batch 25/7000 loss 7.774346 loss_att 11.965843 loss_ctc 11.005356 loss_rnnt 6.399618 hw_loss 0.198054 lr 0.00034064 rank 3
2023-02-25 08:19:35,789 DEBUG TRAIN Batch 25/7000 loss 11.659972 loss_att 12.509663 loss_ctc 15.105244 loss_rnnt 10.931993 hw_loss 0.185008 lr 0.00034060 rank 6
2023-02-25 08:19:35,793 DEBUG TRAIN Batch 25/7000 loss 2.282914 loss_att 4.360281 loss_ctc 2.495305 loss_rnnt 1.700282 hw_loss 0.260326 lr 0.00034062 rank 2
2023-02-25 08:19:35,793 DEBUG TRAIN Batch 25/7000 loss 5.218517 loss_att 7.540403 loss_ctc 7.889838 loss_rnnt 4.347570 hw_loss 0.094488 lr 0.00034063 rank 4
2023-02-25 08:19:35,800 DEBUG TRAIN Batch 25/7000 loss 12.807783 loss_att 12.993994 loss_ctc 17.284595 loss_rnnt 12.029585 hw_loss 0.270091 lr 0.00034058 rank 5
2023-02-25 08:20:48,657 DEBUG TRAIN Batch 25/7100 loss 6.425617 loss_att 10.210022 loss_ctc 14.072150 loss_rnnt 4.550821 hw_loss 0.184458 lr 0.00034054 rank 0
2023-02-25 08:20:48,659 DEBUG TRAIN Batch 25/7100 loss 8.367097 loss_att 8.456745 loss_ctc 11.752405 loss_rnnt 7.716718 hw_loss 0.339515 lr 0.00034054 rank 2
2023-02-25 08:20:48,660 DEBUG TRAIN Batch 25/7100 loss 13.380152 loss_att 16.479612 loss_ctc 19.554417 loss_rnnt 11.870504 hw_loss 0.124727 lr 0.00034055 rank 4
2023-02-25 08:20:48,661 DEBUG TRAIN Batch 25/7100 loss 2.360216 loss_att 5.975040 loss_ctc 5.788273 loss_rnnt 1.048044 hw_loss 0.247749 lr 0.00034056 rank 3
2023-02-25 08:20:48,662 DEBUG TRAIN Batch 25/7100 loss 10.856278 loss_att 14.419655 loss_ctc 17.884140 loss_rnnt 9.054586 hw_loss 0.284941 lr 0.00034060 rank 1
2023-02-25 08:20:48,670 DEBUG TRAIN Batch 25/7100 loss 7.609800 loss_att 8.286290 loss_ctc 10.432058 loss_rnnt 6.986639 hw_loss 0.209181 lr 0.00034050 rank 5
2023-02-25 08:20:48,681 DEBUG TRAIN Batch 25/7100 loss 3.449620 loss_att 6.156726 loss_ctc 6.283377 loss_rnnt 2.469280 hw_loss 0.114533 lr 0.00034050 rank 7
2023-02-25 08:20:48,695 DEBUG TRAIN Batch 25/7100 loss 6.203335 loss_att 8.713774 loss_ctc 8.834993 loss_rnnt 5.250267 hw_loss 0.187673 lr 0.00034052 rank 6
2023-02-25 08:21:59,235 DEBUG TRAIN Batch 25/7200 loss 9.462120 loss_att 13.116344 loss_ctc 12.309104 loss_rnnt 8.264197 hw_loss 0.164026 lr 0.00034046 rank 0
2023-02-25 08:21:59,238 DEBUG TRAIN Batch 25/7200 loss 8.773140 loss_att 12.961840 loss_ctc 13.446707 loss_rnnt 7.178794 hw_loss 0.250243 lr 0.00034048 rank 3
2023-02-25 08:21:59,238 DEBUG TRAIN Batch 25/7200 loss 15.179720 loss_att 16.727461 loss_ctc 20.844599 loss_rnnt 14.016060 hw_loss 0.185238 lr 0.00034047 rank 4
2023-02-25 08:21:59,239 DEBUG TRAIN Batch 25/7200 loss 8.865940 loss_att 12.812887 loss_ctc 12.996916 loss_rnnt 7.444732 hw_loss 0.151917 lr 0.00034042 rank 7
2023-02-25 08:21:59,240 DEBUG TRAIN Batch 25/7200 loss 5.033264 loss_att 7.611620 loss_ctc 7.185147 loss_rnnt 4.158868 hw_loss 0.134637 lr 0.00034046 rank 2
2023-02-25 08:21:59,242 DEBUG TRAIN Batch 25/7200 loss 6.938751 loss_att 9.357338 loss_ctc 9.591761 loss_rnnt 6.064226 hw_loss 0.069513 lr 0.00034044 rank 6
2023-02-25 08:21:59,243 DEBUG TRAIN Batch 25/7200 loss 6.979085 loss_att 9.001717 loss_ctc 9.635132 loss_rnnt 6.134437 hw_loss 0.161217 lr 0.00034043 rank 5
2023-02-25 08:21:59,244 DEBUG TRAIN Batch 25/7200 loss 8.791396 loss_att 11.151986 loss_ctc 12.567171 loss_rnnt 7.685068 hw_loss 0.245200 lr 0.00034052 rank 1
2023-02-25 08:23:08,996 DEBUG TRAIN Batch 25/7300 loss 8.990344 loss_att 12.395965 loss_ctc 11.535048 loss_rnnt 7.817866 hw_loss 0.285113 lr 0.00034040 rank 3
2023-02-25 08:23:08,998 DEBUG TRAIN Batch 25/7300 loss 8.703873 loss_att 10.298327 loss_ctc 13.912439 loss_rnnt 7.528159 hw_loss 0.304400 lr 0.00034044 rank 1
2023-02-25 08:23:08,998 DEBUG TRAIN Batch 25/7300 loss 7.326951 loss_att 10.913383 loss_ctc 12.020437 loss_rnnt 5.859913 hw_loss 0.232411 lr 0.00034038 rank 0
2023-02-25 08:23:09,000 DEBUG TRAIN Batch 25/7300 loss 15.470866 loss_att 20.835291 loss_ctc 22.584423 loss_rnnt 13.372089 hw_loss 0.145157 lr 0.00034037 rank 6
2023-02-25 08:23:09,003 DEBUG TRAIN Batch 25/7300 loss 8.403520 loss_att 11.101706 loss_ctc 17.119118 loss_rnnt 6.623851 hw_loss 0.146160 lr 0.00034034 rank 7
2023-02-25 08:23:09,004 DEBUG TRAIN Batch 25/7300 loss 3.267836 loss_att 7.029892 loss_ctc 6.595953 loss_rnnt 1.978912 hw_loss 0.173933 lr 0.00034038 rank 2
2023-02-25 08:23:09,005 DEBUG TRAIN Batch 25/7300 loss 6.505763 loss_att 7.193320 loss_ctc 8.829622 loss_rnnt 5.936969 hw_loss 0.227689 lr 0.00034039 rank 4
2023-02-25 08:23:09,009 DEBUG TRAIN Batch 25/7300 loss 6.670495 loss_att 10.091801 loss_ctc 9.966321 loss_rnnt 5.460540 hw_loss 0.161720 lr 0.00034035 rank 5
2023-02-25 08:24:19,872 DEBUG TRAIN Batch 25/7400 loss 9.130763 loss_att 10.386109 loss_ctc 12.639967 loss_rnnt 8.340691 hw_loss 0.133330 lr 0.00034027 rank 5
2023-02-25 08:24:19,879 DEBUG TRAIN Batch 25/7400 loss 9.634029 loss_att 11.229120 loss_ctc 12.900024 loss_rnnt 8.776738 hw_loss 0.192761 lr 0.00034032 rank 3
2023-02-25 08:24:19,880 DEBUG TRAIN Batch 25/7400 loss 7.397789 loss_att 10.879018 loss_ctc 12.655441 loss_rnnt 5.853918 hw_loss 0.274885 lr 0.00034031 rank 4
2023-02-25 08:24:19,882 DEBUG TRAIN Batch 25/7400 loss 16.007332 loss_att 19.309706 loss_ctc 24.330324 loss_rnnt 14.148706 hw_loss 0.165781 lr 0.00034036 rank 1
2023-02-25 08:24:19,883 DEBUG TRAIN Batch 25/7400 loss 10.316158 loss_att 13.304981 loss_ctc 13.408918 loss_rnnt 9.184850 hw_loss 0.227203 lr 0.00034026 rank 7
2023-02-25 08:24:19,883 DEBUG TRAIN Batch 25/7400 loss 2.665394 loss_att 4.702246 loss_ctc 3.347715 loss_rnnt 2.052739 hw_loss 0.214329 lr 0.00034030 rank 0
2023-02-25 08:24:19,886 DEBUG TRAIN Batch 25/7400 loss 5.020712 loss_att 7.436587 loss_ctc 10.987680 loss_rnnt 3.627173 hw_loss 0.215190 lr 0.00034030 rank 2
2023-02-25 08:24:19,917 DEBUG TRAIN Batch 25/7400 loss 11.633969 loss_att 15.138978 loss_ctc 18.771616 loss_rnnt 9.891600 hw_loss 0.168155 lr 0.00034029 rank 6
2023-02-25 08:25:32,966 DEBUG TRAIN Batch 25/7500 loss 8.798138 loss_att 11.786673 loss_ctc 13.598812 loss_rnnt 7.481434 hw_loss 0.147948 lr 0.00034021 rank 6
2023-02-25 08:25:32,968 DEBUG TRAIN Batch 25/7500 loss 9.360153 loss_att 11.071407 loss_ctc 12.879333 loss_rnnt 8.425016 hw_loss 0.231867 lr 0.00034023 rank 0
2023-02-25 08:25:32,972 DEBUG TRAIN Batch 25/7500 loss 14.928383 loss_att 15.956137 loss_ctc 19.760592 loss_rnnt 13.952311 hw_loss 0.236674 lr 0.00034024 rank 3
2023-02-25 08:25:32,973 DEBUG TRAIN Batch 25/7500 loss 7.261718 loss_att 8.120038 loss_ctc 9.979511 loss_rnnt 6.631521 hw_loss 0.180300 lr 0.00034023 rank 4
2023-02-25 08:25:32,975 DEBUG TRAIN Batch 25/7500 loss 8.183124 loss_att 11.762003 loss_ctc 10.301476 loss_rnnt 7.093489 hw_loss 0.171397 lr 0.00034018 rank 7
2023-02-25 08:25:32,977 DEBUG TRAIN Batch 25/7500 loss 5.246735 loss_att 7.835292 loss_ctc 6.595689 loss_rnnt 4.447188 hw_loss 0.191202 lr 0.00034019 rank 5
2023-02-25 08:25:32,978 DEBUG TRAIN Batch 25/7500 loss 7.801323 loss_att 8.812153 loss_ctc 9.308984 loss_rnnt 7.249197 hw_loss 0.279261 lr 0.00034028 rank 1
2023-02-25 08:25:32,979 DEBUG TRAIN Batch 25/7500 loss 8.480265 loss_att 11.745127 loss_ctc 13.761074 loss_rnnt 7.049940 hw_loss 0.137331 lr 0.00034022 rank 2
2023-02-25 08:26:44,418 DEBUG TRAIN Batch 25/7600 loss 9.913922 loss_att 12.958104 loss_ctc 12.713984 loss_rnnt 8.804159 hw_loss 0.239223 lr 0.00034020 rank 1
2023-02-25 08:26:44,420 DEBUG TRAIN Batch 25/7600 loss 16.653460 loss_att 19.317991 loss_ctc 21.493454 loss_rnnt 15.430101 hw_loss 0.084598 lr 0.00034011 rank 5
2023-02-25 08:26:44,428 DEBUG TRAIN Batch 25/7600 loss 6.904624 loss_att 9.126645 loss_ctc 11.746201 loss_rnnt 5.715947 hw_loss 0.185116 lr 0.00034015 rank 0
2023-02-25 08:26:44,431 DEBUG TRAIN Batch 25/7600 loss 2.298508 loss_att 4.283424 loss_ctc 4.705113 loss_rnnt 1.531597 hw_loss 0.091964 lr 0.00034015 rank 4
2023-02-25 08:26:44,438 DEBUG TRAIN Batch 25/7600 loss 7.601645 loss_att 13.558065 loss_ctc 12.820216 loss_rnnt 5.626822 hw_loss 0.164493 lr 0.00034014 rank 2
2023-02-25 08:26:44,442 DEBUG TRAIN Batch 25/7600 loss 10.158279 loss_att 10.695007 loss_ctc 12.784678 loss_rnnt 9.567988 hw_loss 0.248922 lr 0.00034010 rank 7
2023-02-25 08:26:44,442 DEBUG TRAIN Batch 25/7600 loss 10.750827 loss_att 11.111126 loss_ctc 14.914972 loss_rnnt 9.967197 hw_loss 0.293157 lr 0.00034013 rank 6
2023-02-25 08:26:44,486 DEBUG TRAIN Batch 25/7600 loss 5.055753 loss_att 8.936361 loss_ctc 7.720470 loss_rnnt 3.821005 hw_loss 0.193745 lr 0.00034017 rank 3
2023-02-25 08:27:55,422 DEBUG TRAIN Batch 25/7700 loss 4.329239 loss_att 6.773623 loss_ctc 5.731437 loss_rnnt 3.464186 hw_loss 0.354780 lr 0.00034012 rank 1
2023-02-25 08:27:55,422 DEBUG TRAIN Batch 25/7700 loss 9.854726 loss_att 9.864977 loss_ctc 13.513691 loss_rnnt 9.239042 hw_loss 0.235819 lr 0.00034005 rank 6
2023-02-25 08:27:55,427 DEBUG TRAIN Batch 25/7700 loss 4.491344 loss_att 5.435853 loss_ctc 5.771468 loss_rnnt 3.991309 hw_loss 0.263343 lr 0.00034007 rank 4
2023-02-25 08:27:55,427 DEBUG TRAIN Batch 25/7700 loss 2.396375 loss_att 4.835634 loss_ctc 3.099875 loss_rnnt 1.716974 hw_loss 0.183278 lr 0.00034007 rank 0
2023-02-25 08:27:55,429 DEBUG TRAIN Batch 25/7700 loss 5.577582 loss_att 10.361913 loss_ctc 10.326853 loss_rnnt 3.850895 hw_loss 0.256095 lr 0.00034002 rank 7
2023-02-25 08:27:55,429 DEBUG TRAIN Batch 25/7700 loss 8.680964 loss_att 13.482069 loss_ctc 10.716540 loss_rnnt 7.360290 hw_loss 0.166956 lr 0.00034009 rank 3
2023-02-25 08:27:55,429 DEBUG TRAIN Batch 25/7700 loss 17.256016 loss_att 17.111975 loss_ctc 21.977818 loss_rnnt 16.524790 hw_loss 0.244615 lr 0.00034007 rank 2
2023-02-25 08:27:55,431 DEBUG TRAIN Batch 25/7700 loss 6.315122 loss_att 7.110276 loss_ctc 7.515609 loss_rnnt 5.823225 hw_loss 0.324003 lr 0.00034003 rank 5
2023-02-25 08:29:08,021 DEBUG TRAIN Batch 25/7800 loss 5.850336 loss_att 8.905781 loss_ctc 12.865184 loss_rnnt 4.207277 hw_loss 0.181231 lr 0.00034004 rank 1
2023-02-25 08:29:08,030 DEBUG TRAIN Batch 25/7800 loss 6.566231 loss_att 9.005111 loss_ctc 9.066914 loss_rnnt 5.642454 hw_loss 0.192331 lr 0.00034001 rank 3
2023-02-25 08:29:08,030 DEBUG TRAIN Batch 25/7800 loss 7.428352 loss_att 8.910137 loss_ctc 12.222773 loss_rnnt 6.350202 hw_loss 0.267257 lr 0.00034000 rank 4
2023-02-25 08:29:08,030 DEBUG TRAIN Batch 25/7800 loss 15.645964 loss_att 20.789095 loss_ctc 27.689758 loss_rnnt 12.945544 hw_loss 0.123663 lr 0.00033999 rank 0
2023-02-25 08:29:08,031 DEBUG TRAIN Batch 25/7800 loss 8.771106 loss_att 12.007809 loss_ctc 14.300724 loss_rnnt 7.318254 hw_loss 0.127928 lr 0.00033997 rank 6
2023-02-25 08:29:08,033 DEBUG TRAIN Batch 25/7800 loss 5.222429 loss_att 8.949825 loss_ctc 12.544830 loss_rnnt 3.394291 hw_loss 0.199385 lr 0.00033995 rank 7
2023-02-25 08:29:08,033 DEBUG TRAIN Batch 25/7800 loss 16.564821 loss_att 18.897198 loss_ctc 21.229254 loss_rnnt 15.371913 hw_loss 0.195952 lr 0.00033995 rank 5
2023-02-25 08:29:08,062 DEBUG TRAIN Batch 25/7800 loss 7.865622 loss_att 10.670030 loss_ctc 10.009234 loss_rnnt 6.940564 hw_loss 0.146927 lr 0.00033999 rank 2
2023-02-25 08:30:19,369 DEBUG TRAIN Batch 25/7900 loss 6.762506 loss_att 11.495143 loss_ctc 7.408817 loss_rnnt 5.615663 hw_loss 0.214012 lr 0.00033989 rank 6
2023-02-25 08:30:19,368 DEBUG TRAIN Batch 25/7900 loss 9.722817 loss_att 12.552381 loss_ctc 11.043293 loss_rnnt 8.867794 hw_loss 0.211964 lr 0.00033997 rank 1
2023-02-25 08:30:19,369 DEBUG TRAIN Batch 25/7900 loss 6.213756 loss_att 8.016268 loss_ctc 7.866056 loss_rnnt 5.507102 hw_loss 0.235958 lr 0.00033991 rank 0
2023-02-25 08:30:19,370 DEBUG TRAIN Batch 25/7900 loss 5.152601 loss_att 8.578932 loss_ctc 7.875588 loss_rnnt 3.996905 hw_loss 0.201310 lr 0.00033992 rank 4
2023-02-25 08:30:19,373 DEBUG TRAIN Batch 25/7900 loss 3.464685 loss_att 5.431292 loss_ctc 6.258177 loss_rnnt 2.583664 hw_loss 0.216065 lr 0.00033993 rank 3
2023-02-25 08:30:19,374 DEBUG TRAIN Batch 25/7900 loss 8.536663 loss_att 14.151026 loss_ctc 15.364860 loss_rnnt 6.389682 hw_loss 0.213157 lr 0.00033987 rank 7
2023-02-25 08:30:19,379 DEBUG TRAIN Batch 25/7900 loss 11.464528 loss_att 15.415753 loss_ctc 16.512707 loss_rnnt 9.916038 hw_loss 0.159667 lr 0.00033988 rank 5
2023-02-25 08:30:19,381 DEBUG TRAIN Batch 25/7900 loss 6.657421 loss_att 12.653902 loss_ctc 14.234981 loss_rnnt 4.370935 hw_loss 0.144091 lr 0.00033991 rank 2
2023-02-25 08:31:29,756 DEBUG TRAIN Batch 25/8000 loss 6.136487 loss_att 8.687711 loss_ctc 7.776746 loss_rnnt 5.291539 hw_loss 0.217503 lr 0.00033983 rank 2
2023-02-25 08:31:29,758 DEBUG TRAIN Batch 25/8000 loss 6.575824 loss_att 9.127323 loss_ctc 8.716926 loss_rnnt 5.674496 hw_loss 0.197902 lr 0.00033985 rank 3
2023-02-25 08:31:29,757 DEBUG TRAIN Batch 25/8000 loss 15.838957 loss_att 19.130257 loss_ctc 23.141201 loss_rnnt 14.174231 hw_loss 0.061565 lr 0.00033989 rank 1
2023-02-25 08:31:29,759 DEBUG TRAIN Batch 25/8000 loss 11.810966 loss_att 15.647453 loss_ctc 14.182030 loss_rnnt 10.623577 hw_loss 0.194904 lr 0.00033982 rank 6
2023-02-25 08:31:29,760 DEBUG TRAIN Batch 25/8000 loss 13.283625 loss_att 15.575806 loss_ctc 24.596292 loss_rnnt 11.221587 hw_loss 0.178584 lr 0.00033983 rank 0
2023-02-25 08:31:29,762 DEBUG TRAIN Batch 25/8000 loss 9.303979 loss_att 14.306738 loss_ctc 12.402765 loss_rnnt 7.820930 hw_loss 0.129989 lr 0.00033979 rank 7
2023-02-25 08:31:29,762 DEBUG TRAIN Batch 25/8000 loss 3.047923 loss_att 4.994331 loss_ctc 3.684852 loss_rnnt 2.424772 hw_loss 0.279271 lr 0.00033984 rank 4
2023-02-25 08:31:29,766 DEBUG TRAIN Batch 25/8000 loss 10.856629 loss_att 12.506605 loss_ctc 15.187968 loss_rnnt 9.851472 hw_loss 0.183094 lr 0.00033980 rank 5
2023-02-25 08:32:41,704 DEBUG TRAIN Batch 25/8100 loss 5.477194 loss_att 8.554014 loss_ctc 11.222219 loss_rnnt 3.964480 hw_loss 0.246276 lr 0.00033976 rank 4
2023-02-25 08:32:41,713 DEBUG TRAIN Batch 25/8100 loss 4.738369 loss_att 7.508511 loss_ctc 7.034801 loss_rnnt 3.730921 hw_loss 0.276053 lr 0.00033975 rank 0
2023-02-25 08:32:41,720 DEBUG TRAIN Batch 25/8100 loss 7.126594 loss_att 9.370682 loss_ctc 13.482923 loss_rnnt 5.711748 hw_loss 0.222220 lr 0.00033981 rank 1
2023-02-25 08:32:41,723 DEBUG TRAIN Batch 25/8100 loss 3.036358 loss_att 4.853248 loss_ctc 3.645643 loss_rnnt 2.464748 hw_loss 0.238112 lr 0.00033977 rank 3
2023-02-25 08:32:41,726 DEBUG TRAIN Batch 25/8100 loss 10.959583 loss_att 14.038537 loss_ctc 17.920120 loss_rnnt 9.322239 hw_loss 0.175277 lr 0.00033971 rank 7
2023-02-25 08:32:41,726 DEBUG TRAIN Batch 25/8100 loss 15.314830 loss_att 13.971169 loss_ctc 18.524914 loss_rnnt 15.063278 hw_loss 0.173011 lr 0.00033974 rank 6
2023-02-25 08:32:41,728 DEBUG TRAIN Batch 25/8100 loss 7.472469 loss_att 12.105838 loss_ctc 10.393955 loss_rnnt 6.025465 hw_loss 0.245246 lr 0.00033975 rank 2
2023-02-25 08:32:41,735 DEBUG TRAIN Batch 25/8100 loss 2.067118 loss_att 4.145047 loss_ctc 2.830549 loss_rnnt 1.468170 hw_loss 0.152946 lr 0.00033972 rank 5
2023-02-25 08:33:53,132 DEBUG TRAIN Batch 25/8200 loss 4.701149 loss_att 5.637173 loss_ctc 7.439441 loss_rnnt 3.995264 hw_loss 0.287952 lr 0.00033963 rank 7
2023-02-25 08:33:53,136 DEBUG TRAIN Batch 25/8200 loss 8.600579 loss_att 9.025436 loss_ctc 12.096309 loss_rnnt 7.896750 hw_loss 0.286425 lr 0.00033969 rank 3
2023-02-25 08:33:53,138 DEBUG TRAIN Batch 25/8200 loss 12.844205 loss_att 17.459461 loss_ctc 22.927258 loss_rnnt 10.461277 hw_loss 0.216505 lr 0.00033973 rank 1
2023-02-25 08:33:53,138 DEBUG TRAIN Batch 25/8200 loss 10.279959 loss_att 12.598066 loss_ctc 19.201206 loss_rnnt 8.545025 hw_loss 0.153398 lr 0.00033966 rank 6
2023-02-25 08:33:53,138 DEBUG TRAIN Batch 25/8200 loss 9.633015 loss_att 12.578120 loss_ctc 11.390575 loss_rnnt 8.658228 hw_loss 0.283921 lr 0.00033968 rank 4
2023-02-25 08:33:53,139 DEBUG TRAIN Batch 25/8200 loss 10.724074 loss_att 12.323538 loss_ctc 15.729015 loss_rnnt 9.642448 hw_loss 0.177014 lr 0.00033964 rank 5
2023-02-25 08:33:53,140 DEBUG TRAIN Batch 25/8200 loss 11.609396 loss_att 14.109693 loss_ctc 16.036465 loss_rnnt 10.415913 hw_loss 0.193404 lr 0.00033967 rank 2
2023-02-25 08:33:53,146 DEBUG TRAIN Batch 25/8200 loss 8.699681 loss_att 17.061127 loss_ctc 9.843397 loss_rnnt 6.721521 hw_loss 0.287581 lr 0.00033968 rank 0
2023-02-25 08:35:03,098 DEBUG TRAIN Batch 25/8300 loss 9.291818 loss_att 11.448020 loss_ctc 13.224268 loss_rnnt 8.246970 hw_loss 0.167401 lr 0.00033965 rank 1
2023-02-25 08:35:03,115 DEBUG TRAIN Batch 25/8300 loss 4.001355 loss_att 5.024041 loss_ctc 3.592173 loss_rnnt 3.749847 hw_loss 0.190366 lr 0.00033960 rank 0
2023-02-25 08:35:03,117 DEBUG TRAIN Batch 25/8300 loss 2.954089 loss_att 5.683355 loss_ctc 4.916020 loss_rnnt 2.076500 hw_loss 0.131522 lr 0.00033959 rank 2
2023-02-25 08:35:03,121 DEBUG TRAIN Batch 25/8300 loss 6.609629 loss_att 9.858524 loss_ctc 6.628113 loss_rnnt 5.890041 hw_loss 0.126270 lr 0.00033960 rank 4
2023-02-25 08:35:03,121 DEBUG TRAIN Batch 25/8300 loss 13.012820 loss_att 18.217148 loss_ctc 15.553524 loss_rnnt 11.508394 hw_loss 0.233999 lr 0.00033962 rank 3
2023-02-25 08:35:03,123 DEBUG TRAIN Batch 25/8300 loss 11.675464 loss_att 11.785104 loss_ctc 16.160303 loss_rnnt 10.995244 hw_loss 0.113084 lr 0.00033958 rank 6
2023-02-25 08:35:03,124 DEBUG TRAIN Batch 25/8300 loss 5.877383 loss_att 9.640114 loss_ctc 11.166160 loss_rnnt 4.302113 hw_loss 0.220412 lr 0.00033955 rank 7
2023-02-25 08:35:03,128 DEBUG TRAIN Batch 25/8300 loss 12.841691 loss_att 15.138993 loss_ctc 18.624317 loss_rnnt 11.516134 hw_loss 0.178274 lr 0.00033956 rank 5
2023-02-25 08:35:53,110 DEBUG CV Batch 25/0 loss 1.444644 loss_att 1.385556 loss_ctc 1.828947 loss_rnnt 1.268354 hw_loss 0.256625 history loss 1.391139 rank 1
2023-02-25 08:35:53,111 DEBUG CV Batch 25/0 loss 1.444644 loss_att 1.385556 loss_ctc 1.828947 loss_rnnt 1.268354 hw_loss 0.256625 history loss 1.391139 rank 3
2023-02-25 08:35:53,117 DEBUG CV Batch 25/0 loss 1.444644 loss_att 1.385556 loss_ctc 1.828947 loss_rnnt 1.268354 hw_loss 0.256625 history loss 1.391139 rank 6
2023-02-25 08:35:53,118 DEBUG CV Batch 25/0 loss 1.444644 loss_att 1.385556 loss_ctc 1.828947 loss_rnnt 1.268354 hw_loss 0.256625 history loss 1.391139 rank 5
2023-02-25 08:35:53,124 DEBUG CV Batch 25/0 loss 1.444644 loss_att 1.385556 loss_ctc 1.828947 loss_rnnt 1.268354 hw_loss 0.256625 history loss 1.391139 rank 2
2023-02-25 08:35:53,125 DEBUG CV Batch 25/0 loss 1.444644 loss_att 1.385556 loss_ctc 1.828947 loss_rnnt 1.268354 hw_loss 0.256625 history loss 1.391139 rank 0
2023-02-25 08:35:53,131 DEBUG CV Batch 25/0 loss 1.444644 loss_att 1.385556 loss_ctc 1.828947 loss_rnnt 1.268354 hw_loss 0.256625 history loss 1.391139 rank 7
2023-02-25 08:35:53,138 DEBUG CV Batch 25/0 loss 1.444644 loss_att 1.385556 loss_ctc 1.828947 loss_rnnt 1.268354 hw_loss 0.256625 history loss 1.391139 rank 4
2023-02-25 08:36:04,435 DEBUG CV Batch 25/100 loss 6.956578 loss_att 7.004634 loss_ctc 9.967947 loss_rnnt 6.411903 hw_loss 0.250404 history loss 3.187920 rank 4
2023-02-25 08:36:04,558 DEBUG CV Batch 25/100 loss 6.956578 loss_att 7.004634 loss_ctc 9.967947 loss_rnnt 6.411903 hw_loss 0.250404 history loss 3.187920 rank 3
2023-02-25 08:36:04,654 DEBUG CV Batch 25/100 loss 6.956578 loss_att 7.004634 loss_ctc 9.967947 loss_rnnt 6.411903 hw_loss 0.250404 history loss 3.187920 rank 1
2023-02-25 08:36:04,693 DEBUG CV Batch 25/100 loss 6.956578 loss_att 7.004634 loss_ctc 9.967947 loss_rnnt 6.411903 hw_loss 0.250404 history loss 3.187920 rank 5
2023-02-25 08:36:04,759 DEBUG CV Batch 25/100 loss 6.956578 loss_att 7.004634 loss_ctc 9.967947 loss_rnnt 6.411903 hw_loss 0.250404 history loss 3.187920 rank 0
2023-02-25 08:36:04,775 DEBUG CV Batch 25/100 loss 6.956578 loss_att 7.004634 loss_ctc 9.967947 loss_rnnt 6.411903 hw_loss 0.250404 history loss 3.187920 rank 6
2023-02-25 08:36:04,804 DEBUG CV Batch 25/100 loss 6.956578 loss_att 7.004634 loss_ctc 9.967947 loss_rnnt 6.411903 hw_loss 0.250404 history loss 3.187920 rank 2
2023-02-25 08:36:05,191 DEBUG CV Batch 25/100 loss 6.956578 loss_att 7.004634 loss_ctc 9.967947 loss_rnnt 6.411903 hw_loss 0.250404 history loss 3.187920 rank 7
2023-02-25 08:36:18,399 DEBUG CV Batch 25/200 loss 3.631707 loss_att 12.219601 loss_ctc 3.519326 loss_rnnt 1.831847 hw_loss 0.182373 history loss 3.840589 rank 1
2023-02-25 08:36:18,420 DEBUG CV Batch 25/200 loss 3.631707 loss_att 12.219601 loss_ctc 3.519326 loss_rnnt 1.831847 hw_loss 0.182373 history loss 3.840589 rank 6
2023-02-25 08:36:18,444 DEBUG CV Batch 25/200 loss 3.631707 loss_att 12.219601 loss_ctc 3.519326 loss_rnnt 1.831847 hw_loss 0.182373 history loss 3.840589 rank 5
2023-02-25 08:36:18,589 DEBUG CV Batch 25/200 loss 3.631707 loss_att 12.219601 loss_ctc 3.519326 loss_rnnt 1.831847 hw_loss 0.182373 history loss 3.840589 rank 0
2023-02-25 08:36:18,638 DEBUG CV Batch 25/200 loss 3.631707 loss_att 12.219601 loss_ctc 3.519326 loss_rnnt 1.831847 hw_loss 0.182373 history loss 3.840589 rank 4
2023-02-25 08:36:18,638 DEBUG CV Batch 25/200 loss 3.631707 loss_att 12.219601 loss_ctc 3.519326 loss_rnnt 1.831847 hw_loss 0.182373 history loss 3.840589 rank 2
2023-02-25 08:36:18,651 DEBUG CV Batch 25/200 loss 3.631707 loss_att 12.219601 loss_ctc 3.519326 loss_rnnt 1.831847 hw_loss 0.182373 history loss 3.840589 rank 3
2023-02-25 08:36:19,196 DEBUG CV Batch 25/200 loss 3.631707 loss_att 12.219601 loss_ctc 3.519326 loss_rnnt 1.831847 hw_loss 0.182373 history loss 3.840589 rank 7
2023-02-25 08:36:30,356 DEBUG CV Batch 25/300 loss 4.919632 loss_att 5.336845 loss_ctc 7.881840 loss_rnnt 4.279691 hw_loss 0.302884 history loss 3.973512 rank 6
2023-02-25 08:36:30,607 DEBUG CV Batch 25/300 loss 4.919632 loss_att 5.336845 loss_ctc 7.881840 loss_rnnt 4.279691 hw_loss 0.302884 history loss 3.973512 rank 3
2023-02-25 08:36:30,631 DEBUG CV Batch 25/300 loss 4.919632 loss_att 5.336845 loss_ctc 7.881840 loss_rnnt 4.279691 hw_loss 0.302884 history loss 3.973512 rank 5
2023-02-25 08:36:30,727 DEBUG CV Batch 25/300 loss 4.919632 loss_att 5.336845 loss_ctc 7.881840 loss_rnnt 4.279691 hw_loss 0.302884 history loss 3.973512 rank 1
2023-02-25 08:36:30,964 DEBUG CV Batch 25/300 loss 4.919632 loss_att 5.336845 loss_ctc 7.881840 loss_rnnt 4.279691 hw_loss 0.302884 history loss 3.973512 rank 0
2023-02-25 08:36:30,990 DEBUG CV Batch 25/300 loss 4.919632 loss_att 5.336845 loss_ctc 7.881840 loss_rnnt 4.279691 hw_loss 0.302884 history loss 3.973512 rank 4
2023-02-25 08:36:31,019 DEBUG CV Batch 25/300 loss 4.919632 loss_att 5.336845 loss_ctc 7.881840 loss_rnnt 4.279691 hw_loss 0.302884 history loss 3.973512 rank 2
2023-02-25 08:36:31,777 DEBUG CV Batch 25/300 loss 4.919632 loss_att 5.336845 loss_ctc 7.881840 loss_rnnt 4.279691 hw_loss 0.302884 history loss 3.973512 rank 7
2023-02-25 08:36:42,256 DEBUG CV Batch 25/400 loss 16.019522 loss_att 91.170876 loss_ctc 5.270670 loss_rnnt 2.400328 hw_loss 0.041440 history loss 4.836377 rank 6
2023-02-25 08:36:42,786 DEBUG CV Batch 25/400 loss 16.019522 loss_att 91.170876 loss_ctc 5.270670 loss_rnnt 2.400328 hw_loss 0.041440 history loss 4.836377 rank 5
2023-02-25 08:36:42,851 DEBUG CV Batch 25/400 loss 16.019522 loss_att 91.170876 loss_ctc 5.270670 loss_rnnt 2.400328 hw_loss 0.041440 history loss 4.836377 rank 4
2023-02-25 08:36:42,989 DEBUG CV Batch 25/400 loss 16.019522 loss_att 91.170876 loss_ctc 5.270670 loss_rnnt 2.400328 hw_loss 0.041440 history loss 4.836377 rank 3
2023-02-25 08:36:43,008 DEBUG CV Batch 25/400 loss 16.019522 loss_att 91.170876 loss_ctc 5.270670 loss_rnnt 2.400328 hw_loss 0.041440 history loss 4.836377 rank 1
2023-02-25 08:36:43,386 DEBUG CV Batch 25/400 loss 16.019522 loss_att 91.170876 loss_ctc 5.270670 loss_rnnt 2.400328 hw_loss 0.041440 history loss 4.836377 rank 0
2023-02-25 08:36:43,481 DEBUG CV Batch 25/400 loss 16.019522 loss_att 91.170876 loss_ctc 5.270670 loss_rnnt 2.400328 hw_loss 0.041440 history loss 4.836377 rank 2
2023-02-25 08:36:44,422 DEBUG CV Batch 25/400 loss 16.019522 loss_att 91.170876 loss_ctc 5.270670 loss_rnnt 2.400328 hw_loss 0.041440 history loss 4.836377 rank 7
2023-02-25 08:36:52,894 DEBUG CV Batch 25/500 loss 5.442822 loss_att 6.076341 loss_ctc 7.754106 loss_rnnt 4.868035 hw_loss 0.262333 history loss 5.529741 rank 6
2023-02-25 08:36:53,195 DEBUG CV Batch 25/500 loss 5.442822 loss_att 6.076341 loss_ctc 7.754106 loss_rnnt 4.868035 hw_loss 0.262333 history loss 5.529741 rank 4
2023-02-25 08:36:53,322 DEBUG CV Batch 25/500 loss 5.442822 loss_att 6.076341 loss_ctc 7.754106 loss_rnnt 4.868035 hw_loss 0.262333 history loss 5.529741 rank 5
2023-02-25 08:36:53,459 DEBUG CV Batch 25/500 loss 5.442822 loss_att 6.076341 loss_ctc 7.754106 loss_rnnt 4.868035 hw_loss 0.262333 history loss 5.529741 rank 3
2023-02-25 08:36:53,644 DEBUG CV Batch 25/500 loss 5.442822 loss_att 6.076341 loss_ctc 7.754106 loss_rnnt 4.868035 hw_loss 0.262333 history loss 5.529741 rank 1
2023-02-25 08:36:54,202 DEBUG CV Batch 25/500 loss 5.442822 loss_att 6.076341 loss_ctc 7.754106 loss_rnnt 4.868035 hw_loss 0.262333 history loss 5.529741 rank 0
2023-02-25 08:36:54,316 DEBUG CV Batch 25/500 loss 5.442822 loss_att 6.076341 loss_ctc 7.754106 loss_rnnt 4.868035 hw_loss 0.262333 history loss 5.529741 rank 2
2023-02-25 08:36:55,379 DEBUG CV Batch 25/500 loss 5.442822 loss_att 6.076341 loss_ctc 7.754106 loss_rnnt 4.868035 hw_loss 0.262333 history loss 5.529741 rank 7
2023-02-25 08:37:05,207 DEBUG CV Batch 25/600 loss 5.863211 loss_att 6.440553 loss_ctc 7.857634 loss_rnnt 5.347645 hw_loss 0.251577 history loss 6.476589 rank 6
2023-02-25 08:37:05,311 DEBUG CV Batch 25/600 loss 5.863211 loss_att 6.440553 loss_ctc 7.857634 loss_rnnt 5.347645 hw_loss 0.251577 history loss 6.476589 rank 4
2023-02-25 08:37:05,429 DEBUG CV Batch 25/600 loss 5.863211 loss_att 6.440553 loss_ctc 7.857634 loss_rnnt 5.347645 hw_loss 0.251577 history loss 6.476589 rank 5
2023-02-25 08:37:05,626 DEBUG CV Batch 25/600 loss 5.863211 loss_att 6.440553 loss_ctc 7.857634 loss_rnnt 5.347645 hw_loss 0.251577 history loss 6.476589 rank 3
2023-02-25 08:37:05,906 DEBUG CV Batch 25/600 loss 5.863211 loss_att 6.440553 loss_ctc 7.857634 loss_rnnt 5.347645 hw_loss 0.251577 history loss 6.476589 rank 1
2023-02-25 08:37:06,711 DEBUG CV Batch 25/600 loss 5.863211 loss_att 6.440553 loss_ctc 7.857634 loss_rnnt 5.347645 hw_loss 0.251577 history loss 6.476589 rank 0
2023-02-25 08:37:06,726 DEBUG CV Batch 25/600 loss 5.863211 loss_att 6.440553 loss_ctc 7.857634 loss_rnnt 5.347645 hw_loss 0.251577 history loss 6.476589 rank 2
2023-02-25 08:37:07,895 DEBUG CV Batch 25/600 loss 5.863211 loss_att 6.440553 loss_ctc 7.857634 loss_rnnt 5.347645 hw_loss 0.251577 history loss 6.476589 rank 7
2023-02-25 08:37:17,172 DEBUG CV Batch 25/700 loss 17.379671 loss_att 48.531368 loss_ctc 14.614338 loss_rnnt 11.407617 hw_loss 0.207047 history loss 7.130841 rank 5
2023-02-25 08:37:17,255 DEBUG CV Batch 25/700 loss 17.379671 loss_att 48.531368 loss_ctc 14.614338 loss_rnnt 11.407617 hw_loss 0.207047 history loss 7.130841 rank 4
2023-02-25 08:37:17,404 DEBUG CV Batch 25/700 loss 17.379671 loss_att 48.531368 loss_ctc 14.614338 loss_rnnt 11.407617 hw_loss 0.207047 history loss 7.130841 rank 3
2023-02-25 08:37:17,475 DEBUG CV Batch 25/700 loss 17.379671 loss_att 48.531368 loss_ctc 14.614338 loss_rnnt 11.407617 hw_loss 0.207047 history loss 7.130841 rank 6
2023-02-25 08:37:17,600 DEBUG CV Batch 25/700 loss 17.379671 loss_att 48.531368 loss_ctc 14.614338 loss_rnnt 11.407617 hw_loss 0.207047 history loss 7.130841 rank 1
2023-02-25 08:37:18,550 DEBUG CV Batch 25/700 loss 17.379671 loss_att 48.531368 loss_ctc 14.614338 loss_rnnt 11.407617 hw_loss 0.207047 history loss 7.130841 rank 2
2023-02-25 08:37:18,642 DEBUG CV Batch 25/700 loss 17.379671 loss_att 48.531368 loss_ctc 14.614338 loss_rnnt 11.407617 hw_loss 0.207047 history loss 7.130841 rank 0
2023-02-25 08:37:19,840 DEBUG CV Batch 25/700 loss 17.379671 loss_att 48.531368 loss_ctc 14.614338 loss_rnnt 11.407617 hw_loss 0.207047 history loss 7.130841 rank 7
2023-02-25 08:37:28,582 DEBUG CV Batch 25/800 loss 11.681102 loss_att 10.630109 loss_ctc 17.431215 loss_rnnt 11.006413 hw_loss 0.221637 history loss 6.604642 rank 5
2023-02-25 08:37:28,944 DEBUG CV Batch 25/800 loss 11.681102 loss_att 10.630109 loss_ctc 17.431215 loss_rnnt 11.006413 hw_loss 0.221637 history loss 6.604642 rank 1
2023-02-25 08:37:29,193 DEBUG CV Batch 25/800 loss 11.681102 loss_att 10.630109 loss_ctc 17.431215 loss_rnnt 11.006413 hw_loss 0.221637 history loss 6.604642 rank 4
2023-02-25 08:37:29,384 DEBUG CV Batch 25/800 loss 11.681102 loss_att 10.630109 loss_ctc 17.431215 loss_rnnt 11.006413 hw_loss 0.221637 history loss 6.604642 rank 6
2023-02-25 08:37:29,572 DEBUG CV Batch 25/800 loss 11.681102 loss_att 10.630109 loss_ctc 17.431215 loss_rnnt 11.006413 hw_loss 0.221637 history loss 6.604642 rank 3
2023-02-25 08:37:30,028 DEBUG CV Batch 25/800 loss 11.681102 loss_att 10.630109 loss_ctc 17.431215 loss_rnnt 11.006413 hw_loss 0.221637 history loss 6.604642 rank 2
2023-02-25 08:37:30,239 DEBUG CV Batch 25/800 loss 11.681102 loss_att 10.630109 loss_ctc 17.431215 loss_rnnt 11.006413 hw_loss 0.221637 history loss 6.604642 rank 0
2023-02-25 08:37:31,393 DEBUG CV Batch 25/800 loss 11.681102 loss_att 10.630109 loss_ctc 17.431215 loss_rnnt 11.006413 hw_loss 0.221637 history loss 6.604642 rank 7
2023-02-25 08:37:42,283 DEBUG CV Batch 25/900 loss 12.376632 loss_att 14.686539 loss_ctc 21.840120 loss_rnnt 10.602708 hw_loss 0.094019 history loss 6.420511 rank 5
2023-02-25 08:37:42,617 DEBUG CV Batch 25/900 loss 12.376632 loss_att 14.686539 loss_ctc 21.840120 loss_rnnt 10.602708 hw_loss 0.094019 history loss 6.420511 rank 1
2023-02-25 08:37:43,019 DEBUG CV Batch 25/900 loss 12.376632 loss_att 14.686539 loss_ctc 21.840120 loss_rnnt 10.602708 hw_loss 0.094019 history loss 6.420511 rank 4
2023-02-25 08:37:43,040 DEBUG CV Batch 25/900 loss 12.376632 loss_att 14.686539 loss_ctc 21.840120 loss_rnnt 10.602708 hw_loss 0.094019 history loss 6.420511 rank 6
2023-02-25 08:37:43,151 DEBUG CV Batch 25/900 loss 12.376632 loss_att 14.686539 loss_ctc 21.840120 loss_rnnt 10.602708 hw_loss 0.094019 history loss 6.420511 rank 3
2023-02-25 08:37:43,855 DEBUG CV Batch 25/900 loss 12.376632 loss_att 14.686539 loss_ctc 21.840120 loss_rnnt 10.602708 hw_loss 0.094019 history loss 6.420511 rank 2
2023-02-25 08:37:44,170 DEBUG CV Batch 25/900 loss 12.376632 loss_att 14.686539 loss_ctc 21.840120 loss_rnnt 10.602708 hw_loss 0.094019 history loss 6.420511 rank 0
2023-02-25 08:37:45,291 DEBUG CV Batch 25/900 loss 12.376632 loss_att 14.686539 loss_ctc 21.840120 loss_rnnt 10.602708 hw_loss 0.094019 history loss 6.420511 rank 7
2023-02-25 08:37:54,522 DEBUG CV Batch 25/1000 loss 5.180766 loss_att 4.605190 loss_ctc 4.786562 loss_rnnt 5.236917 hw_loss 0.209108 history loss 6.199938 rank 5
2023-02-25 08:37:55,148 DEBUG CV Batch 25/1000 loss 5.180766 loss_att 4.605190 loss_ctc 4.786562 loss_rnnt 5.236917 hw_loss 0.209108 history loss 6.199938 rank 4
2023-02-25 08:37:55,327 DEBUG CV Batch 25/1000 loss 5.180766 loss_att 4.605190 loss_ctc 4.786562 loss_rnnt 5.236917 hw_loss 0.209108 history loss 6.199938 rank 1
2023-02-25 08:37:55,338 DEBUG CV Batch 25/1000 loss 5.180766 loss_att 4.605190 loss_ctc 4.786562 loss_rnnt 5.236917 hw_loss 0.209108 history loss 6.199938 rank 6
2023-02-25 08:37:55,382 DEBUG CV Batch 25/1000 loss 5.180766 loss_att 4.605190 loss_ctc 4.786562 loss_rnnt 5.236917 hw_loss 0.209108 history loss 6.199938 rank 3
2023-02-25 08:37:56,541 DEBUG CV Batch 25/1000 loss 5.180766 loss_att 4.605190 loss_ctc 4.786562 loss_rnnt 5.236917 hw_loss 0.209108 history loss 6.199938 rank 2
2023-02-25 08:37:56,945 DEBUG CV Batch 25/1000 loss 5.180766 loss_att 4.605190 loss_ctc 4.786562 loss_rnnt 5.236917 hw_loss 0.209108 history loss 6.199938 rank 0
2023-02-25 08:37:58,091 DEBUG CV Batch 25/1000 loss 5.180766 loss_att 4.605190 loss_ctc 4.786562 loss_rnnt 5.236917 hw_loss 0.209108 history loss 6.199938 rank 7
2023-02-25 08:38:06,702 DEBUG CV Batch 25/1100 loss 5.291222 loss_att 4.989313 loss_ctc 7.389005 loss_rnnt 4.960535 hw_loss 0.208808 history loss 6.165786 rank 5
2023-02-25 08:38:07,037 DEBUG CV Batch 25/1100 loss 5.291222 loss_att 4.989313 loss_ctc 7.389005 loss_rnnt 4.960535 hw_loss 0.208808 history loss 6.165786 rank 4
2023-02-25 08:38:07,259 DEBUG CV Batch 25/1100 loss 5.291222 loss_att 4.989313 loss_ctc 7.389005 loss_rnnt 4.960535 hw_loss 0.208808 history loss 6.165786 rank 6
2023-02-25 08:38:07,288 DEBUG CV Batch 25/1100 loss 5.291222 loss_att 4.989313 loss_ctc 7.389005 loss_rnnt 4.960535 hw_loss 0.208808 history loss 6.165786 rank 3
2023-02-25 08:38:07,453 DEBUG CV Batch 25/1100 loss 5.291222 loss_att 4.989313 loss_ctc 7.389005 loss_rnnt 4.960535 hw_loss 0.208808 history loss 6.165786 rank 1
2023-02-25 08:38:08,926 DEBUG CV Batch 25/1100 loss 5.291222 loss_att 4.989313 loss_ctc 7.389005 loss_rnnt 4.960535 hw_loss 0.208808 history loss 6.165786 rank 2
2023-02-25 08:38:09,403 DEBUG CV Batch 25/1100 loss 5.291222 loss_att 4.989313 loss_ctc 7.389005 loss_rnnt 4.960535 hw_loss 0.208808 history loss 6.165786 rank 0
2023-02-25 08:38:10,565 DEBUG CV Batch 25/1100 loss 5.291222 loss_att 4.989313 loss_ctc 7.389005 loss_rnnt 4.960535 hw_loss 0.208808 history loss 6.165786 rank 7
2023-02-25 08:38:17,343 DEBUG CV Batch 25/1200 loss 5.694055 loss_att 7.212872 loss_ctc 7.604492 loss_rnnt 5.024526 hw_loss 0.208201 history loss 6.459233 rank 5
2023-02-25 08:38:17,696 DEBUG CV Batch 25/1200 loss 5.694055 loss_att 7.212872 loss_ctc 7.604492 loss_rnnt 5.024526 hw_loss 0.208201 history loss 6.459233 rank 3
2023-02-25 08:38:17,701 DEBUG CV Batch 25/1200 loss 5.694055 loss_att 7.212872 loss_ctc 7.604492 loss_rnnt 5.024526 hw_loss 0.208201 history loss 6.459233 rank 4
2023-02-25 08:38:18,093 DEBUG CV Batch 25/1200 loss 5.694055 loss_att 7.212872 loss_ctc 7.604492 loss_rnnt 5.024526 hw_loss 0.208201 history loss 6.459233 rank 6
2023-02-25 08:38:18,461 DEBUG CV Batch 25/1200 loss 5.694055 loss_att 7.212872 loss_ctc 7.604492 loss_rnnt 5.024526 hw_loss 0.208201 history loss 6.459233 rank 1
2023-02-25 08:38:20,143 DEBUG CV Batch 25/1200 loss 5.694055 loss_att 7.212872 loss_ctc 7.604492 loss_rnnt 5.024526 hw_loss 0.208201 history loss 6.459233 rank 2
2023-02-25 08:38:20,586 DEBUG CV Batch 25/1200 loss 5.694055 loss_att 7.212872 loss_ctc 7.604492 loss_rnnt 5.024526 hw_loss 0.208201 history loss 6.459233 rank 0
2023-02-25 08:38:21,821 DEBUG CV Batch 25/1200 loss 5.694055 loss_att 7.212872 loss_ctc 7.604492 loss_rnnt 5.024526 hw_loss 0.208201 history loss 6.459233 rank 7
2023-02-25 08:38:29,595 DEBUG CV Batch 25/1300 loss 4.615521 loss_att 5.224161 loss_ctc 6.710240 loss_rnnt 4.111203 hw_loss 0.193676 history loss 6.780162 rank 5
2023-02-25 08:38:29,730 DEBUG CV Batch 25/1300 loss 4.615521 loss_att 5.224161 loss_ctc 6.710240 loss_rnnt 4.111203 hw_loss 0.193676 history loss 6.780162 rank 3
2023-02-25 08:38:29,736 DEBUG CV Batch 25/1300 loss 4.615521 loss_att 5.224161 loss_ctc 6.710240 loss_rnnt 4.111203 hw_loss 0.193676 history loss 6.780162 rank 4
2023-02-25 08:38:30,123 DEBUG CV Batch 25/1300 loss 4.615521 loss_att 5.224161 loss_ctc 6.710240 loss_rnnt 4.111203 hw_loss 0.193676 history loss 6.780162 rank 6
2023-02-25 08:38:30,714 DEBUG CV Batch 25/1300 loss 4.615521 loss_att 5.224161 loss_ctc 6.710240 loss_rnnt 4.111203 hw_loss 0.193676 history loss 6.780162 rank 1
2023-02-25 08:38:32,591 DEBUG CV Batch 25/1300 loss 4.615521 loss_att 5.224161 loss_ctc 6.710240 loss_rnnt 4.111203 hw_loss 0.193676 history loss 6.780162 rank 2
2023-02-25 08:38:33,059 DEBUG CV Batch 25/1300 loss 4.615521 loss_att 5.224161 loss_ctc 6.710240 loss_rnnt 4.111203 hw_loss 0.193676 history loss 6.780162 rank 0
2023-02-25 08:38:34,455 DEBUG CV Batch 25/1300 loss 4.615521 loss_att 5.224161 loss_ctc 6.710240 loss_rnnt 4.111203 hw_loss 0.193676 history loss 6.780162 rank 7
2023-02-25 08:38:41,012 DEBUG CV Batch 25/1400 loss 8.854861 loss_att 20.042942 loss_ctc 9.834773 loss_rnnt 6.460986 hw_loss 0.048007 history loss 7.095968 rank 3
2023-02-25 08:38:41,264 DEBUG CV Batch 25/1400 loss 8.854861 loss_att 20.042942 loss_ctc 9.834773 loss_rnnt 6.460986 hw_loss 0.048007 history loss 7.095968 rank 6
2023-02-25 08:38:41,308 DEBUG CV Batch 25/1400 loss 8.854861 loss_att 20.042942 loss_ctc 9.834773 loss_rnnt 6.460986 hw_loss 0.048007 history loss 7.095968 rank 4
2023-02-25 08:38:41,469 DEBUG CV Batch 25/1400 loss 8.854861 loss_att 20.042942 loss_ctc 9.834773 loss_rnnt 6.460986 hw_loss 0.048007 history loss 7.095968 rank 5
2023-02-25 08:38:42,180 DEBUG CV Batch 25/1400 loss 8.854861 loss_att 20.042942 loss_ctc 9.834773 loss_rnnt 6.460986 hw_loss 0.048007 history loss 7.095968 rank 1
2023-02-25 08:38:44,333 DEBUG CV Batch 25/1400 loss 8.854861 loss_att 20.042942 loss_ctc 9.834773 loss_rnnt 6.460986 hw_loss 0.048007 history loss 7.095968 rank 2
2023-02-25 08:38:44,758 DEBUG CV Batch 25/1400 loss 8.854861 loss_att 20.042942 loss_ctc 9.834773 loss_rnnt 6.460986 hw_loss 0.048007 history loss 7.095968 rank 0
2023-02-25 08:38:46,331 DEBUG CV Batch 25/1400 loss 8.854861 loss_att 20.042942 loss_ctc 9.834773 loss_rnnt 6.460986 hw_loss 0.048007 history loss 7.095968 rank 7
2023-02-25 08:38:52,799 DEBUG CV Batch 25/1500 loss 6.295722 loss_att 6.747047 loss_ctc 5.595338 loss_rnnt 6.212440 hw_loss 0.162004 history loss 6.943236 rank 5
2023-02-25 08:38:53,485 DEBUG CV Batch 25/1500 loss 6.295722 loss_att 6.747047 loss_ctc 5.595338 loss_rnnt 6.212440 hw_loss 0.162004 history loss 6.943236 rank 3
2023-02-25 08:38:53,564 DEBUG CV Batch 25/1500 loss 6.295722 loss_att 6.747047 loss_ctc 5.595338 loss_rnnt 6.212440 hw_loss 0.162004 history loss 6.943236 rank 6
2023-02-25 08:38:53,874 DEBUG CV Batch 25/1500 loss 6.295722 loss_att 6.747047 loss_ctc 5.595338 loss_rnnt 6.212440 hw_loss 0.162004 history loss 6.943236 rank 4
2023-02-25 08:38:54,018 DEBUG CV Batch 25/1500 loss 6.295722 loss_att 6.747047 loss_ctc 5.595338 loss_rnnt 6.212440 hw_loss 0.162004 history loss 6.943236 rank 1
2023-02-25 08:38:56,278 DEBUG CV Batch 25/1500 loss 6.295722 loss_att 6.747047 loss_ctc 5.595338 loss_rnnt 6.212440 hw_loss 0.162004 history loss 6.943236 rank 2
2023-02-25 08:38:56,730 DEBUG CV Batch 25/1500 loss 6.295722 loss_att 6.747047 loss_ctc 5.595338 loss_rnnt 6.212440 hw_loss 0.162004 history loss 6.943236 rank 0
2023-02-25 08:38:58,516 DEBUG CV Batch 25/1500 loss 6.295722 loss_att 6.747047 loss_ctc 5.595338 loss_rnnt 6.212440 hw_loss 0.162004 history loss 6.943236 rank 7
2023-02-25 08:39:06,000 DEBUG CV Batch 25/1600 loss 11.962925 loss_att 12.420841 loss_ctc 9.050811 loss_rnnt 12.177110 hw_loss 0.154715 history loss 6.884064 rank 5
2023-02-25 08:39:06,969 DEBUG CV Batch 25/1600 loss 11.962925 loss_att 12.420841 loss_ctc 9.050811 loss_rnnt 12.177110 hw_loss 0.154715 history loss 6.884064 rank 6
2023-02-25 08:39:07,235 DEBUG CV Batch 25/1600 loss 11.962925 loss_att 12.420841 loss_ctc 9.050811 loss_rnnt 12.177110 hw_loss 0.154715 history loss 6.884064 rank 3
2023-02-25 08:39:07,372 DEBUG CV Batch 25/1600 loss 11.962925 loss_att 12.420841 loss_ctc 9.050811 loss_rnnt 12.177110 hw_loss 0.154715 history loss 6.884064 rank 1
2023-02-25 08:39:07,428 DEBUG CV Batch 25/1600 loss 11.962925 loss_att 12.420841 loss_ctc 9.050811 loss_rnnt 12.177110 hw_loss 0.154715 history loss 6.884064 rank 4
2023-02-25 08:39:09,728 DEBUG CV Batch 25/1600 loss 11.962925 loss_att 12.420841 loss_ctc 9.050811 loss_rnnt 12.177110 hw_loss 0.154715 history loss 6.884064 rank 2
2023-02-25 08:39:10,176 DEBUG CV Batch 25/1600 loss 11.962925 loss_att 12.420841 loss_ctc 9.050811 loss_rnnt 12.177110 hw_loss 0.154715 history loss 6.884064 rank 0
2023-02-25 08:39:12,146 DEBUG CV Batch 25/1600 loss 11.962925 loss_att 12.420841 loss_ctc 9.050811 loss_rnnt 12.177110 hw_loss 0.154715 history loss 6.884064 rank 7
2023-02-25 08:39:18,994 DEBUG CV Batch 25/1700 loss 10.096857 loss_att 7.946252 loss_ctc 13.951764 loss_rnnt 9.871687 hw_loss 0.264945 history loss 6.801537 rank 5
2023-02-25 08:39:19,390 DEBUG CV Batch 25/1700 loss 10.096857 loss_att 7.946252 loss_ctc 13.951764 loss_rnnt 9.871687 hw_loss 0.264945 history loss 6.801537 rank 6
2023-02-25 08:39:19,822 DEBUG CV Batch 25/1700 loss 10.096857 loss_att 7.946252 loss_ctc 13.951764 loss_rnnt 9.871687 hw_loss 0.264945 history loss 6.801537 rank 3
2023-02-25 08:39:19,959 DEBUG CV Batch 25/1700 loss 10.096857 loss_att 7.946252 loss_ctc 13.951764 loss_rnnt 9.871687 hw_loss 0.264945 history loss 6.801537 rank 4
2023-02-25 08:39:20,107 DEBUG CV Batch 25/1700 loss 10.096857 loss_att 7.946252 loss_ctc 13.951764 loss_rnnt 9.871687 hw_loss 0.264945 history loss 6.801537 rank 1
2023-02-25 08:39:22,507 DEBUG CV Batch 25/1700 loss 10.096857 loss_att 7.946252 loss_ctc 13.951764 loss_rnnt 9.871687 hw_loss 0.264945 history loss 6.801537 rank 2
2023-02-25 08:39:22,929 DEBUG CV Batch 25/1700 loss 10.096857 loss_att 7.946252 loss_ctc 13.951764 loss_rnnt 9.871687 hw_loss 0.264945 history loss 6.801537 rank 0
2023-02-25 08:39:24,809 DEBUG CV Batch 25/1700 loss 10.096857 loss_att 7.946252 loss_ctc 13.951764 loss_rnnt 9.871687 hw_loss 0.264945 history loss 6.801537 rank 7
2023-02-25 08:39:28,646 INFO Epoch 25 CV info cv_loss 6.777752661958288
2023-02-25 08:39:28,647 INFO Epoch 26 TRAIN info lr 0.0003395574972035283
2023-02-25 08:39:28,649 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 08:39:28,702 INFO Epoch 25 CV info cv_loss 6.777752661307883
2023-02-25 08:39:28,703 INFO Epoch 26 TRAIN info lr 0.0003395465355272452
2023-02-25 08:39:28,705 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 08:39:29,094 INFO Epoch 25 CV info cv_loss 6.777752663078189
2023-02-25 08:39:29,095 INFO Epoch 26 TRAIN info lr 0.0003395535821972709
2023-02-25 08:39:29,100 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 08:39:29,112 INFO Epoch 25 CV info cv_loss 6.777752660859922
2023-02-25 08:39:29,112 INFO Epoch 26 TRAIN info lr 0.000339605271186361
2023-02-25 08:39:29,114 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 08:39:29,163 INFO Epoch 25 CV info cv_loss 6.7777526624665505
2023-02-25 08:39:29,164 INFO Epoch 26 TRAIN info lr 0.0003395770742662949
2023-02-25 08:39:29,166 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 08:39:31,717 INFO Epoch 25 CV info cv_loss 6.777752662384712
2023-02-25 08:39:31,717 INFO Epoch 26 TRAIN info lr 0.0003395762911187696
2023-02-25 08:39:31,722 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 08:39:32,077 INFO Epoch 25 CV info cv_loss 6.777752662264107
2023-02-25 08:39:32,077 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/25.pt
2023-02-25 08:39:32,639 INFO Epoch 26 TRAIN info lr 0.00033955123325851564
2023-02-25 08:39:32,644 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 08:39:34,053 INFO Epoch 25 CV info cv_loss 6.777752661333726
2023-02-25 08:39:34,054 INFO Epoch 26 TRAIN info lr 0.0003395222670214523
2023-02-25 08:39:34,059 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 08:40:36,400 DEBUG TRAIN Batch 26/0 loss 8.506087 loss_att 8.643490 loss_ctc 11.521132 loss_rnnt 7.939550 hw_loss 0.256969 lr 0.00033956 rank 6
2023-02-25 08:40:36,404 DEBUG TRAIN Batch 26/0 loss 10.438619 loss_att 9.807396 loss_ctc 13.382617 loss_rnnt 10.076569 hw_loss 0.179553 lr 0.00033958 rank 3
2023-02-25 08:40:36,404 DEBUG TRAIN Batch 26/0 loss 7.379242 loss_att 7.457100 loss_ctc 9.437565 loss_rnnt 6.956612 hw_loss 0.248654 lr 0.00033952 rank 7
2023-02-25 08:40:36,405 DEBUG TRAIN Batch 26/0 loss 11.398726 loss_att 10.234425 loss_ctc 14.827723 loss_rnnt 11.033882 hw_loss 0.263449 lr 0.00033955 rank 0
2023-02-25 08:40:36,406 DEBUG TRAIN Batch 26/0 loss 11.335090 loss_att 11.032192 loss_ctc 16.118507 loss_rnnt 10.625439 hw_loss 0.248326 lr 0.00033955 rank 4
2023-02-25 08:40:36,406 DEBUG TRAIN Batch 26/0 loss 10.187373 loss_att 8.701845 loss_ctc 11.304537 loss_rnnt 10.227320 hw_loss 0.202882 lr 0.00033960 rank 1
2023-02-25 08:40:36,407 DEBUG TRAIN Batch 26/0 loss 9.255877 loss_att 8.888681 loss_ctc 12.088737 loss_rnnt 8.831124 hw_loss 0.225895 lr 0.00033955 rank 5
2023-02-25 08:40:36,432 DEBUG TRAIN Batch 26/0 loss 9.472201 loss_att 8.907548 loss_ctc 10.867864 loss_rnnt 9.247777 hw_loss 0.283625 lr 0.00033958 rank 2
2023-02-25 08:41:45,922 DEBUG TRAIN Batch 26/100 loss 7.576936 loss_att 10.929764 loss_ctc 12.204742 loss_rnnt 6.184717 hw_loss 0.196151 lr 0.00033950 rank 3
2023-02-25 08:41:45,924 DEBUG TRAIN Batch 26/100 loss 9.229801 loss_att 13.797537 loss_ctc 13.462719 loss_rnnt 7.680882 hw_loss 0.133089 lr 0.00033947 rank 0
2023-02-25 08:41:45,925 DEBUG TRAIN Batch 26/100 loss 9.969503 loss_att 12.902256 loss_ctc 16.935913 loss_rnnt 8.328119 hw_loss 0.236211 lr 0.00033948 rank 6
2023-02-25 08:41:45,927 DEBUG TRAIN Batch 26/100 loss 7.682778 loss_att 14.791120 loss_ctc 14.338041 loss_rnnt 5.233688 hw_loss 0.262599 lr 0.00033953 rank 1
2023-02-25 08:41:45,927 DEBUG TRAIN Batch 26/100 loss 8.002925 loss_att 10.999791 loss_ctc 11.529498 loss_rnnt 6.862435 hw_loss 0.132950 lr 0.00033950 rank 2
2023-02-25 08:41:45,928 DEBUG TRAIN Batch 26/100 loss 3.114157 loss_att 6.335512 loss_ctc 5.130318 loss_rnnt 2.152488 hw_loss 0.091080 lr 0.00033944 rank 7
2023-02-25 08:41:45,930 DEBUG TRAIN Batch 26/100 loss 8.855670 loss_att 12.774504 loss_ctc 12.308556 loss_rnnt 7.488737 hw_loss 0.230216 lr 0.00033947 rank 4
2023-02-25 08:41:45,936 DEBUG TRAIN Batch 26/100 loss 10.955691 loss_att 13.763710 loss_ctc 16.773083 loss_rnnt 9.594686 hw_loss 0.044531 lr 0.00033947 rank 5
2023-02-25 08:42:56,153 DEBUG TRAIN Batch 26/200 loss 4.413468 loss_att 7.808028 loss_ctc 9.035427 loss_rnnt 2.963042 hw_loss 0.291099 lr 0.00033939 rank 0
2023-02-25 08:42:56,155 DEBUG TRAIN Batch 26/200 loss 7.185597 loss_att 11.002222 loss_ctc 8.885159 loss_rnnt 6.052917 hw_loss 0.267648 lr 0.00033937 rank 7
2023-02-25 08:42:56,159 DEBUG TRAIN Batch 26/200 loss 9.422908 loss_att 10.723302 loss_ctc 14.498786 loss_rnnt 8.432262 hw_loss 0.100841 lr 0.00033942 rank 3
2023-02-25 08:42:56,159 DEBUG TRAIN Batch 26/200 loss 2.738338 loss_att 5.815123 loss_ctc 6.547916 loss_rnnt 1.512878 hw_loss 0.191548 lr 0.00033940 rank 6
2023-02-25 08:42:56,160 DEBUG TRAIN Batch 26/200 loss 7.131990 loss_att 10.338322 loss_ctc 14.069370 loss_rnnt 5.460392 hw_loss 0.197529 lr 0.00033940 rank 4
2023-02-25 08:42:56,160 DEBUG TRAIN Batch 26/200 loss 14.108931 loss_att 19.354372 loss_ctc 18.346256 loss_rnnt 12.397604 hw_loss 0.182366 lr 0.00033945 rank 1
2023-02-25 08:42:56,162 DEBUG TRAIN Batch 26/200 loss 8.018398 loss_att 12.134401 loss_ctc 12.915706 loss_rnnt 6.446440 hw_loss 0.179593 lr 0.00033942 rank 2
2023-02-25 08:42:56,162 DEBUG TRAIN Batch 26/200 loss 3.166801 loss_att 8.367270 loss_ctc 4.453721 loss_rnnt 1.893090 hw_loss 0.116304 lr 0.00033939 rank 5
2023-02-25 08:44:06,844 DEBUG TRAIN Batch 26/300 loss 10.276109 loss_att 11.221153 loss_ctc 12.735270 loss_rnnt 9.626730 hw_loss 0.248402 lr 0.00033932 rank 0
2023-02-25 08:44:06,850 DEBUG TRAIN Batch 26/300 loss 12.176427 loss_att 14.569930 loss_ctc 19.443829 loss_rnnt 10.584980 hw_loss 0.269550 lr 0.00033931 rank 5
2023-02-25 08:44:06,855 DEBUG TRAIN Batch 26/300 loss 10.892740 loss_att 14.033197 loss_ctc 14.101612 loss_rnnt 9.735871 hw_loss 0.189239 lr 0.00033934 rank 3
2023-02-25 08:44:06,856 DEBUG TRAIN Batch 26/300 loss 6.551865 loss_att 9.091595 loss_ctc 9.237599 loss_rnnt 5.555977 hw_loss 0.243456 lr 0.00033932 rank 6
2023-02-25 08:44:06,858 DEBUG TRAIN Batch 26/300 loss 3.130319 loss_att 6.142131 loss_ctc 5.357797 loss_rnnt 2.139207 hw_loss 0.172036 lr 0.00033937 rank 1
2023-02-25 08:44:06,861 DEBUG TRAIN Batch 26/300 loss 6.968982 loss_att 8.279012 loss_ctc 7.749911 loss_rnnt 6.504542 hw_loss 0.184332 lr 0.00033934 rank 2
2023-02-25 08:44:06,861 DEBUG TRAIN Batch 26/300 loss 7.695590 loss_att 10.756203 loss_ctc 12.844595 loss_rnnt 6.272179 hw_loss 0.233915 lr 0.00033929 rank 7
2023-02-25 08:44:06,886 DEBUG TRAIN Batch 26/300 loss 15.714826 loss_att 21.654400 loss_ctc 19.976658 loss_rnnt 13.796814 hw_loss 0.303473 lr 0.00033932 rank 4
2023-02-25 08:45:18,511 DEBUG TRAIN Batch 26/400 loss 6.200805 loss_att 8.020356 loss_ctc 9.435568 loss_rnnt 5.303663 hw_loss 0.191118 lr 0.00033929 rank 1
2023-02-25 08:45:18,512 DEBUG TRAIN Batch 26/400 loss 7.234238 loss_att 9.315922 loss_ctc 9.654093 loss_rnnt 6.385454 hw_loss 0.205873 lr 0.00033924 rank 0
2023-02-25 08:45:18,512 DEBUG TRAIN Batch 26/400 loss 7.672515 loss_att 9.562243 loss_ctc 8.989167 loss_rnnt 7.044147 hw_loss 0.140381 lr 0.00033926 rank 3
2023-02-25 08:45:18,513 DEBUG TRAIN Batch 26/400 loss 17.157204 loss_att 19.805002 loss_ctc 24.663864 loss_rnnt 15.560287 hw_loss 0.124632 lr 0.00033923 rank 5
2023-02-25 08:45:18,514 DEBUG TRAIN Batch 26/400 loss 3.441206 loss_att 5.082150 loss_ctc 5.446786 loss_rnnt 2.756633 hw_loss 0.166826 lr 0.00033924 rank 4
2023-02-25 08:45:18,513 DEBUG TRAIN Batch 26/400 loss 10.730678 loss_att 12.758256 loss_ctc 14.421894 loss_rnnt 9.719160 hw_loss 0.213450 lr 0.00033926 rank 2
2023-02-25 08:45:18,514 DEBUG TRAIN Batch 26/400 loss 16.747801 loss_att 25.095215 loss_ctc 26.195030 loss_rnnt 13.700056 hw_loss 0.222436 lr 0.00033924 rank 6
2023-02-25 08:45:18,518 DEBUG TRAIN Batch 26/400 loss 8.243466 loss_att 10.674797 loss_ctc 10.487074 loss_rnnt 7.339262 hw_loss 0.222732 lr 0.00033921 rank 7
2023-02-25 08:46:28,790 DEBUG TRAIN Batch 26/500 loss 6.654498 loss_att 8.421752 loss_ctc 11.835111 loss_rnnt 5.445199 hw_loss 0.309562 lr 0.00033915 rank 5
2023-02-25 08:46:28,801 DEBUG TRAIN Batch 26/500 loss 6.120278 loss_att 6.827743 loss_ctc 7.011519 loss_rnnt 5.719252 hw_loss 0.263815 lr 0.00033916 rank 0
2023-02-25 08:46:28,807 DEBUG TRAIN Batch 26/500 loss 10.494327 loss_att 15.435661 loss_ctc 14.406460 loss_rnnt 8.896612 hw_loss 0.164681 lr 0.00033913 rank 7
2023-02-25 08:46:28,807 DEBUG TRAIN Batch 26/500 loss 6.400471 loss_att 8.079149 loss_ctc 9.879438 loss_rnnt 5.520310 hw_loss 0.151053 lr 0.00033918 rank 2
2023-02-25 08:46:28,808 DEBUG TRAIN Batch 26/500 loss 9.251077 loss_att 11.065770 loss_ctc 12.819877 loss_rnnt 8.310572 hw_loss 0.190737 lr 0.00033921 rank 1
2023-02-25 08:46:28,813 DEBUG TRAIN Batch 26/500 loss 17.424671 loss_att 15.897854 loss_ctc 16.883522 loss_rnnt 17.725046 hw_loss 0.144637 lr 0.00033919 rank 3
2023-02-25 08:46:28,825 DEBUG TRAIN Batch 26/500 loss 25.225866 loss_att 24.528004 loss_ctc 27.556477 loss_rnnt 24.934710 hw_loss 0.224964 lr 0.00033917 rank 6
2023-02-25 08:46:28,826 DEBUG TRAIN Batch 26/500 loss 2.564549 loss_att 4.008456 loss_ctc 3.246120 loss_rnnt 2.095627 hw_loss 0.167370 lr 0.00033916 rank 4
2023-02-25 08:47:39,366 DEBUG TRAIN Batch 26/600 loss 12.266436 loss_att 12.227037 loss_ctc 15.680758 loss_rnnt 11.649313 hw_loss 0.318299 lr 0.00033908 rank 0
2023-02-25 08:47:39,368 DEBUG TRAIN Batch 26/600 loss 10.649736 loss_att 12.174748 loss_ctc 16.455212 loss_rnnt 9.422458 hw_loss 0.277901 lr 0.00033914 rank 1
2023-02-25 08:47:39,370 DEBUG TRAIN Batch 26/600 loss 7.336966 loss_att 8.840719 loss_ctc 12.845106 loss_rnnt 6.116887 hw_loss 0.346703 lr 0.00033911 rank 2
2023-02-25 08:47:39,372 DEBUG TRAIN Batch 26/600 loss 8.032124 loss_att 8.425739 loss_ctc 12.035710 loss_rnnt 7.290869 hw_loss 0.241349 lr 0.00033911 rank 3
2023-02-25 08:47:39,373 DEBUG TRAIN Batch 26/600 loss 9.182315 loss_att 9.944337 loss_ctc 14.272867 loss_rnnt 8.244140 hw_loss 0.200684 lr 0.00033905 rank 7
2023-02-25 08:47:39,375 DEBUG TRAIN Batch 26/600 loss 6.077184 loss_att 7.472880 loss_ctc 9.640105 loss_rnnt 5.212319 hw_loss 0.207504 lr 0.00033909 rank 6
2023-02-25 08:47:39,376 DEBUG TRAIN Batch 26/600 loss 6.893093 loss_att 7.362262 loss_ctc 7.577689 loss_rnnt 6.562434 hw_loss 0.272898 lr 0.00033908 rank 5
2023-02-25 08:47:39,380 DEBUG TRAIN Batch 26/600 loss 4.937680 loss_att 7.485312 loss_ctc 9.249310 loss_rnnt 3.721531 hw_loss 0.247008 lr 0.00033908 rank 4
2023-02-25 08:48:52,170 DEBUG TRAIN Batch 26/700 loss 15.862949 loss_att 18.522114 loss_ctc 20.139118 loss_rnnt 14.658504 hw_loss 0.192104 lr 0.00033900 rank 0
2023-02-25 08:48:52,177 DEBUG TRAIN Batch 26/700 loss 2.525862 loss_att 5.882082 loss_ctc 3.431528 loss_rnnt 1.632013 hw_loss 0.190968 lr 0.00033901 rank 6
2023-02-25 08:48:52,186 DEBUG TRAIN Batch 26/700 loss 6.936743 loss_att 11.051376 loss_ctc 7.388475 loss_rnnt 5.941978 hw_loss 0.209265 lr 0.00033897 rank 7
2023-02-25 08:48:52,187 DEBUG TRAIN Batch 26/700 loss 3.684504 loss_att 5.546007 loss_ctc 7.708783 loss_rnnt 2.644629 hw_loss 0.245632 lr 0.00033903 rank 2
2023-02-25 08:48:52,189 DEBUG TRAIN Batch 26/700 loss 7.067992 loss_att 11.779354 loss_ctc 13.168369 loss_rnnt 5.269991 hw_loss 0.079396 lr 0.00033900 rank 5
2023-02-25 08:48:52,192 DEBUG TRAIN Batch 26/700 loss 5.222162 loss_att 7.836406 loss_ctc 8.143752 loss_rnnt 4.222783 hw_loss 0.163097 lr 0.00033903 rank 3
2023-02-25 08:48:52,193 DEBUG TRAIN Batch 26/700 loss 5.042675 loss_att 7.545031 loss_ctc 6.506016 loss_rnnt 4.250093 hw_loss 0.181871 lr 0.00033901 rank 4
2023-02-25 08:48:52,226 DEBUG TRAIN Batch 26/700 loss 2.952114 loss_att 5.522432 loss_ctc 5.636036 loss_rnnt 2.017357 hw_loss 0.117820 lr 0.00033906 rank 1
2023-02-25 08:50:02,514 DEBUG TRAIN Batch 26/800 loss 6.151788 loss_att 10.175432 loss_ctc 10.086291 loss_rnnt 4.721838 hw_loss 0.188665 lr 0.00033895 rank 3
2023-02-25 08:50:02,517 DEBUG TRAIN Batch 26/800 loss 8.477489 loss_att 11.343204 loss_ctc 13.694446 loss_rnnt 7.082578 hw_loss 0.236573 lr 0.00033893 rank 6
2023-02-25 08:50:02,517 DEBUG TRAIN Batch 26/800 loss 6.280098 loss_att 6.591650 loss_ctc 5.457733 loss_rnnt 6.197856 hw_loss 0.242964 lr 0.00033895 rank 2
2023-02-25 08:50:02,518 DEBUG TRAIN Batch 26/800 loss 11.241012 loss_att 15.075843 loss_ctc 16.907469 loss_rnnt 9.619071 hw_loss 0.186463 lr 0.00033893 rank 4
2023-02-25 08:50:02,520 DEBUG TRAIN Batch 26/800 loss 2.470789 loss_att 4.706303 loss_ctc 4.450302 loss_rnnt 1.630941 hw_loss 0.241519 lr 0.00033890 rank 7
2023-02-25 08:50:02,520 DEBUG TRAIN Batch 26/800 loss 6.106749 loss_att 9.297207 loss_ctc 7.167037 loss_rnnt 5.187670 hw_loss 0.261778 lr 0.00033893 rank 0
2023-02-25 08:50:02,520 DEBUG TRAIN Batch 26/800 loss 1.250860 loss_att 3.554629 loss_ctc 1.822948 loss_rnnt 0.637589 hw_loss 0.142948 lr 0.00033898 rank 1
2023-02-25 08:50:02,532 DEBUG TRAIN Batch 26/800 loss 6.374740 loss_att 8.431451 loss_ctc 8.901270 loss_rnnt 5.516719 hw_loss 0.205889 lr 0.00033892 rank 5
2023-02-25 08:51:12,534 DEBUG TRAIN Batch 26/900 loss 9.402200 loss_att 13.250334 loss_ctc 12.260161 loss_rnnt 8.174561 hw_loss 0.144285 lr 0.00033885 rank 0
2023-02-25 08:51:12,539 DEBUG TRAIN Batch 26/900 loss 7.802663 loss_att 12.892614 loss_ctc 12.572771 loss_rnnt 6.073515 hw_loss 0.140893 lr 0.00033884 rank 5
2023-02-25 08:51:12,539 DEBUG TRAIN Batch 26/900 loss 4.363022 loss_att 7.206684 loss_ctc 6.547028 loss_rnnt 3.382217 hw_loss 0.226634 lr 0.00033885 rank 6
2023-02-25 08:51:12,540 DEBUG TRAIN Batch 26/900 loss 4.246066 loss_att 5.521367 loss_ctc 4.721813 loss_rnnt 3.828250 hw_loss 0.186231 lr 0.00033887 rank 3
2023-02-25 08:51:12,541 DEBUG TRAIN Batch 26/900 loss 7.558508 loss_att 10.356056 loss_ctc 10.161068 loss_rnnt 6.566353 hw_loss 0.160572 lr 0.00033882 rank 7
2023-02-25 08:51:12,547 DEBUG TRAIN Batch 26/900 loss 9.443960 loss_att 10.166851 loss_ctc 13.120582 loss_rnnt 8.681418 hw_loss 0.239525 lr 0.00033887 rank 2
2023-02-25 08:51:12,549 DEBUG TRAIN Batch 26/900 loss 11.160336 loss_att 14.151766 loss_ctc 22.362659 loss_rnnt 8.946340 hw_loss 0.228875 lr 0.00033890 rank 1
2023-02-25 08:51:12,549 DEBUG TRAIN Batch 26/900 loss 10.744855 loss_att 12.738592 loss_ctc 13.357368 loss_rnnt 9.864265 hw_loss 0.250325 lr 0.00033885 rank 4
2023-02-25 08:52:23,796 DEBUG TRAIN Batch 26/1000 loss 11.484459 loss_att 12.252582 loss_ctc 16.392641 loss_rnnt 10.565914 hw_loss 0.207180 lr 0.00033877 rank 4
2023-02-25 08:52:23,798 DEBUG TRAIN Batch 26/1000 loss 9.338457 loss_att 13.135798 loss_ctc 13.849581 loss_rnnt 7.810654 hw_loss 0.312849 lr 0.00033877 rank 0
2023-02-25 08:52:23,803 DEBUG TRAIN Batch 26/1000 loss 10.609896 loss_att 14.552923 loss_ctc 16.006313 loss_rnnt 9.015747 hw_loss 0.161287 lr 0.00033874 rank 7
2023-02-25 08:52:23,804 DEBUG TRAIN Batch 26/1000 loss 3.460190 loss_att 7.091649 loss_ctc 6.853840 loss_rnnt 2.140397 hw_loss 0.264404 lr 0.00033882 rank 1
2023-02-25 08:52:23,806 DEBUG TRAIN Batch 26/1000 loss 9.293810 loss_att 11.787262 loss_ctc 14.295646 loss_rnnt 8.018309 hw_loss 0.206062 lr 0.00033880 rank 2
2023-02-25 08:52:23,806 DEBUG TRAIN Batch 26/1000 loss 5.076839 loss_att 5.554938 loss_ctc 8.350376 loss_rnnt 4.436899 hw_loss 0.202215 lr 0.00033880 rank 3
2023-02-25 08:52:23,839 DEBUG TRAIN Batch 26/1000 loss 8.252266 loss_att 8.410630 loss_ctc 12.181366 loss_rnnt 7.591690 hw_loss 0.196919 lr 0.00033877 rank 5
2023-02-25 08:52:23,864 DEBUG TRAIN Batch 26/1000 loss 8.267069 loss_att 11.421517 loss_ctc 12.080397 loss_rnnt 7.036968 hw_loss 0.170187 lr 0.00033878 rank 6
2023-02-25 08:53:35,923 DEBUG TRAIN Batch 26/1100 loss 4.912405 loss_att 8.963999 loss_ctc 8.710924 loss_rnnt 3.489815 hw_loss 0.198379 lr 0.00033872 rank 3
2023-02-25 08:53:35,923 DEBUG TRAIN Batch 26/1100 loss 7.748673 loss_att 12.735230 loss_ctc 15.365204 loss_rnnt 5.609048 hw_loss 0.237706 lr 0.00033869 rank 0
2023-02-25 08:53:35,926 DEBUG TRAIN Batch 26/1100 loss 6.817531 loss_att 9.703944 loss_ctc 9.336172 loss_rnnt 5.779202 hw_loss 0.234802 lr 0.00033872 rank 2
2023-02-25 08:53:35,927 DEBUG TRAIN Batch 26/1100 loss 12.281399 loss_att 15.372322 loss_ctc 16.837894 loss_rnnt 10.904177 hw_loss 0.284070 lr 0.00033866 rank 7
2023-02-25 08:53:35,928 DEBUG TRAIN Batch 26/1100 loss 11.028453 loss_att 12.592241 loss_ctc 12.188087 loss_rnnt 10.420950 hw_loss 0.262739 lr 0.00033870 rank 6
2023-02-25 08:53:35,931 DEBUG TRAIN Batch 26/1100 loss 20.785219 loss_att 22.483265 loss_ctc 32.789783 loss_rnnt 18.735033 hw_loss 0.206191 lr 0.00033869 rank 4
2023-02-25 08:53:35,932 DEBUG TRAIN Batch 26/1100 loss 11.766409 loss_att 12.446887 loss_ctc 14.001170 loss_rnnt 11.210999 hw_loss 0.227525 lr 0.00033875 rank 1
2023-02-25 08:53:35,932 DEBUG TRAIN Batch 26/1100 loss 12.145206 loss_att 13.983441 loss_ctc 15.676401 loss_rnnt 11.186104 hw_loss 0.226181 lr 0.00033869 rank 5
2023-02-25 08:54:46,524 DEBUG TRAIN Batch 26/1200 loss 12.835185 loss_att 12.621933 loss_ctc 17.874748 loss_rnnt 12.041689 hw_loss 0.307883 lr 0.00033864 rank 3
2023-02-25 08:54:46,526 DEBUG TRAIN Batch 26/1200 loss 6.766095 loss_att 7.304638 loss_ctc 8.745144 loss_rnnt 6.227378 hw_loss 0.313379 lr 0.00033861 rank 0
2023-02-25 08:54:46,527 DEBUG TRAIN Batch 26/1200 loss 4.464167 loss_att 7.087829 loss_ctc 6.680782 loss_rnnt 3.508307 hw_loss 0.254211 lr 0.00033867 rank 1
2023-02-25 08:54:46,527 DEBUG TRAIN Batch 26/1200 loss 13.052287 loss_att 16.263163 loss_ctc 15.799036 loss_rnnt 11.951508 hw_loss 0.173194 lr 0.00033862 rank 6
2023-02-25 08:54:46,527 DEBUG TRAIN Batch 26/1200 loss 12.557712 loss_att 13.701794 loss_ctc 20.204374 loss_rnnt 11.190648 hw_loss 0.222547 lr 0.00033862 rank 4
2023-02-25 08:54:46,530 DEBUG TRAIN Batch 26/1200 loss 8.669045 loss_att 10.690239 loss_ctc 11.380730 loss_rnnt 7.827181 hw_loss 0.142627 lr 0.00033859 rank 7
2023-02-25 08:54:46,540 DEBUG TRAIN Batch 26/1200 loss 4.600463 loss_att 5.786584 loss_ctc 4.688350 loss_rnnt 4.247536 hw_loss 0.194971 lr 0.00033864 rank 2
2023-02-25 08:54:46,549 DEBUG TRAIN Batch 26/1200 loss 12.102324 loss_att 13.371817 loss_ctc 17.240154 loss_rnnt 11.042335 hw_loss 0.226964 lr 0.00033861 rank 5
2023-02-25 08:55:56,496 DEBUG TRAIN Batch 26/1300 loss 9.036078 loss_att 12.658047 loss_ctc 20.537582 loss_rnnt 6.697094 hw_loss 0.151980 lr 0.00033856 rank 3
2023-02-25 08:55:56,501 DEBUG TRAIN Batch 26/1300 loss 10.566601 loss_att 10.331591 loss_ctc 12.836996 loss_rnnt 10.174137 hw_loss 0.256398 lr 0.00033859 rank 1
2023-02-25 08:55:56,502 DEBUG TRAIN Batch 26/1300 loss 2.596337 loss_att 6.773147 loss_ctc 5.700054 loss_rnnt 1.271306 hw_loss 0.142198 lr 0.00033854 rank 0
2023-02-25 08:55:56,505 DEBUG TRAIN Batch 26/1300 loss 5.915207 loss_att 14.535904 loss_ctc 4.172479 loss_rnnt 4.346496 hw_loss 0.144255 lr 0.00033854 rank 4
2023-02-25 08:55:56,506 DEBUG TRAIN Batch 26/1300 loss 4.368470 loss_att 9.843799 loss_ctc 9.337416 loss_rnnt 2.512863 hw_loss 0.183778 lr 0.00033854 rank 6
2023-02-25 08:55:56,507 DEBUG TRAIN Batch 26/1300 loss 8.961172 loss_att 10.103024 loss_ctc 12.454383 loss_rnnt 8.124412 hw_loss 0.267430 lr 0.00033851 rank 7
2023-02-25 08:55:56,509 DEBUG TRAIN Batch 26/1300 loss 7.730443 loss_att 12.708052 loss_ctc 19.294331 loss_rnnt 5.110273 hw_loss 0.155244 lr 0.00033853 rank 5
2023-02-25 08:55:56,526 DEBUG TRAIN Batch 26/1300 loss 13.518947 loss_att 17.069513 loss_ctc 22.301735 loss_rnnt 11.559278 hw_loss 0.147221 lr 0.00033856 rank 2
2023-02-25 08:57:08,931 DEBUG TRAIN Batch 26/1400 loss 4.465594 loss_att 8.361814 loss_ctc 8.901823 loss_rnnt 3.013659 hw_loss 0.152238 lr 0.00033846 rank 4
2023-02-25 08:57:08,931 DEBUG TRAIN Batch 26/1400 loss 20.584076 loss_att 21.291004 loss_ctc 27.186596 loss_rnnt 19.441229 hw_loss 0.227107 lr 0.00033847 rank 6
2023-02-25 08:57:08,934 DEBUG TRAIN Batch 26/1400 loss 7.323657 loss_att 9.721345 loss_ctc 11.102187 loss_rnnt 6.256973 hw_loss 0.156265 lr 0.00033848 rank 2
2023-02-25 08:57:08,934 DEBUG TRAIN Batch 26/1400 loss 7.324817 loss_att 12.531446 loss_ctc 15.670924 loss_rnnt 5.071622 hw_loss 0.185728 lr 0.00033851 rank 1
2023-02-25 08:57:08,936 DEBUG TRAIN Batch 26/1400 loss 7.798973 loss_att 11.247057 loss_ctc 13.714308 loss_rnnt 6.124333 hw_loss 0.368082 lr 0.00033843 rank 7
2023-02-25 08:57:08,940 DEBUG TRAIN Batch 26/1400 loss 8.993981 loss_att 11.892493 loss_ctc 14.957153 loss_rnnt 7.536390 hw_loss 0.155251 lr 0.00033846 rank 0
2023-02-25 08:57:08,965 DEBUG TRAIN Batch 26/1400 loss 8.774685 loss_att 11.461015 loss_ctc 12.274864 loss_rnnt 7.650665 hw_loss 0.225117 lr 0.00033849 rank 3
2023-02-25 08:57:09,007 DEBUG TRAIN Batch 26/1400 loss 16.161982 loss_att 20.659168 loss_ctc 25.372040 loss_rnnt 13.854664 hw_loss 0.337260 lr 0.00033845 rank 5
2023-02-25 08:58:20,176 DEBUG TRAIN Batch 26/1500 loss 5.723835 loss_att 7.732865 loss_ctc 10.739495 loss_rnnt 4.550966 hw_loss 0.191828 lr 0.00033838 rank 0
2023-02-25 08:58:20,176 DEBUG TRAIN Batch 26/1500 loss 7.316735 loss_att 9.061143 loss_ctc 11.966628 loss_rnnt 6.268070 hw_loss 0.149621 lr 0.00033835 rank 7
2023-02-25 08:58:20,177 DEBUG TRAIN Batch 26/1500 loss 21.036438 loss_att 24.264275 loss_ctc 33.776749 loss_rnnt 18.588390 hw_loss 0.194574 lr 0.00033838 rank 4
2023-02-25 08:58:20,178 DEBUG TRAIN Batch 26/1500 loss 2.665559 loss_att 5.086089 loss_ctc 3.634927 loss_rnnt 1.958630 hw_loss 0.175450 lr 0.00033841 rank 2
2023-02-25 08:58:20,178 DEBUG TRAIN Batch 26/1500 loss 10.100100 loss_att 12.729119 loss_ctc 13.769768 loss_rnnt 8.972136 hw_loss 0.211635 lr 0.00033839 rank 6
2023-02-25 08:58:20,179 DEBUG TRAIN Batch 26/1500 loss 6.707263 loss_att 8.060721 loss_ctc 7.835325 loss_rnnt 6.217964 hw_loss 0.127873 lr 0.00033844 rank 1
2023-02-25 08:58:20,193 DEBUG TRAIN Batch 26/1500 loss 13.884773 loss_att 16.609856 loss_ctc 22.412027 loss_rnnt 12.064388 hw_loss 0.259504 lr 0.00033841 rank 3
2023-02-25 08:58:20,240 DEBUG TRAIN Batch 26/1500 loss 6.588107 loss_att 9.819814 loss_ctc 9.760449 loss_rnnt 5.389738 hw_loss 0.241965 lr 0.00033838 rank 5
2023-02-25 08:59:29,681 DEBUG TRAIN Batch 26/1600 loss 6.922397 loss_att 9.508296 loss_ctc 11.569644 loss_rnnt 5.704254 hw_loss 0.152492 lr 0.00033830 rank 0
2023-02-25 08:59:29,686 DEBUG TRAIN Batch 26/1600 loss 8.715893 loss_att 13.829432 loss_ctc 15.587132 loss_rnnt 6.617068 hw_loss 0.299910 lr 0.00033833 rank 3
2023-02-25 08:59:29,687 DEBUG TRAIN Batch 26/1600 loss 5.456137 loss_att 8.688101 loss_ctc 9.289120 loss_rnnt 4.182664 hw_loss 0.217531 lr 0.00033830 rank 5
2023-02-25 08:59:29,688 DEBUG TRAIN Batch 26/1600 loss 3.883827 loss_att 6.262847 loss_ctc 4.140262 loss_rnnt 3.260098 hw_loss 0.213250 lr 0.00033831 rank 6
2023-02-25 08:59:29,689 DEBUG TRAIN Batch 26/1600 loss 7.533105 loss_att 11.141826 loss_ctc 10.542230 loss_rnnt 6.310254 hw_loss 0.187295 lr 0.00033836 rank 1
2023-02-25 08:59:29,689 DEBUG TRAIN Batch 26/1600 loss 10.804049 loss_att 12.281176 loss_ctc 14.539345 loss_rnnt 9.902741 hw_loss 0.202202 lr 0.00033833 rank 2
2023-02-25 08:59:29,689 DEBUG TRAIN Batch 26/1600 loss 12.372829 loss_att 14.339588 loss_ctc 15.330150 loss_rnnt 11.520884 hw_loss 0.120532 lr 0.00033831 rank 4
2023-02-25 08:59:29,691 DEBUG TRAIN Batch 26/1600 loss 5.510607 loss_att 7.342990 loss_ctc 5.347571 loss_rnnt 5.050089 hw_loss 0.217085 lr 0.00033828 rank 7
2023-02-25 09:00:40,371 DEBUG TRAIN Batch 26/1700 loss 14.006094 loss_att 18.448235 loss_ctc 17.153337 loss_rnnt 12.570572 hw_loss 0.238991 lr 0.00033823 rank 4
2023-02-25 09:00:40,371 DEBUG TRAIN Batch 26/1700 loss 14.450343 loss_att 17.164503 loss_ctc 19.097267 loss_rnnt 13.219405 hw_loss 0.128467 lr 0.00033823 rank 0
2023-02-25 09:00:40,372 DEBUG TRAIN Batch 26/1700 loss 7.157169 loss_att 9.876514 loss_ctc 9.649386 loss_rnnt 6.189682 hw_loss 0.171230 lr 0.00033820 rank 7
2023-02-25 09:00:40,372 DEBUG TRAIN Batch 26/1700 loss 9.495340 loss_att 13.714769 loss_ctc 10.279902 loss_rnnt 8.472801 hw_loss 0.138833 lr 0.00033825 rank 3
2023-02-25 09:00:40,373 DEBUG TRAIN Batch 26/1700 loss 10.464435 loss_att 14.165804 loss_ctc 17.836105 loss_rnnt 8.683025 hw_loss 0.109213 lr 0.00033825 rank 2
2023-02-25 09:00:40,375 DEBUG TRAIN Batch 26/1700 loss 6.930540 loss_att 9.639774 loss_ctc 14.102570 loss_rnnt 5.315627 hw_loss 0.218990 lr 0.00033828 rank 1
2023-02-25 09:00:40,382 DEBUG TRAIN Batch 26/1700 loss 7.628661 loss_att 10.126701 loss_ctc 11.003819 loss_rnnt 6.596371 hw_loss 0.154988 lr 0.00033822 rank 5
2023-02-25 09:00:40,397 DEBUG TRAIN Batch 26/1700 loss 12.919179 loss_att 14.962250 loss_ctc 15.159831 loss_rnnt 12.059616 hw_loss 0.285366 lr 0.00033823 rank 6
2023-02-25 09:01:54,814 DEBUG TRAIN Batch 26/1800 loss 8.894444 loss_att 9.720775 loss_ctc 10.863047 loss_rnnt 8.333012 hw_loss 0.250661 lr 0.00033815 rank 0
2023-02-25 09:01:54,815 DEBUG TRAIN Batch 26/1800 loss 7.613304 loss_att 9.413887 loss_ctc 11.863674 loss_rnnt 6.575562 hw_loss 0.207953 lr 0.00033812 rank 7
2023-02-25 09:01:54,815 DEBUG TRAIN Batch 26/1800 loss 8.536211 loss_att 10.919291 loss_ctc 12.317058 loss_rnnt 7.402186 hw_loss 0.287431 lr 0.00033818 rank 3
2023-02-25 09:01:54,815 DEBUG TRAIN Batch 26/1800 loss 4.313815 loss_att 7.242465 loss_ctc 4.634217 loss_rnnt 3.598728 hw_loss 0.162442 lr 0.00033815 rank 4
2023-02-25 09:01:54,817 DEBUG TRAIN Batch 26/1800 loss 17.791822 loss_att 19.925543 loss_ctc 19.604284 loss_rnnt 17.017042 hw_loss 0.199458 lr 0.00033817 rank 2
2023-02-25 09:01:54,818 DEBUG TRAIN Batch 26/1800 loss 25.090759 loss_att 25.433102 loss_ctc 30.201706 loss_rnnt 24.204609 hw_loss 0.255413 lr 0.00033816 rank 6
2023-02-25 09:01:54,819 DEBUG TRAIN Batch 26/1800 loss 7.224197 loss_att 9.631203 loss_ctc 8.361303 loss_rnnt 6.506142 hw_loss 0.159452 lr 0.00033815 rank 5
2023-02-25 09:01:54,826 DEBUG TRAIN Batch 26/1800 loss 5.886305 loss_att 8.084503 loss_ctc 7.738558 loss_rnnt 5.139900 hw_loss 0.112123 lr 0.00033820 rank 1
2023-02-25 09:03:05,138 DEBUG TRAIN Batch 26/1900 loss 9.247132 loss_att 13.565910 loss_ctc 13.115633 loss_rnnt 7.792398 hw_loss 0.140961 lr 0.00033807 rank 0
2023-02-25 09:03:05,140 DEBUG TRAIN Batch 26/1900 loss 8.393610 loss_att 9.558195 loss_ctc 10.841180 loss_rnnt 7.702808 hw_loss 0.246642 lr 0.00033813 rank 1
2023-02-25 09:03:05,142 DEBUG TRAIN Batch 26/1900 loss 8.549273 loss_att 9.795077 loss_ctc 13.437574 loss_rnnt 7.559079 hw_loss 0.167362 lr 0.00033804 rank 7
2023-02-25 09:03:05,144 DEBUG TRAIN Batch 26/1900 loss 4.757739 loss_att 6.549700 loss_ctc 7.926812 loss_rnnt 3.885506 hw_loss 0.171183 lr 0.00033808 rank 6
2023-02-25 09:03:05,143 DEBUG TRAIN Batch 26/1900 loss 8.646858 loss_att 9.561705 loss_ctc 15.013079 loss_rnnt 7.517196 hw_loss 0.183494 lr 0.00033807 rank 4
2023-02-25 09:03:05,146 DEBUG TRAIN Batch 26/1900 loss 8.203759 loss_att 9.104473 loss_ctc 10.465870 loss_rnnt 7.623246 hw_loss 0.185165 lr 0.00033807 rank 5
2023-02-25 09:03:05,148 DEBUG TRAIN Batch 26/1900 loss 11.429234 loss_att 13.875129 loss_ctc 15.770528 loss_rnnt 10.168512 hw_loss 0.361318 lr 0.00033810 rank 2
2023-02-25 09:03:05,194 DEBUG TRAIN Batch 26/1900 loss 12.099880 loss_att 13.581203 loss_ctc 13.966549 loss_rnnt 11.457609 hw_loss 0.182094 lr 0.00033810 rank 3
2023-02-25 09:04:15,586 DEBUG TRAIN Batch 26/2000 loss 3.285652 loss_att 5.623719 loss_ctc 5.095029 loss_rnnt 2.477286 hw_loss 0.186568 lr 0.00033802 rank 3
2023-02-25 09:04:15,596 DEBUG TRAIN Batch 26/2000 loss 4.617758 loss_att 6.747190 loss_ctc 7.870379 loss_rnnt 3.673246 hw_loss 0.159268 lr 0.00033799 rank 5
2023-02-25 09:04:15,601 DEBUG TRAIN Batch 26/2000 loss 5.688079 loss_att 9.896896 loss_ctc 10.689329 loss_rnnt 4.077745 hw_loss 0.190758 lr 0.00033805 rank 1
2023-02-25 09:04:15,602 DEBUG TRAIN Batch 26/2000 loss 3.295696 loss_att 6.990559 loss_ctc 2.748944 loss_rnnt 2.532493 hw_loss 0.182120 lr 0.00033802 rank 2
2023-02-25 09:04:15,603 DEBUG TRAIN Batch 26/2000 loss 6.434038 loss_att 7.953834 loss_ctc 8.954617 loss_rnnt 5.692688 hw_loss 0.189962 lr 0.00033800 rank 0
2023-02-25 09:04:15,607 DEBUG TRAIN Batch 26/2000 loss 2.841828 loss_att 6.302403 loss_ctc 6.942813 loss_rnnt 1.552905 hw_loss 0.093769 lr 0.00033797 rank 7
2023-02-25 09:04:15,609 DEBUG TRAIN Batch 26/2000 loss 4.623726 loss_att 8.367123 loss_ctc 9.408833 loss_rnnt 3.120207 hw_loss 0.219048 lr 0.00033800 rank 6
2023-02-25 09:04:15,612 DEBUG TRAIN Batch 26/2000 loss 11.016875 loss_att 12.772855 loss_ctc 10.913433 loss_rnnt 10.584928 hw_loss 0.177272 lr 0.00033800 rank 4
2023-02-25 09:05:27,663 DEBUG TRAIN Batch 26/2100 loss 7.558647 loss_att 9.660206 loss_ctc 10.313321 loss_rnnt 6.655822 hw_loss 0.216044 lr 0.00033792 rank 6
2023-02-25 09:05:27,663 DEBUG TRAIN Batch 26/2100 loss 7.421409 loss_att 10.461651 loss_ctc 8.676075 loss_rnnt 6.553555 hw_loss 0.173467 lr 0.00033791 rank 5
2023-02-25 09:05:27,673 DEBUG TRAIN Batch 26/2100 loss 9.493096 loss_att 13.204105 loss_ctc 16.344284 loss_rnnt 7.736871 hw_loss 0.188495 lr 0.00033792 rank 0
2023-02-25 09:05:27,677 DEBUG TRAIN Batch 26/2100 loss 4.306140 loss_att 5.692917 loss_ctc 5.036418 loss_rnnt 3.849460 hw_loss 0.153665 lr 0.00033792 rank 4
2023-02-25 09:05:27,681 DEBUG TRAIN Batch 26/2100 loss 6.737075 loss_att 8.700603 loss_ctc 14.632370 loss_rnnt 5.173009 hw_loss 0.222478 lr 0.00033794 rank 2
2023-02-25 09:05:27,683 DEBUG TRAIN Batch 26/2100 loss 9.152090 loss_att 11.519375 loss_ctc 10.878666 loss_rnnt 8.358707 hw_loss 0.168217 lr 0.00033789 rank 7
2023-02-25 09:05:27,685 DEBUG TRAIN Batch 26/2100 loss 12.679599 loss_att 14.692809 loss_ctc 15.640411 loss_rnnt 11.781777 hw_loss 0.188257 lr 0.00033794 rank 3
2023-02-25 09:05:27,687 DEBUG TRAIN Batch 26/2100 loss 5.113195 loss_att 8.444580 loss_ctc 5.847694 loss_rnnt 4.264722 hw_loss 0.157993 lr 0.00033797 rank 1
2023-02-25 09:06:38,682 DEBUG TRAIN Batch 26/2200 loss 2.664026 loss_att 4.594977 loss_ctc 2.910730 loss_rnnt 2.195418 hw_loss 0.092858 lr 0.00033784 rank 0
2023-02-25 09:06:38,684 DEBUG TRAIN Batch 26/2200 loss 7.293308 loss_att 10.216928 loss_ctc 11.290710 loss_rnnt 6.033258 hw_loss 0.266886 lr 0.00033785 rank 6
2023-02-25 09:06:38,686 DEBUG TRAIN Batch 26/2200 loss 3.086499 loss_att 4.977672 loss_ctc 4.756578 loss_rnnt 2.414767 hw_loss 0.132788 lr 0.00033789 rank 1
2023-02-25 09:06:38,686 DEBUG TRAIN Batch 26/2200 loss 12.930629 loss_att 14.824091 loss_ctc 20.000360 loss_rnnt 11.496756 hw_loss 0.211031 lr 0.00033787 rank 3
2023-02-25 09:06:38,687 DEBUG TRAIN Batch 26/2200 loss 6.311308 loss_att 7.993689 loss_ctc 9.401310 loss_rnnt 5.442824 hw_loss 0.225015 lr 0.00033787 rank 2
2023-02-25 09:06:38,689 DEBUG TRAIN Batch 26/2200 loss 5.826504 loss_att 7.818583 loss_ctc 10.207838 loss_rnnt 4.759203 hw_loss 0.158826 lr 0.00033781 rank 7
2023-02-25 09:06:38,691 DEBUG TRAIN Batch 26/2200 loss 4.975839 loss_att 6.691506 loss_ctc 7.241044 loss_rnnt 4.226169 hw_loss 0.195954 lr 0.00033784 rank 4
2023-02-25 09:06:38,751 DEBUG TRAIN Batch 26/2200 loss 7.591445 loss_att 9.809758 loss_ctc 10.227439 loss_rnnt 6.726694 hw_loss 0.130543 lr 0.00033784 rank 5
2023-02-25 09:07:48,223 DEBUG TRAIN Batch 26/2300 loss 10.025177 loss_att 10.881497 loss_ctc 12.499339 loss_rnnt 9.426311 hw_loss 0.183210 lr 0.00033776 rank 0
2023-02-25 09:07:48,226 DEBUG TRAIN Batch 26/2300 loss 9.416365 loss_att 11.513967 loss_ctc 11.585690 loss_rnnt 8.593253 hw_loss 0.214403 lr 0.00033779 rank 3
2023-02-25 09:07:48,226 DEBUG TRAIN Batch 26/2300 loss 7.226076 loss_att 10.034113 loss_ctc 11.348663 loss_rnnt 6.036838 hw_loss 0.146163 lr 0.00033774 rank 7
2023-02-25 09:07:48,229 DEBUG TRAIN Batch 26/2300 loss 3.757623 loss_att 5.888705 loss_ctc 4.334477 loss_rnnt 3.158365 hw_loss 0.180241 lr 0.00033782 rank 1
2023-02-25 09:07:48,229 DEBUG TRAIN Batch 26/2300 loss 6.206307 loss_att 7.997399 loss_ctc 8.177710 loss_rnnt 5.460063 hw_loss 0.234699 lr 0.00033776 rank 5
2023-02-25 09:07:48,230 DEBUG TRAIN Batch 26/2300 loss 9.066867 loss_att 11.484955 loss_ctc 12.690963 loss_rnnt 8.008941 hw_loss 0.170806 lr 0.00033777 rank 6
2023-02-25 09:07:48,233 DEBUG TRAIN Batch 26/2300 loss 8.953556 loss_att 10.936500 loss_ctc 11.428331 loss_rnnt 8.148360 hw_loss 0.147445 lr 0.00033779 rank 2
2023-02-25 09:07:48,233 DEBUG TRAIN Batch 26/2300 loss 9.178017 loss_att 11.926699 loss_ctc 14.615726 loss_rnnt 7.810045 hw_loss 0.174762 lr 0.00033777 rank 4
2023-02-25 09:08:59,087 DEBUG TRAIN Batch 26/2400 loss 4.036687 loss_att 5.729771 loss_ctc 5.943182 loss_rnnt 3.307293 hw_loss 0.256083 lr 0.00033769 rank 0
2023-02-25 09:08:59,108 DEBUG TRAIN Batch 26/2400 loss 4.509384 loss_att 6.754702 loss_ctc 5.637356 loss_rnnt 3.806621 hw_loss 0.193693 lr 0.00033769 rank 4
2023-02-25 09:08:59,110 DEBUG TRAIN Batch 26/2400 loss 7.094196 loss_att 8.354095 loss_ctc 9.780553 loss_rnnt 6.352370 hw_loss 0.246871 lr 0.00033771 rank 3
2023-02-25 09:08:59,110 DEBUG TRAIN Batch 26/2400 loss 8.863047 loss_att 9.486303 loss_ctc 13.773523 loss_rnnt 7.988672 hw_loss 0.178112 lr 0.00033766 rank 7
2023-02-25 09:08:59,112 DEBUG TRAIN Batch 26/2400 loss 3.694422 loss_att 6.006003 loss_ctc 3.808903 loss_rnnt 3.145927 hw_loss 0.132966 lr 0.00033771 rank 2
2023-02-25 09:08:59,114 DEBUG TRAIN Batch 26/2400 loss 9.925557 loss_att 12.646929 loss_ctc 18.575521 loss_rnnt 8.139215 hw_loss 0.166386 lr 0.00033774 rank 1
2023-02-25 09:08:59,116 DEBUG TRAIN Batch 26/2400 loss 10.513968 loss_att 12.551746 loss_ctc 13.670398 loss_rnnt 9.533381 hw_loss 0.285325 lr 0.00033768 rank 5
2023-02-25 09:08:59,133 DEBUG TRAIN Batch 26/2400 loss 6.862301 loss_att 9.295391 loss_ctc 8.566662 loss_rnnt 5.992325 hw_loss 0.292707 lr 0.00033769 rank 6
2023-02-25 09:10:12,242 DEBUG TRAIN Batch 26/2500 loss 7.151217 loss_att 7.884220 loss_ctc 10.769390 loss_rnnt 6.433520 hw_loss 0.166263 lr 0.00033761 rank 0
2023-02-25 09:10:12,245 DEBUG TRAIN Batch 26/2500 loss 6.584303 loss_att 10.019235 loss_ctc 8.531881 loss_rnnt 5.578879 hw_loss 0.110176 lr 0.00033758 rank 7
2023-02-25 09:10:12,247 DEBUG TRAIN Batch 26/2500 loss 11.089009 loss_att 13.860369 loss_ctc 13.356280 loss_rnnt 10.117871 hw_loss 0.214804 lr 0.00033766 rank 1
2023-02-25 09:10:12,250 DEBUG TRAIN Batch 26/2500 loss 8.689440 loss_att 9.078481 loss_ctc 9.901268 loss_rnnt 8.344904 hw_loss 0.197159 lr 0.00033761 rank 4
2023-02-25 09:10:12,252 DEBUG TRAIN Batch 26/2500 loss 15.463619 loss_att 15.884108 loss_ctc 24.940430 loss_rnnt 14.021812 hw_loss 0.176502 lr 0.00033764 rank 3
2023-02-25 09:10:12,255 DEBUG TRAIN Batch 26/2500 loss 6.870295 loss_att 7.553395 loss_ctc 9.184835 loss_rnnt 6.256105 hw_loss 0.316807 lr 0.00033761 rank 5
2023-02-25 09:10:12,258 DEBUG TRAIN Batch 26/2500 loss 5.285473 loss_att 6.197999 loss_ctc 7.277697 loss_rnnt 4.681288 hw_loss 0.292594 lr 0.00033763 rank 2
2023-02-25 09:10:12,273 DEBUG TRAIN Batch 26/2500 loss 10.936066 loss_att 14.216022 loss_ctc 17.469378 loss_rnnt 9.312518 hw_loss 0.180840 lr 0.00033762 rank 6
2023-02-25 09:11:22,127 DEBUG TRAIN Batch 26/2600 loss 8.229217 loss_att 10.738110 loss_ctc 11.720657 loss_rnnt 7.176292 hw_loss 0.160538 lr 0.00033753 rank 0
2023-02-25 09:11:22,129 DEBUG TRAIN Batch 26/2600 loss 7.416818 loss_att 10.859826 loss_ctc 10.206836 loss_rnnt 6.224472 hw_loss 0.247016 lr 0.00033754 rank 4
2023-02-25 09:11:22,130 DEBUG TRAIN Batch 26/2600 loss 5.881327 loss_att 9.899079 loss_ctc 7.933046 loss_rnnt 4.730491 hw_loss 0.138230 lr 0.00033759 rank 1
2023-02-25 09:11:22,130 DEBUG TRAIN Batch 26/2600 loss 6.533082 loss_att 9.099965 loss_ctc 11.366538 loss_rnnt 5.311963 hw_loss 0.118655 lr 0.00033756 rank 3
2023-02-25 09:11:22,131 DEBUG TRAIN Batch 26/2600 loss 13.096960 loss_att 13.284640 loss_ctc 14.537992 loss_rnnt 12.733281 hw_loss 0.251259 lr 0.00033756 rank 2
2023-02-25 09:11:22,134 DEBUG TRAIN Batch 26/2600 loss 4.813783 loss_att 9.408401 loss_ctc 6.380641 loss_rnnt 3.608341 hw_loss 0.145506 lr 0.00033754 rank 6
2023-02-25 09:11:22,136 DEBUG TRAIN Batch 26/2600 loss 13.365919 loss_att 14.260355 loss_ctc 19.683458 loss_rnnt 12.223880 hw_loss 0.226527 lr 0.00033750 rank 7
2023-02-25 09:11:22,187 DEBUG TRAIN Batch 26/2600 loss 6.641818 loss_att 12.480012 loss_ctc 8.833919 loss_rnnt 5.102602 hw_loss 0.148680 lr 0.00033753 rank 5
2023-02-25 09:12:32,088 DEBUG TRAIN Batch 26/2700 loss 4.484048 loss_att 7.601346 loss_ctc 8.024920 loss_rnnt 3.267215 hw_loss 0.227356 lr 0.00033746 rank 0
2023-02-25 09:12:32,089 DEBUG TRAIN Batch 26/2700 loss 2.793903 loss_att 6.774556 loss_ctc 2.541439 loss_rnnt 1.926789 hw_loss 0.196209 lr 0.00033743 rank 7
2023-02-25 09:12:32,090 DEBUG TRAIN Batch 26/2700 loss 4.530959 loss_att 8.871954 loss_ctc 7.212755 loss_rnnt 3.242332 hw_loss 0.117852 lr 0.00033746 rank 6
2023-02-25 09:12:32,091 DEBUG TRAIN Batch 26/2700 loss 11.552312 loss_att 13.456175 loss_ctc 15.756325 loss_rnnt 10.513711 hw_loss 0.182427 lr 0.00033748 rank 3
2023-02-25 09:12:32,093 DEBUG TRAIN Batch 26/2700 loss 17.207140 loss_att 17.828712 loss_ctc 25.260265 loss_rnnt 15.921282 hw_loss 0.164608 lr 0.00033746 rank 4
2023-02-25 09:12:32,092 DEBUG TRAIN Batch 26/2700 loss 2.411009 loss_att 5.839542 loss_ctc 5.042801 loss_rnnt 1.314250 hw_loss 0.112775 lr 0.00033751 rank 1
2023-02-25 09:12:32,093 DEBUG TRAIN Batch 26/2700 loss 7.119348 loss_att 11.295620 loss_ctc 9.164651 loss_rnnt 5.895334 hw_loss 0.217596 lr 0.00033745 rank 5
2023-02-25 09:12:32,096 DEBUG TRAIN Batch 26/2700 loss 9.030163 loss_att 11.560081 loss_ctc 10.158922 loss_rnnt 8.297557 hw_loss 0.142728 lr 0.00033748 rank 2
2023-02-25 09:13:44,201 DEBUG TRAIN Batch 26/2800 loss 6.089532 loss_att 8.408225 loss_ctc 7.940912 loss_rnnt 5.290139 hw_loss 0.166508 lr 0.00033739 rank 6
2023-02-25 09:13:44,210 DEBUG TRAIN Batch 26/2800 loss 4.417385 loss_att 7.532358 loss_ctc 6.330577 loss_rnnt 3.432050 hw_loss 0.201090 lr 0.00033743 rank 1
2023-02-25 09:13:44,212 DEBUG TRAIN Batch 26/2800 loss 6.976803 loss_att 11.616858 loss_ctc 11.013849 loss_rnnt 5.437586 hw_loss 0.136750 lr 0.00033738 rank 0
2023-02-25 09:13:44,213 DEBUG TRAIN Batch 26/2800 loss 19.745676 loss_att 21.766699 loss_ctc 28.571833 loss_rnnt 18.027782 hw_loss 0.256630 lr 0.00033740 rank 3
2023-02-25 09:13:44,215 DEBUG TRAIN Batch 26/2800 loss 5.565365 loss_att 7.829382 loss_ctc 7.458027 loss_rnnt 4.743416 hw_loss 0.218983 lr 0.00033738 rank 4
2023-02-25 09:13:44,215 DEBUG TRAIN Batch 26/2800 loss 7.928590 loss_att 11.000217 loss_ctc 13.897455 loss_rnnt 6.456463 hw_loss 0.116161 lr 0.00033735 rank 7
2023-02-25 09:13:44,218 DEBUG TRAIN Batch 26/2800 loss 4.331129 loss_att 8.288813 loss_ctc 8.211517 loss_rnnt 2.929092 hw_loss 0.174591 lr 0.00033740 rank 2
2023-02-25 09:13:44,276 DEBUG TRAIN Batch 26/2800 loss 5.514385 loss_att 9.825480 loss_ctc 9.035852 loss_rnnt 4.081656 hw_loss 0.189339 lr 0.00033737 rank 5
2023-02-25 09:14:56,404 DEBUG TRAIN Batch 26/2900 loss 5.313546 loss_att 8.450249 loss_ctc 7.302920 loss_rnnt 4.315420 hw_loss 0.197879 lr 0.00033733 rank 2
2023-02-25 09:14:56,405 DEBUG TRAIN Batch 26/2900 loss 4.899283 loss_att 7.654701 loss_ctc 6.942822 loss_rnnt 4.038863 hw_loss 0.069121 lr 0.00033727 rank 7
2023-02-25 09:14:56,415 DEBUG TRAIN Batch 26/2900 loss 9.800920 loss_att 9.890134 loss_ctc 13.657330 loss_rnnt 9.156092 hw_loss 0.211496 lr 0.00033730 rank 0
2023-02-25 09:14:56,418 DEBUG TRAIN Batch 26/2900 loss 6.687636 loss_att 12.069330 loss_ctc 7.375498 loss_rnnt 5.425821 hw_loss 0.175804 lr 0.00033736 rank 1
2023-02-25 09:14:56,420 DEBUG TRAIN Batch 26/2900 loss 17.987967 loss_att 25.355362 loss_ctc 30.995729 loss_rnnt 14.655125 hw_loss 0.234365 lr 0.00033731 rank 6
2023-02-25 09:14:56,424 DEBUG TRAIN Batch 26/2900 loss 5.772840 loss_att 7.407185 loss_ctc 8.595345 loss_rnnt 4.945431 hw_loss 0.232885 lr 0.00033730 rank 4
2023-02-25 09:14:56,425 DEBUG TRAIN Batch 26/2900 loss 12.359334 loss_att 17.143930 loss_ctc 18.138231 loss_rnnt 10.530943 hw_loss 0.189285 lr 0.00033730 rank 5
2023-02-25 09:14:56,470 DEBUG TRAIN Batch 26/2900 loss 6.823607 loss_att 10.017617 loss_ctc 9.932685 loss_rnnt 5.621870 hw_loss 0.278233 lr 0.00033733 rank 3
2023-02-25 09:16:06,884 DEBUG TRAIN Batch 26/3000 loss 5.421427 loss_att 7.776734 loss_ctc 9.252181 loss_rnnt 4.361474 hw_loss 0.146483 lr 0.00033723 rank 0
2023-02-25 09:16:06,885 DEBUG TRAIN Batch 26/3000 loss 5.661422 loss_att 8.575508 loss_ctc 7.039013 loss_rnnt 4.804976 hw_loss 0.168655 lr 0.00033723 rank 4
2023-02-25 09:16:06,886 DEBUG TRAIN Batch 26/3000 loss 10.668539 loss_att 12.248549 loss_ctc 13.856150 loss_rnnt 9.823446 hw_loss 0.195143 lr 0.00033725 rank 3
2023-02-25 09:16:06,889 DEBUG TRAIN Batch 26/3000 loss 7.435741 loss_att 9.255098 loss_ctc 8.791794 loss_rnnt 6.759224 hw_loss 0.247199 lr 0.00033725 rank 2
2023-02-25 09:16:06,891 DEBUG TRAIN Batch 26/3000 loss 4.106922 loss_att 6.805664 loss_ctc 5.977692 loss_rnnt 3.230155 hw_loss 0.164218 lr 0.00033728 rank 1
2023-02-25 09:16:06,891 DEBUG TRAIN Batch 26/3000 loss 12.568707 loss_att 14.435744 loss_ctc 18.799343 loss_rnnt 11.266083 hw_loss 0.184625 lr 0.00033720 rank 7
2023-02-25 09:16:06,892 DEBUG TRAIN Batch 26/3000 loss 5.008826 loss_att 8.901931 loss_ctc 11.117436 loss_rnnt 3.315350 hw_loss 0.188201 lr 0.00033722 rank 5
2023-02-25 09:16:06,893 DEBUG TRAIN Batch 26/3000 loss 4.653505 loss_att 9.008162 loss_ctc 9.354059 loss_rnnt 3.042311 hw_loss 0.212855 lr 0.00033723 rank 6
2023-02-25 09:17:17,503 DEBUG TRAIN Batch 26/3100 loss 14.004293 loss_att 15.146040 loss_ctc 21.979279 loss_rnnt 12.593851 hw_loss 0.222677 lr 0.00033715 rank 0
2023-02-25 09:17:17,508 DEBUG TRAIN Batch 26/3100 loss 2.494701 loss_att 4.460937 loss_ctc 2.593597 loss_rnnt 1.985648 hw_loss 0.192412 lr 0.00033716 rank 6
2023-02-25 09:17:17,509 DEBUG TRAIN Batch 26/3100 loss 4.609421 loss_att 7.984481 loss_ctc 6.528228 loss_rnnt 3.570368 hw_loss 0.202876 lr 0.00033720 rank 1
2023-02-25 09:17:17,513 DEBUG TRAIN Batch 26/3100 loss 12.454287 loss_att 13.694734 loss_ctc 20.596945 loss_rnnt 10.995838 hw_loss 0.233759 lr 0.00033714 rank 5
2023-02-25 09:17:17,515 DEBUG TRAIN Batch 26/3100 loss 6.864897 loss_att 9.670120 loss_ctc 9.617816 loss_rnnt 5.772711 hw_loss 0.307661 lr 0.00033715 rank 4
2023-02-25 09:17:17,514 DEBUG TRAIN Batch 26/3100 loss 8.106843 loss_att 8.857053 loss_ctc 10.347133 loss_rnnt 7.487553 hw_loss 0.319768 lr 0.00033717 rank 3
2023-02-25 09:17:17,515 DEBUG TRAIN Batch 26/3100 loss 17.639885 loss_att 27.702091 loss_ctc 28.786156 loss_rnnt 14.019823 hw_loss 0.227717 lr 0.00033717 rank 2
2023-02-25 09:17:17,516 DEBUG TRAIN Batch 26/3100 loss 9.138155 loss_att 10.400694 loss_ctc 10.855876 loss_rnnt 8.560098 hw_loss 0.180974 lr 0.00033712 rank 7
2023-02-25 09:18:31,794 DEBUG TRAIN Batch 26/3200 loss 5.411949 loss_att 10.850816 loss_ctc 6.003925 loss_rnnt 4.163062 hw_loss 0.154095 lr 0.00033707 rank 0
2023-02-25 09:18:31,796 DEBUG TRAIN Batch 26/3200 loss 8.069299 loss_att 8.442135 loss_ctc 10.868964 loss_rnnt 7.460481 hw_loss 0.301803 lr 0.00033713 rank 1
2023-02-25 09:18:31,797 DEBUG TRAIN Batch 26/3200 loss 5.731219 loss_att 7.369185 loss_ctc 9.565439 loss_rnnt 4.732554 hw_loss 0.299705 lr 0.00033708 rank 6
2023-02-25 09:18:31,800 DEBUG TRAIN Batch 26/3200 loss 8.244336 loss_att 8.436306 loss_ctc 10.840115 loss_rnnt 7.749914 hw_loss 0.206109 lr 0.00033707 rank 4
2023-02-25 09:18:31,802 DEBUG TRAIN Batch 26/3200 loss 8.000516 loss_att 9.938084 loss_ctc 10.048671 loss_rnnt 7.179352 hw_loss 0.301057 lr 0.00033710 rank 2
2023-02-25 09:18:31,805 DEBUG TRAIN Batch 26/3200 loss 14.735871 loss_att 16.736481 loss_ctc 16.984589 loss_rnnt 13.927684 hw_loss 0.202944 lr 0.00033707 rank 5
2023-02-25 09:18:31,806 DEBUG TRAIN Batch 26/3200 loss 6.331638 loss_att 6.916019 loss_ctc 8.245444 loss_rnnt 5.807001 hw_loss 0.286098 lr 0.00033704 rank 7
2023-02-25 09:18:31,848 DEBUG TRAIN Batch 26/3200 loss 5.085651 loss_att 7.455341 loss_ctc 7.430275 loss_rnnt 4.154583 hw_loss 0.270964 lr 0.00033710 rank 3
2023-02-25 09:19:42,569 DEBUG TRAIN Batch 26/3300 loss 9.411305 loss_att 14.048849 loss_ctc 12.998492 loss_rnnt 7.941003 hw_loss 0.120940 lr 0.00033700 rank 4
2023-02-25 09:19:42,572 DEBUG TRAIN Batch 26/3300 loss 5.459291 loss_att 7.374970 loss_ctc 9.203278 loss_rnnt 4.437516 hw_loss 0.261451 lr 0.00033700 rank 0
2023-02-25 09:19:42,574 DEBUG TRAIN Batch 26/3300 loss 5.738122 loss_att 9.095916 loss_ctc 7.727052 loss_rnnt 4.709421 hw_loss 0.172411 lr 0.00033702 rank 2
2023-02-25 09:19:42,574 DEBUG TRAIN Batch 26/3300 loss 4.293756 loss_att 7.297992 loss_ctc 5.269355 loss_rnnt 3.503948 hw_loss 0.110404 lr 0.00033705 rank 1
2023-02-25 09:19:42,574 DEBUG TRAIN Batch 26/3300 loss 6.549713 loss_att 10.329359 loss_ctc 11.717269 loss_rnnt 4.970087 hw_loss 0.252542 lr 0.00033700 rank 6
2023-02-25 09:19:42,574 DEBUG TRAIN Batch 26/3300 loss 3.411826 loss_att 7.320434 loss_ctc 7.804117 loss_rnnt 1.989528 hw_loss 0.103010 lr 0.00033702 rank 3
2023-02-25 09:19:42,574 DEBUG TRAIN Batch 26/3300 loss 5.987486 loss_att 8.478491 loss_ctc 9.372322 loss_rnnt 4.971068 hw_loss 0.125448 lr 0.00033699 rank 5
2023-02-25 09:19:42,578 DEBUG TRAIN Batch 26/3300 loss 9.299870 loss_att 13.042039 loss_ctc 11.011753 loss_rnnt 8.248392 hw_loss 0.140235 lr 0.00033697 rank 7
2023-02-25 09:20:53,459 DEBUG TRAIN Batch 26/3400 loss 9.189155 loss_att 12.346211 loss_ctc 15.575804 loss_rnnt 7.608078 hw_loss 0.183958 lr 0.00033694 rank 3
2023-02-25 09:20:53,462 DEBUG TRAIN Batch 26/3400 loss 4.359360 loss_att 7.442997 loss_ctc 7.088957 loss_rnnt 3.252734 hw_loss 0.236161 lr 0.00033692 rank 0
2023-02-25 09:20:53,462 DEBUG TRAIN Batch 26/3400 loss 5.678708 loss_att 6.421773 loss_ctc 8.144107 loss_rnnt 5.109097 hw_loss 0.173022 lr 0.00033693 rank 6
2023-02-25 09:20:53,464 DEBUG TRAIN Batch 26/3400 loss 13.216479 loss_att 14.305093 loss_ctc 16.897995 loss_rnnt 12.391060 hw_loss 0.219052 lr 0.00033697 rank 1
2023-02-25 09:20:53,464 DEBUG TRAIN Batch 26/3400 loss 9.104404 loss_att 11.718812 loss_ctc 17.120888 loss_rnnt 7.450887 hw_loss 0.115822 lr 0.00033689 rank 7
2023-02-25 09:20:53,472 DEBUG TRAIN Batch 26/3400 loss 16.277971 loss_att 21.242420 loss_ctc 23.298134 loss_rnnt 14.268940 hw_loss 0.150224 lr 0.00033692 rank 4
2023-02-25 09:20:53,473 DEBUG TRAIN Batch 26/3400 loss 14.524212 loss_att 18.796133 loss_ctc 19.872608 loss_rnnt 12.897926 hw_loss 0.110216 lr 0.00033694 rank 2
2023-02-25 09:20:53,473 DEBUG TRAIN Batch 26/3400 loss 8.321427 loss_att 12.031739 loss_ctc 15.994637 loss_rnnt 6.465762 hw_loss 0.169703 lr 0.00033691 rank 5
2023-02-25 09:22:05,287 DEBUG TRAIN Batch 26/3500 loss 6.535620 loss_att 7.337350 loss_ctc 7.708079 loss_rnnt 6.124795 hw_loss 0.176534 lr 0.00033687 rank 3
2023-02-25 09:22:05,298 DEBUG TRAIN Batch 26/3500 loss 7.120889 loss_att 11.161132 loss_ctc 12.004060 loss_rnnt 5.575842 hw_loss 0.161079 lr 0.00033685 rank 6
2023-02-25 09:22:05,302 DEBUG TRAIN Batch 26/3500 loss 4.177725 loss_att 6.038808 loss_ctc 3.541527 loss_rnnt 3.726831 hw_loss 0.306570 lr 0.00033687 rank 2
2023-02-25 09:22:05,302 DEBUG TRAIN Batch 26/3500 loss 5.138791 loss_att 7.284002 loss_ctc 6.552975 loss_rnnt 4.466660 hw_loss 0.102246 lr 0.00033690 rank 1
2023-02-25 09:22:05,302 DEBUG TRAIN Batch 26/3500 loss 3.921819 loss_att 6.719144 loss_ctc 6.918325 loss_rnnt 2.833721 hw_loss 0.242061 lr 0.00033685 rank 4
2023-02-25 09:22:05,303 DEBUG TRAIN Batch 26/3500 loss 5.668752 loss_att 8.478395 loss_ctc 8.938785 loss_rnnt 4.607930 hw_loss 0.117915 lr 0.00033684 rank 0
2023-02-25 09:22:05,305 DEBUG TRAIN Batch 26/3500 loss 5.612986 loss_att 9.207836 loss_ctc 9.028135 loss_rnnt 4.333381 hw_loss 0.197403 lr 0.00033684 rank 5
2023-02-25 09:22:05,308 DEBUG TRAIN Batch 26/3500 loss 4.125920 loss_att 8.294229 loss_ctc 8.688623 loss_rnnt 2.551160 hw_loss 0.248883 lr 0.00033681 rank 7
2023-02-25 09:23:18,035 DEBUG TRAIN Batch 26/3600 loss 11.347515 loss_att 11.963415 loss_ctc 15.848543 loss_rnnt 10.574204 hw_loss 0.093737 lr 0.00033682 rank 1
2023-02-25 09:23:18,035 DEBUG TRAIN Batch 26/3600 loss 13.908321 loss_att 19.108761 loss_ctc 21.646564 loss_rnnt 11.762913 hw_loss 0.137916 lr 0.00033677 rank 6
2023-02-25 09:23:18,038 DEBUG TRAIN Batch 26/3600 loss 13.336640 loss_att 14.635591 loss_ctc 17.504519 loss_rnnt 12.426237 hw_loss 0.177929 lr 0.00033679 rank 2
2023-02-25 09:23:18,039 DEBUG TRAIN Batch 26/3600 loss 4.978839 loss_att 8.875454 loss_ctc 5.340281 loss_rnnt 4.071307 hw_loss 0.150031 lr 0.00033679 rank 3
2023-02-25 09:23:18,040 DEBUG TRAIN Batch 26/3600 loss 6.366898 loss_att 10.175806 loss_ctc 9.706438 loss_rnnt 5.075109 hw_loss 0.158876 lr 0.00033677 rank 0
2023-02-25 09:23:18,043 DEBUG TRAIN Batch 26/3600 loss 8.087494 loss_att 8.832078 loss_ctc 9.439850 loss_rnnt 7.648919 hw_loss 0.205020 lr 0.00033677 rank 4
2023-02-25 09:23:18,044 DEBUG TRAIN Batch 26/3600 loss 13.533192 loss_att 15.384105 loss_ctc 17.631348 loss_rnnt 12.506825 hw_loss 0.205804 lr 0.00033676 rank 5
2023-02-25 09:23:18,044 DEBUG TRAIN Batch 26/3600 loss 11.819118 loss_att 14.996810 loss_ctc 19.302755 loss_rnnt 10.093062 hw_loss 0.173810 lr 0.00033674 rank 7
2023-02-25 09:24:28,300 DEBUG TRAIN Batch 26/3700 loss 9.120608 loss_att 10.209779 loss_ctc 13.404726 loss_rnnt 8.213103 hw_loss 0.222103 lr 0.00033672 rank 3
2023-02-25 09:24:28,301 DEBUG TRAIN Batch 26/3700 loss 4.374736 loss_att 6.688447 loss_ctc 6.545210 loss_rnnt 3.515127 hw_loss 0.201507 lr 0.00033670 rank 6
2023-02-25 09:24:28,301 DEBUG TRAIN Batch 26/3700 loss 10.380902 loss_att 12.684937 loss_ctc 13.849436 loss_rnnt 9.371236 hw_loss 0.161977 lr 0.00033669 rank 4
2023-02-25 09:24:28,302 DEBUG TRAIN Batch 26/3700 loss 7.031863 loss_att 10.155678 loss_ctc 10.904972 loss_rnnt 5.785423 hw_loss 0.197367 lr 0.00033669 rank 0
2023-02-25 09:24:28,305 DEBUG TRAIN Batch 26/3700 loss 7.657697 loss_att 12.738369 loss_ctc 11.592805 loss_rnnt 5.983882 hw_loss 0.249376 lr 0.00033674 rank 1
2023-02-25 09:24:28,309 DEBUG TRAIN Batch 26/3700 loss 13.338408 loss_att 17.211626 loss_ctc 22.439753 loss_rnnt 11.229435 hw_loss 0.226532 lr 0.00033666 rank 7
2023-02-25 09:24:28,310 DEBUG TRAIN Batch 26/3700 loss 8.956464 loss_att 9.051796 loss_ctc 11.746634 loss_rnnt 8.437342 hw_loss 0.240061 lr 0.00033669 rank 5
2023-02-25 09:24:28,311 DEBUG TRAIN Batch 26/3700 loss 12.192889 loss_att 13.534712 loss_ctc 16.984879 loss_rnnt 11.185792 hw_loss 0.187124 lr 0.00033671 rank 2
2023-02-25 09:25:39,140 DEBUG TRAIN Batch 26/3800 loss 7.481229 loss_att 7.923761 loss_ctc 9.304928 loss_rnnt 7.016012 hw_loss 0.250406 lr 0.00033661 rank 0
2023-02-25 09:25:39,143 DEBUG TRAIN Batch 26/3800 loss 4.386817 loss_att 7.206308 loss_ctc 7.857426 loss_rnnt 3.269917 hw_loss 0.169228 lr 0.00033664 rank 3
2023-02-25 09:25:39,144 DEBUG TRAIN Batch 26/3800 loss 2.563859 loss_att 4.999548 loss_ctc 4.968507 loss_rnnt 1.647584 hw_loss 0.203469 lr 0.00033667 rank 1
2023-02-25 09:25:39,144 DEBUG TRAIN Batch 26/3800 loss 5.248738 loss_att 8.982891 loss_ctc 9.437909 loss_rnnt 3.855449 hw_loss 0.164815 lr 0.00033659 rank 7
2023-02-25 09:25:39,147 DEBUG TRAIN Batch 26/3800 loss 13.267985 loss_att 13.413854 loss_ctc 14.963837 loss_rnnt 12.855346 hw_loss 0.295035 lr 0.00033662 rank 4
2023-02-25 09:25:39,148 DEBUG TRAIN Batch 26/3800 loss 11.816872 loss_att 13.298405 loss_ctc 18.628693 loss_rnnt 10.497550 hw_loss 0.215199 lr 0.00033662 rank 6
2023-02-25 09:25:39,151 DEBUG TRAIN Batch 26/3800 loss 9.159764 loss_att 11.068619 loss_ctc 11.736857 loss_rnnt 8.322969 hw_loss 0.208898 lr 0.00033664 rank 2
2023-02-25 09:25:39,150 DEBUG TRAIN Batch 26/3800 loss 16.582443 loss_att 18.781672 loss_ctc 27.078894 loss_rnnt 14.617407 hw_loss 0.235620 lr 0.00033661 rank 5
2023-02-25 09:26:52,632 DEBUG TRAIN Batch 26/3900 loss 5.327369 loss_att 8.396709 loss_ctc 6.669868 loss_rnnt 4.404996 hw_loss 0.242823 lr 0.00033654 rank 0
2023-02-25 09:26:52,637 DEBUG TRAIN Batch 26/3900 loss 11.018675 loss_att 11.440069 loss_ctc 16.356350 loss_rnnt 10.118939 hw_loss 0.194563 lr 0.00033654 rank 4
2023-02-25 09:26:52,638 DEBUG TRAIN Batch 26/3900 loss 3.824052 loss_att 9.675581 loss_ctc 6.246870 loss_rnnt 2.259862 hw_loss 0.132829 lr 0.00033659 rank 1
2023-02-25 09:26:52,642 DEBUG TRAIN Batch 26/3900 loss 3.756724 loss_att 7.016810 loss_ctc 6.784045 loss_rnnt 2.584407 hw_loss 0.218732 lr 0.00033651 rank 7
2023-02-25 09:26:52,642 DEBUG TRAIN Batch 26/3900 loss 5.215418 loss_att 8.034094 loss_ctc 6.529084 loss_rnnt 4.357217 hw_loss 0.223706 lr 0.00033656 rank 3
2023-02-25 09:26:52,642 DEBUG TRAIN Batch 26/3900 loss 5.231339 loss_att 6.433048 loss_ctc 4.432435 loss_rnnt 5.050901 hw_loss 0.087405 lr 0.00033654 rank 6
2023-02-25 09:26:52,644 DEBUG TRAIN Batch 26/3900 loss 1.961335 loss_att 4.472880 loss_ctc 3.643992 loss_rnnt 1.159774 hw_loss 0.140433 lr 0.00033653 rank 5
2023-02-25 09:26:52,645 DEBUG TRAIN Batch 26/3900 loss 9.687783 loss_att 13.677378 loss_ctc 14.681674 loss_rnnt 8.160698 hw_loss 0.118716 lr 0.00033656 rank 2
2023-02-25 09:28:03,209 DEBUG TRAIN Batch 26/4000 loss 18.526712 loss_att 19.425142 loss_ctc 30.069677 loss_rnnt 16.687420 hw_loss 0.226020 lr 0.00033649 rank 3
2023-02-25 09:28:03,224 DEBUG TRAIN Batch 26/4000 loss 4.269746 loss_att 6.104025 loss_ctc 6.236400 loss_rnnt 3.534052 hw_loss 0.199908 lr 0.00033646 rank 4
2023-02-25 09:28:03,226 DEBUG TRAIN Batch 26/4000 loss 8.844051 loss_att 9.914312 loss_ctc 11.070548 loss_rnnt 8.274463 hw_loss 0.110006 lr 0.00033646 rank 0
2023-02-25 09:28:03,228 DEBUG TRAIN Batch 26/4000 loss 8.492735 loss_att 11.749964 loss_ctc 11.625220 loss_rnnt 7.298663 hw_loss 0.234302 lr 0.00033649 rank 2
2023-02-25 09:28:03,232 DEBUG TRAIN Batch 26/4000 loss 14.774613 loss_att 21.507717 loss_ctc 21.717505 loss_rnnt 12.398028 hw_loss 0.195459 lr 0.00033643 rank 7
2023-02-25 09:28:03,233 DEBUG TRAIN Batch 26/4000 loss 7.529860 loss_att 10.002631 loss_ctc 10.185740 loss_rnnt 6.565523 hw_loss 0.216871 lr 0.00033647 rank 6
2023-02-25 09:28:03,235 DEBUG TRAIN Batch 26/4000 loss 9.465326 loss_att 12.122744 loss_ctc 13.284877 loss_rnnt 8.331699 hw_loss 0.174133 lr 0.00033646 rank 5
2023-02-25 09:28:03,236 DEBUG TRAIN Batch 26/4000 loss 4.076327 loss_att 6.618434 loss_ctc 6.226760 loss_rnnt 3.176452 hw_loss 0.196367 lr 0.00033651 rank 1
2023-02-25 09:29:13,141 DEBUG TRAIN Batch 26/4100 loss 10.914286 loss_att 16.824999 loss_ctc 15.471100 loss_rnnt 8.983372 hw_loss 0.264744 lr 0.00033639 rank 6
2023-02-25 09:29:13,142 DEBUG TRAIN Batch 26/4100 loss 9.523673 loss_att 12.951993 loss_ctc 23.967319 loss_rnnt 6.800994 hw_loss 0.208492 lr 0.00033641 rank 3
2023-02-25 09:29:13,141 DEBUG TRAIN Batch 26/4100 loss 2.667362 loss_att 4.881054 loss_ctc 2.543817 loss_rnnt 2.169320 hw_loss 0.134580 lr 0.00033639 rank 4
2023-02-25 09:29:13,142 DEBUG TRAIN Batch 26/4100 loss 3.667196 loss_att 7.245922 loss_ctc 5.721664 loss_rnnt 2.499201 hw_loss 0.334351 lr 0.00033641 rank 2
2023-02-25 09:29:13,143 DEBUG TRAIN Batch 26/4100 loss 11.780345 loss_att 10.963902 loss_ctc 14.626872 loss_rnnt 11.449974 hw_loss 0.213978 lr 0.00033639 rank 0
2023-02-25 09:29:13,146 DEBUG TRAIN Batch 26/4100 loss 10.146967 loss_att 11.961679 loss_ctc 12.659721 loss_rnnt 9.359865 hw_loss 0.167108 lr 0.00033644 rank 1
2023-02-25 09:29:13,147 DEBUG TRAIN Batch 26/4100 loss 12.674018 loss_att 16.803560 loss_ctc 17.435658 loss_rnnt 11.148778 hw_loss 0.120837 lr 0.00033638 rank 5
2023-02-25 09:29:13,149 DEBUG TRAIN Batch 26/4100 loss 6.888554 loss_att 10.556603 loss_ctc 10.358787 loss_rnnt 5.601620 hw_loss 0.169925 lr 0.00033636 rank 7
2023-02-25 09:30:23,442 DEBUG TRAIN Batch 26/4200 loss 6.214071 loss_att 7.093105 loss_ctc 7.954056 loss_rnnt 5.699564 hw_loss 0.200067 lr 0.00033631 rank 4
2023-02-25 09:30:23,453 DEBUG TRAIN Batch 26/4200 loss 5.224036 loss_att 8.413721 loss_ctc 7.881241 loss_rnnt 4.134525 hw_loss 0.182400 lr 0.00033631 rank 0
2023-02-25 09:30:23,456 DEBUG TRAIN Batch 26/4200 loss 14.105911 loss_att 16.286976 loss_ctc 27.244438 loss_rnnt 11.824335 hw_loss 0.175423 lr 0.00033632 rank 6
2023-02-25 09:30:23,456 DEBUG TRAIN Batch 26/4200 loss 4.261538 loss_att 6.361996 loss_ctc 4.851045 loss_rnnt 3.667059 hw_loss 0.179599 lr 0.00033628 rank 7
2023-02-25 09:30:23,458 DEBUG TRAIN Batch 26/4200 loss 5.882710 loss_att 9.550624 loss_ctc 9.084822 loss_rnnt 4.623424 hw_loss 0.185168 lr 0.00033636 rank 1
2023-02-25 09:30:23,459 DEBUG TRAIN Batch 26/4200 loss 7.262344 loss_att 8.312796 loss_ctc 10.006606 loss_rnnt 6.567662 hw_loss 0.222542 lr 0.00033633 rank 3
2023-02-25 09:30:23,459 DEBUG TRAIN Batch 26/4200 loss 3.067639 loss_att 5.515445 loss_ctc 4.094800 loss_rnnt 2.337268 hw_loss 0.194728 lr 0.00033633 rank 2
2023-02-25 09:30:23,464 DEBUG TRAIN Batch 26/4200 loss 8.689032 loss_att 13.121149 loss_ctc 10.544329 loss_rnnt 7.440244 hw_loss 0.215610 lr 0.00033630 rank 5
2023-02-25 09:31:37,736 DEBUG TRAIN Batch 26/4300 loss 5.254367 loss_att 8.730818 loss_ctc 11.863743 loss_rnnt 3.573791 hw_loss 0.195068 lr 0.00033626 rank 2
2023-02-25 09:31:37,737 DEBUG TRAIN Batch 26/4300 loss 13.752187 loss_att 13.927298 loss_ctc 15.698692 loss_rnnt 13.367218 hw_loss 0.169524 lr 0.00033623 rank 0
2023-02-25 09:31:37,743 DEBUG TRAIN Batch 26/4300 loss 14.865845 loss_att 18.487518 loss_ctc 20.551836 loss_rnnt 13.336998 hw_loss 0.086964 lr 0.00033629 rank 1
2023-02-25 09:31:37,743 DEBUG TRAIN Batch 26/4300 loss 8.465969 loss_att 11.232522 loss_ctc 12.905691 loss_rnnt 7.225288 hw_loss 0.178891 lr 0.00033620 rank 7
2023-02-25 09:31:37,744 DEBUG TRAIN Batch 26/4300 loss 10.436613 loss_att 13.561831 loss_ctc 15.372349 loss_rnnt 9.023169 hw_loss 0.244318 lr 0.00033624 rank 6
2023-02-25 09:31:37,746 DEBUG TRAIN Batch 26/4300 loss 6.515676 loss_att 10.043742 loss_ctc 7.507380 loss_rnnt 5.573523 hw_loss 0.195585 lr 0.00033623 rank 5
2023-02-25 09:31:37,748 DEBUG TRAIN Batch 26/4300 loss 8.023540 loss_att 10.519486 loss_ctc 9.380194 loss_rnnt 7.209693 hw_loss 0.250819 lr 0.00033624 rank 4
2023-02-25 09:31:37,802 DEBUG TRAIN Batch 26/4300 loss 6.357750 loss_att 6.775728 loss_ctc 8.663145 loss_rnnt 5.792754 hw_loss 0.326277 lr 0.00033626 rank 3
2023-02-25 09:32:48,988 DEBUG TRAIN Batch 26/4400 loss 3.387065 loss_att 5.692548 loss_ctc 6.026017 loss_rnnt 2.471094 hw_loss 0.193152 lr 0.00033618 rank 3
2023-02-25 09:32:48,991 DEBUG TRAIN Batch 26/4400 loss 12.949404 loss_att 15.812082 loss_ctc 21.657520 loss_rnnt 11.122857 hw_loss 0.174242 lr 0.00033616 rank 6
2023-02-25 09:32:48,990 DEBUG TRAIN Batch 26/4400 loss 7.355055 loss_att 8.406194 loss_ctc 9.450809 loss_rnnt 6.749645 hw_loss 0.217028 lr 0.00033615 rank 5
2023-02-25 09:32:48,994 DEBUG TRAIN Batch 26/4400 loss 12.691795 loss_att 14.952834 loss_ctc 22.003651 loss_rnnt 10.844951 hw_loss 0.286979 lr 0.00033613 rank 7
2023-02-25 09:32:48,995 DEBUG TRAIN Batch 26/4400 loss 10.073284 loss_att 13.349303 loss_ctc 12.525796 loss_rnnt 8.960045 hw_loss 0.245688 lr 0.00033618 rank 2
2023-02-25 09:32:48,994 DEBUG TRAIN Batch 26/4400 loss 8.721537 loss_att 12.177581 loss_ctc 11.550713 loss_rnnt 7.524837 hw_loss 0.240502 lr 0.00033616 rank 4
2023-02-25 09:32:48,995 DEBUG TRAIN Batch 26/4400 loss 10.443385 loss_att 11.679815 loss_ctc 13.175562 loss_rnnt 9.699861 hw_loss 0.247403 lr 0.00033621 rank 1
2023-02-25 09:32:48,996 DEBUG TRAIN Batch 26/4400 loss 9.239006 loss_att 11.359567 loss_ctc 13.084318 loss_rnnt 8.161003 hw_loss 0.264717 lr 0.00033616 rank 0
2023-02-25 09:33:59,173 DEBUG TRAIN Batch 26/4500 loss 7.163858 loss_att 7.857282 loss_ctc 8.559480 loss_rnnt 6.702402 hw_loss 0.256292 lr 0.00033608 rank 4
2023-02-25 09:33:59,174 DEBUG TRAIN Batch 26/4500 loss 8.414742 loss_att 12.057086 loss_ctc 12.794188 loss_rnnt 7.026154 hw_loss 0.142862 lr 0.00033608 rank 0
2023-02-25 09:33:59,174 DEBUG TRAIN Batch 26/4500 loss 4.972236 loss_att 8.544234 loss_ctc 4.710019 loss_rnnt 4.184680 hw_loss 0.202723 lr 0.00033609 rank 6
2023-02-25 09:33:59,176 DEBUG TRAIN Batch 26/4500 loss 9.548288 loss_att 10.481569 loss_ctc 11.965109 loss_rnnt 8.882133 hw_loss 0.294854 lr 0.00033613 rank 1
2023-02-25 09:33:59,176 DEBUG TRAIN Batch 26/4500 loss 7.008618 loss_att 8.474474 loss_ctc 10.243838 loss_rnnt 6.159377 hw_loss 0.233827 lr 0.00033605 rank 7
2023-02-25 09:33:59,179 DEBUG TRAIN Batch 26/4500 loss 14.351330 loss_att 14.808541 loss_ctc 18.336241 loss_rnnt 13.612521 hw_loss 0.217586 lr 0.00033611 rank 2
2023-02-25 09:33:59,184 DEBUG TRAIN Batch 26/4500 loss 12.986727 loss_att 13.958719 loss_ctc 14.717315 loss_rnnt 12.447806 hw_loss 0.213332 lr 0.00033608 rank 5
2023-02-25 09:33:59,224 DEBUG TRAIN Batch 26/4500 loss 12.954051 loss_att 14.371342 loss_ctc 13.814105 loss_rnnt 12.426570 hw_loss 0.242530 lr 0.00033611 rank 3
2023-02-25 09:35:11,595 DEBUG TRAIN Batch 26/4600 loss 2.279555 loss_att 4.410562 loss_ctc 2.399985 loss_rnnt 1.767126 hw_loss 0.131569 lr 0.00033601 rank 0
2023-02-25 09:35:11,596 DEBUG TRAIN Batch 26/4600 loss 7.612590 loss_att 9.621000 loss_ctc 14.360973 loss_rnnt 6.215964 hw_loss 0.178426 lr 0.00033601 rank 4
2023-02-25 09:35:11,597 DEBUG TRAIN Batch 26/4600 loss 7.343914 loss_att 10.300446 loss_ctc 9.381881 loss_rnnt 6.347104 hw_loss 0.250826 lr 0.00033603 rank 3
2023-02-25 09:35:11,597 DEBUG TRAIN Batch 26/4600 loss 5.461253 loss_att 7.471299 loss_ctc 8.790482 loss_rnnt 4.497417 hw_loss 0.221119 lr 0.00033601 rank 6
2023-02-25 09:35:11,599 DEBUG TRAIN Batch 26/4600 loss 4.839214 loss_att 10.642736 loss_ctc 8.012633 loss_rnnt 3.137624 hw_loss 0.220806 lr 0.00033603 rank 2
2023-02-25 09:35:11,599 DEBUG TRAIN Batch 26/4600 loss 4.141529 loss_att 4.789053 loss_ctc 4.182559 loss_rnnt 3.899964 hw_loss 0.199854 lr 0.00033606 rank 1
2023-02-25 09:35:11,601 DEBUG TRAIN Batch 26/4600 loss 6.586442 loss_att 11.730145 loss_ctc 11.439579 loss_rnnt 4.805352 hw_loss 0.197372 lr 0.00033598 rank 7
2023-02-25 09:35:11,621 DEBUG TRAIN Batch 26/4600 loss 10.995998 loss_att 13.921355 loss_ctc 17.903826 loss_rnnt 9.423182 hw_loss 0.125064 lr 0.00033600 rank 5
2023-02-25 09:36:23,759 DEBUG TRAIN Batch 26/4700 loss 6.468262 loss_att 8.589569 loss_ctc 8.577682 loss_rnnt 5.686546 hw_loss 0.142872 lr 0.00033593 rank 4
2023-02-25 09:36:23,758 DEBUG TRAIN Batch 26/4700 loss 7.887509 loss_att 11.781881 loss_ctc 12.034113 loss_rnnt 6.461353 hw_loss 0.177001 lr 0.00033593 rank 0
2023-02-25 09:36:23,760 DEBUG TRAIN Batch 26/4700 loss 5.911454 loss_att 8.283815 loss_ctc 8.127750 loss_rnnt 5.035748 hw_loss 0.198237 lr 0.00033592 rank 5
2023-02-25 09:36:23,761 DEBUG TRAIN Batch 26/4700 loss 11.194932 loss_att 11.284589 loss_ctc 12.148227 loss_rnnt 10.960795 hw_loss 0.167063 lr 0.00033590 rank 7
2023-02-25 09:36:23,762 DEBUG TRAIN Batch 26/4700 loss 12.839924 loss_att 12.894412 loss_ctc 13.871806 loss_rnnt 12.585449 hw_loss 0.198737 lr 0.00033594 rank 6
2023-02-25 09:36:23,762 DEBUG TRAIN Batch 26/4700 loss 5.046576 loss_att 9.329041 loss_ctc 7.403394 loss_rnnt 3.757489 hw_loss 0.221910 lr 0.00033598 rank 1
2023-02-25 09:36:23,764 DEBUG TRAIN Batch 26/4700 loss 4.957075 loss_att 8.875387 loss_ctc 9.292748 loss_rnnt 3.472884 hw_loss 0.229573 lr 0.00033595 rank 3
2023-02-25 09:36:23,766 DEBUG TRAIN Batch 26/4700 loss 6.541281 loss_att 11.395094 loss_ctc 10.841259 loss_rnnt 4.869152 hw_loss 0.240068 lr 0.00033595 rank 2
2023-02-25 09:37:33,714 DEBUG TRAIN Batch 26/4800 loss 8.201508 loss_att 10.484914 loss_ctc 11.615365 loss_rnnt 7.239196 hw_loss 0.094594 lr 0.00033591 rank 1
2023-02-25 09:37:33,726 DEBUG TRAIN Batch 26/4800 loss 14.273300 loss_att 14.419256 loss_ctc 17.621056 loss_rnnt 13.710215 hw_loss 0.164110 lr 0.00033585 rank 0
2023-02-25 09:37:33,729 DEBUG TRAIN Batch 26/4800 loss 6.925978 loss_att 10.568959 loss_ctc 11.780825 loss_rnnt 5.373531 hw_loss 0.331009 lr 0.00033583 rank 7
2023-02-25 09:37:33,731 DEBUG TRAIN Batch 26/4800 loss 9.380743 loss_att 13.185933 loss_ctc 16.002872 loss_rnnt 7.568947 hw_loss 0.314638 lr 0.00033588 rank 2
2023-02-25 09:37:33,734 DEBUG TRAIN Batch 26/4800 loss 7.104994 loss_att 9.462793 loss_ctc 10.375243 loss_rnnt 6.131897 hw_loss 0.122820 lr 0.00033586 rank 4
2023-02-25 09:37:33,734 DEBUG TRAIN Batch 26/4800 loss 11.289845 loss_att 13.574156 loss_ctc 16.660959 loss_rnnt 10.075576 hw_loss 0.077359 lr 0.00033585 rank 5
2023-02-25 09:37:33,737 DEBUG TRAIN Batch 26/4800 loss 8.471903 loss_att 11.082648 loss_ctc 11.918315 loss_rnnt 7.410533 hw_loss 0.149436 lr 0.00033588 rank 3
2023-02-25 09:37:33,773 DEBUG TRAIN Batch 26/4800 loss 4.271510 loss_att 6.298578 loss_ctc 7.981397 loss_rnnt 3.311893 hw_loss 0.111661 lr 0.00033586 rank 6
2023-02-25 09:38:44,641 DEBUG TRAIN Batch 26/4900 loss 3.955242 loss_att 7.878263 loss_ctc 6.897777 loss_rnnt 2.645077 hw_loss 0.249794 lr 0.00033577 rank 5
2023-02-25 09:38:44,650 DEBUG TRAIN Batch 26/4900 loss 9.272615 loss_att 11.922947 loss_ctc 12.540606 loss_rnnt 8.221578 hw_loss 0.159821 lr 0.00033578 rank 4
2023-02-25 09:38:44,652 DEBUG TRAIN Batch 26/4900 loss 11.628656 loss_att 12.385743 loss_ctc 23.854057 loss_rnnt 9.752407 hw_loss 0.177710 lr 0.00033578 rank 0
2023-02-25 09:38:44,656 DEBUG TRAIN Batch 26/4900 loss 16.195030 loss_att 20.661482 loss_ctc 26.148882 loss_rnnt 13.911786 hw_loss 0.117696 lr 0.00033580 rank 2
2023-02-25 09:38:44,657 DEBUG TRAIN Batch 26/4900 loss 4.504626 loss_att 5.832934 loss_ctc 6.870125 loss_rnnt 3.804988 hw_loss 0.222332 lr 0.00033580 rank 3
2023-02-25 09:38:44,657 DEBUG TRAIN Batch 26/4900 loss 5.037449 loss_att 8.890797 loss_ctc 5.435613 loss_rnnt 4.095780 hw_loss 0.221083 lr 0.00033575 rank 7
2023-02-25 09:38:44,659 DEBUG TRAIN Batch 26/4900 loss 14.748058 loss_att 18.483047 loss_ctc 19.360302 loss_rnnt 13.319502 hw_loss 0.124862 lr 0.00033583 rank 1
2023-02-25 09:38:44,661 DEBUG TRAIN Batch 26/4900 loss 4.764410 loss_att 7.866299 loss_ctc 6.624188 loss_rnnt 3.768800 hw_loss 0.238614 lr 0.00033578 rank 6
2023-02-25 09:39:57,938 DEBUG TRAIN Batch 26/5000 loss 9.391879 loss_att 10.250310 loss_ctc 13.150774 loss_rnnt 8.605600 hw_loss 0.212638 lr 0.00033570 rank 0
2023-02-25 09:39:57,953 DEBUG TRAIN Batch 26/5000 loss 7.371933 loss_att 8.848438 loss_ctc 10.479217 loss_rnnt 6.557640 hw_loss 0.196291 lr 0.00033575 rank 1
2023-02-25 09:39:57,953 DEBUG TRAIN Batch 26/5000 loss 7.576776 loss_att 10.885345 loss_ctc 10.864098 loss_rnnt 6.340669 hw_loss 0.255157 lr 0.00033567 rank 7
2023-02-25 09:39:57,954 DEBUG TRAIN Batch 26/5000 loss 16.109514 loss_att 17.083374 loss_ctc 17.752554 loss_rnnt 15.620468 hw_loss 0.141004 lr 0.00033573 rank 3
2023-02-25 09:39:57,953 DEBUG TRAIN Batch 26/5000 loss 2.523952 loss_att 5.015528 loss_ctc 3.051188 loss_rnnt 1.855716 hw_loss 0.186793 lr 0.00033573 rank 2
2023-02-25 09:39:57,960 DEBUG TRAIN Batch 26/5000 loss 8.330807 loss_att 10.600889 loss_ctc 12.428337 loss_rnnt 7.237953 hw_loss 0.173438 lr 0.00033570 rank 5
2023-02-25 09:39:57,961 DEBUG TRAIN Batch 26/5000 loss 8.178123 loss_att 11.376395 loss_ctc 11.717967 loss_rnnt 6.935165 hw_loss 0.246233 lr 0.00033571 rank 6
2023-02-25 09:39:58,007 DEBUG TRAIN Batch 26/5000 loss 5.941373 loss_att 9.740929 loss_ctc 9.357238 loss_rnnt 4.632061 hw_loss 0.176161 lr 0.00033570 rank 4
2023-02-25 09:41:07,974 DEBUG TRAIN Batch 26/5100 loss 8.232394 loss_att 11.439752 loss_ctc 11.364345 loss_rnnt 7.102660 hw_loss 0.132507 lr 0.00033563 rank 0
2023-02-25 09:41:07,974 DEBUG TRAIN Batch 26/5100 loss 11.144038 loss_att 12.989733 loss_ctc 14.321956 loss_rnnt 10.258045 hw_loss 0.174622 lr 0.00033568 rank 1
2023-02-25 09:41:07,977 DEBUG TRAIN Batch 26/5100 loss 11.223989 loss_att 11.525125 loss_ctc 18.001369 loss_rnnt 10.128856 hw_loss 0.246107 lr 0.00033565 rank 2
2023-02-25 09:41:07,979 DEBUG TRAIN Batch 26/5100 loss 6.434480 loss_att 7.198854 loss_ctc 8.981346 loss_rnnt 5.795895 hw_loss 0.273990 lr 0.00033563 rank 6
2023-02-25 09:41:07,978 DEBUG TRAIN Batch 26/5100 loss 9.205850 loss_att 10.447654 loss_ctc 12.714720 loss_rnnt 8.315952 hw_loss 0.325662 lr 0.00033563 rank 4
2023-02-25 09:41:07,978 DEBUG TRAIN Batch 26/5100 loss 13.038720 loss_att 13.573164 loss_ctc 19.554996 loss_rnnt 11.894809 hw_loss 0.315350 lr 0.00033560 rank 7
2023-02-25 09:41:07,980 DEBUG TRAIN Batch 26/5100 loss 7.852053 loss_att 11.438074 loss_ctc 12.892822 loss_rnnt 6.375664 hw_loss 0.163279 lr 0.00033565 rank 3
2023-02-25 09:41:07,985 DEBUG TRAIN Batch 26/5100 loss 7.595410 loss_att 8.507651 loss_ctc 8.819529 loss_rnnt 7.107912 hw_loss 0.265941 lr 0.00033562 rank 5
2023-02-25 09:42:18,175 DEBUG TRAIN Batch 26/5200 loss 3.643491 loss_att 6.583898 loss_ctc 7.853542 loss_rnnt 2.384926 hw_loss 0.204644 lr 0.00033555 rank 0
2023-02-25 09:42:18,178 DEBUG TRAIN Batch 26/5200 loss 6.757967 loss_att 8.544729 loss_ctc 8.077389 loss_rnnt 6.160534 hw_loss 0.120295 lr 0.00033555 rank 4
2023-02-25 09:42:18,179 DEBUG TRAIN Batch 26/5200 loss 4.224873 loss_att 7.294362 loss_ctc 7.762863 loss_rnnt 3.024782 hw_loss 0.214615 lr 0.00033556 rank 6
2023-02-25 09:42:18,179 DEBUG TRAIN Batch 26/5200 loss 4.430085 loss_att 8.901094 loss_ctc 8.185100 loss_rnnt 2.985026 hw_loss 0.094103 lr 0.00033552 rank 7
2023-02-25 09:42:18,183 DEBUG TRAIN Batch 26/5200 loss 3.925570 loss_att 8.976460 loss_ctc 7.722373 loss_rnnt 2.341964 hw_loss 0.125977 lr 0.00033560 rank 1
2023-02-25 09:42:18,184 DEBUG TRAIN Batch 26/5200 loss 4.363176 loss_att 7.351439 loss_ctc 4.930063 loss_rnnt 3.636468 hw_loss 0.100257 lr 0.00033558 rank 2
2023-02-25 09:42:18,185 DEBUG TRAIN Batch 26/5200 loss 8.785316 loss_att 11.239179 loss_ctc 10.410788 loss_rnnt 7.972237 hw_loss 0.197959 lr 0.00033558 rank 3
2023-02-25 09:42:18,188 DEBUG TRAIN Batch 26/5200 loss 7.896276 loss_att 12.226113 loss_ctc 14.617808 loss_rnnt 6.067775 hw_loss 0.124366 lr 0.00033555 rank 5
2023-02-25 09:43:31,250 DEBUG TRAIN Batch 26/5300 loss 12.169001 loss_att 13.744692 loss_ctc 16.685257 loss_rnnt 11.162566 hw_loss 0.167117 lr 0.00033553 rank 1
2023-02-25 09:43:31,250 DEBUG TRAIN Batch 26/5300 loss 14.244402 loss_att 21.118153 loss_ctc 25.572783 loss_rnnt 11.230762 hw_loss 0.240821 lr 0.00033548 rank 0
2023-02-25 09:43:31,252 DEBUG TRAIN Batch 26/5300 loss 8.354574 loss_att 15.804461 loss_ctc 13.062420 loss_rnnt 6.137534 hw_loss 0.186281 lr 0.00033550 rank 2
2023-02-25 09:43:31,254 DEBUG TRAIN Batch 26/5300 loss 7.482525 loss_att 12.514132 loss_ctc 13.079767 loss_rnnt 5.652112 hw_loss 0.145860 lr 0.00033545 rank 7
2023-02-25 09:43:31,255 DEBUG TRAIN Batch 26/5300 loss 15.161857 loss_att 19.237307 loss_ctc 23.323935 loss_rnnt 13.134041 hw_loss 0.233341 lr 0.00033548 rank 6
2023-02-25 09:43:31,256 DEBUG TRAIN Batch 26/5300 loss 10.869522 loss_att 15.993208 loss_ctc 14.455175 loss_rnnt 9.284822 hw_loss 0.153517 lr 0.00033547 rank 5
2023-02-25 09:43:31,258 DEBUG TRAIN Batch 26/5300 loss 9.426517 loss_att 14.769989 loss_ctc 14.389583 loss_rnnt 7.624950 hw_loss 0.133372 lr 0.00033548 rank 4
2023-02-25 09:43:31,301 DEBUG TRAIN Batch 26/5300 loss 9.814308 loss_att 13.149388 loss_ctc 16.737251 loss_rnnt 8.127642 hw_loss 0.181108 lr 0.00033550 rank 3
2023-02-25 09:44:43,631 DEBUG TRAIN Batch 26/5400 loss 6.046995 loss_att 7.285542 loss_ctc 5.945000 loss_rnnt 5.669186 hw_loss 0.269435 lr 0.00033540 rank 0
2023-02-25 09:44:43,636 DEBUG TRAIN Batch 26/5400 loss 18.961672 loss_att 20.357647 loss_ctc 26.196381 loss_rnnt 17.600773 hw_loss 0.219518 lr 0.00033541 rank 6
2023-02-25 09:44:43,638 DEBUG TRAIN Batch 26/5400 loss 6.389410 loss_att 9.397943 loss_ctc 11.395598 loss_rnnt 4.984717 hw_loss 0.254052 lr 0.00033537 rank 7
2023-02-25 09:44:43,642 DEBUG TRAIN Batch 26/5400 loss 4.815353 loss_att 8.295200 loss_ctc 9.363696 loss_rnnt 3.443470 hw_loss 0.130251 lr 0.00033542 rank 3
2023-02-25 09:44:43,642 DEBUG TRAIN Batch 26/5400 loss 7.362225 loss_att 10.738543 loss_ctc 9.988614 loss_rnnt 6.264842 hw_loss 0.134876 lr 0.00033540 rank 4
2023-02-25 09:44:43,645 DEBUG TRAIN Batch 26/5400 loss 10.868750 loss_att 15.383194 loss_ctc 15.057064 loss_rnnt 9.317626 hw_loss 0.168362 lr 0.00033545 rank 1
2023-02-25 09:44:43,649 DEBUG TRAIN Batch 26/5400 loss 8.273216 loss_att 10.784028 loss_ctc 12.184370 loss_rnnt 7.170846 hw_loss 0.147600 lr 0.00033540 rank 5
2023-02-25 09:44:43,651 DEBUG TRAIN Batch 26/5400 loss 8.814404 loss_att 9.726534 loss_ctc 9.293236 loss_rnnt 8.493626 hw_loss 0.139705 lr 0.00033542 rank 2
2023-02-25 09:45:53,589 DEBUG TRAIN Batch 26/5500 loss 9.720860 loss_att 13.025591 loss_ctc 12.858788 loss_rnnt 8.564829 hw_loss 0.143806 lr 0.00033533 rank 4
2023-02-25 09:45:53,591 DEBUG TRAIN Batch 26/5500 loss 11.182061 loss_att 14.392741 loss_ctc 17.942272 loss_rnnt 9.553390 hw_loss 0.159700 lr 0.00033532 rank 0
2023-02-25 09:45:53,592 DEBUG TRAIN Batch 26/5500 loss 2.927231 loss_att 5.488151 loss_ctc 5.767784 loss_rnnt 1.936594 hw_loss 0.186961 lr 0.00033535 rank 3
2023-02-25 09:45:53,592 DEBUG TRAIN Batch 26/5500 loss 12.645398 loss_att 15.643695 loss_ctc 17.163340 loss_rnnt 11.336649 hw_loss 0.200055 lr 0.00033535 rank 2
2023-02-25 09:45:53,594 DEBUG TRAIN Batch 26/5500 loss 2.965895 loss_att 4.277086 loss_ctc 2.893778 loss_rnnt 2.564067 hw_loss 0.279760 lr 0.00033530 rank 7
2023-02-25 09:45:53,594 DEBUG TRAIN Batch 26/5500 loss 14.006960 loss_att 16.875782 loss_ctc 15.530959 loss_rnnt 13.145083 hw_loss 0.159210 lr 0.00033533 rank 6
2023-02-25 09:45:53,597 DEBUG TRAIN Batch 26/5500 loss 23.413397 loss_att 26.032879 loss_ctc 32.989296 loss_rnnt 21.484154 hw_loss 0.241051 lr 0.00033532 rank 5
2023-02-25 09:45:53,598 DEBUG TRAIN Batch 26/5500 loss 4.825150 loss_att 8.345193 loss_ctc 7.892766 loss_rnnt 3.646873 hw_loss 0.122350 lr 0.00033538 rank 1
2023-02-25 09:47:04,239 DEBUG TRAIN Batch 26/5600 loss 7.049596 loss_att 10.271747 loss_ctc 11.585058 loss_rnnt 5.719068 hw_loss 0.152568 lr 0.00033526 rank 6
2023-02-25 09:47:04,244 DEBUG TRAIN Batch 26/5600 loss 6.665398 loss_att 8.412666 loss_ctc 9.014853 loss_rnnt 5.857135 hw_loss 0.272903 lr 0.00033524 rank 5
2023-02-25 09:47:04,246 DEBUG TRAIN Batch 26/5600 loss 9.642129 loss_att 11.223784 loss_ctc 13.206099 loss_rnnt 8.777575 hw_loss 0.136926 lr 0.00033522 rank 7
2023-02-25 09:47:04,247 DEBUG TRAIN Batch 26/5600 loss 9.130056 loss_att 9.885867 loss_ctc 12.089735 loss_rnnt 8.454657 hw_loss 0.243025 lr 0.00033527 rank 3
2023-02-25 09:47:04,249 DEBUG TRAIN Batch 26/5600 loss 10.133165 loss_att 12.529707 loss_ctc 18.702091 loss_rnnt 8.445745 hw_loss 0.122980 lr 0.00033527 rank 2
2023-02-25 09:47:04,250 DEBUG TRAIN Batch 26/5600 loss 3.026004 loss_att 5.185703 loss_ctc 4.937899 loss_rnnt 2.295503 hw_loss 0.081828 lr 0.00033525 rank 4
2023-02-25 09:47:04,268 DEBUG TRAIN Batch 26/5600 loss 11.270589 loss_att 13.622003 loss_ctc 17.037710 loss_rnnt 9.904478 hw_loss 0.237896 lr 0.00033525 rank 0
2023-02-25 09:47:04,272 DEBUG TRAIN Batch 26/5600 loss 8.363815 loss_att 12.552631 loss_ctc 13.788034 loss_rnnt 6.689734 hw_loss 0.212043 lr 0.00033530 rank 1
2023-02-25 09:48:17,128 DEBUG TRAIN Batch 26/5700 loss 16.096073 loss_att 17.333185 loss_ctc 19.929535 loss_rnnt 15.225481 hw_loss 0.210078 lr 0.00033518 rank 6
2023-02-25 09:48:17,130 DEBUG TRAIN Batch 26/5700 loss 7.522790 loss_att 7.050817 loss_ctc 11.606319 loss_rnnt 6.902730 hw_loss 0.318719 lr 0.00033517 rank 0
2023-02-25 09:48:17,130 DEBUG TRAIN Batch 26/5700 loss 9.583293 loss_att 10.826055 loss_ctc 11.534122 loss_rnnt 8.950686 hw_loss 0.232393 lr 0.00033515 rank 7
2023-02-25 09:48:17,131 DEBUG TRAIN Batch 26/5700 loss 11.925160 loss_att 13.317286 loss_ctc 16.342533 loss_rnnt 10.948859 hw_loss 0.204175 lr 0.00033520 rank 2
2023-02-25 09:48:17,131 DEBUG TRAIN Batch 26/5700 loss 14.617993 loss_att 14.528250 loss_ctc 19.382515 loss_rnnt 13.852878 hw_loss 0.277116 lr 0.00033517 rank 5
2023-02-25 09:48:17,132 DEBUG TRAIN Batch 26/5700 loss 13.439063 loss_att 15.699759 loss_ctc 17.363689 loss_rnnt 12.376162 hw_loss 0.164020 lr 0.00033523 rank 1
2023-02-25 09:48:17,134 DEBUG TRAIN Batch 26/5700 loss 3.906639 loss_att 5.076586 loss_ctc 5.039968 loss_rnnt 3.422480 hw_loss 0.185734 lr 0.00033518 rank 4
2023-02-25 09:48:17,134 DEBUG TRAIN Batch 26/5700 loss 1.585348 loss_att 4.739029 loss_ctc 2.361785 loss_rnnt 0.774496 hw_loss 0.143609 lr 0.00033520 rank 3
2023-02-25 09:49:27,953 DEBUG TRAIN Batch 26/5800 loss 6.568373 loss_att 9.005135 loss_ctc 8.840616 loss_rnnt 5.655567 hw_loss 0.229666 lr 0.00033510 rank 4
2023-02-25 09:49:27,955 DEBUG TRAIN Batch 26/5800 loss 2.361965 loss_att 5.590982 loss_ctc 2.070307 loss_rnnt 1.688491 hw_loss 0.124796 lr 0.00033510 rank 6
2023-02-25 09:49:27,957 DEBUG TRAIN Batch 26/5800 loss 7.724491 loss_att 10.330713 loss_ctc 13.678882 loss_rnnt 6.292001 hw_loss 0.219985 lr 0.00033510 rank 0
2023-02-25 09:49:27,958 DEBUG TRAIN Batch 26/5800 loss 8.902102 loss_att 9.212852 loss_ctc 12.168025 loss_rnnt 8.269180 hw_loss 0.253715 lr 0.00033512 rank 3
2023-02-25 09:49:27,960 DEBUG TRAIN Batch 26/5800 loss 4.793521 loss_att 4.953841 loss_ctc 6.762727 loss_rnnt 4.386372 hw_loss 0.210984 lr 0.00033515 rank 1
2023-02-25 09:49:27,960 DEBUG TRAIN Batch 26/5800 loss 7.813213 loss_att 8.087574 loss_ctc 11.431849 loss_rnnt 7.141293 hw_loss 0.252305 lr 0.00033507 rank 7
2023-02-25 09:49:27,961 DEBUG TRAIN Batch 26/5800 loss 6.569792 loss_att 10.701142 loss_ctc 10.090216 loss_rnnt 5.163629 hw_loss 0.207194 lr 0.00033509 rank 5
2023-02-25 09:49:27,964 DEBUG TRAIN Batch 26/5800 loss 14.359958 loss_att 16.275166 loss_ctc 20.078825 loss_rnnt 13.042315 hw_loss 0.322659 lr 0.00033512 rank 2
2023-02-25 09:50:37,895 DEBUG TRAIN Batch 26/5900 loss 8.946833 loss_att 11.921792 loss_ctc 14.496748 loss_rnnt 7.515451 hw_loss 0.180749 lr 0.00033502 rank 0
2023-02-25 09:50:37,898 DEBUG TRAIN Batch 26/5900 loss 2.639303 loss_att 5.167512 loss_ctc 3.397121 loss_rnnt 1.926585 hw_loss 0.198814 lr 0.00033505 rank 2
2023-02-25 09:50:37,898 DEBUG TRAIN Batch 26/5900 loss 5.048680 loss_att 7.737448 loss_ctc 7.063945 loss_rnnt 4.090386 hw_loss 0.284697 lr 0.00033505 rank 3
2023-02-25 09:50:37,899 DEBUG TRAIN Batch 26/5900 loss 7.688702 loss_att 11.503883 loss_ctc 10.415806 loss_rnnt 6.474029 hw_loss 0.165043 lr 0.00033503 rank 6
2023-02-25 09:50:37,901 DEBUG TRAIN Batch 26/5900 loss 6.427047 loss_att 11.290696 loss_ctc 8.377700 loss_rnnt 5.056854 hw_loss 0.257581 lr 0.00033508 rank 1
2023-02-25 09:50:37,903 DEBUG TRAIN Batch 26/5900 loss 6.379037 loss_att 11.340475 loss_ctc 13.188057 loss_rnnt 4.350477 hw_loss 0.240758 lr 0.00033500 rank 7
2023-02-25 09:50:37,904 DEBUG TRAIN Batch 26/5900 loss 14.880934 loss_att 23.283724 loss_ctc 21.017042 loss_rnnt 12.258537 hw_loss 0.231920 lr 0.00033502 rank 5
2023-02-25 09:50:37,947 DEBUG TRAIN Batch 26/5900 loss 4.574203 loss_att 6.022018 loss_ctc 7.850597 loss_rnnt 3.762112 hw_loss 0.160643 lr 0.00033503 rank 4
2023-02-25 09:51:50,830 DEBUG TRAIN Batch 26/6000 loss 7.986175 loss_att 12.105793 loss_ctc 13.048925 loss_rnnt 6.415216 hw_loss 0.135004 lr 0.00033495 rank 4
2023-02-25 09:51:50,836 DEBUG TRAIN Batch 26/6000 loss 10.070667 loss_att 11.324975 loss_ctc 14.184556 loss_rnnt 9.186472 hw_loss 0.159027 lr 0.00033495 rank 0
2023-02-25 09:51:50,836 DEBUG TRAIN Batch 26/6000 loss 6.849312 loss_att 11.106739 loss_ctc 11.421013 loss_rnnt 5.273570 hw_loss 0.215056 lr 0.00033495 rank 6
2023-02-25 09:51:50,836 DEBUG TRAIN Batch 26/6000 loss 17.006266 loss_att 17.624489 loss_ctc 19.388540 loss_rnnt 16.409712 hw_loss 0.291138 lr 0.00033494 rank 5
2023-02-25 09:51:50,840 DEBUG TRAIN Batch 26/6000 loss 4.054212 loss_att 4.900040 loss_ctc 4.148229 loss_rnnt 3.806756 hw_loss 0.123289 lr 0.00033500 rank 1
2023-02-25 09:51:50,842 DEBUG TRAIN Batch 26/6000 loss 14.751585 loss_att 20.366156 loss_ctc 21.831356 loss_rnnt 12.606110 hw_loss 0.147356 lr 0.00033497 rank 2
2023-02-25 09:51:50,846 DEBUG TRAIN Batch 26/6000 loss 4.723637 loss_att 9.117688 loss_ctc 7.900760 loss_rnnt 3.343750 hw_loss 0.145237 lr 0.00033492 rank 7
2023-02-25 09:51:50,891 DEBUG TRAIN Batch 26/6000 loss 8.710504 loss_att 13.759271 loss_ctc 12.686745 loss_rnnt 7.060200 hw_loss 0.206971 lr 0.00033497 rank 3
2023-02-25 09:53:02,969 DEBUG TRAIN Batch 26/6100 loss 4.419525 loss_att 7.851766 loss_ctc 7.401197 loss_rnnt 3.182951 hw_loss 0.286068 lr 0.00033490 rank 3
2023-02-25 09:53:02,971 DEBUG TRAIN Batch 26/6100 loss 6.920697 loss_att 10.451591 loss_ctc 9.950016 loss_rnnt 5.708201 hw_loss 0.192015 lr 0.00033488 rank 6
2023-02-25 09:53:02,972 DEBUG TRAIN Batch 26/6100 loss 10.764961 loss_att 14.388878 loss_ctc 22.680567 loss_rnnt 8.272434 hw_loss 0.335616 lr 0.00033487 rank 5
2023-02-25 09:53:02,971 DEBUG TRAIN Batch 26/6100 loss 6.616275 loss_att 7.746275 loss_ctc 8.656745 loss_rnnt 6.024230 hw_loss 0.176218 lr 0.00033485 rank 7
2023-02-25 09:53:02,972 DEBUG TRAIN Batch 26/6100 loss 18.073452 loss_att 21.623779 loss_ctc 21.633297 loss_rnnt 16.811295 hw_loss 0.145211 lr 0.00033490 rank 2
2023-02-25 09:53:02,972 DEBUG TRAIN Batch 26/6100 loss 8.538129 loss_att 12.416920 loss_ctc 12.751757 loss_rnnt 7.076042 hw_loss 0.233460 lr 0.00033487 rank 0
2023-02-25 09:53:02,973 DEBUG TRAIN Batch 26/6100 loss 7.273749 loss_att 11.858191 loss_ctc 12.925088 loss_rnnt 5.476105 hw_loss 0.238582 lr 0.00033488 rank 4
2023-02-25 09:53:02,975 DEBUG TRAIN Batch 26/6100 loss 9.254525 loss_att 14.886131 loss_ctc 12.973793 loss_rnnt 7.467418 hw_loss 0.309156 lr 0.00033492 rank 1
2023-02-25 09:54:13,061 DEBUG TRAIN Batch 26/6200 loss 9.635670 loss_att 10.675011 loss_ctc 11.038372 loss_rnnt 9.095397 hw_loss 0.272584 lr 0.00033480 rank 6
2023-02-25 09:54:13,062 DEBUG TRAIN Batch 26/6200 loss 10.791446 loss_att 12.798330 loss_ctc 15.277086 loss_rnnt 9.724396 hw_loss 0.126725 lr 0.00033477 rank 7
2023-02-25 09:54:13,064 DEBUG TRAIN Batch 26/6200 loss 10.440031 loss_att 13.168406 loss_ctc 17.203979 loss_rnnt 8.898692 hw_loss 0.175885 lr 0.00033480 rank 0
2023-02-25 09:54:13,065 DEBUG TRAIN Batch 26/6200 loss 4.661036 loss_att 10.026484 loss_ctc 7.678955 loss_rnnt 3.049955 hw_loss 0.254253 lr 0.00033485 rank 1
2023-02-25 09:54:13,066 DEBUG TRAIN Batch 26/6200 loss 6.470794 loss_att 9.376420 loss_ctc 10.364618 loss_rnnt 5.265184 hw_loss 0.197453 lr 0.00033482 rank 3
2023-02-25 09:54:13,066 DEBUG TRAIN Batch 26/6200 loss 12.724940 loss_att 17.793434 loss_ctc 17.839474 loss_rnnt 10.903753 hw_loss 0.235411 lr 0.00033482 rank 2
2023-02-25 09:54:13,071 DEBUG TRAIN Batch 26/6200 loss 9.329664 loss_att 12.949194 loss_ctc 11.764584 loss_rnnt 8.158618 hw_loss 0.229658 lr 0.00033479 rank 5
2023-02-25 09:54:13,074 DEBUG TRAIN Batch 26/6200 loss 5.260839 loss_att 9.063942 loss_ctc 9.137057 loss_rnnt 3.875446 hw_loss 0.202392 lr 0.00033480 rank 4
2023-02-25 09:55:23,163 DEBUG TRAIN Batch 26/6300 loss 6.908004 loss_att 9.423250 loss_ctc 10.031661 loss_rnnt 5.820740 hw_loss 0.314488 lr 0.00033472 rank 5
2023-02-25 09:55:23,163 DEBUG TRAIN Batch 26/6300 loss 5.956659 loss_att 8.031057 loss_ctc 8.011349 loss_rnnt 5.099280 hw_loss 0.316014 lr 0.00033472 rank 0
2023-02-25 09:55:23,166 DEBUG TRAIN Batch 26/6300 loss 2.796699 loss_att 6.788634 loss_ctc 5.872291 loss_rnnt 1.532958 hw_loss 0.103639 lr 0.00033475 rank 3
2023-02-25 09:55:23,166 DEBUG TRAIN Batch 26/6300 loss 8.704283 loss_att 10.114464 loss_ctc 14.172150 loss_rnnt 7.630038 hw_loss 0.118425 lr 0.00033477 rank 1
2023-02-25 09:55:23,167 DEBUG TRAIN Batch 26/6300 loss 6.184228 loss_att 8.782093 loss_ctc 10.798110 loss_rnnt 4.970243 hw_loss 0.148552 lr 0.00033469 rank 7
2023-02-25 09:55:23,167 DEBUG TRAIN Batch 26/6300 loss 9.187078 loss_att 12.336600 loss_ctc 18.838820 loss_rnnt 7.186741 hw_loss 0.156626 lr 0.00033472 rank 4
2023-02-25 09:55:23,168 DEBUG TRAIN Batch 26/6300 loss 5.407009 loss_att 9.115000 loss_ctc 11.726103 loss_rnnt 3.711744 hw_loss 0.208352 lr 0.00033475 rank 2
2023-02-25 09:55:23,190 DEBUG TRAIN Batch 26/6300 loss 7.239892 loss_att 8.408933 loss_ctc 12.431088 loss_rnnt 6.148242 hw_loss 0.310655 lr 0.00033473 rank 6
2023-02-25 09:56:37,174 DEBUG TRAIN Batch 26/6400 loss 6.900208 loss_att 10.423897 loss_ctc 11.291881 loss_rnnt 5.522136 hw_loss 0.164585 lr 0.00033470 rank 1
2023-02-25 09:56:37,176 DEBUG TRAIN Batch 26/6400 loss 4.393370 loss_att 7.297407 loss_ctc 7.250727 loss_rnnt 3.372346 hw_loss 0.111067 lr 0.00033465 rank 0
2023-02-25 09:56:37,176 DEBUG TRAIN Batch 26/6400 loss 5.808763 loss_att 7.588972 loss_ctc 8.450947 loss_rnnt 4.947593 hw_loss 0.286570 lr 0.00033465 rank 4
2023-02-25 09:56:37,176 DEBUG TRAIN Batch 26/6400 loss 4.706879 loss_att 5.141812 loss_ctc 5.818894 loss_rnnt 4.341198 hw_loss 0.244546 lr 0.00033467 rank 2
2023-02-25 09:56:37,179 DEBUG TRAIN Batch 26/6400 loss 4.333600 loss_att 5.881042 loss_ctc 6.181428 loss_rnnt 3.647559 hw_loss 0.244079 lr 0.00033462 rank 7
2023-02-25 09:56:37,184 DEBUG TRAIN Batch 26/6400 loss 8.234473 loss_att 10.379066 loss_ctc 12.179061 loss_rnnt 7.140134 hw_loss 0.261516 lr 0.00033464 rank 5
2023-02-25 09:56:37,186 DEBUG TRAIN Batch 26/6400 loss 11.821724 loss_att 14.732412 loss_ctc 22.594906 loss_rnnt 9.658000 hw_loss 0.272179 lr 0.00033467 rank 3
2023-02-25 09:56:37,223 DEBUG TRAIN Batch 26/6400 loss 5.482379 loss_att 11.072350 loss_ctc 8.160692 loss_rnnt 3.936180 hw_loss 0.133306 lr 0.00033465 rank 6
2023-02-25 09:57:47,857 DEBUG TRAIN Batch 26/6500 loss 10.337779 loss_att 10.760243 loss_ctc 10.714825 loss_rnnt 10.128734 hw_loss 0.139275 lr 0.00033457 rank 0
2023-02-25 09:57:47,861 DEBUG TRAIN Batch 26/6500 loss 9.492248 loss_att 10.771193 loss_ctc 11.246857 loss_rnnt 8.886879 hw_loss 0.216809 lr 0.00033460 rank 3
2023-02-25 09:57:47,861 DEBUG TRAIN Batch 26/6500 loss 8.662179 loss_att 11.659800 loss_ctc 14.671496 loss_rnnt 7.173743 hw_loss 0.164381 lr 0.00033462 rank 1
2023-02-25 09:57:47,862 DEBUG TRAIN Batch 26/6500 loss 8.390937 loss_att 8.749670 loss_ctc 13.367480 loss_rnnt 7.552197 hw_loss 0.193976 lr 0.00033460 rank 2
2023-02-25 09:57:47,867 DEBUG TRAIN Batch 26/6500 loss 6.286437 loss_att 8.710506 loss_ctc 7.077265 loss_rnnt 5.577623 hw_loss 0.222291 lr 0.00033458 rank 6
2023-02-25 09:57:47,867 DEBUG TRAIN Batch 26/6500 loss 2.066872 loss_att 4.809887 loss_ctc 2.735034 loss_rnnt 1.318460 hw_loss 0.207602 lr 0.00033455 rank 7
2023-02-25 09:57:47,869 DEBUG TRAIN Batch 26/6500 loss 3.685604 loss_att 7.453215 loss_ctc 5.428913 loss_rnnt 2.613267 hw_loss 0.161952 lr 0.00033458 rank 4
2023-02-25 09:57:47,870 DEBUG TRAIN Batch 26/6500 loss 4.450989 loss_att 8.021044 loss_ctc 5.628230 loss_rnnt 3.470486 hw_loss 0.205362 lr 0.00033457 rank 5
2023-02-25 09:58:58,105 DEBUG TRAIN Batch 26/6600 loss 3.861385 loss_att 8.196751 loss_ctc 5.133188 loss_rnnt 2.758534 hw_loss 0.124132 lr 0.00033452 rank 3
2023-02-25 09:58:58,106 DEBUG TRAIN Batch 26/6600 loss 2.447853 loss_att 5.638757 loss_ctc 5.619686 loss_rnnt 1.300652 hw_loss 0.161455 lr 0.00033450 rank 0
2023-02-25 09:58:58,107 DEBUG TRAIN Batch 26/6600 loss 12.821589 loss_att 15.356177 loss_ctc 19.043812 loss_rnnt 11.387909 hw_loss 0.182122 lr 0.00033452 rank 2
2023-02-25 09:58:58,108 DEBUG TRAIN Batch 26/6600 loss 5.935013 loss_att 7.940846 loss_ctc 6.975763 loss_rnnt 5.309726 hw_loss 0.160040 lr 0.00033455 rank 1
2023-02-25 09:58:58,112 DEBUG TRAIN Batch 26/6600 loss 9.121518 loss_att 14.673849 loss_ctc 17.486536 loss_rnnt 6.794574 hw_loss 0.189641 lr 0.00033447 rank 7
2023-02-25 09:58:58,113 DEBUG TRAIN Batch 26/6600 loss 6.425401 loss_att 8.824480 loss_ctc 9.118639 loss_rnnt 5.499792 hw_loss 0.162554 lr 0.00033450 rank 6
2023-02-25 09:58:58,112 DEBUG TRAIN Batch 26/6600 loss 13.143140 loss_att 15.408142 loss_ctc 18.140486 loss_rnnt 11.950389 hw_loss 0.137696 lr 0.00033450 rank 4
2023-02-25 09:58:58,122 DEBUG TRAIN Batch 26/6600 loss 8.238738 loss_att 10.452335 loss_ctc 13.167171 loss_rnnt 7.038404 hw_loss 0.188421 lr 0.00033449 rank 5
2023-02-25 10:00:09,685 DEBUG TRAIN Batch 26/6700 loss 3.336529 loss_att 5.179795 loss_ctc 5.203862 loss_rnnt 2.608670 hw_loss 0.206678 lr 0.00033442 rank 5
2023-02-25 10:00:09,693 DEBUG TRAIN Batch 26/6700 loss 5.162032 loss_att 6.851106 loss_ctc 7.289764 loss_rnnt 4.437959 hw_loss 0.192300 lr 0.00033445 rank 3
2023-02-25 10:00:09,696 DEBUG TRAIN Batch 26/6700 loss 9.064817 loss_att 12.967197 loss_ctc 13.262167 loss_rnnt 7.649409 hw_loss 0.141161 lr 0.00033443 rank 4
2023-02-25 10:00:09,696 DEBUG TRAIN Batch 26/6700 loss 10.809459 loss_att 12.016415 loss_ctc 14.713875 loss_rnnt 9.987099 hw_loss 0.113211 lr 0.00033442 rank 0
2023-02-25 10:00:09,697 DEBUG TRAIN Batch 26/6700 loss 11.904139 loss_att 12.870191 loss_ctc 13.394154 loss_rnnt 11.434208 hw_loss 0.146346 lr 0.00033445 rank 2
2023-02-25 10:00:09,701 DEBUG TRAIN Batch 26/6700 loss 6.817972 loss_att 11.851931 loss_ctc 10.844084 loss_rnnt 5.196330 hw_loss 0.146315 lr 0.00033447 rank 1
2023-02-25 10:00:09,701 DEBUG TRAIN Batch 26/6700 loss 2.775740 loss_att 5.162931 loss_ctc 4.101181 loss_rnnt 2.006780 hw_loss 0.215242 lr 0.00033440 rank 7
2023-02-25 10:00:09,711 DEBUG TRAIN Batch 26/6700 loss 7.466484 loss_att 8.586839 loss_ctc 10.346750 loss_rnnt 6.753070 hw_loss 0.197452 lr 0.00033443 rank 6
2023-02-25 10:01:21,125 DEBUG TRAIN Batch 26/6800 loss 8.718957 loss_att 11.612811 loss_ctc 17.791742 loss_rnnt 6.825871 hw_loss 0.196143 lr 0.00033435 rank 0
2023-02-25 10:01:21,128 DEBUG TRAIN Batch 26/6800 loss 4.392091 loss_att 7.320681 loss_ctc 5.600859 loss_rnnt 3.528380 hw_loss 0.219045 lr 0.00033432 rank 7
2023-02-25 10:01:21,129 DEBUG TRAIN Batch 26/6800 loss 10.252678 loss_att 12.450144 loss_ctc 14.437355 loss_rnnt 9.108679 hw_loss 0.274781 lr 0.00033437 rank 3
2023-02-25 10:01:21,129 DEBUG TRAIN Batch 26/6800 loss 3.685136 loss_att 6.416806 loss_ctc 4.807070 loss_rnnt 2.906405 hw_loss 0.155261 lr 0.00033437 rank 2
2023-02-25 10:01:21,131 DEBUG TRAIN Batch 26/6800 loss 12.491824 loss_att 13.313627 loss_ctc 14.164279 loss_rnnt 12.032295 hw_loss 0.135327 lr 0.00033435 rank 6
2023-02-25 10:01:21,130 DEBUG TRAIN Batch 26/6800 loss 9.470656 loss_att 11.435891 loss_ctc 11.985009 loss_rnnt 8.637105 hw_loss 0.197355 lr 0.00033440 rank 1
2023-02-25 10:01:21,134 DEBUG TRAIN Batch 26/6800 loss 7.986979 loss_att 10.392162 loss_ctc 10.674179 loss_rnnt 7.051482 hw_loss 0.180313 lr 0.00033434 rank 5
2023-02-25 10:01:21,135 DEBUG TRAIN Batch 26/6800 loss 2.902308 loss_att 5.947399 loss_ctc 6.824975 loss_rnnt 1.689637 hw_loss 0.151183 lr 0.00033435 rank 4
2023-02-25 10:02:32,114 DEBUG TRAIN Batch 26/6900 loss 13.377862 loss_att 16.394600 loss_ctc 18.588346 loss_rnnt 11.979332 hw_loss 0.188346 lr 0.00033427 rank 0
2023-02-25 10:02:32,118 DEBUG TRAIN Batch 26/6900 loss 16.611124 loss_att 17.503925 loss_ctc 19.475418 loss_rnnt 15.918781 hw_loss 0.247267 lr 0.00033425 rank 7
2023-02-25 10:02:32,120 DEBUG TRAIN Batch 26/6900 loss 10.447373 loss_att 11.277547 loss_ctc 13.897154 loss_rnnt 9.690606 hw_loss 0.245178 lr 0.00033430 rank 2
2023-02-25 10:02:32,121 DEBUG TRAIN Batch 26/6900 loss 8.485423 loss_att 9.728108 loss_ctc 13.000241 loss_rnnt 7.498561 hw_loss 0.255656 lr 0.00033430 rank 3
2023-02-25 10:02:32,121 DEBUG TRAIN Batch 26/6900 loss 8.318606 loss_att 8.968500 loss_ctc 11.337370 loss_rnnt 7.690043 hw_loss 0.180154 lr 0.00033433 rank 1
2023-02-25 10:02:32,123 DEBUG TRAIN Batch 26/6900 loss 10.603847 loss_att 13.444055 loss_ctc 14.311785 loss_rnnt 9.446095 hw_loss 0.178721 lr 0.00033428 rank 6
2023-02-25 10:02:32,124 DEBUG TRAIN Batch 26/6900 loss 6.583225 loss_att 9.104813 loss_ctc 9.104910 loss_rnnt 5.632808 hw_loss 0.206017 lr 0.00033427 rank 5
2023-02-25 10:02:32,171 DEBUG TRAIN Batch 26/6900 loss 7.671668 loss_att 11.121880 loss_ctc 9.373915 loss_rnnt 6.647202 hw_loss 0.201485 lr 0.00033428 rank 4
2023-02-25 10:03:42,973 DEBUG TRAIN Batch 26/7000 loss 2.604596 loss_att 5.023574 loss_ctc 4.580163 loss_rnnt 1.745841 hw_loss 0.209157 lr 0.00033420 rank 0
2023-02-25 10:03:42,975 DEBUG TRAIN Batch 26/7000 loss 6.081673 loss_att 8.393143 loss_ctc 10.420217 loss_rnnt 4.930186 hw_loss 0.207602 lr 0.00033417 rank 7
2023-02-25 10:03:42,977 DEBUG TRAIN Batch 26/7000 loss 5.622699 loss_att 6.608371 loss_ctc 7.758129 loss_rnnt 5.004620 hw_loss 0.255414 lr 0.00033420 rank 6
2023-02-25 10:03:42,977 DEBUG TRAIN Batch 26/7000 loss 5.543235 loss_att 7.128533 loss_ctc 8.060866 loss_rnnt 4.798163 hw_loss 0.173113 lr 0.00033425 rank 1
2023-02-25 10:03:42,978 DEBUG TRAIN Batch 26/7000 loss 9.933024 loss_att 15.375450 loss_ctc 19.135538 loss_rnnt 7.429498 hw_loss 0.352573 lr 0.00033422 rank 3
2023-02-25 10:03:42,981 DEBUG TRAIN Batch 26/7000 loss 5.934046 loss_att 7.627262 loss_ctc 8.860303 loss_rnnt 5.086343 hw_loss 0.222924 lr 0.00033419 rank 5
2023-02-25 10:03:42,987 DEBUG TRAIN Batch 26/7000 loss 12.278948 loss_att 14.319895 loss_ctc 16.467070 loss_rnnt 11.222735 hw_loss 0.168014 lr 0.00033422 rank 2
2023-02-25 10:03:43,038 DEBUG TRAIN Batch 26/7000 loss 4.829208 loss_att 6.986222 loss_ctc 5.270145 loss_rnnt 4.220790 hw_loss 0.221670 lr 0.00033420 rank 4
2023-02-25 10:04:57,154 DEBUG TRAIN Batch 26/7100 loss 4.878457 loss_att 6.271583 loss_ctc 7.548065 loss_rnnt 4.165306 hw_loss 0.147333 lr 0.00033412 rank 5
2023-02-25 10:04:57,156 DEBUG TRAIN Batch 26/7100 loss 6.732955 loss_att 8.658235 loss_ctc 8.432340 loss_rnnt 5.993488 hw_loss 0.239674 lr 0.00033412 rank 0
2023-02-25 10:04:57,155 DEBUG TRAIN Batch 26/7100 loss 4.407995 loss_att 7.223934 loss_ctc 8.581320 loss_rnnt 3.173851 hw_loss 0.214712 lr 0.00033410 rank 7
2023-02-25 10:04:57,163 DEBUG TRAIN Batch 26/7100 loss 8.099755 loss_att 12.820126 loss_ctc 12.403220 loss_rnnt 6.427610 hw_loss 0.289267 lr 0.00033418 rank 1
2023-02-25 10:04:57,165 DEBUG TRAIN Batch 26/7100 loss 8.003642 loss_att 11.427307 loss_ctc 12.105014 loss_rnnt 6.639843 hw_loss 0.247907 lr 0.00033413 rank 6
2023-02-25 10:04:57,170 DEBUG TRAIN Batch 26/7100 loss 3.181548 loss_att 8.188415 loss_ctc 6.556623 loss_rnnt 1.619144 hw_loss 0.208162 lr 0.00033415 rank 3
2023-02-25 10:04:57,178 DEBUG TRAIN Batch 26/7100 loss 9.582738 loss_att 11.097615 loss_ctc 14.842792 loss_rnnt 8.512888 hw_loss 0.122877 lr 0.00033415 rank 2
2023-02-25 10:04:57,200 DEBUG TRAIN Batch 26/7100 loss 10.092634 loss_att 13.713156 loss_ctc 13.500450 loss_rnnt 8.793797 hw_loss 0.225670 lr 0.00033413 rank 4
2023-02-25 10:06:07,862 DEBUG TRAIN Batch 26/7200 loss 6.076308 loss_att 8.021276 loss_ctc 8.298768 loss_rnnt 5.354047 hw_loss 0.069259 lr 0.00033410 rank 1
2023-02-25 10:06:07,864 DEBUG TRAIN Batch 26/7200 loss 6.192973 loss_att 7.285263 loss_ctc 7.034655 loss_rnnt 5.812932 hw_loss 0.092546 lr 0.00033405 rank 0
2023-02-25 10:06:07,867 DEBUG TRAIN Batch 26/7200 loss 4.617254 loss_att 8.036036 loss_ctc 8.136932 loss_rnnt 3.378686 hw_loss 0.160354 lr 0.00033406 rank 6
2023-02-25 10:06:07,868 DEBUG TRAIN Batch 26/7200 loss 20.788559 loss_att 24.493649 loss_ctc 30.242447 loss_rnnt 18.685352 hw_loss 0.190631 lr 0.00033407 rank 3
2023-02-25 10:06:07,869 DEBUG TRAIN Batch 26/7200 loss 4.991296 loss_att 9.047064 loss_ctc 12.062304 loss_rnnt 3.143126 hw_loss 0.176654 lr 0.00033405 rank 4
2023-02-25 10:06:07,872 DEBUG TRAIN Batch 26/7200 loss 11.515383 loss_att 14.350134 loss_ctc 18.055847 loss_rnnt 9.963878 hw_loss 0.210925 lr 0.00033402 rank 7
2023-02-25 10:06:07,873 DEBUG TRAIN Batch 26/7200 loss 8.933583 loss_att 12.656756 loss_ctc 12.677881 loss_rnnt 7.556413 hw_loss 0.249929 lr 0.00033407 rank 2
2023-02-25 10:06:07,874 DEBUG TRAIN Batch 26/7200 loss 7.853854 loss_att 10.859574 loss_ctc 16.190252 loss_rnnt 6.034153 hw_loss 0.200694 lr 0.00033405 rank 5
2023-02-25 10:07:18,605 DEBUG TRAIN Batch 26/7300 loss 14.520435 loss_att 17.051434 loss_ctc 21.203846 loss_rnnt 13.004065 hw_loss 0.223219 lr 0.00033397 rank 5
2023-02-25 10:07:18,621 DEBUG TRAIN Batch 26/7300 loss 4.654122 loss_att 7.992965 loss_ctc 7.200030 loss_rnnt 3.504134 hw_loss 0.267683 lr 0.00033398 rank 0
2023-02-25 10:07:18,622 DEBUG TRAIN Batch 26/7300 loss 12.706515 loss_att 14.040710 loss_ctc 18.605469 loss_rnnt 11.584909 hw_loss 0.127948 lr 0.00033400 rank 3
2023-02-25 10:07:18,623 DEBUG TRAIN Batch 26/7300 loss 11.379136 loss_att 13.112941 loss_ctc 14.844075 loss_rnnt 10.442809 hw_loss 0.239203 lr 0.00033398 rank 4
2023-02-25 10:07:18,626 DEBUG TRAIN Batch 26/7300 loss 0.726013 loss_att 2.858139 loss_ctc 0.919882 loss_rnnt 0.158895 hw_loss 0.215333 lr 0.00033398 rank 6
2023-02-25 10:07:18,627 DEBUG TRAIN Batch 26/7300 loss 8.713995 loss_att 14.705721 loss_ctc 12.883441 loss_rnnt 6.887585 hw_loss 0.135259 lr 0.00033400 rank 2
2023-02-25 10:07:18,656 DEBUG TRAIN Batch 26/7300 loss 8.631242 loss_att 10.282432 loss_ctc 11.308009 loss_rnnt 7.817868 hw_loss 0.236687 lr 0.00033403 rank 1
2023-02-25 10:07:18,663 DEBUG TRAIN Batch 26/7300 loss 2.167558 loss_att 6.012727 loss_ctc 4.683912 loss_rnnt 0.948572 hw_loss 0.214572 lr 0.00033395 rank 7
2023-02-25 10:08:29,857 DEBUG TRAIN Batch 26/7400 loss 10.597498 loss_att 13.306658 loss_ctc 20.131256 loss_rnnt 8.638393 hw_loss 0.273947 lr 0.00033390 rank 0
2023-02-25 10:08:29,862 DEBUG TRAIN Batch 26/7400 loss 4.789085 loss_att 8.074215 loss_ctc 6.173304 loss_rnnt 3.767859 hw_loss 0.336822 lr 0.00033391 rank 6
2023-02-25 10:08:29,868 DEBUG TRAIN Batch 26/7400 loss 5.114442 loss_att 7.890421 loss_ctc 7.197865 loss_rnnt 4.164440 hw_loss 0.219407 lr 0.00033392 rank 2
2023-02-25 10:08:29,869 DEBUG TRAIN Batch 26/7400 loss 10.977914 loss_att 14.709269 loss_ctc 19.233746 loss_rnnt 9.021679 hw_loss 0.204726 lr 0.00033390 rank 4
2023-02-25 10:08:29,872 DEBUG TRAIN Batch 26/7400 loss 10.742512 loss_att 15.527301 loss_ctc 20.541901 loss_rnnt 8.349346 hw_loss 0.243042 lr 0.00033393 rank 3
2023-02-25 10:08:29,874 DEBUG TRAIN Batch 26/7400 loss 12.859309 loss_att 14.121145 loss_ctc 14.928314 loss_rnnt 12.245451 hw_loss 0.160547 lr 0.00033395 rank 1
2023-02-25 10:08:29,891 DEBUG TRAIN Batch 26/7400 loss 5.965858 loss_att 7.575001 loss_ctc 6.288784 loss_rnnt 5.508947 hw_loss 0.172549 lr 0.00033390 rank 5
2023-02-25 10:08:29,897 DEBUG TRAIN Batch 26/7400 loss 8.440032 loss_att 13.654306 loss_ctc 13.245253 loss_rnnt 6.651067 hw_loss 0.197652 lr 0.00033387 rank 7
2023-02-25 10:09:43,361 DEBUG TRAIN Batch 26/7500 loss 8.162118 loss_att 11.204829 loss_ctc 13.475638 loss_rnnt 6.701150 hw_loss 0.269919 lr 0.00033385 rank 3
2023-02-25 10:09:43,371 DEBUG TRAIN Batch 26/7500 loss 5.357723 loss_att 7.568439 loss_ctc 6.232358 loss_rnnt 4.613145 hw_loss 0.348407 lr 0.00033383 rank 0
2023-02-25 10:09:43,372 DEBUG TRAIN Batch 26/7500 loss 2.582713 loss_att 4.964094 loss_ctc 3.347344 loss_rnnt 1.889642 hw_loss 0.215332 lr 0.00033383 rank 4
2023-02-25 10:09:43,375 DEBUG TRAIN Batch 26/7500 loss 9.407477 loss_att 11.605530 loss_ctc 18.162628 loss_rnnt 7.710330 hw_loss 0.169095 lr 0.00033382 rank 5
2023-02-25 10:09:43,376 DEBUG TRAIN Batch 26/7500 loss 10.332577 loss_att 14.184697 loss_ctc 12.365143 loss_rnnt 9.135530 hw_loss 0.291776 lr 0.00033385 rank 2
2023-02-25 10:09:43,379 DEBUG TRAIN Batch 26/7500 loss 5.777681 loss_att 9.957113 loss_ctc 7.570440 loss_rnnt 4.586524 hw_loss 0.217943 lr 0.00033388 rank 1
2023-02-25 10:09:43,379 DEBUG TRAIN Batch 26/7500 loss 9.190907 loss_att 11.122156 loss_ctc 11.495221 loss_rnnt 8.384753 hw_loss 0.211242 lr 0.00033380 rank 7
2023-02-25 10:09:43,380 DEBUG TRAIN Batch 26/7500 loss 5.421481 loss_att 8.030008 loss_ctc 8.374518 loss_rnnt 4.404070 hw_loss 0.191189 lr 0.00033383 rank 6
2023-02-25 10:10:54,718 DEBUG TRAIN Batch 26/7600 loss 9.698776 loss_att 11.241034 loss_ctc 11.376637 loss_rnnt 9.033648 hw_loss 0.249306 lr 0.00033376 rank 6
2023-02-25 10:10:54,722 DEBUG TRAIN Batch 26/7600 loss 9.239530 loss_att 13.744946 loss_ctc 16.520105 loss_rnnt 7.219981 hw_loss 0.276978 lr 0.00033375 rank 4
2023-02-25 10:10:54,724 DEBUG TRAIN Batch 26/7600 loss 1.542788 loss_att 4.021715 loss_ctc 2.337649 loss_rnnt 0.890284 hw_loss 0.095133 lr 0.00033378 rank 3
2023-02-25 10:10:54,725 DEBUG TRAIN Batch 26/7600 loss 4.624366 loss_att 6.793191 loss_ctc 4.992818 loss_rnnt 3.974937 hw_loss 0.312258 lr 0.00033372 rank 7
2023-02-25 10:10:54,727 DEBUG TRAIN Batch 26/7600 loss 8.806186 loss_att 12.515676 loss_ctc 8.522766 loss_rnnt 8.042028 hw_loss 0.112591 lr 0.00033375 rank 0
2023-02-25 10:10:54,726 DEBUG TRAIN Batch 26/7600 loss 8.573188 loss_att 10.852902 loss_ctc 13.996982 loss_rnnt 7.336568 hw_loss 0.107821 lr 0.00033380 rank 1
2023-02-25 10:10:54,727 DEBUG TRAIN Batch 26/7600 loss 5.253531 loss_att 6.728483 loss_ctc 5.802649 loss_rnnt 4.746826 hw_loss 0.259683 lr 0.00033375 rank 5
2023-02-25 10:10:54,728 DEBUG TRAIN Batch 26/7600 loss 9.808327 loss_att 13.642573 loss_ctc 15.796041 loss_rnnt 8.110556 hw_loss 0.248548 lr 0.00033378 rank 2
2023-02-25 10:12:05,561 DEBUG TRAIN Batch 26/7700 loss 6.876202 loss_att 7.922539 loss_ctc 8.013811 loss_rnnt 6.383345 hw_loss 0.247328 lr 0.00033368 rank 4
2023-02-25 10:12:05,577 DEBUG TRAIN Batch 26/7700 loss 7.810688 loss_att 9.104076 loss_ctc 9.230965 loss_rnnt 7.193607 hw_loss 0.316934 lr 0.00033373 rank 1
2023-02-25 10:12:05,578 DEBUG TRAIN Batch 26/7700 loss 7.974168 loss_att 11.160933 loss_ctc 13.721592 loss_rnnt 6.494927 hw_loss 0.141684 lr 0.00033368 rank 0
2023-02-25 10:12:05,579 DEBUG TRAIN Batch 26/7700 loss 8.283417 loss_att 9.332028 loss_ctc 12.892202 loss_rnnt 7.364283 hw_loss 0.177947 lr 0.00033365 rank 7
2023-02-25 10:12:05,581 DEBUG TRAIN Batch 26/7700 loss 5.050530 loss_att 9.882364 loss_ctc 8.279291 loss_rnnt 3.585197 hw_loss 0.128372 lr 0.00033367 rank 5
2023-02-25 10:12:05,587 DEBUG TRAIN Batch 26/7700 loss 6.891318 loss_att 10.199489 loss_ctc 12.172859 loss_rnnt 5.401901 hw_loss 0.231707 lr 0.00033370 rank 2
2023-02-25 10:12:05,588 DEBUG TRAIN Batch 26/7700 loss 2.217887 loss_att 6.132002 loss_ctc 4.505128 loss_rnnt 1.055411 hw_loss 0.140039 lr 0.00033368 rank 6
2023-02-25 10:12:05,634 DEBUG TRAIN Batch 26/7700 loss 7.127359 loss_att 9.883139 loss_ctc 11.449114 loss_rnnt 5.933580 hw_loss 0.124480 lr 0.00033370 rank 3
2023-02-25 10:13:17,746 DEBUG TRAIN Batch 26/7800 loss 9.110298 loss_att 14.804104 loss_ctc 21.995533 loss_rnnt 6.144456 hw_loss 0.204468 lr 0.00033361 rank 6
2023-02-25 10:13:17,761 DEBUG TRAIN Batch 26/7800 loss 8.586601 loss_att 14.042199 loss_ctc 15.482220 loss_rnnt 6.474895 hw_loss 0.189695 lr 0.00033360 rank 0
2023-02-25 10:13:17,762 DEBUG TRAIN Batch 26/7800 loss 8.473517 loss_att 10.416439 loss_ctc 15.075577 loss_rnnt 7.095359 hw_loss 0.204938 lr 0.00033363 rank 3
2023-02-25 10:13:17,762 DEBUG TRAIN Batch 26/7800 loss 7.332715 loss_att 11.052574 loss_ctc 15.719402 loss_rnnt 5.383976 hw_loss 0.162266 lr 0.00033365 rank 1
2023-02-25 10:13:17,762 DEBUG TRAIN Batch 26/7800 loss 10.876931 loss_att 18.847046 loss_ctc 18.000988 loss_rnnt 8.261778 hw_loss 0.133604 lr 0.00033363 rank 2
2023-02-25 10:13:17,765 DEBUG TRAIN Batch 26/7800 loss 4.388513 loss_att 9.258848 loss_ctc 5.507702 loss_rnnt 3.072618 hw_loss 0.361130 lr 0.00033361 rank 4
2023-02-25 10:13:17,765 DEBUG TRAIN Batch 26/7800 loss 3.233450 loss_att 7.234062 loss_ctc 4.440604 loss_rnnt 2.139780 hw_loss 0.248615 lr 0.00033360 rank 5
2023-02-25 10:13:17,790 DEBUG TRAIN Batch 26/7800 loss 6.379752 loss_att 9.459106 loss_ctc 8.475760 loss_rnnt 5.365097 hw_loss 0.223717 lr 0.00033358 rank 7
2023-02-25 10:14:29,446 DEBUG TRAIN Batch 26/7900 loss 5.806150 loss_att 8.331841 loss_ctc 9.786742 loss_rnnt 4.613816 hw_loss 0.293344 lr 0.00033353 rank 0
2023-02-25 10:14:29,449 DEBUG TRAIN Batch 26/7900 loss 10.664504 loss_att 17.990805 loss_ctc 21.920515 loss_rnnt 7.622480 hw_loss 0.142429 lr 0.00033353 rank 6
2023-02-25 10:14:29,451 DEBUG TRAIN Batch 26/7900 loss 1.392203 loss_att 4.539626 loss_ctc 2.686028 loss_rnnt 0.481415 hw_loss 0.203987 lr 0.00033350 rank 7
2023-02-25 10:14:29,455 DEBUG TRAIN Batch 26/7900 loss 4.336469 loss_att 9.089138 loss_ctc 7.370423 loss_rnnt 2.874430 hw_loss 0.200582 lr 0.00033358 rank 1
2023-02-25 10:14:29,456 DEBUG TRAIN Batch 26/7900 loss 13.850129 loss_att 17.848047 loss_ctc 17.327257 loss_rnnt 12.418742 hw_loss 0.315348 lr 0.00033355 rank 3
2023-02-25 10:14:29,460 DEBUG TRAIN Batch 26/7900 loss 8.051567 loss_att 11.599677 loss_ctc 8.457024 loss_rnnt 7.160144 hw_loss 0.239512 lr 0.00033355 rank 2
2023-02-25 10:14:29,463 DEBUG TRAIN Batch 26/7900 loss 8.409158 loss_att 9.813484 loss_ctc 11.646980 loss_rnnt 7.614519 hw_loss 0.153870 lr 0.00033353 rank 4
2023-02-25 10:14:29,463 DEBUG TRAIN Batch 26/7900 loss 9.562961 loss_att 13.621166 loss_ctc 14.706894 loss_rnnt 7.955966 hw_loss 0.205305 lr 0.00033352 rank 5
2023-02-25 10:15:40,093 DEBUG TRAIN Batch 26/8000 loss 10.190374 loss_att 12.518024 loss_ctc 17.731922 loss_rnnt 8.594215 hw_loss 0.234543 lr 0.00033345 rank 0
2023-02-25 10:15:40,097 DEBUG TRAIN Batch 26/8000 loss 24.632149 loss_att 27.322247 loss_ctc 27.891609 loss_rnnt 23.577652 hw_loss 0.153526 lr 0.00033351 rank 1
2023-02-25 10:15:40,098 DEBUG TRAIN Batch 26/8000 loss 10.645450 loss_att 11.247568 loss_ctc 13.887803 loss_rnnt 10.019446 hw_loss 0.137373 lr 0.00033348 rank 3
2023-02-25 10:15:40,098 DEBUG TRAIN Batch 26/8000 loss 11.144357 loss_att 16.280691 loss_ctc 18.030876 loss_rnnt 9.125800 hw_loss 0.137040 lr 0.00033343 rank 7
2023-02-25 10:15:40,102 DEBUG TRAIN Batch 26/8000 loss 3.749195 loss_att 7.361851 loss_ctc 7.787085 loss_rnnt 2.369536 hw_loss 0.222643 lr 0.00033346 rank 6
2023-02-25 10:15:40,105 DEBUG TRAIN Batch 26/8000 loss 14.597113 loss_att 14.828839 loss_ctc 27.606304 loss_rnnt 12.733315 hw_loss 0.155425 lr 0.00033348 rank 2
2023-02-25 10:15:40,106 DEBUG TRAIN Batch 26/8000 loss 6.043900 loss_att 8.524036 loss_ctc 8.330831 loss_rnnt 5.129891 hw_loss 0.211984 lr 0.00033346 rank 4
2023-02-25 10:15:40,110 DEBUG TRAIN Batch 26/8000 loss 4.629893 loss_att 8.626747 loss_ctc 12.670296 loss_rnnt 2.619714 hw_loss 0.260164 lr 0.00033345 rank 5
2023-02-25 10:16:50,862 DEBUG TRAIN Batch 26/8100 loss 4.954600 loss_att 6.224762 loss_ctc 5.469556 loss_rnnt 4.526108 hw_loss 0.198373 lr 0.00033338 rank 4
2023-02-25 10:16:50,863 DEBUG TRAIN Batch 26/8100 loss 16.920822 loss_att 18.996586 loss_ctc 27.360109 loss_rnnt 15.063615 hw_loss 0.094032 lr 0.00033335 rank 7
2023-02-25 10:16:50,871 DEBUG TRAIN Batch 26/8100 loss 5.117935 loss_att 7.370869 loss_ctc 8.200449 loss_rnnt 4.143396 hw_loss 0.211781 lr 0.00033338 rank 0
2023-02-25 10:16:50,874 DEBUG TRAIN Batch 26/8100 loss 3.296830 loss_att 6.581365 loss_ctc 5.177948 loss_rnnt 2.280711 hw_loss 0.203244 lr 0.00033343 rank 1
2023-02-25 10:16:50,874 DEBUG TRAIN Batch 26/8100 loss 7.030035 loss_att 9.022811 loss_ctc 11.015697 loss_rnnt 5.991188 hw_loss 0.204132 lr 0.00033341 rank 3
2023-02-25 10:16:50,877 DEBUG TRAIN Batch 26/8100 loss 6.630012 loss_att 9.493044 loss_ctc 9.128222 loss_rnnt 5.602642 hw_loss 0.228130 lr 0.00033339 rank 6
2023-02-25 10:16:50,880 DEBUG TRAIN Batch 26/8100 loss 3.566536 loss_att 5.880189 loss_ctc 6.153042 loss_rnnt 2.643636 hw_loss 0.216190 lr 0.00033338 rank 5
2023-02-25 10:16:50,880 DEBUG TRAIN Batch 26/8100 loss 11.034773 loss_att 16.012917 loss_ctc 19.867798 loss_rnnt 8.740541 hw_loss 0.226623 lr 0.00033340 rank 2
2023-02-25 10:18:01,729 DEBUG TRAIN Batch 26/8200 loss 11.050377 loss_att 12.212273 loss_ctc 16.458328 loss_rnnt 9.969401 hw_loss 0.239130 lr 0.00033333 rank 3
2023-02-25 10:18:01,730 DEBUG TRAIN Batch 26/8200 loss 7.294816 loss_att 7.609718 loss_ctc 8.806842 loss_rnnt 6.827521 hw_loss 0.380083 lr 0.00033331 rank 0
2023-02-25 10:18:01,734 DEBUG TRAIN Batch 26/8200 loss 9.522206 loss_att 13.906265 loss_ctc 14.686448 loss_rnnt 7.886221 hw_loss 0.132392 lr 0.00033331 rank 6
2023-02-25 10:18:01,735 DEBUG TRAIN Batch 26/8200 loss 7.658303 loss_att 10.248592 loss_ctc 12.132236 loss_rnnt 6.460858 hw_loss 0.155368 lr 0.00033328 rank 7
2023-02-25 10:18:01,738 DEBUG TRAIN Batch 26/8200 loss 6.872159 loss_att 11.445416 loss_ctc 9.441104 loss_rnnt 5.472873 hw_loss 0.266456 lr 0.00033333 rank 2
2023-02-25 10:18:01,738 DEBUG TRAIN Batch 26/8200 loss 12.215027 loss_att 16.595001 loss_ctc 23.098278 loss_rnnt 9.747870 hw_loss 0.262615 lr 0.00033336 rank 1
2023-02-25 10:18:01,738 DEBUG TRAIN Batch 26/8200 loss 5.050255 loss_att 7.899169 loss_ctc 6.763938 loss_rnnt 4.110431 hw_loss 0.265406 lr 0.00033331 rank 4
2023-02-25 10:18:01,740 DEBUG TRAIN Batch 26/8200 loss 7.757987 loss_att 11.340267 loss_ctc 14.550626 loss_rnnt 5.995307 hw_loss 0.263511 lr 0.00033330 rank 5
2023-02-25 10:19:10,932 DEBUG TRAIN Batch 26/8300 loss 6.541884 loss_att 10.569678 loss_ctc 10.710385 loss_rnnt 5.075216 hw_loss 0.197456 lr 0.00033326 rank 2
2023-02-25 10:19:10,934 DEBUG TRAIN Batch 26/8300 loss 9.040692 loss_att 11.407954 loss_ctc 10.209581 loss_rnnt 8.328372 hw_loss 0.155656 lr 0.00033321 rank 7
2023-02-25 10:19:10,934 DEBUG TRAIN Batch 26/8300 loss 7.078183 loss_att 10.523877 loss_ctc 13.036514 loss_rnnt 5.520222 hw_loss 0.139458 lr 0.00033323 rank 0
2023-02-25 10:19:10,934 DEBUG TRAIN Batch 26/8300 loss 4.625385 loss_att 7.038159 loss_ctc 7.262424 loss_rnnt 3.730821 hw_loss 0.113258 lr 0.00033323 rank 4
2023-02-25 10:19:10,935 DEBUG TRAIN Batch 26/8300 loss 3.461400 loss_att 5.411140 loss_ctc 6.987028 loss_rnnt 2.490505 hw_loss 0.207868 lr 0.00033326 rank 3
2023-02-25 10:19:10,939 DEBUG TRAIN Batch 26/8300 loss 7.839479 loss_att 9.739593 loss_ctc 10.227076 loss_rnnt 7.003224 hw_loss 0.258538 lr 0.00033324 rank 6
2023-02-25 10:19:10,940 DEBUG TRAIN Batch 26/8300 loss 6.527461 loss_att 9.115512 loss_ctc 9.979566 loss_rnnt 5.460145 hw_loss 0.167672 lr 0.00033323 rank 5
2023-02-25 10:19:10,942 DEBUG TRAIN Batch 26/8300 loss 16.313072 loss_att 19.334759 loss_ctc 27.498058 loss_rnnt 14.123106 hw_loss 0.176811 lr 0.00033328 rank 1
2023-02-25 10:19:52,720 DEBUG CV Batch 26/0 loss 2.065691 loss_att 2.204496 loss_ctc 2.702765 loss_rnnt 1.829641 hw_loss 0.231274 history loss 1.989184 rank 1
2023-02-25 10:19:52,727 DEBUG CV Batch 26/0 loss 2.065691 loss_att 2.204496 loss_ctc 2.702765 loss_rnnt 1.829641 hw_loss 0.231274 history loss 1.989184 rank 4
2023-02-25 10:19:52,728 DEBUG CV Batch 26/0 loss 2.065691 loss_att 2.204496 loss_ctc 2.702765 loss_rnnt 1.829641 hw_loss 0.231274 history loss 1.989184 rank 0
2023-02-25 10:19:52,733 DEBUG CV Batch 26/0 loss 2.065691 loss_att 2.204496 loss_ctc 2.702765 loss_rnnt 1.829641 hw_loss 0.231274 history loss 1.989184 rank 3
2023-02-25 10:19:52,737 DEBUG CV Batch 26/0 loss 2.065691 loss_att 2.204496 loss_ctc 2.702765 loss_rnnt 1.829641 hw_loss 0.231274 history loss 1.989184 rank 7
2023-02-25 10:19:52,737 DEBUG CV Batch 26/0 loss 2.065691 loss_att 2.204496 loss_ctc 2.702765 loss_rnnt 1.829641 hw_loss 0.231274 history loss 1.989184 rank 2
2023-02-25 10:19:52,746 DEBUG CV Batch 26/0 loss 2.065691 loss_att 2.204496 loss_ctc 2.702765 loss_rnnt 1.829641 hw_loss 0.231274 history loss 1.989184 rank 6
2023-02-25 10:19:52,755 DEBUG CV Batch 26/0 loss 2.065691 loss_att 2.204496 loss_ctc 2.702765 loss_rnnt 1.829641 hw_loss 0.231274 history loss 1.989184 rank 5
2023-02-25 10:20:04,127 DEBUG CV Batch 26/100 loss 6.840672 loss_att 6.614441 loss_ctc 8.861349 loss_rnnt 6.488206 hw_loss 0.240541 history loss 3.326772 rank 4
2023-02-25 10:20:04,198 DEBUG CV Batch 26/100 loss 6.840672 loss_att 6.614441 loss_ctc 8.861349 loss_rnnt 6.488206 hw_loss 0.240541 history loss 3.326772 rank 1
2023-02-25 10:20:04,258 DEBUG CV Batch 26/100 loss 6.840672 loss_att 6.614441 loss_ctc 8.861349 loss_rnnt 6.488206 hw_loss 0.240541 history loss 3.326772 rank 3
2023-02-25 10:20:04,416 DEBUG CV Batch 26/100 loss 6.840672 loss_att 6.614441 loss_ctc 8.861349 loss_rnnt 6.488206 hw_loss 0.240541 history loss 3.326772 rank 7
2023-02-25 10:20:04,443 DEBUG CV Batch 26/100 loss 6.840672 loss_att 6.614441 loss_ctc 8.861349 loss_rnnt 6.488206 hw_loss 0.240541 history loss 3.326772 rank 5
2023-02-25 10:20:04,464 DEBUG CV Batch 26/100 loss 6.840672 loss_att 6.614441 loss_ctc 8.861349 loss_rnnt 6.488206 hw_loss 0.240541 history loss 3.326772 rank 0
2023-02-25 10:20:04,522 DEBUG CV Batch 26/100 loss 6.840672 loss_att 6.614441 loss_ctc 8.861349 loss_rnnt 6.488206 hw_loss 0.240541 history loss 3.326772 rank 2
2023-02-25 10:20:04,804 DEBUG CV Batch 26/100 loss 6.840672 loss_att 6.614441 loss_ctc 8.861349 loss_rnnt 6.488206 hw_loss 0.240541 history loss 3.326772 rank 6
2023-02-25 10:20:17,938 DEBUG CV Batch 26/200 loss 5.271438 loss_att 12.125507 loss_ctc 8.014957 loss_rnnt 3.442708 hw_loss 0.172711 history loss 3.941133 rank 1
2023-02-25 10:20:17,982 DEBUG CV Batch 26/200 loss 5.271438 loss_att 12.125507 loss_ctc 8.014957 loss_rnnt 3.442708 hw_loss 0.172711 history loss 3.941133 rank 3
2023-02-25 10:20:18,085 DEBUG CV Batch 26/200 loss 5.271438 loss_att 12.125507 loss_ctc 8.014957 loss_rnnt 3.442708 hw_loss 0.172711 history loss 3.941133 rank 4
2023-02-25 10:20:18,131 DEBUG CV Batch 26/200 loss 5.271438 loss_att 12.125507 loss_ctc 8.014957 loss_rnnt 3.442708 hw_loss 0.172711 history loss 3.941133 rank 5
2023-02-25 10:20:18,276 DEBUG CV Batch 26/200 loss 5.271438 loss_att 12.125507 loss_ctc 8.014957 loss_rnnt 3.442708 hw_loss 0.172711 history loss 3.941133 rank 7
2023-02-25 10:20:18,307 DEBUG CV Batch 26/200 loss 5.271438 loss_att 12.125507 loss_ctc 8.014957 loss_rnnt 3.442708 hw_loss 0.172711 history loss 3.941133 rank 6
2023-02-25 10:20:18,325 DEBUG CV Batch 26/200 loss 5.271438 loss_att 12.125507 loss_ctc 8.014957 loss_rnnt 3.442708 hw_loss 0.172711 history loss 3.941133 rank 0
2023-02-25 10:20:18,497 DEBUG CV Batch 26/200 loss 5.271438 loss_att 12.125507 loss_ctc 8.014957 loss_rnnt 3.442708 hw_loss 0.172711 history loss 3.941133 rank 2
2023-02-25 10:20:29,999 DEBUG CV Batch 26/300 loss 3.464999 loss_att 4.600565 loss_ctc 6.433243 loss_rnnt 2.705274 hw_loss 0.256586 history loss 4.027346 rank 4
2023-02-25 10:20:30,035 DEBUG CV Batch 26/300 loss 3.464999 loss_att 4.600565 loss_ctc 6.433243 loss_rnnt 2.705274 hw_loss 0.256586 history loss 4.027346 rank 3
2023-02-25 10:20:30,221 DEBUG CV Batch 26/300 loss 3.464999 loss_att 4.600565 loss_ctc 6.433243 loss_rnnt 2.705274 hw_loss 0.256586 history loss 4.027346 rank 5
2023-02-25 10:20:30,253 DEBUG CV Batch 26/300 loss 3.464999 loss_att 4.600565 loss_ctc 6.433243 loss_rnnt 2.705274 hw_loss 0.256586 history loss 4.027346 rank 1
2023-02-25 10:20:30,616 DEBUG CV Batch 26/300 loss 3.464999 loss_att 4.600565 loss_ctc 6.433243 loss_rnnt 2.705274 hw_loss 0.256586 history loss 4.027346 rank 6
2023-02-25 10:20:30,726 DEBUG CV Batch 26/300 loss 3.464999 loss_att 4.600565 loss_ctc 6.433243 loss_rnnt 2.705274 hw_loss 0.256586 history loss 4.027346 rank 7
2023-02-25 10:20:30,797 DEBUG CV Batch 26/300 loss 3.464999 loss_att 4.600565 loss_ctc 6.433243 loss_rnnt 2.705274 hw_loss 0.256586 history loss 4.027346 rank 0
2023-02-25 10:20:30,955 DEBUG CV Batch 26/300 loss 3.464999 loss_att 4.600565 loss_ctc 6.433243 loss_rnnt 2.705274 hw_loss 0.256586 history loss 4.027346 rank 2
2023-02-25 10:20:42,086 DEBUG CV Batch 26/400 loss 30.360771 loss_att 97.711784 loss_ctc 28.166471 loss_rnnt 17.154867 hw_loss 0.053016 history loss 4.907104 rank 4
2023-02-25 10:20:42,274 DEBUG CV Batch 26/400 loss 30.360771 loss_att 97.711784 loss_ctc 28.166471 loss_rnnt 17.154867 hw_loss 0.053016 history loss 4.907104 rank 3
2023-02-25 10:20:42,555 DEBUG CV Batch 26/400 loss 30.360771 loss_att 97.711784 loss_ctc 28.166471 loss_rnnt 17.154867 hw_loss 0.053016 history loss 4.907104 rank 5
2023-02-25 10:20:42,591 DEBUG CV Batch 26/400 loss 30.360771 loss_att 97.711784 loss_ctc 28.166471 loss_rnnt 17.154867 hw_loss 0.053016 history loss 4.907104 rank 1
2023-02-25 10:20:42,727 DEBUG CV Batch 26/400 loss 30.360771 loss_att 97.711784 loss_ctc 28.166471 loss_rnnt 17.154867 hw_loss 0.053016 history loss 4.907104 rank 6
2023-02-25 10:20:43,158 DEBUG CV Batch 26/400 loss 30.360771 loss_att 97.711784 loss_ctc 28.166471 loss_rnnt 17.154867 hw_loss 0.053016 history loss 4.907104 rank 7
2023-02-25 10:20:43,264 DEBUG CV Batch 26/400 loss 30.360771 loss_att 97.711784 loss_ctc 28.166471 loss_rnnt 17.154867 hw_loss 0.053016 history loss 4.907104 rank 0
2023-02-25 10:20:43,427 DEBUG CV Batch 26/400 loss 30.360771 loss_att 97.711784 loss_ctc 28.166471 loss_rnnt 17.154867 hw_loss 0.053016 history loss 4.907104 rank 2
2023-02-25 10:20:52,525 DEBUG CV Batch 26/500 loss 5.260511 loss_att 5.337387 loss_ctc 6.353187 loss_rnnt 4.995177 hw_loss 0.195504 history loss 5.594143 rank 4
2023-02-25 10:20:52,690 DEBUG CV Batch 26/500 loss 5.260511 loss_att 5.337387 loss_ctc 6.353187 loss_rnnt 4.995177 hw_loss 0.195504 history loss 5.594143 rank 3
2023-02-25 10:20:53,202 DEBUG CV Batch 26/500 loss 5.260511 loss_att 5.337387 loss_ctc 6.353187 loss_rnnt 4.995177 hw_loss 0.195504 history loss 5.594143 rank 5
2023-02-25 10:20:53,263 DEBUG CV Batch 26/500 loss 5.260511 loss_att 5.337387 loss_ctc 6.353187 loss_rnnt 4.995177 hw_loss 0.195504 history loss 5.594143 rank 1
2023-02-25 10:20:53,707 DEBUG CV Batch 26/500 loss 5.260511 loss_att 5.337387 loss_ctc 6.353187 loss_rnnt 4.995177 hw_loss 0.195504 history loss 5.594143 rank 6
2023-02-25 10:20:54,030 DEBUG CV Batch 26/500 loss 5.260511 loss_att 5.337387 loss_ctc 6.353187 loss_rnnt 4.995177 hw_loss 0.195504 history loss 5.594143 rank 0
2023-02-25 10:20:54,046 DEBUG CV Batch 26/500 loss 5.260511 loss_att 5.337387 loss_ctc 6.353187 loss_rnnt 4.995177 hw_loss 0.195504 history loss 5.594143 rank 7
2023-02-25 10:20:54,340 DEBUG CV Batch 26/500 loss 5.260511 loss_att 5.337387 loss_ctc 6.353187 loss_rnnt 4.995177 hw_loss 0.195504 history loss 5.594143 rank 2
2023-02-25 10:21:04,929 DEBUG CV Batch 26/600 loss 6.387950 loss_att 6.706849 loss_ctc 8.786361 loss_rnnt 5.859156 hw_loss 0.272299 history loss 6.519078 rank 4
2023-02-25 10:21:05,147 DEBUG CV Batch 26/600 loss 6.387950 loss_att 6.706849 loss_ctc 8.786361 loss_rnnt 5.859156 hw_loss 0.272299 history loss 6.519078 rank 3
2023-02-25 10:21:05,354 DEBUG CV Batch 26/600 loss 6.387950 loss_att 6.706849 loss_ctc 8.786361 loss_rnnt 5.859156 hw_loss 0.272299 history loss 6.519078 rank 5
2023-02-25 10:21:05,539 DEBUG CV Batch 26/600 loss 6.387950 loss_att 6.706849 loss_ctc 8.786361 loss_rnnt 5.859156 hw_loss 0.272299 history loss 6.519078 rank 1
2023-02-25 10:21:05,798 DEBUG CV Batch 26/600 loss 6.387950 loss_att 6.706849 loss_ctc 8.786361 loss_rnnt 5.859156 hw_loss 0.272299 history loss 6.519078 rank 6
2023-02-25 10:21:06,427 DEBUG CV Batch 26/600 loss 6.387950 loss_att 6.706849 loss_ctc 8.786361 loss_rnnt 5.859156 hw_loss 0.272299 history loss 6.519078 rank 7
2023-02-25 10:21:06,691 DEBUG CV Batch 26/600 loss 6.387950 loss_att 6.706849 loss_ctc 8.786361 loss_rnnt 5.859156 hw_loss 0.272299 history loss 6.519078 rank 0
2023-02-25 10:21:06,794 DEBUG CV Batch 26/600 loss 6.387950 loss_att 6.706849 loss_ctc 8.786361 loss_rnnt 5.859156 hw_loss 0.272299 history loss 6.519078 rank 2
2023-02-25 10:21:16,516 DEBUG CV Batch 26/700 loss 16.129089 loss_att 35.784496 loss_ctc 22.977013 loss_rnnt 11.234930 hw_loss 0.093793 history loss 7.146173 rank 3
2023-02-25 10:21:16,753 DEBUG CV Batch 26/700 loss 16.129089 loss_att 35.784496 loss_ctc 22.977013 loss_rnnt 11.234930 hw_loss 0.093793 history loss 7.146173 rank 5
2023-02-25 10:21:16,995 DEBUG CV Batch 26/700 loss 16.129089 loss_att 35.784496 loss_ctc 22.977013 loss_rnnt 11.234930 hw_loss 0.093793 history loss 7.146173 rank 4
2023-02-25 10:21:17,042 DEBUG CV Batch 26/700 loss 16.129089 loss_att 35.784496 loss_ctc 22.977013 loss_rnnt 11.234930 hw_loss 0.093793 history loss 7.146173 rank 1
2023-02-25 10:21:17,080 DEBUG CV Batch 26/700 loss 16.129089 loss_att 35.784496 loss_ctc 22.977013 loss_rnnt 11.234930 hw_loss 0.093793 history loss 7.146173 rank 6
2023-02-25 10:21:18,287 DEBUG CV Batch 26/700 loss 16.129089 loss_att 35.784496 loss_ctc 22.977013 loss_rnnt 11.234930 hw_loss 0.093793 history loss 7.146173 rank 7
2023-02-25 10:21:18,560 DEBUG CV Batch 26/700 loss 16.129089 loss_att 35.784496 loss_ctc 22.977013 loss_rnnt 11.234930 hw_loss 0.093793 history loss 7.146173 rank 0
2023-02-25 10:21:18,618 DEBUG CV Batch 26/700 loss 16.129089 loss_att 35.784496 loss_ctc 22.977013 loss_rnnt 11.234930 hw_loss 0.093793 history loss 7.146173 rank 2
2023-02-25 10:21:27,592 DEBUG CV Batch 26/800 loss 11.908995 loss_att 10.216798 loss_ctc 15.539115 loss_rnnt 11.604803 hw_loss 0.297407 history loss 6.638924 rank 3
2023-02-25 10:21:28,105 DEBUG CV Batch 26/800 loss 11.908995 loss_att 10.216798 loss_ctc 15.539115 loss_rnnt 11.604803 hw_loss 0.297407 history loss 6.638924 rank 5
2023-02-25 10:21:28,375 DEBUG CV Batch 26/800 loss 11.908995 loss_att 10.216798 loss_ctc 15.539115 loss_rnnt 11.604803 hw_loss 0.297407 history loss 6.638924 rank 1
2023-02-25 10:21:28,394 DEBUG CV Batch 26/800 loss 11.908995 loss_att 10.216798 loss_ctc 15.539115 loss_rnnt 11.604803 hw_loss 0.297407 history loss 6.638924 rank 6
2023-02-25 10:21:29,303 DEBUG CV Batch 26/800 loss 11.908995 loss_att 10.216798 loss_ctc 15.539115 loss_rnnt 11.604803 hw_loss 0.297407 history loss 6.638924 rank 4
2023-02-25 10:21:29,795 DEBUG CV Batch 26/800 loss 11.908995 loss_att 10.216798 loss_ctc 15.539115 loss_rnnt 11.604803 hw_loss 0.297407 history loss 6.638924 rank 7
2023-02-25 10:21:30,240 DEBUG CV Batch 26/800 loss 11.908995 loss_att 10.216798 loss_ctc 15.539115 loss_rnnt 11.604803 hw_loss 0.297407 history loss 6.638924 rank 2
2023-02-25 10:21:30,753 DEBUG CV Batch 26/800 loss 11.908995 loss_att 10.216798 loss_ctc 15.539115 loss_rnnt 11.604803 hw_loss 0.297407 history loss 6.638924 rank 0
2023-02-25 10:21:41,112 DEBUG CV Batch 26/900 loss 13.512894 loss_att 16.710388 loss_ctc 25.602287 loss_rnnt 11.229068 hw_loss 0.060765 history loss 6.459381 rank 3
2023-02-25 10:21:41,663 DEBUG CV Batch 26/900 loss 13.512894 loss_att 16.710388 loss_ctc 25.602287 loss_rnnt 11.229068 hw_loss 0.060765 history loss 6.459381 rank 5
2023-02-25 10:21:41,939 DEBUG CV Batch 26/900 loss 13.512894 loss_att 16.710388 loss_ctc 25.602287 loss_rnnt 11.229068 hw_loss 0.060765 history loss 6.459381 rank 1
2023-02-25 10:21:42,408 DEBUG CV Batch 26/900 loss 13.512894 loss_att 16.710388 loss_ctc 25.602287 loss_rnnt 11.229068 hw_loss 0.060765 history loss 6.459381 rank 6
2023-02-25 10:21:43,037 DEBUG CV Batch 26/900 loss 13.512894 loss_att 16.710388 loss_ctc 25.602287 loss_rnnt 11.229068 hw_loss 0.060765 history loss 6.459381 rank 4
2023-02-25 10:21:43,673 DEBUG CV Batch 26/900 loss 13.512894 loss_att 16.710388 loss_ctc 25.602287 loss_rnnt 11.229068 hw_loss 0.060765 history loss 6.459381 rank 7
2023-02-25 10:21:44,055 DEBUG CV Batch 26/900 loss 13.512894 loss_att 16.710388 loss_ctc 25.602287 loss_rnnt 11.229068 hw_loss 0.060765 history loss 6.459381 rank 2
2023-02-25 10:21:44,689 DEBUG CV Batch 26/900 loss 13.512894 loss_att 16.710388 loss_ctc 25.602287 loss_rnnt 11.229068 hw_loss 0.060765 history loss 6.459381 rank 0
2023-02-25 10:21:53,313 DEBUG CV Batch 26/1000 loss 6.579825 loss_att 6.440340 loss_ctc 6.830050 loss_rnnt 6.436521 hw_loss 0.258448 history loss 6.232775 rank 3
2023-02-25 10:21:54,005 DEBUG CV Batch 26/1000 loss 6.579825 loss_att 6.440340 loss_ctc 6.830050 loss_rnnt 6.436521 hw_loss 0.258448 history loss 6.232775 rank 5
2023-02-25 10:21:54,597 DEBUG CV Batch 26/1000 loss 6.579825 loss_att 6.440340 loss_ctc 6.830050 loss_rnnt 6.436521 hw_loss 0.258448 history loss 6.232775 rank 1
2023-02-25 10:21:54,794 DEBUG CV Batch 26/1000 loss 6.579826 loss_att 6.440340 loss_ctc 6.830050 loss_rnnt 6.436521 hw_loss 0.258448 history loss 6.232775 rank 6
2023-02-25 10:21:55,113 DEBUG CV Batch 26/1000 loss 6.579826 loss_att 6.440340 loss_ctc 6.830050 loss_rnnt 6.436521 hw_loss 0.258448 history loss 6.232775 rank 4
2023-02-25 10:21:56,456 DEBUG CV Batch 26/1000 loss 6.579825 loss_att 6.440340 loss_ctc 6.830050 loss_rnnt 6.436521 hw_loss 0.258448 history loss 6.232775 rank 7
2023-02-25 10:21:56,817 DEBUG CV Batch 26/1000 loss 6.579826 loss_att 6.440340 loss_ctc 6.830050 loss_rnnt 6.436521 hw_loss 0.258448 history loss 6.232775 rank 2
2023-02-25 10:21:57,288 DEBUG CV Batch 26/1000 loss 6.579825 loss_att 6.440340 loss_ctc 6.830050 loss_rnnt 6.436521 hw_loss 0.258448 history loss 6.232775 rank 0
2023-02-25 10:22:06,023 DEBUG CV Batch 26/1100 loss 6.504708 loss_att 5.798870 loss_ctc 9.479664 loss_rnnt 6.127561 hw_loss 0.228100 history loss 6.194307 rank 3
2023-02-25 10:22:06,219 DEBUG CV Batch 26/1100 loss 6.504708 loss_att 5.798870 loss_ctc 9.479664 loss_rnnt 6.127561 hw_loss 0.228100 history loss 6.194307 rank 5
2023-02-25 10:22:06,632 DEBUG CV Batch 26/1100 loss 6.504708 loss_att 5.798870 loss_ctc 9.479664 loss_rnnt 6.127561 hw_loss 0.228100 history loss 6.194307 rank 6
2023-02-25 10:22:06,721 DEBUG CV Batch 26/1100 loss 6.504708 loss_att 5.798870 loss_ctc 9.479664 loss_rnnt 6.127561 hw_loss 0.228100 history loss 6.194307 rank 1
2023-02-25 10:22:06,865 DEBUG CV Batch 26/1100 loss 6.504708 loss_att 5.798870 loss_ctc 9.479664 loss_rnnt 6.127561 hw_loss 0.228100 history loss 6.194307 rank 4
2023-02-25 10:22:08,787 DEBUG CV Batch 26/1100 loss 6.504708 loss_att 5.798870 loss_ctc 9.479664 loss_rnnt 6.127561 hw_loss 0.228100 history loss 6.194307 rank 7
2023-02-25 10:22:09,257 DEBUG CV Batch 26/1100 loss 6.504708 loss_att 5.798870 loss_ctc 9.479664 loss_rnnt 6.127561 hw_loss 0.228100 history loss 6.194307 rank 2
2023-02-25 10:22:09,858 DEBUG CV Batch 26/1100 loss 6.504708 loss_att 5.798870 loss_ctc 9.479664 loss_rnnt 6.127561 hw_loss 0.228100 history loss 6.194307 rank 0
2023-02-25 10:22:16,640 DEBUG CV Batch 26/1200 loss 7.557700 loss_att 8.145542 loss_ctc 7.912923 loss_rnnt 7.298534 hw_loss 0.176688 history loss 6.498004 rank 3
2023-02-25 10:22:16,678 DEBUG CV Batch 26/1200 loss 7.557700 loss_att 8.145542 loss_ctc 7.912923 loss_rnnt 7.298534 hw_loss 0.176688 history loss 6.498004 rank 5
2023-02-25 10:22:17,008 DEBUG CV Batch 26/1200 loss 7.557700 loss_att 8.145542 loss_ctc 7.912923 loss_rnnt 7.298534 hw_loss 0.176688 history loss 6.498004 rank 6
2023-02-25 10:22:17,407 DEBUG CV Batch 26/1200 loss 7.557700 loss_att 8.145542 loss_ctc 7.912923 loss_rnnt 7.298534 hw_loss 0.176688 history loss 6.498004 rank 4
2023-02-25 10:22:17,788 DEBUG CV Batch 26/1200 loss 7.557700 loss_att 8.145542 loss_ctc 7.912923 loss_rnnt 7.298534 hw_loss 0.176688 history loss 6.498004 rank 1
2023-02-25 10:22:19,937 DEBUG CV Batch 26/1200 loss 7.557700 loss_att 8.145542 loss_ctc 7.912923 loss_rnnt 7.298534 hw_loss 0.176688 history loss 6.498004 rank 7
2023-02-25 10:22:20,477 DEBUG CV Batch 26/1200 loss 7.557700 loss_att 8.145542 loss_ctc 7.912923 loss_rnnt 7.298534 hw_loss 0.176688 history loss 6.498004 rank 2
2023-02-25 10:22:20,865 DEBUG CV Batch 26/1200 loss 7.557700 loss_att 8.145542 loss_ctc 7.912923 loss_rnnt 7.298534 hw_loss 0.176688 history loss 6.498004 rank 0
2023-02-25 10:22:28,645 DEBUG CV Batch 26/1300 loss 4.782076 loss_att 5.059147 loss_ctc 6.466065 loss_rnnt 4.373612 hw_loss 0.240970 history loss 6.815169 rank 3
2023-02-25 10:22:28,815 DEBUG CV Batch 26/1300 loss 4.782076 loss_att 5.059147 loss_ctc 6.466065 loss_rnnt 4.373612 hw_loss 0.240970 history loss 6.815169 rank 5
2023-02-25 10:22:29,121 DEBUG CV Batch 26/1300 loss 4.782076 loss_att 5.059147 loss_ctc 6.466065 loss_rnnt 4.373612 hw_loss 0.240970 history loss 6.815169 rank 6
2023-02-25 10:22:29,508 DEBUG CV Batch 26/1300 loss 4.782076 loss_att 5.059147 loss_ctc 6.466065 loss_rnnt 4.373612 hw_loss 0.240970 history loss 6.815169 rank 4
2023-02-25 10:22:30,038 DEBUG CV Batch 26/1300 loss 4.782076 loss_att 5.059147 loss_ctc 6.466065 loss_rnnt 4.373612 hw_loss 0.240970 history loss 6.815169 rank 1
2023-02-25 10:22:32,325 DEBUG CV Batch 26/1300 loss 4.782076 loss_att 5.059147 loss_ctc 6.466065 loss_rnnt 4.373612 hw_loss 0.240970 history loss 6.815169 rank 7
2023-02-25 10:22:32,981 DEBUG CV Batch 26/1300 loss 4.782076 loss_att 5.059147 loss_ctc 6.466065 loss_rnnt 4.373612 hw_loss 0.240970 history loss 6.815169 rank 2
2023-02-25 10:22:33,483 DEBUG CV Batch 26/1300 loss 4.782076 loss_att 5.059147 loss_ctc 6.466065 loss_rnnt 4.373612 hw_loss 0.240970 history loss 6.815169 rank 0
2023-02-25 10:22:40,061 DEBUG CV Batch 26/1400 loss 5.002995 loss_att 14.668025 loss_ctc 3.666082 loss_rnnt 3.167898 hw_loss 0.150648 history loss 7.118556 rank 5
2023-02-25 10:22:40,110 DEBUG CV Batch 26/1400 loss 5.002995 loss_att 14.668025 loss_ctc 3.666082 loss_rnnt 3.167898 hw_loss 0.150648 history loss 7.118556 rank 3
2023-02-25 10:22:40,809 DEBUG CV Batch 26/1400 loss 5.002995 loss_att 14.668025 loss_ctc 3.666082 loss_rnnt 3.167898 hw_loss 0.150648 history loss 7.118556 rank 6
2023-02-25 10:22:41,545 DEBUG CV Batch 26/1400 loss 5.002995 loss_att 14.668025 loss_ctc 3.666082 loss_rnnt 3.167898 hw_loss 0.150648 history loss 7.118556 rank 1
2023-02-25 10:22:42,055 DEBUG CV Batch 26/1400 loss 5.002995 loss_att 14.668025 loss_ctc 3.666082 loss_rnnt 3.167898 hw_loss 0.150648 history loss 7.118556 rank 4
2023-02-25 10:22:44,030 DEBUG CV Batch 26/1400 loss 5.002995 loss_att 14.668025 loss_ctc 3.666082 loss_rnnt 3.167898 hw_loss 0.150648 history loss 7.118556 rank 7
2023-02-25 10:22:44,801 DEBUG CV Batch 26/1400 loss 5.002995 loss_att 14.668025 loss_ctc 3.666082 loss_rnnt 3.167898 hw_loss 0.150648 history loss 7.118556 rank 2
2023-02-25 10:22:45,139 DEBUG CV Batch 26/1400 loss 5.002995 loss_att 14.668025 loss_ctc 3.666082 loss_rnnt 3.167898 hw_loss 0.150648 history loss 7.118556 rank 0
2023-02-25 10:22:51,399 DEBUG CV Batch 26/1500 loss 5.849383 loss_att 7.162589 loss_ctc 6.662914 loss_rnnt 5.403890 hw_loss 0.139464 history loss 6.955748 rank 3
2023-02-25 10:22:51,451 DEBUG CV Batch 26/1500 loss 5.849383 loss_att 7.162589 loss_ctc 6.662914 loss_rnnt 5.403890 hw_loss 0.139464 history loss 6.955748 rank 5
2023-02-25 10:22:53,279 DEBUG CV Batch 26/1500 loss 5.849383 loss_att 7.162589 loss_ctc 6.662914 loss_rnnt 5.403890 hw_loss 0.139464 history loss 6.955748 rank 1
2023-02-25 10:22:53,540 DEBUG CV Batch 26/1500 loss 5.849383 loss_att 7.162589 loss_ctc 6.662914 loss_rnnt 5.403890 hw_loss 0.139464 history loss 6.955748 rank 6
2023-02-25 10:22:54,708 DEBUG CV Batch 26/1500 loss 5.849383 loss_att 7.162589 loss_ctc 6.662914 loss_rnnt 5.403890 hw_loss 0.139464 history loss 6.955748 rank 4
2023-02-25 10:22:55,886 DEBUG CV Batch 26/1500 loss 5.849383 loss_att 7.162589 loss_ctc 6.662914 loss_rnnt 5.403890 hw_loss 0.139464 history loss 6.955748 rank 7
2023-02-25 10:22:56,768 DEBUG CV Batch 26/1500 loss 5.849383 loss_att 7.162589 loss_ctc 6.662914 loss_rnnt 5.403890 hw_loss 0.139464 history loss 6.955748 rank 2
2023-02-25 10:22:57,125 DEBUG CV Batch 26/1500 loss 5.849383 loss_att 7.162589 loss_ctc 6.662914 loss_rnnt 5.403890 hw_loss 0.139464 history loss 6.955748 rank 0
2023-02-25 10:23:04,685 DEBUG CV Batch 26/1600 loss 10.445875 loss_att 12.848943 loss_ctc 10.970234 loss_rnnt 9.766633 hw_loss 0.241339 history loss 6.899379 rank 3
2023-02-25 10:23:04,775 DEBUG CV Batch 26/1600 loss 10.445875 loss_att 12.848943 loss_ctc 10.970234 loss_rnnt 9.766633 hw_loss 0.241339 history loss 6.899379 rank 5
2023-02-25 10:23:06,668 DEBUG CV Batch 26/1600 loss 10.445875 loss_att 12.848943 loss_ctc 10.970234 loss_rnnt 9.766633 hw_loss 0.241339 history loss 6.899379 rank 1
2023-02-25 10:23:07,216 DEBUG CV Batch 26/1600 loss 10.445875 loss_att 12.848943 loss_ctc 10.970234 loss_rnnt 9.766633 hw_loss 0.241339 history loss 6.899379 rank 6
2023-02-25 10:23:08,416 DEBUG CV Batch 26/1600 loss 10.445875 loss_att 12.848943 loss_ctc 10.970234 loss_rnnt 9.766633 hw_loss 0.241339 history loss 6.899379 rank 4
2023-02-25 10:23:09,395 DEBUG CV Batch 26/1600 loss 10.445875 loss_att 12.848943 loss_ctc 10.970234 loss_rnnt 9.766633 hw_loss 0.241339 history loss 6.899379 rank 7
2023-02-25 10:23:10,281 DEBUG CV Batch 26/1600 loss 10.445875 loss_att 12.848943 loss_ctc 10.970234 loss_rnnt 9.766633 hw_loss 0.241339 history loss 6.899379 rank 2
2023-02-25 10:23:10,785 DEBUG CV Batch 26/1600 loss 10.445875 loss_att 12.848943 loss_ctc 10.970234 loss_rnnt 9.766633 hw_loss 0.241339 history loss 6.899379 rank 0
2023-02-25 10:23:17,423 DEBUG CV Batch 26/1700 loss 12.479794 loss_att 10.446864 loss_ctc 18.714340 loss_rnnt 11.925679 hw_loss 0.242675 history loss 6.804284 rank 3
2023-02-25 10:23:17,589 DEBUG CV Batch 26/1700 loss 12.479794 loss_att 10.446864 loss_ctc 18.714340 loss_rnnt 11.925679 hw_loss 0.242675 history loss 6.804284 rank 5
2023-02-25 10:23:19,315 DEBUG CV Batch 26/1700 loss 12.479794 loss_att 10.446864 loss_ctc 18.714340 loss_rnnt 11.925679 hw_loss 0.242675 history loss 6.804284 rank 1
2023-02-25 10:23:19,618 DEBUG CV Batch 26/1700 loss 12.479794 loss_att 10.446864 loss_ctc 18.714340 loss_rnnt 11.925679 hw_loss 0.242675 history loss 6.804284 rank 6
2023-02-25 10:23:21,109 DEBUG CV Batch 26/1700 loss 12.479794 loss_att 10.446864 loss_ctc 18.714340 loss_rnnt 11.925679 hw_loss 0.242675 history loss 6.804284 rank 4
2023-02-25 10:23:22,041 DEBUG CV Batch 26/1700 loss 12.479794 loss_att 10.446864 loss_ctc 18.714340 loss_rnnt 11.925679 hw_loss 0.242675 history loss 6.804284 rank 7
2023-02-25 10:23:22,962 DEBUG CV Batch 26/1700 loss 12.479794 loss_att 10.446864 loss_ctc 18.714340 loss_rnnt 11.925679 hw_loss 0.242675 history loss 6.804284 rank 2
2023-02-25 10:23:23,432 DEBUG CV Batch 26/1700 loss 12.479794 loss_att 10.446864 loss_ctc 18.714340 loss_rnnt 11.925679 hw_loss 0.242675 history loss 6.804284 rank 0
2023-02-25 10:23:26,943 INFO Epoch 26 CV info cv_loss 6.773364791768551
2023-02-25 10:23:26,943 INFO Epoch 27 TRAIN info lr 0.00033322153774877983
2023-02-25 10:23:26,945 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 10:23:27,194 INFO Epoch 26 CV info cv_loss 6.773364792285428
2023-02-25 10:23:27,195 INFO Epoch 27 TRAIN info lr 0.00033320525903663247
2023-02-25 10:23:27,196 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 10:23:28,298 INFO Epoch 26 CV info cv_loss 6.7733647923112725
2023-02-25 10:23:28,299 INFO Epoch 27 TRAIN info lr 0.0003332644657926469
2023-02-25 10:23:28,300 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 10:23:28,983 INFO Epoch 26 CV info cv_loss 6.7733647913636625
2023-02-25 10:23:28,983 INFO Epoch 27 TRAIN info lr 0.00033322079775555797
2023-02-25 10:23:28,989 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 10:23:30,530 INFO Epoch 26 CV info cv_loss 6.773364790648649
2023-02-25 10:23:30,531 INFO Epoch 27 TRAIN info lr 0.0003332178378319697
2023-02-25 10:23:30,536 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 10:23:31,234 INFO Epoch 26 CV info cv_loss 6.77336479355178
2023-02-25 10:23:31,234 INFO Epoch 27 TRAIN info lr 0.0003331860236341609
2023-02-25 10:23:31,236 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 10:23:32,202 INFO Epoch 26 CV info cv_loss 6.773364792285428
2023-02-25 10:23:32,202 INFO Epoch 27 TRAIN info lr 0.0003332385589536707
2023-02-25 10:23:32,204 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 10:23:32,848 INFO Epoch 26 CV info cv_loss 6.773364791703941
2023-02-25 10:23:32,848 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/26.pt
2023-02-25 10:23:33,542 INFO Epoch 27 TRAIN info lr 0.00033319490109826744
2023-02-25 10:23:33,546 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 10:24:33,058 DEBUG TRAIN Batch 27/0 loss 8.109727 loss_att 8.407292 loss_ctc 10.239445 loss_rnnt 7.590730 hw_loss 0.329104 lr 0.00033322 rank 3
2023-02-25 10:24:33,058 DEBUG TRAIN Batch 27/0 loss 6.244808 loss_att 6.105458 loss_ctc 7.550295 loss_rnnt 5.959290 hw_loss 0.261231 lr 0.00033326 rank 1
2023-02-25 10:24:33,060 DEBUG TRAIN Batch 27/0 loss 5.681354 loss_att 6.156668 loss_ctc 8.025333 loss_rnnt 5.173858 hw_loss 0.187319 lr 0.00033320 rank 5
2023-02-25 10:24:33,061 DEBUG TRAIN Batch 27/0 loss 8.999490 loss_att 9.028719 loss_ctc 11.983001 loss_rnnt 8.456967 hw_loss 0.260389 lr 0.00033322 rank 6
2023-02-25 10:24:33,077 DEBUG TRAIN Batch 27/0 loss 8.065670 loss_att 8.365051 loss_ctc 10.646720 loss_rnnt 7.534526 hw_loss 0.238365 lr 0.00033319 rank 0
2023-02-25 10:24:33,080 DEBUG TRAIN Batch 27/0 loss 8.726384 loss_att 8.045208 loss_ctc 10.371947 loss_rnnt 8.487918 hw_loss 0.291175 lr 0.00033322 rank 4
2023-02-25 10:24:33,097 DEBUG TRAIN Batch 27/0 loss 11.189211 loss_att 10.459318 loss_ctc 15.003617 loss_rnnt 10.677152 hw_loss 0.280218 lr 0.00033324 rank 2
2023-02-25 10:24:33,104 DEBUG TRAIN Batch 27/0 loss 6.504078 loss_att 7.109737 loss_ctc 9.540878 loss_rnnt 5.862829 hw_loss 0.216021 lr 0.00033319 rank 7
2023-02-25 10:25:42,362 DEBUG TRAIN Batch 27/100 loss 4.609964 loss_att 7.994035 loss_ctc 7.899382 loss_rnnt 3.428763 hw_loss 0.123371 lr 0.00033312 rank 0
2023-02-25 10:25:42,362 DEBUG TRAIN Batch 27/100 loss 16.403273 loss_att 17.787531 loss_ctc 19.998383 loss_rnnt 15.543980 hw_loss 0.193299 lr 0.00033319 rank 1
2023-02-25 10:25:42,365 DEBUG TRAIN Batch 27/100 loss 2.072098 loss_att 5.629754 loss_ctc 3.618138 loss_rnnt 1.005764 hw_loss 0.278745 lr 0.00033313 rank 5
2023-02-25 10:25:42,366 DEBUG TRAIN Batch 27/100 loss 3.402625 loss_att 4.513313 loss_ctc 5.085288 loss_rnnt 2.884775 hw_loss 0.133796 lr 0.00033314 rank 4
2023-02-25 10:25:42,367 DEBUG TRAIN Batch 27/100 loss 5.410839 loss_att 8.460768 loss_ctc 9.089169 loss_rnnt 4.233068 hw_loss 0.145014 lr 0.00033315 rank 3
2023-02-25 10:25:42,369 DEBUG TRAIN Batch 27/100 loss 7.447639 loss_att 11.696363 loss_ctc 10.708292 loss_rnnt 6.026506 hw_loss 0.256189 lr 0.00033316 rank 2
2023-02-25 10:25:42,374 DEBUG TRAIN Batch 27/100 loss 6.837589 loss_att 8.770456 loss_ctc 9.611408 loss_rnnt 5.989989 hw_loss 0.170970 lr 0.00033311 rank 7
2023-02-25 10:25:42,407 DEBUG TRAIN Batch 27/100 loss 4.968762 loss_att 9.026590 loss_ctc 7.088216 loss_rnnt 3.773494 hw_loss 0.189578 lr 0.00033315 rank 6
2023-02-25 10:26:53,188 DEBUG TRAIN Batch 27/200 loss 3.138789 loss_att 6.470963 loss_ctc 4.053194 loss_rnnt 2.267825 hw_loss 0.154890 lr 0.00033306 rank 5
2023-02-25 10:26:53,205 DEBUG TRAIN Batch 27/200 loss 7.896096 loss_att 10.087933 loss_ctc 9.471717 loss_rnnt 7.154822 hw_loss 0.174044 lr 0.00033312 rank 1
2023-02-25 10:26:53,206 DEBUG TRAIN Batch 27/200 loss 7.304307 loss_att 9.476083 loss_ctc 10.099506 loss_rnnt 6.389339 hw_loss 0.202347 lr 0.00033307 rank 3
2023-02-25 10:26:53,207 DEBUG TRAIN Batch 27/200 loss 7.686756 loss_att 10.893344 loss_ctc 14.828837 loss_rnnt 6.030903 hw_loss 0.116734 lr 0.00033305 rank 0
2023-02-25 10:26:53,209 DEBUG TRAIN Batch 27/200 loss 12.834481 loss_att 18.440105 loss_ctc 20.753038 loss_rnnt 10.584963 hw_loss 0.136100 lr 0.00033307 rank 6
2023-02-25 10:26:53,212 DEBUG TRAIN Batch 27/200 loss 9.822465 loss_att 16.360500 loss_ctc 13.039515 loss_rnnt 7.988612 hw_loss 0.182451 lr 0.00033304 rank 7
2023-02-25 10:26:53,212 DEBUG TRAIN Batch 27/200 loss 10.825833 loss_att 13.570305 loss_ctc 15.336914 loss_rnnt 9.592497 hw_loss 0.155562 lr 0.00033309 rank 2
2023-02-25 10:26:53,224 DEBUG TRAIN Batch 27/200 loss 9.113856 loss_att 12.441666 loss_ctc 15.140800 loss_rnnt 7.572654 hw_loss 0.135091 lr 0.00033307 rank 4
2023-02-25 10:28:04,906 DEBUG TRAIN Batch 27/300 loss 13.496560 loss_att 15.620155 loss_ctc 19.418180 loss_rnnt 12.193151 hw_loss 0.167139 lr 0.00033297 rank 0
2023-02-25 10:28:04,911 DEBUG TRAIN Batch 27/300 loss 12.215437 loss_att 16.530315 loss_ctc 18.584400 loss_rnnt 10.423594 hw_loss 0.149386 lr 0.00033302 rank 2
2023-02-25 10:28:04,912 DEBUG TRAIN Batch 27/300 loss 5.308644 loss_att 8.394606 loss_ctc 7.092672 loss_rnnt 4.345633 hw_loss 0.202404 lr 0.00033300 rank 3
2023-02-25 10:28:04,912 DEBUG TRAIN Batch 27/300 loss 5.088545 loss_att 7.936897 loss_ctc 6.306703 loss_rnnt 4.267155 hw_loss 0.167435 lr 0.00033304 rank 1
2023-02-25 10:28:04,913 DEBUG TRAIN Batch 27/300 loss 8.772876 loss_att 10.445644 loss_ctc 14.508369 loss_rnnt 7.588087 hw_loss 0.160317 lr 0.00033298 rank 5
2023-02-25 10:28:04,913 DEBUG TRAIN Batch 27/300 loss 8.889668 loss_att 12.814305 loss_ctc 15.433952 loss_rnnt 7.093163 hw_loss 0.260633 lr 0.00033296 rank 7
2023-02-25 10:28:04,916 DEBUG TRAIN Batch 27/300 loss 4.772795 loss_att 7.446559 loss_ctc 5.573604 loss_rnnt 4.040937 hw_loss 0.169369 lr 0.00033300 rank 4
2023-02-25 10:28:04,916 DEBUG TRAIN Batch 27/300 loss 7.993828 loss_att 10.364819 loss_ctc 9.424813 loss_rnnt 7.225007 hw_loss 0.194670 lr 0.00033300 rank 6
2023-02-25 10:29:16,792 DEBUG TRAIN Batch 27/400 loss 5.782209 loss_att 7.149197 loss_ctc 7.771218 loss_rnnt 5.154979 hw_loss 0.166183 lr 0.00033290 rank 0
2023-02-25 10:29:16,798 DEBUG TRAIN Batch 27/400 loss 9.279929 loss_att 11.480484 loss_ctc 9.679295 loss_rnnt 8.676116 hw_loss 0.207100 lr 0.00033294 rank 2
2023-02-25 10:29:16,798 DEBUG TRAIN Batch 27/400 loss 10.923683 loss_att 13.817138 loss_ctc 11.489786 loss_rnnt 10.162868 hw_loss 0.199956 lr 0.00033293 rank 3
2023-02-25 10:29:16,799 DEBUG TRAIN Batch 27/400 loss 9.692506 loss_att 12.022062 loss_ctc 10.997533 loss_rnnt 8.941904 hw_loss 0.207537 lr 0.00033297 rank 1
2023-02-25 10:29:16,803 DEBUG TRAIN Batch 27/400 loss 12.276862 loss_att 13.525161 loss_ctc 18.760202 loss_rnnt 11.114292 hw_loss 0.090872 lr 0.00033292 rank 6
2023-02-25 10:29:16,803 DEBUG TRAIN Batch 27/400 loss 8.872476 loss_att 11.760120 loss_ctc 12.029427 loss_rnnt 7.784661 hw_loss 0.167547 lr 0.00033289 rank 7
2023-02-25 10:29:16,803 DEBUG TRAIN Batch 27/400 loss 4.701859 loss_att 8.670408 loss_ctc 9.113617 loss_rnnt 3.255408 hw_loss 0.120949 lr 0.00033292 rank 4
2023-02-25 10:29:16,805 DEBUG TRAIN Batch 27/400 loss 4.004632 loss_att 6.188708 loss_ctc 6.868865 loss_rnnt 3.071754 hw_loss 0.214060 lr 0.00033291 rank 5
2023-02-25 10:30:26,817 DEBUG TRAIN Batch 27/500 loss 16.134657 loss_att 17.129829 loss_ctc 31.297598 loss_rnnt 13.846887 hw_loss 0.125643 lr 0.00033285 rank 4
2023-02-25 10:30:26,818 DEBUG TRAIN Batch 27/500 loss 12.416604 loss_att 13.514549 loss_ctc 16.958782 loss_rnnt 11.461787 hw_loss 0.243008 lr 0.00033285 rank 3
2023-02-25 10:30:26,821 DEBUG TRAIN Batch 27/500 loss 11.130469 loss_att 15.806628 loss_ctc 18.569843 loss_rnnt 9.091784 hw_loss 0.209131 lr 0.00033285 rank 6
2023-02-25 10:30:26,822 DEBUG TRAIN Batch 27/500 loss 13.021777 loss_att 15.410778 loss_ctc 17.608683 loss_rnnt 11.835168 hw_loss 0.182290 lr 0.00033287 rank 2
2023-02-25 10:30:26,822 DEBUG TRAIN Batch 27/500 loss 4.429586 loss_att 8.370968 loss_ctc 6.649186 loss_rnnt 3.266460 hw_loss 0.147944 lr 0.00033282 rank 7
2023-02-25 10:30:26,822 DEBUG TRAIN Batch 27/500 loss 3.712713 loss_att 5.115976 loss_ctc 5.734911 loss_rnnt 3.019392 hw_loss 0.268205 lr 0.00033282 rank 0
2023-02-25 10:30:26,824 DEBUG TRAIN Batch 27/500 loss 8.458289 loss_att 9.955996 loss_ctc 10.220977 loss_rnnt 7.837163 hw_loss 0.162300 lr 0.00033289 rank 1
2023-02-25 10:30:26,826 DEBUG TRAIN Batch 27/500 loss 16.069918 loss_att 17.590843 loss_ctc 22.394995 loss_rnnt 14.842514 hw_loss 0.149765 lr 0.00033284 rank 5
2023-02-25 10:31:37,720 DEBUG TRAIN Batch 27/600 loss 8.985762 loss_att 8.844837 loss_ctc 13.589209 loss_rnnt 8.249196 hw_loss 0.283045 lr 0.00033275 rank 0
2023-02-25 10:31:37,724 DEBUG TRAIN Batch 27/600 loss 8.299611 loss_att 10.871837 loss_ctc 11.589522 loss_rnnt 7.224254 hw_loss 0.229233 lr 0.00033278 rank 3
2023-02-25 10:31:37,724 DEBUG TRAIN Batch 27/600 loss 7.865057 loss_att 9.625296 loss_ctc 11.173052 loss_rnnt 6.931146 hw_loss 0.263995 lr 0.00033279 rank 2
2023-02-25 10:31:37,726 DEBUG TRAIN Batch 27/600 loss 8.751755 loss_att 10.112353 loss_ctc 12.316309 loss_rnnt 7.843434 hw_loss 0.301737 lr 0.00033282 rank 1
2023-02-25 10:31:37,727 DEBUG TRAIN Batch 27/600 loss 8.061559 loss_att 10.155732 loss_ctc 11.442821 loss_rnnt 7.072427 hw_loss 0.223990 lr 0.00033277 rank 4
2023-02-25 10:31:37,730 DEBUG TRAIN Batch 27/600 loss 9.615099 loss_att 12.101873 loss_ctc 15.101856 loss_rnnt 8.255732 hw_loss 0.244585 lr 0.00033274 rank 7
2023-02-25 10:31:37,731 DEBUG TRAIN Batch 27/600 loss 6.904028 loss_att 9.434260 loss_ctc 8.718570 loss_rnnt 6.060055 hw_loss 0.179975 lr 0.00033278 rank 6
2023-02-25 10:31:37,733 DEBUG TRAIN Batch 27/600 loss 7.274414 loss_att 7.652193 loss_ctc 10.617492 loss_rnnt 6.605684 hw_loss 0.276432 lr 0.00033276 rank 5
2023-02-25 10:32:51,039 DEBUG TRAIN Batch 27/700 loss 9.805188 loss_att 16.362690 loss_ctc 20.450264 loss_rnnt 6.972227 hw_loss 0.191469 lr 0.00033268 rank 0
2023-02-25 10:32:51,041 DEBUG TRAIN Batch 27/700 loss 13.460328 loss_att 19.179125 loss_ctc 24.525190 loss_rnnt 10.670768 hw_loss 0.319661 lr 0.00033270 rank 6
2023-02-25 10:32:51,049 DEBUG TRAIN Batch 27/700 loss 9.321213 loss_att 13.598640 loss_ctc 11.813640 loss_rnnt 8.106283 hw_loss 0.050851 lr 0.00033275 rank 1
2023-02-25 10:32:51,048 DEBUG TRAIN Batch 27/700 loss 7.059984 loss_att 11.355970 loss_ctc 11.452427 loss_rnnt 5.497869 hw_loss 0.219860 lr 0.00033269 rank 5
2023-02-25 10:32:51,050 DEBUG TRAIN Batch 27/700 loss 2.876826 loss_att 5.054559 loss_ctc 5.732341 loss_rnnt 1.985486 hw_loss 0.140735 lr 0.00033272 rank 2
2023-02-25 10:32:51,070 DEBUG TRAIN Batch 27/700 loss 9.526772 loss_att 13.432726 loss_ctc 12.385183 loss_rnnt 8.267178 hw_loss 0.182404 lr 0.00033270 rank 3
2023-02-25 10:32:51,070 DEBUG TRAIN Batch 27/700 loss 3.502517 loss_att 6.247156 loss_ctc 2.666434 loss_rnnt 2.949878 hw_loss 0.215978 lr 0.00033270 rank 4
2023-02-25 10:32:51,093 DEBUG TRAIN Batch 27/700 loss 5.957978 loss_att 7.577259 loss_ctc 10.782639 loss_rnnt 4.909576 hw_loss 0.152358 lr 0.00033267 rank 7
2023-02-25 10:34:01,189 DEBUG TRAIN Batch 27/800 loss 2.075426 loss_att 4.962366 loss_ctc 1.886413 loss_rnnt 1.409746 hw_loss 0.212801 lr 0.00033263 rank 6
2023-02-25 10:34:01,190 DEBUG TRAIN Batch 27/800 loss 8.364214 loss_att 10.703342 loss_ctc 13.138474 loss_rnnt 7.158257 hw_loss 0.190431 lr 0.00033263 rank 3
2023-02-25 10:34:01,193 DEBUG TRAIN Batch 27/800 loss 6.150757 loss_att 8.817173 loss_ctc 6.600111 loss_rnnt 5.482911 hw_loss 0.139966 lr 0.00033267 rank 1
2023-02-25 10:34:01,193 DEBUG TRAIN Batch 27/800 loss 1.957052 loss_att 4.600156 loss_ctc 1.231016 loss_rnnt 1.451476 hw_loss 0.138301 lr 0.00033260 rank 0
2023-02-25 10:34:01,193 DEBUG TRAIN Batch 27/800 loss 11.677158 loss_att 14.248551 loss_ctc 15.024843 loss_rnnt 10.640781 hw_loss 0.142013 lr 0.00033263 rank 4
2023-02-25 10:34:01,198 DEBUG TRAIN Batch 27/800 loss 3.614320 loss_att 7.507370 loss_ctc 6.776565 loss_rnnt 2.318676 hw_loss 0.178878 lr 0.00033265 rank 2
2023-02-25 10:34:01,199 DEBUG TRAIN Batch 27/800 loss 5.402102 loss_att 8.691405 loss_ctc 8.388559 loss_rnnt 4.243911 hw_loss 0.191505 lr 0.00033260 rank 7
2023-02-25 10:34:01,200 DEBUG TRAIN Batch 27/800 loss 5.514681 loss_att 8.892479 loss_ctc 7.444868 loss_rnnt 4.503530 hw_loss 0.146688 lr 0.00033261 rank 5
2023-02-25 10:35:11,554 DEBUG TRAIN Batch 27/900 loss 9.120747 loss_att 15.018356 loss_ctc 14.836464 loss_rnnt 7.065543 hw_loss 0.212975 lr 0.00033253 rank 0
2023-02-25 10:35:11,558 DEBUG TRAIN Batch 27/900 loss 1.534363 loss_att 4.194030 loss_ctc 1.709682 loss_rnnt 0.862234 hw_loss 0.219036 lr 0.00033256 rank 6
2023-02-25 10:35:11,561 DEBUG TRAIN Batch 27/900 loss 4.270426 loss_att 7.666430 loss_ctc 6.401338 loss_rnnt 3.202973 hw_loss 0.195244 lr 0.00033255 rank 4
2023-02-25 10:35:11,561 DEBUG TRAIN Batch 27/900 loss 5.484969 loss_att 6.637322 loss_ctc 5.886578 loss_rnnt 5.138338 hw_loss 0.117400 lr 0.00033257 rank 2
2023-02-25 10:35:11,564 DEBUG TRAIN Batch 27/900 loss 15.695094 loss_att 18.038399 loss_ctc 23.480988 loss_rnnt 14.097525 hw_loss 0.170231 lr 0.00033256 rank 3
2023-02-25 10:35:11,565 DEBUG TRAIN Batch 27/900 loss 8.892504 loss_att 12.885658 loss_ctc 18.471558 loss_rnnt 6.731484 hw_loss 0.159714 lr 0.00033260 rank 1
2023-02-25 10:35:11,577 DEBUG TRAIN Batch 27/900 loss 4.718765 loss_att 8.087331 loss_ctc 10.374174 loss_rnnt 3.204918 hw_loss 0.161398 lr 0.00033252 rank 7
2023-02-25 10:35:11,611 DEBUG TRAIN Batch 27/900 loss 12.451530 loss_att 14.152095 loss_ctc 15.770128 loss_rnnt 11.524420 hw_loss 0.270973 lr 0.00033254 rank 5
2023-02-25 10:36:23,202 DEBUG TRAIN Batch 27/1000 loss 17.128103 loss_att 17.705091 loss_ctc 23.878016 loss_rnnt 16.081594 hw_loss 0.058354 lr 0.00033246 rank 0
2023-02-25 10:36:23,205 DEBUG TRAIN Batch 27/1000 loss 4.762625 loss_att 8.191483 loss_ctc 5.505541 loss_rnnt 3.867142 hw_loss 0.207480 lr 0.00033248 rank 4
2023-02-25 10:36:23,207 DEBUG TRAIN Batch 27/1000 loss 6.005458 loss_att 11.163519 loss_ctc 10.402746 loss_rnnt 4.291572 hw_loss 0.179943 lr 0.00033247 rank 5
2023-02-25 10:36:23,213 DEBUG TRAIN Batch 27/1000 loss 7.390580 loss_att 10.999553 loss_ctc 14.532823 loss_rnnt 5.667369 hw_loss 0.092095 lr 0.00033250 rank 2
2023-02-25 10:36:23,217 DEBUG TRAIN Batch 27/1000 loss 4.457270 loss_att 7.246436 loss_ctc 7.014608 loss_rnnt 3.439628 hw_loss 0.222806 lr 0.00033245 rank 7
2023-02-25 10:36:23,218 DEBUG TRAIN Batch 27/1000 loss 5.883100 loss_att 7.292622 loss_ctc 9.272053 loss_rnnt 5.057350 hw_loss 0.172471 lr 0.00033253 rank 1
2023-02-25 10:36:23,222 DEBUG TRAIN Batch 27/1000 loss 16.673977 loss_att 20.397877 loss_ctc 20.591274 loss_rnnt 15.285792 hw_loss 0.227061 lr 0.00033248 rank 3
2023-02-25 10:36:23,233 DEBUG TRAIN Batch 27/1000 loss 6.717555 loss_att 8.715667 loss_ctc 6.613647 loss_rnnt 6.211790 hw_loss 0.224993 lr 0.00033248 rank 6
2023-02-25 10:37:35,509 DEBUG TRAIN Batch 27/1100 loss 5.961493 loss_att 8.210789 loss_ctc 8.284803 loss_rnnt 5.090085 hw_loss 0.209575 lr 0.00033238 rank 0
2023-02-25 10:37:35,518 DEBUG TRAIN Batch 27/1100 loss 13.962690 loss_att 14.180101 loss_ctc 21.110594 loss_rnnt 12.872193 hw_loss 0.176178 lr 0.00033243 rank 2
2023-02-25 10:37:35,517 DEBUG TRAIN Batch 27/1100 loss 10.859780 loss_att 13.898418 loss_ctc 16.094614 loss_rnnt 9.477913 hw_loss 0.142802 lr 0.00033245 rank 1
2023-02-25 10:37:35,521 DEBUG TRAIN Batch 27/1100 loss 10.638992 loss_att 13.612605 loss_ctc 22.171120 loss_rnnt 8.425356 hw_loss 0.152431 lr 0.00033241 rank 3
2023-02-25 10:37:35,522 DEBUG TRAIN Batch 27/1100 loss 11.051372 loss_att 13.334186 loss_ctc 11.771778 loss_rnnt 10.406109 hw_loss 0.173711 lr 0.00033237 rank 7
2023-02-25 10:37:35,522 DEBUG TRAIN Batch 27/1100 loss 7.447344 loss_att 10.776745 loss_ctc 11.601830 loss_rnnt 6.181574 hw_loss 0.086171 lr 0.00033241 rank 6
2023-02-25 10:37:35,526 DEBUG TRAIN Batch 27/1100 loss 5.177994 loss_att 8.498159 loss_ctc 7.681130 loss_rnnt 4.128881 hw_loss 0.096240 lr 0.00033241 rank 4
2023-02-25 10:37:35,526 DEBUG TRAIN Batch 27/1100 loss 8.112515 loss_att 11.457345 loss_ctc 13.154938 loss_rnnt 6.648377 hw_loss 0.230341 lr 0.00033239 rank 5
2023-02-25 10:38:46,482 DEBUG TRAIN Batch 27/1200 loss 7.076736 loss_att 9.469151 loss_ctc 13.240137 loss_rnnt 5.684198 hw_loss 0.173003 lr 0.00033231 rank 0
2023-02-25 10:38:46,484 DEBUG TRAIN Batch 27/1200 loss 9.621141 loss_att 12.756998 loss_ctc 13.098111 loss_rnnt 8.446608 hw_loss 0.157063 lr 0.00033230 rank 7
2023-02-25 10:38:46,485 DEBUG TRAIN Batch 27/1200 loss 8.862901 loss_att 8.727917 loss_ctc 11.802073 loss_rnnt 8.395845 hw_loss 0.191556 lr 0.00033232 rank 5
2023-02-25 10:38:46,486 DEBUG TRAIN Batch 27/1200 loss 8.827902 loss_att 9.992731 loss_ctc 11.301792 loss_rnnt 8.162115 hw_loss 0.193067 lr 0.00033234 rank 3
2023-02-25 10:38:46,485 DEBUG TRAIN Batch 27/1200 loss 6.729517 loss_att 8.964068 loss_ctc 10.349161 loss_rnnt 5.675232 hw_loss 0.233916 lr 0.00033234 rank 6
2023-02-25 10:38:46,485 DEBUG TRAIN Batch 27/1200 loss 7.170933 loss_att 7.904700 loss_ctc 8.677677 loss_rnnt 6.694143 hw_loss 0.242131 lr 0.00033238 rank 1
2023-02-25 10:38:46,488 DEBUG TRAIN Batch 27/1200 loss 6.013117 loss_att 9.944678 loss_ctc 9.647943 loss_rnnt 4.625654 hw_loss 0.218452 lr 0.00033235 rank 2
2023-02-25 10:38:46,492 DEBUG TRAIN Batch 27/1200 loss 5.118342 loss_att 7.206186 loss_ctc 6.184938 loss_rnnt 4.479569 hw_loss 0.148109 lr 0.00033233 rank 4
2023-02-25 10:39:57,070 DEBUG TRAIN Batch 27/1300 loss 11.782726 loss_att 13.081676 loss_ctc 16.252756 loss_rnnt 10.873492 hw_loss 0.100200 lr 0.00033224 rank 0
2023-02-25 10:39:57,075 DEBUG TRAIN Batch 27/1300 loss 6.693848 loss_att 7.347348 loss_ctc 8.888877 loss_rnnt 6.128335 hw_loss 0.266517 lr 0.00033226 rank 4
2023-02-25 10:39:57,076 DEBUG TRAIN Batch 27/1300 loss 6.830910 loss_att 9.441618 loss_ctc 7.584177 loss_rnnt 6.121797 hw_loss 0.162254 lr 0.00033226 rank 3
2023-02-25 10:39:57,076 DEBUG TRAIN Batch 27/1300 loss 7.947699 loss_att 10.736232 loss_ctc 12.042986 loss_rnnt 6.653165 hw_loss 0.357730 lr 0.00033226 rank 6
2023-02-25 10:39:57,077 DEBUG TRAIN Batch 27/1300 loss 7.522030 loss_att 12.929132 loss_ctc 19.271912 loss_rnnt 4.799707 hw_loss 0.139221 lr 0.00033223 rank 7
2023-02-25 10:39:57,077 DEBUG TRAIN Batch 27/1300 loss 6.762311 loss_att 9.364408 loss_ctc 8.290908 loss_rnnt 5.969578 hw_loss 0.128441 lr 0.00033228 rank 2
2023-02-25 10:39:57,078 DEBUG TRAIN Batch 27/1300 loss 6.921664 loss_att 8.426224 loss_ctc 10.607170 loss_rnnt 6.059638 hw_loss 0.130713 lr 0.00033231 rank 1
2023-02-25 10:39:57,124 DEBUG TRAIN Batch 27/1300 loss 14.086393 loss_att 12.679880 loss_ctc 18.819885 loss_rnnt 13.620772 hw_loss 0.217110 lr 0.00033225 rank 5
2023-02-25 10:41:09,421 DEBUG TRAIN Batch 27/1400 loss 9.468494 loss_att 12.448181 loss_ctc 14.450210 loss_rnnt 8.088141 hw_loss 0.225350 lr 0.00033219 rank 6
2023-02-25 10:41:09,428 DEBUG TRAIN Batch 27/1400 loss 15.341867 loss_att 16.396824 loss_ctc 22.093536 loss_rnnt 14.185615 hw_loss 0.084449 lr 0.00033216 rank 0
2023-02-25 10:41:09,429 DEBUG TRAIN Batch 27/1400 loss 4.346206 loss_att 6.907072 loss_ctc 4.713579 loss_rnnt 3.734293 hw_loss 0.095169 lr 0.00033215 rank 7
2023-02-25 10:41:09,430 DEBUG TRAIN Batch 27/1400 loss 2.891304 loss_att 4.160353 loss_ctc 3.771014 loss_rnnt 2.359588 hw_loss 0.301147 lr 0.00033221 rank 2
2023-02-25 10:41:09,434 DEBUG TRAIN Batch 27/1400 loss 4.004819 loss_att 6.502104 loss_ctc 5.076514 loss_rnnt 3.281228 hw_loss 0.152330 lr 0.00033223 rank 1
2023-02-25 10:41:09,439 DEBUG TRAIN Batch 27/1400 loss 17.090784 loss_att 22.613945 loss_ctc 26.495090 loss_rnnt 14.650356 hw_loss 0.153536 lr 0.00033219 rank 3
2023-02-25 10:41:09,446 DEBUG TRAIN Batch 27/1400 loss 5.066422 loss_att 8.747448 loss_ctc 6.043628 loss_rnnt 4.083608 hw_loss 0.218091 lr 0.00033219 rank 4
2023-02-25 10:41:09,452 DEBUG TRAIN Batch 27/1400 loss 6.700228 loss_att 11.671133 loss_ctc 14.352866 loss_rnnt 4.521362 hw_loss 0.308124 lr 0.00033217 rank 5
2023-02-25 10:42:21,213 DEBUG TRAIN Batch 27/1500 loss 5.336009 loss_att 8.473645 loss_ctc 9.427761 loss_rnnt 4.074936 hw_loss 0.164959 lr 0.00033213 rank 2
2023-02-25 10:42:21,214 DEBUG TRAIN Batch 27/1500 loss 9.648268 loss_att 11.058999 loss_ctc 11.369151 loss_rnnt 8.985514 hw_loss 0.283417 lr 0.00033212 rank 3
2023-02-25 10:42:21,214 DEBUG TRAIN Batch 27/1500 loss 9.417520 loss_att 10.192660 loss_ctc 12.657056 loss_rnnt 8.731363 hw_loss 0.185981 lr 0.00033209 rank 0
2023-02-25 10:42:21,218 DEBUG TRAIN Batch 27/1500 loss 11.719023 loss_att 14.494217 loss_ctc 15.147388 loss_rnnt 10.575865 hw_loss 0.245634 lr 0.00033216 rank 1
2023-02-25 10:42:21,222 DEBUG TRAIN Batch 27/1500 loss 5.086497 loss_att 9.391479 loss_ctc 10.811774 loss_rnnt 3.377594 hw_loss 0.158505 lr 0.00033212 rank 6
2023-02-25 10:42:21,224 DEBUG TRAIN Batch 27/1500 loss 5.572530 loss_att 8.840938 loss_ctc 8.116823 loss_rnnt 4.519623 hw_loss 0.112474 lr 0.00033208 rank 7
2023-02-25 10:42:21,227 DEBUG TRAIN Batch 27/1500 loss 5.051626 loss_att 8.994936 loss_ctc 10.079893 loss_rnnt 3.469696 hw_loss 0.230312 lr 0.00033210 rank 5
2023-02-25 10:42:21,273 DEBUG TRAIN Batch 27/1500 loss 6.206961 loss_att 9.445768 loss_ctc 12.246252 loss_rnnt 4.668163 hw_loss 0.160871 lr 0.00033211 rank 4
2023-02-25 10:43:31,078 DEBUG TRAIN Batch 27/1600 loss 18.531683 loss_att 19.931356 loss_ctc 26.230812 loss_rnnt 17.136091 hw_loss 0.167074 lr 0.00033204 rank 6
2023-02-25 10:43:31,081 DEBUG TRAIN Batch 27/1600 loss 9.404534 loss_att 10.984769 loss_ctc 13.178270 loss_rnnt 8.449179 hw_loss 0.255269 lr 0.00033201 rank 7
2023-02-25 10:43:31,082 DEBUG TRAIN Batch 27/1600 loss 8.132709 loss_att 11.802252 loss_ctc 15.140205 loss_rnnt 6.367688 hw_loss 0.181460 lr 0.00033203 rank 5
2023-02-25 10:43:31,084 DEBUG TRAIN Batch 27/1600 loss 8.231982 loss_att 9.457795 loss_ctc 10.009563 loss_rnnt 7.662455 hw_loss 0.163788 lr 0.00033209 rank 1
2023-02-25 10:43:31,085 DEBUG TRAIN Batch 27/1600 loss 7.901673 loss_att 11.484804 loss_ctc 10.411757 loss_rnnt 6.704955 hw_loss 0.272650 lr 0.00033204 rank 4
2023-02-25 10:43:31,085 DEBUG TRAIN Batch 27/1600 loss 9.226749 loss_att 11.339873 loss_ctc 12.265733 loss_rnnt 8.352995 hw_loss 0.086122 lr 0.00033202 rank 0
2023-02-25 10:43:31,087 DEBUG TRAIN Batch 27/1600 loss 16.569887 loss_att 27.737122 loss_ctc 23.438948 loss_rnnt 13.334351 hw_loss 0.161651 lr 0.00033206 rank 2
2023-02-25 10:43:31,091 DEBUG TRAIN Batch 27/1600 loss 20.444839 loss_att 24.150967 loss_ctc 30.468700 loss_rnnt 18.263000 hw_loss 0.195187 lr 0.00033204 rank 3
2023-02-25 10:44:42,091 DEBUG TRAIN Batch 27/1700 loss 3.768443 loss_att 5.730632 loss_ctc 5.087176 loss_rnnt 3.091722 hw_loss 0.203348 lr 0.00033201 rank 1
2023-02-25 10:44:42,099 DEBUG TRAIN Batch 27/1700 loss 14.314914 loss_att 18.917706 loss_ctc 22.518196 loss_rnnt 12.194319 hw_loss 0.199249 lr 0.00033197 rank 6
2023-02-25 10:44:42,100 DEBUG TRAIN Batch 27/1700 loss 3.476548 loss_att 5.945269 loss_ctc 4.569696 loss_rnnt 2.716846 hw_loss 0.225386 lr 0.00033197 rank 3
2023-02-25 10:44:42,101 DEBUG TRAIN Batch 27/1700 loss 9.052152 loss_att 9.954792 loss_ctc 12.448419 loss_rnnt 8.337241 hw_loss 0.152901 lr 0.00033197 rank 4
2023-02-25 10:44:42,105 DEBUG TRAIN Batch 27/1700 loss 12.671177 loss_att 18.243790 loss_ctc 17.590801 loss_rnnt 10.797261 hw_loss 0.193955 lr 0.00033199 rank 2
2023-02-25 10:44:42,137 DEBUG TRAIN Batch 27/1700 loss 11.768439 loss_att 14.782515 loss_ctc 15.570906 loss_rnnt 10.581807 hw_loss 0.144040 lr 0.00033194 rank 0
2023-02-25 10:44:42,147 DEBUG TRAIN Batch 27/1700 loss 14.378017 loss_att 19.673857 loss_ctc 22.590105 loss_rnnt 12.136834 hw_loss 0.163257 lr 0.00033195 rank 5
2023-02-25 10:44:42,150 DEBUG TRAIN Batch 27/1700 loss 7.375590 loss_att 10.140671 loss_ctc 9.355981 loss_rnnt 6.445148 hw_loss 0.212575 lr 0.00033193 rank 7
2023-02-25 10:45:56,204 DEBUG TRAIN Batch 27/1800 loss 7.462646 loss_att 8.196649 loss_ctc 9.212021 loss_rnnt 7.007951 hw_loss 0.139956 lr 0.00033186 rank 7
2023-02-25 10:45:56,205 DEBUG TRAIN Batch 27/1800 loss 6.254489 loss_att 7.449745 loss_ctc 8.308993 loss_rnnt 5.600594 hw_loss 0.264209 lr 0.00033187 rank 0
2023-02-25 10:45:56,210 DEBUG TRAIN Batch 27/1800 loss 8.304988 loss_att 11.924121 loss_ctc 9.020841 loss_rnnt 7.413268 hw_loss 0.135836 lr 0.00033190 rank 6
2023-02-25 10:45:56,212 DEBUG TRAIN Batch 27/1800 loss 10.414187 loss_att 13.397194 loss_ctc 13.499891 loss_rnnt 9.306267 hw_loss 0.187300 lr 0.00033194 rank 1
2023-02-25 10:45:56,214 DEBUG TRAIN Batch 27/1800 loss 3.972934 loss_att 6.885506 loss_ctc 5.001387 loss_rnnt 3.153959 hw_loss 0.186249 lr 0.00033191 rank 2
2023-02-25 10:45:56,214 DEBUG TRAIN Batch 27/1800 loss 7.601091 loss_att 12.356676 loss_ctc 13.681193 loss_rnnt 5.743707 hw_loss 0.179227 lr 0.00033190 rank 3
2023-02-25 10:45:56,214 DEBUG TRAIN Batch 27/1800 loss 5.267293 loss_att 7.583149 loss_ctc 8.959610 loss_rnnt 4.189643 hw_loss 0.229068 lr 0.00033189 rank 4
2023-02-25 10:45:56,217 DEBUG TRAIN Batch 27/1800 loss 2.563264 loss_att 4.177169 loss_ctc 3.552682 loss_rnnt 2.024502 hw_loss 0.157611 lr 0.00033188 rank 5
2023-02-25 10:47:07,161 DEBUG TRAIN Batch 27/1900 loss 2.462962 loss_att 6.836530 loss_ctc 3.736946 loss_rnnt 1.312388 hw_loss 0.198743 lr 0.00033180 rank 0
2023-02-25 10:47:07,162 DEBUG TRAIN Batch 27/1900 loss 8.654957 loss_att 10.353335 loss_ctc 14.920746 loss_rnnt 7.378764 hw_loss 0.189525 lr 0.00033182 rank 6
2023-02-25 10:47:07,163 DEBUG TRAIN Batch 27/1900 loss 3.631965 loss_att 6.133603 loss_ctc 4.824919 loss_rnnt 2.886813 hw_loss 0.160808 lr 0.00033179 rank 7
2023-02-25 10:47:07,166 DEBUG TRAIN Batch 27/1900 loss 11.814218 loss_att 13.068359 loss_ctc 17.505924 loss_rnnt 10.682400 hw_loss 0.228928 lr 0.00033187 rank 1
2023-02-25 10:47:07,166 DEBUG TRAIN Batch 27/1900 loss 8.058490 loss_att 9.457104 loss_ctc 9.696630 loss_rnnt 7.412980 hw_loss 0.276313 lr 0.00033182 rank 4
2023-02-25 10:47:07,172 DEBUG TRAIN Batch 27/1900 loss 11.457191 loss_att 11.916410 loss_ctc 13.418148 loss_rnnt 11.007132 hw_loss 0.181417 lr 0.00033181 rank 5
2023-02-25 10:47:07,173 DEBUG TRAIN Batch 27/1900 loss 5.778780 loss_att 7.834349 loss_ctc 9.361195 loss_rnnt 4.733042 hw_loss 0.294317 lr 0.00033184 rank 2
2023-02-25 10:47:07,211 DEBUG TRAIN Batch 27/1900 loss 10.331294 loss_att 10.588573 loss_ctc 14.926780 loss_rnnt 9.585910 hw_loss 0.152245 lr 0.00033182 rank 3
2023-02-25 10:48:17,556 DEBUG TRAIN Batch 27/2000 loss 12.057943 loss_att 17.277401 loss_ctc 14.274275 loss_rnnt 10.642352 hw_loss 0.142855 lr 0.00033172 rank 0
2023-02-25 10:48:17,556 DEBUG TRAIN Batch 27/2000 loss 6.407961 loss_att 9.011440 loss_ctc 11.861282 loss_rnnt 5.077975 hw_loss 0.154088 lr 0.00033179 rank 1
2023-02-25 10:48:17,559 DEBUG TRAIN Batch 27/2000 loss 2.130575 loss_att 6.807434 loss_ctc 3.506293 loss_rnnt 0.914721 hw_loss 0.181974 lr 0.00033172 rank 7
2023-02-25 10:48:17,561 DEBUG TRAIN Batch 27/2000 loss 8.419197 loss_att 13.907151 loss_ctc 19.146721 loss_rnnt 5.785192 hw_loss 0.198896 lr 0.00033175 rank 6
2023-02-25 10:48:17,561 DEBUG TRAIN Batch 27/2000 loss 9.611234 loss_att 14.450987 loss_ctc 9.171458 loss_rnnt 8.607325 hw_loss 0.177367 lr 0.00033175 rank 3
2023-02-25 10:48:17,563 DEBUG TRAIN Batch 27/2000 loss 4.600787 loss_att 7.197767 loss_ctc 5.309772 loss_rnnt 3.905180 hw_loss 0.153151 lr 0.00033175 rank 4
2023-02-25 10:48:17,563 DEBUG TRAIN Batch 27/2000 loss 3.171049 loss_att 6.900844 loss_ctc 7.158077 loss_rnnt 1.820455 hw_loss 0.136933 lr 0.00033177 rank 2
2023-02-25 10:48:17,567 DEBUG TRAIN Batch 27/2000 loss 5.465067 loss_att 9.394002 loss_ctc 10.219135 loss_rnnt 3.955448 hw_loss 0.168668 lr 0.00033173 rank 5
2023-02-25 10:49:30,461 DEBUG TRAIN Batch 27/2100 loss 1.369203 loss_att 2.830522 loss_ctc 1.420850 loss_rnnt 0.991424 hw_loss 0.147431 lr 0.00033164 rank 7
2023-02-25 10:49:30,466 DEBUG TRAIN Batch 27/2100 loss 16.386616 loss_att 19.959396 loss_ctc 25.737782 loss_rnnt 14.304641 hw_loss 0.226120 lr 0.00033168 rank 3
2023-02-25 10:49:30,468 DEBUG TRAIN Batch 27/2100 loss 4.100272 loss_att 6.206627 loss_ctc 7.831848 loss_rnnt 3.024955 hw_loss 0.293441 lr 0.00033165 rank 0
2023-02-25 10:49:30,469 DEBUG TRAIN Batch 27/2100 loss 7.866341 loss_att 9.237964 loss_ctc 15.961943 loss_rnnt 6.399076 hw_loss 0.212864 lr 0.00033168 rank 6
2023-02-25 10:49:30,470 DEBUG TRAIN Batch 27/2100 loss 8.638480 loss_att 12.174541 loss_ctc 11.316139 loss_rnnt 7.449392 hw_loss 0.234101 lr 0.00033166 rank 5
2023-02-25 10:49:30,470 DEBUG TRAIN Batch 27/2100 loss 4.681971 loss_att 8.747508 loss_ctc 8.798268 loss_rnnt 3.247986 hw_loss 0.135069 lr 0.00033169 rank 2
2023-02-25 10:49:30,471 DEBUG TRAIN Batch 27/2100 loss 5.271551 loss_att 7.176756 loss_ctc 5.822713 loss_rnnt 4.730321 hw_loss 0.162562 lr 0.00033167 rank 4
2023-02-25 10:49:30,480 DEBUG TRAIN Batch 27/2100 loss 11.144224 loss_att 10.833743 loss_ctc 10.520342 loss_rnnt 11.142543 hw_loss 0.275555 lr 0.00033172 rank 1
2023-02-25 10:50:41,837 DEBUG TRAIN Batch 27/2200 loss 14.333378 loss_att 17.834171 loss_ctc 19.699507 loss_rnnt 12.854072 hw_loss 0.119370 lr 0.00033165 rank 1
2023-02-25 10:50:41,851 DEBUG TRAIN Batch 27/2200 loss 8.996704 loss_att 13.636965 loss_ctc 10.983889 loss_rnnt 7.737204 hw_loss 0.124670 lr 0.00033160 rank 3
2023-02-25 10:50:41,855 DEBUG TRAIN Batch 27/2200 loss 5.940811 loss_att 8.944417 loss_ctc 7.540794 loss_rnnt 5.015378 hw_loss 0.208837 lr 0.00033157 rank 7
2023-02-25 10:50:41,856 DEBUG TRAIN Batch 27/2200 loss 3.621683 loss_att 5.709856 loss_ctc 4.396207 loss_rnnt 3.021281 hw_loss 0.149058 lr 0.00033162 rank 2
2023-02-25 10:50:41,859 DEBUG TRAIN Batch 27/2200 loss 5.543301 loss_att 7.255713 loss_ctc 8.173374 loss_rnnt 4.758661 hw_loss 0.171527 lr 0.00033158 rank 0
2023-02-25 10:50:41,859 DEBUG TRAIN Batch 27/2200 loss 4.830740 loss_att 8.696227 loss_ctc 6.534854 loss_rnnt 3.702807 hw_loss 0.239290 lr 0.00033160 rank 4
2023-02-25 10:50:41,862 DEBUG TRAIN Batch 27/2200 loss 6.312733 loss_att 8.017889 loss_ctc 7.185397 loss_rnnt 5.728655 hw_loss 0.237546 lr 0.00033159 rank 5
2023-02-25 10:50:41,864 DEBUG TRAIN Batch 27/2200 loss 11.957320 loss_att 14.199917 loss_ctc 13.975734 loss_rnnt 11.179790 hw_loss 0.112291 lr 0.00033160 rank 6
2023-02-25 10:51:51,852 DEBUG TRAIN Batch 27/2300 loss 7.450216 loss_att 11.732449 loss_ctc 10.015352 loss_rnnt 6.112889 hw_loss 0.260366 lr 0.00033153 rank 6
2023-02-25 10:51:51,854 DEBUG TRAIN Batch 27/2300 loss 6.791900 loss_att 10.912501 loss_ctc 12.953728 loss_rnnt 5.025252 hw_loss 0.226782 lr 0.00033153 rank 3
2023-02-25 10:51:51,855 DEBUG TRAIN Batch 27/2300 loss 6.116680 loss_att 8.783286 loss_ctc 9.562810 loss_rnnt 5.023678 hw_loss 0.187868 lr 0.00033151 rank 0
2023-02-25 10:51:51,858 DEBUG TRAIN Batch 27/2300 loss 13.891275 loss_att 16.932161 loss_ctc 22.248306 loss_rnnt 12.086888 hw_loss 0.153633 lr 0.00033155 rank 2
2023-02-25 10:51:51,858 DEBUG TRAIN Batch 27/2300 loss 4.601508 loss_att 6.765859 loss_ctc 8.460121 loss_rnnt 3.548809 hw_loss 0.197525 lr 0.00033152 rank 5
2023-02-25 10:51:51,858 DEBUG TRAIN Batch 27/2300 loss 8.154154 loss_att 10.282769 loss_ctc 11.675305 loss_rnnt 7.203740 hw_loss 0.103508 lr 0.00033150 rank 7
2023-02-25 10:51:51,859 DEBUG TRAIN Batch 27/2300 loss 9.088295 loss_att 10.292920 loss_ctc 8.082367 loss_rnnt 8.871770 hw_loss 0.205732 lr 0.00033157 rank 1
2023-02-25 10:51:51,861 DEBUG TRAIN Batch 27/2300 loss 6.488081 loss_att 9.689588 loss_ctc 11.106574 loss_rnnt 5.191895 hw_loss 0.075160 lr 0.00033153 rank 4
2023-02-25 10:53:03,284 DEBUG TRAIN Batch 27/2400 loss 7.445073 loss_att 9.620504 loss_ctc 8.896680 loss_rnnt 6.703160 hw_loss 0.212398 lr 0.00033146 rank 6
2023-02-25 10:53:03,292 DEBUG TRAIN Batch 27/2400 loss 6.180617 loss_att 9.559151 loss_ctc 9.174241 loss_rnnt 4.962889 hw_loss 0.267884 lr 0.00033146 rank 3
2023-02-25 10:53:03,297 DEBUG TRAIN Batch 27/2400 loss 16.592289 loss_att 17.427780 loss_ctc 23.829603 loss_rnnt 15.358874 hw_loss 0.190013 lr 0.00033143 rank 0
2023-02-25 10:53:03,298 DEBUG TRAIN Batch 27/2400 loss 2.275799 loss_att 5.063519 loss_ctc 5.845808 loss_rnnt 1.104099 hw_loss 0.259040 lr 0.00033142 rank 7
2023-02-25 10:53:03,299 DEBUG TRAIN Batch 27/2400 loss 17.255339 loss_att 18.413410 loss_ctc 20.921093 loss_rnnt 16.464827 hw_loss 0.131495 lr 0.00033144 rank 5
2023-02-25 10:53:03,302 DEBUG TRAIN Batch 27/2400 loss 4.778425 loss_att 9.132133 loss_ctc 7.516036 loss_rnnt 3.440019 hw_loss 0.192468 lr 0.00033146 rank 4
2023-02-25 10:53:03,302 DEBUG TRAIN Batch 27/2400 loss 13.729247 loss_att 17.988230 loss_ctc 25.351336 loss_rnnt 11.203110 hw_loss 0.233866 lr 0.00033148 rank 2
2023-02-25 10:53:03,302 DEBUG TRAIN Batch 27/2400 loss 5.827548 loss_att 9.829222 loss_ctc 10.760801 loss_rnnt 4.269785 hw_loss 0.186862 lr 0.00033150 rank 1
2023-02-25 10:54:17,259 DEBUG TRAIN Batch 27/2500 loss 4.902659 loss_att 7.919871 loss_ctc 6.735568 loss_rnnt 3.911716 hw_loss 0.268337 lr 0.00033139 rank 6
2023-02-25 10:54:17,259 DEBUG TRAIN Batch 27/2500 loss 13.236203 loss_att 17.687063 loss_ctc 18.983572 loss_rnnt 11.439247 hw_loss 0.263378 lr 0.00033140 rank 2
2023-02-25 10:54:17,262 DEBUG TRAIN Batch 27/2500 loss 4.829092 loss_att 10.877145 loss_ctc 10.388897 loss_rnnt 2.788790 hw_loss 0.167597 lr 0.00033136 rank 0
2023-02-25 10:54:17,262 DEBUG TRAIN Batch 27/2500 loss 5.616407 loss_att 7.244861 loss_ctc 8.038664 loss_rnnt 4.838478 hw_loss 0.242385 lr 0.00033139 rank 3
2023-02-25 10:54:17,263 DEBUG TRAIN Batch 27/2500 loss 7.509783 loss_att 9.671116 loss_ctc 11.778352 loss_rnnt 6.353736 hw_loss 0.289946 lr 0.00033143 rank 1
2023-02-25 10:54:17,266 DEBUG TRAIN Batch 27/2500 loss 5.244260 loss_att 6.851219 loss_ctc 6.790750 loss_rnnt 4.588978 hw_loss 0.239422 lr 0.00033135 rank 7
2023-02-25 10:54:17,269 DEBUG TRAIN Batch 27/2500 loss 3.904702 loss_att 6.319871 loss_ctc 5.334141 loss_rnnt 3.149693 hw_loss 0.152593 lr 0.00033137 rank 5
2023-02-25 10:54:17,313 DEBUG TRAIN Batch 27/2500 loss 8.569514 loss_att 10.406674 loss_ctc 12.128969 loss_rnnt 7.575257 hw_loss 0.285432 lr 0.00033138 rank 4
2023-02-25 10:55:27,544 DEBUG TRAIN Batch 27/2600 loss 4.104215 loss_att 5.022679 loss_ctc 5.867157 loss_rnnt 3.591166 hw_loss 0.176807 lr 0.00033136 rank 1
2023-02-25 10:55:27,557 DEBUG TRAIN Batch 27/2600 loss 2.638029 loss_att 5.765395 loss_ctc 3.781649 loss_rnnt 1.790763 hw_loss 0.129957 lr 0.00033129 rank 0
2023-02-25 10:55:27,560 DEBUG TRAIN Batch 27/2600 loss 12.121282 loss_att 14.392439 loss_ctc 22.240040 loss_rnnt 10.202478 hw_loss 0.216383 lr 0.00033128 rank 7
2023-02-25 10:55:27,562 DEBUG TRAIN Batch 27/2600 loss 8.536777 loss_att 9.106283 loss_ctc 12.673777 loss_rnnt 7.726551 hw_loss 0.271355 lr 0.00033131 rank 6
2023-02-25 10:55:27,565 DEBUG TRAIN Batch 27/2600 loss 10.375148 loss_att 10.517286 loss_ctc 14.462855 loss_rnnt 9.657337 hw_loss 0.270666 lr 0.00033131 rank 4
2023-02-25 10:55:27,565 DEBUG TRAIN Batch 27/2600 loss 12.266585 loss_att 16.691057 loss_ctc 17.048777 loss_rnnt 10.630968 hw_loss 0.212056 lr 0.00033131 rank 3
2023-02-25 10:55:27,566 DEBUG TRAIN Batch 27/2600 loss 11.603520 loss_att 14.551722 loss_ctc 16.807087 loss_rnnt 10.275441 hw_loss 0.083679 lr 0.00033133 rank 2
2023-02-25 10:55:27,568 DEBUG TRAIN Batch 27/2600 loss 7.105554 loss_att 7.539132 loss_ctc 8.449847 loss_rnnt 6.671159 hw_loss 0.315826 lr 0.00033130 rank 5
2023-02-25 10:56:37,850 DEBUG TRAIN Batch 27/2700 loss 10.949005 loss_att 11.950838 loss_ctc 16.243570 loss_rnnt 9.971527 hw_loss 0.133443 lr 0.00033126 rank 2
2023-02-25 10:56:37,850 DEBUG TRAIN Batch 27/2700 loss 3.289203 loss_att 6.987530 loss_ctc 4.792113 loss_rnnt 2.271681 hw_loss 0.145254 lr 0.00033121 rank 0
2023-02-25 10:56:37,852 DEBUG TRAIN Batch 27/2700 loss 10.298251 loss_att 13.640171 loss_ctc 11.232807 loss_rnnt 9.402388 hw_loss 0.192883 lr 0.00033121 rank 7
2023-02-25 10:56:37,854 DEBUG TRAIN Batch 27/2700 loss 13.698834 loss_att 18.956163 loss_ctc 21.292517 loss_rnnt 11.541438 hw_loss 0.175200 lr 0.00033124 rank 4
2023-02-25 10:56:37,854 DEBUG TRAIN Batch 27/2700 loss 9.370712 loss_att 12.407370 loss_ctc 14.101749 loss_rnnt 8.028093 hw_loss 0.195903 lr 0.00033128 rank 1
2023-02-25 10:56:37,857 DEBUG TRAIN Batch 27/2700 loss 7.510209 loss_att 13.130329 loss_ctc 13.851610 loss_rnnt 5.415350 hw_loss 0.234965 lr 0.00033124 rank 3
2023-02-25 10:56:37,859 DEBUG TRAIN Batch 27/2700 loss 7.921425 loss_att 13.606711 loss_ctc 16.299797 loss_rnnt 5.571023 hw_loss 0.180430 lr 0.00033122 rank 5
2023-02-25 10:56:37,899 DEBUG TRAIN Batch 27/2700 loss 2.309019 loss_att 6.211701 loss_ctc 3.498982 loss_rnnt 1.271208 hw_loss 0.184898 lr 0.00033124 rank 6
2023-02-25 10:57:50,473 DEBUG TRAIN Batch 27/2800 loss 8.514598 loss_att 15.823153 loss_ctc 11.126945 loss_rnnt 6.568510 hw_loss 0.255120 lr 0.00033117 rank 3
2023-02-25 10:57:50,474 DEBUG TRAIN Batch 27/2800 loss 2.836453 loss_att 5.516506 loss_ctc 5.944162 loss_rnnt 1.767492 hw_loss 0.222355 lr 0.00033117 rank 6
2023-02-25 10:57:50,476 DEBUG TRAIN Batch 27/2800 loss 8.136988 loss_att 10.335616 loss_ctc 12.082369 loss_rnnt 7.043185 hw_loss 0.240048 lr 0.00033116 rank 4
2023-02-25 10:57:50,477 DEBUG TRAIN Batch 27/2800 loss 3.124458 loss_att 8.582856 loss_ctc 3.491515 loss_rnnt 1.918180 hw_loss 0.123107 lr 0.00033113 rank 7
2023-02-25 10:57:50,478 DEBUG TRAIN Batch 27/2800 loss 6.733308 loss_att 12.074539 loss_ctc 11.821814 loss_rnnt 4.879194 hw_loss 0.201376 lr 0.00033114 rank 0
2023-02-25 10:57:50,485 DEBUG TRAIN Batch 27/2800 loss 2.357780 loss_att 4.874237 loss_ctc 4.755105 loss_rnnt 1.478858 hw_loss 0.104977 lr 0.00033121 rank 1
2023-02-25 10:57:50,486 DEBUG TRAIN Batch 27/2800 loss 3.266059 loss_att 4.053481 loss_ctc 5.029321 loss_rnnt 2.762545 hw_loss 0.207991 lr 0.00033118 rank 2
2023-02-25 10:57:50,521 DEBUG TRAIN Batch 27/2800 loss 9.150183 loss_att 11.157673 loss_ctc 16.466141 loss_rnnt 7.690675 hw_loss 0.154779 lr 0.00033115 rank 5
2023-02-25 10:59:02,866 DEBUG TRAIN Batch 27/2900 loss 7.533176 loss_att 12.163194 loss_ctc 10.160648 loss_rnnt 6.149147 hw_loss 0.201931 lr 0.00033110 rank 3
2023-02-25 10:59:02,869 DEBUG TRAIN Batch 27/2900 loss 3.914923 loss_att 8.684382 loss_ctc 7.138552 loss_rnnt 2.415227 hw_loss 0.217476 lr 0.00033109 rank 4
2023-02-25 10:59:02,870 DEBUG TRAIN Batch 27/2900 loss 3.994890 loss_att 7.312977 loss_ctc 7.131004 loss_rnnt 2.793551 hw_loss 0.224200 lr 0.00033108 rank 5
2023-02-25 10:59:02,871 DEBUG TRAIN Batch 27/2900 loss 6.706715 loss_att 7.834782 loss_ctc 7.821082 loss_rnnt 6.201915 hw_loss 0.244884 lr 0.00033106 rank 7
2023-02-25 10:59:02,871 DEBUG TRAIN Batch 27/2900 loss 9.895508 loss_att 11.884287 loss_ctc 16.167797 loss_rnnt 8.564461 hw_loss 0.181848 lr 0.00033114 rank 1
2023-02-25 10:59:02,872 DEBUG TRAIN Batch 27/2900 loss 10.286036 loss_att 12.288288 loss_ctc 12.968814 loss_rnnt 9.399968 hw_loss 0.239835 lr 0.00033109 rank 6
2023-02-25 10:59:02,872 DEBUG TRAIN Batch 27/2900 loss 6.033956 loss_att 8.063281 loss_ctc 8.686857 loss_rnnt 5.223869 hw_loss 0.094689 lr 0.00033107 rank 0
2023-02-25 10:59:02,874 DEBUG TRAIN Batch 27/2900 loss 15.127310 loss_att 17.922436 loss_ctc 30.109650 loss_rnnt 12.496178 hw_loss 0.139613 lr 0.00033111 rank 2
2023-02-25 11:00:14,004 DEBUG TRAIN Batch 27/3000 loss 5.813160 loss_att 7.492012 loss_ctc 8.571687 loss_rnnt 5.044388 hw_loss 0.122248 lr 0.00033102 rank 3
2023-02-25 11:00:14,010 DEBUG TRAIN Batch 27/3000 loss 4.756356 loss_att 7.918631 loss_ctc 5.939834 loss_rnnt 3.837027 hw_loss 0.242020 lr 0.00033102 rank 6
2023-02-25 11:00:14,009 DEBUG TRAIN Batch 27/3000 loss 9.526384 loss_att 12.585503 loss_ctc 15.804901 loss_rnnt 7.927289 hw_loss 0.281506 lr 0.00033100 rank 0
2023-02-25 11:00:14,011 DEBUG TRAIN Batch 27/3000 loss 6.883106 loss_att 9.544085 loss_ctc 11.138367 loss_rnnt 5.677011 hw_loss 0.199747 lr 0.00033099 rank 7
2023-02-25 11:00:14,011 DEBUG TRAIN Batch 27/3000 loss 18.478773 loss_att 22.772184 loss_ctc 24.502010 loss_rnnt 16.714882 hw_loss 0.191455 lr 0.00033101 rank 5
2023-02-25 11:00:14,012 DEBUG TRAIN Batch 27/3000 loss 7.267462 loss_att 8.979527 loss_ctc 10.884049 loss_rnnt 6.358416 hw_loss 0.158291 lr 0.00033104 rank 2
2023-02-25 11:00:14,013 DEBUG TRAIN Batch 27/3000 loss 14.672487 loss_att 19.437267 loss_ctc 24.759487 loss_rnnt 12.257489 hw_loss 0.219579 lr 0.00033102 rank 4
2023-02-25 11:00:14,015 DEBUG TRAIN Batch 27/3000 loss 9.125710 loss_att 14.584785 loss_ctc 14.481104 loss_rnnt 7.210014 hw_loss 0.205927 lr 0.00033106 rank 1
2023-02-25 11:01:25,348 DEBUG TRAIN Batch 27/3100 loss 9.891778 loss_att 13.499594 loss_ctc 16.131889 loss_rnnt 8.212707 hw_loss 0.235303 lr 0.00033095 rank 3
2023-02-25 11:01:25,349 DEBUG TRAIN Batch 27/3100 loss 10.866229 loss_att 11.854834 loss_ctc 12.362024 loss_rnnt 10.385149 hw_loss 0.157348 lr 0.00033095 rank 4
2023-02-25 11:01:25,350 DEBUG TRAIN Batch 27/3100 loss 13.637389 loss_att 17.743000 loss_ctc 20.453178 loss_rnnt 11.798940 hw_loss 0.203541 lr 0.00033097 rank 2
2023-02-25 11:01:25,351 DEBUG TRAIN Batch 27/3100 loss 7.275766 loss_att 7.255385 loss_ctc 8.651111 loss_rnnt 6.956036 hw_loss 0.263300 lr 0.00033092 rank 0
2023-02-25 11:01:25,353 DEBUG TRAIN Batch 27/3100 loss 5.601185 loss_att 7.445827 loss_ctc 5.773426 loss_rnnt 5.085827 hw_loss 0.231496 lr 0.00033093 rank 5
2023-02-25 11:01:25,353 DEBUG TRAIN Batch 27/3100 loss 8.644386 loss_att 10.615417 loss_ctc 12.677234 loss_rnnt 7.616182 hw_loss 0.180534 lr 0.00033099 rank 1
2023-02-25 11:01:25,355 DEBUG TRAIN Batch 27/3100 loss 7.835565 loss_att 13.800949 loss_ctc 12.337879 loss_rnnt 5.872496 hw_loss 0.318155 lr 0.00033095 rank 6
2023-02-25 11:01:25,357 DEBUG TRAIN Batch 27/3100 loss 6.024283 loss_att 8.280663 loss_ctc 9.599792 loss_rnnt 4.942012 hw_loss 0.289238 lr 0.00033092 rank 7
2023-02-25 11:02:39,800 DEBUG TRAIN Batch 27/3200 loss 7.143224 loss_att 11.370797 loss_ctc 14.374076 loss_rnnt 5.252693 hw_loss 0.151692 lr 0.00033085 rank 0
2023-02-25 11:02:39,803 DEBUG TRAIN Batch 27/3200 loss 9.915231 loss_att 10.399840 loss_ctc 12.743136 loss_rnnt 9.337299 hw_loss 0.194913 lr 0.00033092 rank 1
2023-02-25 11:02:39,803 DEBUG TRAIN Batch 27/3200 loss 4.143019 loss_att 5.812483 loss_ctc 3.234271 loss_rnnt 3.841636 hw_loss 0.166232 lr 0.00033088 rank 3
2023-02-25 11:02:39,804 DEBUG TRAIN Batch 27/3200 loss 4.755289 loss_att 8.234833 loss_ctc 8.403583 loss_rnnt 3.475037 hw_loss 0.183569 lr 0.00033088 rank 6
2023-02-25 11:02:39,804 DEBUG TRAIN Batch 27/3200 loss 2.255040 loss_att 5.367718 loss_ctc 3.373827 loss_rnnt 1.385260 hw_loss 0.183887 lr 0.00033084 rank 7
2023-02-25 11:02:39,807 DEBUG TRAIN Batch 27/3200 loss 9.289602 loss_att 9.174906 loss_ctc 12.857345 loss_rnnt 8.661667 hw_loss 0.328455 lr 0.00033089 rank 2
2023-02-25 11:02:39,817 DEBUG TRAIN Batch 27/3200 loss 14.308005 loss_att 17.240952 loss_ctc 21.727728 loss_rnnt 12.627220 hw_loss 0.196686 lr 0.00033086 rank 5
2023-02-25 11:02:39,844 DEBUG TRAIN Batch 27/3200 loss 4.807895 loss_att 4.941757 loss_ctc 7.018275 loss_rnnt 4.349327 hw_loss 0.257024 lr 0.00033087 rank 4
2023-02-25 11:03:51,211 DEBUG TRAIN Batch 27/3300 loss 9.573124 loss_att 14.310648 loss_ctc 15.592910 loss_rnnt 7.742532 hw_loss 0.150843 lr 0.00033078 rank 0
2023-02-25 11:03:51,213 DEBUG TRAIN Batch 27/3300 loss 16.916529 loss_att 21.017488 loss_ctc 25.472067 loss_rnnt 14.873497 hw_loss 0.153938 lr 0.00033082 rank 2
2023-02-25 11:03:51,218 DEBUG TRAIN Batch 27/3300 loss 7.136115 loss_att 10.757140 loss_ctc 12.149536 loss_rnnt 5.577047 hw_loss 0.312012 lr 0.00033081 rank 3
2023-02-25 11:03:51,218 DEBUG TRAIN Batch 27/3300 loss 6.407761 loss_att 14.270111 loss_ctc 12.558814 loss_rnnt 3.933420 hw_loss 0.153244 lr 0.00033085 rank 1
2023-02-25 11:03:51,220 DEBUG TRAIN Batch 27/3300 loss 9.683223 loss_att 13.742865 loss_ctc 12.513309 loss_rnnt 8.394612 hw_loss 0.186257 lr 0.00033079 rank 5
2023-02-25 11:03:51,219 DEBUG TRAIN Batch 27/3300 loss 7.138022 loss_att 10.832836 loss_ctc 14.464149 loss_rnnt 5.306579 hw_loss 0.216868 lr 0.00033077 rank 7
2023-02-25 11:03:51,222 DEBUG TRAIN Batch 27/3300 loss 7.831853 loss_att 11.623421 loss_ctc 11.677557 loss_rnnt 6.461690 hw_loss 0.185792 lr 0.00033080 rank 4
2023-02-25 11:03:51,265 DEBUG TRAIN Batch 27/3300 loss 4.393216 loss_att 10.376670 loss_ctc 10.048855 loss_rnnt 2.266684 hw_loss 0.329542 lr 0.00033080 rank 6
2023-02-25 11:05:02,479 DEBUG TRAIN Batch 27/3400 loss 12.539228 loss_att 15.070404 loss_ctc 20.929810 loss_rnnt 10.841911 hw_loss 0.135631 lr 0.00033077 rank 1
2023-02-25 11:05:02,481 DEBUG TRAIN Batch 27/3400 loss 11.339278 loss_att 15.136429 loss_ctc 15.449161 loss_rnnt 9.916404 hw_loss 0.216487 lr 0.00033073 rank 3
2023-02-25 11:05:02,484 DEBUG TRAIN Batch 27/3400 loss 7.655706 loss_att 10.097311 loss_ctc 12.091413 loss_rnnt 6.488298 hw_loss 0.164361 lr 0.00033071 rank 0
2023-02-25 11:05:02,485 DEBUG TRAIN Batch 27/3400 loss 4.466977 loss_att 9.745654 loss_ctc 5.690956 loss_rnnt 3.154907 hw_loss 0.174633 lr 0.00033073 rank 4
2023-02-25 11:05:02,487 DEBUG TRAIN Batch 27/3400 loss 2.938758 loss_att 6.987392 loss_ctc 4.076574 loss_rnnt 1.850395 hw_loss 0.237988 lr 0.00033075 rank 2
2023-02-25 11:05:02,487 DEBUG TRAIN Batch 27/3400 loss 10.713723 loss_att 12.986685 loss_ctc 15.437343 loss_rnnt 9.556252 hw_loss 0.136994 lr 0.00033070 rank 7
2023-02-25 11:05:02,491 DEBUG TRAIN Batch 27/3400 loss 9.440089 loss_att 13.803411 loss_ctc 12.970916 loss_rnnt 8.026561 hw_loss 0.131414 lr 0.00033073 rank 6
2023-02-25 11:05:02,494 DEBUG TRAIN Batch 27/3400 loss 6.116567 loss_att 8.420469 loss_ctc 9.494578 loss_rnnt 5.101203 hw_loss 0.195340 lr 0.00033072 rank 5
2023-02-25 11:06:14,357 DEBUG TRAIN Batch 27/3500 loss 15.013597 loss_att 18.919580 loss_ctc 21.074062 loss_rnnt 13.338964 hw_loss 0.160076 lr 0.00033064 rank 5
2023-02-25 11:06:14,362 DEBUG TRAIN Batch 27/3500 loss 16.032093 loss_att 18.161137 loss_ctc 21.341057 loss_rnnt 14.803903 hw_loss 0.177226 lr 0.00033068 rank 2
2023-02-25 11:06:14,370 DEBUG TRAIN Batch 27/3500 loss 11.431805 loss_att 14.205406 loss_ctc 18.925179 loss_rnnt 9.782436 hw_loss 0.179121 lr 0.00033063 rank 0
2023-02-25 11:06:14,372 DEBUG TRAIN Batch 27/3500 loss 5.762124 loss_att 9.710407 loss_ctc 11.683909 loss_rnnt 4.037691 hw_loss 0.272258 lr 0.00033066 rank 6
2023-02-25 11:06:14,373 DEBUG TRAIN Batch 27/3500 loss 2.952448 loss_att 5.534924 loss_ctc 3.742980 loss_rnnt 2.204763 hw_loss 0.235849 lr 0.00033066 rank 3
2023-02-25 11:06:14,373 DEBUG TRAIN Batch 27/3500 loss 8.993794 loss_att 13.070764 loss_ctc 12.590186 loss_rnnt 7.637681 hw_loss 0.114750 lr 0.00033063 rank 7
2023-02-25 11:06:14,373 DEBUG TRAIN Batch 27/3500 loss 5.917655 loss_att 7.569852 loss_ctc 7.229522 loss_rnnt 5.320899 hw_loss 0.171376 lr 0.00033070 rank 1
2023-02-25 11:06:14,383 DEBUG TRAIN Batch 27/3500 loss 7.646395 loss_att 9.724431 loss_ctc 10.223686 loss_rnnt 6.792571 hw_loss 0.177334 lr 0.00033066 rank 4
2023-02-25 11:07:25,946 DEBUG TRAIN Batch 27/3600 loss 7.886862 loss_att 9.796590 loss_ctc 10.433235 loss_rnnt 7.068247 hw_loss 0.182160 lr 0.00033056 rank 0
2023-02-25 11:07:25,947 DEBUG TRAIN Batch 27/3600 loss 4.965136 loss_att 10.518631 loss_ctc 10.102201 loss_rnnt 3.070829 hw_loss 0.185000 lr 0.00033060 rank 2
2023-02-25 11:07:25,949 DEBUG TRAIN Batch 27/3600 loss 4.441833 loss_att 7.128772 loss_ctc 7.568814 loss_rnnt 3.362664 hw_loss 0.234095 lr 0.00033059 rank 3
2023-02-25 11:07:25,946 DEBUG TRAIN Batch 27/3600 loss 12.324891 loss_att 14.487008 loss_ctc 16.325525 loss_rnnt 11.232471 hw_loss 0.237334 lr 0.00033055 rank 7
2023-02-25 11:07:25,950 DEBUG TRAIN Batch 27/3600 loss 4.401681 loss_att 5.500682 loss_ctc 6.414587 loss_rnnt 3.835926 hw_loss 0.145441 lr 0.00033063 rank 1
2023-02-25 11:07:25,954 DEBUG TRAIN Batch 27/3600 loss 5.484731 loss_att 8.759382 loss_ctc 10.685573 loss_rnnt 4.017704 hw_loss 0.222468 lr 0.00033059 rank 6
2023-02-25 11:07:25,956 DEBUG TRAIN Batch 27/3600 loss 3.865962 loss_att 5.885915 loss_ctc 6.167851 loss_rnnt 3.003687 hw_loss 0.283810 lr 0.00033058 rank 4
2023-02-25 11:07:25,956 DEBUG TRAIN Batch 27/3600 loss 7.508758 loss_att 11.112064 loss_ctc 9.752136 loss_rnnt 6.358766 hw_loss 0.244152 lr 0.00033057 rank 5
2023-02-25 11:08:36,145 DEBUG TRAIN Batch 27/3700 loss 5.713818 loss_att 6.224964 loss_ctc 8.327268 loss_rnnt 5.097074 hw_loss 0.311351 lr 0.00033049 rank 0
2023-02-25 11:08:36,146 DEBUG TRAIN Batch 27/3700 loss 10.807419 loss_att 11.811010 loss_ctc 12.489368 loss_rnnt 10.284669 hw_loss 0.183321 lr 0.00033052 rank 6
2023-02-25 11:08:36,151 DEBUG TRAIN Batch 27/3700 loss 8.367815 loss_att 9.528255 loss_ctc 13.368067 loss_rnnt 7.381179 hw_loss 0.164717 lr 0.00033056 rank 1
2023-02-25 11:08:36,151 DEBUG TRAIN Batch 27/3700 loss 6.145728 loss_att 7.189671 loss_ctc 8.353018 loss_rnnt 5.522765 hw_loss 0.224755 lr 0.00033052 rank 3
2023-02-25 11:08:36,153 DEBUG TRAIN Batch 27/3700 loss 6.866621 loss_att 9.695359 loss_ctc 8.641237 loss_rnnt 5.971340 hw_loss 0.174223 lr 0.00033048 rank 7
2023-02-25 11:08:36,154 DEBUG TRAIN Batch 27/3700 loss 2.363332 loss_att 5.439743 loss_ctc 5.291688 loss_rnnt 1.245564 hw_loss 0.210073 lr 0.00033051 rank 4
2023-02-25 11:08:36,155 DEBUG TRAIN Batch 27/3700 loss 11.815248 loss_att 17.228447 loss_ctc 18.894928 loss_rnnt 9.668949 hw_loss 0.224441 lr 0.00033053 rank 2
2023-02-25 11:08:36,158 DEBUG TRAIN Batch 27/3700 loss 7.534335 loss_att 9.709120 loss_ctc 12.159832 loss_rnnt 6.342814 hw_loss 0.262182 lr 0.00033050 rank 5
2023-02-25 11:09:46,672 DEBUG TRAIN Batch 27/3800 loss 7.597565 loss_att 10.624762 loss_ctc 10.621885 loss_rnnt 6.521591 hw_loss 0.126171 lr 0.00033042 rank 0
2023-02-25 11:09:46,672 DEBUG TRAIN Batch 27/3800 loss 8.140819 loss_att 9.679847 loss_ctc 8.983855 loss_rnnt 7.640121 hw_loss 0.150915 lr 0.00033044 rank 6
2023-02-25 11:09:46,675 DEBUG TRAIN Batch 27/3800 loss 9.079082 loss_att 10.730604 loss_ctc 11.601407 loss_rnnt 8.299049 hw_loss 0.212658 lr 0.00033044 rank 4
2023-02-25 11:09:46,676 DEBUG TRAIN Batch 27/3800 loss 8.372062 loss_att 8.758740 loss_ctc 12.674911 loss_rnnt 7.567972 hw_loss 0.286951 lr 0.00033046 rank 2
2023-02-25 11:09:46,677 DEBUG TRAIN Batch 27/3800 loss 9.657628 loss_att 11.159384 loss_ctc 12.991351 loss_rnnt 8.775046 hw_loss 0.258252 lr 0.00033049 rank 1
2023-02-25 11:09:46,678 DEBUG TRAIN Batch 27/3800 loss 7.438663 loss_att 7.508775 loss_ctc 9.320188 loss_rnnt 7.042348 hw_loss 0.246416 lr 0.00033044 rank 3
2023-02-25 11:09:46,684 DEBUG TRAIN Batch 27/3800 loss 2.614604 loss_att 4.340903 loss_ctc 3.928636 loss_rnnt 2.000021 hw_loss 0.176471 lr 0.00033041 rank 7
2023-02-25 11:09:46,725 DEBUG TRAIN Batch 27/3800 loss 9.885477 loss_att 12.542954 loss_ctc 14.537000 loss_rnnt 8.676627 hw_loss 0.107159 lr 0.00033043 rank 5
2023-02-25 11:10:59,738 DEBUG TRAIN Batch 27/3900 loss 11.769813 loss_att 16.098530 loss_ctc 16.841099 loss_rnnt 10.116464 hw_loss 0.208938 lr 0.00033035 rank 0
2023-02-25 11:10:59,756 DEBUG TRAIN Batch 27/3900 loss 2.568512 loss_att 6.176275 loss_ctc 4.583633 loss_rnnt 1.453295 hw_loss 0.234341 lr 0.00033037 rank 4
2023-02-25 11:10:59,756 DEBUG TRAIN Batch 27/3900 loss 8.656742 loss_att 13.610422 loss_ctc 16.540478 loss_rnnt 6.535768 hw_loss 0.148264 lr 0.00033037 rank 6
2023-02-25 11:10:59,762 DEBUG TRAIN Batch 27/3900 loss 3.300480 loss_att 7.466919 loss_ctc 4.088779 loss_rnnt 2.262366 hw_loss 0.186974 lr 0.00033039 rank 2
2023-02-25 11:10:59,764 DEBUG TRAIN Batch 27/3900 loss 8.875667 loss_att 13.024992 loss_ctc 12.543770 loss_rnnt 7.493555 hw_loss 0.118437 lr 0.00033041 rank 1
2023-02-25 11:10:59,769 DEBUG TRAIN Batch 27/3900 loss 5.678673 loss_att 7.950965 loss_ctc 7.011068 loss_rnnt 4.964285 hw_loss 0.154267 lr 0.00033037 rank 3
2023-02-25 11:10:59,778 DEBUG TRAIN Batch 27/3900 loss 9.488083 loss_att 10.980314 loss_ctc 13.317381 loss_rnnt 8.579067 hw_loss 0.187494 lr 0.00033036 rank 5
2023-02-25 11:10:59,794 DEBUG TRAIN Batch 27/3900 loss 17.159538 loss_att 19.225933 loss_ctc 22.078045 loss_rnnt 16.009140 hw_loss 0.152473 lr 0.00033034 rank 7
2023-02-25 11:12:09,782 DEBUG TRAIN Batch 27/4000 loss 7.753015 loss_att 10.859437 loss_ctc 10.915073 loss_rnnt 6.588724 hw_loss 0.227621 lr 0.00033034 rank 1
2023-02-25 11:12:09,782 DEBUG TRAIN Batch 27/4000 loss 5.145209 loss_att 6.763203 loss_ctc 2.776484 loss_rnnt 5.008831 hw_loss 0.241144 lr 0.00033030 rank 3
2023-02-25 11:12:09,785 DEBUG TRAIN Batch 27/4000 loss 9.863984 loss_att 14.473574 loss_ctc 15.361724 loss_rnnt 8.131580 hw_loss 0.145226 lr 0.00033032 rank 2
2023-02-25 11:12:09,787 DEBUG TRAIN Batch 27/4000 loss 6.599972 loss_att 9.811132 loss_ctc 11.952266 loss_rnnt 5.069314 hw_loss 0.327724 lr 0.00033028 rank 5
2023-02-25 11:12:09,788 DEBUG TRAIN Batch 27/4000 loss 4.791729 loss_att 8.457584 loss_ctc 8.583632 loss_rnnt 3.431819 hw_loss 0.227162 lr 0.00033027 rank 7
2023-02-25 11:12:09,792 DEBUG TRAIN Batch 27/4000 loss 3.927261 loss_att 8.386839 loss_ctc 6.502880 loss_rnnt 2.584388 hw_loss 0.201641 lr 0.00033027 rank 0
2023-02-25 11:12:09,792 DEBUG TRAIN Batch 27/4000 loss 8.123090 loss_att 12.453644 loss_ctc 11.341272 loss_rnnt 6.650706 hw_loss 0.332217 lr 0.00033030 rank 4
2023-02-25 11:12:09,836 DEBUG TRAIN Batch 27/4000 loss 6.204389 loss_att 9.215902 loss_ctc 9.673046 loss_rnnt 5.007222 hw_loss 0.248207 lr 0.00033030 rank 6
2023-02-25 11:13:21,448 DEBUG TRAIN Batch 27/4100 loss 8.624935 loss_att 11.816536 loss_ctc 20.382095 loss_rnnt 6.322908 hw_loss 0.180158 lr 0.00033020 rank 0
2023-02-25 11:13:21,449 DEBUG TRAIN Batch 27/4100 loss 11.516247 loss_att 14.964937 loss_ctc 17.560917 loss_rnnt 9.850290 hw_loss 0.319242 lr 0.00033023 rank 6
2023-02-25 11:13:21,451 DEBUG TRAIN Batch 27/4100 loss 13.007812 loss_att 16.553913 loss_ctc 20.219921 loss_rnnt 11.227652 hw_loss 0.204986 lr 0.00033027 rank 1
2023-02-25 11:13:21,454 DEBUG TRAIN Batch 27/4100 loss 7.219029 loss_att 10.038990 loss_ctc 11.691418 loss_rnnt 5.926605 hw_loss 0.247714 lr 0.00033024 rank 2
2023-02-25 11:13:21,453 DEBUG TRAIN Batch 27/4100 loss 11.821376 loss_att 13.199974 loss_ctc 16.185228 loss_rnnt 10.876877 hw_loss 0.162999 lr 0.00033021 rank 5
2023-02-25 11:13:21,455 DEBUG TRAIN Batch 27/4100 loss 5.939754 loss_att 7.438722 loss_ctc 8.492697 loss_rnnt 5.104546 hw_loss 0.365665 lr 0.00033023 rank 3
2023-02-25 11:13:21,455 DEBUG TRAIN Batch 27/4100 loss 10.557370 loss_att 11.751799 loss_ctc 13.928826 loss_rnnt 9.789524 hw_loss 0.148939 lr 0.00033019 rank 7
2023-02-25 11:13:21,459 DEBUG TRAIN Batch 27/4100 loss 9.644318 loss_att 13.402090 loss_ctc 12.384508 loss_rnnt 8.428632 hw_loss 0.185199 lr 0.00033022 rank 4
2023-02-25 11:14:32,339 DEBUG TRAIN Batch 27/4200 loss 5.782301 loss_att 7.960788 loss_ctc 7.083978 loss_rnnt 5.071232 hw_loss 0.190902 lr 0.00033016 rank 3
2023-02-25 11:14:32,343 DEBUG TRAIN Batch 27/4200 loss 9.789399 loss_att 13.509136 loss_ctc 24.389545 loss_rnnt 6.946138 hw_loss 0.286176 lr 0.00033013 rank 0
2023-02-25 11:14:32,345 DEBUG TRAIN Batch 27/4200 loss 5.031392 loss_att 8.173767 loss_ctc 8.735487 loss_rnnt 3.785134 hw_loss 0.232318 lr 0.00033020 rank 1
2023-02-25 11:14:32,345 DEBUG TRAIN Batch 27/4200 loss 5.196607 loss_att 8.903516 loss_ctc 13.289035 loss_rnnt 3.269392 hw_loss 0.200330 lr 0.00033015 rank 4
2023-02-25 11:14:32,346 DEBUG TRAIN Batch 27/4200 loss 9.960347 loss_att 10.845644 loss_ctc 12.942196 loss_rnnt 9.315755 hw_loss 0.131161 lr 0.00033017 rank 2
2023-02-25 11:14:32,345 DEBUG TRAIN Batch 27/4200 loss 6.719312 loss_att 9.808508 loss_ctc 8.773751 loss_rnnt 5.719924 hw_loss 0.201794 lr 0.00033012 rank 7
2023-02-25 11:14:32,351 DEBUG TRAIN Batch 27/4200 loss 5.479322 loss_att 7.882687 loss_ctc 10.704259 loss_rnnt 4.153213 hw_loss 0.278958 lr 0.00033015 rank 6
2023-02-25 11:14:32,352 DEBUG TRAIN Batch 27/4200 loss 2.730833 loss_att 5.608884 loss_ctc 7.009298 loss_rnnt 1.487812 hw_loss 0.181778 lr 0.00033014 rank 5
2023-02-25 11:15:45,898 DEBUG TRAIN Batch 27/4300 loss 7.626182 loss_att 9.215064 loss_ctc 9.904361 loss_rnnt 6.864286 hw_loss 0.263180 lr 0.00033006 rank 0
2023-02-25 11:15:45,899 DEBUG TRAIN Batch 27/4300 loss 6.539527 loss_att 9.906774 loss_ctc 8.666540 loss_rnnt 5.395296 hw_loss 0.350963 lr 0.00033013 rank 1
2023-02-25 11:15:45,904 DEBUG TRAIN Batch 27/4300 loss 13.548985 loss_att 15.280851 loss_ctc 22.336643 loss_rnnt 11.941434 hw_loss 0.167794 lr 0.00033005 rank 7
2023-02-25 11:15:45,905 DEBUG TRAIN Batch 27/4300 loss 7.489574 loss_att 9.185813 loss_ctc 11.725908 loss_rnnt 6.506753 hw_loss 0.147615 lr 0.00033008 rank 4
2023-02-25 11:15:45,905 DEBUG TRAIN Batch 27/4300 loss 9.775708 loss_att 14.355381 loss_ctc 15.636064 loss_rnnt 8.000205 hw_loss 0.146605 lr 0.00033008 rank 3
2023-02-25 11:15:45,905 DEBUG TRAIN Batch 27/4300 loss 4.956520 loss_att 5.607590 loss_ctc 5.422303 loss_rnnt 4.641846 hw_loss 0.229418 lr 0.00033010 rank 2
2023-02-25 11:15:45,912 DEBUG TRAIN Batch 27/4300 loss 9.392494 loss_att 12.497563 loss_ctc 13.160064 loss_rnnt 8.156479 hw_loss 0.211235 lr 0.00033008 rank 6
2023-02-25 11:15:45,956 DEBUG TRAIN Batch 27/4300 loss 11.447637 loss_att 12.979347 loss_ctc 15.642509 loss_rnnt 10.491796 hw_loss 0.169091 lr 0.00033007 rank 5
2023-02-25 11:16:57,274 DEBUG TRAIN Batch 27/4400 loss 4.570973 loss_att 7.403830 loss_ctc 7.326572 loss_rnnt 3.551657 hw_loss 0.159997 lr 0.00032999 rank 0
2023-02-25 11:16:57,274 DEBUG TRAIN Batch 27/4400 loss 4.905867 loss_att 5.309657 loss_ctc 7.002954 loss_rnnt 4.423465 hw_loss 0.228810 lr 0.00033001 rank 6
2023-02-25 11:16:57,276 DEBUG TRAIN Batch 27/4400 loss 8.904993 loss_att 12.448381 loss_ctc 14.897696 loss_rnnt 7.339973 hw_loss 0.107466 lr 0.00033000 rank 5
2023-02-25 11:16:57,277 DEBUG TRAIN Batch 27/4400 loss 6.835399 loss_att 7.566999 loss_ctc 9.736818 loss_rnnt 6.178514 hw_loss 0.231955 lr 0.00033005 rank 1
2023-02-25 11:16:57,276 DEBUG TRAIN Batch 27/4400 loss 4.850278 loss_att 8.025248 loss_ctc 7.297925 loss_rnnt 3.775842 hw_loss 0.212041 lr 0.00033001 rank 3
2023-02-25 11:16:57,278 DEBUG TRAIN Batch 27/4400 loss 4.400625 loss_att 6.449953 loss_ctc 6.377741 loss_rnnt 3.634544 hw_loss 0.173624 lr 0.00032998 rank 7
2023-02-25 11:16:57,282 DEBUG TRAIN Batch 27/4400 loss 10.159476 loss_att 11.004202 loss_ctc 12.870399 loss_rnnt 9.480609 hw_loss 0.278374 lr 0.00033001 rank 4
2023-02-25 11:16:57,283 DEBUG TRAIN Batch 27/4400 loss 9.985609 loss_att 10.223472 loss_ctc 13.600843 loss_rnnt 9.305702 hw_loss 0.281817 lr 0.00033003 rank 2
2023-02-25 11:18:08,073 DEBUG TRAIN Batch 27/4500 loss 4.525328 loss_att 7.260584 loss_ctc 7.371980 loss_rnnt 3.562503 hw_loss 0.067913 lr 0.00032994 rank 4
2023-02-25 11:18:08,075 DEBUG TRAIN Batch 27/4500 loss 10.227972 loss_att 14.482662 loss_ctc 17.411390 loss_rnnt 8.364562 hw_loss 0.102530 lr 0.00032996 rank 2
2023-02-25 11:18:08,075 DEBUG TRAIN Batch 27/4500 loss 2.133723 loss_att 5.588571 loss_ctc 5.107436 loss_rnnt 0.920479 hw_loss 0.235838 lr 0.00032994 rank 3
2023-02-25 11:18:08,079 DEBUG TRAIN Batch 27/4500 loss 6.463940 loss_att 10.964526 loss_ctc 11.001533 loss_rnnt 4.868018 hw_loss 0.170234 lr 0.00032998 rank 1
2023-02-25 11:18:08,080 DEBUG TRAIN Batch 27/4500 loss 7.335539 loss_att 7.995893 loss_ctc 8.216166 loss_rnnt 6.960324 hw_loss 0.235738 lr 0.00032994 rank 6
2023-02-25 11:18:08,079 DEBUG TRAIN Batch 27/4500 loss 9.546517 loss_att 14.634142 loss_ctc 12.440264 loss_rnnt 8.059649 hw_loss 0.156581 lr 0.00032991 rank 7
2023-02-25 11:18:08,081 DEBUG TRAIN Batch 27/4500 loss 3.637484 loss_att 5.769170 loss_ctc 2.999840 loss_rnnt 3.246851 hw_loss 0.092466 lr 0.00032991 rank 0
2023-02-25 11:18:08,126 DEBUG TRAIN Batch 27/4500 loss 7.689843 loss_att 8.645987 loss_ctc 9.407949 loss_rnnt 7.190097 hw_loss 0.148943 lr 0.00032992 rank 5
2023-02-25 11:19:21,148 DEBUG TRAIN Batch 27/4600 loss 11.049969 loss_att 15.313494 loss_ctc 17.458937 loss_rnnt 9.253214 hw_loss 0.167851 lr 0.00032983 rank 7
2023-02-25 11:19:21,159 DEBUG TRAIN Batch 27/4600 loss 4.809550 loss_att 6.373289 loss_ctc 4.355292 loss_rnnt 4.441871 hw_loss 0.216559 lr 0.00032984 rank 0
2023-02-25 11:19:21,161 DEBUG TRAIN Batch 27/4600 loss 25.971113 loss_att 31.261154 loss_ctc 39.684143 loss_rnnt 22.979294 hw_loss 0.197640 lr 0.00032991 rank 1
2023-02-25 11:19:21,162 DEBUG TRAIN Batch 27/4600 loss 6.882175 loss_att 9.760166 loss_ctc 8.375395 loss_rnnt 6.054317 hw_loss 0.099683 lr 0.00032985 rank 5
2023-02-25 11:19:21,163 DEBUG TRAIN Batch 27/4600 loss 18.380648 loss_att 20.652334 loss_ctc 30.649443 loss_rnnt 16.225279 hw_loss 0.122235 lr 0.00032987 rank 6
2023-02-25 11:19:21,165 DEBUG TRAIN Batch 27/4600 loss 10.648052 loss_att 12.866756 loss_ctc 16.484598 loss_rnnt 9.382058 hw_loss 0.082587 lr 0.00032988 rank 2
2023-02-25 11:19:21,167 DEBUG TRAIN Batch 27/4600 loss 4.152005 loss_att 7.540411 loss_ctc 3.638613 loss_rnnt 3.435694 hw_loss 0.200778 lr 0.00032986 rank 4
2023-02-25 11:19:21,207 DEBUG TRAIN Batch 27/4600 loss 10.515630 loss_att 14.561543 loss_ctc 15.862463 loss_rnnt 8.906624 hw_loss 0.162959 lr 0.00032987 rank 3
2023-02-25 11:20:32,662 DEBUG TRAIN Batch 27/4700 loss 11.871110 loss_att 16.067478 loss_ctc 13.832342 loss_rnnt 10.680977 hw_loss 0.167555 lr 0.00032977 rank 0
2023-02-25 11:20:32,668 DEBUG TRAIN Batch 27/4700 loss 11.172269 loss_att 14.826347 loss_ctc 20.161327 loss_rnnt 9.157654 hw_loss 0.159859 lr 0.00032981 rank 2
2023-02-25 11:20:32,667 DEBUG TRAIN Batch 27/4700 loss 6.570445 loss_att 9.225656 loss_ctc 12.015727 loss_rnnt 5.216122 hw_loss 0.182330 lr 0.00032980 rank 3
2023-02-25 11:20:32,672 DEBUG TRAIN Batch 27/4700 loss 15.991209 loss_att 19.133419 loss_ctc 27.314457 loss_rnnt 13.763405 hw_loss 0.167993 lr 0.00032984 rank 1
2023-02-25 11:20:32,673 DEBUG TRAIN Batch 27/4700 loss 1.905307 loss_att 5.280322 loss_ctc 3.170319 loss_rnnt 0.952601 hw_loss 0.204439 lr 0.00032980 rank 6
2023-02-25 11:20:32,675 DEBUG TRAIN Batch 27/4700 loss 1.877422 loss_att 4.865839 loss_ctc 2.289260 loss_rnnt 1.104483 hw_loss 0.225645 lr 0.00032979 rank 4
2023-02-25 11:20:32,676 DEBUG TRAIN Batch 27/4700 loss 2.316153 loss_att 5.191673 loss_ctc 3.697937 loss_rnnt 1.463597 hw_loss 0.174777 lr 0.00032978 rank 5
2023-02-25 11:20:32,679 DEBUG TRAIN Batch 27/4700 loss 0.847430 loss_att 3.550198 loss_ctc 0.824654 loss_rnnt 0.209859 hw_loss 0.187600 lr 0.00032976 rank 7
2023-02-25 11:21:43,192 DEBUG TRAIN Batch 27/4800 loss 4.269282 loss_att 7.981204 loss_ctc 8.706884 loss_rnnt 2.801509 hw_loss 0.250703 lr 0.00032972 rank 4
2023-02-25 11:21:43,192 DEBUG TRAIN Batch 27/4800 loss 14.120768 loss_att 17.207466 loss_ctc 21.284687 loss_rnnt 12.440482 hw_loss 0.202046 lr 0.00032970 rank 0
2023-02-25 11:21:43,193 DEBUG TRAIN Batch 27/4800 loss 6.479430 loss_att 9.004162 loss_ctc 10.368699 loss_rnnt 5.374369 hw_loss 0.152898 lr 0.00032977 rank 1
2023-02-25 11:21:43,195 DEBUG TRAIN Batch 27/4800 loss 5.567514 loss_att 8.955113 loss_ctc 9.458035 loss_rnnt 4.298213 hw_loss 0.136960 lr 0.00032971 rank 5
2023-02-25 11:21:43,197 DEBUG TRAIN Batch 27/4800 loss 10.809596 loss_att 14.063463 loss_ctc 16.043411 loss_rnnt 9.388156 hw_loss 0.136545 lr 0.00032972 rank 3
2023-02-25 11:21:43,197 DEBUG TRAIN Batch 27/4800 loss 9.907421 loss_att 13.983952 loss_ctc 11.316109 loss_rnnt 8.818127 hw_loss 0.161558 lr 0.00032969 rank 7
2023-02-25 11:21:43,199 DEBUG TRAIN Batch 27/4800 loss 8.921601 loss_att 12.351395 loss_ctc 11.810086 loss_rnnt 7.758603 hw_loss 0.172328 lr 0.00032974 rank 2
2023-02-25 11:21:43,201 DEBUG TRAIN Batch 27/4800 loss 9.508455 loss_att 12.496919 loss_ctc 11.500238 loss_rnnt 8.533291 hw_loss 0.209813 lr 0.00032972 rank 6
2023-02-25 11:22:54,761 DEBUG TRAIN Batch 27/4900 loss 2.320099 loss_att 4.605496 loss_ctc 3.099544 loss_rnnt 1.695185 hw_loss 0.119827 lr 0.00032965 rank 3
2023-02-25 11:22:54,764 DEBUG TRAIN Batch 27/4900 loss 6.457952 loss_att 8.014879 loss_ctc 9.360289 loss_rnnt 5.657405 hw_loss 0.191594 lr 0.00032963 rank 0
2023-02-25 11:22:54,765 DEBUG TRAIN Batch 27/4900 loss 5.448390 loss_att 7.940840 loss_ctc 8.023163 loss_rnnt 4.473574 hw_loss 0.249418 lr 0.00032969 rank 1
2023-02-25 11:22:54,766 DEBUG TRAIN Batch 27/4900 loss 4.389237 loss_att 7.057567 loss_ctc 5.959691 loss_rnnt 3.517153 hw_loss 0.241922 lr 0.00032965 rank 4
2023-02-25 11:22:54,767 DEBUG TRAIN Batch 27/4900 loss 11.096230 loss_att 14.037788 loss_ctc 16.809357 loss_rnnt 9.658265 hw_loss 0.164816 lr 0.00032965 rank 6
2023-02-25 11:22:54,769 DEBUG TRAIN Batch 27/4900 loss 8.494987 loss_att 10.225962 loss_ctc 14.047655 loss_rnnt 7.316406 hw_loss 0.172558 lr 0.00032962 rank 7
2023-02-25 11:22:54,770 DEBUG TRAIN Batch 27/4900 loss 15.058969 loss_att 17.909826 loss_ctc 23.379572 loss_rnnt 13.305859 hw_loss 0.137860 lr 0.00032967 rank 2
2023-02-25 11:22:54,815 DEBUG TRAIN Batch 27/4900 loss 8.218053 loss_att 11.751778 loss_ctc 13.894051 loss_rnnt 6.622320 hw_loss 0.247852 lr 0.00032964 rank 5
2023-02-25 11:24:07,429 DEBUG TRAIN Batch 27/5000 loss 3.534187 loss_att 6.483320 loss_ctc 6.774145 loss_rnnt 2.395300 hw_loss 0.219497 lr 0.00032958 rank 3
2023-02-25 11:24:07,434 DEBUG TRAIN Batch 27/5000 loss 6.177642 loss_att 7.205525 loss_ctc 8.780144 loss_rnnt 5.539907 hw_loss 0.159671 lr 0.00032960 rank 2
2023-02-25 11:24:07,434 DEBUG TRAIN Batch 27/5000 loss 4.932458 loss_att 7.982234 loss_ctc 9.361229 loss_rnnt 3.664742 hw_loss 0.126111 lr 0.00032958 rank 6
2023-02-25 11:24:07,435 DEBUG TRAIN Batch 27/5000 loss 3.639642 loss_att 5.920023 loss_ctc 7.517975 loss_rnnt 2.540410 hw_loss 0.236334 lr 0.00032955 rank 7
2023-02-25 11:24:07,436 DEBUG TRAIN Batch 27/5000 loss 10.931791 loss_att 12.262123 loss_ctc 14.854606 loss_rnnt 10.016112 hw_loss 0.237321 lr 0.00032962 rank 1
2023-02-25 11:24:07,438 DEBUG TRAIN Batch 27/5000 loss 16.668955 loss_att 15.711484 loss_ctc 23.365856 loss_rnnt 15.819059 hw_loss 0.278384 lr 0.00032958 rank 4
2023-02-25 11:24:07,442 DEBUG TRAIN Batch 27/5000 loss 5.211888 loss_att 6.586931 loss_ctc 7.091406 loss_rnnt 4.590800 hw_loss 0.179019 lr 0.00032957 rank 5
2023-02-25 11:24:07,504 DEBUG TRAIN Batch 27/5000 loss 3.619799 loss_att 7.897143 loss_ctc 4.723684 loss_rnnt 2.509061 hw_loss 0.202660 lr 0.00032956 rank 0
2023-02-25 11:25:17,759 DEBUG TRAIN Batch 27/5100 loss 2.955208 loss_att 5.469167 loss_ctc 5.102332 loss_rnnt 2.048855 hw_loss 0.219898 lr 0.00032948 rank 0
2023-02-25 11:25:17,760 DEBUG TRAIN Batch 27/5100 loss 8.915595 loss_att 11.798669 loss_ctc 11.565659 loss_rnnt 7.896006 hw_loss 0.168061 lr 0.00032948 rank 7
2023-02-25 11:25:17,762 DEBUG TRAIN Batch 27/5100 loss 15.355700 loss_att 18.061827 loss_ctc 21.515833 loss_rnnt 13.881038 hw_loss 0.210159 lr 0.00032951 rank 4
2023-02-25 11:25:17,763 DEBUG TRAIN Batch 27/5100 loss 7.837861 loss_att 9.108421 loss_ctc 9.860474 loss_rnnt 7.177714 hw_loss 0.255661 lr 0.00032951 rank 6
2023-02-25 11:25:17,765 DEBUG TRAIN Batch 27/5100 loss 6.823397 loss_att 7.647787 loss_ctc 10.047053 loss_rnnt 6.058073 hw_loss 0.319921 lr 0.00032955 rank 1
2023-02-25 11:25:17,769 DEBUG TRAIN Batch 27/5100 loss 7.561110 loss_att 9.114614 loss_ctc 9.511625 loss_rnnt 6.885773 hw_loss 0.196063 lr 0.00032949 rank 5
2023-02-25 11:25:17,770 DEBUG TRAIN Batch 27/5100 loss 4.485921 loss_att 8.432938 loss_ctc 9.580189 loss_rnnt 2.933473 hw_loss 0.157142 lr 0.00032953 rank 2
2023-02-25 11:25:17,808 DEBUG TRAIN Batch 27/5100 loss 11.417942 loss_att 11.458461 loss_ctc 16.976971 loss_rnnt 10.535198 hw_loss 0.250194 lr 0.00032951 rank 3
2023-02-25 11:26:28,443 DEBUG TRAIN Batch 27/5200 loss 7.396922 loss_att 9.138026 loss_ctc 10.106825 loss_rnnt 6.595675 hw_loss 0.171950 lr 0.00032944 rank 3
2023-02-25 11:26:28,447 DEBUG TRAIN Batch 27/5200 loss 2.089639 loss_att 4.839320 loss_ctc 2.095856 loss_rnnt 1.429477 hw_loss 0.205119 lr 0.00032941 rank 0
2023-02-25 11:26:28,447 DEBUG TRAIN Batch 27/5200 loss 8.661036 loss_att 12.724444 loss_ctc 12.983007 loss_rnnt 7.221668 hw_loss 0.094541 lr 0.00032943 rank 4
2023-02-25 11:26:28,448 DEBUG TRAIN Batch 27/5200 loss 7.229263 loss_att 9.613926 loss_ctc 9.901354 loss_rnnt 6.294186 hw_loss 0.190999 lr 0.00032944 rank 6
2023-02-25 11:26:28,449 DEBUG TRAIN Batch 27/5200 loss 4.393567 loss_att 8.413085 loss_ctc 7.007449 loss_rnnt 3.151553 hw_loss 0.167988 lr 0.00032948 rank 1
2023-02-25 11:26:28,448 DEBUG TRAIN Batch 27/5200 loss 10.906921 loss_att 9.848661 loss_ctc 13.259178 loss_rnnt 10.763746 hw_loss 0.077238 lr 0.00032942 rank 5
2023-02-25 11:26:28,450 DEBUG TRAIN Batch 27/5200 loss 8.619376 loss_att 11.312602 loss_ctc 11.854429 loss_rnnt 7.574474 hw_loss 0.140469 lr 0.00032945 rank 2
2023-02-25 11:26:28,450 DEBUG TRAIN Batch 27/5200 loss 4.379264 loss_att 7.211480 loss_ctc 6.032201 loss_rnnt 3.470771 hw_loss 0.228111 lr 0.00032940 rank 7
2023-02-25 11:27:40,896 DEBUG TRAIN Batch 27/5300 loss 7.001931 loss_att 9.352946 loss_ctc 11.654028 loss_rnnt 5.806699 hw_loss 0.196404 lr 0.00032936 rank 4
2023-02-25 11:27:40,905 DEBUG TRAIN Batch 27/5300 loss 4.338660 loss_att 6.580886 loss_ctc 6.373074 loss_rnnt 3.549797 hw_loss 0.129680 lr 0.00032937 rank 3
2023-02-25 11:27:40,912 DEBUG TRAIN Batch 27/5300 loss 4.534746 loss_att 7.306154 loss_ctc 8.149465 loss_rnnt 3.418399 hw_loss 0.150194 lr 0.00032941 rank 1
2023-02-25 11:27:40,912 DEBUG TRAIN Batch 27/5300 loss 11.141320 loss_att 16.090704 loss_ctc 23.332415 loss_rnnt 8.422386 hw_loss 0.194209 lr 0.00032934 rank 0
2023-02-25 11:27:40,911 DEBUG TRAIN Batch 27/5300 loss 6.887724 loss_att 11.739828 loss_ctc 15.776607 loss_rnnt 4.611001 hw_loss 0.227096 lr 0.00032935 rank 5
2023-02-25 11:27:40,912 DEBUG TRAIN Batch 27/5300 loss 7.196666 loss_att 9.239142 loss_ctc 6.202098 loss_rnnt 6.827023 hw_loss 0.175795 lr 0.00032933 rank 7
2023-02-25 11:27:40,914 DEBUG TRAIN Batch 27/5300 loss 11.618537 loss_att 15.603395 loss_ctc 16.191397 loss_rnnt 10.127022 hw_loss 0.159053 lr 0.00032938 rank 2
2023-02-25 11:27:40,959 DEBUG TRAIN Batch 27/5300 loss 16.005154 loss_att 18.184273 loss_ctc 25.893085 loss_rnnt 14.065985 hw_loss 0.346788 lr 0.00032937 rank 6
2023-02-25 11:28:53,720 DEBUG TRAIN Batch 27/5400 loss 5.775434 loss_att 8.960927 loss_ctc 7.424900 loss_rnnt 4.806327 hw_loss 0.210150 lr 0.00032927 rank 0
2023-02-25 11:28:53,721 DEBUG TRAIN Batch 27/5400 loss 7.423734 loss_att 10.773405 loss_ctc 11.940288 loss_rnnt 6.053083 hw_loss 0.184706 lr 0.00032930 rank 3
2023-02-25 11:28:53,722 DEBUG TRAIN Batch 27/5400 loss 8.585188 loss_att 10.145893 loss_ctc 11.182678 loss_rnnt 7.788440 hw_loss 0.259265 lr 0.00032929 rank 6
2023-02-25 11:28:53,724 DEBUG TRAIN Batch 27/5400 loss 15.075169 loss_att 16.419569 loss_ctc 19.933996 loss_rnnt 13.993624 hw_loss 0.309038 lr 0.00032934 rank 1
2023-02-25 11:28:53,724 DEBUG TRAIN Batch 27/5400 loss 11.855401 loss_att 17.097271 loss_ctc 15.371821 loss_rnnt 10.251249 hw_loss 0.162978 lr 0.00032929 rank 4
2023-02-25 11:28:53,724 DEBUG TRAIN Batch 27/5400 loss 6.653900 loss_att 9.613934 loss_ctc 10.214153 loss_rnnt 5.471846 hw_loss 0.216277 lr 0.00032931 rank 2
2023-02-25 11:28:53,729 DEBUG TRAIN Batch 27/5400 loss 3.434662 loss_att 5.668180 loss_ctc 4.400792 loss_rnnt 2.741878 hw_loss 0.219869 lr 0.00032926 rank 7
2023-02-25 11:28:53,778 DEBUG TRAIN Batch 27/5400 loss 0.986463 loss_att 2.765140 loss_ctc 1.483835 loss_rnnt 0.406547 hw_loss 0.295997 lr 0.00032928 rank 5
2023-02-25 11:30:04,062 DEBUG TRAIN Batch 27/5500 loss 4.036304 loss_att 6.902618 loss_ctc 9.542700 loss_rnnt 2.642913 hw_loss 0.161143 lr 0.00032922 rank 3
2023-02-25 11:30:04,063 DEBUG TRAIN Batch 27/5500 loss 6.144818 loss_att 7.506211 loss_ctc 10.722841 loss_rnnt 5.185676 hw_loss 0.143363 lr 0.00032922 rank 4
2023-02-25 11:30:04,064 DEBUG TRAIN Batch 27/5500 loss 12.882969 loss_att 19.517710 loss_ctc 20.791586 loss_rnnt 10.404638 hw_loss 0.181689 lr 0.00032927 rank 1
2023-02-25 11:30:04,064 DEBUG TRAIN Batch 27/5500 loss 7.809593 loss_att 10.685742 loss_ctc 14.566498 loss_rnnt 6.201881 hw_loss 0.246679 lr 0.00032919 rank 7
2023-02-25 11:30:04,065 DEBUG TRAIN Batch 27/5500 loss 17.354443 loss_att 20.950621 loss_ctc 25.041750 loss_rnnt 15.486170 hw_loss 0.232614 lr 0.00032920 rank 0
2023-02-25 11:30:04,071 DEBUG TRAIN Batch 27/5500 loss 14.221511 loss_att 16.772049 loss_ctc 19.943380 loss_rnnt 12.848835 hw_loss 0.186849 lr 0.00032921 rank 5
2023-02-25 11:30:04,071 DEBUG TRAIN Batch 27/5500 loss 3.615738 loss_att 8.593085 loss_ctc 7.365235 loss_rnnt 2.006695 hw_loss 0.213078 lr 0.00032922 rank 6
2023-02-25 11:30:04,074 DEBUG TRAIN Batch 27/5500 loss 6.302216 loss_att 8.592546 loss_ctc 11.780582 loss_rnnt 4.977782 hw_loss 0.254848 lr 0.00032924 rank 2
2023-02-25 11:31:14,632 DEBUG TRAIN Batch 27/5600 loss 4.118816 loss_att 7.247373 loss_ctc 6.347851 loss_rnnt 3.071208 hw_loss 0.233798 lr 0.00032915 rank 6
2023-02-25 11:31:14,643 DEBUG TRAIN Batch 27/5600 loss 8.678177 loss_att 9.675848 loss_ctc 12.616154 loss_rnnt 7.857790 hw_loss 0.179605 lr 0.00032915 rank 3
2023-02-25 11:31:14,647 DEBUG TRAIN Batch 27/5600 loss 1.659304 loss_att 4.278859 loss_ctc 3.325024 loss_rnnt 0.771567 hw_loss 0.265745 lr 0.00032919 rank 1
2023-02-25 11:31:14,648 DEBUG TRAIN Batch 27/5600 loss 14.359300 loss_att 16.130713 loss_ctc 21.703762 loss_rnnt 12.875089 hw_loss 0.282500 lr 0.00032912 rank 7
2023-02-25 11:31:14,649 DEBUG TRAIN Batch 27/5600 loss 10.197104 loss_att 10.203014 loss_ctc 12.379275 loss_rnnt 9.794667 hw_loss 0.206808 lr 0.00032913 rank 0
2023-02-25 11:31:14,650 DEBUG TRAIN Batch 27/5600 loss 8.427390 loss_att 12.103575 loss_ctc 10.718925 loss_rnnt 7.315830 hw_loss 0.132721 lr 0.00032914 rank 5
2023-02-25 11:31:14,654 DEBUG TRAIN Batch 27/5600 loss 6.853439 loss_att 8.877758 loss_ctc 9.558475 loss_rnnt 5.966171 hw_loss 0.228252 lr 0.00032917 rank 2
2023-02-25 11:31:14,658 DEBUG TRAIN Batch 27/5600 loss 5.077019 loss_att 6.249719 loss_ctc 6.771148 loss_rnnt 4.512406 hw_loss 0.195354 lr 0.00032915 rank 4
2023-02-25 11:32:29,037 DEBUG TRAIN Batch 27/5700 loss 3.370288 loss_att 6.285548 loss_ctc 6.306603 loss_rnnt 2.289869 hw_loss 0.198483 lr 0.00032906 rank 0
2023-02-25 11:32:29,043 DEBUG TRAIN Batch 27/5700 loss 8.424649 loss_att 8.631989 loss_ctc 12.328335 loss_rnnt 7.721592 hw_loss 0.264558 lr 0.00032905 rank 7
2023-02-25 11:32:29,046 DEBUG TRAIN Batch 27/5700 loss 7.390513 loss_att 8.289968 loss_ctc 10.096618 loss_rnnt 6.748968 hw_loss 0.189077 lr 0.00032908 rank 6
2023-02-25 11:32:29,045 DEBUG TRAIN Batch 27/5700 loss 8.550613 loss_att 8.678205 loss_ctc 10.834242 loss_rnnt 8.159265 hw_loss 0.115025 lr 0.00032908 rank 4
2023-02-25 11:32:29,046 DEBUG TRAIN Batch 27/5700 loss 7.535461 loss_att 10.192316 loss_ctc 10.467844 loss_rnnt 6.512273 hw_loss 0.189061 lr 0.00032908 rank 3
2023-02-25 11:32:29,046 DEBUG TRAIN Batch 27/5700 loss 4.549635 loss_att 6.840999 loss_ctc 8.008005 loss_rnnt 3.530390 hw_loss 0.187230 lr 0.00032912 rank 1
2023-02-25 11:32:29,046 DEBUG TRAIN Batch 27/5700 loss 3.225323 loss_att 6.093498 loss_ctc 5.768008 loss_rnnt 2.224482 hw_loss 0.165340 lr 0.00032907 rank 5
2023-02-25 11:32:29,048 DEBUG TRAIN Batch 27/5700 loss 7.634783 loss_att 9.991300 loss_ctc 11.445579 loss_rnnt 6.494958 hw_loss 0.300779 lr 0.00032910 rank 2
2023-02-25 11:33:39,718 DEBUG TRAIN Batch 27/5800 loss 6.262719 loss_att 8.559900 loss_ctc 13.328169 loss_rnnt 4.781731 hw_loss 0.149047 lr 0.00032898 rank 0
2023-02-25 11:33:39,721 DEBUG TRAIN Batch 27/5800 loss 4.850356 loss_att 10.184522 loss_ctc 11.072070 loss_rnnt 2.868651 hw_loss 0.159954 lr 0.00032903 rank 2
2023-02-25 11:33:39,723 DEBUG TRAIN Batch 27/5800 loss 14.716190 loss_att 19.456280 loss_ctc 24.725662 loss_rnnt 12.330878 hw_loss 0.192560 lr 0.00032901 rank 3
2023-02-25 11:33:39,723 DEBUG TRAIN Batch 27/5800 loss 12.631052 loss_att 12.366848 loss_ctc 19.992947 loss_rnnt 11.584846 hw_loss 0.220239 lr 0.00032899 rank 5
2023-02-25 11:33:39,723 DEBUG TRAIN Batch 27/5800 loss 12.747811 loss_att 16.557611 loss_ctc 23.395391 loss_rnnt 10.454086 hw_loss 0.210164 lr 0.00032905 rank 1
2023-02-25 11:33:39,727 DEBUG TRAIN Batch 27/5800 loss 5.559282 loss_att 7.645507 loss_ctc 7.791915 loss_rnnt 4.701674 hw_loss 0.267522 lr 0.00032901 rank 6
2023-02-25 11:33:39,727 DEBUG TRAIN Batch 27/5800 loss 23.437660 loss_att 25.417580 loss_ctc 44.302986 loss_rnnt 20.184280 hw_loss 0.141285 lr 0.00032901 rank 4
2023-02-25 11:33:39,728 DEBUG TRAIN Batch 27/5800 loss 2.956852 loss_att 4.544418 loss_ctc 5.634227 loss_rnnt 2.175242 hw_loss 0.200839 lr 0.00032898 rank 7
2023-02-25 11:34:50,218 DEBUG TRAIN Batch 27/5900 loss 5.504630 loss_att 8.331284 loss_ctc 6.077117 loss_rnnt 4.752270 hw_loss 0.207558 lr 0.00032894 rank 3
2023-02-25 11:34:50,217 DEBUG TRAIN Batch 27/5900 loss 9.178402 loss_att 13.283329 loss_ctc 14.107748 loss_rnnt 7.596581 hw_loss 0.194229 lr 0.00032894 rank 4
2023-02-25 11:34:50,219 DEBUG TRAIN Batch 27/5900 loss 7.334065 loss_att 11.428541 loss_ctc 14.302023 loss_rnnt 5.481878 hw_loss 0.195433 lr 0.00032892 rank 5
2023-02-25 11:34:50,222 DEBUG TRAIN Batch 27/5900 loss 6.106198 loss_att 11.468660 loss_ctc 10.794157 loss_rnnt 4.283564 hw_loss 0.234527 lr 0.00032890 rank 7
2023-02-25 11:34:50,224 DEBUG TRAIN Batch 27/5900 loss 6.159955 loss_att 9.124389 loss_ctc 11.177144 loss_rnnt 4.798700 hw_loss 0.186394 lr 0.00032891 rank 0
2023-02-25 11:34:50,223 DEBUG TRAIN Batch 27/5900 loss 6.974728 loss_att 11.534219 loss_ctc 10.850749 loss_rnnt 5.482021 hw_loss 0.120009 lr 0.00032896 rank 2
2023-02-25 11:34:50,225 DEBUG TRAIN Batch 27/5900 loss 5.639392 loss_att 9.059372 loss_ctc 10.553037 loss_rnnt 4.226298 hw_loss 0.138647 lr 0.00032894 rank 6
2023-02-25 11:34:50,225 DEBUG TRAIN Batch 27/5900 loss 11.917033 loss_att 13.319803 loss_ctc 16.639206 loss_rnnt 10.939032 hw_loss 0.127173 lr 0.00032898 rank 1
2023-02-25 11:36:02,191 DEBUG TRAIN Batch 27/6000 loss 7.959248 loss_att 8.987896 loss_ctc 8.266438 loss_rnnt 7.568410 hw_loss 0.270279 lr 0.00032883 rank 7
2023-02-25 11:36:02,198 DEBUG TRAIN Batch 27/6000 loss 5.092553 loss_att 7.831523 loss_ctc 8.739581 loss_rnnt 3.990730 hw_loss 0.127048 lr 0.00032887 rank 3
2023-02-25 11:36:02,199 DEBUG TRAIN Batch 27/6000 loss 5.616033 loss_att 7.742325 loss_ctc 7.332155 loss_rnnt 4.845246 hw_loss 0.218836 lr 0.00032886 rank 4
2023-02-25 11:36:02,200 DEBUG TRAIN Batch 27/6000 loss 3.251084 loss_att 5.985633 loss_ctc 5.655927 loss_rnnt 2.262702 hw_loss 0.226549 lr 0.00032884 rank 0
2023-02-25 11:36:02,199 DEBUG TRAIN Batch 27/6000 loss 13.967382 loss_att 16.626087 loss_ctc 23.895460 loss_rnnt 11.976656 hw_loss 0.253578 lr 0.00032887 rank 6
2023-02-25 11:36:02,202 DEBUG TRAIN Batch 27/6000 loss 7.726308 loss_att 9.782182 loss_ctc 9.292311 loss_rnnt 6.980858 hw_loss 0.235264 lr 0.00032888 rank 2
2023-02-25 11:36:02,203 DEBUG TRAIN Batch 27/6000 loss 12.033844 loss_att 14.138753 loss_ctc 16.197578 loss_rnnt 10.972888 hw_loss 0.159019 lr 0.00032885 rank 5
2023-02-25 11:36:02,204 DEBUG TRAIN Batch 27/6000 loss 3.818049 loss_att 5.899049 loss_ctc 3.462705 loss_rnnt 3.348610 hw_loss 0.188660 lr 0.00032891 rank 1
2023-02-25 11:37:15,422 DEBUG TRAIN Batch 27/6100 loss 1.301636 loss_att 3.976881 loss_ctc 1.872510 loss_rnnt 0.628725 hw_loss 0.115772 lr 0.00032880 rank 6
2023-02-25 11:37:15,425 DEBUG TRAIN Batch 27/6100 loss 10.510741 loss_att 12.619453 loss_ctc 17.653610 loss_rnnt 9.018869 hw_loss 0.220772 lr 0.00032877 rank 0
2023-02-25 11:37:15,425 DEBUG TRAIN Batch 27/6100 loss 1.420941 loss_att 3.408221 loss_ctc 2.501855 loss_rnnt 0.805815 hw_loss 0.137904 lr 0.00032878 rank 5
2023-02-25 11:37:15,427 DEBUG TRAIN Batch 27/6100 loss 7.037558 loss_att 11.652944 loss_ctc 10.177322 loss_rnnt 5.618812 hw_loss 0.144439 lr 0.00032884 rank 1
2023-02-25 11:37:15,428 DEBUG TRAIN Batch 27/6100 loss 4.442300 loss_att 6.979431 loss_ctc 7.643950 loss_rnnt 3.384387 hw_loss 0.231750 lr 0.00032881 rank 2
2023-02-25 11:37:15,430 DEBUG TRAIN Batch 27/6100 loss 6.441938 loss_att 10.800051 loss_ctc 9.156233 loss_rnnt 5.099360 hw_loss 0.204467 lr 0.00032876 rank 7
2023-02-25 11:37:15,430 DEBUG TRAIN Batch 27/6100 loss 8.447263 loss_att 8.546407 loss_ctc 10.500204 loss_rnnt 8.081214 hw_loss 0.135928 lr 0.00032880 rank 3
2023-02-25 11:37:15,431 DEBUG TRAIN Batch 27/6100 loss 9.979972 loss_att 10.568919 loss_ctc 12.289995 loss_rnnt 9.466722 hw_loss 0.163982 lr 0.00032879 rank 4
2023-02-25 11:38:26,428 DEBUG TRAIN Batch 27/6200 loss 5.449191 loss_att 5.598100 loss_ctc 7.469085 loss_rnnt 5.036076 hw_loss 0.213777 lr 0.00032877 rank 1
2023-02-25 11:38:26,430 DEBUG TRAIN Batch 27/6200 loss 14.787745 loss_att 15.551908 loss_ctc 21.394527 loss_rnnt 13.635161 hw_loss 0.222838 lr 0.00032870 rank 0
2023-02-25 11:38:26,430 DEBUG TRAIN Batch 27/6200 loss 11.890436 loss_att 14.106750 loss_ctc 23.412495 loss_rnnt 9.822755 hw_loss 0.165268 lr 0.00032874 rank 2
2023-02-25 11:38:26,431 DEBUG TRAIN Batch 27/6200 loss 10.945922 loss_att 14.724083 loss_ctc 15.825523 loss_rnnt 9.447120 hw_loss 0.173546 lr 0.00032869 rank 7
2023-02-25 11:38:26,436 DEBUG TRAIN Batch 27/6200 loss 18.125484 loss_att 18.844563 loss_ctc 25.467693 loss_rnnt 16.877029 hw_loss 0.235650 lr 0.00032872 rank 4
2023-02-25 11:38:26,437 DEBUG TRAIN Batch 27/6200 loss 7.911720 loss_att 9.890160 loss_ctc 11.368035 loss_rnnt 6.941668 hw_loss 0.212856 lr 0.00032873 rank 3
2023-02-25 11:38:26,441 DEBUG TRAIN Batch 27/6200 loss 9.218678 loss_att 13.462601 loss_ctc 15.493946 loss_rnnt 7.433876 hw_loss 0.186215 lr 0.00032871 rank 5
2023-02-25 11:38:26,481 DEBUG TRAIN Batch 27/6200 loss 5.741522 loss_att 7.616832 loss_ctc 6.441411 loss_rnnt 5.155362 hw_loss 0.220836 lr 0.00032872 rank 6
2023-02-25 11:39:38,362 DEBUG TRAIN Batch 27/6300 loss 3.858047 loss_att 8.502608 loss_ctc 8.992438 loss_rnnt 2.169544 hw_loss 0.140634 lr 0.00032863 rank 0
2023-02-25 11:39:38,374 DEBUG TRAIN Batch 27/6300 loss 5.530310 loss_att 6.536256 loss_ctc 7.909495 loss_rnnt 4.894303 hw_loss 0.220485 lr 0.00032865 rank 3
2023-02-25 11:39:38,375 DEBUG TRAIN Batch 27/6300 loss 9.396571 loss_att 11.822591 loss_ctc 12.423265 loss_rnnt 8.448106 hw_loss 0.111939 lr 0.00032865 rank 6
2023-02-25 11:39:38,376 DEBUG TRAIN Batch 27/6300 loss 12.187246 loss_att 12.590590 loss_ctc 14.810428 loss_rnnt 11.651263 hw_loss 0.197918 lr 0.00032870 rank 1
2023-02-25 11:39:38,377 DEBUG TRAIN Batch 27/6300 loss 12.017509 loss_att 14.064962 loss_ctc 18.045593 loss_rnnt 10.713074 hw_loss 0.171000 lr 0.00032867 rank 2
2023-02-25 11:39:38,377 DEBUG TRAIN Batch 27/6300 loss 9.027344 loss_att 9.832814 loss_ctc 14.779648 loss_rnnt 8.010390 hw_loss 0.166659 lr 0.00032862 rank 7
2023-02-25 11:39:38,382 DEBUG TRAIN Batch 27/6300 loss 8.479181 loss_att 10.280514 loss_ctc 10.364914 loss_rnnt 7.803807 hw_loss 0.119394 lr 0.00032864 rank 5
2023-02-25 11:39:38,428 DEBUG TRAIN Batch 27/6300 loss 8.993370 loss_att 10.338530 loss_ctc 11.662085 loss_rnnt 8.233599 hw_loss 0.252958 lr 0.00032865 rank 4
2023-02-25 11:40:52,848 DEBUG TRAIN Batch 27/6400 loss 5.887904 loss_att 9.176216 loss_ctc 8.119190 loss_rnnt 4.886391 hw_loss 0.086896 lr 0.00032856 rank 0
2023-02-25 11:40:52,848 DEBUG TRAIN Batch 27/6400 loss 5.313415 loss_att 7.760550 loss_ctc 8.336386 loss_rnnt 4.310967 hw_loss 0.206170 lr 0.00032855 rank 7
2023-02-25 11:40:52,852 DEBUG TRAIN Batch 27/6400 loss 9.399977 loss_att 10.028498 loss_ctc 12.506456 loss_rnnt 8.771500 hw_loss 0.166080 lr 0.00032862 rank 1
2023-02-25 11:40:52,852 DEBUG TRAIN Batch 27/6400 loss 5.984829 loss_att 9.898674 loss_ctc 8.043723 loss_rnnt 4.874575 hw_loss 0.099312 lr 0.00032858 rank 3
2023-02-25 11:40:52,854 DEBUG TRAIN Batch 27/6400 loss 2.169047 loss_att 4.611086 loss_ctc 3.471644 loss_rnnt 1.463759 hw_loss 0.081001 lr 0.00032860 rank 2
2023-02-25 11:40:52,856 DEBUG TRAIN Batch 27/6400 loss 7.313703 loss_att 9.595470 loss_ctc 10.671942 loss_rnnt 6.275764 hw_loss 0.250916 lr 0.00032857 rank 5
2023-02-25 11:40:52,860 DEBUG TRAIN Batch 27/6400 loss 2.240923 loss_att 6.125903 loss_ctc 3.589935 loss_rnnt 1.203727 hw_loss 0.150621 lr 0.00032858 rank 4
2023-02-25 11:40:52,895 DEBUG TRAIN Batch 27/6400 loss 10.565031 loss_att 14.077303 loss_ctc 16.105034 loss_rnnt 9.037250 hw_loss 0.162487 lr 0.00032858 rank 6
2023-02-25 11:42:04,481 DEBUG TRAIN Batch 27/6500 loss 3.734105 loss_att 7.299253 loss_ctc 4.035469 loss_rnnt 2.894009 hw_loss 0.162908 lr 0.00032853 rank 2
2023-02-25 11:42:04,485 DEBUG TRAIN Batch 27/6500 loss 8.174836 loss_att 8.715153 loss_ctc 9.426981 loss_rnnt 7.819387 hw_loss 0.150812 lr 0.00032849 rank 0
2023-02-25 11:42:04,485 DEBUG TRAIN Batch 27/6500 loss 8.498794 loss_att 10.249712 loss_ctc 13.471508 loss_rnnt 7.336571 hw_loss 0.279394 lr 0.00032851 rank 6
2023-02-25 11:42:04,486 DEBUG TRAIN Batch 27/6500 loss 3.103019 loss_att 4.376053 loss_ctc 3.271682 loss_rnnt 2.708820 hw_loss 0.219570 lr 0.00032851 rank 3
2023-02-25 11:42:04,486 DEBUG TRAIN Batch 27/6500 loss 6.334901 loss_att 6.139549 loss_ctc 6.290536 loss_rnnt 6.268348 hw_loss 0.209136 lr 0.00032851 rank 4
2023-02-25 11:42:04,488 DEBUG TRAIN Batch 27/6500 loss 4.587020 loss_att 7.968498 loss_ctc 9.488426 loss_rnnt 3.199356 hw_loss 0.108465 lr 0.00032848 rank 7
2023-02-25 11:42:04,492 DEBUG TRAIN Batch 27/6500 loss 8.814575 loss_att 11.397872 loss_ctc 13.372699 loss_rnnt 7.615189 hw_loss 0.140583 lr 0.00032855 rank 1
2023-02-25 11:42:04,491 DEBUG TRAIN Batch 27/6500 loss 8.207224 loss_att 9.910801 loss_ctc 11.102187 loss_rnnt 7.389974 hw_loss 0.169761 lr 0.00032850 rank 5
2023-02-25 11:43:14,953 DEBUG TRAIN Batch 27/6600 loss 4.918164 loss_att 9.852492 loss_ctc 5.823751 loss_rnnt 3.703988 hw_loss 0.199810 lr 0.00032843 rank 5
2023-02-25 11:43:14,965 DEBUG TRAIN Batch 27/6600 loss 11.270348 loss_att 11.862148 loss_ctc 18.468416 loss_rnnt 10.118842 hw_loss 0.137629 lr 0.00032842 rank 0
2023-02-25 11:43:14,966 DEBUG TRAIN Batch 27/6600 loss 8.662566 loss_att 12.391258 loss_ctc 9.482103 loss_rnnt 7.750375 hw_loss 0.107214 lr 0.00032844 rank 3
2023-02-25 11:43:14,967 DEBUG TRAIN Batch 27/6600 loss 6.941354 loss_att 10.036392 loss_ctc 10.200310 loss_rnnt 5.809610 hw_loss 0.146641 lr 0.00032846 rank 2
2023-02-25 11:43:14,971 DEBUG TRAIN Batch 27/6600 loss 4.077662 loss_att 7.785092 loss_ctc 5.692075 loss_rnnt 3.008657 hw_loss 0.210494 lr 0.00032844 rank 6
2023-02-25 11:43:14,973 DEBUG TRAIN Batch 27/6600 loss 5.984000 loss_att 7.515413 loss_ctc 8.047573 loss_rnnt 5.298403 hw_loss 0.195322 lr 0.00032841 rank 7
2023-02-25 11:43:14,974 DEBUG TRAIN Batch 27/6600 loss 2.871006 loss_att 4.287259 loss_ctc 4.015094 loss_rnnt 2.345243 hw_loss 0.168689 lr 0.00032844 rank 4
2023-02-25 11:43:14,975 DEBUG TRAIN Batch 27/6600 loss 2.625141 loss_att 5.694330 loss_ctc 3.141597 loss_rnnt 1.840964 hw_loss 0.190271 lr 0.00032848 rank 1
2023-02-25 11:44:27,613 DEBUG TRAIN Batch 27/6700 loss 5.914900 loss_att 9.574148 loss_ctc 9.085218 loss_rnnt 4.661594 hw_loss 0.185152 lr 0.00032835 rank 0
2023-02-25 11:44:27,630 DEBUG TRAIN Batch 27/6700 loss 4.109908 loss_att 6.486530 loss_ctc 8.145805 loss_rnnt 2.988014 hw_loss 0.203343 lr 0.00032837 rank 3
2023-02-25 11:44:27,631 DEBUG TRAIN Batch 27/6700 loss 18.842712 loss_att 23.811310 loss_ctc 23.692616 loss_rnnt 17.126160 hw_loss 0.142838 lr 0.00032837 rank 4
2023-02-25 11:44:27,632 DEBUG TRAIN Batch 27/6700 loss 10.690880 loss_att 13.491129 loss_ctc 13.421249 loss_rnnt 9.629792 hw_loss 0.256854 lr 0.00032841 rank 1
2023-02-25 11:44:27,632 DEBUG TRAIN Batch 27/6700 loss 7.076025 loss_att 9.165647 loss_ctc 8.663258 loss_rnnt 6.357381 hw_loss 0.167039 lr 0.00032837 rank 6
2023-02-25 11:44:27,635 DEBUG TRAIN Batch 27/6700 loss 6.900949 loss_att 9.822179 loss_ctc 12.358318 loss_rnnt 5.522587 hw_loss 0.124624 lr 0.00032839 rank 2
2023-02-25 11:44:27,636 DEBUG TRAIN Batch 27/6700 loss 5.205177 loss_att 8.170998 loss_ctc 6.966524 loss_rnnt 4.259597 hw_loss 0.220442 lr 0.00032836 rank 5
2023-02-25 11:44:27,637 DEBUG TRAIN Batch 27/6700 loss 6.006964 loss_att 9.207113 loss_ctc 11.639262 loss_rnnt 4.495718 hw_loss 0.225457 lr 0.00032834 rank 7
2023-02-25 11:45:40,372 DEBUG TRAIN Batch 27/6800 loss 11.407869 loss_att 12.616554 loss_ctc 17.390518 loss_rnnt 10.286117 hw_loss 0.154366 lr 0.00032830 rank 3
2023-02-25 11:45:40,375 DEBUG TRAIN Batch 27/6800 loss 12.256335 loss_att 14.362615 loss_ctc 20.031759 loss_rnnt 10.665047 hw_loss 0.249955 lr 0.00032827 rank 0
2023-02-25 11:45:40,375 DEBUG TRAIN Batch 27/6800 loss 4.359024 loss_att 6.727843 loss_ctc 6.722875 loss_rnnt 3.447415 hw_loss 0.229997 lr 0.00032827 rank 7
2023-02-25 11:45:40,376 DEBUG TRAIN Batch 27/6800 loss 10.384420 loss_att 12.469190 loss_ctc 10.440616 loss_rnnt 9.833069 hw_loss 0.237946 lr 0.00032830 rank 6
2023-02-25 11:45:40,378 DEBUG TRAIN Batch 27/6800 loss 5.273265 loss_att 8.222885 loss_ctc 7.134468 loss_rnnt 4.308879 hw_loss 0.236815 lr 0.00032832 rank 2
2023-02-25 11:45:40,382 DEBUG TRAIN Batch 27/6800 loss 5.263329 loss_att 6.067635 loss_ctc 5.937557 loss_rnnt 4.942966 hw_loss 0.130508 lr 0.00032830 rank 4
2023-02-25 11:45:40,383 DEBUG TRAIN Batch 27/6800 loss 9.901045 loss_att 14.979477 loss_ctc 20.482944 loss_rnnt 7.366552 hw_loss 0.202287 lr 0.00032828 rank 5
2023-02-25 11:45:40,386 DEBUG TRAIN Batch 27/6800 loss 9.366011 loss_att 12.865088 loss_ctc 15.341648 loss_rnnt 7.709973 hw_loss 0.299006 lr 0.00032834 rank 1
2023-02-25 11:46:51,199 DEBUG TRAIN Batch 27/6900 loss 9.193565 loss_att 11.523544 loss_ctc 12.031821 loss_rnnt 8.271578 hw_loss 0.145421 lr 0.00032825 rank 2
2023-02-25 11:46:51,199 DEBUG TRAIN Batch 27/6900 loss 2.269608 loss_att 4.416198 loss_ctc 3.806618 loss_rnnt 1.561408 hw_loss 0.138653 lr 0.00032821 rank 5
2023-02-25 11:46:51,207 DEBUG TRAIN Batch 27/6900 loss 10.696824 loss_att 11.666235 loss_ctc 14.412662 loss_rnnt 9.938166 hw_loss 0.129995 lr 0.00032820 rank 7
2023-02-25 11:46:51,210 DEBUG TRAIN Batch 27/6900 loss 6.597571 loss_att 10.852747 loss_ctc 9.422821 loss_rnnt 5.279955 hw_loss 0.168525 lr 0.00032823 rank 3
2023-02-25 11:46:51,209 DEBUG TRAIN Batch 27/6900 loss 9.309650 loss_att 9.760895 loss_ctc 12.606112 loss_rnnt 8.673618 hw_loss 0.199227 lr 0.00032823 rank 4
2023-02-25 11:46:51,209 DEBUG TRAIN Batch 27/6900 loss 6.592636 loss_att 10.534401 loss_ctc 11.036041 loss_rnnt 5.111104 hw_loss 0.188859 lr 0.00032820 rank 0
2023-02-25 11:46:51,210 DEBUG TRAIN Batch 27/6900 loss 8.926144 loss_att 12.912951 loss_ctc 18.120907 loss_rnnt 6.829929 hw_loss 0.136656 lr 0.00032823 rank 6
2023-02-25 11:46:51,215 DEBUG TRAIN Batch 27/6900 loss 8.391645 loss_att 10.153407 loss_ctc 10.306242 loss_rnnt 7.656916 hw_loss 0.238309 lr 0.00032827 rank 1
2023-02-25 11:48:02,227 DEBUG TRAIN Batch 27/7000 loss 8.997704 loss_att 13.998034 loss_ctc 15.129870 loss_rnnt 7.065944 hw_loss 0.213882 lr 0.00032812 rank 7
2023-02-25 11:48:02,232 DEBUG TRAIN Batch 27/7000 loss 9.667829 loss_att 13.049866 loss_ctc 17.514954 loss_rnnt 7.860801 hw_loss 0.158131 lr 0.00032816 rank 6
2023-02-25 11:48:02,244 DEBUG TRAIN Batch 27/7000 loss 7.466043 loss_att 10.419066 loss_ctc 13.026511 loss_rnnt 6.070611 hw_loss 0.118932 lr 0.00032813 rank 0
2023-02-25 11:48:02,247 DEBUG TRAIN Batch 27/7000 loss 9.211761 loss_att 11.791899 loss_ctc 13.794785 loss_rnnt 7.970404 hw_loss 0.214237 lr 0.00032816 rank 3
2023-02-25 11:48:02,248 DEBUG TRAIN Batch 27/7000 loss 12.643148 loss_att 15.722747 loss_ctc 23.691732 loss_rnnt 10.424812 hw_loss 0.242387 lr 0.00032816 rank 4
2023-02-25 11:48:02,251 DEBUG TRAIN Batch 27/7000 loss 9.217618 loss_att 9.526301 loss_ctc 10.648321 loss_rnnt 8.825957 hw_loss 0.260933 lr 0.00032820 rank 1
2023-02-25 11:48:02,251 DEBUG TRAIN Batch 27/7000 loss 8.828893 loss_att 12.056746 loss_ctc 11.727310 loss_rnnt 7.678794 hw_loss 0.221386 lr 0.00032814 rank 5
2023-02-25 11:48:02,252 DEBUG TRAIN Batch 27/7000 loss 9.008134 loss_att 9.504028 loss_ctc 12.391604 loss_rnnt 8.345995 hw_loss 0.209681 lr 0.00032817 rank 2
2023-02-25 11:49:15,315 DEBUG TRAIN Batch 27/7100 loss 5.321495 loss_att 7.020253 loss_ctc 7.424453 loss_rnnt 4.614854 hw_loss 0.162177 lr 0.00032809 rank 6
2023-02-25 11:49:15,318 DEBUG TRAIN Batch 27/7100 loss 3.091732 loss_att 6.372654 loss_ctc 4.292617 loss_rnnt 2.162324 hw_loss 0.212072 lr 0.00032813 rank 1
2023-02-25 11:49:15,320 DEBUG TRAIN Batch 27/7100 loss 6.419585 loss_att 9.282682 loss_ctc 10.248635 loss_rnnt 5.265374 hw_loss 0.133220 lr 0.00032806 rank 0
2023-02-25 11:49:15,321 DEBUG TRAIN Batch 27/7100 loss 12.437265 loss_att 11.319515 loss_ctc 12.551166 loss_rnnt 12.539009 hw_loss 0.199912 lr 0.00032808 rank 4
2023-02-25 11:49:15,321 DEBUG TRAIN Batch 27/7100 loss 26.628338 loss_att 28.451597 loss_ctc 42.135090 loss_rnnt 24.081486 hw_loss 0.214936 lr 0.00032810 rank 2
2023-02-25 11:49:15,322 DEBUG TRAIN Batch 27/7100 loss 12.385517 loss_att 12.647425 loss_ctc 14.794525 loss_rnnt 11.925598 hw_loss 0.161882 lr 0.00032805 rank 7
2023-02-25 11:49:15,322 DEBUG TRAIN Batch 27/7100 loss 5.982520 loss_att 10.073908 loss_ctc 9.534697 loss_rnnt 4.615798 hw_loss 0.140290 lr 0.00032807 rank 5
2023-02-25 11:49:15,360 DEBUG TRAIN Batch 27/7100 loss 5.680300 loss_att 9.547964 loss_ctc 11.020080 loss_rnnt 4.078833 hw_loss 0.217433 lr 0.00032809 rank 3
2023-02-25 11:50:26,838 DEBUG TRAIN Batch 27/7200 loss 5.216997 loss_att 7.164493 loss_ctc 5.607593 loss_rnnt 4.643516 hw_loss 0.247319 lr 0.00032806 rank 1
2023-02-25 11:50:26,846 DEBUG TRAIN Batch 27/7200 loss 11.977504 loss_att 16.281820 loss_ctc 17.632183 loss_rnnt 10.265680 hw_loss 0.181881 lr 0.00032802 rank 3
2023-02-25 11:50:26,848 DEBUG TRAIN Batch 27/7200 loss 4.080558 loss_att 7.927517 loss_ctc 7.708111 loss_rnnt 2.700467 hw_loss 0.238172 lr 0.00032801 rank 4
2023-02-25 11:50:26,848 DEBUG TRAIN Batch 27/7200 loss 14.272631 loss_att 14.964863 loss_ctc 19.843884 loss_rnnt 13.287528 hw_loss 0.194669 lr 0.00032800 rank 5
2023-02-25 11:50:26,850 DEBUG TRAIN Batch 27/7200 loss 10.140639 loss_att 13.732666 loss_ctc 18.698513 loss_rnnt 8.202356 hw_loss 0.147800 lr 0.00032799 rank 0
2023-02-25 11:50:26,851 DEBUG TRAIN Batch 27/7200 loss 8.036058 loss_att 7.976331 loss_ctc 10.737702 loss_rnnt 7.532785 hw_loss 0.290624 lr 0.00032802 rank 6
2023-02-25 11:50:26,852 DEBUG TRAIN Batch 27/7200 loss 6.753962 loss_att 10.545025 loss_ctc 11.961563 loss_rnnt 5.215469 hw_loss 0.161123 lr 0.00032803 rank 2
2023-02-25 11:50:26,864 DEBUG TRAIN Batch 27/7200 loss 8.826090 loss_att 10.512609 loss_ctc 10.815628 loss_rnnt 8.127675 hw_loss 0.179695 lr 0.00032798 rank 7
2023-02-25 11:51:37,184 DEBUG TRAIN Batch 27/7300 loss 14.887213 loss_att 16.101429 loss_ctc 22.849463 loss_rnnt 13.503998 hw_loss 0.147634 lr 0.00032792 rank 0
2023-02-25 11:51:37,183 DEBUG TRAIN Batch 27/7300 loss 7.925365 loss_att 11.979323 loss_ctc 11.119170 loss_rnnt 6.627161 hw_loss 0.115447 lr 0.00032799 rank 1
2023-02-25 11:51:37,186 DEBUG TRAIN Batch 27/7300 loss 3.858184 loss_att 5.233973 loss_ctc 4.526502 loss_rnnt 3.414458 hw_loss 0.148987 lr 0.00032795 rank 6
2023-02-25 11:51:37,191 DEBUG TRAIN Batch 27/7300 loss 10.893694 loss_att 16.147924 loss_ctc 24.089176 loss_rnnt 8.011642 hw_loss 0.134640 lr 0.00032796 rank 2
2023-02-25 11:51:37,191 DEBUG TRAIN Batch 27/7300 loss 8.362350 loss_att 12.235453 loss_ctc 13.413341 loss_rnnt 6.817426 hw_loss 0.181573 lr 0.00032791 rank 7
2023-02-25 11:51:37,192 DEBUG TRAIN Batch 27/7300 loss 3.547605 loss_att 6.716726 loss_ctc 5.241993 loss_rnnt 2.571434 hw_loss 0.218303 lr 0.00032795 rank 3
2023-02-25 11:51:37,195 DEBUG TRAIN Batch 27/7300 loss 4.269961 loss_att 6.326245 loss_ctc 8.831894 loss_rnnt 3.163559 hw_loss 0.162915 lr 0.00032794 rank 4
2023-02-25 11:51:37,196 DEBUG TRAIN Batch 27/7300 loss 19.083542 loss_att 21.170685 loss_ctc 29.711224 loss_rnnt 17.129707 hw_loss 0.223836 lr 0.00032793 rank 5
2023-02-25 11:52:48,391 DEBUG TRAIN Batch 27/7400 loss 6.699639 loss_att 7.388604 loss_ctc 13.791745 loss_rnnt 5.539638 hw_loss 0.143615 lr 0.00032788 rank 3
2023-02-25 11:52:48,392 DEBUG TRAIN Batch 27/7400 loss 14.903264 loss_att 16.652897 loss_ctc 25.011143 loss_rnnt 13.108504 hw_loss 0.182093 lr 0.00032788 rank 6
2023-02-25 11:52:48,395 DEBUG TRAIN Batch 27/7400 loss 7.044465 loss_att 9.444679 loss_ctc 11.176140 loss_rnnt 5.961104 hw_loss 0.098302 lr 0.00032787 rank 4
2023-02-25 11:52:48,396 DEBUG TRAIN Batch 27/7400 loss 5.909165 loss_att 6.414994 loss_ctc 7.258441 loss_rnnt 5.520334 hw_loss 0.202053 lr 0.00032789 rank 2
2023-02-25 11:52:48,400 DEBUG TRAIN Batch 27/7400 loss 11.141553 loss_att 14.290099 loss_ctc 17.559986 loss_rnnt 9.543543 hw_loss 0.210955 lr 0.00032786 rank 5
2023-02-25 11:52:48,400 DEBUG TRAIN Batch 27/7400 loss 10.184455 loss_att 14.828433 loss_ctc 19.187834 loss_rnnt 7.955682 hw_loss 0.186612 lr 0.00032784 rank 7
2023-02-25 11:52:48,408 DEBUG TRAIN Batch 27/7400 loss 9.861504 loss_att 13.020446 loss_ctc 14.347338 loss_rnnt 8.535237 hw_loss 0.180686 lr 0.00032785 rank 0
2023-02-25 11:52:48,416 DEBUG TRAIN Batch 27/7400 loss 10.428726 loss_att 13.520996 loss_ctc 14.412418 loss_rnnt 9.158568 hw_loss 0.226020 lr 0.00032792 rank 1
2023-02-25 11:54:02,709 DEBUG TRAIN Batch 27/7500 loss 7.807214 loss_att 11.299406 loss_ctc 12.986995 loss_rnnt 6.348807 hw_loss 0.129996 lr 0.00032778 rank 0
2023-02-25 11:54:02,710 DEBUG TRAIN Batch 27/7500 loss 9.855413 loss_att 11.835408 loss_ctc 16.828535 loss_rnnt 8.435056 hw_loss 0.177392 lr 0.00032781 rank 3
2023-02-25 11:54:02,713 DEBUG TRAIN Batch 27/7500 loss 7.612726 loss_att 10.566681 loss_ctc 8.991720 loss_rnnt 6.777273 hw_loss 0.113993 lr 0.00032782 rank 2
2023-02-25 11:54:02,712 DEBUG TRAIN Batch 27/7500 loss 6.332257 loss_att 9.096531 loss_ctc 10.205659 loss_rnnt 5.181492 hw_loss 0.152731 lr 0.00032780 rank 4
2023-02-25 11:54:02,714 DEBUG TRAIN Batch 27/7500 loss 5.939449 loss_att 8.852401 loss_ctc 8.184947 loss_rnnt 4.962505 hw_loss 0.178038 lr 0.00032785 rank 1
2023-02-25 11:54:02,714 DEBUG TRAIN Batch 27/7500 loss 5.773128 loss_att 7.545864 loss_ctc 8.571566 loss_rnnt 4.899519 hw_loss 0.273628 lr 0.00032781 rank 6
2023-02-25 11:54:02,715 DEBUG TRAIN Batch 27/7500 loss 8.887423 loss_att 12.319286 loss_ctc 12.630446 loss_rnnt 7.600004 hw_loss 0.191205 lr 0.00032777 rank 7
2023-02-25 11:54:02,718 DEBUG TRAIN Batch 27/7500 loss 10.141440 loss_att 11.904360 loss_ctc 11.217848 loss_rnnt 9.566025 hw_loss 0.148706 lr 0.00032779 rank 5
2023-02-25 11:55:13,320 DEBUG TRAIN Batch 27/7600 loss 3.519146 loss_att 8.097004 loss_ctc 5.265250 loss_rnnt 2.262383 hw_loss 0.203210 lr 0.00032771 rank 0
2023-02-25 11:55:13,325 DEBUG TRAIN Batch 27/7600 loss 8.902566 loss_att 11.186535 loss_ctc 13.887171 loss_rnnt 7.651606 hw_loss 0.242909 lr 0.00032778 rank 1
2023-02-25 11:55:13,327 DEBUG TRAIN Batch 27/7600 loss 11.546044 loss_att 12.682589 loss_ctc 13.356360 loss_rnnt 10.991981 hw_loss 0.160087 lr 0.00032774 rank 3
2023-02-25 11:55:13,329 DEBUG TRAIN Batch 27/7600 loss 11.577874 loss_att 13.358793 loss_ctc 19.644011 loss_rnnt 9.968615 hw_loss 0.332982 lr 0.00032775 rank 2
2023-02-25 11:55:13,328 DEBUG TRAIN Batch 27/7600 loss 7.726647 loss_att 8.918520 loss_ctc 12.988942 loss_rnnt 6.666263 hw_loss 0.225693 lr 0.00032770 rank 7
2023-02-25 11:55:13,333 DEBUG TRAIN Batch 27/7600 loss 12.728131 loss_att 14.297701 loss_ctc 19.997269 loss_rnnt 11.338850 hw_loss 0.199029 lr 0.00032773 rank 6
2023-02-25 11:55:13,334 DEBUG TRAIN Batch 27/7600 loss 6.432469 loss_att 10.743759 loss_ctc 13.369479 loss_rnnt 4.536832 hw_loss 0.203333 lr 0.00032773 rank 4
2023-02-25 11:55:13,337 DEBUG TRAIN Batch 27/7600 loss 9.144406 loss_att 10.962564 loss_ctc 12.046801 loss_rnnt 8.324927 hw_loss 0.129114 lr 0.00032772 rank 5
2023-02-25 11:56:23,652 DEBUG TRAIN Batch 27/7700 loss 7.442458 loss_att 9.322983 loss_ctc 8.510351 loss_rnnt 6.789250 hw_loss 0.252594 lr 0.00032764 rank 0
2023-02-25 11:56:23,655 DEBUG TRAIN Batch 27/7700 loss 5.022041 loss_att 6.555520 loss_ctc 7.667847 loss_rnnt 4.256877 hw_loss 0.198176 lr 0.00032766 rank 3
2023-02-25 11:56:23,657 DEBUG TRAIN Batch 27/7700 loss 5.622348 loss_att 10.796335 loss_ctc 7.630095 loss_rnnt 4.240952 hw_loss 0.147935 lr 0.00032768 rank 2
2023-02-25 11:56:23,658 DEBUG TRAIN Batch 27/7700 loss 7.253686 loss_att 8.664557 loss_ctc 7.015920 loss_rnnt 6.883537 hw_loss 0.224394 lr 0.00032766 rank 6
2023-02-25 11:56:23,658 DEBUG TRAIN Batch 27/7700 loss 8.159098 loss_att 11.983029 loss_ctc 14.096610 loss_rnnt 6.514185 hw_loss 0.165859 lr 0.00032766 rank 4
2023-02-25 11:56:23,659 DEBUG TRAIN Batch 27/7700 loss 2.845592 loss_att 5.060295 loss_ctc 3.361278 loss_rnnt 2.188570 hw_loss 0.272480 lr 0.00032763 rank 7
2023-02-25 11:56:23,660 DEBUG TRAIN Batch 27/7700 loss 5.629903 loss_att 9.876364 loss_ctc 7.350521 loss_rnnt 4.463396 hw_loss 0.164624 lr 0.00032771 rank 1
2023-02-25 11:56:23,714 DEBUG TRAIN Batch 27/7700 loss 8.437748 loss_att 8.772046 loss_ctc 10.821178 loss_rnnt 7.953506 hw_loss 0.186733 lr 0.00032765 rank 5
2023-02-25 11:57:37,691 DEBUG TRAIN Batch 27/7800 loss 6.483037 loss_att 8.782888 loss_ctc 10.551777 loss_rnnt 5.371415 hw_loss 0.204661 lr 0.00032759 rank 6
2023-02-25 11:57:37,694 DEBUG TRAIN Batch 27/7800 loss 7.979869 loss_att 13.760447 loss_ctc 14.853451 loss_rnnt 5.802968 hw_loss 0.195577 lr 0.00032759 rank 4
2023-02-25 11:57:37,694 DEBUG TRAIN Batch 27/7800 loss 3.217430 loss_att 4.864586 loss_ctc 2.230177 loss_rnnt 2.878601 hw_loss 0.264436 lr 0.00032756 rank 7
2023-02-25 11:57:37,697 DEBUG TRAIN Batch 27/7800 loss 5.404868 loss_att 7.802742 loss_ctc 7.844167 loss_rnnt 4.475743 hw_loss 0.233080 lr 0.00032758 rank 5
2023-02-25 11:57:37,699 DEBUG TRAIN Batch 27/7800 loss 14.508512 loss_att 20.528763 loss_ctc 27.582525 loss_rnnt 11.432700 hw_loss 0.241052 lr 0.00032764 rank 1
2023-02-25 11:57:37,699 DEBUG TRAIN Batch 27/7800 loss 7.994185 loss_att 10.102922 loss_ctc 8.251425 loss_rnnt 7.424683 hw_loss 0.212730 lr 0.00032761 rank 2
2023-02-25 11:57:37,702 DEBUG TRAIN Batch 27/7800 loss 5.045812 loss_att 7.158018 loss_ctc 8.359647 loss_rnnt 4.124570 hw_loss 0.106794 lr 0.00032759 rank 3
2023-02-25 11:57:37,703 DEBUG TRAIN Batch 27/7800 loss 9.110045 loss_att 13.521528 loss_ctc 14.683102 loss_rnnt 7.387067 hw_loss 0.183012 lr 0.00032757 rank 0
2023-02-25 11:58:49,256 DEBUG TRAIN Batch 27/7900 loss 6.208933 loss_att 8.742584 loss_ctc 8.402820 loss_rnnt 5.322917 hw_loss 0.162689 lr 0.00032752 rank 6
2023-02-25 11:58:49,256 DEBUG TRAIN Batch 27/7900 loss 3.635114 loss_att 5.734820 loss_ctc 4.716376 loss_rnnt 2.959058 hw_loss 0.209900 lr 0.00032749 rank 7
2023-02-25 11:58:49,260 DEBUG TRAIN Batch 27/7900 loss 12.985237 loss_att 14.119862 loss_ctc 15.991581 loss_rnnt 12.276033 hw_loss 0.152685 lr 0.00032752 rank 4
2023-02-25 11:58:49,262 DEBUG TRAIN Batch 27/7900 loss 5.963493 loss_att 6.971078 loss_ctc 5.015257 loss_rnnt 5.806417 hw_loss 0.153732 lr 0.00032754 rank 2
2023-02-25 11:58:49,261 DEBUG TRAIN Batch 27/7900 loss 5.468874 loss_att 6.951537 loss_ctc 8.762553 loss_rnnt 4.634380 hw_loss 0.185257 lr 0.00032757 rank 1
2023-02-25 11:58:49,265 DEBUG TRAIN Batch 27/7900 loss 9.291869 loss_att 12.830483 loss_ctc 16.134319 loss_rnnt 7.611007 hw_loss 0.114025 lr 0.00032751 rank 5
2023-02-25 11:58:49,266 DEBUG TRAIN Batch 27/7900 loss 7.145753 loss_att 8.779388 loss_ctc 9.854347 loss_rnnt 6.333917 hw_loss 0.232431 lr 0.00032750 rank 0
2023-02-25 11:58:49,311 DEBUG TRAIN Batch 27/7900 loss 5.356248 loss_att 7.136857 loss_ctc 6.333319 loss_rnnt 4.794585 hw_loss 0.141123 lr 0.00032752 rank 3
2023-02-25 11:59:59,422 DEBUG TRAIN Batch 27/8000 loss 7.541613 loss_att 9.961828 loss_ctc 11.110741 loss_rnnt 6.467027 hw_loss 0.214986 lr 0.00032745 rank 6
2023-02-25 11:59:59,422 DEBUG TRAIN Batch 27/8000 loss 7.721121 loss_att 9.725763 loss_ctc 14.154487 loss_rnnt 6.382425 hw_loss 0.149973 lr 0.00032743 rank 0
2023-02-25 11:59:59,427 DEBUG TRAIN Batch 27/8000 loss 13.243564 loss_att 16.018063 loss_ctc 13.296352 loss_rnnt 12.555028 hw_loss 0.237367 lr 0.00032749 rank 1
2023-02-25 11:59:59,429 DEBUG TRAIN Batch 27/8000 loss 4.969878 loss_att 8.281641 loss_ctc 7.279408 loss_rnnt 3.916828 hw_loss 0.155173 lr 0.00032742 rank 7
2023-02-25 11:59:59,430 DEBUG TRAIN Batch 27/8000 loss 12.141656 loss_att 13.781132 loss_ctc 16.268661 loss_rnnt 11.164490 hw_loss 0.185631 lr 0.00032747 rank 2
2023-02-25 11:59:59,432 DEBUG TRAIN Batch 27/8000 loss 5.053961 loss_att 8.372962 loss_ctc 7.456852 loss_rnnt 3.983396 hw_loss 0.161961 lr 0.00032745 rank 3
2023-02-25 11:59:59,432 DEBUG TRAIN Batch 27/8000 loss 5.897878 loss_att 9.721740 loss_ctc 9.463264 loss_rnnt 4.539560 hw_loss 0.221551 lr 0.00032745 rank 4
2023-02-25 11:59:59,435 DEBUG TRAIN Batch 27/8000 loss 6.284426 loss_att 8.059938 loss_ctc 5.904061 loss_rnnt 5.902068 hw_loss 0.146197 lr 0.00032744 rank 5
2023-02-25 12:01:10,924 DEBUG TRAIN Batch 27/8100 loss 4.077437 loss_att 6.008935 loss_ctc 3.954530 loss_rnnt 3.608264 hw_loss 0.186116 lr 0.00032738 rank 6
2023-02-25 12:01:10,926 DEBUG TRAIN Batch 27/8100 loss 14.016717 loss_att 17.290304 loss_ctc 21.969915 loss_rnnt 12.241048 hw_loss 0.113482 lr 0.00032735 rank 7
2023-02-25 12:01:10,927 DEBUG TRAIN Batch 27/8100 loss 13.918584 loss_att 14.517637 loss_ctc 17.016867 loss_rnnt 13.241119 hw_loss 0.271030 lr 0.00032736 rank 0
2023-02-25 12:01:10,927 DEBUG TRAIN Batch 27/8100 loss 8.133246 loss_att 9.796535 loss_ctc 11.816326 loss_rnnt 7.177915 hw_loss 0.246744 lr 0.00032738 rank 3
2023-02-25 12:01:10,928 DEBUG TRAIN Batch 27/8100 loss 17.605310 loss_att 20.143396 loss_ctc 29.532646 loss_rnnt 15.443563 hw_loss 0.119661 lr 0.00032740 rank 2
2023-02-25 12:01:10,930 DEBUG TRAIN Batch 27/8100 loss 15.503266 loss_att 17.681213 loss_ctc 21.763485 loss_rnnt 14.133613 hw_loss 0.186317 lr 0.00032738 rank 4
2023-02-25 12:01:10,933 DEBUG TRAIN Batch 27/8100 loss 7.900897 loss_att 11.351864 loss_ctc 11.703907 loss_rnnt 6.595913 hw_loss 0.201978 lr 0.00032742 rank 1
2023-02-25 12:01:10,937 DEBUG TRAIN Batch 27/8100 loss 8.058621 loss_att 9.658472 loss_ctc 11.102729 loss_rnnt 7.207013 hw_loss 0.235793 lr 0.00032737 rank 5
2023-02-25 12:02:22,349 DEBUG TRAIN Batch 27/8200 loss 9.142478 loss_att 13.802845 loss_ctc 13.974972 loss_rnnt 7.508947 hw_loss 0.107110 lr 0.00032729 rank 0
2023-02-25 12:02:22,349 DEBUG TRAIN Batch 27/8200 loss 4.571308 loss_att 8.319999 loss_ctc 7.970405 loss_rnnt 3.274974 hw_loss 0.175091 lr 0.00032731 rank 6
2023-02-25 12:02:22,354 DEBUG TRAIN Batch 27/8200 loss 13.166999 loss_att 16.367790 loss_ctc 26.372999 loss_rnnt 10.679913 hw_loss 0.161490 lr 0.00032730 rank 5
2023-02-25 12:02:22,357 DEBUG TRAIN Batch 27/8200 loss 5.768789 loss_att 7.326707 loss_ctc 6.556335 loss_rnnt 5.255657 hw_loss 0.181016 lr 0.00032728 rank 7
2023-02-25 12:02:22,358 DEBUG TRAIN Batch 27/8200 loss 3.180860 loss_att 5.399724 loss_ctc 5.175329 loss_rnnt 2.381216 hw_loss 0.168640 lr 0.00032735 rank 1
2023-02-25 12:02:22,359 DEBUG TRAIN Batch 27/8200 loss 10.094281 loss_att 11.756979 loss_ctc 14.172198 loss_rnnt 9.111552 hw_loss 0.199624 lr 0.00032733 rank 2
2023-02-25 12:02:22,375 DEBUG TRAIN Batch 27/8200 loss 9.335794 loss_att 11.075932 loss_ctc 13.878289 loss_rnnt 8.322623 hw_loss 0.111523 lr 0.00032731 rank 4
2023-02-25 12:02:22,383 DEBUG TRAIN Batch 27/8200 loss 13.811556 loss_att 16.846668 loss_ctc 20.038124 loss_rnnt 12.251038 hw_loss 0.231162 lr 0.00032731 rank 3
2023-02-25 12:03:32,013 DEBUG TRAIN Batch 27/8300 loss 9.309955 loss_att 13.604141 loss_ctc 13.832970 loss_rnnt 7.778204 hw_loss 0.130958 lr 0.00032721 rank 7
2023-02-25 12:03:32,024 DEBUG TRAIN Batch 27/8300 loss 8.109589 loss_att 9.710533 loss_ctc 16.412258 loss_rnnt 6.603229 hw_loss 0.148403 lr 0.00032726 rank 2
2023-02-25 12:03:32,024 DEBUG TRAIN Batch 27/8300 loss 5.484600 loss_att 8.266793 loss_ctc 8.856335 loss_rnnt 4.331287 hw_loss 0.276205 lr 0.00032724 rank 3
2023-02-25 12:03:32,024 DEBUG TRAIN Batch 27/8300 loss 9.725160 loss_att 12.151145 loss_ctc 11.365302 loss_rnnt 8.899813 hw_loss 0.227745 lr 0.00032728 rank 1
2023-02-25 12:03:32,026 DEBUG TRAIN Batch 27/8300 loss 7.032049 loss_att 9.398460 loss_ctc 9.388930 loss_rnnt 6.171086 hw_loss 0.137681 lr 0.00032722 rank 0
2023-02-25 12:03:32,026 DEBUG TRAIN Batch 27/8300 loss 16.693256 loss_att 18.255466 loss_ctc 21.902647 loss_rnnt 15.621794 hw_loss 0.120815 lr 0.00032724 rank 6
2023-02-25 12:03:32,028 DEBUG TRAIN Batch 27/8300 loss 9.140179 loss_att 9.517968 loss_ctc 9.962877 loss_rnnt 8.911768 hw_loss 0.080925 lr 0.00032724 rank 4
2023-02-25 12:03:32,032 DEBUG TRAIN Batch 27/8300 loss 9.172140 loss_att 12.244169 loss_ctc 12.715488 loss_rnnt 8.001694 hw_loss 0.156740 lr 0.00032723 rank 5
2023-02-25 12:04:26,895 DEBUG CV Batch 27/0 loss 1.826468 loss_att 1.803166 loss_ctc 2.660717 loss_rnnt 1.624080 hw_loss 0.179654 history loss 1.758821 rank 4
2023-02-25 12:04:26,901 DEBUG CV Batch 27/0 loss 1.826468 loss_att 1.803166 loss_ctc 2.660717 loss_rnnt 1.624080 hw_loss 0.179654 history loss 1.758821 rank 7
2023-02-25 12:04:26,907 DEBUG CV Batch 27/0 loss 1.826468 loss_att 1.803166 loss_ctc 2.660717 loss_rnnt 1.624080 hw_loss 0.179654 history loss 1.758821 rank 1
2023-02-25 12:04:26,907 DEBUG CV Batch 27/0 loss 1.826468 loss_att 1.803166 loss_ctc 2.660717 loss_rnnt 1.624080 hw_loss 0.179654 history loss 1.758821 rank 5
2023-02-25 12:04:26,924 DEBUG CV Batch 27/0 loss 1.826468 loss_att 1.803166 loss_ctc 2.660717 loss_rnnt 1.624080 hw_loss 0.179654 history loss 1.758821 rank 3
2023-02-25 12:04:26,925 DEBUG CV Batch 27/0 loss 1.826468 loss_att 1.803166 loss_ctc 2.660717 loss_rnnt 1.624080 hw_loss 0.179654 history loss 1.758821 rank 6
2023-02-25 12:04:26,926 DEBUG CV Batch 27/0 loss 1.826468 loss_att 1.803166 loss_ctc 2.660717 loss_rnnt 1.624080 hw_loss 0.179654 history loss 1.758821 rank 2
2023-02-25 12:04:26,930 DEBUG CV Batch 27/0 loss 1.826468 loss_att 1.803166 loss_ctc 2.660717 loss_rnnt 1.624080 hw_loss 0.179654 history loss 1.758821 rank 0
2023-02-25 12:04:38,283 DEBUG CV Batch 27/100 loss 5.363167 loss_att 6.172505 loss_ctc 8.337442 loss_rnnt 4.649556 hw_loss 0.290952 history loss 3.282750 rank 6
2023-02-25 12:04:38,287 DEBUG CV Batch 27/100 loss 5.363167 loss_att 6.172505 loss_ctc 8.337442 loss_rnnt 4.649556 hw_loss 0.290952 history loss 3.282750 rank 4
2023-02-25 12:04:38,344 DEBUG CV Batch 27/100 loss 5.363167 loss_att 6.172505 loss_ctc 8.337442 loss_rnnt 4.649556 hw_loss 0.290952 history loss 3.282750 rank 3
2023-02-25 12:04:38,519 DEBUG CV Batch 27/100 loss 5.363167 loss_att 6.172505 loss_ctc 8.337442 loss_rnnt 4.649556 hw_loss 0.290952 history loss 3.282750 rank 5
2023-02-25 12:04:38,633 DEBUG CV Batch 27/100 loss 5.363167 loss_att 6.172505 loss_ctc 8.337442 loss_rnnt 4.649556 hw_loss 0.290952 history loss 3.282750 rank 1
2023-02-25 12:04:38,709 DEBUG CV Batch 27/100 loss 5.363167 loss_att 6.172505 loss_ctc 8.337442 loss_rnnt 4.649556 hw_loss 0.290952 history loss 3.282750 rank 7
2023-02-25 12:04:38,713 DEBUG CV Batch 27/100 loss 5.363167 loss_att 6.172505 loss_ctc 8.337442 loss_rnnt 4.649556 hw_loss 0.290952 history loss 3.282750 rank 2
2023-02-25 12:04:38,937 DEBUG CV Batch 27/100 loss 5.363167 loss_att 6.172505 loss_ctc 8.337442 loss_rnnt 4.649556 hw_loss 0.290952 history loss 3.282750 rank 0
2023-02-25 12:04:51,927 DEBUG CV Batch 27/200 loss 10.364361 loss_att 19.645786 loss_ctc 11.705387 loss_rnnt 8.218897 hw_loss 0.206953 history loss 3.832463 rank 4
2023-02-25 12:04:52,025 DEBUG CV Batch 27/200 loss 10.364361 loss_att 19.645786 loss_ctc 11.705387 loss_rnnt 8.218897 hw_loss 0.206953 history loss 3.832463 rank 6
2023-02-25 12:04:52,200 DEBUG CV Batch 27/200 loss 10.364361 loss_att 19.645786 loss_ctc 11.705387 loss_rnnt 8.218897 hw_loss 0.206953 history loss 3.832463 rank 5
2023-02-25 12:04:52,217 DEBUG CV Batch 27/200 loss 10.364361 loss_att 19.645786 loss_ctc 11.705387 loss_rnnt 8.218897 hw_loss 0.206953 history loss 3.832463 rank 3
2023-02-25 12:04:52,410 DEBUG CV Batch 27/200 loss 10.364361 loss_att 19.645786 loss_ctc 11.705387 loss_rnnt 8.218897 hw_loss 0.206953 history loss 3.832463 rank 1
2023-02-25 12:04:52,811 DEBUG CV Batch 27/200 loss 10.364361 loss_att 19.645786 loss_ctc 11.705387 loss_rnnt 8.218897 hw_loss 0.206953 history loss 3.832463 rank 2
2023-02-25 12:04:53,012 DEBUG CV Batch 27/200 loss 10.364361 loss_att 19.645786 loss_ctc 11.705387 loss_rnnt 8.218897 hw_loss 0.206953 history loss 3.832463 rank 0
2023-02-25 12:04:53,195 DEBUG CV Batch 27/200 loss 10.364361 loss_att 19.645786 loss_ctc 11.705387 loss_rnnt 8.218897 hw_loss 0.206953 history loss 3.832463 rank 7
2023-02-25 12:05:03,794 DEBUG CV Batch 27/300 loss 3.671248 loss_att 4.110638 loss_ctc 4.829303 loss_rnnt 3.369989 hw_loss 0.110574 history loss 3.965102 rank 4
2023-02-25 12:05:03,906 DEBUG CV Batch 27/300 loss 3.671248 loss_att 4.110638 loss_ctc 4.829303 loss_rnnt 3.369989 hw_loss 0.110574 history loss 3.965102 rank 6
2023-02-25 12:05:04,100 DEBUG CV Batch 27/300 loss 3.671248 loss_att 4.110638 loss_ctc 4.829303 loss_rnnt 3.369989 hw_loss 0.110574 history loss 3.965102 rank 3
2023-02-25 12:05:04,572 DEBUG CV Batch 27/300 loss 3.671248 loss_att 4.110638 loss_ctc 4.829303 loss_rnnt 3.369989 hw_loss 0.110574 history loss 3.965102 rank 5
2023-02-25 12:05:04,886 DEBUG CV Batch 27/300 loss 3.671248 loss_att 4.110638 loss_ctc 4.829303 loss_rnnt 3.369989 hw_loss 0.110574 history loss 3.965102 rank 1
2023-02-25 12:05:05,533 DEBUG CV Batch 27/300 loss 3.671248 loss_att 4.110638 loss_ctc 4.829303 loss_rnnt 3.369989 hw_loss 0.110574 history loss 3.965102 rank 2
2023-02-25 12:05:05,717 DEBUG CV Batch 27/300 loss 3.671248 loss_att 4.110638 loss_ctc 4.829303 loss_rnnt 3.369989 hw_loss 0.110574 history loss 3.965102 rank 0
2023-02-25 12:05:05,906 DEBUG CV Batch 27/300 loss 3.671248 loss_att 4.110638 loss_ctc 4.829303 loss_rnnt 3.369989 hw_loss 0.110574 history loss 3.965102 rank 7
2023-02-25 12:05:15,729 DEBUG CV Batch 27/400 loss 17.488302 loss_att 82.064186 loss_ctc 7.157754 loss_rnnt 5.855256 hw_loss 0.178642 history loss 4.886798 rank 4
2023-02-25 12:05:15,889 DEBUG CV Batch 27/400 loss 17.488302 loss_att 82.064186 loss_ctc 7.157754 loss_rnnt 5.855256 hw_loss 0.178642 history loss 4.886798 rank 6
2023-02-25 12:05:16,004 DEBUG CV Batch 27/400 loss 17.488302 loss_att 82.064186 loss_ctc 7.157754 loss_rnnt 5.855256 hw_loss 0.178642 history loss 4.886798 rank 3
2023-02-25 12:05:16,836 DEBUG CV Batch 27/400 loss 17.488302 loss_att 82.064186 loss_ctc 7.157754 loss_rnnt 5.855256 hw_loss 0.178642 history loss 4.886798 rank 5
2023-02-25 12:05:17,271 DEBUG CV Batch 27/400 loss 17.488302 loss_att 82.064186 loss_ctc 7.157754 loss_rnnt 5.855256 hw_loss 0.178642 history loss 4.886798 rank 1
2023-02-25 12:05:18,130 DEBUG CV Batch 27/400 loss 17.488302 loss_att 82.064186 loss_ctc 7.157754 loss_rnnt 5.855256 hw_loss 0.178642 history loss 4.886798 rank 2
2023-02-25 12:05:18,443 DEBUG CV Batch 27/400 loss 17.488302 loss_att 82.064186 loss_ctc 7.157754 loss_rnnt 5.855256 hw_loss 0.178642 history loss 4.886798 rank 0
2023-02-25 12:05:18,711 DEBUG CV Batch 27/400 loss 17.488302 loss_att 82.064186 loss_ctc 7.157754 loss_rnnt 5.855256 hw_loss 0.178642 history loss 4.886798 rank 7
2023-02-25 12:05:26,086 DEBUG CV Batch 27/500 loss 4.001632 loss_att 5.069839 loss_ctc 5.855782 loss_rnnt 3.466552 hw_loss 0.139159 history loss 5.613180 rank 4
2023-02-25 12:05:26,304 DEBUG CV Batch 27/500 loss 4.001632 loss_att 5.069839 loss_ctc 5.855782 loss_rnnt 3.466552 hw_loss 0.139159 history loss 5.613180 rank 6
2023-02-25 12:05:26,545 DEBUG CV Batch 27/500 loss 4.001632 loss_att 5.069839 loss_ctc 5.855782 loss_rnnt 3.466552 hw_loss 0.139159 history loss 5.613180 rank 3
2023-02-25 12:05:27,192 DEBUG CV Batch 27/500 loss 4.001632 loss_att 5.069839 loss_ctc 5.855782 loss_rnnt 3.466552 hw_loss 0.139159 history loss 5.613180 rank 5
2023-02-25 12:05:28,107 DEBUG CV Batch 27/500 loss 4.001632 loss_att 5.069839 loss_ctc 5.855782 loss_rnnt 3.466552 hw_loss 0.139159 history loss 5.613180 rank 1
2023-02-25 12:05:29,234 DEBUG CV Batch 27/500 loss 4.001632 loss_att 5.069839 loss_ctc 5.855782 loss_rnnt 3.466552 hw_loss 0.139159 history loss 5.613180 rank 2
2023-02-25 12:05:29,616 DEBUG CV Batch 27/500 loss 4.001632 loss_att 5.069839 loss_ctc 5.855782 loss_rnnt 3.466552 hw_loss 0.139159 history loss 5.613180 rank 0
2023-02-25 12:05:29,918 DEBUG CV Batch 27/500 loss 4.001632 loss_att 5.069839 loss_ctc 5.855782 loss_rnnt 3.466552 hw_loss 0.139159 history loss 5.613180 rank 7
2023-02-25 12:05:38,071 DEBUG CV Batch 27/600 loss 5.446663 loss_att 6.050978 loss_ctc 7.453609 loss_rnnt 4.919920 hw_loss 0.259289 history loss 6.522149 rank 4
2023-02-25 12:05:38,493 DEBUG CV Batch 27/600 loss 5.446663 loss_att 6.050978 loss_ctc 7.453609 loss_rnnt 4.919920 hw_loss 0.259289 history loss 6.522149 rank 6
2023-02-25 12:05:38,861 DEBUG CV Batch 27/600 loss 5.446663 loss_att 6.050978 loss_ctc 7.453609 loss_rnnt 4.919920 hw_loss 0.259289 history loss 6.522149 rank 3
2023-02-25 12:05:39,302 DEBUG CV Batch 27/600 loss 5.446663 loss_att 6.050978 loss_ctc 7.453609 loss_rnnt 4.919920 hw_loss 0.259289 history loss 6.522149 rank 5
2023-02-25 12:05:40,626 DEBUG CV Batch 27/600 loss 5.446663 loss_att 6.050978 loss_ctc 7.453609 loss_rnnt 4.919920 hw_loss 0.259289 history loss 6.522149 rank 1
2023-02-25 12:05:41,889 DEBUG CV Batch 27/600 loss 5.446663 loss_att 6.050978 loss_ctc 7.453609 loss_rnnt 4.919920 hw_loss 0.259289 history loss 6.522149 rank 2
2023-02-25 12:05:42,420 DEBUG CV Batch 27/600 loss 5.446663 loss_att 6.050978 loss_ctc 7.453609 loss_rnnt 4.919920 hw_loss 0.259289 history loss 6.522149 rank 0
2023-02-25 12:05:42,653 DEBUG CV Batch 27/600 loss 5.446663 loss_att 6.050978 loss_ctc 7.453609 loss_rnnt 4.919920 hw_loss 0.259289 history loss 6.522149 rank 7
2023-02-25 12:05:50,313 DEBUG CV Batch 27/700 loss 16.505365 loss_att 42.405487 loss_ctc 17.202396 loss_rnnt 11.187761 hw_loss 0.083703 history loss 7.158444 rank 4
2023-02-25 12:05:50,699 DEBUG CV Batch 27/700 loss 16.505365 loss_att 42.405487 loss_ctc 17.202396 loss_rnnt 11.187761 hw_loss 0.083703 history loss 7.158444 rank 6
2023-02-25 12:05:50,772 DEBUG CV Batch 27/700 loss 16.505365 loss_att 42.405487 loss_ctc 17.202396 loss_rnnt 11.187761 hw_loss 0.083703 history loss 7.158444 rank 5
2023-02-25 12:05:50,982 DEBUG CV Batch 27/700 loss 16.505365 loss_att 42.405487 loss_ctc 17.202396 loss_rnnt 11.187761 hw_loss 0.083703 history loss 7.158444 rank 3
2023-02-25 12:05:52,406 DEBUG CV Batch 27/700 loss 16.505365 loss_att 42.405487 loss_ctc 17.202396 loss_rnnt 11.187761 hw_loss 0.083703 history loss 7.158444 rank 1
2023-02-25 12:05:53,803 DEBUG CV Batch 27/700 loss 16.505365 loss_att 42.405487 loss_ctc 17.202396 loss_rnnt 11.187761 hw_loss 0.083703 history loss 7.158444 rank 2
2023-02-25 12:05:54,308 DEBUG CV Batch 27/700 loss 16.505365 loss_att 42.405487 loss_ctc 17.202396 loss_rnnt 11.187761 hw_loss 0.083703 history loss 7.158444 rank 0
2023-02-25 12:05:54,811 DEBUG CV Batch 27/700 loss 16.505365 loss_att 42.405487 loss_ctc 17.202396 loss_rnnt 11.187761 hw_loss 0.083703 history loss 7.158444 rank 7
2023-02-25 12:06:02,215 DEBUG CV Batch 27/800 loss 9.591028 loss_att 9.607896 loss_ctc 15.386648 loss_rnnt 8.690501 hw_loss 0.233260 history loss 6.640899 rank 4
2023-02-25 12:06:02,236 DEBUG CV Batch 27/800 loss 9.591028 loss_att 9.607896 loss_ctc 15.386648 loss_rnnt 8.690501 hw_loss 0.233260 history loss 6.640899 rank 5
2023-02-25 12:06:02,275 DEBUG CV Batch 27/800 loss 9.591028 loss_att 9.607896 loss_ctc 15.386648 loss_rnnt 8.690501 hw_loss 0.233260 history loss 6.640899 rank 6
2023-02-25 12:06:02,919 DEBUG CV Batch 27/800 loss 9.591028 loss_att 9.607896 loss_ctc 15.386648 loss_rnnt 8.690501 hw_loss 0.233260 history loss 6.640899 rank 3
2023-02-25 12:06:03,894 DEBUG CV Batch 27/800 loss 9.591028 loss_att 9.607896 loss_ctc 15.386648 loss_rnnt 8.690501 hw_loss 0.233260 history loss 6.640899 rank 1
2023-02-25 12:06:05,543 DEBUG CV Batch 27/800 loss 9.591028 loss_att 9.607896 loss_ctc 15.386648 loss_rnnt 8.690501 hw_loss 0.233260 history loss 6.640899 rank 2
2023-02-25 12:06:06,056 DEBUG CV Batch 27/800 loss 9.591028 loss_att 9.607896 loss_ctc 15.386648 loss_rnnt 8.690501 hw_loss 0.233260 history loss 6.640899 rank 0
2023-02-25 12:06:06,613 DEBUG CV Batch 27/800 loss 9.591028 loss_att 9.607896 loss_ctc 15.386648 loss_rnnt 8.690501 hw_loss 0.233260 history loss 6.640899 rank 7
2023-02-25 12:06:16,016 DEBUG CV Batch 27/900 loss 13.441978 loss_att 16.125145 loss_ctc 23.645296 loss_rnnt 11.498075 hw_loss 0.087800 history loss 6.444551 rank 6
2023-02-25 12:06:16,021 DEBUG CV Batch 27/900 loss 13.441978 loss_att 16.125145 loss_ctc 23.645296 loss_rnnt 11.498075 hw_loss 0.087800 history loss 6.444551 rank 5
2023-02-25 12:06:16,278 DEBUG CV Batch 27/900 loss 13.441978 loss_att 16.125145 loss_ctc 23.645296 loss_rnnt 11.498075 hw_loss 0.087800 history loss 6.444551 rank 4
2023-02-25 12:06:16,413 DEBUG CV Batch 27/900 loss 13.441978 loss_att 16.125145 loss_ctc 23.645296 loss_rnnt 11.498075 hw_loss 0.087800 history loss 6.444551 rank 3
2023-02-25 12:06:17,556 DEBUG CV Batch 27/900 loss 13.441978 loss_att 16.125145 loss_ctc 23.645296 loss_rnnt 11.498075 hw_loss 0.087800 history loss 6.444551 rank 1
2023-02-25 12:06:19,585 DEBUG CV Batch 27/900 loss 13.441978 loss_att 16.125145 loss_ctc 23.645296 loss_rnnt 11.498075 hw_loss 0.087800 history loss 6.444551 rank 2
2023-02-25 12:06:20,190 DEBUG CV Batch 27/900 loss 13.441978 loss_att 16.125145 loss_ctc 23.645296 loss_rnnt 11.498075 hw_loss 0.087800 history loss 6.444551 rank 0
2023-02-25 12:06:20,740 DEBUG CV Batch 27/900 loss 13.441978 loss_att 16.125145 loss_ctc 23.645296 loss_rnnt 11.498075 hw_loss 0.087800 history loss 6.444551 rank 7
2023-02-25 12:06:28,135 DEBUG CV Batch 27/1000 loss 4.715549 loss_att 4.543360 loss_ctc 5.020784 loss_rnnt 4.576194 hw_loss 0.249552 history loss 6.220786 rank 6
2023-02-25 12:06:28,177 DEBUG CV Batch 27/1000 loss 4.715549 loss_att 4.543360 loss_ctc 5.020784 loss_rnnt 4.576194 hw_loss 0.249552 history loss 6.220786 rank 5
2023-02-25 12:06:28,283 DEBUG CV Batch 27/1000 loss 4.715549 loss_att 4.543360 loss_ctc 5.020784 loss_rnnt 4.576194 hw_loss 0.249552 history loss 6.220786 rank 4
2023-02-25 12:06:28,545 DEBUG CV Batch 27/1000 loss 4.715549 loss_att 4.543360 loss_ctc 5.020784 loss_rnnt 4.576194 hw_loss 0.249552 history loss 6.220786 rank 3
2023-02-25 12:06:30,573 DEBUG CV Batch 27/1000 loss 4.715549 loss_att 4.543360 loss_ctc 5.020784 loss_rnnt 4.576194 hw_loss 0.249552 history loss 6.220786 rank 1
2023-02-25 12:06:32,523 DEBUG CV Batch 27/1000 loss 4.715549 loss_att 4.543360 loss_ctc 5.020784 loss_rnnt 4.576194 hw_loss 0.249552 history loss 6.220786 rank 2
2023-02-25 12:06:33,149 DEBUG CV Batch 27/1000 loss 4.715549 loss_att 4.543360 loss_ctc 5.020784 loss_rnnt 4.576194 hw_loss 0.249552 history loss 6.220786 rank 0
2023-02-25 12:06:33,702 DEBUG CV Batch 27/1000 loss 4.715549 loss_att 4.543360 loss_ctc 5.020784 loss_rnnt 4.576194 hw_loss 0.249552 history loss 6.220786 rank 7
2023-02-25 12:06:39,933 DEBUG CV Batch 27/1100 loss 6.258822 loss_att 5.623457 loss_ctc 8.815641 loss_rnnt 5.943309 hw_loss 0.190645 history loss 6.197483 rank 6
2023-02-25 12:06:40,010 DEBUG CV Batch 27/1100 loss 6.258822 loss_att 5.623457 loss_ctc 8.815641 loss_rnnt 5.943309 hw_loss 0.190645 history loss 6.197483 rank 5
2023-02-25 12:06:40,113 DEBUG CV Batch 27/1100 loss 6.258822 loss_att 5.623457 loss_ctc 8.815641 loss_rnnt 5.943309 hw_loss 0.190645 history loss 6.197483 rank 4
2023-02-25 12:06:40,515 DEBUG CV Batch 27/1100 loss 6.258822 loss_att 5.623457 loss_ctc 8.815641 loss_rnnt 5.943309 hw_loss 0.190645 history loss 6.197483 rank 3
2023-02-25 12:06:42,831 DEBUG CV Batch 27/1100 loss 6.258822 loss_att 5.623457 loss_ctc 8.815641 loss_rnnt 5.943309 hw_loss 0.190645 history loss 6.197483 rank 1
2023-02-25 12:06:45,167 DEBUG CV Batch 27/1100 loss 6.258822 loss_att 5.623457 loss_ctc 8.815641 loss_rnnt 5.943309 hw_loss 0.190645 history loss 6.197483 rank 2
2023-02-25 12:06:45,826 DEBUG CV Batch 27/1100 loss 6.258822 loss_att 5.623457 loss_ctc 8.815641 loss_rnnt 5.943309 hw_loss 0.190645 history loss 6.197483 rank 0
2023-02-25 12:06:46,414 DEBUG CV Batch 27/1100 loss 6.258822 loss_att 5.623457 loss_ctc 8.815641 loss_rnnt 5.943309 hw_loss 0.190645 history loss 6.197483 rank 7
2023-02-25 12:06:50,142 DEBUG CV Batch 27/1200 loss 8.083862 loss_att 8.078136 loss_ctc 9.859908 loss_rnnt 7.681377 hw_loss 0.312794 history loss 6.507983 rank 6
2023-02-25 12:06:50,387 DEBUG CV Batch 27/1200 loss 8.083862 loss_att 8.078136 loss_ctc 9.859908 loss_rnnt 7.681377 hw_loss 0.312794 history loss 6.507983 rank 4
2023-02-25 12:06:50,784 DEBUG CV Batch 27/1200 loss 8.083862 loss_att 8.078136 loss_ctc 9.859908 loss_rnnt 7.681377 hw_loss 0.312794 history loss 6.507983 rank 5
2023-02-25 12:06:50,905 DEBUG CV Batch 27/1200 loss 8.083862 loss_att 8.078136 loss_ctc 9.859908 loss_rnnt 7.681377 hw_loss 0.312794 history loss 6.507983 rank 3
2023-02-25 12:06:53,953 DEBUG CV Batch 27/1200 loss 8.083862 loss_att 8.078136 loss_ctc 9.859908 loss_rnnt 7.681377 hw_loss 0.312794 history loss 6.507983 rank 1
2023-02-25 12:06:56,515 DEBUG CV Batch 27/1200 loss 8.083862 loss_att 8.078136 loss_ctc 9.859908 loss_rnnt 7.681377 hw_loss 0.312794 history loss 6.507983 rank 2
2023-02-25 12:06:57,178 DEBUG CV Batch 27/1200 loss 8.083862 loss_att 8.078136 loss_ctc 9.859908 loss_rnnt 7.681377 hw_loss 0.312794 history loss 6.507983 rank 0
2023-02-25 12:06:58,032 DEBUG CV Batch 27/1200 loss 8.083862 loss_att 8.078136 loss_ctc 9.859908 loss_rnnt 7.681377 hw_loss 0.312794 history loss 6.507983 rank 7
2023-02-25 12:07:02,138 DEBUG CV Batch 27/1300 loss 4.237757 loss_att 5.052580 loss_ctc 6.403131 loss_rnnt 3.689033 hw_loss 0.181955 history loss 6.806515 rank 6
2023-02-25 12:07:02,376 DEBUG CV Batch 27/1300 loss 4.237757 loss_att 5.052580 loss_ctc 6.403131 loss_rnnt 3.689033 hw_loss 0.181955 history loss 6.806515 rank 4
2023-02-25 12:07:02,686 DEBUG CV Batch 27/1300 loss 4.237757 loss_att 5.052580 loss_ctc 6.403131 loss_rnnt 3.689033 hw_loss 0.181955 history loss 6.806515 rank 5
2023-02-25 12:07:02,820 DEBUG CV Batch 27/1300 loss 4.237757 loss_att 5.052580 loss_ctc 6.403131 loss_rnnt 3.689033 hw_loss 0.181955 history loss 6.806515 rank 3
2023-02-25 12:07:06,269 DEBUG CV Batch 27/1300 loss 4.237757 loss_att 5.052580 loss_ctc 6.403131 loss_rnnt 3.689033 hw_loss 0.181955 history loss 6.806515 rank 1
2023-02-25 12:07:09,345 DEBUG CV Batch 27/1300 loss 4.237757 loss_att 5.052580 loss_ctc 6.403131 loss_rnnt 3.689033 hw_loss 0.181955 history loss 6.806515 rank 2
2023-02-25 12:07:09,990 DEBUG CV Batch 27/1300 loss 4.237757 loss_att 5.052580 loss_ctc 6.403131 loss_rnnt 3.689033 hw_loss 0.181955 history loss 6.806515 rank 0
2023-02-25 12:07:10,726 DEBUG CV Batch 27/1300 loss 4.237757 loss_att 5.052580 loss_ctc 6.403131 loss_rnnt 3.689033 hw_loss 0.181955 history loss 6.806515 rank 7
2023-02-25 12:07:13,406 DEBUG CV Batch 27/1400 loss 6.357575 loss_att 14.904720 loss_ctc 6.239138 loss_rnnt 4.550518 hw_loss 0.212661 history loss 7.123816 rank 6
2023-02-25 12:07:13,743 DEBUG CV Batch 27/1400 loss 6.357575 loss_att 14.904720 loss_ctc 6.239138 loss_rnnt 4.550518 hw_loss 0.212661 history loss 7.123816 rank 5
2023-02-25 12:07:13,893 DEBUG CV Batch 27/1400 loss 6.357575 loss_att 14.904720 loss_ctc 6.239138 loss_rnnt 4.550518 hw_loss 0.212661 history loss 7.123816 rank 3
2023-02-25 12:07:14,040 DEBUG CV Batch 27/1400 loss 6.357575 loss_att 14.904720 loss_ctc 6.239138 loss_rnnt 4.550518 hw_loss 0.212661 history loss 7.123816 rank 4
2023-02-25 12:07:17,958 DEBUG CV Batch 27/1400 loss 6.357575 loss_att 14.904720 loss_ctc 6.239138 loss_rnnt 4.550518 hw_loss 0.212661 history loss 7.123816 rank 1
2023-02-25 12:07:21,302 DEBUG CV Batch 27/1400 loss 6.357575 loss_att 14.904720 loss_ctc 6.239138 loss_rnnt 4.550518 hw_loss 0.212661 history loss 7.123816 rank 2
2023-02-25 12:07:22,015 DEBUG CV Batch 27/1400 loss 6.357575 loss_att 14.904720 loss_ctc 6.239138 loss_rnnt 4.550518 hw_loss 0.212661 history loss 7.123816 rank 0
2023-02-25 12:07:22,730 DEBUG CV Batch 27/1400 loss 6.357575 loss_att 14.904720 loss_ctc 6.239138 loss_rnnt 4.550518 hw_loss 0.212661 history loss 7.123816 rank 7
2023-02-25 12:07:25,311 DEBUG CV Batch 27/1500 loss 7.598319 loss_att 8.947539 loss_ctc 7.245159 loss_rnnt 7.227110 hw_loss 0.278349 history loss 6.959938 rank 6
2023-02-25 12:07:25,596 DEBUG CV Batch 27/1500 loss 7.598319 loss_att 8.947539 loss_ctc 7.245159 loss_rnnt 7.227110 hw_loss 0.278349 history loss 6.959938 rank 5
2023-02-25 12:07:25,924 DEBUG CV Batch 27/1500 loss 7.598319 loss_att 8.947539 loss_ctc 7.245159 loss_rnnt 7.227110 hw_loss 0.278349 history loss 6.959938 rank 3
2023-02-25 12:07:26,857 DEBUG CV Batch 27/1500 loss 7.598319 loss_att 8.947539 loss_ctc 7.245159 loss_rnnt 7.227110 hw_loss 0.278349 history loss 6.959938 rank 4
2023-02-25 12:07:29,938 DEBUG CV Batch 27/1500 loss 7.598319 loss_att 8.947539 loss_ctc 7.245159 loss_rnnt 7.227110 hw_loss 0.278349 history loss 6.959938 rank 1
2023-02-25 12:07:33,494 DEBUG CV Batch 27/1500 loss 7.598319 loss_att 8.947539 loss_ctc 7.245159 loss_rnnt 7.227110 hw_loss 0.278349 history loss 6.959938 rank 2
2023-02-25 12:07:33,949 DEBUG CV Batch 27/1500 loss 7.598319 loss_att 8.947539 loss_ctc 7.245159 loss_rnnt 7.227110 hw_loss 0.278349 history loss 6.959938 rank 0
2023-02-25 12:07:35,022 DEBUG CV Batch 27/1500 loss 7.598319 loss_att 8.947539 loss_ctc 7.245159 loss_rnnt 7.227110 hw_loss 0.278349 history loss 6.959938 rank 7
2023-02-25 12:07:38,852 DEBUG CV Batch 27/1600 loss 10.107347 loss_att 16.051413 loss_ctc 10.339099 loss_rnnt 8.808555 hw_loss 0.148274 history loss 6.895552 rank 6
2023-02-25 12:07:39,276 DEBUG CV Batch 27/1600 loss 10.107347 loss_att 16.051413 loss_ctc 10.339099 loss_rnnt 8.808555 hw_loss 0.148274 history loss 6.895552 rank 3
2023-02-25 12:07:39,380 DEBUG CV Batch 27/1600 loss 10.107347 loss_att 16.051413 loss_ctc 10.339099 loss_rnnt 8.808555 hw_loss 0.148274 history loss 6.895552 rank 5
2023-02-25 12:07:40,415 DEBUG CV Batch 27/1600 loss 10.107347 loss_att 16.051413 loss_ctc 10.339099 loss_rnnt 8.808555 hw_loss 0.148274 history loss 6.895552 rank 4
2023-02-25 12:07:43,525 DEBUG CV Batch 27/1600 loss 10.107347 loss_att 16.051413 loss_ctc 10.339099 loss_rnnt 8.808555 hw_loss 0.148274 history loss 6.895552 rank 1
2023-02-25 12:07:47,257 DEBUG CV Batch 27/1600 loss 10.107347 loss_att 16.051413 loss_ctc 10.339099 loss_rnnt 8.808555 hw_loss 0.148274 history loss 6.895552 rank 2
2023-02-25 12:07:47,812 DEBUG CV Batch 27/1600 loss 10.107347 loss_att 16.051413 loss_ctc 10.339099 loss_rnnt 8.808555 hw_loss 0.148274 history loss 6.895552 rank 0
2023-02-25 12:07:48,853 DEBUG CV Batch 27/1600 loss 10.107347 loss_att 16.051413 loss_ctc 10.339099 loss_rnnt 8.808555 hw_loss 0.148274 history loss 6.895552 rank 7
2023-02-25 12:07:51,292 DEBUG CV Batch 27/1700 loss 9.296119 loss_att 8.168533 loss_ctc 13.875814 loss_rnnt 8.782482 hw_loss 0.240988 history loss 6.799708 rank 6
2023-02-25 12:07:51,728 DEBUG CV Batch 27/1700 loss 9.296119 loss_att 8.168533 loss_ctc 13.875814 loss_rnnt 8.782482 hw_loss 0.240988 history loss 6.799708 rank 3
2023-02-25 12:07:51,833 DEBUG CV Batch 27/1700 loss 9.296119 loss_att 8.168533 loss_ctc 13.875814 loss_rnnt 8.782482 hw_loss 0.240988 history loss 6.799708 rank 5
2023-02-25 12:07:52,790 DEBUG CV Batch 27/1700 loss 9.296119 loss_att 8.168533 loss_ctc 13.875814 loss_rnnt 8.782482 hw_loss 0.240988 history loss 6.799708 rank 4
2023-02-25 12:07:56,287 DEBUG CV Batch 27/1700 loss 9.296119 loss_att 8.168533 loss_ctc 13.875814 loss_rnnt 8.782482 hw_loss 0.240988 history loss 6.799708 rank 1
2023-02-25 12:07:59,998 DEBUG CV Batch 27/1700 loss 9.296119 loss_att 8.168533 loss_ctc 13.875814 loss_rnnt 8.782482 hw_loss 0.240988 history loss 6.799708 rank 2
2023-02-25 12:08:00,491 DEBUG CV Batch 27/1700 loss 9.296119 loss_att 8.168533 loss_ctc 13.875814 loss_rnnt 8.782482 hw_loss 0.240988 history loss 6.799708 rank 0
2023-02-25 12:08:00,805 INFO Epoch 27 CV info cv_loss 6.769415466536726
2023-02-25 12:08:00,805 INFO Epoch 28 TRAIN info lr 0.0003272415299309786
2023-02-25 12:08:00,813 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 12:08:01,169 INFO Epoch 27 CV info cv_loss 6.769415466716557
2023-02-25 12:08:01,169 INFO Epoch 28 TRAIN info lr 0.0003272212066988427
2023-02-25 12:08:01,174 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 12:08:01,485 DEBUG CV Batch 27/1700 loss 9.296119 loss_att 8.168533 loss_ctc 13.875814 loss_rnnt 8.782482 hw_loss 0.240988 history loss 6.799708 rank 7
2023-02-25 12:08:01,555 INFO Epoch 27 CV info cv_loss 6.7694154678784555
2023-02-25 12:08:01,556 INFO Epoch 28 TRAIN info lr 0.0003272113968384503
2023-02-25 12:08:01,561 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 12:08:02,114 INFO Epoch 27 CV info cv_loss 6.76941546655611
2023-02-25 12:08:02,114 INFO Epoch 28 TRAIN info lr 0.0003272036897099136
2023-02-25 12:08:02,116 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 12:08:05,357 INFO Epoch 27 CV info cv_loss 6.769415466891004
2023-02-25 12:08:05,358 INFO Epoch 28 TRAIN info lr 0.0003272632589533567
2023-02-25 12:08:05,363 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 12:08:09,469 INFO Epoch 27 CV info cv_loss 6.7694154686117765
2023-02-25 12:08:09,469 INFO Epoch 28 TRAIN info lr 0.0003272331198584196
2023-02-25 12:08:09,471 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 12:08:09,861 INFO Epoch 27 CV info cv_loss 6.769415465561119
2023-02-25 12:08:09,862 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/27.pt
2023-02-25 12:08:10,474 INFO Epoch 28 TRAIN info lr 0.00032716866416996254
2023-02-25 12:08:10,478 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 12:08:10,934 INFO Epoch 27 CV info cv_loss 6.769415466054307
2023-02-25 12:08:10,934 INFO Epoch 28 TRAIN info lr 0.000327178470187341
2023-02-25 12:08:10,936 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 12:09:12,421 DEBUG TRAIN Batch 28/0 loss 9.939474 loss_att 9.259398 loss_ctc 12.857775 loss_rnnt 9.544308 hw_loss 0.266389 lr 0.00032726 rank 1
2023-02-25 12:09:12,422 DEBUG TRAIN Batch 28/0 loss 6.531158 loss_att 6.507062 loss_ctc 8.789795 loss_rnnt 6.145744 hw_loss 0.167027 lr 0.00032722 rank 3
2023-02-25 12:09:12,426 DEBUG TRAIN Batch 28/0 loss 5.914519 loss_att 6.039136 loss_ctc 7.608024 loss_rnnt 5.553844 hw_loss 0.206160 lr 0.00032724 rank 6
2023-02-25 12:09:12,428 DEBUG TRAIN Batch 28/0 loss 11.062161 loss_att 10.997599 loss_ctc 15.124082 loss_rnnt 10.423718 hw_loss 0.205810 lr 0.00032720 rank 4
2023-02-25 12:09:12,429 DEBUG TRAIN Batch 28/0 loss 9.787356 loss_att 8.742506 loss_ctc 12.201352 loss_rnnt 9.555138 hw_loss 0.223733 lr 0.00032723 rank 2
2023-02-25 12:09:12,432 DEBUG TRAIN Batch 28/0 loss 10.237109 loss_att 9.795016 loss_ctc 12.939868 loss_rnnt 9.852582 hw_loss 0.211083 lr 0.00032717 rank 0
2023-02-25 12:09:12,437 DEBUG TRAIN Batch 28/0 loss 11.288737 loss_att 10.571695 loss_ctc 13.670094 loss_rnnt 11.010729 hw_loss 0.194817 lr 0.00032721 rank 5
2023-02-25 12:09:12,452 DEBUG TRAIN Batch 28/0 loss 8.114679 loss_att 7.899941 loss_ctc 11.290698 loss_rnnt 7.575958 hw_loss 0.296625 lr 0.00032718 rank 7
2023-02-25 12:10:22,123 DEBUG TRAIN Batch 28/100 loss 11.122059 loss_att 15.104975 loss_ctc 16.179142 loss_rnnt 9.532549 hw_loss 0.222466 lr 0.00032714 rank 5
2023-02-25 12:10:22,132 DEBUG TRAIN Batch 28/100 loss 7.573629 loss_att 10.246778 loss_ctc 17.563084 loss_rnnt 5.588903 hw_loss 0.221565 lr 0.00032711 rank 7
2023-02-25 12:10:22,132 DEBUG TRAIN Batch 28/100 loss 3.953946 loss_att 7.470224 loss_ctc 6.603998 loss_rnnt 2.830880 hw_loss 0.124631 lr 0.00032717 rank 6
2023-02-25 12:10:22,133 DEBUG TRAIN Batch 28/100 loss 20.834129 loss_att 26.279701 loss_ctc 42.470192 loss_rnnt 16.788357 hw_loss 0.134715 lr 0.00032710 rank 0
2023-02-25 12:10:22,134 DEBUG TRAIN Batch 28/100 loss 5.253545 loss_att 9.188938 loss_ctc 10.579912 loss_rnnt 3.676599 hw_loss 0.149409 lr 0.00032719 rank 1
2023-02-25 12:10:22,134 DEBUG TRAIN Batch 28/100 loss 4.880743 loss_att 7.062191 loss_ctc 6.261280 loss_rnnt 4.190107 hw_loss 0.131766 lr 0.00032715 rank 3
2023-02-25 12:10:22,139 DEBUG TRAIN Batch 28/100 loss 8.506284 loss_att 12.073473 loss_ctc 12.881270 loss_rnnt 7.131101 hw_loss 0.147026 lr 0.00032716 rank 2
2023-02-25 12:10:22,139 DEBUG TRAIN Batch 28/100 loss 9.418113 loss_att 8.910216 loss_ctc 14.196956 loss_rnnt 8.829492 hw_loss 0.099413 lr 0.00032713 rank 4
2023-02-25 12:11:31,539 DEBUG TRAIN Batch 28/200 loss 10.746401 loss_att 12.509954 loss_ctc 13.139560 loss_rnnt 9.987509 hw_loss 0.163300 lr 0.00032710 rank 6
2023-02-25 12:11:31,541 DEBUG TRAIN Batch 28/200 loss 9.245828 loss_att 12.725824 loss_ctc 15.739131 loss_rnnt 7.594479 hw_loss 0.167954 lr 0.00032704 rank 7
2023-02-25 12:11:31,543 DEBUG TRAIN Batch 28/200 loss 8.989919 loss_att 14.218414 loss_ctc 15.359432 loss_rnnt 7.040508 hw_loss 0.102081 lr 0.00032708 rank 3
2023-02-25 12:11:31,544 DEBUG TRAIN Batch 28/200 loss 5.623900 loss_att 8.247293 loss_ctc 8.055351 loss_rnnt 4.670088 hw_loss 0.196762 lr 0.00032707 rank 5
2023-02-25 12:11:31,544 DEBUG TRAIN Batch 28/200 loss 28.035948 loss_att 34.061020 loss_ctc 42.200813 loss_rnnt 24.841660 hw_loss 0.188670 lr 0.00032712 rank 1
2023-02-25 12:11:31,546 DEBUG TRAIN Batch 28/200 loss 6.516184 loss_att 9.664769 loss_ctc 12.713039 loss_rnnt 4.922258 hw_loss 0.258678 lr 0.00032706 rank 4
2023-02-25 12:11:31,546 DEBUG TRAIN Batch 28/200 loss 11.602168 loss_att 12.482091 loss_ctc 14.857313 loss_rnnt 10.912887 hw_loss 0.148646 lr 0.00032703 rank 0
2023-02-25 12:11:31,548 DEBUG TRAIN Batch 28/200 loss 4.657820 loss_att 9.045770 loss_ctc 7.546322 loss_rnnt 3.246797 hw_loss 0.278061 lr 0.00032709 rank 2
2023-02-25 12:12:43,004 DEBUG TRAIN Batch 28/300 loss 6.038984 loss_att 10.361298 loss_ctc 8.579308 loss_rnnt 4.720352 hw_loss 0.216486 lr 0.00032700 rank 5
2023-02-25 12:12:43,006 DEBUG TRAIN Batch 28/300 loss 5.380116 loss_att 8.623411 loss_ctc 7.701688 loss_rnnt 4.344285 hw_loss 0.145556 lr 0.00032696 rank 0
2023-02-25 12:12:43,012 DEBUG TRAIN Batch 28/300 loss 3.112043 loss_att 6.747718 loss_ctc 5.390712 loss_rnnt 1.958795 hw_loss 0.229294 lr 0.00032701 rank 3
2023-02-25 12:12:43,012 DEBUG TRAIN Batch 28/300 loss 7.836230 loss_att 12.319567 loss_ctc 14.160357 loss_rnnt 6.009961 hw_loss 0.161970 lr 0.00032703 rank 6
2023-02-25 12:12:43,015 DEBUG TRAIN Batch 28/300 loss 7.027843 loss_att 11.368147 loss_ctc 11.310574 loss_rnnt 5.489132 hw_loss 0.186786 lr 0.00032702 rank 2
2023-02-25 12:12:43,017 DEBUG TRAIN Batch 28/300 loss 3.385684 loss_att 7.189526 loss_ctc 5.653799 loss_rnnt 2.261784 hw_loss 0.113843 lr 0.00032699 rank 4
2023-02-25 12:12:43,035 DEBUG TRAIN Batch 28/300 loss 9.910953 loss_att 10.848076 loss_ctc 12.857283 loss_rnnt 9.225592 hw_loss 0.197048 lr 0.00032705 rank 1
2023-02-25 12:12:43,037 DEBUG TRAIN Batch 28/300 loss 9.595543 loss_att 11.577301 loss_ctc 10.998449 loss_rnnt 8.949963 hw_loss 0.116576 lr 0.00032697 rank 7
2023-02-25 12:13:54,865 DEBUG TRAIN Batch 28/400 loss 5.327780 loss_att 7.851739 loss_ctc 9.342809 loss_rnnt 4.166056 hw_loss 0.227990 lr 0.00032694 rank 3
2023-02-25 12:13:54,866 DEBUG TRAIN Batch 28/400 loss 5.741992 loss_att 8.548681 loss_ctc 9.422945 loss_rnnt 4.638972 hw_loss 0.095416 lr 0.00032689 rank 0
2023-02-25 12:13:54,866 DEBUG TRAIN Batch 28/400 loss 3.895611 loss_att 5.977837 loss_ctc 4.745985 loss_rnnt 3.241755 hw_loss 0.232552 lr 0.00032695 rank 2
2023-02-25 12:13:54,867 DEBUG TRAIN Batch 28/400 loss 3.474681 loss_att 5.874818 loss_ctc 5.145735 loss_rnnt 2.652667 hw_loss 0.223462 lr 0.00032698 rank 1
2023-02-25 12:13:54,867 DEBUG TRAIN Batch 28/400 loss 7.052909 loss_att 10.461905 loss_ctc 12.473995 loss_rnnt 5.523909 hw_loss 0.233229 lr 0.00032696 rank 6
2023-02-25 12:13:54,868 DEBUG TRAIN Batch 28/400 loss 6.781762 loss_att 8.630233 loss_ctc 8.000947 loss_rnnt 6.167725 hw_loss 0.153345 lr 0.00032692 rank 4
2023-02-25 12:13:54,872 DEBUG TRAIN Batch 28/400 loss 5.392127 loss_att 9.494766 loss_ctc 9.762537 loss_rnnt 3.851326 hw_loss 0.257908 lr 0.00032693 rank 5
2023-02-25 12:13:54,873 DEBUG TRAIN Batch 28/400 loss 5.298326 loss_att 7.530460 loss_ctc 6.673481 loss_rnnt 4.584868 hw_loss 0.156893 lr 0.00032690 rank 7
2023-02-25 12:15:04,858 DEBUG TRAIN Batch 28/500 loss 9.061995 loss_att 11.437562 loss_ctc 11.849028 loss_rnnt 8.108308 hw_loss 0.200567 lr 0.00032682 rank 0
2023-02-25 12:15:04,866 DEBUG TRAIN Batch 28/500 loss 8.005924 loss_att 10.839571 loss_ctc 13.991629 loss_rnnt 6.524639 hw_loss 0.218367 lr 0.00032686 rank 5
2023-02-25 12:15:04,866 DEBUG TRAIN Batch 28/500 loss 12.217707 loss_att 14.276845 loss_ctc 14.671549 loss_rnnt 11.369556 hw_loss 0.204645 lr 0.00032685 rank 4
2023-02-25 12:15:04,867 DEBUG TRAIN Batch 28/500 loss 6.494839 loss_att 9.302096 loss_ctc 12.096303 loss_rnnt 5.096020 hw_loss 0.169697 lr 0.00032689 rank 6
2023-02-25 12:15:04,868 DEBUG TRAIN Batch 28/500 loss 9.283097 loss_att 11.950814 loss_ctc 13.745068 loss_rnnt 8.053761 hw_loss 0.189118 lr 0.00032691 rank 1
2023-02-25 12:15:04,868 DEBUG TRAIN Batch 28/500 loss 8.143184 loss_att 10.268163 loss_ctc 11.214820 loss_rnnt 7.217041 hw_loss 0.171743 lr 0.00032688 rank 2
2023-02-25 12:15:04,871 DEBUG TRAIN Batch 28/500 loss 3.336634 loss_att 4.333971 loss_ctc 3.412355 loss_rnnt 3.015152 hw_loss 0.209847 lr 0.00032687 rank 3
2023-02-25 12:15:04,874 DEBUG TRAIN Batch 28/500 loss 8.321783 loss_att 10.792910 loss_ctc 12.450397 loss_rnnt 7.158025 hw_loss 0.223219 lr 0.00032683 rank 7
2023-02-25 12:16:15,634 DEBUG TRAIN Batch 28/600 loss 8.963013 loss_att 9.801825 loss_ctc 11.845725 loss_rnnt 8.272743 hw_loss 0.259023 lr 0.00032675 rank 0
2023-02-25 12:16:15,641 DEBUG TRAIN Batch 28/600 loss 7.880035 loss_att 10.454948 loss_ctc 12.064734 loss_rnnt 6.654990 hw_loss 0.285194 lr 0.00032684 rank 1
2023-02-25 12:16:15,641 DEBUG TRAIN Batch 28/600 loss 9.978585 loss_att 12.474472 loss_ctc 11.988954 loss_rnnt 9.086574 hw_loss 0.233973 lr 0.00032682 rank 6
2023-02-25 12:16:15,644 DEBUG TRAIN Batch 28/600 loss 9.769297 loss_att 10.043980 loss_ctc 16.224628 loss_rnnt 8.757573 hw_loss 0.180139 lr 0.00032680 rank 3
2023-02-25 12:16:15,645 DEBUG TRAIN Batch 28/600 loss 3.044042 loss_att 7.503612 loss_ctc 5.128170 loss_rnnt 1.797799 hw_loss 0.143334 lr 0.00032676 rank 7
2023-02-25 12:16:15,644 DEBUG TRAIN Batch 28/600 loss 3.678287 loss_att 4.953675 loss_ctc 5.993057 loss_rnnt 2.971267 hw_loss 0.268699 lr 0.00032681 rank 2
2023-02-25 12:16:15,647 DEBUG TRAIN Batch 28/600 loss 12.107644 loss_att 13.718019 loss_ctc 16.301443 loss_rnnt 11.054689 hw_loss 0.321949 lr 0.00032678 rank 4
2023-02-25 12:16:15,651 DEBUG TRAIN Batch 28/600 loss 7.726460 loss_att 8.873825 loss_ctc 9.397634 loss_rnnt 7.089570 hw_loss 0.346112 lr 0.00032679 rank 5
2023-02-25 12:17:29,075 DEBUG TRAIN Batch 28/700 loss 5.424888 loss_att 10.026521 loss_ctc 13.267752 loss_rnnt 3.338004 hw_loss 0.226578 lr 0.00032672 rank 5
2023-02-25 12:17:29,083 DEBUG TRAIN Batch 28/700 loss 13.013030 loss_att 18.238461 loss_ctc 19.392269 loss_rnnt 11.016516 hw_loss 0.189119 lr 0.00032668 rank 0
2023-02-25 12:17:29,086 DEBUG TRAIN Batch 28/700 loss 8.434723 loss_att 12.111073 loss_ctc 13.005702 loss_rnnt 7.026066 hw_loss 0.119856 lr 0.00032677 rank 1
2023-02-25 12:17:29,090 DEBUG TRAIN Batch 28/700 loss 8.324563 loss_att 11.504048 loss_ctc 11.850131 loss_rnnt 7.136384 hw_loss 0.154135 lr 0.00032669 rank 7
2023-02-25 12:17:29,092 DEBUG TRAIN Batch 28/700 loss 5.841503 loss_att 7.173957 loss_ctc 7.551284 loss_rnnt 5.300148 hw_loss 0.087925 lr 0.00032671 rank 4
2023-02-25 12:17:29,098 DEBUG TRAIN Batch 28/700 loss 8.214264 loss_att 10.786545 loss_ctc 12.424030 loss_rnnt 6.986308 hw_loss 0.285371 lr 0.00032674 rank 2
2023-02-25 12:17:29,110 DEBUG TRAIN Batch 28/700 loss 8.602110 loss_att 10.656759 loss_ctc 13.358187 loss_rnnt 7.480226 hw_loss 0.144020 lr 0.00032675 rank 6
2023-02-25 12:17:29,130 DEBUG TRAIN Batch 28/700 loss 7.348797 loss_att 9.933552 loss_ctc 12.880722 loss_rnnt 5.973934 hw_loss 0.225605 lr 0.00032673 rank 3
2023-02-25 12:18:40,045 DEBUG TRAIN Batch 28/800 loss 1.530086 loss_att 3.410847 loss_ctc 1.021033 loss_rnnt 1.104321 hw_loss 0.220286 lr 0.00032661 rank 0
2023-02-25 12:18:40,054 DEBUG TRAIN Batch 28/800 loss 2.886118 loss_att 4.891504 loss_ctc 4.008085 loss_rnnt 2.218801 hw_loss 0.218708 lr 0.00032664 rank 4
2023-02-25 12:18:40,055 DEBUG TRAIN Batch 28/800 loss 8.565005 loss_att 9.377769 loss_ctc 9.379333 loss_rnnt 8.217201 hw_loss 0.143765 lr 0.00032666 rank 3
2023-02-25 12:18:40,056 DEBUG TRAIN Batch 28/800 loss 5.718520 loss_att 9.204859 loss_ctc 9.543685 loss_rnnt 4.374811 hw_loss 0.255786 lr 0.00032662 rank 7
2023-02-25 12:18:40,058 DEBUG TRAIN Batch 28/800 loss 1.055019 loss_att 2.973455 loss_ctc 1.167245 loss_rnnt 0.519129 hw_loss 0.257325 lr 0.00032667 rank 2
2023-02-25 12:18:40,060 DEBUG TRAIN Batch 28/800 loss 4.431167 loss_att 7.646007 loss_ctc 6.588918 loss_rnnt 3.448803 hw_loss 0.096930 lr 0.00032670 rank 1
2023-02-25 12:18:40,063 DEBUG TRAIN Batch 28/800 loss 3.099313 loss_att 5.947825 loss_ctc 4.436720 loss_rnnt 2.264642 hw_loss 0.162463 lr 0.00032665 rank 5
2023-02-25 12:18:40,114 DEBUG TRAIN Batch 28/800 loss 9.279469 loss_att 11.630816 loss_ctc 15.999235 loss_rnnt 7.826427 hw_loss 0.162759 lr 0.00032668 rank 6
2023-02-25 12:19:50,163 DEBUG TRAIN Batch 28/900 loss 3.578779 loss_att 6.466694 loss_ctc 5.967243 loss_rnnt 2.548036 hw_loss 0.252560 lr 0.00032659 rank 3
2023-02-25 12:19:50,163 DEBUG TRAIN Batch 28/900 loss 9.775438 loss_att 12.978003 loss_ctc 17.478771 loss_rnnt 8.015372 hw_loss 0.173329 lr 0.00032661 rank 6
2023-02-25 12:19:50,168 DEBUG TRAIN Batch 28/900 loss 10.842773 loss_att 12.436832 loss_ctc 14.940415 loss_rnnt 9.888656 hw_loss 0.166788 lr 0.00032655 rank 7
2023-02-25 12:19:50,167 DEBUG TRAIN Batch 28/900 loss 8.757612 loss_att 10.649386 loss_ctc 18.301556 loss_rnnt 7.027813 hw_loss 0.147972 lr 0.00032654 rank 0
2023-02-25 12:19:50,170 DEBUG TRAIN Batch 28/900 loss 6.764694 loss_att 9.666978 loss_ctc 13.369921 loss_rnnt 5.186361 hw_loss 0.219709 lr 0.00032657 rank 4
2023-02-25 12:19:50,173 DEBUG TRAIN Batch 28/900 loss 6.646002 loss_att 8.786754 loss_ctc 10.286974 loss_rnnt 5.653173 hw_loss 0.148527 lr 0.00032658 rank 5
2023-02-25 12:19:50,174 DEBUG TRAIN Batch 28/900 loss 8.663797 loss_att 13.185536 loss_ctc 20.741528 loss_rnnt 6.052396 hw_loss 0.181290 lr 0.00032660 rank 2
2023-02-25 12:19:50,176 DEBUG TRAIN Batch 28/900 loss 9.312412 loss_att 13.132217 loss_ctc 17.082800 loss_rnnt 7.410354 hw_loss 0.191336 lr 0.00032663 rank 1
2023-02-25 12:21:00,622 DEBUG TRAIN Batch 28/1000 loss 6.279592 loss_att 9.567355 loss_ctc 9.671679 loss_rnnt 5.050581 hw_loss 0.223460 lr 0.00032652 rank 3
2023-02-25 12:21:00,623 DEBUG TRAIN Batch 28/1000 loss 6.488797 loss_att 9.878993 loss_ctc 11.049763 loss_rnnt 5.065367 hw_loss 0.257366 lr 0.00032648 rank 7
2023-02-25 12:21:00,622 DEBUG TRAIN Batch 28/1000 loss 9.989872 loss_att 13.196361 loss_ctc 13.583402 loss_rnnt 8.728577 hw_loss 0.264113 lr 0.00032647 rank 0
2023-02-25 12:21:00,625 DEBUG TRAIN Batch 28/1000 loss 10.190613 loss_att 11.835526 loss_ctc 13.978932 loss_rnnt 9.255914 hw_loss 0.188639 lr 0.00032650 rank 4
2023-02-25 12:21:00,626 DEBUG TRAIN Batch 28/1000 loss 12.316793 loss_att 16.494556 loss_ctc 17.402565 loss_rnnt 10.754867 hw_loss 0.090508 lr 0.00032651 rank 5
2023-02-25 12:21:00,626 DEBUG TRAIN Batch 28/1000 loss 10.161706 loss_att 13.351030 loss_ctc 14.296506 loss_rnnt 8.878056 hw_loss 0.177148 lr 0.00032654 rank 6
2023-02-25 12:21:00,626 DEBUG TRAIN Batch 28/1000 loss 8.043224 loss_att 9.174308 loss_ctc 10.930887 loss_rnnt 7.291685 hw_loss 0.263063 lr 0.00032656 rank 1
2023-02-25 12:21:00,629 DEBUG TRAIN Batch 28/1000 loss 7.487206 loss_att 11.471556 loss_ctc 9.020155 loss_rnnt 6.400366 hw_loss 0.160457 lr 0.00032653 rank 2
2023-02-25 12:22:14,779 DEBUG TRAIN Batch 28/1100 loss 12.750367 loss_att 15.285913 loss_ctc 19.048145 loss_rnnt 11.292712 hw_loss 0.207827 lr 0.00032644 rank 5
2023-02-25 12:22:14,780 DEBUG TRAIN Batch 28/1100 loss 11.587560 loss_att 12.401234 loss_ctc 17.132595 loss_rnnt 10.591761 hw_loss 0.175737 lr 0.00032640 rank 0
2023-02-25 12:22:14,781 DEBUG TRAIN Batch 28/1100 loss 12.052629 loss_att 11.794729 loss_ctc 15.084535 loss_rnnt 11.610085 hw_loss 0.168505 lr 0.00032649 rank 1
2023-02-25 12:22:14,781 DEBUG TRAIN Batch 28/1100 loss 7.164263 loss_att 9.354555 loss_ctc 10.307690 loss_rnnt 6.184464 hw_loss 0.229905 lr 0.00032647 rank 6
2023-02-25 12:22:14,784 DEBUG TRAIN Batch 28/1100 loss 2.802521 loss_att 5.244680 loss_ctc 5.558280 loss_rnnt 1.861099 hw_loss 0.160414 lr 0.00032644 rank 4
2023-02-25 12:22:14,786 DEBUG TRAIN Batch 28/1100 loss 8.199835 loss_att 10.918988 loss_ctc 12.696442 loss_rnnt 6.955377 hw_loss 0.189525 lr 0.00032646 rank 2
2023-02-25 12:22:14,789 DEBUG TRAIN Batch 28/1100 loss 10.181274 loss_att 11.679379 loss_ctc 13.528077 loss_rnnt 9.311870 hw_loss 0.231642 lr 0.00032645 rank 3
2023-02-25 12:22:14,790 DEBUG TRAIN Batch 28/1100 loss 9.597569 loss_att 13.492247 loss_ctc 17.832846 loss_rnnt 7.607549 hw_loss 0.211965 lr 0.00032641 rank 7
2023-02-25 12:23:25,558 DEBUG TRAIN Batch 28/1200 loss 11.682467 loss_att 13.084493 loss_ctc 14.639716 loss_rnnt 10.928447 hw_loss 0.148714 lr 0.00032633 rank 0
2023-02-25 12:23:25,563 DEBUG TRAIN Batch 28/1200 loss 8.914721 loss_att 9.560839 loss_ctc 11.886077 loss_rnnt 8.205514 hw_loss 0.344628 lr 0.00032634 rank 7
2023-02-25 12:23:25,562 DEBUG TRAIN Batch 28/1200 loss 3.237390 loss_att 5.838591 loss_ctc 8.907299 loss_rnnt 1.859229 hw_loss 0.191122 lr 0.00032642 rank 1
2023-02-25 12:23:25,563 DEBUG TRAIN Batch 28/1200 loss 9.634516 loss_att 10.257309 loss_ctc 13.705970 loss_rnnt 8.855586 hw_loss 0.209085 lr 0.00032637 rank 4
2023-02-25 12:23:25,564 DEBUG TRAIN Batch 28/1200 loss 6.816941 loss_att 10.045232 loss_ctc 12.636249 loss_rnnt 5.251063 hw_loss 0.270584 lr 0.00032638 rank 3
2023-02-25 12:23:25,564 DEBUG TRAIN Batch 28/1200 loss 10.788692 loss_att 12.527786 loss_ctc 16.670454 loss_rnnt 9.516224 hw_loss 0.263276 lr 0.00032640 rank 6
2023-02-25 12:23:25,565 DEBUG TRAIN Batch 28/1200 loss 5.560395 loss_att 6.290236 loss_ctc 9.409831 loss_rnnt 4.809031 hw_loss 0.172757 lr 0.00032639 rank 2
2023-02-25 12:23:25,572 DEBUG TRAIN Batch 28/1200 loss 11.751471 loss_att 12.423619 loss_ctc 13.331744 loss_rnnt 11.288000 hw_loss 0.221884 lr 0.00032637 rank 5
2023-02-25 12:24:35,525 DEBUG TRAIN Batch 28/1300 loss 12.069077 loss_att 16.264091 loss_ctc 15.764229 loss_rnnt 10.621029 hw_loss 0.218173 lr 0.00032626 rank 0
2023-02-25 12:24:35,530 DEBUG TRAIN Batch 28/1300 loss 6.813791 loss_att 11.840480 loss_ctc 12.939222 loss_rnnt 4.877637 hw_loss 0.213922 lr 0.00032633 rank 6
2023-02-25 12:24:35,533 DEBUG TRAIN Batch 28/1300 loss 10.156121 loss_att 10.251861 loss_ctc 12.481553 loss_rnnt 9.723061 hw_loss 0.194728 lr 0.00032631 rank 3
2023-02-25 12:24:35,534 DEBUG TRAIN Batch 28/1300 loss 9.922595 loss_att 13.699899 loss_ctc 12.969882 loss_rnnt 8.697273 hw_loss 0.119167 lr 0.00032630 rank 4
2023-02-25 12:24:35,537 DEBUG TRAIN Batch 28/1300 loss 8.374341 loss_att 12.601531 loss_ctc 12.472408 loss_rnnt 6.882389 hw_loss 0.187695 lr 0.00032627 rank 7
2023-02-25 12:24:35,538 DEBUG TRAIN Batch 28/1300 loss 10.215345 loss_att 13.395456 loss_ctc 15.785555 loss_rnnt 8.727886 hw_loss 0.203894 lr 0.00032633 rank 2
2023-02-25 12:24:35,540 DEBUG TRAIN Batch 28/1300 loss 2.653323 loss_att 6.664283 loss_ctc 3.966532 loss_rnnt 1.563964 hw_loss 0.210136 lr 0.00032636 rank 1
2023-02-25 12:24:35,542 DEBUG TRAIN Batch 28/1300 loss 27.838358 loss_att 35.286827 loss_ctc 43.816650 loss_rnnt 24.139881 hw_loss 0.146894 lr 0.00032630 rank 5
2023-02-25 12:25:47,234 DEBUG TRAIN Batch 28/1400 loss 4.703743 loss_att 8.515133 loss_ctc 6.256104 loss_rnnt 3.649270 hw_loss 0.159775 lr 0.00032624 rank 3
2023-02-25 12:25:47,235 DEBUG TRAIN Batch 28/1400 loss 8.181681 loss_att 12.812429 loss_ctc 12.934652 loss_rnnt 6.521749 hw_loss 0.187597 lr 0.00032626 rank 6
2023-02-25 12:25:47,236 DEBUG TRAIN Batch 28/1400 loss 5.215244 loss_att 7.211917 loss_ctc 5.663085 loss_rnnt 4.706795 hw_loss 0.092629 lr 0.00032629 rank 1
2023-02-25 12:25:47,240 DEBUG TRAIN Batch 28/1400 loss 7.466809 loss_att 9.190950 loss_ctc 10.750865 loss_rnnt 6.590122 hw_loss 0.176223 lr 0.00032623 rank 5
2023-02-25 12:25:47,239 DEBUG TRAIN Batch 28/1400 loss 8.012285 loss_att 12.050249 loss_ctc 13.227684 loss_rnnt 6.410848 hw_loss 0.184609 lr 0.00032620 rank 7
2023-02-25 12:25:47,264 DEBUG TRAIN Batch 28/1400 loss 3.244873 loss_att 7.981581 loss_ctc 7.232261 loss_rnnt 1.618848 hw_loss 0.275683 lr 0.00032626 rank 2
2023-02-25 12:25:47,269 DEBUG TRAIN Batch 28/1400 loss 3.244251 loss_att 6.280314 loss_ctc 6.274620 loss_rnnt 2.130065 hw_loss 0.192982 lr 0.00032619 rank 0
2023-02-25 12:25:47,281 DEBUG TRAIN Batch 28/1400 loss 7.241917 loss_att 10.119574 loss_ctc 6.812305 loss_rnnt 6.643398 hw_loss 0.150504 lr 0.00032623 rank 4
2023-02-25 12:27:00,500 DEBUG TRAIN Batch 28/1500 loss 8.398626 loss_att 10.111341 loss_ctc 10.610727 loss_rnnt 7.676324 hw_loss 0.159023 lr 0.00032617 rank 3
2023-02-25 12:27:00,499 DEBUG TRAIN Batch 28/1500 loss 8.647196 loss_att 11.939696 loss_ctc 15.626627 loss_rnnt 6.983314 hw_loss 0.140235 lr 0.00032612 rank 0
2023-02-25 12:27:00,501 DEBUG TRAIN Batch 28/1500 loss 4.512612 loss_att 9.821513 loss_ctc 7.116490 loss_rnnt 3.050764 hw_loss 0.099158 lr 0.00032613 rank 7
2023-02-25 12:27:00,502 DEBUG TRAIN Batch 28/1500 loss 12.822841 loss_att 14.315748 loss_ctc 13.984317 loss_rnnt 12.265974 hw_loss 0.193918 lr 0.00032622 rank 1
2023-02-25 12:27:00,505 DEBUG TRAIN Batch 28/1500 loss 6.531790 loss_att 9.417646 loss_ctc 12.279751 loss_rnnt 5.124133 hw_loss 0.120170 lr 0.00032619 rank 6
2023-02-25 12:27:00,510 DEBUG TRAIN Batch 28/1500 loss 8.267059 loss_att 10.518378 loss_ctc 11.956675 loss_rnnt 7.276206 hw_loss 0.091200 lr 0.00032619 rank 2
2023-02-25 12:27:00,512 DEBUG TRAIN Batch 28/1500 loss 1.444220 loss_att 4.552486 loss_ctc 3.160398 loss_rnnt 0.524131 hw_loss 0.130524 lr 0.00032616 rank 4
2023-02-25 12:27:00,565 DEBUG TRAIN Batch 28/1500 loss 7.485816 loss_att 9.982073 loss_ctc 10.759513 loss_rnnt 6.444501 hw_loss 0.197945 lr 0.00032616 rank 5
2023-02-25 12:28:11,290 DEBUG TRAIN Batch 28/1600 loss 5.245033 loss_att 7.003901 loss_ctc 5.997294 loss_rnnt 4.697080 hw_loss 0.179770 lr 0.00032605 rank 0
2023-02-25 12:28:11,292 DEBUG TRAIN Batch 28/1600 loss 3.487120 loss_att 6.770564 loss_ctc 5.899339 loss_rnnt 2.403959 hw_loss 0.196581 lr 0.00032606 rank 7
2023-02-25 12:28:11,292 DEBUG TRAIN Batch 28/1600 loss 4.836284 loss_att 9.296038 loss_ctc 6.154709 loss_rnnt 3.660067 hw_loss 0.203392 lr 0.00032615 rank 1
2023-02-25 12:28:11,294 DEBUG TRAIN Batch 28/1600 loss 10.753761 loss_att 12.904207 loss_ctc 14.603588 loss_rnnt 9.697088 hw_loss 0.212389 lr 0.00032611 rank 3
2023-02-25 12:28:11,295 DEBUG TRAIN Batch 28/1600 loss 9.067472 loss_att 9.984455 loss_ctc 16.305021 loss_rnnt 7.807454 hw_loss 0.209277 lr 0.00032613 rank 6
2023-02-25 12:28:11,298 DEBUG TRAIN Batch 28/1600 loss 2.462884 loss_att 4.607539 loss_ctc 3.008675 loss_rnnt 1.880919 hw_loss 0.150491 lr 0.00032612 rank 2
2023-02-25 12:28:11,298 DEBUG TRAIN Batch 28/1600 loss 11.167756 loss_att 15.982010 loss_ctc 19.305283 loss_rnnt 8.965040 hw_loss 0.290365 lr 0.00032609 rank 4
2023-02-25 12:28:11,303 DEBUG TRAIN Batch 28/1600 loss 8.166187 loss_att 12.154364 loss_ctc 13.879567 loss_rnnt 6.503047 hw_loss 0.194477 lr 0.00032610 rank 5
2023-02-25 12:29:21,649 DEBUG TRAIN Batch 28/1700 loss 6.787181 loss_att 8.613213 loss_ctc 9.530897 loss_rnnt 5.906540 hw_loss 0.280510 lr 0.00032599 rank 7
2023-02-25 12:29:21,656 DEBUG TRAIN Batch 28/1700 loss 9.966333 loss_att 14.442671 loss_ctc 13.562531 loss_rnnt 8.479351 hw_loss 0.210416 lr 0.00032598 rank 0
2023-02-25 12:29:21,663 DEBUG TRAIN Batch 28/1700 loss 9.177298 loss_att 11.575134 loss_ctc 13.361026 loss_rnnt 8.101308 hw_loss 0.072360 lr 0.00032608 rank 1
2023-02-25 12:29:21,663 DEBUG TRAIN Batch 28/1700 loss 5.492354 loss_att 7.910838 loss_ctc 6.928064 loss_rnnt 4.682394 hw_loss 0.252817 lr 0.00032605 rank 2
2023-02-25 12:29:21,665 DEBUG TRAIN Batch 28/1700 loss 6.095986 loss_att 10.718638 loss_ctc 9.399377 loss_rnnt 4.600997 hw_loss 0.243763 lr 0.00032606 rank 6
2023-02-25 12:29:21,666 DEBUG TRAIN Batch 28/1700 loss 9.247294 loss_att 11.600472 loss_ctc 13.564588 loss_rnnt 8.085677 hw_loss 0.216267 lr 0.00032604 rank 3
2023-02-25 12:29:21,669 DEBUG TRAIN Batch 28/1700 loss 4.312330 loss_att 5.799339 loss_ctc 5.711889 loss_rnnt 3.754030 hw_loss 0.139293 lr 0.00032603 rank 5
2023-02-25 12:29:21,712 DEBUG TRAIN Batch 28/1700 loss 9.390561 loss_att 11.285062 loss_ctc 15.851696 loss_rnnt 8.020793 hw_loss 0.242594 lr 0.00032602 rank 4
2023-02-25 12:30:35,704 DEBUG TRAIN Batch 28/1800 loss 7.915445 loss_att 11.646491 loss_ctc 11.633733 loss_rnnt 6.508852 hw_loss 0.308650 lr 0.00032599 rank 6
2023-02-25 12:30:35,707 DEBUG TRAIN Batch 28/1800 loss 7.420506 loss_att 7.491664 loss_ctc 8.929529 loss_rnnt 7.121048 hw_loss 0.157544 lr 0.00032591 rank 0
2023-02-25 12:30:35,712 DEBUG TRAIN Batch 28/1800 loss 2.501195 loss_att 2.932323 loss_ctc 3.573521 loss_rnnt 2.133909 hw_loss 0.258907 lr 0.00032592 rank 7
2023-02-25 12:30:35,711 DEBUG TRAIN Batch 28/1800 loss 6.705615 loss_att 9.152634 loss_ctc 9.856739 loss_rnnt 5.701238 hw_loss 0.177794 lr 0.00032601 rank 1
2023-02-25 12:30:35,712 DEBUG TRAIN Batch 28/1800 loss 10.399879 loss_att 14.557270 loss_ctc 15.498609 loss_rnnt 8.771053 hw_loss 0.220341 lr 0.00032596 rank 5
2023-02-25 12:30:35,712 DEBUG TRAIN Batch 28/1800 loss 11.725643 loss_att 13.545212 loss_ctc 15.315005 loss_rnnt 10.755574 hw_loss 0.239197 lr 0.00032595 rank 4
2023-02-25 12:30:35,714 DEBUG TRAIN Batch 28/1800 loss 10.002923 loss_att 11.526021 loss_ctc 14.259954 loss_rnnt 9.048443 hw_loss 0.154231 lr 0.00032598 rank 2
2023-02-25 12:30:35,716 DEBUG TRAIN Batch 28/1800 loss 6.149125 loss_att 8.597660 loss_ctc 9.224709 loss_rnnt 5.176908 hw_loss 0.135808 lr 0.00032597 rank 3
2023-02-25 12:31:46,250 DEBUG TRAIN Batch 28/1900 loss 10.951638 loss_att 13.268051 loss_ctc 15.719513 loss_rnnt 9.732969 hw_loss 0.224380 lr 0.00032590 rank 3
2023-02-25 12:31:46,256 DEBUG TRAIN Batch 28/1900 loss 1.100798 loss_att 3.882399 loss_ctc 1.685714 loss_rnnt 0.362811 hw_loss 0.194396 lr 0.00032585 rank 7
2023-02-25 12:31:46,256 DEBUG TRAIN Batch 28/1900 loss 10.196830 loss_att 11.320940 loss_ctc 17.918394 loss_rnnt 8.850702 hw_loss 0.172058 lr 0.00032588 rank 4
2023-02-25 12:31:46,258 DEBUG TRAIN Batch 28/1900 loss 4.725547 loss_att 6.146654 loss_ctc 4.386268 loss_rnnt 4.445733 hw_loss 0.076557 lr 0.00032585 rank 0
2023-02-25 12:31:46,257 DEBUG TRAIN Batch 28/1900 loss 10.547247 loss_att 12.824154 loss_ctc 17.304783 loss_rnnt 9.040004 hw_loss 0.282857 lr 0.00032592 rank 6
2023-02-25 12:31:46,259 DEBUG TRAIN Batch 28/1900 loss 13.117113 loss_att 14.916130 loss_ctc 20.407713 loss_rnnt 11.663632 hw_loss 0.227994 lr 0.00032594 rank 1
2023-02-25 12:31:46,261 DEBUG TRAIN Batch 28/1900 loss 5.786434 loss_att 7.956264 loss_ctc 7.874354 loss_rnnt 4.929557 hw_loss 0.270977 lr 0.00032591 rank 2
2023-02-25 12:31:46,262 DEBUG TRAIN Batch 28/1900 loss 2.510177 loss_att 4.042246 loss_ctc 4.295649 loss_rnnt 1.864814 hw_loss 0.189162 lr 0.00032589 rank 5
2023-02-25 12:32:56,617 DEBUG TRAIN Batch 28/2000 loss 8.035671 loss_att 10.412133 loss_ctc 10.633891 loss_rnnt 7.108305 hw_loss 0.198084 lr 0.00032585 rank 6
2023-02-25 12:32:56,618 DEBUG TRAIN Batch 28/2000 loss 1.956045 loss_att 4.411827 loss_ctc 5.350866 loss_rnnt 0.920469 hw_loss 0.172080 lr 0.00032578 rank 0
2023-02-25 12:32:56,622 DEBUG TRAIN Batch 28/2000 loss 6.834614 loss_att 10.419363 loss_ctc 11.828701 loss_rnnt 5.385901 hw_loss 0.123534 lr 0.00032584 rank 2
2023-02-25 12:32:56,622 DEBUG TRAIN Batch 28/2000 loss 14.009230 loss_att 14.724773 loss_ctc 17.073818 loss_rnnt 13.377472 hw_loss 0.150069 lr 0.00032587 rank 1
2023-02-25 12:32:56,622 DEBUG TRAIN Batch 28/2000 loss 5.039980 loss_att 7.454981 loss_ctc 5.854105 loss_rnnt 4.358856 hw_loss 0.167952 lr 0.00032579 rank 7
2023-02-25 12:32:56,624 DEBUG TRAIN Batch 28/2000 loss 4.667406 loss_att 7.215984 loss_ctc 5.081340 loss_rnnt 4.014034 hw_loss 0.165872 lr 0.00032581 rank 4
2023-02-25 12:32:56,625 DEBUG TRAIN Batch 28/2000 loss 2.752115 loss_att 6.193807 loss_ctc 3.998368 loss_rnnt 1.838374 hw_loss 0.111069 lr 0.00032582 rank 5
2023-02-25 12:32:56,672 DEBUG TRAIN Batch 28/2000 loss 7.838869 loss_att 10.482999 loss_ctc 13.771597 loss_rnnt 6.437801 hw_loss 0.152271 lr 0.00032583 rank 3
2023-02-25 12:34:09,673 DEBUG TRAIN Batch 28/2100 loss 4.790102 loss_att 6.015091 loss_ctc 5.891452 loss_rnnt 4.331841 hw_loss 0.124531 lr 0.00032578 rank 6
2023-02-25 12:34:09,676 DEBUG TRAIN Batch 28/2100 loss 1.309136 loss_att 4.720864 loss_ctc 1.431546 loss_rnnt 0.502176 hw_loss 0.203048 lr 0.00032580 rank 1
2023-02-25 12:34:09,677 DEBUG TRAIN Batch 28/2100 loss 6.929440 loss_att 8.712220 loss_ctc 9.728888 loss_rnnt 6.084655 hw_loss 0.215568 lr 0.00032572 rank 7
2023-02-25 12:34:09,679 DEBUG TRAIN Batch 28/2100 loss 5.724230 loss_att 8.360119 loss_ctc 7.565646 loss_rnnt 4.822262 hw_loss 0.242377 lr 0.00032571 rank 0
2023-02-25 12:34:09,680 DEBUG TRAIN Batch 28/2100 loss 2.597502 loss_att 4.677565 loss_ctc 5.171208 loss_rnnt 1.773227 hw_loss 0.122064 lr 0.00032574 rank 4
2023-02-25 12:34:09,682 DEBUG TRAIN Batch 28/2100 loss 7.349413 loss_att 10.543511 loss_ctc 9.487124 loss_rnnt 6.306301 hw_loss 0.223621 lr 0.00032575 rank 5
2023-02-25 12:34:09,683 DEBUG TRAIN Batch 28/2100 loss 5.166231 loss_att 8.660950 loss_ctc 6.277478 loss_rnnt 4.263175 hw_loss 0.104898 lr 0.00032577 rank 2
2023-02-25 12:34:09,702 DEBUG TRAIN Batch 28/2100 loss 4.722096 loss_att 7.422131 loss_ctc 7.328451 loss_rnnt 3.764339 hw_loss 0.131693 lr 0.00032576 rank 3
2023-02-25 12:35:21,330 DEBUG TRAIN Batch 28/2200 loss 5.542978 loss_att 7.814021 loss_ctc 5.129590 loss_rnnt 5.005169 hw_loss 0.260096 lr 0.00032564 rank 0
2023-02-25 12:35:21,338 DEBUG TRAIN Batch 28/2200 loss 4.109591 loss_att 6.188752 loss_ctc 7.058595 loss_rnnt 3.225563 hw_loss 0.140615 lr 0.00032570 rank 2
2023-02-25 12:35:21,340 DEBUG TRAIN Batch 28/2200 loss 8.604792 loss_att 11.911439 loss_ctc 10.540138 loss_rnnt 7.597683 hw_loss 0.164499 lr 0.00032565 rank 7
2023-02-25 12:35:21,342 DEBUG TRAIN Batch 28/2200 loss 5.651368 loss_att 7.680419 loss_ctc 9.433159 loss_rnnt 4.601555 hw_loss 0.262058 lr 0.00032569 rank 3
2023-02-25 12:35:21,342 DEBUG TRAIN Batch 28/2200 loss 3.836001 loss_att 6.760386 loss_ctc 6.214067 loss_rnnt 2.789512 hw_loss 0.271005 lr 0.00032571 rank 6
2023-02-25 12:35:21,344 DEBUG TRAIN Batch 28/2200 loss 11.551195 loss_att 14.008181 loss_ctc 14.340113 loss_rnnt 10.593967 hw_loss 0.176201 lr 0.00032573 rank 1
2023-02-25 12:35:21,348 DEBUG TRAIN Batch 28/2200 loss 7.677314 loss_att 10.695908 loss_ctc 10.368740 loss_rnnt 6.639965 hw_loss 0.140201 lr 0.00032568 rank 5
2023-02-25 12:35:21,771 DEBUG TRAIN Batch 28/2200 loss 7.544036 loss_att 8.958389 loss_ctc 12.205136 loss_rnnt 6.551424 hw_loss 0.165489 lr 0.00032567 rank 4
2023-02-25 12:36:31,665 DEBUG TRAIN Batch 28/2300 loss 1.762977 loss_att 3.679265 loss_ctc 2.334550 loss_rnnt 1.173845 hw_loss 0.243122 lr 0.00032557 rank 0
2023-02-25 12:36:31,674 DEBUG TRAIN Batch 28/2300 loss 5.701230 loss_att 8.916140 loss_ctc 6.007504 loss_rnnt 4.903422 hw_loss 0.213729 lr 0.00032563 rank 2
2023-02-25 12:36:31,674 DEBUG TRAIN Batch 28/2300 loss 14.891011 loss_att 18.878872 loss_ctc 22.982910 loss_rnnt 12.923618 hw_loss 0.170439 lr 0.00032558 rank 7
2023-02-25 12:36:31,675 DEBUG TRAIN Batch 28/2300 loss 7.980329 loss_att 13.112939 loss_ctc 12.105043 loss_rnnt 6.257153 hw_loss 0.275047 lr 0.00032564 rank 6
2023-02-25 12:36:31,677 DEBUG TRAIN Batch 28/2300 loss 2.897800 loss_att 4.984320 loss_ctc 5.080137 loss_rnnt 2.098656 hw_loss 0.170365 lr 0.00032561 rank 5
2023-02-25 12:36:31,677 DEBUG TRAIN Batch 28/2300 loss 7.912759 loss_att 9.122446 loss_ctc 11.360746 loss_rnnt 7.128207 hw_loss 0.155406 lr 0.00032566 rank 1
2023-02-25 12:36:31,678 DEBUG TRAIN Batch 28/2300 loss 5.837164 loss_att 7.552539 loss_ctc 6.276366 loss_rnnt 5.326488 hw_loss 0.204452 lr 0.00032562 rank 3
2023-02-25 12:36:31,720 DEBUG TRAIN Batch 28/2300 loss 9.832817 loss_att 9.909243 loss_ctc 11.363230 loss_rnnt 9.517977 hw_loss 0.179063 lr 0.00032560 rank 4
2023-02-25 12:37:42,764 DEBUG TRAIN Batch 28/2400 loss 7.785863 loss_att 10.174953 loss_ctc 9.344843 loss_rnnt 7.045207 hw_loss 0.103077 lr 0.00032556 rank 2
2023-02-25 12:37:42,774 DEBUG TRAIN Batch 28/2400 loss 3.752141 loss_att 7.299943 loss_ctc 6.854817 loss_rnnt 2.508661 hw_loss 0.225430 lr 0.00032557 rank 6
2023-02-25 12:37:42,774 DEBUG TRAIN Batch 28/2400 loss 15.839562 loss_att 15.757286 loss_ctc 21.683760 loss_rnnt 14.948167 hw_loss 0.241169 lr 0.00032555 rank 3
2023-02-25 12:37:42,774 DEBUG TRAIN Batch 28/2400 loss 6.619793 loss_att 9.192979 loss_ctc 8.396444 loss_rnnt 5.761423 hw_loss 0.200336 lr 0.00032551 rank 7
2023-02-25 12:37:42,776 DEBUG TRAIN Batch 28/2400 loss 7.811525 loss_att 10.177654 loss_ctc 14.266330 loss_rnnt 6.363925 hw_loss 0.213250 lr 0.00032550 rank 0
2023-02-25 12:37:42,775 DEBUG TRAIN Batch 28/2400 loss 4.974302 loss_att 7.087274 loss_ctc 9.372257 loss_rnnt 3.875260 hw_loss 0.168850 lr 0.00032559 rank 1
2023-02-25 12:37:42,783 DEBUG TRAIN Batch 28/2400 loss 5.766912 loss_att 9.445952 loss_ctc 9.882257 loss_rnnt 4.417058 hw_loss 0.122502 lr 0.00032554 rank 5
2023-02-25 12:37:42,828 DEBUG TRAIN Batch 28/2400 loss 11.024864 loss_att 14.706179 loss_ctc 18.409691 loss_rnnt 9.111360 hw_loss 0.361121 lr 0.00032553 rank 4
2023-02-25 12:38:56,822 DEBUG TRAIN Batch 28/2500 loss 7.742399 loss_att 9.371098 loss_ctc 11.829605 loss_rnnt 6.721387 hw_loss 0.281835 lr 0.00032547 rank 4
2023-02-25 12:38:56,823 DEBUG TRAIN Batch 28/2500 loss 2.988304 loss_att 4.611778 loss_ctc 4.591940 loss_rnnt 2.318746 hw_loss 0.245711 lr 0.00032552 rank 1
2023-02-25 12:38:56,824 DEBUG TRAIN Batch 28/2500 loss 7.048741 loss_att 7.012707 loss_ctc 10.045578 loss_rnnt 6.499327 hw_loss 0.294456 lr 0.00032543 rank 0
2023-02-25 12:38:56,826 DEBUG TRAIN Batch 28/2500 loss 6.030296 loss_att 8.443744 loss_ctc 10.782084 loss_rnnt 4.811412 hw_loss 0.192417 lr 0.00032550 rank 6
2023-02-25 12:38:56,828 DEBUG TRAIN Batch 28/2500 loss 5.290176 loss_att 7.047606 loss_ctc 8.452833 loss_rnnt 4.397754 hw_loss 0.223591 lr 0.00032548 rank 3
2023-02-25 12:38:56,829 DEBUG TRAIN Batch 28/2500 loss 13.727525 loss_att 14.742905 loss_ctc 17.286121 loss_rnnt 12.899138 hw_loss 0.282810 lr 0.00032549 rank 2
2023-02-25 12:38:56,831 DEBUG TRAIN Batch 28/2500 loss 10.586138 loss_att 12.674013 loss_ctc 14.186438 loss_rnnt 9.596137 hw_loss 0.173222 lr 0.00032547 rank 5
2023-02-25 12:38:56,855 DEBUG TRAIN Batch 28/2500 loss 3.967619 loss_att 5.827591 loss_ctc 6.705867 loss_rnnt 3.130857 hw_loss 0.186878 lr 0.00032544 rank 7
2023-02-25 12:40:06,908 DEBUG TRAIN Batch 28/2600 loss 7.329047 loss_att 11.965809 loss_ctc 11.265067 loss_rnnt 5.745397 hw_loss 0.246554 lr 0.00032541 rank 3
2023-02-25 12:40:06,908 DEBUG TRAIN Batch 28/2600 loss 10.661376 loss_att 14.160213 loss_ctc 12.746325 loss_rnnt 9.573563 hw_loss 0.206349 lr 0.00032536 rank 0
2023-02-25 12:40:06,910 DEBUG TRAIN Batch 28/2600 loss 4.647954 loss_att 8.523760 loss_ctc 6.386996 loss_rnnt 3.596084 hw_loss 0.084067 lr 0.00032543 rank 6
2023-02-25 12:40:06,913 DEBUG TRAIN Batch 28/2600 loss 11.301266 loss_att 16.013100 loss_ctc 14.814575 loss_rnnt 9.798569 hw_loss 0.172291 lr 0.00032546 rank 1
2023-02-25 12:40:06,913 DEBUG TRAIN Batch 28/2600 loss 4.952529 loss_att 8.076126 loss_ctc 8.452244 loss_rnnt 3.790468 hw_loss 0.132586 lr 0.00032540 rank 4
2023-02-25 12:40:06,913 DEBUG TRAIN Batch 28/2600 loss 6.832115 loss_att 8.801725 loss_ctc 10.130367 loss_rnnt 5.870257 hw_loss 0.240315 lr 0.00032543 rank 2
2023-02-25 12:40:06,913 DEBUG TRAIN Batch 28/2600 loss 4.243431 loss_att 6.830949 loss_ctc 6.787227 loss_rnnt 3.265713 hw_loss 0.226952 lr 0.00032537 rank 7
2023-02-25 12:40:06,925 DEBUG TRAIN Batch 28/2600 loss 9.661329 loss_att 14.505229 loss_ctc 17.707603 loss_rnnt 7.518242 hw_loss 0.190257 lr 0.00032540 rank 5
2023-02-25 12:41:16,360 DEBUG TRAIN Batch 28/2700 loss 4.803619 loss_att 8.949257 loss_ctc 5.312549 loss_rnnt 3.837982 hw_loss 0.128721 lr 0.00032529 rank 0
2023-02-25 12:41:16,365 DEBUG TRAIN Batch 28/2700 loss 5.890147 loss_att 8.292627 loss_ctc 8.766743 loss_rnnt 4.949202 hw_loss 0.144193 lr 0.00032530 rank 7
2023-02-25 12:41:16,366 DEBUG TRAIN Batch 28/2700 loss 8.139488 loss_att 11.312672 loss_ctc 12.754352 loss_rnnt 6.750167 hw_loss 0.261317 lr 0.00032536 rank 2
2023-02-25 12:41:16,367 DEBUG TRAIN Batch 28/2700 loss 16.444227 loss_att 22.274582 loss_ctc 30.859226 loss_rnnt 13.227158 hw_loss 0.241874 lr 0.00032539 rank 1
2023-02-25 12:41:16,371 DEBUG TRAIN Batch 28/2700 loss 7.746632 loss_att 13.005857 loss_ctc 12.084986 loss_rnnt 6.058835 hw_loss 0.107821 lr 0.00032536 rank 6
2023-02-25 12:41:16,373 DEBUG TRAIN Batch 28/2700 loss 5.740149 loss_att 11.031363 loss_ctc 10.422094 loss_rnnt 3.955931 hw_loss 0.190715 lr 0.00032533 rank 4
2023-02-25 12:41:16,373 DEBUG TRAIN Batch 28/2700 loss 3.467171 loss_att 5.893139 loss_ctc 5.714037 loss_rnnt 2.599497 hw_loss 0.155435 lr 0.00032534 rank 3
2023-02-25 12:41:16,376 DEBUG TRAIN Batch 28/2700 loss 10.903494 loss_att 11.977549 loss_ctc 12.437472 loss_rnnt 10.381525 hw_loss 0.192425 lr 0.00032534 rank 5
2023-02-25 12:42:27,881 DEBUG TRAIN Batch 28/2800 loss 10.153255 loss_att 11.554025 loss_ctc 12.067091 loss_rnnt 9.551641 hw_loss 0.124276 lr 0.00032528 rank 3
2023-02-25 12:42:27,882 DEBUG TRAIN Batch 28/2800 loss 8.925102 loss_att 12.616799 loss_ctc 11.698881 loss_rnnt 7.673167 hw_loss 0.269547 lr 0.00032526 rank 4
2023-02-25 12:42:27,890 DEBUG TRAIN Batch 28/2800 loss 4.464493 loss_att 8.321263 loss_ctc 7.114795 loss_rnnt 3.256003 hw_loss 0.157055 lr 0.00032522 rank 0
2023-02-25 12:42:27,893 DEBUG TRAIN Batch 28/2800 loss 9.505750 loss_att 11.762437 loss_ctc 15.874728 loss_rnnt 8.135412 hw_loss 0.130881 lr 0.00032532 rank 1
2023-02-25 12:42:27,895 DEBUG TRAIN Batch 28/2800 loss 12.629348 loss_att 15.737152 loss_ctc 17.148878 loss_rnnt 11.291435 hw_loss 0.213277 lr 0.00032530 rank 6
2023-02-25 12:42:27,896 DEBUG TRAIN Batch 28/2800 loss 17.401953 loss_att 23.864834 loss_ctc 33.830742 loss_rnnt 13.814769 hw_loss 0.195191 lr 0.00032527 rank 5
2023-02-25 12:42:27,897 DEBUG TRAIN Batch 28/2800 loss 7.237824 loss_att 9.786995 loss_ctc 7.943501 loss_rnnt 6.534439 hw_loss 0.186489 lr 0.00032523 rank 7
2023-02-25 12:42:27,897 DEBUG TRAIN Batch 28/2800 loss 5.960259 loss_att 9.481667 loss_ctc 7.042674 loss_rnnt 5.028971 hw_loss 0.155034 lr 0.00032529 rank 2
2023-02-25 12:43:39,694 DEBUG TRAIN Batch 28/2900 loss 4.014762 loss_att 6.040780 loss_ctc 7.965421 loss_rnnt 2.996666 hw_loss 0.161510 lr 0.00032516 rank 0
2023-02-25 12:43:39,695 DEBUG TRAIN Batch 28/2900 loss 13.590165 loss_att 17.624994 loss_ctc 18.101212 loss_rnnt 12.090530 hw_loss 0.170991 lr 0.00032523 rank 6
2023-02-25 12:43:39,696 DEBUG TRAIN Batch 28/2900 loss 11.567686 loss_att 16.372681 loss_ctc 22.259525 loss_rnnt 9.091109 hw_loss 0.168748 lr 0.00032521 rank 3
2023-02-25 12:43:39,698 DEBUG TRAIN Batch 28/2900 loss 8.564713 loss_att 10.352179 loss_ctc 10.501016 loss_rnnt 7.906780 hw_loss 0.079251 lr 0.00032517 rank 7
2023-02-25 12:43:39,701 DEBUG TRAIN Batch 28/2900 loss 11.770476 loss_att 14.811581 loss_ctc 15.605062 loss_rnnt 10.561432 hw_loss 0.167899 lr 0.00032525 rank 1
2023-02-25 12:43:39,702 DEBUG TRAIN Batch 28/2900 loss 3.631577 loss_att 7.120833 loss_ctc 5.623624 loss_rnnt 2.572810 hw_loss 0.178706 lr 0.00032522 rank 2
2023-02-25 12:43:39,706 DEBUG TRAIN Batch 28/2900 loss 2.450921 loss_att 6.542178 loss_ctc 4.230493 loss_rnnt 1.292940 hw_loss 0.192100 lr 0.00032520 rank 5
2023-02-25 12:43:39,709 DEBUG TRAIN Batch 28/2900 loss 2.364654 loss_att 3.611510 loss_ctc 3.885048 loss_rnnt 1.818159 hw_loss 0.177008 lr 0.00032519 rank 4
2023-02-25 12:44:49,396 DEBUG TRAIN Batch 28/3000 loss 5.478095 loss_att 7.822416 loss_ctc 7.560340 loss_rnnt 4.589050 hw_loss 0.267277 lr 0.00032514 rank 3
2023-02-25 12:44:49,397 DEBUG TRAIN Batch 28/3000 loss 13.242846 loss_att 13.752293 loss_ctc 17.044121 loss_rnnt 12.538143 hw_loss 0.179954 lr 0.00032509 rank 0
2023-02-25 12:44:49,401 DEBUG TRAIN Batch 28/3000 loss 5.395515 loss_att 7.261460 loss_ctc 8.084587 loss_rnnt 4.577224 hw_loss 0.162299 lr 0.00032515 rank 2
2023-02-25 12:44:49,401 DEBUG TRAIN Batch 28/3000 loss 8.739162 loss_att 11.512126 loss_ctc 11.699293 loss_rnnt 7.724137 hw_loss 0.123277 lr 0.00032512 rank 4
2023-02-25 12:44:49,402 DEBUG TRAIN Batch 28/3000 loss 7.270869 loss_att 8.191922 loss_ctc 9.228979 loss_rnnt 6.710849 hw_loss 0.215115 lr 0.00032518 rank 1
2023-02-25 12:44:49,404 DEBUG TRAIN Batch 28/3000 loss 5.344161 loss_att 7.993902 loss_ctc 8.696764 loss_rnnt 4.290428 hw_loss 0.143947 lr 0.00032516 rank 6
2023-02-25 12:44:49,405 DEBUG TRAIN Batch 28/3000 loss 11.230094 loss_att 13.106609 loss_ctc 14.136176 loss_rnnt 10.377312 hw_loss 0.168750 lr 0.00032510 rank 7
2023-02-25 12:44:49,408 DEBUG TRAIN Batch 28/3000 loss 17.122284 loss_att 20.062153 loss_ctc 27.073715 loss_rnnt 15.113334 hw_loss 0.176473 lr 0.00032513 rank 5
2023-02-25 12:45:59,522 DEBUG TRAIN Batch 28/3100 loss 2.510595 loss_att 5.262362 loss_ctc 3.912918 loss_rnnt 1.700511 hw_loss 0.136414 lr 0.00032511 rank 1
2023-02-25 12:45:59,522 DEBUG TRAIN Batch 28/3100 loss 10.646240 loss_att 10.538679 loss_ctc 14.676558 loss_rnnt 10.034672 hw_loss 0.179445 lr 0.00032503 rank 7
2023-02-25 12:45:59,523 DEBUG TRAIN Batch 28/3100 loss 6.539120 loss_att 8.532423 loss_ctc 10.017239 loss_rnnt 5.575798 hw_loss 0.189211 lr 0.00032509 rank 6
2023-02-25 12:45:59,525 DEBUG TRAIN Batch 28/3100 loss 7.554964 loss_att 8.705264 loss_ctc 12.025640 loss_rnnt 6.623512 hw_loss 0.197441 lr 0.00032502 rank 0
2023-02-25 12:45:59,525 DEBUG TRAIN Batch 28/3100 loss 9.783843 loss_att 11.175667 loss_ctc 12.933460 loss_rnnt 8.979465 hw_loss 0.198873 lr 0.00032508 rank 2
2023-02-25 12:45:59,527 DEBUG TRAIN Batch 28/3100 loss 9.555331 loss_att 11.738685 loss_ctc 12.574962 loss_rnnt 8.630416 hw_loss 0.160551 lr 0.00032506 rank 5
2023-02-25 12:45:59,570 DEBUG TRAIN Batch 28/3100 loss 2.826804 loss_att 4.917187 loss_ctc 5.009247 loss_rnnt 1.995097 hw_loss 0.229947 lr 0.00032507 rank 3
2023-02-25 12:45:59,576 DEBUG TRAIN Batch 28/3100 loss 1.808998 loss_att 4.035526 loss_ctc 2.907762 loss_rnnt 1.089250 hw_loss 0.239890 lr 0.00032505 rank 4
2023-02-25 12:47:13,886 DEBUG TRAIN Batch 28/3200 loss 5.331050 loss_att 5.738826 loss_ctc 7.699085 loss_rnnt 4.742119 hw_loss 0.359321 lr 0.00032500 rank 3
2023-02-25 12:47:13,887 DEBUG TRAIN Batch 28/3200 loss 3.571090 loss_att 7.214379 loss_ctc 5.326476 loss_rnnt 2.492120 hw_loss 0.217990 lr 0.00032495 rank 0
2023-02-25 12:47:13,890 DEBUG TRAIN Batch 28/3200 loss 4.696131 loss_att 11.608171 loss_ctc 5.713356 loss_rnnt 3.062875 hw_loss 0.216035 lr 0.00032499 rank 5
2023-02-25 12:47:13,891 DEBUG TRAIN Batch 28/3200 loss 12.846991 loss_att 14.912282 loss_ctc 16.190790 loss_rnnt 11.940745 hw_loss 0.088778 lr 0.00032504 rank 1
2023-02-25 12:47:13,894 DEBUG TRAIN Batch 28/3200 loss 5.370244 loss_att 7.006653 loss_ctc 7.823938 loss_rnnt 4.599505 hw_loss 0.218057 lr 0.00032496 rank 7
2023-02-25 12:47:13,897 DEBUG TRAIN Batch 28/3200 loss 15.412679 loss_att 18.055473 loss_ctc 20.183868 loss_rnnt 14.172318 hw_loss 0.141829 lr 0.00032501 rank 2
2023-02-25 12:47:13,897 DEBUG TRAIN Batch 28/3200 loss 7.855179 loss_att 13.427701 loss_ctc 12.425585 loss_rnnt 6.021929 hw_loss 0.205047 lr 0.00032498 rank 4
2023-02-25 12:47:13,942 DEBUG TRAIN Batch 28/3200 loss 16.770819 loss_att 17.388128 loss_ctc 28.806210 loss_rnnt 14.982141 hw_loss 0.113428 lr 0.00032502 rank 6
2023-02-25 12:48:23,413 DEBUG TRAIN Batch 28/3300 loss 2.220505 loss_att 4.752129 loss_ctc 6.579360 loss_rnnt 1.071077 hw_loss 0.116103 lr 0.00032495 rank 6
2023-02-25 12:48:23,421 DEBUG TRAIN Batch 28/3300 loss 6.341751 loss_att 8.554675 loss_ctc 9.893085 loss_rnnt 5.335926 hw_loss 0.168241 lr 0.00032488 rank 0
2023-02-25 12:48:23,423 DEBUG TRAIN Batch 28/3300 loss 2.628407 loss_att 5.288694 loss_ctc 4.337876 loss_rnnt 1.787610 hw_loss 0.151518 lr 0.00032493 rank 3
2023-02-25 12:48:23,432 DEBUG TRAIN Batch 28/3300 loss 8.740603 loss_att 12.079176 loss_ctc 15.098999 loss_rnnt 7.155437 hw_loss 0.130622 lr 0.00032489 rank 7
2023-02-25 12:48:23,433 DEBUG TRAIN Batch 28/3300 loss 11.311213 loss_att 16.847103 loss_ctc 25.565802 loss_rnnt 8.234642 hw_loss 0.128965 lr 0.00032492 rank 4
2023-02-25 12:48:23,454 DEBUG TRAIN Batch 28/3300 loss 5.142033 loss_att 6.917113 loss_ctc 7.991861 loss_rnnt 4.343411 hw_loss 0.119304 lr 0.00032497 rank 1
2023-02-25 12:48:23,462 DEBUG TRAIN Batch 28/3300 loss 5.042161 loss_att 7.819295 loss_ctc 7.674684 loss_rnnt 4.059654 hw_loss 0.142644 lr 0.00032494 rank 2
2023-02-25 12:48:23,494 DEBUG TRAIN Batch 28/3300 loss 5.160234 loss_att 9.878409 loss_ctc 7.547390 loss_rnnt 3.825342 hw_loss 0.136817 lr 0.00032492 rank 5
2023-02-25 12:49:34,108 DEBUG TRAIN Batch 28/3400 loss 4.654122 loss_att 6.423653 loss_ctc 6.622693 loss_rnnt 3.975449 hw_loss 0.116797 lr 0.00032481 rank 0
2023-02-25 12:49:34,114 DEBUG TRAIN Batch 28/3400 loss 6.861903 loss_att 10.312040 loss_ctc 12.383986 loss_rnnt 5.339883 hw_loss 0.179464 lr 0.00032488 rank 6
2023-02-25 12:49:34,113 DEBUG TRAIN Batch 28/3400 loss 5.972783 loss_att 8.321714 loss_ctc 7.457086 loss_rnnt 5.196130 hw_loss 0.204300 lr 0.00032482 rank 7
2023-02-25 12:49:34,115 DEBUG TRAIN Batch 28/3400 loss 8.328242 loss_att 11.119507 loss_ctc 11.998444 loss_rnnt 7.150180 hw_loss 0.244594 lr 0.00032486 rank 3
2023-02-25 12:49:34,118 DEBUG TRAIN Batch 28/3400 loss 6.879666 loss_att 10.880310 loss_ctc 11.567289 loss_rnnt 5.352958 hw_loss 0.190432 lr 0.00032485 rank 5
2023-02-25 12:49:34,119 DEBUG TRAIN Batch 28/3400 loss 9.799339 loss_att 12.541740 loss_ctc 20.226265 loss_rnnt 7.682771 hw_loss 0.333434 lr 0.00032488 rank 2
2023-02-25 12:49:34,120 DEBUG TRAIN Batch 28/3400 loss 15.170252 loss_att 20.258644 loss_ctc 23.519712 loss_rnnt 12.969526 hw_loss 0.130848 lr 0.00032490 rank 1
2023-02-25 12:49:34,122 DEBUG TRAIN Batch 28/3400 loss 9.262539 loss_att 9.815225 loss_ctc 14.035618 loss_rnnt 8.466603 hw_loss 0.091853 lr 0.00032485 rank 4
2023-02-25 12:50:45,591 DEBUG TRAIN Batch 28/3500 loss 16.800634 loss_att 22.147961 loss_ctc 21.963139 loss_rnnt 14.908075 hw_loss 0.252669 lr 0.00032479 rank 5
2023-02-25 12:50:45,597 DEBUG TRAIN Batch 28/3500 loss 6.137593 loss_att 8.584601 loss_ctc 9.174222 loss_rnnt 5.152940 hw_loss 0.169439 lr 0.00032482 rank 6
2023-02-25 12:50:45,598 DEBUG TRAIN Batch 28/3500 loss 7.494754 loss_att 11.703259 loss_ctc 9.490797 loss_rnnt 6.284909 hw_loss 0.191258 lr 0.00032480 rank 3
2023-02-25 12:50:45,599 DEBUG TRAIN Batch 28/3500 loss 5.235617 loss_att 7.098172 loss_ctc 5.759738 loss_rnnt 4.697777 hw_loss 0.178961 lr 0.00032484 rank 1
2023-02-25 12:50:45,599 DEBUG TRAIN Batch 28/3500 loss 5.298052 loss_att 8.257288 loss_ctc 8.780552 loss_rnnt 4.173034 hw_loss 0.129070 lr 0.00032475 rank 7
2023-02-25 12:50:45,601 DEBUG TRAIN Batch 28/3500 loss 7.256931 loss_att 8.211427 loss_ctc 6.605890 loss_rnnt 7.062673 hw_loss 0.169058 lr 0.00032478 rank 4
2023-02-25 12:50:45,601 DEBUG TRAIN Batch 28/3500 loss 4.347427 loss_att 5.663261 loss_ctc 5.797774 loss_rnnt 3.787266 hw_loss 0.194277 lr 0.00032474 rank 0
2023-02-25 12:50:45,606 DEBUG TRAIN Batch 28/3500 loss 7.991583 loss_att 11.188310 loss_ctc 11.616488 loss_rnnt 6.818899 hw_loss 0.093784 lr 0.00032481 rank 2
2023-02-25 12:51:58,229 DEBUG TRAIN Batch 28/3600 loss 7.527024 loss_att 13.224804 loss_ctc 11.877645 loss_rnnt 5.754181 hw_loss 0.099756 lr 0.00032468 rank 0
2023-02-25 12:51:58,232 DEBUG TRAIN Batch 28/3600 loss 6.465497 loss_att 9.065773 loss_ctc 10.465403 loss_rnnt 5.311084 hw_loss 0.189444 lr 0.00032468 rank 7
2023-02-25 12:51:58,235 DEBUG TRAIN Batch 28/3600 loss 10.613112 loss_att 11.963060 loss_ctc 14.605955 loss_rnnt 9.718655 hw_loss 0.172667 lr 0.00032474 rank 2
2023-02-25 12:51:58,237 DEBUG TRAIN Batch 28/3600 loss 12.381765 loss_att 13.921864 loss_ctc 19.118141 loss_rnnt 11.058376 hw_loss 0.219724 lr 0.00032471 rank 4
2023-02-25 12:51:58,238 DEBUG TRAIN Batch 28/3600 loss 3.796232 loss_att 6.328542 loss_ctc 6.423902 loss_rnnt 2.854691 hw_loss 0.158857 lr 0.00032475 rank 6
2023-02-25 12:51:58,240 DEBUG TRAIN Batch 28/3600 loss 20.924734 loss_att 20.395039 loss_ctc 28.420271 loss_rnnt 19.975761 hw_loss 0.104075 lr 0.00032477 rank 1
2023-02-25 12:51:58,240 DEBUG TRAIN Batch 28/3600 loss 8.859766 loss_att 12.975060 loss_ctc 14.738367 loss_rnnt 7.082356 hw_loss 0.319755 lr 0.00032473 rank 3
2023-02-25 12:51:58,241 DEBUG TRAIN Batch 28/3600 loss 9.774629 loss_att 12.880240 loss_ctc 15.661693 loss_rnnt 8.300130 hw_loss 0.128315 lr 0.00032472 rank 5
2023-02-25 12:53:08,946 DEBUG TRAIN Batch 28/3700 loss 13.879132 loss_att 16.425081 loss_ctc 20.741585 loss_rnnt 12.325818 hw_loss 0.242119 lr 0.00032461 rank 0
2023-02-25 12:53:08,948 DEBUG TRAIN Batch 28/3700 loss 8.953765 loss_att 10.752815 loss_ctc 12.824122 loss_rnnt 7.942455 hw_loss 0.253970 lr 0.00032470 rank 1
2023-02-25 12:53:08,951 DEBUG TRAIN Batch 28/3700 loss 7.391687 loss_att 9.137051 loss_ctc 11.659065 loss_rnnt 6.328985 hw_loss 0.271211 lr 0.00032467 rank 2
2023-02-25 12:53:08,952 DEBUG TRAIN Batch 28/3700 loss 3.990952 loss_att 6.365000 loss_ctc 6.946638 loss_rnnt 3.020982 hw_loss 0.189503 lr 0.00032466 rank 3
2023-02-25 12:53:08,953 DEBUG TRAIN Batch 28/3700 loss 9.920895 loss_att 11.670156 loss_ctc 16.209833 loss_rnnt 8.615376 hw_loss 0.219638 lr 0.00032464 rank 4
2023-02-25 12:53:08,953 DEBUG TRAIN Batch 28/3700 loss 9.472529 loss_att 10.566833 loss_ctc 11.930059 loss_rnnt 8.806362 hw_loss 0.224316 lr 0.00032462 rank 7
2023-02-25 12:53:08,960 DEBUG TRAIN Batch 28/3700 loss 13.920826 loss_att 15.345681 loss_ctc 18.773966 loss_rnnt 12.911928 hw_loss 0.144075 lr 0.00032468 rank 6
2023-02-25 12:53:08,965 DEBUG TRAIN Batch 28/3700 loss 7.944216 loss_att 10.374781 loss_ctc 18.376806 loss_rnnt 5.976781 hw_loss 0.169330 lr 0.00032465 rank 5
2023-02-25 12:54:19,020 DEBUG TRAIN Batch 28/3800 loss 8.842666 loss_att 14.558414 loss_ctc 17.767584 loss_rnnt 6.395663 hw_loss 0.213494 lr 0.00032460 rank 2
2023-02-25 12:54:19,021 DEBUG TRAIN Batch 28/3800 loss 5.210755 loss_att 6.301310 loss_ctc 6.675573 loss_rnnt 4.679657 hw_loss 0.220645 lr 0.00032454 rank 0
2023-02-25 12:54:19,022 DEBUG TRAIN Batch 28/3800 loss 8.951349 loss_att 10.778211 loss_ctc 10.904713 loss_rnnt 8.214313 hw_loss 0.208530 lr 0.00032459 rank 3
2023-02-25 12:54:19,025 DEBUG TRAIN Batch 28/3800 loss 10.803599 loss_att 14.504592 loss_ctc 14.556991 loss_rnnt 9.436123 hw_loss 0.237798 lr 0.00032455 rank 7
2023-02-25 12:54:19,027 DEBUG TRAIN Batch 28/3800 loss 3.390200 loss_att 4.181051 loss_ctc 4.772791 loss_rnnt 2.903212 hw_loss 0.270885 lr 0.00032463 rank 1
2023-02-25 12:54:19,027 DEBUG TRAIN Batch 28/3800 loss 6.807260 loss_att 8.953605 loss_ctc 9.685959 loss_rnnt 5.892938 hw_loss 0.189800 lr 0.00032461 rank 6
2023-02-25 12:54:19,027 DEBUG TRAIN Batch 28/3800 loss 4.869175 loss_att 8.943846 loss_ctc 10.624506 loss_rnnt 3.202043 hw_loss 0.159038 lr 0.00032457 rank 4
2023-02-25 12:54:19,029 DEBUG TRAIN Batch 28/3800 loss 6.777910 loss_att 6.844243 loss_ctc 8.899448 loss_rnnt 6.325823 hw_loss 0.292402 lr 0.00032458 rank 5
2023-02-25 12:55:31,744 DEBUG TRAIN Batch 28/3900 loss 4.640317 loss_att 7.756212 loss_ctc 9.332065 loss_rnnt 3.261118 hw_loss 0.244602 lr 0.00032447 rank 0
2023-02-25 12:55:31,747 DEBUG TRAIN Batch 28/3900 loss 8.523939 loss_att 14.146296 loss_ctc 16.801329 loss_rnnt 6.194562 hw_loss 0.189851 lr 0.00032456 rank 1
2023-02-25 12:55:31,747 DEBUG TRAIN Batch 28/3900 loss 4.737576 loss_att 7.136699 loss_ctc 9.070585 loss_rnnt 3.613407 hw_loss 0.124894 lr 0.00032451 rank 5
2023-02-25 12:55:31,749 DEBUG TRAIN Batch 28/3900 loss 3.633445 loss_att 7.233256 loss_ctc 8.310289 loss_rnnt 2.173990 hw_loss 0.217338 lr 0.00032452 rank 3
2023-02-25 12:55:31,750 DEBUG TRAIN Batch 28/3900 loss 4.826102 loss_att 7.738811 loss_ctc 10.888536 loss_rnnt 3.308776 hw_loss 0.237111 lr 0.00032448 rank 7
2023-02-25 12:55:31,762 DEBUG TRAIN Batch 28/3900 loss 7.354748 loss_att 10.175301 loss_ctc 10.290206 loss_rnnt 6.285682 hw_loss 0.212925 lr 0.00032453 rank 2
2023-02-25 12:55:31,773 DEBUG TRAIN Batch 28/3900 loss 3.387288 loss_att 6.311769 loss_ctc 3.865504 loss_rnnt 2.642264 hw_loss 0.180686 lr 0.00032454 rank 6
2023-02-25 12:55:31,799 DEBUG TRAIN Batch 28/3900 loss 17.041927 loss_att 16.637072 loss_ctc 21.891514 loss_rnnt 16.375473 hw_loss 0.189025 lr 0.00032450 rank 4
2023-02-25 12:56:42,391 DEBUG TRAIN Batch 28/4000 loss 7.637727 loss_att 9.952516 loss_ctc 10.768424 loss_rnnt 6.660253 hw_loss 0.182045 lr 0.00032440 rank 0
2023-02-25 12:56:42,396 DEBUG TRAIN Batch 28/4000 loss 5.583846 loss_att 7.635280 loss_ctc 7.070518 loss_rnnt 4.879383 hw_loss 0.179912 lr 0.00032449 rank 1
2023-02-25 12:56:42,398 DEBUG TRAIN Batch 28/4000 loss 7.357631 loss_att 10.425654 loss_ctc 9.517555 loss_rnnt 6.385629 hw_loss 0.132014 lr 0.00032445 rank 3
2023-02-25 12:56:42,401 DEBUG TRAIN Batch 28/4000 loss 14.968241 loss_att 18.921833 loss_ctc 26.216057 loss_rnnt 12.563759 hw_loss 0.213853 lr 0.00032447 rank 6
2023-02-25 12:56:42,405 DEBUG TRAIN Batch 28/4000 loss 8.553940 loss_att 10.815777 loss_ctc 12.612013 loss_rnnt 7.465202 hw_loss 0.178675 lr 0.00032441 rank 7
2023-02-25 12:56:42,405 DEBUG TRAIN Batch 28/4000 loss 9.396648 loss_att 12.907162 loss_ctc 14.706242 loss_rnnt 7.901649 hw_loss 0.159282 lr 0.00032444 rank 4
2023-02-25 12:56:42,405 DEBUG TRAIN Batch 28/4000 loss 3.357224 loss_att 6.319044 loss_ctc 5.445390 loss_rnnt 2.441557 hw_loss 0.084153 lr 0.00032446 rank 2
2023-02-25 12:56:42,403 DEBUG TRAIN Batch 28/4000 loss 4.165261 loss_att 5.433985 loss_ctc 4.139040 loss_rnnt 3.789272 hw_loss 0.235764 lr 0.00032444 rank 5
2023-02-25 12:57:51,954 DEBUG TRAIN Batch 28/4100 loss 11.676949 loss_att 11.609599 loss_ctc 12.796630 loss_rnnt 11.423941 hw_loss 0.219727 lr 0.00032433 rank 0
2023-02-25 12:57:51,957 DEBUG TRAIN Batch 28/4100 loss 7.732063 loss_att 9.685352 loss_ctc 9.838770 loss_rnnt 6.939606 hw_loss 0.226697 lr 0.00032438 rank 3
2023-02-25 12:57:51,957 DEBUG TRAIN Batch 28/4100 loss 8.040693 loss_att 9.661415 loss_ctc 12.395590 loss_rnnt 7.036057 hw_loss 0.187196 lr 0.00032438 rank 5
2023-02-25 12:57:51,958 DEBUG TRAIN Batch 28/4100 loss 13.680339 loss_att 16.948151 loss_ctc 13.757994 loss_rnnt 12.928408 hw_loss 0.165026 lr 0.00032440 rank 6
2023-02-25 12:57:51,959 DEBUG TRAIN Batch 28/4100 loss 2.630874 loss_att 4.932276 loss_ctc 6.421844 loss_rnnt 1.591643 hw_loss 0.137790 lr 0.00032437 rank 4
2023-02-25 12:57:51,965 DEBUG TRAIN Batch 28/4100 loss 8.656032 loss_att 11.317218 loss_ctc 9.868698 loss_rnnt 7.884109 hw_loss 0.146243 lr 0.00032434 rank 7
2023-02-25 12:57:51,966 DEBUG TRAIN Batch 28/4100 loss 9.333231 loss_att 10.654442 loss_ctc 13.352582 loss_rnnt 8.410372 hw_loss 0.230069 lr 0.00032443 rank 1
2023-02-25 12:57:51,966 DEBUG TRAIN Batch 28/4100 loss 6.913162 loss_att 11.275026 loss_ctc 12.423941 loss_rnnt 5.221084 hw_loss 0.159252 lr 0.00032440 rank 2
2023-02-25 12:59:02,945 DEBUG TRAIN Batch 28/4200 loss 9.248334 loss_att 10.410957 loss_ctc 15.467155 loss_rnnt 8.087969 hw_loss 0.184994 lr 0.00032432 rank 3
2023-02-25 12:59:02,955 DEBUG TRAIN Batch 28/4200 loss 4.250637 loss_att 7.497972 loss_ctc 5.938982 loss_rnnt 3.226870 hw_loss 0.279726 lr 0.00032427 rank 0
2023-02-25 12:59:02,955 DEBUG TRAIN Batch 28/4200 loss 4.131749 loss_att 8.464163 loss_ctc 5.775640 loss_rnnt 2.943257 hw_loss 0.192795 lr 0.00032434 rank 6
2023-02-25 12:59:02,956 DEBUG TRAIN Batch 28/4200 loss 8.848571 loss_att 11.520848 loss_ctc 12.261580 loss_rnnt 7.760007 hw_loss 0.185700 lr 0.00032436 rank 1
2023-02-25 12:59:02,958 DEBUG TRAIN Batch 28/4200 loss 4.531846 loss_att 6.629643 loss_ctc 4.679599 loss_rnnt 3.936834 hw_loss 0.292034 lr 0.00032433 rank 2
2023-02-25 12:59:02,958 DEBUG TRAIN Batch 28/4200 loss 1.251833 loss_att 3.906967 loss_ctc 1.959808 loss_rnnt 0.524028 hw_loss 0.191966 lr 0.00032430 rank 4
2023-02-25 12:59:02,959 DEBUG TRAIN Batch 28/4200 loss 4.146537 loss_att 8.090007 loss_ctc 10.872356 loss_rnnt 2.335299 hw_loss 0.235817 lr 0.00032427 rank 7
2023-02-25 12:59:02,959 DEBUG TRAIN Batch 28/4200 loss 10.119170 loss_att 12.587798 loss_ctc 17.429945 loss_rnnt 8.546556 hw_loss 0.195218 lr 0.00032431 rank 5
2023-02-25 13:00:15,805 DEBUG TRAIN Batch 28/4300 loss 2.296066 loss_att 4.549081 loss_ctc 2.535299 loss_rnnt 1.735919 hw_loss 0.145588 lr 0.00032420 rank 0
2023-02-25 13:00:15,805 DEBUG TRAIN Batch 28/4300 loss 7.880302 loss_att 9.625881 loss_ctc 9.657514 loss_rnnt 7.153397 hw_loss 0.264052 lr 0.00032425 rank 3
2023-02-25 13:00:15,807 DEBUG TRAIN Batch 28/4300 loss 20.682205 loss_att 21.277075 loss_ctc 27.156805 loss_rnnt 19.627480 hw_loss 0.135884 lr 0.00032427 rank 6
2023-02-25 13:00:15,810 DEBUG TRAIN Batch 28/4300 loss 10.439290 loss_att 13.790675 loss_ctc 13.095673 loss_rnnt 9.308268 hw_loss 0.199803 lr 0.00032421 rank 7
2023-02-25 13:00:15,811 DEBUG TRAIN Batch 28/4300 loss 9.063394 loss_att 11.625448 loss_ctc 12.566520 loss_rnnt 7.956614 hw_loss 0.238659 lr 0.00032423 rank 4
2023-02-25 13:00:15,811 DEBUG TRAIN Batch 28/4300 loss 11.669514 loss_att 14.583279 loss_ctc 18.306353 loss_rnnt 10.112000 hw_loss 0.168467 lr 0.00032429 rank 1
2023-02-25 13:00:15,812 DEBUG TRAIN Batch 28/4300 loss 10.227511 loss_att 12.356163 loss_ctc 15.111005 loss_rnnt 9.030530 hw_loss 0.225222 lr 0.00032426 rank 2
2023-02-25 13:00:15,860 DEBUG TRAIN Batch 28/4300 loss 9.784690 loss_att 12.512967 loss_ctc 12.122115 loss_rnnt 8.844073 hw_loss 0.156194 lr 0.00032424 rank 5
2023-02-25 13:01:25,759 DEBUG TRAIN Batch 28/4400 loss 14.322010 loss_att 16.127924 loss_ctc 16.575293 loss_rnnt 13.551221 hw_loss 0.204693 lr 0.00032418 rank 3
2023-02-25 13:01:25,760 DEBUG TRAIN Batch 28/4400 loss 13.689362 loss_att 14.684326 loss_ctc 17.340994 loss_rnnt 12.831783 hw_loss 0.321940 lr 0.00032413 rank 0
2023-02-25 13:01:25,764 DEBUG TRAIN Batch 28/4400 loss 11.503798 loss_att 16.318295 loss_ctc 20.334011 loss_rnnt 9.321461 hw_loss 0.078890 lr 0.00032416 rank 4
2023-02-25 13:01:25,764 DEBUG TRAIN Batch 28/4400 loss 5.552957 loss_att 6.586648 loss_ctc 8.623614 loss_rnnt 4.810172 hw_loss 0.237424 lr 0.00032414 rank 7
2023-02-25 13:01:25,765 DEBUG TRAIN Batch 28/4400 loss 7.404613 loss_att 12.770480 loss_ctc 10.823586 loss_rnnt 5.757741 hw_loss 0.220942 lr 0.00032420 rank 6
2023-02-25 13:01:25,766 DEBUG TRAIN Batch 28/4400 loss 9.215528 loss_att 10.520250 loss_ctc 13.637168 loss_rnnt 8.243223 hw_loss 0.228394 lr 0.00032417 rank 5
2023-02-25 13:01:25,768 DEBUG TRAIN Batch 28/4400 loss 17.226040 loss_att 19.209461 loss_ctc 23.119726 loss_rnnt 15.935766 hw_loss 0.202058 lr 0.00032422 rank 1
2023-02-25 13:01:25,770 DEBUG TRAIN Batch 28/4400 loss 9.871253 loss_att 9.971841 loss_ctc 11.511062 loss_rnnt 9.482615 hw_loss 0.281023 lr 0.00032419 rank 2
2023-02-25 13:02:35,855 DEBUG TRAIN Batch 28/4500 loss 4.282690 loss_att 6.494733 loss_ctc 3.150724 loss_rnnt 3.920507 hw_loss 0.132566 lr 0.00032415 rank 1
2023-02-25 13:02:35,873 DEBUG TRAIN Batch 28/4500 loss 6.259018 loss_att 8.725962 loss_ctc 9.482265 loss_rnnt 5.241734 hw_loss 0.176494 lr 0.00032407 rank 7
2023-02-25 13:02:35,877 DEBUG TRAIN Batch 28/4500 loss 7.348832 loss_att 9.988688 loss_ctc 8.882193 loss_rnnt 6.546415 hw_loss 0.131247 lr 0.00032406 rank 0
2023-02-25 13:02:35,877 DEBUG TRAIN Batch 28/4500 loss 5.076274 loss_att 7.157356 loss_ctc 7.332198 loss_rnnt 4.258329 hw_loss 0.189260 lr 0.00032410 rank 4
2023-02-25 13:02:35,877 DEBUG TRAIN Batch 28/4500 loss 10.770008 loss_att 9.875779 loss_ctc 13.463793 loss_rnnt 10.469207 hw_loss 0.225892 lr 0.00032411 rank 3
2023-02-25 13:02:35,879 DEBUG TRAIN Batch 28/4500 loss 8.272752 loss_att 8.687181 loss_ctc 13.022696 loss_rnnt 7.476465 hw_loss 0.150140 lr 0.00032412 rank 2
2023-02-25 13:02:35,881 DEBUG TRAIN Batch 28/4500 loss 5.981726 loss_att 8.807732 loss_ctc 5.889198 loss_rnnt 5.319308 hw_loss 0.205412 lr 0.00032410 rank 5
2023-02-25 13:02:35,925 DEBUG TRAIN Batch 28/4500 loss 4.065375 loss_att 8.991568 loss_ctc 10.337566 loss_rnnt 2.148520 hw_loss 0.178733 lr 0.00032413 rank 6
2023-02-25 13:03:48,309 DEBUG TRAIN Batch 28/4600 loss 10.766616 loss_att 14.079236 loss_ctc 15.094564 loss_rnnt 9.386724 hw_loss 0.263079 lr 0.00032399 rank 0
2023-02-25 13:03:48,315 DEBUG TRAIN Batch 28/4600 loss 3.192759 loss_att 7.771283 loss_ctc 2.480881 loss_rnnt 2.297631 hw_loss 0.139389 lr 0.00032403 rank 4
2023-02-25 13:03:48,325 DEBUG TRAIN Batch 28/4600 loss 5.587559 loss_att 8.928428 loss_ctc 9.965048 loss_rnnt 4.276340 hw_loss 0.111339 lr 0.00032404 rank 3
2023-02-25 13:03:48,325 DEBUG TRAIN Batch 28/4600 loss 11.413428 loss_att 11.157388 loss_ctc 14.967224 loss_rnnt 10.884470 hw_loss 0.199363 lr 0.00032400 rank 7
2023-02-25 13:03:48,328 DEBUG TRAIN Batch 28/4600 loss 11.851223 loss_att 13.199454 loss_ctc 15.918745 loss_rnnt 10.899708 hw_loss 0.261623 lr 0.00032406 rank 2
2023-02-25 13:03:48,331 DEBUG TRAIN Batch 28/4600 loss 9.179746 loss_att 10.998742 loss_ctc 10.182867 loss_rnnt 8.555481 hw_loss 0.237592 lr 0.00032408 rank 1
2023-02-25 13:03:48,331 DEBUG TRAIN Batch 28/4600 loss 14.219738 loss_att 16.974316 loss_ctc 22.073936 loss_rnnt 12.512500 hw_loss 0.204557 lr 0.00032403 rank 5
2023-02-25 13:03:48,381 DEBUG TRAIN Batch 28/4600 loss 1.641684 loss_att 4.929093 loss_ctc 2.199161 loss_rnnt 0.812688 hw_loss 0.182220 lr 0.00032406 rank 6
2023-02-25 13:05:00,004 DEBUG TRAIN Batch 28/4700 loss 8.909952 loss_att 12.848341 loss_ctc 12.094241 loss_rnnt 7.595911 hw_loss 0.190860 lr 0.00032399 rank 2
2023-02-25 13:05:00,005 DEBUG TRAIN Batch 28/4700 loss 8.852881 loss_att 11.599078 loss_ctc 18.708134 loss_rnnt 6.882832 hw_loss 0.200206 lr 0.00032398 rank 3
2023-02-25 13:05:00,005 DEBUG TRAIN Batch 28/4700 loss 7.132374 loss_att 8.836308 loss_ctc 10.480642 loss_rnnt 6.282859 hw_loss 0.116800 lr 0.00032392 rank 0
2023-02-25 13:05:00,007 DEBUG TRAIN Batch 28/4700 loss 4.004100 loss_att 6.659243 loss_ctc 5.132543 loss_rnnt 3.114006 hw_loss 0.391139 lr 0.00032400 rank 6
2023-02-25 13:05:00,008 DEBUG TRAIN Batch 28/4700 loss 4.273336 loss_att 8.666254 loss_ctc 5.809856 loss_rnnt 3.059401 hw_loss 0.244654 lr 0.00032393 rank 7
2023-02-25 13:05:00,011 DEBUG TRAIN Batch 28/4700 loss 8.703179 loss_att 9.770912 loss_ctc 10.885822 loss_rnnt 8.094332 hw_loss 0.195528 lr 0.00032402 rank 1
2023-02-25 13:05:00,012 DEBUG TRAIN Batch 28/4700 loss 3.674520 loss_att 6.173422 loss_ctc 4.543959 loss_rnnt 3.006376 hw_loss 0.098323 lr 0.00032396 rank 4
2023-02-25 13:05:00,013 DEBUG TRAIN Batch 28/4700 loss 2.876545 loss_att 7.045345 loss_ctc 4.354854 loss_rnnt 1.745101 hw_loss 0.188581 lr 0.00032397 rank 5
2023-02-25 13:06:09,590 DEBUG TRAIN Batch 28/4800 loss 5.765257 loss_att 7.139188 loss_ctc 7.220295 loss_rnnt 5.200633 hw_loss 0.179687 lr 0.00032386 rank 0
2023-02-25 13:06:09,591 DEBUG TRAIN Batch 28/4800 loss 12.600176 loss_att 16.745214 loss_ctc 17.053101 loss_rnnt 11.064701 hw_loss 0.211396 lr 0.00032387 rank 7
2023-02-25 13:06:09,593 DEBUG TRAIN Batch 28/4800 loss 6.211987 loss_att 9.403925 loss_ctc 8.937027 loss_rnnt 5.146211 hw_loss 0.120093 lr 0.00032393 rank 6
2023-02-25 13:06:09,595 DEBUG TRAIN Batch 28/4800 loss 11.352490 loss_att 12.398318 loss_ctc 14.600435 loss_rnnt 10.634047 hw_loss 0.142910 lr 0.00032391 rank 3
2023-02-25 13:06:09,595 DEBUG TRAIN Batch 28/4800 loss 6.784748 loss_att 9.850984 loss_ctc 10.610929 loss_rnnt 5.611823 hw_loss 0.092852 lr 0.00032390 rank 5
2023-02-25 13:06:09,597 DEBUG TRAIN Batch 28/4800 loss 11.032918 loss_att 11.167805 loss_ctc 13.489977 loss_rnnt 10.583184 hw_loss 0.178402 lr 0.00032389 rank 4
2023-02-25 13:06:09,598 DEBUG TRAIN Batch 28/4800 loss 10.194303 loss_att 12.118913 loss_ctc 11.866239 loss_rnnt 9.478692 hw_loss 0.202057 lr 0.00032395 rank 1
2023-02-25 13:06:09,602 DEBUG TRAIN Batch 28/4800 loss 8.352118 loss_att 13.447212 loss_ctc 13.759926 loss_rnnt 6.558187 hw_loss 0.101008 lr 0.00032392 rank 2
2023-02-25 13:07:19,852 DEBUG TRAIN Batch 28/4900 loss 9.285204 loss_att 11.309845 loss_ctc 13.731913 loss_rnnt 8.215381 hw_loss 0.135001 lr 0.00032384 rank 3
2023-02-25 13:07:19,853 DEBUG TRAIN Batch 28/4900 loss 8.558899 loss_att 10.746302 loss_ctc 13.365147 loss_rnnt 7.354515 hw_loss 0.236382 lr 0.00032388 rank 1
2023-02-25 13:07:19,854 DEBUG TRAIN Batch 28/4900 loss 8.742920 loss_att 9.965857 loss_ctc 10.511627 loss_rnnt 8.176525 hw_loss 0.161212 lr 0.00032379 rank 0
2023-02-25 13:07:19,857 DEBUG TRAIN Batch 28/4900 loss 4.523619 loss_att 8.606208 loss_ctc 5.083838 loss_rnnt 3.495527 hw_loss 0.256647 lr 0.00032386 rank 6
2023-02-25 13:07:19,858 DEBUG TRAIN Batch 28/4900 loss 6.648217 loss_att 9.447583 loss_ctc 10.931168 loss_rnnt 5.421245 hw_loss 0.180071 lr 0.00032382 rank 4
2023-02-25 13:07:19,858 DEBUG TRAIN Batch 28/4900 loss 10.387889 loss_att 13.683797 loss_ctc 18.319431 loss_rnnt 8.589113 hw_loss 0.153854 lr 0.00032380 rank 7
2023-02-25 13:07:19,858 DEBUG TRAIN Batch 28/4900 loss 9.483541 loss_att 12.907808 loss_ctc 16.072844 loss_rnnt 7.800697 hw_loss 0.223905 lr 0.00032385 rank 2
2023-02-25 13:07:19,907 DEBUG TRAIN Batch 28/4900 loss 6.074541 loss_att 8.571604 loss_ctc 7.702629 loss_rnnt 5.251897 hw_loss 0.199037 lr 0.00032383 rank 5
2023-02-25 13:08:33,372 DEBUG TRAIN Batch 28/5000 loss 7.386153 loss_att 10.152974 loss_ctc 8.988904 loss_rnnt 6.548429 hw_loss 0.132485 lr 0.00032377 rank 3
2023-02-25 13:08:33,371 DEBUG TRAIN Batch 28/5000 loss 7.361561 loss_att 9.891788 loss_ctc 10.120134 loss_rnnt 6.380922 hw_loss 0.200219 lr 0.00032373 rank 7
2023-02-25 13:08:33,373 DEBUG TRAIN Batch 28/5000 loss 3.778884 loss_att 4.807655 loss_ctc 7.509006 loss_rnnt 2.982914 hw_loss 0.174124 lr 0.00032379 rank 6
2023-02-25 13:08:33,373 DEBUG TRAIN Batch 28/5000 loss 10.649302 loss_att 12.725788 loss_ctc 16.271786 loss_rnnt 9.371216 hw_loss 0.212110 lr 0.00032372 rank 0
2023-02-25 13:08:33,374 DEBUG TRAIN Batch 28/5000 loss 4.708469 loss_att 6.650449 loss_ctc 5.670245 loss_rnnt 4.063396 hw_loss 0.240828 lr 0.00032378 rank 2
2023-02-25 13:08:33,376 DEBUG TRAIN Batch 28/5000 loss 10.364412 loss_att 13.734248 loss_ctc 15.650130 loss_rnnt 8.903704 hw_loss 0.153709 lr 0.00032381 rank 1
2023-02-25 13:08:33,376 DEBUG TRAIN Batch 28/5000 loss 6.244869 loss_att 8.502627 loss_ctc 8.966036 loss_rnnt 5.305158 hw_loss 0.235007 lr 0.00032376 rank 4
2023-02-25 13:08:33,385 DEBUG TRAIN Batch 28/5000 loss 9.741862 loss_att 11.782698 loss_ctc 14.998969 loss_rnnt 8.501519 hw_loss 0.246054 lr 0.00032376 rank 5
2023-02-25 13:09:43,799 DEBUG TRAIN Batch 28/5100 loss 18.566494 loss_att 20.218109 loss_ctc 19.811008 loss_rnnt 18.031813 hw_loss 0.072043 lr 0.00032374 rank 1
2023-02-25 13:09:43,800 DEBUG TRAIN Batch 28/5100 loss 7.568040 loss_att 7.999044 loss_ctc 10.897242 loss_rnnt 6.909235 hw_loss 0.241333 lr 0.00032370 rank 3
2023-02-25 13:09:43,800 DEBUG TRAIN Batch 28/5100 loss 9.292144 loss_att 11.477810 loss_ctc 11.373534 loss_rnnt 8.468360 hw_loss 0.204623 lr 0.00032372 rank 6
2023-02-25 13:09:43,801 DEBUG TRAIN Batch 28/5100 loss 5.451900 loss_att 9.357581 loss_ctc 10.252670 loss_rnnt 3.940538 hw_loss 0.168979 lr 0.00032365 rank 0
2023-02-25 13:09:43,803 DEBUG TRAIN Batch 28/5100 loss 7.245337 loss_att 9.022816 loss_ctc 10.092227 loss_rnnt 6.468066 hw_loss 0.079107 lr 0.00032372 rank 2
2023-02-25 13:09:43,803 DEBUG TRAIN Batch 28/5100 loss 8.640814 loss_att 11.832311 loss_ctc 9.825183 loss_rnnt 7.762774 hw_loss 0.153421 lr 0.00032366 rank 7
2023-02-25 13:09:43,808 DEBUG TRAIN Batch 28/5100 loss 6.390635 loss_att 9.678041 loss_ctc 9.577703 loss_rnnt 5.190568 hw_loss 0.220581 lr 0.00032369 rank 4
2023-02-25 13:09:43,811 DEBUG TRAIN Batch 28/5100 loss 7.297062 loss_att 7.799854 loss_ctc 10.158731 loss_rnnt 6.678360 hw_loss 0.256100 lr 0.00032369 rank 5
2023-02-25 13:10:53,686 DEBUG TRAIN Batch 28/5200 loss 8.824858 loss_att 11.886781 loss_ctc 11.581908 loss_rnnt 7.737309 hw_loss 0.201671 lr 0.00032359 rank 0
2023-02-25 13:10:53,687 DEBUG TRAIN Batch 28/5200 loss 2.742877 loss_att 6.785645 loss_ctc 4.262345 loss_rnnt 1.651511 hw_loss 0.150406 lr 0.00032366 rank 6
2023-02-25 13:10:53,691 DEBUG TRAIN Batch 28/5200 loss 6.529162 loss_att 9.245713 loss_ctc 14.279475 loss_rnnt 4.860517 hw_loss 0.172424 lr 0.00032364 rank 3
2023-02-25 13:10:53,691 DEBUG TRAIN Batch 28/5200 loss 2.290062 loss_att 3.349597 loss_ctc 2.619122 loss_rnnt 1.924620 hw_loss 0.205615 lr 0.00032365 rank 2
2023-02-25 13:10:53,691 DEBUG TRAIN Batch 28/5200 loss 13.895284 loss_att 19.211687 loss_ctc 26.914251 loss_rnnt 10.931406 hw_loss 0.308876 lr 0.00032368 rank 1
2023-02-25 13:10:53,693 DEBUG TRAIN Batch 28/5200 loss 6.121063 loss_att 7.731407 loss_ctc 10.852320 loss_rnnt 5.078663 hw_loss 0.167805 lr 0.00032360 rank 7
2023-02-25 13:10:53,697 DEBUG TRAIN Batch 28/5200 loss 3.331676 loss_att 5.185905 loss_ctc 6.820941 loss_rnnt 2.331316 hw_loss 0.308022 lr 0.00032363 rank 5
2023-02-25 13:10:53,744 DEBUG TRAIN Batch 28/5200 loss 5.284042 loss_att 9.290874 loss_ctc 10.807703 loss_rnnt 3.648090 hw_loss 0.183932 lr 0.00032362 rank 4
2023-02-25 13:12:06,027 DEBUG TRAIN Batch 28/5300 loss 19.164923 loss_att 19.106546 loss_ctc 24.375042 loss_rnnt 18.363319 hw_loss 0.222369 lr 0.00032355 rank 4
2023-02-25 13:12:06,038 DEBUG TRAIN Batch 28/5300 loss 5.919978 loss_att 8.333576 loss_ctc 8.172137 loss_rnnt 5.023827 hw_loss 0.212144 lr 0.00032361 rank 1
2023-02-25 13:12:06,038 DEBUG TRAIN Batch 28/5300 loss 8.897747 loss_att 11.949110 loss_ctc 11.913004 loss_rnnt 7.836212 hw_loss 0.092300 lr 0.00032352 rank 0
2023-02-25 13:12:06,039 DEBUG TRAIN Batch 28/5300 loss 11.482533 loss_att 14.797422 loss_ctc 21.498627 loss_rnnt 9.353677 hw_loss 0.244497 lr 0.00032359 rank 6
2023-02-25 13:12:06,043 DEBUG TRAIN Batch 28/5300 loss 8.156765 loss_att 10.769079 loss_ctc 15.165848 loss_rnnt 6.643554 hw_loss 0.105380 lr 0.00032357 rank 3
2023-02-25 13:12:06,043 DEBUG TRAIN Batch 28/5300 loss 7.984809 loss_att 9.872924 loss_ctc 8.739161 loss_rnnt 7.436116 hw_loss 0.132167 lr 0.00032358 rank 2
2023-02-25 13:12:06,045 DEBUG TRAIN Batch 28/5300 loss 14.408652 loss_att 17.650589 loss_ctc 21.575483 loss_rnnt 12.734428 hw_loss 0.131733 lr 0.00032353 rank 7
2023-02-25 13:12:06,052 DEBUG TRAIN Batch 28/5300 loss 7.902376 loss_att 12.577546 loss_ctc 14.161654 loss_rnnt 6.035596 hw_loss 0.182204 lr 0.00032356 rank 5
2023-02-25 13:13:17,979 DEBUG TRAIN Batch 28/5400 loss 7.003611 loss_att 10.513444 loss_ctc 8.846723 loss_rnnt 5.955787 hw_loss 0.187705 lr 0.00032354 rank 1
2023-02-25 13:13:17,979 DEBUG TRAIN Batch 28/5400 loss 8.080323 loss_att 11.245813 loss_ctc 13.025942 loss_rnnt 6.648770 hw_loss 0.260697 lr 0.00032348 rank 4
2023-02-25 13:13:17,979 DEBUG TRAIN Batch 28/5400 loss 7.721902 loss_att 11.097572 loss_ctc 12.244032 loss_rnnt 6.358072 hw_loss 0.160770 lr 0.00032346 rank 7
2023-02-25 13:13:17,980 DEBUG TRAIN Batch 28/5400 loss 4.020388 loss_att 9.548770 loss_ctc 6.600899 loss_rnnt 2.446644 hw_loss 0.232499 lr 0.00032352 rank 6
2023-02-25 13:13:17,980 DEBUG TRAIN Batch 28/5400 loss 8.251202 loss_att 10.326385 loss_ctc 10.368928 loss_rnnt 7.480918 hw_loss 0.136658 lr 0.00032345 rank 0
2023-02-25 13:13:17,981 DEBUG TRAIN Batch 28/5400 loss 7.292090 loss_att 8.929057 loss_ctc 8.811883 loss_rnnt 6.652682 hw_loss 0.205080 lr 0.00032351 rank 2
2023-02-25 13:13:17,987 DEBUG TRAIN Batch 28/5400 loss 9.549526 loss_att 11.284571 loss_ctc 14.696585 loss_rnnt 8.417046 hw_loss 0.185995 lr 0.00032350 rank 3
2023-02-25 13:13:17,988 DEBUG TRAIN Batch 28/5400 loss 6.309512 loss_att 9.817602 loss_ctc 14.431608 loss_rnnt 4.398903 hw_loss 0.236334 lr 0.00032349 rank 5
2023-02-25 13:14:27,498 DEBUG TRAIN Batch 28/5500 loss 3.778190 loss_att 5.586459 loss_ctc 4.906037 loss_rnnt 3.159688 hw_loss 0.199630 lr 0.00032345 rank 6
2023-02-25 13:14:27,507 DEBUG TRAIN Batch 28/5500 loss 10.176835 loss_att 12.780846 loss_ctc 14.778400 loss_rnnt 8.974214 hw_loss 0.128018 lr 0.00032342 rank 5
2023-02-25 13:14:27,509 DEBUG TRAIN Batch 28/5500 loss 5.993023 loss_att 8.771571 loss_ctc 12.793541 loss_rnnt 4.412026 hw_loss 0.222283 lr 0.00032338 rank 0
2023-02-25 13:14:27,509 DEBUG TRAIN Batch 28/5500 loss 5.177242 loss_att 9.906134 loss_ctc 10.759886 loss_rnnt 3.395253 hw_loss 0.172235 lr 0.00032347 rank 1
2023-02-25 13:14:27,513 DEBUG TRAIN Batch 28/5500 loss 7.252660 loss_att 10.005074 loss_ctc 11.439103 loss_rnnt 6.011754 hw_loss 0.247932 lr 0.00032342 rank 4
2023-02-25 13:14:27,513 DEBUG TRAIN Batch 28/5500 loss 7.546066 loss_att 10.574924 loss_ctc 11.058304 loss_rnnt 6.394444 hw_loss 0.145409 lr 0.00032339 rank 7
2023-02-25 13:14:27,555 DEBUG TRAIN Batch 28/5500 loss 5.387975 loss_att 9.113411 loss_ctc 7.635118 loss_rnnt 4.236167 hw_loss 0.200816 lr 0.00032344 rank 2
2023-02-25 13:14:27,579 DEBUG TRAIN Batch 28/5500 loss 13.913561 loss_att 14.990625 loss_ctc 18.996008 loss_rnnt 12.909601 hw_loss 0.207910 lr 0.00032343 rank 3
2023-02-25 13:15:38,489 DEBUG TRAIN Batch 28/5600 loss 6.556838 loss_att 10.473952 loss_ctc 11.381950 loss_rnnt 5.028846 hw_loss 0.189788 lr 0.00032339 rank 6
2023-02-25 13:15:38,489 DEBUG TRAIN Batch 28/5600 loss 7.954842 loss_att 9.904394 loss_ctc 9.927385 loss_rnnt 7.203157 hw_loss 0.185191 lr 0.00032337 rank 3
2023-02-25 13:15:38,492 DEBUG TRAIN Batch 28/5600 loss 5.269638 loss_att 8.666823 loss_ctc 9.819505 loss_rnnt 3.880472 hw_loss 0.193275 lr 0.00032336 rank 5
2023-02-25 13:15:38,493 DEBUG TRAIN Batch 28/5600 loss 9.625675 loss_att 12.474642 loss_ctc 14.951377 loss_rnnt 8.252641 hw_loss 0.174651 lr 0.00032335 rank 4
2023-02-25 13:15:38,494 DEBUG TRAIN Batch 28/5600 loss 7.436852 loss_att 10.819792 loss_ctc 11.186065 loss_rnnt 6.103646 hw_loss 0.293854 lr 0.00032332 rank 7
2023-02-25 13:15:38,495 DEBUG TRAIN Batch 28/5600 loss 12.675063 loss_att 13.421719 loss_ctc 18.222702 loss_rnnt 11.713805 hw_loss 0.135453 lr 0.00032331 rank 0
2023-02-25 13:15:38,500 DEBUG TRAIN Batch 28/5600 loss 4.125537 loss_att 7.613039 loss_ctc 6.181499 loss_rnnt 3.013436 hw_loss 0.263384 lr 0.00032341 rank 1
2023-02-25 13:15:38,531 DEBUG TRAIN Batch 28/5600 loss 9.816607 loss_att 9.837843 loss_ctc 13.919269 loss_rnnt 9.098117 hw_loss 0.313540 lr 0.00032338 rank 2
2023-02-25 13:16:52,223 DEBUG TRAIN Batch 28/5700 loss 5.553894 loss_att 7.619651 loss_ctc 9.948424 loss_rnnt 4.482311 hw_loss 0.135926 lr 0.00032332 rank 6
2023-02-25 13:16:52,227 DEBUG TRAIN Batch 28/5700 loss 5.910558 loss_att 10.341671 loss_ctc 10.224903 loss_rnnt 4.334436 hw_loss 0.214974 lr 0.00032325 rank 0
2023-02-25 13:16:52,227 DEBUG TRAIN Batch 28/5700 loss 11.194235 loss_att 14.590451 loss_ctc 17.899231 loss_rnnt 9.491549 hw_loss 0.242705 lr 0.00032330 rank 3
2023-02-25 13:16:52,229 DEBUG TRAIN Batch 28/5700 loss 6.576156 loss_att 7.524012 loss_ctc 9.011468 loss_rnnt 5.921572 hw_loss 0.263068 lr 0.00032334 rank 1
2023-02-25 13:16:52,231 DEBUG TRAIN Batch 28/5700 loss 8.082897 loss_att 10.792227 loss_ctc 14.640829 loss_rnnt 6.566401 hw_loss 0.187949 lr 0.00032329 rank 5
2023-02-25 13:16:52,235 DEBUG TRAIN Batch 28/5700 loss 8.059878 loss_att 9.241844 loss_ctc 9.019182 loss_rnnt 7.602405 hw_loss 0.174699 lr 0.00032331 rank 2
2023-02-25 13:16:52,254 DEBUG TRAIN Batch 28/5700 loss 8.220426 loss_att 11.748684 loss_ctc 11.740657 loss_rnnt 6.984385 hw_loss 0.114423 lr 0.00032328 rank 4
2023-02-25 13:16:52,261 DEBUG TRAIN Batch 28/5700 loss 10.114392 loss_att 12.186262 loss_ctc 16.632448 loss_rnnt 8.670141 hw_loss 0.301506 lr 0.00032326 rank 7
2023-02-25 13:18:02,982 DEBUG TRAIN Batch 28/5800 loss 7.411245 loss_att 10.251500 loss_ctc 11.898642 loss_rnnt 6.173493 hw_loss 0.133840 lr 0.00032327 rank 1
2023-02-25 13:18:02,983 DEBUG TRAIN Batch 28/5800 loss 3.497885 loss_att 5.735578 loss_ctc 4.639436 loss_rnnt 2.810294 hw_loss 0.164711 lr 0.00032318 rank 0
2023-02-25 13:18:02,984 DEBUG TRAIN Batch 28/5800 loss 13.896795 loss_att 17.370289 loss_ctc 20.004990 loss_rnnt 12.321365 hw_loss 0.124322 lr 0.00032323 rank 3
2023-02-25 13:18:02,987 DEBUG TRAIN Batch 28/5800 loss 6.185570 loss_att 7.028562 loss_ctc 7.263838 loss_rnnt 5.788423 hw_loss 0.158962 lr 0.00032324 rank 2
2023-02-25 13:18:02,986 DEBUG TRAIN Batch 28/5800 loss 4.838334 loss_att 5.137085 loss_ctc 7.181948 loss_rnnt 4.305910 hw_loss 0.300359 lr 0.00032325 rank 6
2023-02-25 13:18:02,988 DEBUG TRAIN Batch 28/5800 loss 6.416549 loss_att 8.180145 loss_ctc 6.281231 loss_rnnt 6.023891 hw_loss 0.108713 lr 0.00032321 rank 4
2023-02-25 13:18:02,996 DEBUG TRAIN Batch 28/5800 loss 3.910103 loss_att 7.715848 loss_ctc 5.808355 loss_rnnt 2.805781 hw_loss 0.168885 lr 0.00032319 rank 7
2023-02-25 13:18:03,034 DEBUG TRAIN Batch 28/5800 loss 4.116942 loss_att 6.043585 loss_ctc 6.113159 loss_rnnt 3.370102 hw_loss 0.178779 lr 0.00032322 rank 5
2023-02-25 13:19:14,147 DEBUG TRAIN Batch 28/5900 loss 15.161099 loss_att 20.024923 loss_ctc 25.788433 loss_rnnt 12.668743 hw_loss 0.192402 lr 0.00032318 rank 6
2023-02-25 13:19:14,148 DEBUG TRAIN Batch 28/5900 loss 8.660256 loss_att 10.926627 loss_ctc 9.247622 loss_rnnt 8.042126 hw_loss 0.162265 lr 0.00032320 rank 1
2023-02-25 13:19:14,150 DEBUG TRAIN Batch 28/5900 loss 5.438184 loss_att 6.850934 loss_ctc 7.268836 loss_rnnt 4.808176 hw_loss 0.193820 lr 0.00032311 rank 0
2023-02-25 13:19:14,153 DEBUG TRAIN Batch 28/5900 loss 1.839807 loss_att 4.034055 loss_ctc 3.779294 loss_rnnt 1.070468 hw_loss 0.134796 lr 0.00032316 rank 3
2023-02-25 13:19:14,157 DEBUG TRAIN Batch 28/5900 loss 1.934012 loss_att 3.299439 loss_ctc 2.521921 loss_rnnt 1.478887 hw_loss 0.194346 lr 0.00032315 rank 5
2023-02-25 13:19:14,157 DEBUG TRAIN Batch 28/5900 loss 8.581590 loss_att 9.984368 loss_ctc 12.413157 loss_rnnt 7.708744 hw_loss 0.152653 lr 0.00032315 rank 4
2023-02-25 13:19:14,157 DEBUG TRAIN Batch 28/5900 loss 9.515605 loss_att 12.533936 loss_ctc 13.711523 loss_rnnt 8.273934 hw_loss 0.147278 lr 0.00032312 rank 7
2023-02-25 13:19:14,160 DEBUG TRAIN Batch 28/5900 loss 9.392444 loss_att 12.866884 loss_ctc 13.386156 loss_rnnt 8.074815 hw_loss 0.169210 lr 0.00032317 rank 2
2023-02-25 13:20:25,671 DEBUG TRAIN Batch 28/6000 loss 20.946571 loss_att 19.584152 loss_ctc 26.516506 loss_rnnt 20.359238 hw_loss 0.219669 lr 0.00032305 rank 7
2023-02-25 13:20:25,681 DEBUG TRAIN Batch 28/6000 loss 4.515950 loss_att 7.519974 loss_ctc 7.006220 loss_rnnt 3.502385 hw_loss 0.151359 lr 0.00032312 rank 6
2023-02-25 13:20:25,683 DEBUG TRAIN Batch 28/6000 loss 9.005178 loss_att 10.663844 loss_ctc 12.984692 loss_rnnt 8.062428 hw_loss 0.150782 lr 0.00032311 rank 2
2023-02-25 13:20:25,684 DEBUG TRAIN Batch 28/6000 loss 5.971839 loss_att 8.000035 loss_ctc 7.433122 loss_rnnt 5.266615 hw_loss 0.196401 lr 0.00032314 rank 1
2023-02-25 13:20:25,685 DEBUG TRAIN Batch 28/6000 loss 5.251185 loss_att 7.119025 loss_ctc 10.934223 loss_rnnt 4.049579 hw_loss 0.131812 lr 0.00032304 rank 0
2023-02-25 13:20:25,686 DEBUG TRAIN Batch 28/6000 loss 2.106723 loss_att 4.430138 loss_ctc 3.225845 loss_rnnt 1.431705 hw_loss 0.114597 lr 0.00032310 rank 3
2023-02-25 13:20:25,686 DEBUG TRAIN Batch 28/6000 loss 10.279193 loss_att 13.408713 loss_ctc 16.646072 loss_rnnt 8.672260 hw_loss 0.247710 lr 0.00032309 rank 5
2023-02-25 13:20:25,689 DEBUG TRAIN Batch 28/6000 loss 6.474800 loss_att 7.855978 loss_ctc 7.962059 loss_rnnt 5.897146 hw_loss 0.193345 lr 0.00032308 rank 4
2023-02-25 13:21:38,568 DEBUG TRAIN Batch 28/6100 loss 5.264546 loss_att 8.536056 loss_ctc 7.021934 loss_rnnt 4.271974 hw_loss 0.194911 lr 0.00032298 rank 0
2023-02-25 13:21:38,572 DEBUG TRAIN Batch 28/6100 loss 4.373727 loss_att 5.994983 loss_ctc 5.782340 loss_rnnt 3.792163 hw_loss 0.130309 lr 0.00032303 rank 3
2023-02-25 13:21:38,572 DEBUG TRAIN Batch 28/6100 loss 6.758891 loss_att 8.986484 loss_ctc 8.708529 loss_rnnt 5.958395 hw_loss 0.178171 lr 0.00032307 rank 1
2023-02-25 13:21:38,573 DEBUG TRAIN Batch 28/6100 loss 10.727987 loss_att 11.121156 loss_ctc 11.215475 loss_rnnt 10.479500 hw_loss 0.196603 lr 0.00032305 rank 6
2023-02-25 13:21:38,577 DEBUG TRAIN Batch 28/6100 loss 9.521054 loss_att 10.972345 loss_ctc 17.324812 loss_rnnt 8.109246 hw_loss 0.151966 lr 0.00032302 rank 5
2023-02-25 13:21:38,577 DEBUG TRAIN Batch 28/6100 loss 3.700542 loss_att 5.011123 loss_ctc 4.870148 loss_rnnt 3.197095 hw_loss 0.160093 lr 0.00032304 rank 2
2023-02-25 13:21:38,577 DEBUG TRAIN Batch 28/6100 loss 7.094083 loss_att 10.700928 loss_ctc 10.518406 loss_rnnt 5.831131 hw_loss 0.159387 lr 0.00032299 rank 7
2023-02-25 13:21:38,582 DEBUG TRAIN Batch 28/6100 loss 4.076798 loss_att 6.334191 loss_ctc 5.170738 loss_rnnt 3.400043 hw_loss 0.148908 lr 0.00032301 rank 4
2023-02-25 13:22:48,920 DEBUG TRAIN Batch 28/6200 loss 8.525400 loss_att 11.464451 loss_ctc 14.296473 loss_rnnt 7.062165 hw_loss 0.198653 lr 0.00032291 rank 0
2023-02-25 13:22:48,928 DEBUG TRAIN Batch 28/6200 loss 11.919194 loss_att 15.531887 loss_ctc 16.842424 loss_rnnt 10.456285 hw_loss 0.157384 lr 0.00032300 rank 1
2023-02-25 13:22:48,928 DEBUG TRAIN Batch 28/6200 loss 3.313730 loss_att 4.710217 loss_ctc 5.251606 loss_rnnt 2.674984 hw_loss 0.189499 lr 0.00032292 rank 7
2023-02-25 13:22:48,930 DEBUG TRAIN Batch 28/6200 loss 15.735878 loss_att 17.099812 loss_ctc 18.413177 loss_rnnt 15.015819 hw_loss 0.169310 lr 0.00032297 rank 2
2023-02-25 13:22:48,932 DEBUG TRAIN Batch 28/6200 loss 2.638318 loss_att 6.077445 loss_ctc 5.085128 loss_rnnt 1.509259 hw_loss 0.215609 lr 0.00032296 rank 3
2023-02-25 13:22:48,935 DEBUG TRAIN Batch 28/6200 loss 6.798439 loss_att 8.270997 loss_ctc 9.541718 loss_rnnt 6.074515 hw_loss 0.119327 lr 0.00032294 rank 4
2023-02-25 13:22:48,938 DEBUG TRAIN Batch 28/6200 loss 7.235906 loss_att 8.465155 loss_ctc 9.359062 loss_rnnt 6.587593 hw_loss 0.223828 lr 0.00032298 rank 6
2023-02-25 13:22:48,993 DEBUG TRAIN Batch 28/6200 loss 6.107051 loss_att 9.040091 loss_ctc 10.182255 loss_rnnt 4.920918 hw_loss 0.105310 lr 0.00032295 rank 5
2023-02-25 13:24:00,183 DEBUG TRAIN Batch 28/6300 loss 8.252698 loss_att 9.121893 loss_ctc 12.030910 loss_rnnt 7.456346 hw_loss 0.222660 lr 0.00032284 rank 0
2023-02-25 13:24:00,183 DEBUG TRAIN Batch 28/6300 loss 9.042517 loss_att 12.050409 loss_ctc 12.357609 loss_rnnt 7.871999 hw_loss 0.237987 lr 0.00032289 rank 3
2023-02-25 13:24:00,186 DEBUG TRAIN Batch 28/6300 loss 3.627129 loss_att 6.274590 loss_ctc 5.039429 loss_rnnt 2.794821 hw_loss 0.214704 lr 0.00032293 rank 1
2023-02-25 13:24:00,187 DEBUG TRAIN Batch 28/6300 loss 3.776307 loss_att 4.907977 loss_ctc 4.491561 loss_rnnt 3.391798 hw_loss 0.117765 lr 0.00032291 rank 6
2023-02-25 13:24:00,188 DEBUG TRAIN Batch 28/6300 loss 8.433770 loss_att 9.786816 loss_ctc 13.107039 loss_rnnt 7.417701 hw_loss 0.229420 lr 0.00032288 rank 4
2023-02-25 13:24:00,191 DEBUG TRAIN Batch 28/6300 loss 14.575391 loss_att 18.532749 loss_ctc 23.522186 loss_rnnt 12.519217 hw_loss 0.134617 lr 0.00032290 rank 2
2023-02-25 13:24:00,193 DEBUG TRAIN Batch 28/6300 loss 8.576417 loss_att 10.818540 loss_ctc 13.055239 loss_rnnt 7.388909 hw_loss 0.266075 lr 0.00032288 rank 5
2023-02-25 13:24:00,199 DEBUG TRAIN Batch 28/6300 loss 9.622035 loss_att 8.742897 loss_ctc 13.487427 loss_rnnt 9.142141 hw_loss 0.263128 lr 0.00032285 rank 7
2023-02-25 13:25:13,296 DEBUG TRAIN Batch 28/6400 loss 8.049446 loss_att 11.340519 loss_ctc 15.171143 loss_rnnt 6.295366 hw_loss 0.274324 lr 0.00032278 rank 0
2023-02-25 13:25:13,299 DEBUG TRAIN Batch 28/6400 loss 7.657146 loss_att 11.500688 loss_ctc 9.862650 loss_rnnt 6.522974 hw_loss 0.133867 lr 0.00032278 rank 7
2023-02-25 13:25:13,300 DEBUG TRAIN Batch 28/6400 loss 6.700723 loss_att 8.934284 loss_ctc 11.547136 loss_rnnt 5.537252 hw_loss 0.132320 lr 0.00032284 rank 2
2023-02-25 13:25:13,301 DEBUG TRAIN Batch 28/6400 loss 5.500894 loss_att 5.750204 loss_ctc 7.725451 loss_rnnt 4.986365 hw_loss 0.315111 lr 0.00032282 rank 5
2023-02-25 13:25:13,302 DEBUG TRAIN Batch 28/6400 loss 5.899435 loss_att 9.187439 loss_ctc 9.639522 loss_rnnt 4.673963 hw_loss 0.129737 lr 0.00032287 rank 1
2023-02-25 13:25:13,306 DEBUG TRAIN Batch 28/6400 loss 5.483027 loss_att 6.943629 loss_ctc 7.414957 loss_rnnt 4.821080 hw_loss 0.210442 lr 0.00032283 rank 3
2023-02-25 13:25:13,309 DEBUG TRAIN Batch 28/6400 loss 8.396643 loss_att 12.517517 loss_ctc 15.870166 loss_rnnt 6.480953 hw_loss 0.178210 lr 0.00032281 rank 4
2023-02-25 13:25:13,348 DEBUG TRAIN Batch 28/6400 loss 27.128897 loss_att 28.008665 loss_ctc 35.596985 loss_rnnt 25.651218 hw_loss 0.323711 lr 0.00032285 rank 6
2023-02-25 13:26:24,712 DEBUG TRAIN Batch 28/6500 loss 2.735197 loss_att 5.996298 loss_ctc 4.939800 loss_rnnt 1.684089 hw_loss 0.196765 lr 0.00032271 rank 0
2023-02-25 13:26:24,714 DEBUG TRAIN Batch 28/6500 loss 5.155392 loss_att 8.409496 loss_ctc 8.346701 loss_rnnt 4.022030 hw_loss 0.106936 lr 0.00032275 rank 5
2023-02-25 13:26:24,716 DEBUG TRAIN Batch 28/6500 loss 9.572544 loss_att 15.762266 loss_ctc 12.612700 loss_rnnt 7.896890 hw_loss 0.060666 lr 0.00032278 rank 6
2023-02-25 13:26:24,719 DEBUG TRAIN Batch 28/6500 loss 6.282803 loss_att 7.749938 loss_ctc 8.457445 loss_rnnt 5.611088 hw_loss 0.165629 lr 0.00032276 rank 3
2023-02-25 13:26:24,719 DEBUG TRAIN Batch 28/6500 loss 3.118142 loss_att 5.177995 loss_ctc 4.433137 loss_rnnt 2.424192 hw_loss 0.199963 lr 0.00032280 rank 1
2023-02-25 13:26:24,721 DEBUG TRAIN Batch 28/6500 loss 8.114287 loss_att 9.898490 loss_ctc 12.268284 loss_rnnt 7.109951 hw_loss 0.175557 lr 0.00032277 rank 2
2023-02-25 13:26:24,723 DEBUG TRAIN Batch 28/6500 loss 3.393315 loss_att 5.087513 loss_ctc 4.635307 loss_rnnt 2.827786 hw_loss 0.114545 lr 0.00032274 rank 4
2023-02-25 13:26:24,724 DEBUG TRAIN Batch 28/6500 loss 13.514140 loss_att 18.475193 loss_ctc 25.749092 loss_rnnt 10.819645 hw_loss 0.133045 lr 0.00032272 rank 7
2023-02-25 13:27:34,511 DEBUG TRAIN Batch 28/6600 loss 6.010897 loss_att 7.442134 loss_ctc 9.690456 loss_rnnt 5.150527 hw_loss 0.156590 lr 0.00032264 rank 0
2023-02-25 13:27:34,513 DEBUG TRAIN Batch 28/6600 loss 3.516285 loss_att 8.594521 loss_ctc 6.666870 loss_rnnt 2.023751 hw_loss 0.106516 lr 0.00032265 rank 7
2023-02-25 13:27:34,514 DEBUG TRAIN Batch 28/6600 loss 5.464982 loss_att 7.513461 loss_ctc 6.129921 loss_rnnt 4.891243 hw_loss 0.141345 lr 0.00032267 rank 4
2023-02-25 13:27:34,515 DEBUG TRAIN Batch 28/6600 loss 8.807143 loss_att 12.115964 loss_ctc 15.205863 loss_rnnt 7.179292 hw_loss 0.211733 lr 0.00032271 rank 6
2023-02-25 13:27:34,517 DEBUG TRAIN Batch 28/6600 loss 2.475740 loss_att 4.487930 loss_ctc 3.483529 loss_rnnt 1.864823 hw_loss 0.138950 lr 0.00032273 rank 1
2023-02-25 13:27:34,518 DEBUG TRAIN Batch 28/6600 loss 3.670977 loss_att 5.216498 loss_ctc 4.385068 loss_rnnt 3.180547 hw_loss 0.161463 lr 0.00032270 rank 2
2023-02-25 13:27:34,518 DEBUG TRAIN Batch 28/6600 loss 7.563624 loss_att 9.152283 loss_ctc 7.676125 loss_rnnt 7.180562 hw_loss 0.094369 lr 0.00032269 rank 3
2023-02-25 13:27:34,518 DEBUG TRAIN Batch 28/6600 loss 8.064682 loss_att 11.573066 loss_ctc 13.522010 loss_rnnt 6.539335 hw_loss 0.180048 lr 0.00032268 rank 5
2023-02-25 13:28:45,389 DEBUG TRAIN Batch 28/6700 loss 6.971856 loss_att 10.541515 loss_ctc 13.377418 loss_rnnt 5.264600 hw_loss 0.261092 lr 0.00032261 rank 4
2023-02-25 13:28:45,401 DEBUG TRAIN Batch 28/6700 loss 5.312659 loss_att 5.921826 loss_ctc 6.414123 loss_rnnt 4.940534 hw_loss 0.193931 lr 0.00032264 rank 6
2023-02-25 13:28:45,401 DEBUG TRAIN Batch 28/6700 loss 6.919948 loss_att 8.669134 loss_ctc 8.711253 loss_rnnt 6.253304 hw_loss 0.146187 lr 0.00032262 rank 3
2023-02-25 13:28:45,403 DEBUG TRAIN Batch 28/6700 loss 7.711802 loss_att 10.822628 loss_ctc 11.213097 loss_rnnt 6.470014 hw_loss 0.286467 lr 0.00032257 rank 0
2023-02-25 13:28:45,402 DEBUG TRAIN Batch 28/6700 loss 10.777899 loss_att 14.033604 loss_ctc 20.121666 loss_rnnt 8.753641 hw_loss 0.238652 lr 0.00032264 rank 2
2023-02-25 13:28:45,408 DEBUG TRAIN Batch 28/6700 loss 9.632358 loss_att 11.833212 loss_ctc 12.054987 loss_rnnt 8.786947 hw_loss 0.154166 lr 0.00032258 rank 7
2023-02-25 13:28:45,410 DEBUG TRAIN Batch 28/6700 loss 4.747148 loss_att 7.887578 loss_ctc 7.812088 loss_rnnt 3.583254 hw_loss 0.238403 lr 0.00032266 rank 1
2023-02-25 13:28:45,417 DEBUG TRAIN Batch 28/6700 loss 10.047049 loss_att 12.732878 loss_ctc 13.426999 loss_rnnt 8.909838 hw_loss 0.280096 lr 0.00032261 rank 5
2023-02-25 13:29:57,672 DEBUG TRAIN Batch 28/6800 loss 10.363502 loss_att 14.374397 loss_ctc 19.756613 loss_rnnt 8.169953 hw_loss 0.260538 lr 0.00032251 rank 0
2023-02-25 13:29:57,672 DEBUG TRAIN Batch 28/6800 loss 10.396399 loss_att 12.650864 loss_ctc 13.123128 loss_rnnt 9.470267 hw_loss 0.209390 lr 0.00032252 rank 7
2023-02-25 13:29:57,673 DEBUG TRAIN Batch 28/6800 loss 9.405813 loss_att 9.362529 loss_ctc 10.897631 loss_rnnt 9.120195 hw_loss 0.178808 lr 0.00032256 rank 3
2023-02-25 13:29:57,674 DEBUG TRAIN Batch 28/6800 loss 4.756175 loss_att 9.336434 loss_ctc 8.306285 loss_rnnt 3.275209 hw_loss 0.171688 lr 0.00032257 rank 2
2023-02-25 13:29:57,675 DEBUG TRAIN Batch 28/6800 loss 5.178203 loss_att 8.323658 loss_ctc 12.159809 loss_rnnt 3.524446 hw_loss 0.175847 lr 0.00032258 rank 6
2023-02-25 13:29:57,678 DEBUG TRAIN Batch 28/6800 loss 6.444286 loss_att 8.902926 loss_ctc 8.917020 loss_rnnt 5.497321 hw_loss 0.235385 lr 0.00032260 rank 1
2023-02-25 13:29:57,680 DEBUG TRAIN Batch 28/6800 loss 15.169403 loss_att 15.358276 loss_ctc 22.281557 loss_rnnt 14.077101 hw_loss 0.199201 lr 0.00032254 rank 4
2023-02-25 13:29:57,684 DEBUG TRAIN Batch 28/6800 loss 14.354964 loss_att 16.286747 loss_ctc 18.001535 loss_rnnt 13.376861 hw_loss 0.197883 lr 0.00032255 rank 5
2023-02-25 13:31:08,197 DEBUG TRAIN Batch 28/6900 loss 8.864478 loss_att 8.872452 loss_ctc 12.444946 loss_rnnt 8.231198 hw_loss 0.289293 lr 0.00032244 rank 0
2023-02-25 13:31:08,200 DEBUG TRAIN Batch 28/6900 loss 5.053217 loss_att 8.396278 loss_ctc 9.130132 loss_rnnt 3.715252 hw_loss 0.235809 lr 0.00032253 rank 1
2023-02-25 13:31:08,201 DEBUG TRAIN Batch 28/6900 loss 6.871227 loss_att 8.028856 loss_ctc 8.092808 loss_rnnt 6.366070 hw_loss 0.207663 lr 0.00032247 rank 4
2023-02-25 13:31:08,204 DEBUG TRAIN Batch 28/6900 loss 6.302195 loss_att 8.882803 loss_ctc 8.092010 loss_rnnt 5.453557 hw_loss 0.176015 lr 0.00032251 rank 6
2023-02-25 13:31:08,206 DEBUG TRAIN Batch 28/6900 loss 10.766680 loss_att 11.745304 loss_ctc 15.159343 loss_rnnt 9.879339 hw_loss 0.198611 lr 0.00032245 rank 7
2023-02-25 13:31:08,206 DEBUG TRAIN Batch 28/6900 loss 7.658465 loss_att 9.919443 loss_ctc 10.172673 loss_rnnt 6.774175 hw_loss 0.181626 lr 0.00032248 rank 5
2023-02-25 13:31:08,207 DEBUG TRAIN Batch 28/6900 loss 14.361600 loss_att 16.875366 loss_ctc 23.097080 loss_rnnt 12.578262 hw_loss 0.217225 lr 0.00032249 rank 3
2023-02-25 13:31:08,209 DEBUG TRAIN Batch 28/6900 loss 5.221974 loss_att 8.196468 loss_ctc 7.212623 loss_rnnt 4.210954 hw_loss 0.282566 lr 0.00032250 rank 2
2023-02-25 13:32:18,478 DEBUG TRAIN Batch 28/7000 loss 5.773921 loss_att 8.588312 loss_ctc 9.199375 loss_rnnt 4.685101 hw_loss 0.129775 lr 0.00032237 rank 0
2023-02-25 13:32:18,480 DEBUG TRAIN Batch 28/7000 loss 5.919707 loss_att 9.087152 loss_ctc 10.883150 loss_rnnt 4.549532 hw_loss 0.140424 lr 0.00032244 rank 6
2023-02-25 13:32:18,482 DEBUG TRAIN Batch 28/7000 loss 20.402037 loss_att 27.374943 loss_ctc 34.543720 loss_rnnt 16.956177 hw_loss 0.310727 lr 0.00032238 rank 7
2023-02-25 13:32:18,483 DEBUG TRAIN Batch 28/7000 loss 7.011043 loss_att 7.718642 loss_ctc 11.669224 loss_rnnt 6.144996 hw_loss 0.193943 lr 0.00032241 rank 5
2023-02-25 13:32:18,485 DEBUG TRAIN Batch 28/7000 loss 10.665998 loss_att 13.078764 loss_ctc 16.473982 loss_rnnt 9.299277 hw_loss 0.205821 lr 0.00032242 rank 3
2023-02-25 13:32:18,485 DEBUG TRAIN Batch 28/7000 loss 5.400387 loss_att 7.890575 loss_ctc 6.934400 loss_rnnt 4.636106 hw_loss 0.115704 lr 0.00032241 rank 4
2023-02-25 13:32:18,487 DEBUG TRAIN Batch 28/7000 loss 6.228062 loss_att 6.354872 loss_ctc 9.023621 loss_rnnt 5.698660 hw_loss 0.246184 lr 0.00032246 rank 1
2023-02-25 13:32:18,487 DEBUG TRAIN Batch 28/7000 loss 3.598177 loss_att 6.392310 loss_ctc 4.311559 loss_rnnt 2.844420 hw_loss 0.187148 lr 0.00032243 rank 2
2023-02-25 13:33:31,694 DEBUG TRAIN Batch 28/7100 loss 11.060196 loss_att 13.137927 loss_ctc 12.827692 loss_rnnt 10.253292 hw_loss 0.291922 lr 0.00032238 rank 6
2023-02-25 13:33:31,696 DEBUG TRAIN Batch 28/7100 loss 5.773380 loss_att 7.398839 loss_ctc 11.104574 loss_rnnt 4.660238 hw_loss 0.144795 lr 0.00032231 rank 0
2023-02-25 13:33:31,700 DEBUG TRAIN Batch 28/7100 loss 11.076937 loss_att 12.920044 loss_ctc 11.104793 loss_rnnt 10.607745 hw_loss 0.181607 lr 0.00032232 rank 7
2023-02-25 13:33:31,701 DEBUG TRAIN Batch 28/7100 loss 5.997257 loss_att 11.564560 loss_ctc 12.984240 loss_rnnt 3.886992 hw_loss 0.122262 lr 0.00032236 rank 3
2023-02-25 13:33:31,702 DEBUG TRAIN Batch 28/7100 loss 3.900352 loss_att 6.206992 loss_ctc 10.219324 loss_rnnt 2.496868 hw_loss 0.186801 lr 0.00032240 rank 1
2023-02-25 13:33:31,707 DEBUG TRAIN Batch 28/7100 loss 1.711535 loss_att 4.146049 loss_ctc 3.597148 loss_rnnt 0.901100 hw_loss 0.135219 lr 0.00032237 rank 2
2023-02-25 13:33:31,721 DEBUG TRAIN Batch 28/7100 loss 4.766655 loss_att 8.606955 loss_ctc 7.600856 loss_rnnt 3.541686 hw_loss 0.148154 lr 0.00032234 rank 4
2023-02-25 13:33:31,731 DEBUG TRAIN Batch 28/7100 loss 16.018860 loss_att 18.451380 loss_ctc 26.544624 loss_rnnt 14.051445 hw_loss 0.145266 lr 0.00032235 rank 5
2023-02-25 13:34:42,870 DEBUG TRAIN Batch 28/7200 loss 7.193273 loss_att 10.167833 loss_ctc 11.259707 loss_rnnt 5.942688 hw_loss 0.212777 lr 0.00032233 rank 1
2023-02-25 13:34:42,871 DEBUG TRAIN Batch 28/7200 loss 11.995126 loss_att 14.285200 loss_ctc 17.741344 loss_rnnt 10.636337 hw_loss 0.252394 lr 0.00032224 rank 0
2023-02-25 13:34:42,873 DEBUG TRAIN Batch 28/7200 loss 5.388719 loss_att 7.782410 loss_ctc 6.482978 loss_rnnt 4.665315 hw_loss 0.185183 lr 0.00032228 rank 5
2023-02-25 13:34:42,874 DEBUG TRAIN Batch 28/7200 loss 17.665831 loss_att 20.174026 loss_ctc 28.699886 loss_rnnt 15.589439 hw_loss 0.194144 lr 0.00032231 rank 6
2023-02-25 13:34:42,876 DEBUG TRAIN Batch 28/7200 loss 5.438367 loss_att 9.639647 loss_ctc 9.178543 loss_rnnt 4.005126 hw_loss 0.176802 lr 0.00032229 rank 3
2023-02-25 13:34:42,876 DEBUG TRAIN Batch 28/7200 loss 5.063352 loss_att 7.627131 loss_ctc 10.763099 loss_rnnt 3.689963 hw_loss 0.188750 lr 0.00032225 rank 7
2023-02-25 13:34:42,879 DEBUG TRAIN Batch 28/7200 loss 6.195858 loss_att 10.738830 loss_ctc 11.050045 loss_rnnt 4.540534 hw_loss 0.186572 lr 0.00032230 rank 2
2023-02-25 13:34:42,881 DEBUG TRAIN Batch 28/7200 loss 12.552374 loss_att 17.435219 loss_ctc 16.162024 loss_rnnt 11.020778 hw_loss 0.138263 lr 0.00032227 rank 4
2023-02-25 13:35:52,196 DEBUG TRAIN Batch 28/7300 loss 3.907966 loss_att 7.667616 loss_ctc 4.849291 loss_rnnt 2.938069 hw_loss 0.173356 lr 0.00032217 rank 0
2023-02-25 13:35:52,197 DEBUG TRAIN Batch 28/7300 loss 7.179459 loss_att 12.958446 loss_ctc 10.897289 loss_rnnt 5.394992 hw_loss 0.249298 lr 0.00032224 rank 6
2023-02-25 13:35:52,200 DEBUG TRAIN Batch 28/7300 loss 11.235370 loss_att 12.963723 loss_ctc 15.509241 loss_rnnt 10.180509 hw_loss 0.261264 lr 0.00032218 rank 7
2023-02-25 13:35:52,202 DEBUG TRAIN Batch 28/7300 loss 5.185139 loss_att 8.654770 loss_ctc 9.495034 loss_rnnt 3.807470 hw_loss 0.204543 lr 0.00032221 rank 4
2023-02-25 13:35:52,203 DEBUG TRAIN Batch 28/7300 loss 9.535764 loss_att 13.630217 loss_ctc 13.030282 loss_rnnt 8.145724 hw_loss 0.197274 lr 0.00032223 rank 2
2023-02-25 13:35:52,208 DEBUG TRAIN Batch 28/7300 loss 4.234281 loss_att 5.958037 loss_ctc 3.813870 loss_rnnt 3.842247 hw_loss 0.193759 lr 0.00032226 rank 1
2023-02-25 13:35:52,208 DEBUG TRAIN Batch 28/7300 loss 6.626392 loss_att 8.852271 loss_ctc 10.155679 loss_rnnt 5.555981 hw_loss 0.289994 lr 0.00032221 rank 5
2023-02-25 13:35:52,213 DEBUG TRAIN Batch 28/7300 loss 7.234752 loss_att 10.322042 loss_ctc 10.331343 loss_rnnt 6.111828 hw_loss 0.173601 lr 0.00032222 rank 3
2023-02-25 13:37:03,212 DEBUG TRAIN Batch 28/7400 loss 9.389146 loss_att 11.430277 loss_ctc 13.965656 loss_rnnt 8.284541 hw_loss 0.161583 lr 0.00032211 rank 0
2023-02-25 13:37:03,214 DEBUG TRAIN Batch 28/7400 loss 10.815592 loss_att 14.816722 loss_ctc 16.231428 loss_rnnt 9.207037 hw_loss 0.161658 lr 0.00032217 rank 6
2023-02-25 13:37:03,215 DEBUG TRAIN Batch 28/7400 loss 11.954824 loss_att 15.775537 loss_ctc 16.104614 loss_rnnt 10.539759 hw_loss 0.183034 lr 0.00032216 rank 3
2023-02-25 13:37:03,214 DEBUG TRAIN Batch 28/7400 loss 8.322949 loss_att 11.655170 loss_ctc 18.351669 loss_rnnt 6.191600 hw_loss 0.239517 lr 0.00032214 rank 4
2023-02-25 13:37:03,216 DEBUG TRAIN Batch 28/7400 loss 9.489421 loss_att 11.843405 loss_ctc 10.693027 loss_rnnt 8.771794 hw_loss 0.161903 lr 0.00032215 rank 5
2023-02-25 13:37:03,218 DEBUG TRAIN Batch 28/7400 loss 10.076626 loss_att 11.507291 loss_ctc 19.828257 loss_rnnt 8.379472 hw_loss 0.207758 lr 0.00032217 rank 2
2023-02-25 13:37:03,247 DEBUG TRAIN Batch 28/7400 loss 9.234141 loss_att 9.345709 loss_ctc 14.772272 loss_rnnt 8.356092 hw_loss 0.219972 lr 0.00032211 rank 7
2023-02-25 13:37:03,249 DEBUG TRAIN Batch 28/7400 loss 7.977577 loss_att 10.839289 loss_ctc 10.370487 loss_rnnt 6.969509 hw_loss 0.218758 lr 0.00032220 rank 1
2023-02-25 13:38:16,531 DEBUG TRAIN Batch 28/7500 loss 8.194568 loss_att 11.046108 loss_ctc 9.278021 loss_rnnt 7.346920 hw_loss 0.249149 lr 0.00032204 rank 0
2023-02-25 13:38:16,534 DEBUG TRAIN Batch 28/7500 loss 8.282971 loss_att 10.049520 loss_ctc 10.156686 loss_rnnt 7.556570 hw_loss 0.231118 lr 0.00032207 rank 4
2023-02-25 13:38:16,536 DEBUG TRAIN Batch 28/7500 loss 9.926920 loss_att 10.390970 loss_ctc 10.213426 loss_rnnt 9.702402 hw_loss 0.175325 lr 0.00032211 rank 6
2023-02-25 13:38:16,538 DEBUG TRAIN Batch 28/7500 loss 4.964674 loss_att 7.481170 loss_ctc 8.946136 loss_rnnt 3.809465 hw_loss 0.226965 lr 0.00032209 rank 3
2023-02-25 13:38:16,540 DEBUG TRAIN Batch 28/7500 loss 8.406231 loss_att 11.418881 loss_ctc 11.293620 loss_rnnt 7.283234 hw_loss 0.254029 lr 0.00032205 rank 7
2023-02-25 13:38:16,542 DEBUG TRAIN Batch 28/7500 loss 10.101774 loss_att 13.138823 loss_ctc 19.354647 loss_rnnt 8.181771 hw_loss 0.147894 lr 0.00032213 rank 1
2023-02-25 13:38:16,541 DEBUG TRAIN Batch 28/7500 loss 6.812979 loss_att 8.834621 loss_ctc 11.145709 loss_rnnt 5.662618 hw_loss 0.315626 lr 0.00032210 rank 2
2023-02-25 13:38:16,543 DEBUG TRAIN Batch 28/7500 loss 6.575401 loss_att 9.274310 loss_ctc 9.428898 loss_rnnt 5.580776 hw_loss 0.139458 lr 0.00032208 rank 5
2023-02-25 13:39:26,508 DEBUG TRAIN Batch 28/7600 loss 7.169107 loss_att 9.479979 loss_ctc 9.763647 loss_rnnt 6.236335 hw_loss 0.233739 lr 0.00032197 rank 0
2023-02-25 13:39:26,516 DEBUG TRAIN Batch 28/7600 loss 11.271303 loss_att 13.993506 loss_ctc 22.211979 loss_rnnt 9.156835 hw_loss 0.208633 lr 0.00032202 rank 3
2023-02-25 13:39:26,516 DEBUG TRAIN Batch 28/7600 loss 3.699822 loss_att 4.859046 loss_ctc 6.046244 loss_rnnt 3.074181 hw_loss 0.151763 lr 0.00032206 rank 1
2023-02-25 13:39:26,516 DEBUG TRAIN Batch 28/7600 loss 8.180772 loss_att 11.777425 loss_ctc 14.318136 loss_rnnt 6.573787 hw_loss 0.130013 lr 0.00032203 rank 2
2023-02-25 13:39:26,518 DEBUG TRAIN Batch 28/7600 loss 3.054271 loss_att 4.963040 loss_ctc 4.681486 loss_rnnt 2.353708 hw_loss 0.190963 lr 0.00032201 rank 5
2023-02-25 13:39:26,521 DEBUG TRAIN Batch 28/7600 loss 15.394315 loss_att 15.525830 loss_ctc 11.651678 loss_rnnt 15.842909 hw_loss 0.045229 lr 0.00032198 rank 7
2023-02-25 13:39:26,521 DEBUG TRAIN Batch 28/7600 loss 8.736176 loss_att 10.520683 loss_ctc 10.911831 loss_rnnt 8.002724 hw_loss 0.162117 lr 0.00032204 rank 6
2023-02-25 13:39:26,524 DEBUG TRAIN Batch 28/7600 loss 12.021527 loss_att 13.459178 loss_ctc 15.550001 loss_rnnt 11.200035 hw_loss 0.119058 lr 0.00032200 rank 4
2023-02-25 13:40:36,723 DEBUG TRAIN Batch 28/7700 loss 9.204870 loss_att 9.160065 loss_ctc 11.826585 loss_rnnt 8.743559 hw_loss 0.226331 lr 0.00032195 rank 3
2023-02-25 13:40:36,723 DEBUG TRAIN Batch 28/7700 loss 8.593842 loss_att 13.047342 loss_ctc 14.765259 loss_rnnt 6.804393 hw_loss 0.142300 lr 0.00032199 rank 1
2023-02-25 13:40:36,725 DEBUG TRAIN Batch 28/7700 loss 10.916207 loss_att 14.216833 loss_ctc 17.341108 loss_rnnt 9.283774 hw_loss 0.216854 lr 0.00032195 rank 5
2023-02-25 13:40:36,726 DEBUG TRAIN Batch 28/7700 loss 6.261237 loss_att 8.310981 loss_ctc 7.481494 loss_rnnt 5.603050 hw_loss 0.160382 lr 0.00032191 rank 7
2023-02-25 13:40:36,728 DEBUG TRAIN Batch 28/7700 loss 12.436861 loss_att 15.767538 loss_ctc 24.519623 loss_rnnt 10.089399 hw_loss 0.131797 lr 0.00032190 rank 0
2023-02-25 13:40:36,731 DEBUG TRAIN Batch 28/7700 loss 8.514404 loss_att 10.978871 loss_ctc 9.535086 loss_rnnt 7.774305 hw_loss 0.208342 lr 0.00032194 rank 4
2023-02-25 13:40:36,733 DEBUG TRAIN Batch 28/7700 loss 8.639907 loss_att 11.201231 loss_ctc 8.472170 loss_rnnt 8.043747 hw_loss 0.199237 lr 0.00032197 rank 2
2023-02-25 13:40:36,766 DEBUG TRAIN Batch 28/7700 loss 10.138223 loss_att 11.067902 loss_ctc 15.145408 loss_rnnt 9.164514 hw_loss 0.225278 lr 0.00032197 rank 6
2023-02-25 13:41:48,380 DEBUG TRAIN Batch 28/7800 loss 6.397600 loss_att 9.799279 loss_ctc 7.364536 loss_rnnt 5.464866 hw_loss 0.231513 lr 0.00032185 rank 7
2023-02-25 13:41:48,387 DEBUG TRAIN Batch 28/7800 loss 3.802577 loss_att 6.311601 loss_ctc 6.588495 loss_rnnt 2.824401 hw_loss 0.196717 lr 0.00032189 rank 3
2023-02-25 13:41:48,391 DEBUG TRAIN Batch 28/7800 loss 7.507997 loss_att 10.261455 loss_ctc 8.416701 loss_rnnt 6.737642 hw_loss 0.184692 lr 0.00032187 rank 4
2023-02-25 13:41:48,395 DEBUG TRAIN Batch 28/7800 loss 13.994657 loss_att 13.567577 loss_ctc 15.792426 loss_rnnt 13.761150 hw_loss 0.148537 lr 0.00032184 rank 0
2023-02-25 13:41:48,397 DEBUG TRAIN Batch 28/7800 loss 13.166543 loss_att 16.601685 loss_ctc 21.819317 loss_rnnt 11.205553 hw_loss 0.225483 lr 0.00032193 rank 1
2023-02-25 13:41:48,398 DEBUG TRAIN Batch 28/7800 loss 11.428713 loss_att 14.405512 loss_ctc 13.784575 loss_rnnt 10.464525 hw_loss 0.102588 lr 0.00032191 rank 6
2023-02-25 13:41:48,403 DEBUG TRAIN Batch 28/7800 loss 9.956230 loss_att 10.454584 loss_ctc 17.851614 loss_rnnt 8.723063 hw_loss 0.151459 lr 0.00032190 rank 2
2023-02-25 13:41:48,454 DEBUG TRAIN Batch 28/7800 loss 3.839040 loss_att 8.229697 loss_ctc 4.426695 loss_rnnt 2.776197 hw_loss 0.199420 lr 0.00032188 rank 5
2023-02-25 13:42:59,856 DEBUG TRAIN Batch 28/7900 loss 13.513364 loss_att 16.395313 loss_ctc 18.487179 loss_rnnt 12.156774 hw_loss 0.219422 lr 0.00032178 rank 7
2023-02-25 13:42:59,857 DEBUG TRAIN Batch 28/7900 loss 5.752374 loss_att 6.548789 loss_ctc 6.062397 loss_rnnt 5.472227 hw_loss 0.149115 lr 0.00032181 rank 5
2023-02-25 13:42:59,858 DEBUG TRAIN Batch 28/7900 loss 7.658524 loss_att 10.616477 loss_ctc 11.267941 loss_rnnt 6.512444 hw_loss 0.137313 lr 0.00032177 rank 0
2023-02-25 13:42:59,861 DEBUG TRAIN Batch 28/7900 loss 9.354319 loss_att 11.498275 loss_ctc 13.981060 loss_rnnt 8.208444 hw_loss 0.187844 lr 0.00032186 rank 1
2023-02-25 13:42:59,860 DEBUG TRAIN Batch 28/7900 loss 17.614571 loss_att 19.819742 loss_ctc 19.563740 loss_rnnt 16.784340 hw_loss 0.242454 lr 0.00032180 rank 4
2023-02-25 13:42:59,864 DEBUG TRAIN Batch 28/7900 loss 12.851914 loss_att 16.370117 loss_ctc 16.694176 loss_rnnt 11.541124 hw_loss 0.177839 lr 0.00032184 rank 6
2023-02-25 13:42:59,865 DEBUG TRAIN Batch 28/7900 loss 7.395793 loss_att 11.604458 loss_ctc 12.936018 loss_rnnt 5.727685 hw_loss 0.164396 lr 0.00032182 rank 3
2023-02-25 13:42:59,866 DEBUG TRAIN Batch 28/7900 loss 10.404170 loss_att 12.947402 loss_ctc 13.653461 loss_rnnt 9.348703 hw_loss 0.212964 lr 0.00032183 rank 2
2023-02-25 13:44:09,755 DEBUG TRAIN Batch 28/8000 loss 5.232460 loss_att 6.461567 loss_ctc 10.839680 loss_rnnt 4.111377 hw_loss 0.239310 lr 0.00032170 rank 0
2023-02-25 13:44:09,756 DEBUG TRAIN Batch 28/8000 loss 5.699214 loss_att 8.667589 loss_ctc 8.458708 loss_rnnt 4.620882 hw_loss 0.218859 lr 0.00032175 rank 3
2023-02-25 13:44:09,755 DEBUG TRAIN Batch 28/8000 loss 9.875255 loss_att 10.814838 loss_ctc 11.993875 loss_rnnt 9.338243 hw_loss 0.124897 lr 0.00032174 rank 4
2023-02-25 13:44:09,760 DEBUG TRAIN Batch 28/8000 loss 5.095409 loss_att 8.300854 loss_ctc 11.016193 loss_rnnt 3.587497 hw_loss 0.145097 lr 0.00032179 rank 1
2023-02-25 13:44:09,761 DEBUG TRAIN Batch 28/8000 loss 7.031586 loss_att 9.892973 loss_ctc 12.350586 loss_rnnt 5.611667 hw_loss 0.259577 lr 0.00032175 rank 5
2023-02-25 13:44:09,761 DEBUG TRAIN Batch 28/8000 loss 7.530192 loss_att 11.236755 loss_ctc 11.425326 loss_rnnt 6.162708 hw_loss 0.200287 lr 0.00032177 rank 6
2023-02-25 13:44:09,761 DEBUG TRAIN Batch 28/8000 loss 5.224082 loss_att 7.498417 loss_ctc 11.334037 loss_rnnt 3.856169 hw_loss 0.184471 lr 0.00032171 rank 7
2023-02-25 13:44:09,769 DEBUG TRAIN Batch 28/8000 loss 10.845114 loss_att 12.212814 loss_ctc 18.577177 loss_rnnt 9.419552 hw_loss 0.227026 lr 0.00032177 rank 2
2023-02-25 13:45:20,728 DEBUG TRAIN Batch 28/8100 loss 7.053183 loss_att 7.672661 loss_ctc 9.855788 loss_rnnt 6.409615 hw_loss 0.273733 lr 0.00032164 rank 0
2023-02-25 13:45:20,734 DEBUG TRAIN Batch 28/8100 loss 7.367525 loss_att 10.595895 loss_ctc 12.174005 loss_rnnt 5.973715 hw_loss 0.201135 lr 0.00032173 rank 1
2023-02-25 13:45:20,735 DEBUG TRAIN Batch 28/8100 loss 11.333322 loss_att 14.451656 loss_ctc 15.499929 loss_rnnt 10.061880 hw_loss 0.172924 lr 0.00032165 rank 7
2023-02-25 13:45:20,737 DEBUG TRAIN Batch 28/8100 loss 5.353549 loss_att 7.252032 loss_ctc 6.331767 loss_rnnt 4.718217 hw_loss 0.234761 lr 0.00032168 rank 5
2023-02-25 13:45:20,739 DEBUG TRAIN Batch 28/8100 loss 9.271164 loss_att 10.643897 loss_ctc 11.915146 loss_rnnt 8.540748 hw_loss 0.193759 lr 0.00032170 rank 2
2023-02-25 13:45:20,740 DEBUG TRAIN Batch 28/8100 loss 9.141952 loss_att 10.739832 loss_ctc 14.019204 loss_rnnt 8.083957 hw_loss 0.165222 lr 0.00032169 rank 3
2023-02-25 13:45:20,741 DEBUG TRAIN Batch 28/8100 loss 2.603082 loss_att 5.899173 loss_ctc 4.301749 loss_rnnt 1.591543 hw_loss 0.235935 lr 0.00032171 rank 6
2023-02-25 13:45:20,746 DEBUG TRAIN Batch 28/8100 loss 10.389334 loss_att 11.936594 loss_ctc 15.490867 loss_rnnt 9.279176 hw_loss 0.225940 lr 0.00032167 rank 4
2023-02-25 13:46:31,690 DEBUG TRAIN Batch 28/8200 loss 5.913953 loss_att 8.673232 loss_ctc 10.705726 loss_rnnt 4.629196 hw_loss 0.176245 lr 0.00032164 rank 6
2023-02-25 13:46:31,690 DEBUG TRAIN Batch 28/8200 loss 7.079458 loss_att 10.470131 loss_ctc 9.968703 loss_rnnt 5.887550 hw_loss 0.241013 lr 0.00032162 rank 3
2023-02-25 13:46:31,692 DEBUG TRAIN Batch 28/8200 loss 5.918284 loss_att 6.562857 loss_ctc 7.482713 loss_rnnt 5.469770 hw_loss 0.208140 lr 0.00032160 rank 4
2023-02-25 13:46:31,693 DEBUG TRAIN Batch 28/8200 loss 5.208319 loss_att 9.108396 loss_ctc 8.451406 loss_rnnt 3.906331 hw_loss 0.167927 lr 0.00032157 rank 0
2023-02-25 13:46:31,694 DEBUG TRAIN Batch 28/8200 loss 10.128226 loss_att 10.871277 loss_ctc 12.795727 loss_rnnt 9.466681 hw_loss 0.294878 lr 0.00032163 rank 2
2023-02-25 13:46:31,696 DEBUG TRAIN Batch 28/8200 loss 4.461994 loss_att 6.109301 loss_ctc 5.670405 loss_rnnt 3.887710 hw_loss 0.156940 lr 0.00032158 rank 7
2023-02-25 13:46:31,700 DEBUG TRAIN Batch 28/8200 loss 7.452034 loss_att 10.002657 loss_ctc 10.976049 loss_rnnt 6.375394 hw_loss 0.181212 lr 0.00032166 rank 1
2023-02-25 13:46:31,705 DEBUG TRAIN Batch 28/8200 loss 5.067811 loss_att 8.028990 loss_ctc 5.401478 loss_rnnt 4.386252 hw_loss 0.084063 lr 0.00032161 rank 5
2023-02-25 13:47:41,771 DEBUG TRAIN Batch 28/8300 loss 5.541331 loss_att 8.449594 loss_ctc 8.182330 loss_rnnt 4.485666 hw_loss 0.228525 lr 0.00032156 rank 3
2023-02-25 13:47:41,778 DEBUG TRAIN Batch 28/8300 loss 2.923025 loss_att 5.634836 loss_ctc 6.140422 loss_rnnt 1.833668 hw_loss 0.221266 lr 0.00032151 rank 0
2023-02-25 13:47:41,783 DEBUG TRAIN Batch 28/8300 loss 4.001073 loss_att 5.714759 loss_ctc 4.748359 loss_rnnt 3.452600 hw_loss 0.198933 lr 0.00032157 rank 6
2023-02-25 13:47:41,784 DEBUG TRAIN Batch 28/8300 loss 7.470378 loss_att 8.227577 loss_ctc 11.458934 loss_rnnt 6.649240 hw_loss 0.258546 lr 0.00032155 rank 5
2023-02-25 13:47:41,785 DEBUG TRAIN Batch 28/8300 loss 8.093067 loss_att 8.463631 loss_ctc 11.332354 loss_rnnt 7.430892 hw_loss 0.292796 lr 0.00032159 rank 1
2023-02-25 13:47:41,786 DEBUG TRAIN Batch 28/8300 loss 18.031496 loss_att 20.430540 loss_ctc 23.620556 loss_rnnt 16.685238 hw_loss 0.227327 lr 0.00032151 rank 7
2023-02-25 13:47:41,789 DEBUG TRAIN Batch 28/8300 loss 6.963236 loss_att 11.592189 loss_ctc 12.620639 loss_rnnt 5.206564 hw_loss 0.143550 lr 0.00032154 rank 4
2023-02-25 13:47:41,790 DEBUG TRAIN Batch 28/8300 loss 10.080290 loss_att 12.649859 loss_ctc 16.068062 loss_rnnt 8.670765 hw_loss 0.182329 lr 0.00032157 rank 2
2023-02-25 13:48:27,934 DEBUG CV Batch 28/0 loss 1.486973 loss_att 1.593759 loss_ctc 1.949050 loss_rnnt 1.282106 hw_loss 0.228561 history loss 1.431900 rank 0
2023-02-25 13:48:27,935 DEBUG CV Batch 28/0 loss 1.486973 loss_att 1.593759 loss_ctc 1.949050 loss_rnnt 1.282106 hw_loss 0.228561 history loss 1.431900 rank 4
2023-02-25 13:48:27,939 DEBUG CV Batch 28/0 loss 1.486973 loss_att 1.593759 loss_ctc 1.949050 loss_rnnt 1.282106 hw_loss 0.228561 history loss 1.431900 rank 1
2023-02-25 13:48:27,941 DEBUG CV Batch 28/0 loss 1.486973 loss_att 1.593759 loss_ctc 1.949050 loss_rnnt 1.282106 hw_loss 0.228561 history loss 1.431900 rank 6
2023-02-25 13:48:27,945 DEBUG CV Batch 28/0 loss 1.486973 loss_att 1.593759 loss_ctc 1.949050 loss_rnnt 1.282106 hw_loss 0.228561 history loss 1.431900 rank 5
2023-02-25 13:48:27,957 DEBUG CV Batch 28/0 loss 1.486973 loss_att 1.593759 loss_ctc 1.949050 loss_rnnt 1.282106 hw_loss 0.228561 history loss 1.431900 rank 7
2023-02-25 13:48:27,958 DEBUG CV Batch 28/0 loss 1.486973 loss_att 1.593759 loss_ctc 1.949050 loss_rnnt 1.282106 hw_loss 0.228561 history loss 1.431900 rank 2
2023-02-25 13:48:27,963 DEBUG CV Batch 28/0 loss 1.486973 loss_att 1.593759 loss_ctc 1.949050 loss_rnnt 1.282106 hw_loss 0.228561 history loss 1.431900 rank 3
2023-02-25 13:48:39,235 DEBUG CV Batch 28/100 loss 4.590275 loss_att 6.064238 loss_ctc 8.996067 loss_rnnt 3.582569 hw_loss 0.235264 history loss 3.169498 rank 4
2023-02-25 13:48:39,258 DEBUG CV Batch 28/100 loss 4.590275 loss_att 6.064238 loss_ctc 8.996067 loss_rnnt 3.582569 hw_loss 0.235264 history loss 3.169498 rank 3
2023-02-25 13:48:39,342 DEBUG CV Batch 28/100 loss 4.590275 loss_att 6.064238 loss_ctc 8.996067 loss_rnnt 3.582569 hw_loss 0.235264 history loss 3.169498 rank 5
2023-02-25 13:48:39,358 DEBUG CV Batch 28/100 loss 4.590275 loss_att 6.064238 loss_ctc 8.996067 loss_rnnt 3.582569 hw_loss 0.235264 history loss 3.169498 rank 6
2023-02-25 13:48:39,728 DEBUG CV Batch 28/100 loss 4.590275 loss_att 6.064238 loss_ctc 8.996067 loss_rnnt 3.582569 hw_loss 0.235264 history loss 3.169498 rank 1
2023-02-25 13:48:39,748 DEBUG CV Batch 28/100 loss 4.590275 loss_att 6.064238 loss_ctc 8.996067 loss_rnnt 3.582569 hw_loss 0.235264 history loss 3.169498 rank 0
2023-02-25 13:48:39,934 DEBUG CV Batch 28/100 loss 4.590275 loss_att 6.064238 loss_ctc 8.996067 loss_rnnt 3.582569 hw_loss 0.235264 history loss 3.169498 rank 2
2023-02-25 13:48:40,454 DEBUG CV Batch 28/100 loss 4.590275 loss_att 6.064238 loss_ctc 8.996067 loss_rnnt 3.582569 hw_loss 0.235264 history loss 3.169498 rank 7
2023-02-25 13:48:52,821 DEBUG CV Batch 28/200 loss 8.022510 loss_att 17.678034 loss_ctc 8.167412 loss_rnnt 5.975250 hw_loss 0.181565 history loss 3.728556 rank 4
2023-02-25 13:48:52,944 DEBUG CV Batch 28/200 loss 8.022510 loss_att 17.678034 loss_ctc 8.167412 loss_rnnt 5.975250 hw_loss 0.181565 history loss 3.728556 rank 3
2023-02-25 13:48:52,983 DEBUG CV Batch 28/200 loss 8.022510 loss_att 17.678034 loss_ctc 8.167412 loss_rnnt 5.975250 hw_loss 0.181565 history loss 3.728556 rank 6
2023-02-25 13:48:53,126 DEBUG CV Batch 28/200 loss 8.022510 loss_att 17.678034 loss_ctc 8.167412 loss_rnnt 5.975250 hw_loss 0.181565 history loss 3.728556 rank 5
2023-02-25 13:48:53,639 DEBUG CV Batch 28/200 loss 8.022510 loss_att 17.678034 loss_ctc 8.167412 loss_rnnt 5.975250 hw_loss 0.181565 history loss 3.728556 rank 1
2023-02-25 13:48:53,753 DEBUG CV Batch 28/200 loss 8.022510 loss_att 17.678034 loss_ctc 8.167412 loss_rnnt 5.975250 hw_loss 0.181565 history loss 3.728556 rank 0
2023-02-25 13:48:53,915 DEBUG CV Batch 28/200 loss 8.022510 loss_att 17.678034 loss_ctc 8.167412 loss_rnnt 5.975250 hw_loss 0.181565 history loss 3.728556 rank 2
2023-02-25 13:48:54,595 DEBUG CV Batch 28/200 loss 8.022510 loss_att 17.678034 loss_ctc 8.167412 loss_rnnt 5.975250 hw_loss 0.181565 history loss 3.728556 rank 7
2023-02-25 13:49:04,793 DEBUG CV Batch 28/300 loss 2.717062 loss_att 3.549974 loss_ctc 4.394555 loss_rnnt 2.190440 hw_loss 0.255702 history loss 3.853514 rank 4
2023-02-25 13:49:04,932 DEBUG CV Batch 28/300 loss 2.717062 loss_att 3.549974 loss_ctc 4.394555 loss_rnnt 2.190440 hw_loss 0.255702 history loss 3.853514 rank 3
2023-02-25 13:49:04,955 DEBUG CV Batch 28/300 loss 2.717062 loss_att 3.549974 loss_ctc 4.394555 loss_rnnt 2.190440 hw_loss 0.255702 history loss 3.853514 rank 6
2023-02-25 13:49:05,104 DEBUG CV Batch 28/300 loss 2.717062 loss_att 3.549974 loss_ctc 4.394555 loss_rnnt 2.190440 hw_loss 0.255702 history loss 3.853514 rank 5
2023-02-25 13:49:06,208 DEBUG CV Batch 28/300 loss 2.717062 loss_att 3.549974 loss_ctc 4.394555 loss_rnnt 2.190440 hw_loss 0.255702 history loss 3.853514 rank 1
2023-02-25 13:49:06,270 DEBUG CV Batch 28/300 loss 2.717062 loss_att 3.549974 loss_ctc 4.394555 loss_rnnt 2.190440 hw_loss 0.255702 history loss 3.853514 rank 0
2023-02-25 13:49:06,568 DEBUG CV Batch 28/300 loss 2.717062 loss_att 3.549974 loss_ctc 4.394555 loss_rnnt 2.190440 hw_loss 0.255702 history loss 3.853514 rank 2
2023-02-25 13:49:07,325 DEBUG CV Batch 28/300 loss 2.717062 loss_att 3.549974 loss_ctc 4.394555 loss_rnnt 2.190440 hw_loss 0.255702 history loss 3.853514 rank 7
2023-02-25 13:49:16,501 DEBUG CV Batch 28/400 loss 16.456682 loss_att 69.571487 loss_ctc 9.351140 loss_rnnt 6.695727 hw_loss 0.160126 history loss 4.746536 rank 4
2023-02-25 13:49:16,778 DEBUG CV Batch 28/400 loss 16.456682 loss_att 69.571487 loss_ctc 9.351140 loss_rnnt 6.695727 hw_loss 0.160126 history loss 4.746536 rank 3
2023-02-25 13:49:16,785 DEBUG CV Batch 28/400 loss 16.456682 loss_att 69.571487 loss_ctc 9.351140 loss_rnnt 6.695727 hw_loss 0.160126 history loss 4.746536 rank 6
2023-02-25 13:49:17,120 DEBUG CV Batch 28/400 loss 16.456682 loss_att 69.571487 loss_ctc 9.351140 loss_rnnt 6.695727 hw_loss 0.160126 history loss 4.746536 rank 5
2023-02-25 13:49:18,774 DEBUG CV Batch 28/400 loss 16.456682 loss_att 69.571487 loss_ctc 9.351140 loss_rnnt 6.695727 hw_loss 0.160126 history loss 4.746536 rank 1
2023-02-25 13:49:18,917 DEBUG CV Batch 28/400 loss 16.456682 loss_att 69.571487 loss_ctc 9.351140 loss_rnnt 6.695727 hw_loss 0.160126 history loss 4.746536 rank 0
2023-02-25 13:49:19,172 DEBUG CV Batch 28/400 loss 16.456682 loss_att 69.571487 loss_ctc 9.351140 loss_rnnt 6.695727 hw_loss 0.160126 history loss 4.746536 rank 2
2023-02-25 13:49:20,102 DEBUG CV Batch 28/400 loss 16.456682 loss_att 69.571487 loss_ctc 9.351140 loss_rnnt 6.695727 hw_loss 0.160126 history loss 4.746536 rank 7
2023-02-25 13:49:27,105 DEBUG CV Batch 28/500 loss 5.665725 loss_att 5.555664 loss_ctc 7.451540 loss_rnnt 5.340963 hw_loss 0.203749 history loss 5.403140 rank 3
2023-02-25 13:49:27,117 DEBUG CV Batch 28/500 loss 5.665725 loss_att 5.555664 loss_ctc 7.451540 loss_rnnt 5.340963 hw_loss 0.203749 history loss 5.403140 rank 6
2023-02-25 13:49:27,132 DEBUG CV Batch 28/500 loss 5.665725 loss_att 5.555664 loss_ctc 7.451540 loss_rnnt 5.340963 hw_loss 0.203749 history loss 5.403140 rank 4
2023-02-25 13:49:27,459 DEBUG CV Batch 28/500 loss 5.665725 loss_att 5.555664 loss_ctc 7.451540 loss_rnnt 5.340963 hw_loss 0.203749 history loss 5.403140 rank 5
2023-02-25 13:49:29,633 DEBUG CV Batch 28/500 loss 5.665725 loss_att 5.555664 loss_ctc 7.451540 loss_rnnt 5.340963 hw_loss 0.203749 history loss 5.403140 rank 1
2023-02-25 13:49:29,743 DEBUG CV Batch 28/500 loss 5.665725 loss_att 5.555664 loss_ctc 7.451540 loss_rnnt 5.340963 hw_loss 0.203749 history loss 5.403140 rank 0
2023-02-25 13:49:30,171 DEBUG CV Batch 28/500 loss 5.665725 loss_att 5.555664 loss_ctc 7.451540 loss_rnnt 5.340963 hw_loss 0.203749 history loss 5.403140 rank 2
2023-02-25 13:49:31,208 DEBUG CV Batch 28/500 loss 5.665725 loss_att 5.555664 loss_ctc 7.451540 loss_rnnt 5.340963 hw_loss 0.203749 history loss 5.403140 rank 7
2023-02-25 13:49:39,232 DEBUG CV Batch 28/600 loss 6.506479 loss_att 6.474124 loss_ctc 8.527178 loss_rnnt 6.131571 hw_loss 0.209910 history loss 6.296596 rank 4
2023-02-25 13:49:39,373 DEBUG CV Batch 28/600 loss 6.506479 loss_att 6.474124 loss_ctc 8.527178 loss_rnnt 6.131571 hw_loss 0.209910 history loss 6.296596 rank 3
2023-02-25 13:49:39,466 DEBUG CV Batch 28/600 loss 6.506479 loss_att 6.474124 loss_ctc 8.527178 loss_rnnt 6.131571 hw_loss 0.209910 history loss 6.296596 rank 6
2023-02-25 13:49:39,651 DEBUG CV Batch 28/600 loss 6.506479 loss_att 6.474124 loss_ctc 8.527178 loss_rnnt 6.131571 hw_loss 0.209910 history loss 6.296596 rank 5
2023-02-25 13:49:42,077 DEBUG CV Batch 28/600 loss 6.506479 loss_att 6.474124 loss_ctc 8.527178 loss_rnnt 6.131571 hw_loss 0.209910 history loss 6.296596 rank 1
2023-02-25 13:49:42,346 DEBUG CV Batch 28/600 loss 6.506479 loss_att 6.474124 loss_ctc 8.527178 loss_rnnt 6.131571 hw_loss 0.209910 history loss 6.296596 rank 0
2023-02-25 13:49:42,745 DEBUG CV Batch 28/600 loss 6.506479 loss_att 6.474124 loss_ctc 8.527178 loss_rnnt 6.131571 hw_loss 0.209910 history loss 6.296596 rank 2
2023-02-25 13:49:44,054 DEBUG CV Batch 28/600 loss 6.506479 loss_att 6.474124 loss_ctc 8.527178 loss_rnnt 6.131571 hw_loss 0.209910 history loss 6.296596 rank 7
2023-02-25 13:49:50,760 DEBUG CV Batch 28/700 loss 14.834586 loss_att 50.091511 loss_ctc 13.365745 loss_rnnt 7.927246 hw_loss 0.097126 history loss 6.911409 rank 6
2023-02-25 13:49:50,937 DEBUG CV Batch 28/700 loss 14.834586 loss_att 50.091511 loss_ctc 13.365745 loss_rnnt 7.927246 hw_loss 0.097126 history loss 6.911409 rank 4
2023-02-25 13:49:50,978 DEBUG CV Batch 28/700 loss 14.834586 loss_att 50.091511 loss_ctc 13.365745 loss_rnnt 7.927246 hw_loss 0.097126 history loss 6.911409 rank 5
2023-02-25 13:49:51,660 DEBUG CV Batch 28/700 loss 14.834586 loss_att 50.091511 loss_ctc 13.365745 loss_rnnt 7.927246 hw_loss 0.097126 history loss 6.911409 rank 3
2023-02-25 13:49:53,909 DEBUG CV Batch 28/700 loss 14.834586 loss_att 50.091511 loss_ctc 13.365745 loss_rnnt 7.927246 hw_loss 0.097126 history loss 6.911409 rank 1
2023-02-25 13:49:54,271 DEBUG CV Batch 28/700 loss 14.834586 loss_att 50.091511 loss_ctc 13.365745 loss_rnnt 7.927246 hw_loss 0.097126 history loss 6.911409 rank 0
2023-02-25 13:49:54,632 DEBUG CV Batch 28/700 loss 14.834586 loss_att 50.091511 loss_ctc 13.365745 loss_rnnt 7.927246 hw_loss 0.097126 history loss 6.911409 rank 2
2023-02-25 13:49:56,060 DEBUG CV Batch 28/700 loss 14.834586 loss_att 50.091511 loss_ctc 13.365745 loss_rnnt 7.927246 hw_loss 0.097126 history loss 6.911409 rank 7
2023-02-25 13:50:02,523 DEBUG CV Batch 28/800 loss 8.770804 loss_att 8.844902 loss_ctc 14.152897 loss_rnnt 7.881172 hw_loss 0.294751 history loss 6.405671 rank 6
2023-02-25 13:50:02,878 DEBUG CV Batch 28/800 loss 8.770804 loss_att 8.844902 loss_ctc 14.152897 loss_rnnt 7.881172 hw_loss 0.294751 history loss 6.405671 rank 4
2023-02-25 13:50:03,122 DEBUG CV Batch 28/800 loss 8.770804 loss_att 8.844902 loss_ctc 14.152897 loss_rnnt 7.881172 hw_loss 0.294751 history loss 6.405671 rank 5
2023-02-25 13:50:03,856 DEBUG CV Batch 28/800 loss 8.770804 loss_att 8.844902 loss_ctc 14.152897 loss_rnnt 7.881172 hw_loss 0.294751 history loss 6.405671 rank 3
2023-02-25 13:50:05,508 DEBUG CV Batch 28/800 loss 8.770804 loss_att 8.844902 loss_ctc 14.152897 loss_rnnt 7.881172 hw_loss 0.294751 history loss 6.405671 rank 1
2023-02-25 13:50:06,041 DEBUG CV Batch 28/800 loss 8.770804 loss_att 8.844902 loss_ctc 14.152897 loss_rnnt 7.881172 hw_loss 0.294751 history loss 6.405671 rank 0
2023-02-25 13:50:06,504 DEBUG CV Batch 28/800 loss 8.770804 loss_att 8.844902 loss_ctc 14.152897 loss_rnnt 7.881172 hw_loss 0.294751 history loss 6.405671 rank 2
2023-02-25 13:50:07,753 DEBUG CV Batch 28/800 loss 8.770804 loss_att 8.844902 loss_ctc 14.152897 loss_rnnt 7.881172 hw_loss 0.294751 history loss 6.405671 rank 7
2023-02-25 13:50:16,134 DEBUG CV Batch 28/900 loss 15.012479 loss_att 17.848240 loss_ctc 25.384024 loss_rnnt 13.003173 hw_loss 0.111154 history loss 6.224787 rank 6
2023-02-25 13:50:16,636 DEBUG CV Batch 28/900 loss 15.012479 loss_att 17.848240 loss_ctc 25.384024 loss_rnnt 13.003173 hw_loss 0.111154 history loss 6.224787 rank 4
2023-02-25 13:50:16,782 DEBUG CV Batch 28/900 loss 15.012479 loss_att 17.848240 loss_ctc 25.384024 loss_rnnt 13.003173 hw_loss 0.111154 history loss 6.224787 rank 5
2023-02-25 13:50:17,391 DEBUG CV Batch 28/900 loss 15.012479 loss_att 17.848240 loss_ctc 25.384024 loss_rnnt 13.003173 hw_loss 0.111154 history loss 6.224787 rank 3
2023-02-25 13:50:19,233 DEBUG CV Batch 28/900 loss 15.012479 loss_att 17.848240 loss_ctc 25.384024 loss_rnnt 13.003173 hw_loss 0.111154 history loss 6.224787 rank 1
2023-02-25 13:50:20,172 DEBUG CV Batch 28/900 loss 15.012479 loss_att 17.848240 loss_ctc 25.384024 loss_rnnt 13.003173 hw_loss 0.111154 history loss 6.224787 rank 0
2023-02-25 13:50:20,663 DEBUG CV Batch 28/900 loss 15.012479 loss_att 17.848240 loss_ctc 25.384024 loss_rnnt 13.003173 hw_loss 0.111154 history loss 6.224787 rank 2
2023-02-25 13:50:21,711 DEBUG CV Batch 28/900 loss 15.012479 loss_att 17.848240 loss_ctc 25.384024 loss_rnnt 13.003173 hw_loss 0.111154 history loss 6.224787 rank 7
2023-02-25 13:50:28,232 DEBUG CV Batch 28/1000 loss 3.859255 loss_att 3.811809 loss_ctc 4.188610 loss_rnnt 3.670247 hw_loss 0.289843 history loss 6.019124 rank 6
2023-02-25 13:50:28,552 DEBUG CV Batch 28/1000 loss 3.859255 loss_att 3.811809 loss_ctc 4.188610 loss_rnnt 3.670247 hw_loss 0.289843 history loss 6.019124 rank 4
2023-02-25 13:50:29,003 DEBUG CV Batch 28/1000 loss 3.859255 loss_att 3.811809 loss_ctc 4.188610 loss_rnnt 3.670247 hw_loss 0.289843 history loss 6.019124 rank 5
2023-02-25 13:50:29,379 DEBUG CV Batch 28/1000 loss 3.859255 loss_att 3.811809 loss_ctc 4.188610 loss_rnnt 3.670247 hw_loss 0.289843 history loss 6.019124 rank 3
2023-02-25 13:50:32,111 DEBUG CV Batch 28/1000 loss 3.859255 loss_att 3.811809 loss_ctc 4.188610 loss_rnnt 3.670247 hw_loss 0.289843 history loss 6.019124 rank 1
2023-02-25 13:50:33,075 DEBUG CV Batch 28/1000 loss 3.859255 loss_att 3.811809 loss_ctc 4.188610 loss_rnnt 3.670247 hw_loss 0.289843 history loss 6.019124 rank 0
2023-02-25 13:50:33,562 DEBUG CV Batch 28/1000 loss 3.859255 loss_att 3.811809 loss_ctc 4.188610 loss_rnnt 3.670247 hw_loss 0.289843 history loss 6.019124 rank 2
2023-02-25 13:50:34,676 DEBUG CV Batch 28/1000 loss 3.859255 loss_att 3.811809 loss_ctc 4.188610 loss_rnnt 3.670247 hw_loss 0.289843 history loss 6.019124 rank 7
2023-02-25 13:50:40,021 DEBUG CV Batch 28/1100 loss 5.840930 loss_att 5.035944 loss_ctc 7.924940 loss_rnnt 5.595271 hw_loss 0.241477 history loss 5.997420 rank 6
2023-02-25 13:50:40,204 DEBUG CV Batch 28/1100 loss 5.840930 loss_att 5.035944 loss_ctc 7.924940 loss_rnnt 5.595271 hw_loss 0.241477 history loss 5.997420 rank 4
2023-02-25 13:50:40,801 DEBUG CV Batch 28/1100 loss 5.840930 loss_att 5.035944 loss_ctc 7.924940 loss_rnnt 5.595271 hw_loss 0.241477 history loss 5.997420 rank 5
2023-02-25 13:50:41,109 DEBUG CV Batch 28/1100 loss 5.840930 loss_att 5.035944 loss_ctc 7.924940 loss_rnnt 5.595271 hw_loss 0.241477 history loss 5.997420 rank 3
2023-02-25 13:50:44,476 DEBUG CV Batch 28/1100 loss 5.840930 loss_att 5.035944 loss_ctc 7.924940 loss_rnnt 5.595271 hw_loss 0.241477 history loss 5.997420 rank 1
2023-02-25 13:50:45,773 DEBUG CV Batch 28/1100 loss 5.840930 loss_att 5.035944 loss_ctc 7.924940 loss_rnnt 5.595271 hw_loss 0.241477 history loss 5.997420 rank 0
2023-02-25 13:50:46,211 DEBUG CV Batch 28/1100 loss 5.840930 loss_att 5.035944 loss_ctc 7.924940 loss_rnnt 5.595271 hw_loss 0.241477 history loss 5.997420 rank 2
2023-02-25 13:50:47,238 DEBUG CV Batch 28/1100 loss 5.840930 loss_att 5.035944 loss_ctc 7.924940 loss_rnnt 5.595271 hw_loss 0.241477 history loss 5.997420 rank 7
2023-02-25 13:50:50,298 DEBUG CV Batch 28/1200 loss 6.156953 loss_att 7.621669 loss_ctc 7.971683 loss_rnnt 5.518067 hw_loss 0.194959 history loss 6.290323 rank 6
2023-02-25 13:50:50,507 DEBUG CV Batch 28/1200 loss 6.156953 loss_att 7.621669 loss_ctc 7.971683 loss_rnnt 5.518067 hw_loss 0.194959 history loss 6.290323 rank 4
2023-02-25 13:50:51,224 DEBUG CV Batch 28/1200 loss 6.156953 loss_att 7.621669 loss_ctc 7.971683 loss_rnnt 5.518067 hw_loss 0.194959 history loss 6.290323 rank 5
2023-02-25 13:50:51,962 DEBUG CV Batch 28/1200 loss 6.156953 loss_att 7.621669 loss_ctc 7.971683 loss_rnnt 5.518067 hw_loss 0.194959 history loss 6.290323 rank 3
2023-02-25 13:50:55,649 DEBUG CV Batch 28/1200 loss 6.156953 loss_att 7.621669 loss_ctc 7.971683 loss_rnnt 5.518067 hw_loss 0.194959 history loss 6.290323 rank 1
2023-02-25 13:50:56,904 DEBUG CV Batch 28/1200 loss 6.156953 loss_att 7.621669 loss_ctc 7.971683 loss_rnnt 5.518067 hw_loss 0.194959 history loss 6.290323 rank 0
2023-02-25 13:50:57,693 DEBUG CV Batch 28/1200 loss 6.156953 loss_att 7.621669 loss_ctc 7.971683 loss_rnnt 5.518067 hw_loss 0.194959 history loss 6.290323 rank 2
2023-02-25 13:50:58,755 DEBUG CV Batch 28/1200 loss 6.156953 loss_att 7.621669 loss_ctc 7.971683 loss_rnnt 5.518067 hw_loss 0.194959 history loss 6.290323 rank 7
2023-02-25 13:51:02,362 DEBUG CV Batch 28/1300 loss 4.807854 loss_att 5.251236 loss_ctc 6.730029 loss_rnnt 4.338064 hw_loss 0.234044 history loss 6.586684 rank 6
2023-02-25 13:51:02,589 DEBUG CV Batch 28/1300 loss 4.807854 loss_att 5.251236 loss_ctc 6.730029 loss_rnnt 4.338064 hw_loss 0.234044 history loss 6.586684 rank 4
2023-02-25 13:51:03,187 DEBUG CV Batch 28/1300 loss 4.807854 loss_att 5.251236 loss_ctc 6.730029 loss_rnnt 4.338064 hw_loss 0.234044 history loss 6.586684 rank 5
2023-02-25 13:51:03,932 DEBUG CV Batch 28/1300 loss 4.807854 loss_att 5.251236 loss_ctc 6.730029 loss_rnnt 4.338064 hw_loss 0.234044 history loss 6.586684 rank 3
2023-02-25 13:51:08,094 DEBUG CV Batch 28/1300 loss 4.807854 loss_att 5.251236 loss_ctc 6.730029 loss_rnnt 4.338064 hw_loss 0.234044 history loss 6.586684 rank 1
2023-02-25 13:51:09,464 DEBUG CV Batch 28/1300 loss 4.807854 loss_att 5.251236 loss_ctc 6.730029 loss_rnnt 4.338064 hw_loss 0.234044 history loss 6.586684 rank 0
2023-02-25 13:51:10,468 DEBUG CV Batch 28/1300 loss 4.807854 loss_att 5.251236 loss_ctc 6.730029 loss_rnnt 4.338064 hw_loss 0.234044 history loss 6.586684 rank 2
2023-02-25 13:51:11,530 DEBUG CV Batch 28/1300 loss 4.807854 loss_att 5.251236 loss_ctc 6.730029 loss_rnnt 4.338064 hw_loss 0.234044 history loss 6.586684 rank 7
2023-02-25 13:51:13,532 DEBUG CV Batch 28/1400 loss 5.085224 loss_att 15.318326 loss_ctc 3.700315 loss_rnnt 3.129022 hw_loss 0.176691 history loss 6.884346 rank 6
2023-02-25 13:51:13,746 DEBUG CV Batch 28/1400 loss 5.085224 loss_att 15.318326 loss_ctc 3.700315 loss_rnnt 3.129022 hw_loss 0.176691 history loss 6.884346 rank 4
2023-02-25 13:51:14,811 DEBUG CV Batch 28/1400 loss 5.085224 loss_att 15.318326 loss_ctc 3.700315 loss_rnnt 3.129022 hw_loss 0.176691 history loss 6.884346 rank 5
2023-02-25 13:51:15,392 DEBUG CV Batch 28/1400 loss 5.085224 loss_att 15.318326 loss_ctc 3.700315 loss_rnnt 3.129022 hw_loss 0.176691 history loss 6.884346 rank 3
2023-02-25 13:51:19,779 DEBUG CV Batch 28/1400 loss 5.085224 loss_att 15.318326 loss_ctc 3.700315 loss_rnnt 3.129022 hw_loss 0.176691 history loss 6.884346 rank 1
2023-02-25 13:51:21,408 DEBUG CV Batch 28/1400 loss 5.085224 loss_att 15.318326 loss_ctc 3.700315 loss_rnnt 3.129022 hw_loss 0.176691 history loss 6.884346 rank 0
2023-02-25 13:51:22,349 DEBUG CV Batch 28/1400 loss 5.085224 loss_att 15.318326 loss_ctc 3.700315 loss_rnnt 3.129022 hw_loss 0.176691 history loss 6.884346 rank 2
2023-02-25 13:51:23,654 DEBUG CV Batch 28/1400 loss 5.085224 loss_att 15.318326 loss_ctc 3.700315 loss_rnnt 3.129022 hw_loss 0.176691 history loss 6.884346 rank 7
2023-02-25 13:51:26,091 DEBUG CV Batch 28/1500 loss 7.023534 loss_att 9.265528 loss_ctc 8.066520 loss_rnnt 6.324881 hw_loss 0.208481 history loss 6.729242 rank 6
2023-02-25 13:51:26,463 DEBUG CV Batch 28/1500 loss 7.023534 loss_att 9.265528 loss_ctc 8.066520 loss_rnnt 6.324881 hw_loss 0.208481 history loss 6.729242 rank 4
2023-02-25 13:51:27,105 DEBUG CV Batch 28/1500 loss 7.023534 loss_att 9.265528 loss_ctc 8.066520 loss_rnnt 6.324881 hw_loss 0.208481 history loss 6.729242 rank 5
2023-02-25 13:51:27,936 DEBUG CV Batch 28/1500 loss 7.023534 loss_att 9.265528 loss_ctc 8.066520 loss_rnnt 6.324881 hw_loss 0.208481 history loss 6.729242 rank 3
2023-02-25 13:51:31,858 DEBUG CV Batch 28/1500 loss 7.023534 loss_att 9.265528 loss_ctc 8.066520 loss_rnnt 6.324881 hw_loss 0.208481 history loss 6.729242 rank 1
2023-02-25 13:51:33,476 DEBUG CV Batch 28/1500 loss 7.023534 loss_att 9.265528 loss_ctc 8.066520 loss_rnnt 6.324881 hw_loss 0.208481 history loss 6.729242 rank 0
2023-02-25 13:51:34,450 DEBUG CV Batch 28/1500 loss 7.023534 loss_att 9.265528 loss_ctc 8.066520 loss_rnnt 6.324881 hw_loss 0.208481 history loss 6.729242 rank 2
2023-02-25 13:51:35,679 DEBUG CV Batch 28/1500 loss 7.023534 loss_att 9.265528 loss_ctc 8.066520 loss_rnnt 6.324881 hw_loss 0.208481 history loss 6.729242 rank 7
2023-02-25 13:51:39,629 DEBUG CV Batch 28/1600 loss 6.016449 loss_att 12.591135 loss_ctc 7.166884 loss_rnnt 4.420522 hw_loss 0.239249 history loss 6.670511 rank 6
2023-02-25 13:51:40,218 DEBUG CV Batch 28/1600 loss 6.016449 loss_att 12.591135 loss_ctc 7.166884 loss_rnnt 4.420522 hw_loss 0.239249 history loss 6.670511 rank 4
2023-02-25 13:51:40,742 DEBUG CV Batch 28/1600 loss 6.016449 loss_att 12.591135 loss_ctc 7.166884 loss_rnnt 4.420522 hw_loss 0.239249 history loss 6.670511 rank 5
2023-02-25 13:51:41,749 DEBUG CV Batch 28/1600 loss 6.016449 loss_att 12.591135 loss_ctc 7.166884 loss_rnnt 4.420522 hw_loss 0.239249 history loss 6.670511 rank 3
2023-02-25 13:51:45,371 DEBUG CV Batch 28/1600 loss 6.016449 loss_att 12.591135 loss_ctc 7.166884 loss_rnnt 4.420522 hw_loss 0.239249 history loss 6.670511 rank 1
2023-02-25 13:51:47,221 DEBUG CV Batch 28/1600 loss 6.016449 loss_att 12.591135 loss_ctc 7.166884 loss_rnnt 4.420522 hw_loss 0.239249 history loss 6.670511 rank 0
2023-02-25 13:51:48,104 DEBUG CV Batch 28/1600 loss 6.016449 loss_att 12.591135 loss_ctc 7.166884 loss_rnnt 4.420522 hw_loss 0.239249 history loss 6.670511 rank 2
2023-02-25 13:51:49,469 DEBUG CV Batch 28/1600 loss 6.016449 loss_att 12.591135 loss_ctc 7.166884 loss_rnnt 4.420522 hw_loss 0.239249 history loss 6.670511 rank 7
2023-02-25 13:51:52,193 DEBUG CV Batch 28/1700 loss 7.336691 loss_att 6.977326 loss_ctc 12.446252 loss_rnnt 6.607542 hw_loss 0.224526 history loss 6.579886 rank 6
2023-02-25 13:51:52,453 DEBUG CV Batch 28/1700 loss 7.336691 loss_att 6.977326 loss_ctc 12.446252 loss_rnnt 6.607542 hw_loss 0.224526 history loss 6.579886 rank 4
2023-02-25 13:51:53,482 DEBUG CV Batch 28/1700 loss 7.336691 loss_att 6.977326 loss_ctc 12.446252 loss_rnnt 6.607542 hw_loss 0.224526 history loss 6.579886 rank 5
2023-02-25 13:51:54,226 DEBUG CV Batch 28/1700 loss 7.336691 loss_att 6.977326 loss_ctc 12.446252 loss_rnnt 6.607542 hw_loss 0.224526 history loss 6.579886 rank 3
2023-02-25 13:51:58,185 DEBUG CV Batch 28/1700 loss 7.336691 loss_att 6.977326 loss_ctc 12.446252 loss_rnnt 6.607542 hw_loss 0.224526 history loss 6.579886 rank 1
2023-02-25 13:51:59,905 DEBUG CV Batch 28/1700 loss 7.336691 loss_att 6.977326 loss_ctc 12.446252 loss_rnnt 6.607542 hw_loss 0.224526 history loss 6.579886 rank 0
2023-02-25 13:52:00,739 DEBUG CV Batch 28/1700 loss 7.336691 loss_att 6.977326 loss_ctc 12.446252 loss_rnnt 6.607542 hw_loss 0.224526 history loss 6.579886 rank 2
2023-02-25 13:52:01,583 INFO Epoch 28 CV info cv_loss 6.558895650801501
2023-02-25 13:52:01,583 INFO Epoch 29 TRAIN info lr 0.00032156701450158535
2023-02-25 13:52:01,585 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 13:52:01,827 INFO Epoch 28 CV info cv_loss 6.558895651432524
2023-02-25 13:52:01,828 INFO Epoch 29 TRAIN info lr 0.0003215025255151381
2023-02-25 13:52:01,830 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 13:52:02,125 DEBUG CV Batch 28/1700 loss 7.336691 loss_att 6.977326 loss_ctc 12.446252 loss_rnnt 6.607542 hw_loss 0.224526 history loss 6.579886 rank 7
2023-02-25 13:52:02,936 INFO Epoch 28 CV info cv_loss 6.558895653120992
2023-02-25 13:52:02,937 INFO Epoch 29 TRAIN info lr 0.000321523131178785
2023-02-25 13:52:02,942 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 13:52:03,586 INFO Epoch 28 CV info cv_loss 6.558895652720411
2023-02-25 13:52:03,586 INFO Epoch 29 TRAIN info lr 0.0003215370921053896
2023-02-25 13:52:03,588 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 13:52:07,283 INFO Epoch 28 CV info cv_loss 6.558895652589038
2023-02-25 13:52:07,284 INFO Epoch 29 TRAIN info lr 0.0003215729999833401
2023-02-25 13:52:07,286 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 13:52:09,115 INFO Epoch 28 CV info cv_loss 6.55889565253735
2023-02-25 13:52:09,116 INFO Checkpoint: save to checkpoint exp/2_21_rnnt_bias_loss_2_class_3word_finetune/28.pt
2023-02-25 13:52:09,686 INFO Epoch 29 TRAIN info lr 0.0003214673056520657
2023-02-25 13:52:09,690 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 13:52:10,122 INFO Epoch 28 CV info cv_loss 6.558895650226475
2023-02-25 13:52:10,122 INFO Epoch 29 TRAIN info lr 0.0003215291142103642
2023-02-25 13:52:10,124 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 13:52:11,720 INFO Epoch 28 CV info cv_loss 6.558895653112376
2023-02-25 13:52:11,721 INFO Epoch 29 TRAIN info lr 0.0003214819238126896
2023-02-25 13:52:11,726 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-25 13:53:13,835 DEBUG TRAIN Batch 29/0 loss 4.838772 loss_att 5.347379 loss_ctc 6.295014 loss_rnnt 4.416560 hw_loss 0.236859 lr 0.00032147 rank 0
2023-02-25 13:53:13,835 DEBUG TRAIN Batch 29/0 loss 7.710325 loss_att 7.694957 loss_ctc 10.343769 loss_rnnt 7.244061 hw_loss 0.221645 lr 0.00032157 rank 6
2023-02-25 13:53:13,837 DEBUG TRAIN Batch 29/0 loss 6.245434 loss_att 6.608129 loss_ctc 8.362263 loss_rnnt 5.733412 hw_loss 0.294824 lr 0.00032157 rank 1
2023-02-25 13:53:13,837 DEBUG TRAIN Batch 29/0 loss 7.508910 loss_att 7.112015 loss_ctc 10.498067 loss_rnnt 7.076387 hw_loss 0.212528 lr 0.00032150 rank 4
2023-02-25 13:53:13,838 DEBUG TRAIN Batch 29/0 loss 6.574333 loss_att 6.926351 loss_ctc 10.977930 loss_rnnt 5.768634 hw_loss 0.277781 lr 0.00032154 rank 3
2023-02-25 13:53:13,840 DEBUG TRAIN Batch 29/0 loss 7.348646 loss_att 7.380846 loss_ctc 10.246503 loss_rnnt 6.809573 hw_loss 0.274222 lr 0.00032153 rank 2
2023-02-25 13:53:13,850 DEBUG TRAIN Batch 29/0 loss 6.748385 loss_att 6.929683 loss_ctc 9.640038 loss_rnnt 6.220083 hw_loss 0.199668 lr 0.00032148 rank 7
2023-02-25 13:53:13,887 DEBUG TRAIN Batch 29/0 loss 4.298684 loss_att 4.795521 loss_ctc 5.367117 loss_rnnt 3.959054 hw_loss 0.183385 lr 0.00032152 rank 5
2023-02-25 13:54:24,456 DEBUG TRAIN Batch 29/100 loss 5.753039 loss_att 10.362003 loss_ctc 7.587699 loss_rnnt 4.541240 hw_loss 0.085099 lr 0.00032140 rank 0
2023-02-25 13:54:24,458 DEBUG TRAIN Batch 29/100 loss 5.799452 loss_att 10.480567 loss_ctc 13.866325 loss_rnnt 3.707767 hw_loss 0.149772 lr 0.00032141 rank 7
2023-02-25 13:54:24,461 DEBUG TRAIN Batch 29/100 loss 8.000109 loss_att 11.300707 loss_ctc 12.131109 loss_rnnt 6.724363 hw_loss 0.121549 lr 0.00032150 rank 6
2023-02-25 13:54:24,463 DEBUG TRAIN Batch 29/100 loss 4.423176 loss_att 6.963218 loss_ctc 6.563370 loss_rnnt 3.449993 hw_loss 0.337153 lr 0.00032151 rank 1
2023-02-25 13:54:24,465 DEBUG TRAIN Batch 29/100 loss 7.138650 loss_att 9.633589 loss_ctc 9.895252 loss_rnnt 6.161018 hw_loss 0.208306 lr 0.00032147 rank 3
2023-02-25 13:54:24,467 DEBUG TRAIN Batch 29/100 loss 7.945344 loss_att 13.307981 loss_ctc 12.721374 loss_rnnt 6.071311 hw_loss 0.308816 lr 0.00032146 rank 2
2023-02-25 13:54:24,472 DEBUG TRAIN Batch 29/100 loss 5.796776 loss_att 9.981907 loss_ctc 11.648559 loss_rnnt 4.113667 hw_loss 0.123461 lr 0.00032144 rank 4
2023-02-25 13:54:24,474 DEBUG TRAIN Batch 29/100 loss 10.154372 loss_att 14.554894 loss_ctc 18.342693 loss_rnnt 8.077878 hw_loss 0.196150 lr 0.00032146 rank 5
2023-02-25 13:55:34,328 DEBUG TRAIN Batch 29/200 loss 7.273761 loss_att 9.985817 loss_ctc 9.931846 loss_rnnt 6.281843 hw_loss 0.178303 lr 0.00032144 rank 1
2023-02-25 13:55:34,330 DEBUG TRAIN Batch 29/200 loss 5.542243 loss_att 8.733562 loss_ctc 6.718782 loss_rnnt 4.681050 hw_loss 0.123856 lr 0.00032133 rank 0
2023-02-25 13:55:34,333 DEBUG TRAIN Batch 29/200 loss 8.211876 loss_att 8.920015 loss_ctc 15.476340 loss_rnnt 7.038875 hw_loss 0.117706 lr 0.00032143 rank 6
2023-02-25 13:55:34,336 DEBUG TRAIN Batch 29/200 loss 4.922058 loss_att 7.296413 loss_ctc 6.309938 loss_rnnt 4.133283 hw_loss 0.241600 lr 0.00032140 rank 2
2023-02-25 13:55:34,337 DEBUG TRAIN Batch 29/200 loss 9.860041 loss_att 9.700168 loss_ctc 13.963608 loss_rnnt 9.298414 hw_loss 0.087108 lr 0.00032135 rank 7
2023-02-25 13:55:34,340 DEBUG TRAIN Batch 29/200 loss 10.387694 loss_att 12.747862 loss_ctc 18.429811 loss_rnnt 8.781769 hw_loss 0.115519 lr 0.00032140 rank 3
2023-02-25 13:55:34,341 DEBUG TRAIN Batch 29/200 loss 13.125148 loss_att 12.345375 loss_ctc 22.843639 loss_rnnt 11.889069 hw_loss 0.180440 lr 0.00032139 rank 5
2023-02-25 13:55:34,341 DEBUG TRAIN Batch 29/200 loss 16.837597 loss_att 17.867050 loss_ctc 23.685379 loss_rnnt 15.660460 hw_loss 0.109137 lr 0.00032137 rank 4
2023-02-25 13:56:45,891 DEBUG TRAIN Batch 29/300 loss 10.564119 loss_att 12.836866 loss_ctc 16.066845 loss_rnnt 9.263495 hw_loss 0.210708 lr 0.00032133 rank 2
2023-02-25 13:56:45,892 DEBUG TRAIN Batch 29/300 loss 9.822837 loss_att 12.486619 loss_ctc 14.140001 loss_rnnt 8.652307 hw_loss 0.116534 lr 0.00032137 rank 6
2023-02-25 13:56:45,892 DEBUG TRAIN Batch 29/300 loss 13.849617 loss_att 16.406103 loss_ctc 21.042641 loss_rnnt 12.255533 hw_loss 0.231967 lr 0.00032132 rank 5
2023-02-25 13:56:45,895 DEBUG TRAIN Batch 29/300 loss 13.926311 loss_att 18.993015 loss_ctc 19.644176 loss_rnnt 12.041436 hw_loss 0.204658 lr 0.00032137 rank 1
2023-02-25 13:56:45,895 DEBUG TRAIN Batch 29/300 loss 13.471194 loss_att 16.159016 loss_ctc 18.027857 loss_rnnt 12.230961 hw_loss 0.178339 lr 0.00032130 rank 4
2023-02-25 13:56:45,920 DEBUG TRAIN Batch 29/300 loss 2.072699 loss_att 5.150179 loss_ctc 4.094555 loss_rnnt 1.076904 hw_loss 0.207596 lr 0.00032127 rank 0
2023-02-25 13:56:45,925 DEBUG TRAIN Batch 29/300 loss 3.697940 loss_att 6.079112 loss_ctc 6.988382 loss_rnnt 2.708309 hw_loss 0.140008 lr 0.00032128 rank 7
2023-02-25 13:56:45,942 DEBUG TRAIN Batch 29/300 loss 6.268658 loss_att 9.126648 loss_ctc 10.113216 loss_rnnt 5.079927 hw_loss 0.195985 lr 0.00032134 rank 3
2023-02-25 13:57:58,036 DEBUG TRAIN Batch 29/400 loss 5.983417 loss_att 10.739321 loss_ctc 8.557820 loss_rnnt 4.509706 hw_loss 0.336146 lr 0.00032130 rank 6
2023-02-25 13:57:58,038 DEBUG TRAIN Batch 29/400 loss 4.484895 loss_att 6.490345 loss_ctc 7.428990 loss_rnnt 3.557983 hw_loss 0.249892 lr 0.00032120 rank 0
2023-02-25 13:57:58,040 DEBUG TRAIN Batch 29/400 loss 8.613764 loss_att 12.133924 loss_ctc 12.939295 loss_rnnt 7.282202 hw_loss 0.095235 lr 0.00032127 rank 3
2023-02-25 13:57:58,042 DEBUG TRAIN Batch 29/400 loss 7.676807 loss_att 11.824280 loss_ctc 11.253030 loss_rnnt 6.283530 hw_loss 0.163037 lr 0.00032126 rank 2
2023-02-25 13:57:58,042 DEBUG TRAIN Batch 29/400 loss 6.265293 loss_att 8.631068 loss_ctc 8.504669 loss_rnnt 5.396123 hw_loss 0.182682 lr 0.00032122 rank 7
2023-02-25 13:57:58,041 DEBUG TRAIN Batch 29/400 loss 12.385277 loss_att 15.799896 loss_ctc 17.017179 loss_rnnt 10.977122 hw_loss 0.201831 lr 0.00032131 rank 1
2023-02-25 13:57:58,045 DEBUG TRAIN Batch 29/400 loss 5.920397 loss_att 7.219460 loss_ctc 6.432096 loss_rnnt 5.511380 hw_loss 0.151832 lr 0.00032124 rank 4
2023-02-25 13:57:58,049 DEBUG TRAIN Batch 29/400 loss 7.634381 loss_att 9.534044 loss_ctc 10.199368 loss_rnnt 6.820627 hw_loss 0.172167 lr 0.00032126 rank 5
2023-02-25 13:59:08,518 DEBUG TRAIN Batch 29/500 loss 5.050066 loss_att 7.947166 loss_ctc 8.778496 loss_rnnt 3.833125 hw_loss 0.263243 lr 0.00032113 rank 0
2023-02-25 13:59:08,522 DEBUG TRAIN Batch 29/500 loss 12.784164 loss_att 13.812201 loss_ctc 19.955519 loss_rnnt 11.532966 hw_loss 0.167646 lr 0.00032115 rank 7
2023-02-25 13:59:08,526 DEBUG TRAIN Batch 29/500 loss 9.693418 loss_att 11.432853 loss_ctc 15.589197 loss_rnnt 8.457031 hw_loss 0.191991 lr 0.00032123 rank 6
2023-02-25 13:59:08,526 DEBUG TRAIN Batch 29/500 loss 2.760094 loss_att 6.607107 loss_ctc 6.483950 loss_rnnt 1.395747 hw_loss 0.184557 lr 0.00032117 rank 4
2023-02-25 13:59:08,527 DEBUG TRAIN Batch 29/500 loss 1.720212 loss_att 3.551263 loss_ctc 2.600624 loss_rnnt 1.160936 hw_loss 0.141896 lr 0.00032124 rank 1
2023-02-25 13:59:08,529 DEBUG TRAIN Batch 29/500 loss 9.032520 loss_att 10.674940 loss_ctc 13.008816 loss_rnnt 8.081437 hw_loss 0.173300 lr 0.00032120 rank 3
2023-02-25 13:59:08,529 DEBUG TRAIN Batch 29/500 loss 3.987359 loss_att 6.155289 loss_ctc 5.591367 loss_rnnt 3.287465 hw_loss 0.098327 lr 0.00032120 rank 2
2023-02-25 13:59:08,534 DEBUG TRAIN Batch 29/500 loss 6.207229 loss_att 7.641749 loss_ctc 8.563417 loss_rnnt 5.494545 hw_loss 0.209288 lr 0.00032119 rank 5
2023-02-25 14:00:19,326 DEBUG TRAIN Batch 29/600 loss 8.468852 loss_att 10.070468 loss_ctc 9.908093 loss_rnnt 7.853286 hw_loss 0.193770 lr 0.00032114 rank 3
2023-02-25 14:00:19,328 DEBUG TRAIN Batch 29/600 loss 4.991493 loss_att 7.203135 loss_ctc 8.943837 loss_rnnt 3.864371 hw_loss 0.295902 lr 0.00032107 rank 0
2023-02-25 14:00:19,331 DEBUG TRAIN Batch 29/600 loss 8.581604 loss_att 7.932634 loss_ctc 11.591652 loss_rnnt 8.178177 hw_loss 0.247277 lr 0.00032112 rank 5
2023-02-25 14:00:19,331 DEBUG TRAIN Batch 29/600 loss 3.606491 loss_att 5.596893 loss_ctc 5.496298 loss_rnnt 2.810081 hw_loss 0.274417 lr 0.00032117 rank 1
2023-02-25 14:00:19,331 DEBUG TRAIN Batch 29/600 loss 9.756315 loss_att 9.235929 loss_ctc 11.875615 loss_rnnt 9.476562 hw_loss 0.189858 lr 0.00032117 rank 6
2023-02-25 14:00:19,332 DEBUG TRAIN Batch 29/600 loss 6.409091 loss_att 7.458926 loss_ctc 11.010612 loss_rnnt 5.466895 hw_loss 0.222549 lr 0.00032108 rank 7
2023-02-25 14:00:19,339 DEBUG TRAIN Batch 29/600 loss 9.042418 loss_att 10.252037 loss_ctc 13.473921 loss_rnnt 8.101069 hw_loss 0.203548 lr 0.00032113 rank 2
2023-02-25 14:00:19,384 DEBUG TRAIN Batch 29/600 loss 9.886217 loss_att 11.619073 loss_ctc 11.144618 loss_rnnt 9.222811 hw_loss 0.279467 lr 0.00032110 rank 4
2023-02-25 14:01:31,444 DEBUG TRAIN Batch 29/700 loss 3.598118 loss_att 5.468307 loss_ctc 5.813676 loss_rnnt 2.832578 hw_loss 0.180176 lr 0.00032106 rank 5
2023-02-25 14:01:31,445 DEBUG TRAIN Batch 29/700 loss 6.724039 loss_att 12.285168 loss_ctc 14.312925 loss_rnnt 4.550642 hw_loss 0.092472 lr 0.00032104 rank 4
2023-02-25 14:01:31,446 DEBUG TRAIN Batch 29/700 loss 6.800028 loss_att 7.752270 loss_ctc 12.370455 loss_rnnt 5.767035 hw_loss 0.187165 lr 0.00032102 rank 7
2023-02-25 14:01:31,447 DEBUG TRAIN Batch 29/700 loss 11.348275 loss_att 14.770132 loss_ctc 17.043512 loss_rnnt 9.775977 hw_loss 0.241054 lr 0.00032106 rank 2
2023-02-25 14:01:31,447 DEBUG TRAIN Batch 29/700 loss 6.907992 loss_att 9.269217 loss_ctc 10.293643 loss_rnnt 5.862332 hw_loss 0.228737 lr 0.00032107 rank 3
2023-02-25 14:01:31,449 DEBUG TRAIN Batch 29/700 loss 2.433736 loss_att 4.924356 loss_ctc 4.387642 loss_rnnt 1.626028 hw_loss 0.091993 lr 0.00032111 rank 1
2023-02-25 14:01:31,471 DEBUG TRAIN Batch 29/700 loss 10.677307 loss_att 15.256490 loss_ctc 21.726753 loss_rnnt 8.180769 hw_loss 0.201453 lr 0.00032100 rank 0
2023-02-25 14:01:31,493 DEBUG TRAIN Batch 29/700 loss 6.609548 loss_att 11.402223 loss_ctc 11.953788 loss_rnnt 4.860785 hw_loss 0.145619 lr 0.00032110 rank 6
2023-02-25 14:02:42,302 DEBUG TRAIN Batch 29/800 loss 11.350496 loss_att 16.782623 loss_ctc 16.357914 loss_rnnt 9.478125 hw_loss 0.221795 lr 0.00032104 rank 6
2023-02-25 14:02:42,302 DEBUG TRAIN Batch 29/800 loss 2.870680 loss_att 7.583878 loss_ctc 6.229835 loss_rnnt 1.358234 hw_loss 0.228597 lr 0.00032104 rank 1
2023-02-25 14:02:42,303 DEBUG TRAIN Batch 29/800 loss 4.728802 loss_att 9.510803 loss_ctc 7.876619 loss_rnnt 3.276047 hw_loss 0.143710 lr 0.00032101 rank 3
2023-02-25 14:02:42,304 DEBUG TRAIN Batch 29/800 loss 2.997553 loss_att 5.659439 loss_ctc 4.093030 loss_rnnt 2.240352 hw_loss 0.147676 lr 0.00032095 rank 7
2023-02-25 14:02:42,308 DEBUG TRAIN Batch 29/800 loss 6.193190 loss_att 10.827406 loss_ctc 12.001945 loss_rnnt 4.370411 hw_loss 0.227690 lr 0.00032100 rank 2
2023-02-25 14:02:42,309 DEBUG TRAIN Batch 29/800 loss 4.855097 loss_att 9.010887 loss_ctc 8.379987 loss_rnnt 3.414902 hw_loss 0.260724 lr 0.00032097 rank 4
2023-02-25 14:02:42,308 DEBUG TRAIN Batch 29/800 loss 1.839731 loss_att 5.741420 loss_ctc 3.900465 loss_rnnt 0.708879 hw_loss 0.142030 lr 0.00032094 rank 0
2023-02-25 14:02:42,311 DEBUG TRAIN Batch 29/800 loss 3.924583 loss_att 6.178157 loss_ctc 7.257503 loss_rnnt 2.876490 hw_loss 0.286854 lr 0.00032099 rank 5
2023-02-25 14:03:51,752 DEBUG TRAIN Batch 29/900 loss 10.882260 loss_att 12.438988 loss_ctc 17.826666 loss_rnnt 9.506710 hw_loss 0.259283 lr 0.00032094 rank 3
2023-02-25 14:03:51,757 DEBUG TRAIN Batch 29/900 loss 4.477790 loss_att 6.430434 loss_ctc 4.717959 loss_rnnt 3.971462 hw_loss 0.157082 lr 0.00032093 rank 2
2023-02-25 14:03:51,758 DEBUG TRAIN Batch 29/900 loss 11.505396 loss_att 12.455235 loss_ctc 15.234807 loss_rnnt 10.679420 hw_loss 0.260162 lr 0.00032098 rank 1
2023-02-25 14:03:51,760 DEBUG TRAIN Batch 29/900 loss 4.597099 loss_att 7.641656 loss_ctc 8.041655 loss_rnnt 3.424157 hw_loss 0.196419 lr 0.00032091 rank 4
2023-02-25 14:03:51,760 DEBUG TRAIN Batch 29/900 loss 3.109790 loss_att 4.923606 loss_ctc 5.186201 loss_rnnt 2.389471 hw_loss 0.151315 lr 0.00032087 rank 0
2023-02-25 14:03:51,762 DEBUG TRAIN Batch 29/900 loss 6.832262 loss_att 9.832794 loss_ctc 9.839527 loss_rnnt 5.773651 hw_loss 0.107880 lr 0.00032097 rank 6
2023-02-25 14:03:51,762 DEBUG TRAIN Batch 29/900 loss 4.580439 loss_att 7.797587 loss_ctc 7.616249 loss_rnnt 3.387778 hw_loss 0.270857 lr 0.00032088 rank 7
2023-02-25 14:03:51,770 DEBUG TRAIN Batch 29/900 loss 5.279018 loss_att 7.370797 loss_ctc 8.446576 loss_rnnt 4.325739 hw_loss 0.211093 lr 0.00032093 rank 5
2023-02-25 14:05:02,506 DEBUG TRAIN Batch 29/1000 loss 15.997241 loss_att 18.343971 loss_ctc 26.236794 loss_rnnt 14.046888 hw_loss 0.217001 lr 0.00032087 rank 3
2023-02-25 14:05:02,508 DEBUG TRAIN Batch 29/1000 loss 2.035465 loss_att 5.858202 loss_ctc 2.304378 loss_rnnt 1.187722 hw_loss 0.088765 lr 0.00032084 rank 4
2023-02-25 14:05:02,510 DEBUG TRAIN Batch 29/1000 loss 18.641769 loss_att 20.700212 loss_ctc 24.483015 loss_rnnt 17.369013 hw_loss 0.154189 lr 0.00032091 rank 1
2023-02-25 14:05:02,515 DEBUG TRAIN Batch 29/1000 loss 6.202807 loss_att 9.174024 loss_ctc 11.181234 loss_rnnt 4.827595 hw_loss 0.219709 lr 0.00032080 rank 0
2023-02-25 14:05:02,518 DEBUG TRAIN Batch 29/1000 loss 11.203414 loss_att 13.565588 loss_ctc 18.040737 loss_rnnt 9.696651 hw_loss 0.230033 lr 0.00032087 rank 2
2023-02-25 14:05:02,518 DEBUG TRAIN Batch 29/1000 loss 4.864432 loss_att 7.264735 loss_ctc 7.844666 loss_rnnt 3.912340 hw_loss 0.140000 lr 0.00032082 rank 7
2023-02-25 14:05:02,553 DEBUG TRAIN Batch 29/1000 loss 10.638712 loss_att 13.155266 loss_ctc 16.942696 loss_rnnt 9.128996 hw_loss 0.311011 lr 0.00032090 rank 6
2023-02-25 14:05:02,555 DEBUG TRAIN Batch 29/1000 loss 4.077175 loss_att 6.092415 loss_ctc 5.806392 loss_rnnt 3.284526 hw_loss 0.298198 lr 0.00032086 rank 5
2023-02-25 14:06:14,909 DEBUG TRAIN Batch 29/1100 loss 8.714697 loss_att 10.287134 loss_ctc 10.947496 loss_rnnt 7.988335 hw_loss 0.214068 lr 0.00032084 rank 6
2023-02-25 14:06:14,916 DEBUG TRAIN Batch 29/1100 loss 5.828369 loss_att 7.568621 loss_ctc 8.467991 loss_rnnt 5.022919 hw_loss 0.197717 lr 0.00032077 rank 4
2023-02-25 14:06:14,918 DEBUG TRAIN Batch 29/1100 loss 10.463162 loss_att 12.224403 loss_ctc 13.448689 loss_rnnt 9.610039 hw_loss 0.192759 lr 0.00032079 rank 5
2023-02-25 14:06:14,919 DEBUG TRAIN Batch 29/1100 loss 3.207626 loss_att 5.602763 loss_ctc 6.088743 loss_rnnt 2.267781 hw_loss 0.143754 lr 0.00032080 rank 2
2023-02-25 14:06:14,918 DEBUG TRAIN Batch 29/1100 loss 9.679379 loss_att 12.790615 loss_ctc 14.645826 loss_rnnt 8.197364 hw_loss 0.370456 lr 0.00032081 rank 3
2023-02-25 14:06:14,942 DEBUG TRAIN Batch 29/1100 loss 4.832756 loss_att 7.799879 loss_ctc 6.766088 loss_rnnt 3.912394 hw_loss 0.129672 lr 0.00032075 rank 7
2023-02-25 14:06:14,946 DEBUG TRAIN Batch 29/1100 loss 7.606320 loss_att 10.998376 loss_ctc 10.231220 loss_rnnt 6.506191 hw_loss 0.134496 lr 0.00032084 rank 1
2023-02-25 14:06:15,318 DEBUG TRAIN Batch 29/1100 loss 9.928148 loss_att 13.145140 loss_ctc 19.935999 loss_rnnt 7.853081 hw_loss 0.182417 lr 0.00032074 rank 0
2023-02-25 14:07:26,252 DEBUG TRAIN Batch 29/1200 loss 6.299406 loss_att 8.065495 loss_ctc 8.813840 loss_rnnt 5.523539 hw_loss 0.163857 lr 0.00032078 rank 1
2023-02-25 14:07:26,254 DEBUG TRAIN Batch 29/1200 loss 12.858148 loss_att 13.850079 loss_ctc 16.164234 loss_rnnt 12.044313 hw_loss 0.327443 lr 0.00032077 rank 6
2023-02-25 14:07:26,254 DEBUG TRAIN Batch 29/1200 loss 8.797701 loss_att 11.407267 loss_ctc 13.810770 loss_rnnt 7.491576 hw_loss 0.217131 lr 0.00032073 rank 2
2023-02-25 14:07:26,255 DEBUG TRAIN Batch 29/1200 loss 7.214678 loss_att 11.150893 loss_ctc 10.918729 loss_rnnt 5.823066 hw_loss 0.207178 lr 0.00032067 rank 0
2023-02-25 14:07:26,255 DEBUG TRAIN Batch 29/1200 loss 5.754197 loss_att 8.479671 loss_ctc 7.537927 loss_rnnt 4.909749 hw_loss 0.115355 lr 0.00032074 rank 3
2023-02-25 14:07:26,259 DEBUG TRAIN Batch 29/1200 loss 12.736501 loss_att 14.668459 loss_ctc 18.361679 loss_rnnt 11.509852 hw_loss 0.169187 lr 0.00032069 rank 7
2023-02-25 14:07:26,261 DEBUG TRAIN Batch 29/1200 loss 8.579731 loss_att 8.759236 loss_ctc 11.924863 loss_rnnt 8.001099 hw_loss 0.181339 lr 0.00032073 rank 5
2023-02-25 14:07:26,263 DEBUG TRAIN Batch 29/1200 loss 4.989660 loss_att 6.952789 loss_ctc 6.142328 loss_rnnt 4.328063 hw_loss 0.216155 lr 0.00032071 rank 4
2023-02-25 14:08:37,524 DEBUG TRAIN Batch 29/1300 loss 4.551089 loss_att 8.287758 loss_ctc 8.629440 loss_rnnt 3.190723 hw_loss 0.129847 lr 0.00032062 rank 7
2023-02-25 14:08:37,538 DEBUG TRAIN Batch 29/1300 loss 8.462838 loss_att 8.461620 loss_ctc 11.263130 loss_rnnt 7.941360 hw_loss 0.278153 lr 0.00032061 rank 0
2023-02-25 14:08:37,539 DEBUG TRAIN Batch 29/1300 loss 5.362888 loss_att 6.416774 loss_ctc 7.244349 loss_rnnt 4.745350 hw_loss 0.292311 lr 0.00032068 rank 3
2023-02-25 14:08:37,543 DEBUG TRAIN Batch 29/1300 loss 2.423726 loss_att 7.056166 loss_ctc 3.292495 loss_rnnt 1.283533 hw_loss 0.183505 lr 0.00032067 rank 2
2023-02-25 14:08:37,543 DEBUG TRAIN Batch 29/1300 loss 10.310995 loss_att 11.375420 loss_ctc 13.689568 loss_rnnt 9.568737 hw_loss 0.147932 lr 0.00032066 rank 5
2023-02-25 14:08:37,544 DEBUG TRAIN Batch 29/1300 loss 1.171709 loss_att 3.902143 loss_ctc 2.751626 loss_rnnt 0.397668 hw_loss 0.032434 lr 0.00032064 rank 4
2023-02-25 14:08:37,545 DEBUG TRAIN Batch 29/1300 loss 10.951537 loss_att 15.478191 loss_ctc 17.862453 loss_rnnt 8.985814 hw_loss 0.260506 lr 0.00032071 rank 1
2023-02-25 14:08:37,589 DEBUG TRAIN Batch 29/1300 loss 6.380483 loss_att 8.367374 loss_ctc 7.265628 loss_rnnt 5.811765 hw_loss 0.099974 lr 0.00032071 rank 6
2023-02-25 14:09:50,262 DEBUG TRAIN Batch 29/1400 loss 2.346665 loss_att 4.846213 loss_ctc 4.240180 loss_rnnt 1.480789 hw_loss 0.212808 lr 0.00032065 rank 1
2023-02-25 14:09:50,267 DEBUG TRAIN Batch 29/1400 loss 5.008808 loss_att 9.329275 loss_ctc 6.051165 loss_rnnt 3.910018 hw_loss 0.179468 lr 0.00032058 rank 4
2023-02-25 14:09:50,274 DEBUG TRAIN Batch 29/1400 loss 13.787825 loss_att 15.813545 loss_ctc 17.745022 loss_rnnt 12.773635 hw_loss 0.152662 lr 0.00032054 rank 0
2023-02-25 14:09:50,275 DEBUG TRAIN Batch 29/1400 loss 6.132922 loss_att 9.216969 loss_ctc 7.721757 loss_rnnt 5.185733 hw_loss 0.222253 lr 0.00032055 rank 7
2023-02-25 14:09:50,280 DEBUG TRAIN Batch 29/1400 loss 4.948372 loss_att 7.949150 loss_ctc 7.152448 loss_rnnt 3.968218 hw_loss 0.161479 lr 0.00032060 rank 2
2023-02-25 14:09:50,281 DEBUG TRAIN Batch 29/1400 loss 7.083009 loss_att 10.010608 loss_ctc 11.193176 loss_rnnt 5.861845 hw_loss 0.164293 lr 0.00032064 rank 6
2023-02-25 14:09:50,286 DEBUG TRAIN Batch 29/1400 loss 5.294687 loss_att 7.568376 loss_ctc 4.569593 loss_rnnt 4.869789 hw_loss 0.125325 lr 0.00032061 rank 3
2023-02-25 14:09:50,327 DEBUG TRAIN Batch 29/1400 loss 3.128610 loss_att 7.056266 loss_ctc 3.280626 loss_rnnt 2.225830 hw_loss 0.181839 lr 0.00032060 rank 5
2023-02-25 14:11:02,132 DEBUG TRAIN Batch 29/1500 loss 4.831825 loss_att 8.255519 loss_ctc 8.501817 loss_rnnt 3.556985 hw_loss 0.188943 lr 0.00032047 rank 0
2023-02-25 14:11:02,135 DEBUG TRAIN Batch 29/1500 loss 7.416389 loss_att 9.932137 loss_ctc 12.644610 loss_rnnt 6.128389 hw_loss 0.164538 lr 0.00032058 rank 1
2023-02-25 14:11:02,139 DEBUG TRAIN Batch 29/1500 loss 7.298417 loss_att 10.423210 loss_ctc 10.284439 loss_rnnt 6.152227 hw_loss 0.230803 lr 0.00032054 rank 2
2023-02-25 14:11:02,139 DEBUG TRAIN Batch 29/1500 loss 5.819038 loss_att 7.558404 loss_ctc 11.574308 loss_rnnt 4.645278 hw_loss 0.109721 lr 0.00032057 rank 6
2023-02-25 14:11:02,139 DEBUG TRAIN Batch 29/1500 loss 9.624225 loss_att 16.342043 loss_ctc 11.296984 loss_rnnt 7.916891 hw_loss 0.263880 lr 0.00032049 rank 7
2023-02-25 14:11:02,140 DEBUG TRAIN Batch 29/1500 loss 5.076588 loss_att 7.733840 loss_ctc 8.558121 loss_rnnt 3.950977 hw_loss 0.243667 lr 0.00032053 rank 5
2023-02-25 14:11:02,144 DEBUG TRAIN Batch 29/1500 loss 3.103946 loss_att 6.192670 loss_ctc 8.230082 loss_rnnt 1.702887 hw_loss 0.187180 lr 0.00032051 rank 4
2023-02-25 14:11:02,181 DEBUG TRAIN Batch 29/1500 loss 17.023819 loss_att 19.986591 loss_ctc 27.407125 loss_rnnt 14.928313 hw_loss 0.222205 lr 0.00032054 rank 3
2023-02-25 14:12:12,515 DEBUG TRAIN Batch 29/1600 loss 5.952611 loss_att 7.862629 loss_ctc 7.802885 loss_rnnt 5.193786 hw_loss 0.243973 lr 0.00032041 rank 0
2023-02-25 14:12:12,515 DEBUG TRAIN Batch 29/1600 loss 8.170370 loss_att 12.153887 loss_ctc 10.932059 loss_rnnt 6.933126 hw_loss 0.135590 lr 0.00032051 rank 6
2023-02-25 14:12:12,518 DEBUG TRAIN Batch 29/1600 loss 10.303250 loss_att 11.867510 loss_ctc 17.627903 loss_rnnt 8.861275 hw_loss 0.285943 lr 0.00032042 rank 7
2023-02-25 14:12:12,519 DEBUG TRAIN Batch 29/1600 loss 4.238058 loss_att 7.667951 loss_ctc 7.749689 loss_rnnt 2.995944 hw_loss 0.164847 lr 0.00032047 rank 2
2023-02-25 14:12:12,522 DEBUG TRAIN Batch 29/1600 loss 4.607996 loss_att 7.903668 loss_ctc 7.502267 loss_rnnt 3.473364 hw_loss 0.167990 lr 0.00032044 rank 4
2023-02-25 14:12:12,524 DEBUG TRAIN Batch 29/1600 loss 4.516510 loss_att 6.629462 loss_ctc 5.806195 loss_rnnt 3.822611 hw_loss 0.186281 lr 0.00032051 rank 1
2023-02-25 14:12:12,525 DEBUG TRAIN Batch 29/1600 loss 9.873442 loss_att 11.923384 loss_ctc 13.883245 loss_rnnt 8.842651 hw_loss 0.161552 lr 0.00032046 rank 5
2023-02-25 14:12:12,578 DEBUG TRAIN Batch 29/1600 loss 8.172141 loss_att 10.828533 loss_ctc 13.174708 loss_rnnt 6.902107 hw_loss 0.134524 lr 0.00032048 rank 3
2023-02-25 14:13:22,794 DEBUG TRAIN Batch 29/1700 loss 7.425891 loss_att 11.559012 loss_ctc 11.471069 loss_rnnt 5.917078 hw_loss 0.267807 lr 0.00032040 rank 5
2023-02-25 14:13:22,802 DEBUG TRAIN Batch 29/1700 loss 12.992845 loss_att 14.418275 loss_ctc 16.917336 loss_rnnt 12.081301 hw_loss 0.193484 lr 0.00032038 rank 4
2023-02-25 14:13:22,803 DEBUG TRAIN Batch 29/1700 loss 14.912642 loss_att 16.417912 loss_ctc 24.204433 loss_rnnt 13.209419 hw_loss 0.306121 lr 0.00032045 rank 1
2023-02-25 14:13:22,804 DEBUG TRAIN Batch 29/1700 loss 6.879558 loss_att 11.252419 loss_ctc 8.777247 loss_rnnt 5.681232 hw_loss 0.132616 lr 0.00032034 rank 0
2023-02-25 14:13:22,804 DEBUG TRAIN Batch 29/1700 loss 8.350684 loss_att 9.047571 loss_ctc 10.205374 loss_rnnt 7.851011 hw_loss 0.211880 lr 0.00032044 rank 6
2023-02-25 14:13:22,806 DEBUG TRAIN Batch 29/1700 loss 11.556458 loss_att 15.110600 loss_ctc 18.508991 loss_rnnt 9.797689 hw_loss 0.226755 lr 0.00032036 rank 7
2023-02-25 14:13:22,809 DEBUG TRAIN Batch 29/1700 loss 9.155710 loss_att 11.560501 loss_ctc 10.946161 loss_rnnt 8.364644 hw_loss 0.133838 lr 0.00032040 rank 2
2023-02-25 14:13:22,832 DEBUG TRAIN Batch 29/1700 loss 6.981274 loss_att 8.694052 loss_ctc 9.165052 loss_rnnt 6.240476 hw_loss 0.200759 lr 0.00032041 rank 3
2023-02-25 14:14:36,491 DEBUG TRAIN Batch 29/1800 loss 7.034920 loss_att 6.684900 loss_ctc 11.459164 loss_rnnt 6.418050 hw_loss 0.181827 lr 0.00032031 rank 4
2023-02-25 14:14:36,495 DEBUG TRAIN Batch 29/1800 loss 7.756575 loss_att 13.764169 loss_ctc 12.347430 loss_rnnt 5.835576 hw_loss 0.201312 lr 0.00032028 rank 0
2023-02-25 14:14:36,496 DEBUG TRAIN Batch 29/1800 loss 12.133088 loss_att 14.043992 loss_ctc 12.604176 loss_rnnt 11.584081 hw_loss 0.195026 lr 0.00032035 rank 3
2023-02-25 14:14:36,497 DEBUG TRAIN Batch 29/1800 loss 5.471385 loss_att 7.318127 loss_ctc 8.358206 loss_rnnt 4.627795 hw_loss 0.167497 lr 0.00032038 rank 1
2023-02-25 14:14:36,498 DEBUG TRAIN Batch 29/1800 loss 7.198936 loss_att 9.865873 loss_ctc 12.098994 loss_rnnt 5.919490 hw_loss 0.173846 lr 0.00032029 rank 7
2023-02-25 14:14:36,499 DEBUG TRAIN Batch 29/1800 loss 3.990906 loss_att 6.405457 loss_ctc 6.517101 loss_rnnt 3.088295 hw_loss 0.155390 lr 0.00032034 rank 2
2023-02-25 14:14:36,501 DEBUG TRAIN Batch 29/1800 loss 13.382866 loss_att 15.452099 loss_ctc 18.559671 loss_rnnt 12.148298 hw_loss 0.244652 lr 0.00032033 rank 5
2023-02-25 14:14:36,558 DEBUG TRAIN Batch 29/1800 loss 20.488131 loss_att 21.437012 loss_ctc 27.623571 loss_rnnt 19.191067 hw_loss 0.292307 lr 0.00032038 rank 6
run_2_21_rnnt_bias_0-3word_finetune.sh: line 167:  8680 Terminated              python wenet/bin/train.py --gpu $gpu_id --config $train_config --data_type raw --symbol_table $dict --bpe_model ${bpemodel}.model --train_data $wave_data/$train_set/data.list --cv_data $wave_data/$dev_set/data.list ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --ddp.init_method $init_method --ddp.world_size $num_gpus --ddp.rank $i --ddp.dist_backend $dist_backend --num_workers 1 $cmvn_opts --pin_memory
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [192.168.0.37]:28561: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [192.168.0.37]:57748: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:22954
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:1939
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:61357
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:24219
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:24741
