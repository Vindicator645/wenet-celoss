/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_encoder_decoder_bias_nobi_noatt.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-rnnt-runtime/examples/librispeech/s0/exp/1204_encoder_bias_nobi_noatt/ddp_init
2022-12-06 23:21:11,231 INFO training on multiple gpus, this gpu 6
2022-12-06 23:21:11,235 INFO training on multiple gpus, this gpu 7
2022-12-06 23:21:11,235 INFO training on multiple gpus, this gpu 5
2022-12-06 23:21:11,242 INFO training on multiple gpus, this gpu 4
2022-12-06 23:21:19,575 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2022-12-06 23:21:20,611 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2022-12-06 23:21:20,649 INFO Added key: store_based_barrier_key:1 to store for rank: 3
2022-12-06 23:21:22,714 INFO Added key: store_based_barrier_key:1 to store for rank: 2
2022-12-06 23:21:22,719 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-06 23:21:23,769 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-06 23:21:23,869 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-06 23:21:36,132 INFO Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=4, timeout=0:30:00)
2022-12-06 23:21:36,132 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-06 23:21:38,922 INFO Checkpoint: loading from checkpoint exp/1204_encoder_bias_nobi_noatt/19.pt for GPU
2022-12-06 23:21:39,883 INFO Checkpoint: loading from checkpoint exp/1204_encoder_bias_nobi_noatt/19.pt for GPU
2022-12-06 23:21:40,249 INFO Checkpoint: loading from checkpoint exp/1204_encoder_bias_nobi_noatt/19.pt for GPU
2022-12-06 23:21:42,634 INFO Checkpoint: loading from checkpoint exp/1204_encoder_bias_nobi_noatt/19.pt for GPU
2022-12-06 23:21:47,223 INFO Epoch 20 TRAIN info lr 4e-08
2022-12-06 23:21:47,225 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5002, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5002, bias=True)
    (decoders): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear1): Identity()
        (concat_linear2): Identity()
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear1): Identity()
        (concat_linear2): Identity()
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear1): Identity()
        (concat_linear2): Identity()
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 52377758
2022-12-06 23:21:47,234 INFO Epoch 20 TRAIN info lr 4e-08
2022-12-06 23:21:47,235 INFO Epoch 20 TRAIN info lr 4e-08
2022-12-06 23:21:47,236 INFO using accumulate grad, new batch size is 1 times larger than before
2022-12-06 23:21:47,236 INFO using accumulate grad, new batch size is 1 times larger than before
2022-12-06 23:21:47,239 INFO Epoch 20 TRAIN info lr 4e-08
2022-12-06 23:21:47,240 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5002, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5002, bias=True)
    (decoders): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear1): Identity()
        (concat_linear2): Identity()
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear1): Identity()
        (concat_linear2): Identity()
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear1): Identity()
        (concat_linear2): Identity()
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 52377758
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5002, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5002, bias=True)
    (decoders): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear1): Identity()
        (concat_linear2): Identity()
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear1): Identity()
        (concat_linear2): Identity()
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear1): Identity()
        (concat_linear2): Identity()
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 52377758
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5002, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5002, bias=True)
    (decoders): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear1): Identity()
        (concat_linear2): Identity()
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear1): Identity()
        (concat_linear2): Identity()
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear1): Identity()
        (concat_linear2): Identity()
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 52377758
2022-12-06 23:23:08,340 DEBUG TRAIN Batch 20/0 loss 10.226019 loss_att 85.209778 loss_ctc 14.576185 loss_rnnt 9.742667 lr 0.00028093 rank 3
2022-12-06 23:23:08,409 DEBUG TRAIN Batch 20/0 loss 9.372849 loss_att 73.155197 loss_ctc 14.538442 loss_rnnt 8.798895 lr 0.00028093 rank 2
2022-12-06 23:23:08,413 DEBUG TRAIN Batch 20/0 loss 6.717348 loss_att 62.417744 loss_ctc 9.924155 loss_rnnt 6.361036 lr 0.00028093 rank 1
2022-12-06 23:23:08,447 DEBUG TRAIN Batch 20/0 loss 10.315878 loss_att 75.486237 loss_ctc 15.718473 loss_rnnt 9.715590 lr 0.00028093 rank 0
2022-12-06 23:24:06,017 DEBUG TRAIN Batch 20/100 loss 3.920880 loss_att 280.687317 loss_ctc 13.891552 loss_rnnt 2.813028 lr 0.00028089 rank 3
2022-12-06 23:24:06,022 DEBUG TRAIN Batch 20/100 loss 4.645716 loss_att 419.714447 loss_ctc 16.545078 loss_rnnt 3.323565 lr 0.00028089 rank 2
2022-12-06 23:24:06,034 DEBUG TRAIN Batch 20/100 loss 7.708722 loss_att 381.866058 loss_ctc 21.331715 loss_rnnt 6.195056 lr 0.00028089 rank 1
2022-12-06 23:24:06,042 DEBUG TRAIN Batch 20/100 loss 13.287514 loss_att 408.380096 loss_ctc 24.827971 loss_rnnt 12.005240 lr 0.00028089 rank 0
2022-12-06 23:25:01,223 DEBUG TRAIN Batch 20/200 loss 11.485271 loss_att 454.832764 loss_ctc 21.569265 loss_rnnt 10.364827 lr 0.00028084 rank 0
2022-12-06 23:25:01,223 DEBUG TRAIN Batch 20/200 loss 23.381844 loss_att 394.676727 loss_ctc 45.824707 loss_rnnt 20.888191 lr 0.00028084 rank 2
2022-12-06 23:25:01,225 DEBUG TRAIN Batch 20/200 loss 6.544623 loss_att 417.992889 loss_ctc 12.864141 loss_rnnt 5.842454 lr 0.00028084 rank 3
2022-12-06 23:25:01,278 DEBUG TRAIN Batch 20/200 loss 13.145015 loss_att 382.704102 loss_ctc 33.324051 loss_rnnt 10.902900 lr 0.00028084 rank 1
2022-12-06 23:25:57,810 DEBUG TRAIN Batch 20/300 loss 7.834898 loss_att 428.201111 loss_ctc 15.144722 loss_rnnt 7.022696 lr 0.00028080 rank 3
2022-12-06 23:25:57,816 DEBUG TRAIN Batch 20/300 loss 5.319206 loss_att 357.567566 loss_ctc 14.302782 loss_rnnt 4.321031 lr 0.00028080 rank 1
2022-12-06 23:25:57,819 DEBUG TRAIN Batch 20/300 loss 18.001211 loss_att 375.520264 loss_ctc 31.944365 loss_rnnt 16.451973 lr 0.00028080 rank 0
2022-12-06 23:25:57,830 DEBUG TRAIN Batch 20/300 loss 10.861504 loss_att 406.138916 loss_ctc 18.115263 loss_rnnt 10.055531 lr 0.00028080 rank 2
2022-12-06 23:27:07,037 DEBUG TRAIN Batch 20/400 loss 12.637663 loss_att 362.258575 loss_ctc 25.926199 loss_rnnt 11.161160 lr 0.00028075 rank 1
2022-12-06 23:27:07,047 DEBUG TRAIN Batch 20/400 loss 7.745809 loss_att 391.006561 loss_ctc 26.445744 loss_rnnt 5.668038 lr 0.00028075 rank 3
2022-12-06 23:27:07,051 DEBUG TRAIN Batch 20/400 loss 11.796092 loss_att 329.579712 loss_ctc 20.080276 loss_rnnt 10.875627 lr 0.00028075 rank 0
2022-12-06 23:27:07,054 DEBUG TRAIN Batch 20/400 loss 8.645551 loss_att 378.284454 loss_ctc 20.660015 loss_rnnt 7.310610 lr 0.00028075 rank 2
2022-12-06 23:28:03,274 DEBUG TRAIN Batch 20/500 loss 12.528697 loss_att 322.759033 loss_ctc 26.650915 loss_rnnt 10.959562 lr 0.00028071 rank 2
2022-12-06 23:28:03,277 DEBUG TRAIN Batch 20/500 loss 15.381699 loss_att 366.909607 loss_ctc 22.687420 loss_rnnt 14.569952 lr 0.00028071 rank 3
2022-12-06 23:28:03,278 DEBUG TRAIN Batch 20/500 loss 6.079661 loss_att 297.165192 loss_ctc 11.125373 loss_rnnt 5.519026 lr 0.00028071 rank 1
2022-12-06 23:28:03,322 DEBUG TRAIN Batch 20/500 loss 17.478050 loss_att 351.156158 loss_ctc 29.741201 loss_rnnt 16.115479 lr 0.00028071 rank 0
2022-12-06 23:28:59,507 DEBUG TRAIN Batch 20/600 loss 12.378984 loss_att 310.171783 loss_ctc 17.238077 loss_rnnt 11.839086 lr 0.00028066 rank 3
2022-12-06 23:28:59,507 DEBUG TRAIN Batch 20/600 loss 4.187428 loss_att 170.421066 loss_ctc 6.771814 loss_rnnt 3.900274 lr 0.00028066 rank 2
2022-12-06 23:28:59,512 DEBUG TRAIN Batch 20/600 loss 6.615806 loss_att 147.201721 loss_ctc 9.336194 loss_rnnt 6.313541 lr 0.00028066 rank 1
2022-12-06 23:28:59,516 DEBUG TRAIN Batch 20/600 loss 12.189300 loss_att 127.994377 loss_ctc 18.757793 loss_rnnt 11.459468 lr 0.00028066 rank 0
2022-12-06 23:30:06,553 DEBUG TRAIN Batch 20/700 loss 6.095574 loss_att 388.091003 loss_ctc 11.618117 loss_rnnt 5.481958 lr 0.00028062 rank 2
2022-12-06 23:30:06,562 DEBUG TRAIN Batch 20/700 loss 16.123009 loss_att 405.724915 loss_ctc 29.762821 loss_rnnt 14.607473 lr 0.00028062 rank 1
2022-12-06 23:30:06,570 DEBUG TRAIN Batch 20/700 loss 9.636442 loss_att 425.441772 loss_ctc 24.910969 loss_rnnt 7.939272 lr 0.00028062 rank 0
2022-12-06 23:30:06,578 DEBUG TRAIN Batch 20/700 loss 1.907239 loss_att 404.916687 loss_ctc 5.281718 loss_rnnt 1.532297 lr 0.00028062 rank 3
2022-12-06 23:31:07,187 DEBUG TRAIN Batch 20/800 loss 13.516561 loss_att 357.108246 loss_ctc 16.547184 loss_rnnt 13.179825 lr 0.00028058 rank 3
2022-12-06 23:31:07,191 DEBUG TRAIN Batch 20/800 loss 10.439535 loss_att 402.627197 loss_ctc 19.162716 loss_rnnt 9.470293 lr 0.00028058 rank 1
2022-12-06 23:31:07,194 DEBUG TRAIN Batch 20/800 loss 11.771605 loss_att 450.805359 loss_ctc 21.264549 loss_rnnt 10.716833 lr 0.00028058 rank 0
2022-12-06 23:31:07,195 DEBUG TRAIN Batch 20/800 loss 11.815812 loss_att 372.228943 loss_ctc 26.439077 loss_rnnt 10.191006 lr 0.00028058 rank 2
2022-12-06 23:32:02,846 DEBUG TRAIN Batch 20/900 loss 3.622535 loss_att 328.793213 loss_ctc 12.376887 loss_rnnt 2.649829 lr 0.00028053 rank 3
2022-12-06 23:32:02,864 DEBUG TRAIN Batch 20/900 loss 8.954531 loss_att 372.714661 loss_ctc 13.864912 loss_rnnt 8.408934 lr 0.00028053 rank 1
2022-12-06 23:32:02,866 DEBUG TRAIN Batch 20/900 loss 7.901970 loss_att 326.230774 loss_ctc 15.295232 loss_rnnt 7.080496 lr 0.00028053 rank 2
2022-12-06 23:32:02,916 DEBUG TRAIN Batch 20/900 loss 11.114094 loss_att 408.807678 loss_ctc 24.671452 loss_rnnt 9.607721 lr 0.00028053 rank 0
2022-12-06 23:32:58,980 DEBUG TRAIN Batch 20/1000 loss 18.764830 loss_att 392.798096 loss_ctc 35.125938 loss_rnnt 16.946930 lr 0.00028049 rank 3
2022-12-06 23:32:58,983 DEBUG TRAIN Batch 20/1000 loss 8.960926 loss_att 385.916626 loss_ctc 21.018524 loss_rnnt 7.621193 lr 0.00028049 rank 2
2022-12-06 23:32:58,983 DEBUG TRAIN Batch 20/1000 loss 19.756081 loss_att 384.581055 loss_ctc 29.447813 loss_rnnt 18.679222 lr 0.00028049 rank 1
2022-12-06 23:32:58,989 DEBUG TRAIN Batch 20/1000 loss 14.732567 loss_att 375.153809 loss_ctc 23.332043 loss_rnnt 13.777070 lr 0.00028049 rank 0
2022-12-06 23:34:12,944 DEBUG TRAIN Batch 20/1100 loss 7.333745 loss_att 411.486115 loss_ctc 13.712448 loss_rnnt 6.625001 lr 0.00028044 rank 2
2022-12-06 23:34:12,949 DEBUG TRAIN Batch 20/1100 loss 7.155778 loss_att 291.464447 loss_ctc 15.997052 loss_rnnt 6.173415 lr 0.00028044 rank 0
2022-12-06 23:34:12,957 DEBUG TRAIN Batch 20/1100 loss 2.752003 loss_att 351.407959 loss_ctc 6.344220 loss_rnnt 2.352868 lr 0.00028044 rank 3
2022-12-06 23:34:12,960 DEBUG TRAIN Batch 20/1100 loss 7.730765 loss_att 348.418335 loss_ctc 17.870369 loss_rnnt 6.604143 lr 0.00028044 rank 1
2022-12-06 23:35:10,108 DEBUG TRAIN Batch 20/1200 loss 4.054673 loss_att 257.036011 loss_ctc 9.807546 loss_rnnt 3.415465 lr 0.00028040 rank 1
2022-12-06 23:35:10,108 DEBUG TRAIN Batch 20/1200 loss 13.787711 loss_att 318.659760 loss_ctc 29.762203 loss_rnnt 12.012768 lr 0.00028040 rank 3
2022-12-06 23:35:10,108 DEBUG TRAIN Batch 20/1200 loss 5.420939 loss_att 271.667053 loss_ctc 14.173955 loss_rnnt 4.448382 lr 0.00028040 rank 2
2022-12-06 23:35:10,112 DEBUG TRAIN Batch 20/1200 loss 15.188648 loss_att 137.300735 loss_ctc 25.131847 loss_rnnt 14.083849 lr 0.00028040 rank 0
2022-12-06 23:36:06,543 DEBUG TRAIN Batch 20/1300 loss 20.085930 loss_att 401.146973 loss_ctc 35.806316 loss_rnnt 18.339222 lr 0.00028036 rank 1
2022-12-06 23:36:06,548 DEBUG TRAIN Batch 20/1300 loss 5.715895 loss_att 384.002869 loss_ctc 9.999699 loss_rnnt 5.239917 lr 0.00028036 rank 2
2022-12-06 23:36:06,548 DEBUG TRAIN Batch 20/1300 loss 10.248618 loss_att 174.801270 loss_ctc 16.429962 loss_rnnt 9.561803 lr 0.00028036 rank 3
2022-12-06 23:36:06,550 DEBUG TRAIN Batch 20/1300 loss 3.924589 loss_att 348.373260 loss_ctc 8.354906 loss_rnnt 3.432332 lr 0.00028036 rank 0
2022-12-06 23:37:02,962 DEBUG TRAIN Batch 20/1400 loss 7.154528 loss_att 444.835938 loss_ctc 12.600622 loss_rnnt 6.549407 lr 0.00028031 rank 3
2022-12-06 23:37:02,971 DEBUG TRAIN Batch 20/1400 loss 10.782900 loss_att 382.492523 loss_ctc 15.755245 loss_rnnt 10.230417 lr 0.00028031 rank 0
2022-12-06 23:37:02,975 DEBUG TRAIN Batch 20/1400 loss 8.334657 loss_att 345.141693 loss_ctc 22.789740 loss_rnnt 6.728537 lr 0.00028031 rank 1
2022-12-06 23:37:02,984 DEBUG TRAIN Batch 20/1400 loss 7.453402 loss_att 429.100311 loss_ctc 17.005766 loss_rnnt 6.392028 lr 0.00028031 rank 2
2022-12-06 23:38:12,684 DEBUG TRAIN Batch 20/1500 loss 2.980979 loss_att 409.749359 loss_ctc 8.516199 loss_rnnt 2.365954 lr 0.00028027 rank 3
2022-12-06 23:38:12,685 DEBUG TRAIN Batch 20/1500 loss 8.987688 loss_att 414.012299 loss_ctc 14.229254 loss_rnnt 8.405293 lr 0.00028027 rank 1
2022-12-06 23:38:12,698 DEBUG TRAIN Batch 20/1500 loss 6.808934 loss_att 392.039307 loss_ctc 15.060657 loss_rnnt 5.892076 lr 0.00028027 rank 2
2022-12-06 23:38:12,707 DEBUG TRAIN Batch 20/1500 loss 7.926139 loss_att 358.111420 loss_ctc 11.899868 loss_rnnt 7.484614 lr 0.00028027 rank 0
2022-12-06 23:39:09,286 DEBUG TRAIN Batch 20/1600 loss 9.061180 loss_att 322.775604 loss_ctc 11.583519 loss_rnnt 8.780920 lr 0.00028022 rank 3
2022-12-06 23:39:09,292 DEBUG TRAIN Batch 20/1600 loss 5.831880 loss_att 362.286804 loss_ctc 16.799957 loss_rnnt 4.613204 lr 0.00028022 rank 1
2022-12-06 23:39:09,299 DEBUG TRAIN Batch 20/1600 loss 6.649731 loss_att 362.987030 loss_ctc 15.263329 loss_rnnt 5.692665 lr 0.00028022 rank 2
2022-12-06 23:39:09,305 DEBUG TRAIN Batch 20/1600 loss 9.839350 loss_att 367.233490 loss_ctc 23.305891 loss_rnnt 8.343067 lr 0.00028022 rank 0
2022-12-06 23:40:06,348 DEBUG TRAIN Batch 20/1700 loss 7.110278 loss_att 353.700592 loss_ctc 20.459215 loss_rnnt 5.627063 lr 0.00028018 rank 2
2022-12-06 23:40:06,357 DEBUG TRAIN Batch 20/1700 loss 4.282537 loss_att 385.752136 loss_ctc 12.335690 loss_rnnt 3.387742 lr 0.00028018 rank 3
2022-12-06 23:40:06,360 DEBUG TRAIN Batch 20/1700 loss 14.980463 loss_att 376.820343 loss_ctc 30.708082 loss_rnnt 13.232950 lr 0.00028018 rank 1
2022-12-06 23:40:06,387 DEBUG TRAIN Batch 20/1700 loss 4.296704 loss_att 278.313477 loss_ctc 8.322073 loss_rnnt 3.849441 lr 0.00028018 rank 0
2022-12-06 23:41:32,953 DEBUG TRAIN Batch 20/1800 loss 9.694336 loss_att 223.379471 loss_ctc 18.932959 loss_rnnt 8.667823 lr 0.00028014 rank 0
2022-12-06 23:41:32,955 DEBUG TRAIN Batch 20/1800 loss 12.480212 loss_att 378.857269 loss_ctc 23.595715 loss_rnnt 11.245156 lr 0.00028014 rank 3
2022-12-06 23:41:32,958 DEBUG TRAIN Batch 20/1800 loss 12.991909 loss_att 281.456146 loss_ctc 24.654972 loss_rnnt 11.696013 lr 0.00028014 rank 2
2022-12-06 23:41:32,961 DEBUG TRAIN Batch 20/1800 loss 5.890372 loss_att 330.960907 loss_ctc 16.181189 loss_rnnt 4.746949 lr 0.00028014 rank 1
2022-12-06 23:42:29,699 DEBUG TRAIN Batch 20/1900 loss 7.909749 loss_att 300.536591 loss_ctc 19.999020 loss_rnnt 6.566496 lr 0.00028009 rank 3
2022-12-06 23:42:29,701 DEBUG TRAIN Batch 20/1900 loss 9.302985 loss_att 145.703735 loss_ctc 14.040049 loss_rnnt 8.776645 lr 0.00028009 rank 2
2022-12-06 23:42:29,701 DEBUG TRAIN Batch 20/1900 loss 9.610189 loss_att 155.312317 loss_ctc 14.524665 loss_rnnt 9.064137 lr 0.00028009 rank 1
2022-12-06 23:42:29,706 DEBUG TRAIN Batch 20/1900 loss 4.185921 loss_att 434.447632 loss_ctc 9.072174 loss_rnnt 3.643004 lr 0.00028009 rank 0
2022-12-06 23:43:25,657 DEBUG TRAIN Batch 20/2000 loss 6.644855 loss_att 417.349304 loss_ctc 15.947215 loss_rnnt 5.611259 lr 0.00028005 rank 3
2022-12-06 23:43:25,672 DEBUG TRAIN Batch 20/2000 loss 6.147246 loss_att 390.040619 loss_ctc 12.005584 loss_rnnt 5.496320 lr 0.00028005 rank 2
2022-12-06 23:43:25,684 DEBUG TRAIN Batch 20/2000 loss 6.752555 loss_att 420.955048 loss_ctc 7.374671 loss_rnnt 6.683431 lr 0.00028005 rank 0
2022-12-06 23:43:25,684 DEBUG TRAIN Batch 20/2000 loss 5.397703 loss_att 383.164246 loss_ctc 11.038719 loss_rnnt 4.770924 lr 0.00028005 rank 1
2022-12-06 23:44:22,821 DEBUG TRAIN Batch 20/2100 loss 6.660369 loss_att 476.013763 loss_ctc 16.668247 loss_rnnt 5.548383 lr 0.00028000 rank 1
2022-12-06 23:44:22,821 DEBUG TRAIN Batch 20/2100 loss 11.499572 loss_att 355.943695 loss_ctc 20.312626 loss_rnnt 10.520344 lr 0.00028000 rank 0
2022-12-06 23:44:22,830 DEBUG TRAIN Batch 20/2100 loss 12.280921 loss_att 403.474579 loss_ctc 18.832598 loss_rnnt 11.552958 lr 0.00028000 rank 3
2022-12-06 23:44:22,851 DEBUG TRAIN Batch 20/2100 loss 5.804584 loss_att 376.750214 loss_ctc 11.881427 loss_rnnt 5.129379 lr 0.00028000 rank 2
2022-12-06 23:45:34,485 DEBUG TRAIN Batch 20/2200 loss 13.114208 loss_att 362.041016 loss_ctc 28.138817 loss_rnnt 11.444807 lr 0.00027996 rank 1
2022-12-06 23:45:34,494 DEBUG TRAIN Batch 20/2200 loss 6.133903 loss_att 306.616180 loss_ctc 15.608700 loss_rnnt 5.081147 lr 0.00027996 rank 3
2022-12-06 23:45:34,499 DEBUG TRAIN Batch 20/2200 loss 5.453285 loss_att 370.963898 loss_ctc 9.383708 loss_rnnt 5.016572 lr 0.00027996 rank 2
2022-12-06 23:45:34,543 DEBUG TRAIN Batch 20/2200 loss 4.608494 loss_att 364.169067 loss_ctc 17.034048 loss_rnnt 3.227877 lr 0.00027996 rank 0
2022-12-06 23:46:31,130 DEBUG TRAIN Batch 20/2300 loss 7.268521 loss_att 371.195740 loss_ctc 12.576779 loss_rnnt 6.678714 lr 0.00027992 rank 1
2022-12-06 23:46:31,134 DEBUG TRAIN Batch 20/2300 loss 6.515587 loss_att 412.985748 loss_ctc 19.312502 loss_rnnt 5.093708 lr 0.00027992 rank 0
2022-12-06 23:46:31,134 DEBUG TRAIN Batch 20/2300 loss 8.308414 loss_att 307.347260 loss_ctc 16.505098 loss_rnnt 7.397672 lr 0.00027992 rank 3
2022-12-06 23:46:31,142 DEBUG TRAIN Batch 20/2300 loss 10.316194 loss_att 335.116455 loss_ctc 17.078009 loss_rnnt 9.564881 lr 0.00027992 rank 2
2022-12-06 23:47:27,927 DEBUG TRAIN Batch 20/2400 loss 6.288127 loss_att 362.621307 loss_ctc 13.513615 loss_rnnt 5.485295 lr 0.00027987 rank 3
2022-12-06 23:47:27,930 DEBUG TRAIN Batch 20/2400 loss 12.320690 loss_att 353.824677 loss_ctc 22.577194 loss_rnnt 11.181078 lr 0.00027987 rank 1
2022-12-06 23:47:27,930 DEBUG TRAIN Batch 20/2400 loss 7.320463 loss_att 330.860718 loss_ctc 16.624065 loss_rnnt 6.286730 lr 0.00027987 rank 2
2022-12-06 23:47:27,936 DEBUG TRAIN Batch 20/2400 loss 10.226899 loss_att 309.073395 loss_ctc 23.777033 loss_rnnt 8.721329 lr 0.00027987 rank 0
2022-12-06 23:48:42,665 DEBUG TRAIN Batch 20/2500 loss 6.814861 loss_att 291.404480 loss_ctc 14.790751 loss_rnnt 5.928651 lr 0.00027983 rank 3
2022-12-06 23:48:42,668 DEBUG TRAIN Batch 20/2500 loss 11.562316 loss_att 239.076782 loss_ctc 19.525074 loss_rnnt 10.677566 lr 0.00027983 rank 1
2022-12-06 23:48:42,675 DEBUG TRAIN Batch 20/2500 loss 5.364870 loss_att 296.266754 loss_ctc 10.849088 loss_rnnt 4.755512 lr 0.00027983 rank 2
2022-12-06 23:48:42,675 DEBUG TRAIN Batch 20/2500 loss 14.575925 loss_att 186.774887 loss_ctc 26.966026 loss_rnnt 13.199247 lr 0.00027983 rank 0
2022-12-06 23:49:39,386 DEBUG TRAIN Batch 20/2600 loss 11.850951 loss_att 190.475540 loss_ctc 17.353357 loss_rnnt 11.239573 lr 0.00027978 rank 3
2022-12-06 23:49:39,387 DEBUG TRAIN Batch 20/2600 loss 6.990680 loss_att 478.169128 loss_ctc 21.606758 loss_rnnt 5.366672 lr 0.00027978 rank 1
2022-12-06 23:49:39,389 DEBUG TRAIN Batch 20/2600 loss 13.847588 loss_att 410.911621 loss_ctc 23.095741 loss_rnnt 12.820015 lr 0.00027978 rank 2
2022-12-06 23:49:39,393 DEBUG TRAIN Batch 20/2600 loss 6.353907 loss_att 421.855896 loss_ctc 13.638584 loss_rnnt 5.544498 lr 0.00027978 rank 0
2022-12-06 23:50:34,663 DEBUG TRAIN Batch 20/2700 loss 3.222815 loss_att 391.674988 loss_ctc 7.176510 loss_rnnt 2.783515 lr 0.00027974 rank 3
2022-12-06 23:50:34,668 DEBUG TRAIN Batch 20/2700 loss 13.129047 loss_att 353.378510 loss_ctc 33.863438 loss_rnnt 10.825226 lr 0.00027974 rank 1
2022-12-06 23:50:34,671 DEBUG TRAIN Batch 20/2700 loss 2.622385 loss_att 308.629211 loss_ctc 8.333946 loss_rnnt 1.987767 lr 0.00027974 rank 0
2022-12-06 23:50:34,682 DEBUG TRAIN Batch 20/2700 loss 8.898620 loss_att 348.692566 loss_ctc 21.983166 loss_rnnt 7.444781 lr 0.00027974 rank 2
2022-12-06 23:51:31,758 DEBUG TRAIN Batch 20/2800 loss 4.992394 loss_att 293.515198 loss_ctc 9.559734 loss_rnnt 4.484912 lr 0.00027970 rank 0
2022-12-06 23:51:31,761 DEBUG TRAIN Batch 20/2800 loss 12.015117 loss_att 472.262756 loss_ctc 33.032452 loss_rnnt 9.679857 lr 0.00027970 rank 3
2022-12-06 23:51:31,762 DEBUG TRAIN Batch 20/2800 loss 16.020536 loss_att 425.889465 loss_ctc 33.767776 loss_rnnt 14.048621 lr 0.00027970 rank 2
2022-12-06 23:51:31,765 DEBUG TRAIN Batch 20/2800 loss 10.203843 loss_att 408.601654 loss_ctc 13.510221 loss_rnnt 9.836469 lr 0.00027970 rank 1
2022-12-06 23:52:45,224 DEBUG TRAIN Batch 20/2900 loss 14.479840 loss_att 388.853546 loss_ctc 25.874935 loss_rnnt 13.213718 lr 0.00027965 rank 3
2022-12-06 23:52:45,225 DEBUG TRAIN Batch 20/2900 loss 5.749410 loss_att 353.821289 loss_ctc 15.303385 loss_rnnt 4.687857 lr 0.00027965 rank 1
2022-12-06 23:52:45,226 DEBUG TRAIN Batch 20/2900 loss 11.679236 loss_att 379.671051 loss_ctc 17.112846 loss_rnnt 11.075502 lr 0.00027965 rank 2
2022-12-06 23:52:45,230 DEBUG TRAIN Batch 20/2900 loss 2.943751 loss_att 343.533417 loss_ctc 5.064229 loss_rnnt 2.708142 lr 0.00027965 rank 0
2022-12-06 23:53:41,914 DEBUG TRAIN Batch 20/3000 loss 8.951664 loss_att 335.921936 loss_ctc 14.893513 loss_rnnt 8.291458 lr 0.00027961 rank 0
2022-12-06 23:53:41,915 DEBUG TRAIN Batch 20/3000 loss 25.884493 loss_att 395.493225 loss_ctc 40.989624 loss_rnnt 24.206146 lr 0.00027961 rank 2
2022-12-06 23:53:41,918 DEBUG TRAIN Batch 20/3000 loss 8.289341 loss_att 366.099426 loss_ctc 15.869088 loss_rnnt 7.447147 lr 0.00027961 rank 1
2022-12-06 23:53:41,918 DEBUG TRAIN Batch 20/3000 loss 16.512049 loss_att 339.701965 loss_ctc 27.582333 loss_rnnt 15.282017 lr 0.00027961 rank 3
2022-12-06 23:54:38,780 DEBUG TRAIN Batch 20/3100 loss 10.882692 loss_att 360.886841 loss_ctc 22.324589 loss_rnnt 9.611370 lr 0.00027957 rank 3
2022-12-06 23:54:38,781 DEBUG TRAIN Batch 20/3100 loss 6.971542 loss_att 303.070679 loss_ctc 16.994761 loss_rnnt 5.857852 lr 0.00027957 rank 1
2022-12-06 23:54:38,788 DEBUG TRAIN Batch 20/3100 loss 3.597419 loss_att 246.526230 loss_ctc 8.303326 loss_rnnt 3.074540 lr 0.00027957 rank 0
2022-12-06 23:54:38,818 DEBUG TRAIN Batch 20/3100 loss 10.261567 loss_att 343.122803 loss_ctc 22.543993 loss_rnnt 8.896853 lr 0.00027957 rank 2
2022-12-06 23:55:56,880 DEBUG TRAIN Batch 20/3200 loss 13.412886 loss_att 282.961548 loss_ctc 22.762108 loss_rnnt 12.374084 lr 0.00027952 rank 3
2022-12-06 23:55:56,881 DEBUG TRAIN Batch 20/3200 loss 11.884079 loss_att 206.538101 loss_ctc 24.884706 loss_rnnt 10.439565 lr 0.00027952 rank 2
2022-12-06 23:55:56,884 DEBUG TRAIN Batch 20/3200 loss 9.666764 loss_att 353.000580 loss_ctc 24.956524 loss_rnnt 7.967902 lr 0.00027952 rank 0
2022-12-06 23:55:56,889 DEBUG TRAIN Batch 20/3200 loss 7.092422 loss_att 416.180908 loss_ctc 14.312549 loss_rnnt 6.290185 lr 0.00027952 rank 1
2022-12-06 23:56:53,517 DEBUG TRAIN Batch 20/3300 loss 9.287494 loss_att 369.518158 loss_ctc 19.056805 loss_rnnt 8.202015 lr 0.00027948 rank 2
2022-12-06 23:56:53,519 DEBUG TRAIN Batch 20/3300 loss 5.450237 loss_att 183.353317 loss_ctc 15.320788 loss_rnnt 4.353509 lr 0.00027948 rank 3
2022-12-06 23:56:53,536 DEBUG TRAIN Batch 20/3300 loss 5.681196 loss_att 388.002167 loss_ctc 11.522777 loss_rnnt 5.032132 lr 0.00027948 rank 1
2022-12-06 23:56:53,540 DEBUG TRAIN Batch 20/3300 loss 2.607006 loss_att 308.479248 loss_ctc 6.519774 loss_rnnt 2.172254 lr 0.00027948 rank 0
2022-12-06 23:57:50,143 DEBUG TRAIN Batch 20/3400 loss 4.013287 loss_att 325.607544 loss_ctc 7.699377 loss_rnnt 3.603721 lr 0.00027943 rank 1
2022-12-06 23:57:50,148 DEBUG TRAIN Batch 20/3400 loss 1.754284 loss_att 358.918091 loss_ctc 7.101089 loss_rnnt 1.160195 lr 0.00027943 rank 3
2022-12-06 23:57:50,152 DEBUG TRAIN Batch 20/3400 loss 17.419897 loss_att 385.451233 loss_ctc 32.893093 loss_rnnt 15.700653 lr 0.00027943 rank 2
2022-12-06 23:57:50,156 DEBUG TRAIN Batch 20/3400 loss 7.010273 loss_att 411.949707 loss_ctc 14.199725 loss_rnnt 6.211446 lr 0.00027943 rank 0
2022-12-06 23:58:46,974 DEBUG TRAIN Batch 20/3500 loss 4.694548 loss_att 359.513916 loss_ctc 11.002794 loss_rnnt 3.993631 lr 0.00027939 rank 3
2022-12-06 23:58:46,977 DEBUG TRAIN Batch 20/3500 loss 12.374454 loss_att 422.487152 loss_ctc 27.700123 loss_rnnt 10.671602 lr 0.00027939 rank 1
2022-12-06 23:58:46,979 DEBUG TRAIN Batch 20/3500 loss 15.154652 loss_att 399.493713 loss_ctc 26.070555 loss_rnnt 13.941773 lr 0.00027939 rank 2
2022-12-06 23:58:46,983 DEBUG TRAIN Batch 20/3500 loss 4.528500 loss_att 371.319824 loss_ctc 9.633329 loss_rnnt 3.961296 lr 0.00027939 rank 0
2022-12-06 23:59:56,752 DEBUG TRAIN Batch 20/3600 loss 4.716913 loss_att 414.927307 loss_ctc 11.770477 loss_rnnt 3.933184 lr 0.00027935 rank 3
2022-12-06 23:59:56,755 DEBUG TRAIN Batch 20/3600 loss 5.395770 loss_att 292.606720 loss_ctc 11.611298 loss_rnnt 4.705156 lr 0.00027935 rank 2
2022-12-06 23:59:56,758 DEBUG TRAIN Batch 20/3600 loss 17.219051 loss_att 409.949097 loss_ctc 27.355801 loss_rnnt 16.092747 lr 0.00027935 rank 0
2022-12-06 23:59:56,758 DEBUG TRAIN Batch 20/3600 loss 6.307429 loss_att 324.627136 loss_ctc 12.939926 loss_rnnt 5.570486 lr 0.00027935 rank 1
2022-12-07 00:00:52,223 DEBUG TRAIN Batch 20/3700 loss 5.540869 loss_att 373.910156 loss_ctc 13.361043 loss_rnnt 4.671961 lr 0.00027930 rank 3
2022-12-07 00:00:52,226 DEBUG TRAIN Batch 20/3700 loss 8.003690 loss_att 302.758240 loss_ctc 13.471865 loss_rnnt 7.396115 lr 0.00027930 rank 1
2022-12-07 00:00:52,230 DEBUG TRAIN Batch 20/3700 loss 6.207178 loss_att 349.555878 loss_ctc 16.066784 loss_rnnt 5.111666 lr 0.00027930 rank 0
2022-12-07 00:00:52,265 DEBUG TRAIN Batch 20/3700 loss 7.366077 loss_att 328.508179 loss_ctc 20.776384 loss_rnnt 5.876043 lr 0.00027930 rank 2
2022-12-07 00:01:48,130 DEBUG TRAIN Batch 20/3800 loss 7.414238 loss_att 135.711716 loss_ctc 12.640105 loss_rnnt 6.833587 lr 0.00027926 rank 0
2022-12-07 00:01:48,139 DEBUG TRAIN Batch 20/3800 loss 8.509492 loss_att 367.416748 loss_ctc 18.086273 loss_rnnt 7.445405 lr 0.00027926 rank 3
2022-12-07 00:01:48,141 DEBUG TRAIN Batch 20/3800 loss 5.093541 loss_att 200.980896 loss_ctc 9.888083 loss_rnnt 4.560814 lr 0.00027926 rank 2
2022-12-07 00:01:48,146 DEBUG TRAIN Batch 20/3800 loss 21.182465 loss_att 448.301025 loss_ctc 38.435574 loss_rnnt 19.265453 lr 0.00027926 rank 1
2022-12-07 00:02:53,888 DEBUG TRAIN Batch 20/3900 loss 7.657948 loss_att 463.266113 loss_ctc 29.732391 loss_rnnt 5.205232 lr 0.00027922 rank 1
2022-12-07 00:02:53,895 DEBUG TRAIN Batch 20/3900 loss 7.160347 loss_att 426.093140 loss_ctc 12.421829 loss_rnnt 6.575738 lr 0.00027922 rank 0
2022-12-07 00:02:53,903 DEBUG TRAIN Batch 20/3900 loss 6.740626 loss_att 252.699417 loss_ctc 11.542784 loss_rnnt 6.207054 lr 0.00027922 rank 3
2022-12-07 00:02:53,907 DEBUG TRAIN Batch 20/3900 loss 3.130851 loss_att 412.121460 loss_ctc 8.190985 loss_rnnt 2.568614 lr 0.00027922 rank 2
2022-12-07 00:03:52,579 DEBUG TRAIN Batch 20/4000 loss 8.147007 loss_att 97.704018 loss_ctc 11.716771 loss_rnnt 7.750366 lr 0.00027917 rank 3
2022-12-07 00:03:52,588 DEBUG TRAIN Batch 20/4000 loss 18.199919 loss_att 405.287811 loss_ctc 33.321037 loss_rnnt 16.519794 lr 0.00027917 rank 2
2022-12-07 00:03:52,600 DEBUG TRAIN Batch 20/4000 loss 5.206074 loss_att 352.254272 loss_ctc 14.213832 loss_rnnt 4.205212 lr 0.00027917 rank 1
2022-12-07 00:03:52,622 DEBUG TRAIN Batch 20/4000 loss 7.065170 loss_att 350.455048 loss_ctc 14.556560 loss_rnnt 6.232793 lr 0.00027917 rank 0
2022-12-07 00:04:49,193 DEBUG TRAIN Batch 20/4100 loss 13.241874 loss_att 412.112366 loss_ctc 20.956757 loss_rnnt 12.384665 lr 0.00027913 rank 1
2022-12-07 00:04:49,199 DEBUG TRAIN Batch 20/4100 loss 7.165212 loss_att 322.960144 loss_ctc 16.669506 loss_rnnt 6.109179 lr 0.00027913 rank 2
2022-12-07 00:04:49,200 DEBUG TRAIN Batch 20/4100 loss 8.458393 loss_att 417.267426 loss_ctc 22.823944 loss_rnnt 6.862222 lr 0.00027913 rank 3
2022-12-07 00:04:49,208 DEBUG TRAIN Batch 20/4100 loss 14.828999 loss_att 422.744965 loss_ctc 24.246286 loss_rnnt 13.782633 lr 0.00027913 rank 0
2022-12-07 00:05:46,182 DEBUG TRAIN Batch 20/4200 loss 7.311881 loss_att 328.301758 loss_ctc 13.017450 loss_rnnt 6.677928 lr 0.00027909 rank 1
2022-12-07 00:05:46,195 DEBUG TRAIN Batch 20/4200 loss 9.897183 loss_att 380.418152 loss_ctc 26.937916 loss_rnnt 8.003769 lr 0.00027909 rank 2
2022-12-07 00:05:46,200 DEBUG TRAIN Batch 20/4200 loss 14.049048 loss_att 387.888428 loss_ctc 21.962439 loss_rnnt 13.169783 lr 0.00027909 rank 3
2022-12-07 00:05:46,204 DEBUG TRAIN Batch 20/4200 loss 14.161469 loss_att 395.854645 loss_ctc 26.490845 loss_rnnt 12.791538 lr 0.00027909 rank 0
2022-12-07 00:06:56,605 DEBUG TRAIN Batch 20/4300 loss 8.570411 loss_att 298.964966 loss_ctc 18.617689 loss_rnnt 7.454047 lr 0.00027904 rank 1
2022-12-07 00:06:56,615 DEBUG TRAIN Batch 20/4300 loss 3.139968 loss_att 327.853638 loss_ctc 5.165848 loss_rnnt 2.914870 lr 0.00027904 rank 3
2022-12-07 00:06:56,617 DEBUG TRAIN Batch 20/4300 loss 5.172839 loss_att 345.776459 loss_ctc 10.551133 loss_rnnt 4.575251 lr 0.00027904 rank 2
2022-12-07 00:06:56,623 DEBUG TRAIN Batch 20/4300 loss 10.735456 loss_att 318.768066 loss_ctc 15.868044 loss_rnnt 10.165169 lr 0.00027904 rank 0
2022-12-07 00:07:53,605 DEBUG TRAIN Batch 20/4400 loss 8.944273 loss_att 363.454041 loss_ctc 14.665248 loss_rnnt 8.308609 lr 0.00027900 rank 3
2022-12-07 00:07:53,607 DEBUG TRAIN Batch 20/4400 loss 16.504259 loss_att 423.982727 loss_ctc 31.875660 loss_rnnt 14.796326 lr 0.00027900 rank 2
2022-12-07 00:07:53,612 DEBUG TRAIN Batch 20/4400 loss 12.838242 loss_att 257.275818 loss_ctc 20.835297 loss_rnnt 11.949680 lr 0.00027900 rank 0
2022-12-07 00:07:53,637 DEBUG TRAIN Batch 20/4400 loss 9.006475 loss_att 75.386337 loss_ctc 11.601979 loss_rnnt 8.718086 lr 0.00027900 rank 1
2022-12-07 00:08:50,023 DEBUG TRAIN Batch 20/4500 loss 8.273844 loss_att 357.088867 loss_ctc 18.519110 loss_rnnt 7.135481 lr 0.00027896 rank 3
2022-12-07 00:08:50,036 DEBUG TRAIN Batch 20/4500 loss 9.097610 loss_att 374.259735 loss_ctc 15.650204 loss_rnnt 8.369544 lr 0.00027896 rank 1
2022-12-07 00:08:50,039 DEBUG TRAIN Batch 20/4500 loss 8.643303 loss_att 240.872116 loss_ctc 14.609832 loss_rnnt 7.980355 lr 0.00027896 rank 2
2022-12-07 00:08:50,045 DEBUG TRAIN Batch 20/4500 loss 8.808075 loss_att 432.692444 loss_ctc 16.741133 loss_rnnt 7.926624 lr 0.00027896 rank 0
2022-12-07 00:09:56,513 DEBUG TRAIN Batch 20/4600 loss 1.442581 loss_att 312.252319 loss_ctc 8.810808 loss_rnnt 0.623889 lr 0.00027891 rank 1
2022-12-07 00:09:56,514 DEBUG TRAIN Batch 20/4600 loss 6.168434 loss_att 385.261597 loss_ctc 11.592323 loss_rnnt 5.565780 lr 0.00027891 rank 2
2022-12-07 00:09:56,520 DEBUG TRAIN Batch 20/4600 loss 8.693598 loss_att 251.273560 loss_ctc 16.874653 loss_rnnt 7.784591 lr 0.00027891 rank 3
2022-12-07 00:09:56,529 DEBUG TRAIN Batch 20/4600 loss 6.229158 loss_att 425.222290 loss_ctc 10.416195 loss_rnnt 5.763933 lr 0.00027891 rank 0
2022-12-07 00:10:55,835 DEBUG TRAIN Batch 20/4700 loss 6.200819 loss_att 357.436523 loss_ctc 14.884780 loss_rnnt 5.235934 lr 0.00027887 rank 3
2022-12-07 00:10:55,847 DEBUG TRAIN Batch 20/4700 loss 7.362806 loss_att 380.858856 loss_ctc 17.012518 loss_rnnt 6.290616 lr 0.00027887 rank 2
2022-12-07 00:10:55,866 DEBUG TRAIN Batch 20/4700 loss 9.996240 loss_att 403.518463 loss_ctc 25.455757 loss_rnnt 8.278516 lr 0.00027887 rank 0
2022-12-07 00:10:55,884 DEBUG TRAIN Batch 20/4700 loss 8.743420 loss_att 377.795441 loss_ctc 14.919553 loss_rnnt 8.057182 lr 0.00027887 rank 1
2022-12-07 00:11:52,005 DEBUG TRAIN Batch 20/4800 loss 10.178735 loss_att 345.131531 loss_ctc 17.533710 loss_rnnt 9.361515 lr 0.00027883 rank 2
2022-12-07 00:11:52,006 DEBUG TRAIN Batch 20/4800 loss 6.467133 loss_att 380.268311 loss_ctc 12.226322 loss_rnnt 5.827223 lr 0.00027883 rank 1
2022-12-07 00:11:52,007 DEBUG TRAIN Batch 20/4800 loss 8.860245 loss_att 372.059906 loss_ctc 16.740324 loss_rnnt 7.984680 lr 0.00027883 rank 3
2022-12-07 00:11:52,028 DEBUG TRAIN Batch 20/4800 loss 9.680605 loss_att 398.487854 loss_ctc 20.144258 loss_rnnt 8.517977 lr 0.00027883 rank 0
2022-12-07 00:12:49,044 DEBUG TRAIN Batch 20/4900 loss 5.194659 loss_att 343.067352 loss_ctc 13.039492 loss_rnnt 4.323010 lr 0.00027878 rank 1
2022-12-07 00:12:49,056 DEBUG TRAIN Batch 20/4900 loss 8.325732 loss_att 303.755493 loss_ctc 15.488736 loss_rnnt 7.529843 lr 0.00027878 rank 2
2022-12-07 00:12:49,060 DEBUG TRAIN Batch 20/4900 loss 6.106792 loss_att 315.991333 loss_ctc 11.805243 loss_rnnt 5.473631 lr 0.00027878 rank 0
2022-12-07 00:12:49,066 DEBUG TRAIN Batch 20/4900 loss 8.435080 loss_att 447.083832 loss_ctc 17.017344 loss_rnnt 7.481495 lr 0.00027878 rank 3
2022-12-07 00:13:57,330 DEBUG TRAIN Batch 20/5000 loss 8.736911 loss_att 397.114960 loss_ctc 20.655758 loss_rnnt 7.412595 lr 0.00027874 rank 3
2022-12-07 00:13:57,346 DEBUG TRAIN Batch 20/5000 loss 9.935580 loss_att 430.624207 loss_ctc 18.219112 loss_rnnt 9.015188 lr 0.00027874 rank 2
2022-12-07 00:13:57,351 DEBUG TRAIN Batch 20/5000 loss 15.238064 loss_att 281.803253 loss_ctc 23.679482 loss_rnnt 14.300128 lr 0.00027874 rank 0
2022-12-07 00:13:57,352 DEBUG TRAIN Batch 20/5000 loss 9.994565 loss_att 235.773438 loss_ctc 17.998165 loss_rnnt 9.105277 lr 0.00027874 rank 1
2022-12-07 00:14:53,886 DEBUG TRAIN Batch 20/5100 loss 9.250648 loss_att 374.495483 loss_ctc 17.689108 loss_rnnt 8.313042 lr 0.00027870 rank 3
2022-12-07 00:14:53,894 DEBUG TRAIN Batch 20/5100 loss 5.422324 loss_att 292.100006 loss_ctc 15.116266 loss_rnnt 4.345219 lr 0.00027870 rank 2
2022-12-07 00:14:53,898 DEBUG TRAIN Batch 20/5100 loss 6.745697 loss_att 68.505226 loss_ctc 10.144849 loss_rnnt 6.368014 lr 0.00027870 rank 0
2022-12-07 00:14:53,914 DEBUG TRAIN Batch 20/5100 loss 6.854385 loss_att 425.333191 loss_ctc 21.760023 loss_rnnt 5.198203 lr 0.00027870 rank 1
2022-12-07 00:15:50,230 DEBUG TRAIN Batch 20/5200 loss 5.236657 loss_att 255.440964 loss_ctc 10.422292 loss_rnnt 4.660476 lr 0.00027865 rank 3
2022-12-07 00:15:50,236 DEBUG TRAIN Batch 20/5200 loss 11.692262 loss_att 407.800629 loss_ctc 17.696442 loss_rnnt 11.025131 lr 0.00027865 rank 2
2022-12-07 00:15:50,236 DEBUG TRAIN Batch 20/5200 loss 8.007600 loss_att 395.262695 loss_ctc 13.847537 loss_rnnt 7.358718 lr 0.00027865 rank 1
2022-12-07 00:15:50,237 DEBUG TRAIN Batch 20/5200 loss 7.688270 loss_att 393.908295 loss_ctc 18.103455 loss_rnnt 6.531027 lr 0.00027865 rank 0
2022-12-07 00:16:52,832 DEBUG TRAIN Batch 20/5300 loss 8.317768 loss_att 386.941040 loss_ctc 16.056484 loss_rnnt 7.457911 lr 0.00027861 rank 2
2022-12-07 00:16:52,844 DEBUG TRAIN Batch 20/5300 loss 7.485786 loss_att 272.352173 loss_ctc 14.570225 loss_rnnt 6.698627 lr 0.00027861 rank 3
2022-12-07 00:16:52,848 DEBUG TRAIN Batch 20/5300 loss 8.556295 loss_att 381.000732 loss_ctc 15.299685 loss_rnnt 7.807030 lr 0.00027861 rank 1
2022-12-07 00:16:52,853 DEBUG TRAIN Batch 20/5300 loss 6.456379 loss_att 357.829590 loss_ctc 19.363623 loss_rnnt 5.022241 lr 0.00027861 rank 0
2022-12-07 00:17:56,134 DEBUG TRAIN Batch 20/5400 loss 13.844839 loss_att 420.796234 loss_ctc 30.894573 loss_rnnt 11.950424 lr 0.00027857 rank 3
2022-12-07 00:17:56,149 DEBUG TRAIN Batch 20/5400 loss 7.176268 loss_att 345.211975 loss_ctc 17.245544 loss_rnnt 6.057460 lr 0.00027857 rank 2
2022-12-07 00:17:56,150 DEBUG TRAIN Batch 20/5400 loss 15.520467 loss_att 426.239380 loss_ctc 31.766821 loss_rnnt 13.715317 lr 0.00027857 rank 1
2022-12-07 00:17:56,162 DEBUG TRAIN Batch 20/5400 loss 6.288501 loss_att 385.944641 loss_ctc 15.340528 loss_rnnt 5.282721 lr 0.00027857 rank 0
2022-12-07 00:18:51,828 DEBUG TRAIN Batch 20/5500 loss 3.390976 loss_att 277.987793 loss_ctc 6.898214 loss_rnnt 3.001283 lr 0.00027852 rank 1
2022-12-07 00:18:51,839 DEBUG TRAIN Batch 20/5500 loss 7.482550 loss_att 353.700439 loss_ctc 14.228170 loss_rnnt 6.733036 lr 0.00027852 rank 3
2022-12-07 00:18:51,845 DEBUG TRAIN Batch 20/5500 loss 5.641521 loss_att 320.654663 loss_ctc 12.284348 loss_rnnt 4.903429 lr 0.00027852 rank 0
2022-12-07 00:18:51,875 DEBUG TRAIN Batch 20/5500 loss 5.189038 loss_att 345.913483 loss_ctc 10.604225 loss_rnnt 4.587351 lr 0.00027852 rank 2
2022-12-07 00:19:48,182 DEBUG TRAIN Batch 20/5600 loss 9.131851 loss_att 390.768372 loss_ctc 17.008059 loss_rnnt 8.256717 lr 0.00027848 rank 3
2022-12-07 00:19:48,186 DEBUG TRAIN Batch 20/5600 loss 7.356329 loss_att 350.806519 loss_ctc 14.856447 loss_rnnt 6.522983 lr 0.00027848 rank 2
2022-12-07 00:19:48,190 DEBUG TRAIN Batch 20/5600 loss 7.040030 loss_att 264.937897 loss_ctc 12.239792 loss_rnnt 6.462279 lr 0.00027848 rank 1
2022-12-07 00:19:48,204 DEBUG TRAIN Batch 20/5600 loss 8.156696 loss_att 293.075287 loss_ctc 16.476074 loss_rnnt 7.232321 lr 0.00027848 rank 0
2022-12-07 00:20:55,546 DEBUG TRAIN Batch 20/5700 loss 8.668970 loss_att 398.372437 loss_ctc 21.233534 loss_rnnt 7.272908 lr 0.00027844 rank 3
2022-12-07 00:20:55,552 DEBUG TRAIN Batch 20/5700 loss 4.749772 loss_att 409.217865 loss_ctc 8.520668 loss_rnnt 4.330784 lr 0.00027844 rank 1
2022-12-07 00:20:55,552 DEBUG TRAIN Batch 20/5700 loss 9.133378 loss_att 306.451355 loss_ctc 17.177179 loss_rnnt 8.239623 lr 0.00027844 rank 2
2022-12-07 00:20:55,579 DEBUG TRAIN Batch 20/5700 loss 7.930978 loss_att 313.014496 loss_ctc 19.930605 loss_rnnt 6.597687 lr 0.00027844 rank 0
2022-12-07 00:21:51,213 DEBUG TRAIN Batch 20/5800 loss 5.888655 loss_att 348.372925 loss_ctc 12.686320 loss_rnnt 5.133359 lr 0.00027839 rank 3
2022-12-07 00:21:51,217 DEBUG TRAIN Batch 20/5800 loss 6.102142 loss_att 378.070648 loss_ctc 13.640266 loss_rnnt 5.264573 lr 0.00027839 rank 1
2022-12-07 00:21:51,221 DEBUG TRAIN Batch 20/5800 loss 12.632700 loss_att 76.909195 loss_ctc 17.013891 loss_rnnt 12.145901 lr 0.00027839 rank 2
2022-12-07 00:21:51,227 DEBUG TRAIN Batch 20/5800 loss 10.841864 loss_att 378.402344 loss_ctc 19.094046 loss_rnnt 9.924954 lr 0.00027839 rank 0
2022-12-07 00:22:46,377 DEBUG TRAIN Batch 20/5900 loss 8.849971 loss_att 264.991089 loss_ctc 13.858412 loss_rnnt 8.293478 lr 0.00027835 rank 3
2022-12-07 00:22:46,378 DEBUG TRAIN Batch 20/5900 loss 10.974576 loss_att 348.545410 loss_ctc 16.728931 loss_rnnt 10.335203 lr 0.00027835 rank 1
2022-12-07 00:22:46,386 DEBUG TRAIN Batch 20/5900 loss 11.828873 loss_att 396.067505 loss_ctc 30.903614 loss_rnnt 9.709457 lr 0.00027835 rank 0
2022-12-07 00:22:46,423 DEBUG TRAIN Batch 20/5900 loss 13.360476 loss_att 453.778656 loss_ctc 22.184233 loss_rnnt 12.380058 lr 0.00027835 rank 2
2022-12-07 00:23:42,133 DEBUG TRAIN Batch 20/6000 loss 5.586475 loss_att 364.925049 loss_ctc 10.748314 loss_rnnt 5.012937 lr 0.00027831 rank 2
2022-12-07 00:23:42,136 DEBUG TRAIN Batch 20/6000 loss 6.343595 loss_att 379.677795 loss_ctc 15.283568 loss_rnnt 5.350264 lr 0.00027831 rank 1
2022-12-07 00:23:42,146 DEBUG TRAIN Batch 20/6000 loss 12.939448 loss_att 439.164307 loss_ctc 18.048401 loss_rnnt 12.371787 lr 0.00027831 rank 0
2022-12-07 00:23:42,198 DEBUG TRAIN Batch 20/6000 loss 8.254881 loss_att 441.109375 loss_ctc 16.828415 loss_rnnt 7.302266 lr 0.00027831 rank 3
2022-12-07 00:24:49,555 DEBUG TRAIN Batch 20/6100 loss 4.371824 loss_att 370.280090 loss_ctc 11.191978 loss_rnnt 3.614030 lr 0.00027826 rank 3
2022-12-07 00:24:49,575 DEBUG TRAIN Batch 20/6100 loss 7.910113 loss_att 363.821930 loss_ctc 19.497986 loss_rnnt 6.622572 lr 0.00027826 rank 2
2022-12-07 00:24:49,580 DEBUG TRAIN Batch 20/6100 loss 4.555745 loss_att 349.945221 loss_ctc 16.754185 loss_rnnt 3.200363 lr 0.00027826 rank 1
2022-12-07 00:24:49,585 DEBUG TRAIN Batch 20/6100 loss 13.132991 loss_att 340.548737 loss_ctc 22.021427 loss_rnnt 12.145388 lr 0.00027826 rank 0
2022-12-07 00:25:44,646 DEBUG TRAIN Batch 20/6200 loss 8.918426 loss_att 388.568695 loss_ctc 15.701210 loss_rnnt 8.164783 lr 0.00027822 rank 3
2022-12-07 00:25:44,648 DEBUG TRAIN Batch 20/6200 loss 6.194729 loss_att 323.716003 loss_ctc 13.097045 loss_rnnt 5.427804 lr 0.00027822 rank 0
2022-12-07 00:25:44,656 DEBUG TRAIN Batch 20/6200 loss 15.755998 loss_att 268.365692 loss_ctc 25.139071 loss_rnnt 14.713434 lr 0.00027822 rank 1
2022-12-07 00:25:44,657 DEBUG TRAIN Batch 20/6200 loss 21.715178 loss_att 374.863281 loss_ctc 36.404728 loss_rnnt 20.083006 lr 0.00027822 rank 2
2022-12-07 00:26:40,538 DEBUG TRAIN Batch 20/6300 loss 6.323484 loss_att 317.083649 loss_ctc 14.769687 loss_rnnt 5.385017 lr 0.00027818 rank 2
2022-12-07 00:26:40,552 DEBUG TRAIN Batch 20/6300 loss 1.737933 loss_att 323.546875 loss_ctc 6.409377 loss_rnnt 1.218884 lr 0.00027818 rank 1
2022-12-07 00:26:40,552 DEBUG TRAIN Batch 20/6300 loss 3.979736 loss_att 324.789307 loss_ctc 11.636162 loss_rnnt 3.129022 lr 0.00027818 rank 3
2022-12-07 00:26:40,561 DEBUG TRAIN Batch 20/6300 loss 7.634732 loss_att 81.219124 loss_ctc 12.431684 loss_rnnt 7.101737 lr 0.00027818 rank 0
2022-12-07 00:27:47,898 DEBUG TRAIN Batch 20/6400 loss 8.412914 loss_att 123.431610 loss_ctc 11.051906 loss_rnnt 8.119694 lr 0.00027813 rank 2
2022-12-07 00:27:47,913 DEBUG TRAIN Batch 20/6400 loss 10.562475 loss_att 311.972717 loss_ctc 18.192402 loss_rnnt 9.714705 lr 0.00027813 rank 3
2022-12-07 00:27:47,919 DEBUG TRAIN Batch 20/6400 loss 7.796194 loss_att 381.311218 loss_ctc 20.776711 loss_rnnt 6.353914 lr 0.00027813 rank 1
2022-12-07 00:27:47,923 DEBUG TRAIN Batch 20/6400 loss 4.273379 loss_att 324.810974 loss_ctc 9.559806 loss_rnnt 3.685998 lr 0.00027813 rank 0
2022-12-07 00:28:43,924 DEBUG TRAIN Batch 20/6500 loss 6.736345 loss_att 328.000824 loss_ctc 13.228371 loss_rnnt 6.015009 lr 0.00027809 rank 2
2022-12-07 00:28:43,925 DEBUG TRAIN Batch 20/6500 loss 2.977772 loss_att 302.032501 loss_ctc 7.880554 loss_rnnt 2.433019 lr 0.00027809 rank 3
2022-12-07 00:28:43,926 DEBUG TRAIN Batch 20/6500 loss 12.949069 loss_att 421.561462 loss_ctc 26.979872 loss_rnnt 11.390091 lr 0.00027809 rank 1
2022-12-07 00:28:43,936 DEBUG TRAIN Batch 20/6500 loss 5.929604 loss_att 338.795044 loss_ctc 10.149017 loss_rnnt 5.460781 lr 0.00027809 rank 0
2022-12-07 00:29:39,566 DEBUG TRAIN Batch 20/6600 loss 8.648144 loss_att 451.985596 loss_ctc 20.694620 loss_rnnt 7.309647 lr 0.00027805 rank 2
2022-12-07 00:29:39,572 DEBUG TRAIN Batch 20/6600 loss 18.351416 loss_att 424.260925 loss_ctc 28.817698 loss_rnnt 17.188496 lr 0.00027805 rank 1
2022-12-07 00:29:39,578 DEBUG TRAIN Batch 20/6600 loss 8.973259 loss_att 186.622589 loss_ctc 16.549019 loss_rnnt 8.131508 lr 0.00027805 rank 3
2022-12-07 00:29:39,592 DEBUG TRAIN Batch 20/6600 loss 11.712420 loss_att 358.867493 loss_ctc 22.868942 loss_rnnt 10.472805 lr 0.00027805 rank 0
2022-12-07 00:30:34,644 DEBUG TRAIN Batch 20/6700 loss 5.998405 loss_att 363.182495 loss_ctc 18.717405 loss_rnnt 4.585184 lr 0.00027801 rank 1
2022-12-07 00:30:34,645 DEBUG TRAIN Batch 20/6700 loss 7.161946 loss_att 372.460419 loss_ctc 17.669640 loss_rnnt 5.994424 lr 0.00027801 rank 3
2022-12-07 00:30:34,650 DEBUG TRAIN Batch 20/6700 loss 7.375415 loss_att 354.713531 loss_ctc 13.472197 loss_rnnt 6.697995 lr 0.00027801 rank 2
2022-12-07 00:30:34,650 DEBUG TRAIN Batch 20/6700 loss 13.993279 loss_att 345.095581 loss_ctc 20.282366 loss_rnnt 13.294493 lr 0.00027801 rank 0
2022-12-07 00:31:41,426 DEBUG TRAIN Batch 20/6800 loss 18.660942 loss_att 307.401917 loss_ctc 31.651495 loss_rnnt 17.217548 lr 0.00027796 rank 0
2022-12-07 00:31:41,438 DEBUG TRAIN Batch 20/6800 loss 9.205152 loss_att 358.911072 loss_ctc 12.825917 loss_rnnt 8.802845 lr 0.00027796 rank 2
2022-12-07 00:31:41,439 DEBUG TRAIN Batch 20/6800 loss 14.726608 loss_att 365.366180 loss_ctc 30.564787 loss_rnnt 12.966811 lr 0.00027796 rank 1
2022-12-07 00:31:41,441 DEBUG TRAIN Batch 20/6800 loss 4.182883 loss_att 392.353210 loss_ctc 14.908022 loss_rnnt 2.991201 lr 0.00027796 rank 3
2022-12-07 00:32:37,590 DEBUG TRAIN Batch 20/6900 loss 8.599488 loss_att 375.465759 loss_ctc 15.388348 loss_rnnt 7.845170 lr 0.00027792 rank 3
2022-12-07 00:32:37,591 DEBUG TRAIN Batch 20/6900 loss 16.285540 loss_att 214.167816 loss_ctc 24.927736 loss_rnnt 15.325295 lr 0.00027792 rank 1
2022-12-07 00:32:37,594 DEBUG TRAIN Batch 20/6900 loss 10.256467 loss_att 187.066376 loss_ctc 18.222836 loss_rnnt 9.371315 lr 0.00027792 rank 0
2022-12-07 00:32:37,596 DEBUG TRAIN Batch 20/6900 loss 5.291515 loss_att 301.807312 loss_ctc 14.277287 loss_rnnt 4.293096 lr 0.00027792 rank 2
2022-12-07 00:33:33,300 DEBUG TRAIN Batch 20/7000 loss 14.261395 loss_att 360.951447 loss_ctc 26.019615 loss_rnnt 12.954926 lr 0.00027788 rank 3
2022-12-07 00:33:33,313 DEBUG TRAIN Batch 20/7000 loss 6.482008 loss_att 376.396362 loss_ctc 13.421087 loss_rnnt 5.710999 lr 0.00027788 rank 1
2022-12-07 00:33:33,320 DEBUG TRAIN Batch 20/7000 loss 9.352910 loss_att 218.464142 loss_ctc 13.727447 loss_rnnt 8.866851 lr 0.00027788 rank 2
2022-12-07 00:33:33,323 DEBUG TRAIN Batch 20/7000 loss 11.241052 loss_att 457.126068 loss_ctc 20.554831 loss_rnnt 10.206188 lr 0.00027788 rank 0
2022-12-07 00:34:38,518 DEBUG TRAIN Batch 20/7100 loss 8.770591 loss_att 302.086395 loss_ctc 16.509130 loss_rnnt 7.910753 lr 0.00027783 rank 2
2022-12-07 00:34:38,520 DEBUG TRAIN Batch 20/7100 loss 7.724590 loss_att 429.215210 loss_ctc 12.349968 loss_rnnt 7.210659 lr 0.00027783 rank 1
2022-12-07 00:34:38,534 DEBUG TRAIN Batch 20/7100 loss 11.299836 loss_att 335.994690 loss_ctc 22.748240 loss_rnnt 10.027791 lr 0.00027783 rank 3
2022-12-07 00:34:38,537 DEBUG TRAIN Batch 20/7100 loss 6.473716 loss_att 310.576233 loss_ctc 14.357843 loss_rnnt 5.597702 lr 0.00027783 rank 0
2022-12-07 00:35:34,946 DEBUG TRAIN Batch 20/7200 loss 2.280071 loss_att 330.209503 loss_ctc 7.051997 loss_rnnt 1.749858 lr 0.00027779 rank 0
2022-12-07 00:35:34,956 DEBUG TRAIN Batch 20/7200 loss 15.024590 loss_att 391.939362 loss_ctc 29.329128 loss_rnnt 13.435198 lr 0.00027779 rank 1
2022-12-07 00:35:34,956 DEBUG TRAIN Batch 20/7200 loss 7.903614 loss_att 258.457214 loss_ctc 17.407679 loss_rnnt 6.847607 lr 0.00027779 rank 3
2022-12-07 00:35:34,960 DEBUG TRAIN Batch 20/7200 loss 6.368386 loss_att 360.381683 loss_ctc 13.557125 loss_rnnt 5.569637 lr 0.00027779 rank 2
2022-12-07 00:36:30,778 DEBUG TRAIN Batch 20/7300 loss 5.490685 loss_att 326.508240 loss_ctc 7.486725 loss_rnnt 5.268902 lr 0.00027775 rank 0
2022-12-07 00:36:30,777 DEBUG TRAIN Batch 20/7300 loss 13.814776 loss_att 476.637146 loss_ctc 24.659111 loss_rnnt 12.609851 lr 0.00027775 rank 3
2022-12-07 00:36:30,779 DEBUG TRAIN Batch 20/7300 loss 8.495372 loss_att 441.712402 loss_ctc 11.799416 loss_rnnt 8.128256 lr 0.00027775 rank 2
2022-12-07 00:36:30,785 DEBUG TRAIN Batch 20/7300 loss 13.545563 loss_att 394.044189 loss_ctc 26.977953 loss_rnnt 12.053076 lr 0.00027775 rank 1
2022-12-07 00:37:26,158 DEBUG TRAIN Batch 20/7400 loss 10.819196 loss_att 395.024994 loss_ctc 16.904654 loss_rnnt 10.143034 lr 0.00027771 rank 3
2022-12-07 00:37:26,162 DEBUG TRAIN Batch 20/7400 loss 14.451945 loss_att 371.519226 loss_ctc 27.580967 loss_rnnt 12.993165 lr 0.00027771 rank 2
2022-12-07 00:37:26,165 DEBUG TRAIN Batch 20/7400 loss 8.715787 loss_att 304.427795 loss_ctc 19.548477 loss_rnnt 7.512155 lr 0.00027771 rank 0
2022-12-07 00:37:26,171 DEBUG TRAIN Batch 20/7400 loss 4.562819 loss_att 316.193848 loss_ctc 8.414337 loss_rnnt 4.134873 lr 0.00027771 rank 1
2022-12-07 00:38:33,095 DEBUG TRAIN Batch 20/7500 loss 2.961761 loss_att 339.976074 loss_ctc 12.288886 loss_rnnt 1.925414 lr 0.00027766 rank 3
2022-12-07 00:38:33,097 DEBUG TRAIN Batch 20/7500 loss 6.625442 loss_att 263.008850 loss_ctc 14.367502 loss_rnnt 5.765213 lr 0.00027766 rank 1
2022-12-07 00:38:33,101 DEBUG TRAIN Batch 20/7500 loss 11.808891 loss_att 358.807037 loss_ctc 22.385296 loss_rnnt 10.633735 lr 0.00027766 rank 2
2022-12-07 00:38:33,110 DEBUG TRAIN Batch 20/7500 loss 9.036782 loss_att 276.851776 loss_ctc 16.785084 loss_rnnt 8.175860 lr 0.00027766 rank 0
2022-12-07 00:39:28,666 DEBUG TRAIN Batch 20/7600 loss 3.948714 loss_att 363.434265 loss_ctc 8.524165 loss_rnnt 3.440331 lr 0.00027762 rank 3
2022-12-07 00:39:28,668 DEBUG TRAIN Batch 20/7600 loss 5.323078 loss_att 446.335083 loss_ctc 10.007853 loss_rnnt 4.802548 lr 0.00027762 rank 0
2022-12-07 00:39:28,688 DEBUG TRAIN Batch 20/7600 loss 26.532997 loss_att 316.652313 loss_ctc 55.102421 loss_rnnt 23.358618 lr 0.00027762 rank 2
2022-12-07 00:39:28,695 DEBUG TRAIN Batch 20/7600 loss 10.759284 loss_att 421.498779 loss_ctc 23.451859 loss_rnnt 9.348998 lr 0.00027762 rank 1
2022-12-07 00:40:24,379 DEBUG TRAIN Batch 20/7700 loss 5.570297 loss_att 330.011017 loss_ctc 13.460550 loss_rnnt 4.693603 lr 0.00027758 rank 3
2022-12-07 00:40:24,401 DEBUG TRAIN Batch 20/7700 loss 6.047246 loss_att 175.377518 loss_ctc 12.707313 loss_rnnt 5.307239 lr 0.00027758 rank 2
2022-12-07 00:40:24,407 DEBUG TRAIN Batch 20/7700 loss 5.884400 loss_att 356.817108 loss_ctc 11.602209 loss_rnnt 5.249088 lr 0.00027758 rank 1
2022-12-07 00:40:24,408 DEBUG TRAIN Batch 20/7700 loss 11.254545 loss_att 408.294678 loss_ctc 22.139286 loss_rnnt 10.045130 lr 0.00027758 rank 0
2022-12-07 00:41:29,594 DEBUG TRAIN Batch 20/7800 loss 8.258160 loss_att 425.314667 loss_ctc 12.247704 loss_rnnt 7.814878 lr 0.00027753 rank 2
2022-12-07 00:41:29,604 DEBUG TRAIN Batch 20/7800 loss 15.238399 loss_att 345.871643 loss_ctc 24.324375 loss_rnnt 14.228846 lr 0.00027753 rank 3
2022-12-07 00:41:29,609 DEBUG TRAIN Batch 20/7800 loss 4.047304 loss_att 346.065002 loss_ctc 11.229419 loss_rnnt 3.249291 lr 0.00027753 rank 1
2022-12-07 00:41:29,615 DEBUG TRAIN Batch 20/7800 loss 12.651266 loss_att 373.718262 loss_ctc 24.567656 loss_rnnt 11.327223 lr 0.00027753 rank 0
2022-12-07 00:42:26,691 DEBUG TRAIN Batch 20/7900 loss 14.578937 loss_att 349.556366 loss_ctc 26.933676 loss_rnnt 13.206187 lr 0.00027749 rank 2
2022-12-07 00:42:26,692 DEBUG TRAIN Batch 20/7900 loss 15.794800 loss_att 390.966614 loss_ctc 33.287834 loss_rnnt 13.851130 lr 0.00027749 rank 0
2022-12-07 00:42:26,693 DEBUG TRAIN Batch 20/7900 loss 3.944921 loss_att 368.536499 loss_ctc 8.418288 loss_rnnt 3.447880 lr 0.00027749 rank 1
2022-12-07 00:42:26,716 DEBUG TRAIN Batch 20/7900 loss 8.751830 loss_att 63.893967 loss_ctc 12.410020 loss_rnnt 8.345365 lr 0.00027749 rank 3
2022-12-07 00:43:22,184 DEBUG TRAIN Batch 20/8000 loss 8.860318 loss_att 382.851135 loss_ctc 15.669312 loss_rnnt 8.103764 lr 0.00027745 rank 0
2022-12-07 00:43:22,192 DEBUG TRAIN Batch 20/8000 loss 8.598712 loss_att 335.761169 loss_ctc 20.639236 loss_rnnt 7.260877 lr 0.00027745 rank 2
2022-12-07 00:43:22,218 DEBUG TRAIN Batch 20/8000 loss 13.991016 loss_att 346.798035 loss_ctc 20.258678 loss_rnnt 13.294609 lr 0.00027745 rank 1
2022-12-07 00:43:22,220 DEBUG TRAIN Batch 20/8000 loss 7.122360 loss_att 380.962097 loss_ctc 12.768702 loss_rnnt 6.494989 lr 0.00027745 rank 3
2022-12-07 00:44:16,998 DEBUG TRAIN Batch 20/8100 loss 3.686177 loss_att 326.370789 loss_ctc 9.050776 loss_rnnt 3.090110 lr 0.00027741 rank 3
2022-12-07 00:44:17,001 DEBUG TRAIN Batch 20/8100 loss 5.313574 loss_att 384.797241 loss_ctc 11.692207 loss_rnnt 4.604837 lr 0.00027741 rank 2
2022-12-07 00:44:17,004 DEBUG TRAIN Batch 20/8100 loss 12.414374 loss_att 280.512451 loss_ctc 17.034231 loss_rnnt 11.901057 lr 0.00027741 rank 0
2022-12-07 00:44:17,005 DEBUG TRAIN Batch 20/8100 loss 8.355169 loss_att 128.148544 loss_ctc 14.526477 loss_rnnt 7.669469 lr 0.00027741 rank 1
2022-12-07 00:45:21,977 DEBUG TRAIN Batch 20/8200 loss 6.746048 loss_att 382.483582 loss_ctc 16.491013 loss_rnnt 5.663274 lr 0.00027736 rank 2
2022-12-07 00:45:21,979 DEBUG TRAIN Batch 20/8200 loss 4.067842 loss_att 425.591492 loss_ctc 13.350744 loss_rnnt 3.036408 lr 0.00027736 rank 1
2022-12-07 00:45:21,980 DEBUG TRAIN Batch 20/8200 loss 10.033009 loss_att 431.237213 loss_ctc 19.859859 loss_rnnt 8.941136 lr 0.00027736 rank 3
2022-12-07 00:45:21,985 DEBUG TRAIN Batch 20/8200 loss 12.903454 loss_att 427.637421 loss_ctc 25.225315 loss_rnnt 11.534358 lr 0.00027736 rank 0
2022-12-07 00:46:20,078 DEBUG TRAIN Batch 20/8300 loss 4.529803 loss_att 370.023621 loss_ctc 10.174025 loss_rnnt 3.902668 lr 0.00027732 rank 0
2022-12-07 00:46:20,084 DEBUG TRAIN Batch 20/8300 loss 10.788363 loss_att 387.251495 loss_ctc 24.572287 loss_rnnt 9.256816 lr 0.00027732 rank 3
2022-12-07 00:46:20,087 DEBUG TRAIN Batch 20/8300 loss 14.059503 loss_att 234.897888 loss_ctc 27.350180 loss_rnnt 12.582762 lr 0.00027732 rank 2
2022-12-07 00:46:20,087 DEBUG TRAIN Batch 20/8300 loss 2.963290 loss_att 363.049377 loss_ctc 9.911598 loss_rnnt 2.191256 lr 0.00027732 rank 1
2022-12-07 00:47:16,085 DEBUG TRAIN Batch 20/8400 loss 8.561392 loss_att 344.455170 loss_ctc 23.688740 loss_rnnt 6.880575 lr 0.00027728 rank 2
2022-12-07 00:47:16,086 DEBUG TRAIN Batch 20/8400 loss 6.492008 loss_att 348.175598 loss_ctc 15.086268 loss_rnnt 5.537090 lr 0.00027728 rank 1
2022-12-07 00:47:16,095 DEBUG TRAIN Batch 20/8400 loss 6.362715 loss_att 330.872498 loss_ctc 12.355350 loss_rnnt 5.696867 lr 0.00027728 rank 3
2022-12-07 00:47:16,106 DEBUG TRAIN Batch 20/8400 loss 14.208206 loss_att 398.251801 loss_ctc 33.569801 loss_rnnt 12.056918 lr 0.00027728 rank 0
2022-12-07 00:48:20,836 DEBUG TRAIN Batch 20/8500 loss 7.933123 loss_att 404.425964 loss_ctc 13.290889 loss_rnnt 7.337816 lr 0.00027724 rank 2
2022-12-07 00:48:20,842 DEBUG TRAIN Batch 20/8500 loss 14.564522 loss_att 386.272949 loss_ctc 34.013138 loss_rnnt 12.403564 lr 0.00027724 rank 1
2022-12-07 00:48:20,850 DEBUG TRAIN Batch 20/8500 loss 9.657519 loss_att 213.661575 loss_ctc 16.472616 loss_rnnt 8.900287 lr 0.00027724 rank 3
2022-12-07 00:48:20,863 DEBUG TRAIN Batch 20/8500 loss 11.542669 loss_att 393.153015 loss_ctc 17.533400 loss_rnnt 10.877033 lr 0.00027724 rank 0
2022-12-07 00:49:17,637 DEBUG TRAIN Batch 20/8600 loss 7.328112 loss_att 453.748871 loss_ctc 17.636259 loss_rnnt 6.182762 lr 0.00027719 rank 3
2022-12-07 00:49:17,637 DEBUG TRAIN Batch 20/8600 loss 8.275728 loss_att 270.746613 loss_ctc 13.453799 loss_rnnt 7.700387 lr 0.00027719 rank 1
2022-12-07 00:49:17,658 DEBUG TRAIN Batch 20/8600 loss 4.039938 loss_att 353.356506 loss_ctc 9.137453 loss_rnnt 3.473548 lr 0.00027719 rank 2
2022-12-07 00:49:17,662 DEBUG TRAIN Batch 20/8600 loss 6.164073 loss_att 305.624298 loss_ctc 12.329288 loss_rnnt 5.479049 lr 0.00027719 rank 0
2022-12-07 00:50:13,146 DEBUG TRAIN Batch 20/8700 loss 7.319883 loss_att 382.819336 loss_ctc 14.276012 loss_rnnt 6.546979 lr 0.00027715 rank 3
2022-12-07 00:50:13,161 DEBUG TRAIN Batch 20/8700 loss 5.633214 loss_att 323.624390 loss_ctc 10.637107 loss_rnnt 5.077227 lr 0.00027715 rank 2
2022-12-07 00:50:13,169 DEBUG TRAIN Batch 20/8700 loss 8.275327 loss_att 118.662102 loss_ctc 14.933813 loss_rnnt 7.535495 lr 0.00027715 rank 1
2022-12-07 00:50:13,169 DEBUG TRAIN Batch 20/8700 loss 11.212861 loss_att 281.358795 loss_ctc 22.234594 loss_rnnt 9.988225 lr 0.00027715 rank 0
2022-12-07 00:51:09,195 DEBUG TRAIN Batch 20/8800 loss 12.144854 loss_att 369.033936 loss_ctc 18.639503 loss_rnnt 11.423225 lr 0.00027711 rank 2
2022-12-07 00:51:09,202 DEBUG TRAIN Batch 20/8800 loss 10.039211 loss_att 403.007507 loss_ctc 20.865845 loss_rnnt 8.836252 lr 0.00027711 rank 3
2022-12-07 00:51:09,210 DEBUG TRAIN Batch 20/8800 loss 9.527866 loss_att 428.195007 loss_ctc 22.192799 loss_rnnt 8.120651 lr 0.00027711 rank 1
2022-12-07 00:51:09,214 DEBUG TRAIN Batch 20/8800 loss 9.717711 loss_att 235.346039 loss_ctc 17.861170 loss_rnnt 8.812883 lr 0.00027711 rank 0
2022-12-07 00:52:13,096 DEBUG TRAIN Batch 20/8900 loss 12.295044 loss_att 357.359863 loss_ctc 24.875128 loss_rnnt 10.897257 lr 0.00027707 rank 2
2022-12-07 00:52:13,097 DEBUG TRAIN Batch 20/8900 loss 6.261611 loss_att 390.967407 loss_ctc 14.465892 loss_rnnt 5.350025 lr 0.00027707 rank 0
2022-12-07 00:52:13,101 DEBUG TRAIN Batch 20/8900 loss 18.041460 loss_att 369.411560 loss_ctc 33.610718 loss_rnnt 16.311543 lr 0.00027707 rank 3
2022-12-07 00:52:13,121 DEBUG TRAIN Batch 20/8900 loss 7.257657 loss_att 347.006348 loss_ctc 18.524906 loss_rnnt 6.005741 lr 0.00027707 rank 1
2022-12-07 00:53:10,752 DEBUG TRAIN Batch 20/9000 loss 4.611042 loss_att 344.687195 loss_ctc 10.633094 loss_rnnt 3.941926 lr 0.00027702 rank 1
2022-12-07 00:53:10,758 DEBUG TRAIN Batch 20/9000 loss 10.966080 loss_att 312.911743 loss_ctc 25.146164 loss_rnnt 9.390514 lr 0.00027702 rank 3
2022-12-07 00:53:10,772 DEBUG TRAIN Batch 20/9000 loss 24.036276 loss_att 371.560272 loss_ctc 37.571381 loss_rnnt 22.532377 lr 0.00027702 rank 0
2022-12-07 00:53:10,772 DEBUG TRAIN Batch 20/9000 loss 10.199367 loss_att 464.903076 loss_ctc 21.374197 loss_rnnt 8.957719 lr 0.00027702 rank 2
2022-12-07 00:54:09,869 DEBUG TRAIN Batch 20/9100 loss 13.956127 loss_att 392.612244 loss_ctc 33.862755 loss_rnnt 11.744280 lr 0.00027698 rank 2
2022-12-07 00:54:09,884 DEBUG TRAIN Batch 20/9100 loss 7.595250 loss_att 255.133087 loss_ctc 18.777515 loss_rnnt 6.352777 lr 0.00027698 rank 3
2022-12-07 00:54:09,890 DEBUG TRAIN Batch 20/9100 loss 18.557650 loss_att 365.944336 loss_ctc 34.251770 loss_rnnt 16.813858 lr 0.00027698 rank 0
2022-12-07 00:54:09,894 DEBUG TRAIN Batch 20/9100 loss 15.766022 loss_att 372.056274 loss_ctc 25.565407 loss_rnnt 14.677202 lr 0.00027698 rank 1
2022-12-07 00:55:04,871 DEBUG TRAIN Batch 20/9200 loss 6.795590 loss_att 349.311035 loss_ctc 9.332077 loss_rnnt 6.513759 lr 0.00027694 rank 3
2022-12-07 00:55:04,879 DEBUG TRAIN Batch 20/9200 loss 9.688546 loss_att 348.929749 loss_ctc 13.389696 loss_rnnt 9.277308 lr 0.00027694 rank 0
2022-12-07 00:55:04,888 DEBUG TRAIN Batch 20/9200 loss 3.808735 loss_att 308.881561 loss_ctc 8.625089 loss_rnnt 3.273585 lr 0.00027694 rank 1
2022-12-07 00:55:04,888 DEBUG TRAIN Batch 20/9200 loss 19.093367 loss_att 328.096558 loss_ctc 36.750481 loss_rnnt 17.131466 lr 0.00027694 rank 2
2022-12-07 00:56:10,948 DEBUG TRAIN Batch 20/9300 loss 10.577430 loss_att 376.591797 loss_ctc 20.793282 loss_rnnt 9.442335 lr 0.00027690 rank 0
2022-12-07 00:56:10,955 DEBUG TRAIN Batch 20/9300 loss 9.128118 loss_att 349.358215 loss_ctc 17.403181 loss_rnnt 8.208667 lr 0.00027690 rank 2
2022-12-07 00:56:10,962 DEBUG TRAIN Batch 20/9300 loss 7.338200 loss_att 340.476349 loss_ctc 17.257992 loss_rnnt 6.236001 lr 0.00027690 rank 3
2022-12-07 00:56:10,973 DEBUG TRAIN Batch 20/9300 loss 11.593291 loss_att 191.723633 loss_ctc 18.641432 loss_rnnt 10.810164 lr 0.00027690 rank 1
2022-12-07 00:57:05,094 DEBUG TRAIN Batch 20/9400 loss 7.611292 loss_att 400.367676 loss_ctc 14.455557 loss_rnnt 6.850819 lr 0.00027685 rank 2
2022-12-07 00:57:05,095 DEBUG TRAIN Batch 20/9400 loss 6.809359 loss_att 364.022583 loss_ctc 12.076210 loss_rnnt 6.224154 lr 0.00027685 rank 3
2022-12-07 00:57:05,104 DEBUG TRAIN Batch 20/9400 loss 6.865195 loss_att 345.084747 loss_ctc 19.181280 loss_rnnt 5.496742 lr 0.00027685 rank 0
2022-12-07 00:57:05,153 DEBUG TRAIN Batch 20/9400 loss 6.156903 loss_att 377.219666 loss_ctc 14.588402 loss_rnnt 5.220070 lr 0.00027685 rank 1
2022-12-07 00:57:59,901 DEBUG TRAIN Batch 20/9500 loss 7.881101 loss_att 410.471283 loss_ctc 13.862020 loss_rnnt 7.216554 lr 0.00027681 rank 1
2022-12-07 00:57:59,909 DEBUG TRAIN Batch 20/9500 loss 11.187271 loss_att 416.887207 loss_ctc 23.251665 loss_rnnt 9.846783 lr 0.00027681 rank 3
2022-12-07 00:57:59,915 DEBUG TRAIN Batch 20/9500 loss 9.198212 loss_att 161.259781 loss_ctc 17.531301 loss_rnnt 8.272313 lr 0.00027681 rank 0
2022-12-07 00:57:59,927 DEBUG TRAIN Batch 20/9500 loss 14.768494 loss_att 283.146088 loss_ctc 23.963484 loss_rnnt 13.746828 lr 0.00027681 rank 2
2022-12-07 00:59:03,996 DEBUG TRAIN Batch 20/9600 loss 11.568552 loss_att 353.430206 loss_ctc 27.267067 loss_rnnt 9.824273 lr 0.00027677 rank 1
2022-12-07 00:59:04,004 DEBUG TRAIN Batch 20/9600 loss 15.915975 loss_att 419.337097 loss_ctc 28.151937 loss_rnnt 14.556423 lr 0.00027677 rank 0
2022-12-07 00:59:04,010 DEBUG TRAIN Batch 20/9600 loss 14.190788 loss_att 168.528748 loss_ctc 24.203688 loss_rnnt 13.078244 lr 0.00027677 rank 2
2022-12-07 00:59:04,028 DEBUG TRAIN Batch 20/9600 loss 7.383720 loss_att 313.446106 loss_ctc 8.428272 loss_rnnt 7.267659 lr 0.00027677 rank 3
2022-12-07 01:00:02,227 DEBUG TRAIN Batch 20/9700 loss 9.650228 loss_att 338.393555 loss_ctc 23.863192 loss_rnnt 8.071010 lr 0.00027673 rank 1
2022-12-07 01:00:02,249 DEBUG TRAIN Batch 20/9700 loss 6.624620 loss_att 371.859436 loss_ctc 13.299911 loss_rnnt 5.882921 lr 0.00027673 rank 2
2022-12-07 01:00:02,250 DEBUG TRAIN Batch 20/9700 loss 5.070796 loss_att 289.830627 loss_ctc 8.460007 loss_rnnt 4.694218 lr 0.00027673 rank 3
2022-12-07 01:00:02,257 DEBUG TRAIN Batch 20/9700 loss 4.290458 loss_att 338.348358 loss_ctc 9.853652 loss_rnnt 3.672325 lr 0.00027673 rank 0
2022-12-07 01:00:57,317 DEBUG TRAIN Batch 20/9800 loss 15.917521 loss_att 373.188904 loss_ctc 27.061390 loss_rnnt 14.679313 lr 0.00027668 rank 2
2022-12-07 01:00:57,318 DEBUG TRAIN Batch 20/9800 loss 9.952769 loss_att 399.136414 loss_ctc 18.705765 loss_rnnt 8.980214 lr 0.00027668 rank 1
2022-12-07 01:00:57,336 DEBUG TRAIN Batch 20/9800 loss 8.667821 loss_att 140.920242 loss_ctc 11.627765 loss_rnnt 8.338939 lr 0.00027668 rank 3
2022-12-07 01:00:57,382 DEBUG TRAIN Batch 20/9800 loss 16.670980 loss_att 368.643677 loss_ctc 33.833851 loss_rnnt 14.763995 lr 0.00027668 rank 0
2022-12-07 01:01:52,240 DEBUG TRAIN Batch 20/9900 loss 4.818312 loss_att 234.326538 loss_ctc 11.920357 loss_rnnt 4.029196 lr 0.00027664 rank 1
2022-12-07 01:01:52,241 DEBUG TRAIN Batch 20/9900 loss 4.576164 loss_att 514.055359 loss_ctc 8.858172 loss_rnnt 4.100385 lr 0.00027664 rank 3
2022-12-07 01:01:52,246 DEBUG TRAIN Batch 20/9900 loss 5.683251 loss_att 352.363770 loss_ctc 11.485652 loss_rnnt 5.038540 lr 0.00027664 rank 2
2022-12-07 01:01:52,267 DEBUG TRAIN Batch 20/9900 loss 10.421116 loss_att 335.041138 loss_ctc 22.917072 loss_rnnt 9.032677 lr 0.00027664 rank 0
2022-12-07 01:03:01,266 DEBUG TRAIN Batch 20/10000 loss 13.996498 loss_att 372.796356 loss_ctc 26.526314 loss_rnnt 12.604297 lr 0.00027660 rank 2
2022-12-07 01:03:01,270 DEBUG TRAIN Batch 20/10000 loss 5.875936 loss_att 338.519257 loss_ctc 12.190020 loss_rnnt 5.174371 lr 0.00027660 rank 3
2022-12-07 01:03:01,284 DEBUG TRAIN Batch 20/10000 loss 5.356280 loss_att 387.743256 loss_ctc 10.488444 loss_rnnt 4.786039 lr 0.00027660 rank 1
2022-12-07 01:03:01,291 DEBUG TRAIN Batch 20/10000 loss 10.449300 loss_att 349.693176 loss_ctc 19.196777 loss_rnnt 9.477358 lr 0.00027660 rank 0
2022-12-07 01:03:56,118 DEBUG TRAIN Batch 20/10100 loss 9.645186 loss_att 416.777283 loss_ctc 22.867348 loss_rnnt 8.176058 lr 0.00027656 rank 1
2022-12-07 01:03:56,121 DEBUG TRAIN Batch 20/10100 loss 7.563425 loss_att 329.239563 loss_ctc 18.990490 loss_rnnt 6.293752 lr 0.00027656 rank 3
2022-12-07 01:03:56,136 DEBUG TRAIN Batch 20/10100 loss 14.587544 loss_att 395.622833 loss_ctc 26.715061 loss_rnnt 13.240044 lr 0.00027656 rank 2
2022-12-07 01:03:56,147 DEBUG TRAIN Batch 20/10100 loss 18.280653 loss_att 282.010986 loss_ctc 26.143713 loss_rnnt 17.406981 lr 0.00027656 rank 0
2022-12-07 01:04:50,658 DEBUG TRAIN Batch 20/10200 loss 7.445145 loss_att 274.021301 loss_ctc 13.256485 loss_rnnt 6.799440 lr 0.00027651 rank 2
2022-12-07 01:04:50,660 DEBUG TRAIN Batch 20/10200 loss 11.733586 loss_att 394.272858 loss_ctc 26.087090 loss_rnnt 10.138752 lr 0.00027651 rank 3
2022-12-07 01:04:50,674 DEBUG TRAIN Batch 20/10200 loss 10.525492 loss_att 405.459412 loss_ctc 17.608036 loss_rnnt 9.738544 lr 0.00027651 rank 1
2022-12-07 01:04:50,683 DEBUG TRAIN Batch 20/10200 loss 11.080441 loss_att 397.160126 loss_ctc 24.177456 loss_rnnt 9.625217 lr 0.00027651 rank 0
2022-12-07 01:05:55,636 DEBUG TRAIN Batch 20/10300 loss 10.619374 loss_att 409.109924 loss_ctc 21.525131 loss_rnnt 9.407623 lr 0.00027647 rank 2
2022-12-07 01:05:55,643 DEBUG TRAIN Batch 20/10300 loss 10.611701 loss_att 388.007507 loss_ctc 28.901062 loss_rnnt 8.579550 lr 0.00027647 rank 1
2022-12-07 01:05:55,646 DEBUG TRAIN Batch 20/10300 loss 9.944977 loss_att 295.113617 loss_ctc 17.270330 loss_rnnt 9.131049 lr 0.00027647 rank 3
2022-12-07 01:05:55,653 DEBUG TRAIN Batch 20/10300 loss 7.341553 loss_att 429.548157 loss_ctc 15.919215 loss_rnnt 6.388479 lr 0.00027647 rank 0
2022-12-07 01:06:54,320 DEBUG TRAIN Batch 20/10400 loss 17.001167 loss_att 396.335632 loss_ctc 27.713821 loss_rnnt 15.810873 lr 0.00027643 rank 0
2022-12-07 01:06:54,327 DEBUG TRAIN Batch 20/10400 loss 10.291475 loss_att 273.624603 loss_ctc 17.528761 loss_rnnt 9.487332 lr 0.00027643 rank 3
2022-12-07 01:06:54,330 DEBUG TRAIN Batch 20/10400 loss 11.282647 loss_att 372.254333 loss_ctc 17.916784 loss_rnnt 10.545521 lr 0.00027643 rank 2
2022-12-07 01:06:54,337 DEBUG TRAIN Batch 20/10400 loss 11.209647 loss_att 345.476746 loss_ctc 18.427883 loss_rnnt 10.407620 lr 0.00027643 rank 1
2022-12-07 01:07:48,883 DEBUG TRAIN Batch 20/10500 loss 10.234468 loss_att 341.985443 loss_ctc 22.399038 loss_rnnt 8.882849 lr 0.00027639 rank 1
2022-12-07 01:07:48,895 DEBUG TRAIN Batch 20/10500 loss 9.010975 loss_att 361.542175 loss_ctc 14.439171 loss_rnnt 8.407843 lr 0.00027639 rank 0
2022-12-07 01:07:48,899 DEBUG TRAIN Batch 20/10500 loss 5.709628 loss_att 394.671997 loss_ctc 12.857268 loss_rnnt 4.915445 lr 0.00027639 rank 3
2022-12-07 01:07:48,908 DEBUG TRAIN Batch 20/10500 loss 13.098461 loss_att 381.426849 loss_ctc 26.294323 loss_rnnt 11.632254 lr 0.00027639 rank 2
2022-12-07 01:08:43,329 DEBUG TRAIN Batch 20/10600 loss 4.483543 loss_att 362.420624 loss_ctc 13.048167 loss_rnnt 3.531919 lr 0.00027634 rank 3
2022-12-07 01:08:43,336 DEBUG TRAIN Batch 20/10600 loss 11.155426 loss_att 379.523254 loss_ctc 16.145092 loss_rnnt 10.601019 lr 0.00027634 rank 2
2022-12-07 01:08:43,346 DEBUG TRAIN Batch 20/10600 loss 5.009400 loss_att 315.640625 loss_ctc 8.749544 loss_rnnt 4.593829 lr 0.00027634 rank 0
2022-12-07 01:08:43,352 DEBUG TRAIN Batch 20/10600 loss 12.482225 loss_att 136.847382 loss_ctc 18.622925 loss_rnnt 11.799926 lr 0.00027634 rank 1
2022-12-07 01:09:51,929 DEBUG TRAIN Batch 20/10700 loss 15.379509 loss_att 385.632812 loss_ctc 35.131428 loss_rnnt 13.184852 lr 0.00027630 rank 1
2022-12-07 01:09:51,938 DEBUG TRAIN Batch 20/10700 loss 24.145157 loss_att 423.843567 loss_ctc 39.558716 loss_rnnt 22.432539 lr 0.00027630 rank 2
2022-12-07 01:09:51,952 DEBUG TRAIN Batch 20/10700 loss 14.583178 loss_att 370.278015 loss_ctc 28.982037 loss_rnnt 12.983305 lr 0.00027630 rank 3
2022-12-07 01:09:51,961 DEBUG TRAIN Batch 20/10700 loss 13.191910 loss_att 307.636292 loss_ctc 28.355194 loss_rnnt 11.507100 lr 0.00027630 rank 0
2022-12-07 01:10:46,397 DEBUG TRAIN Batch 20/10800 loss 16.431866 loss_att 377.065430 loss_ctc 29.084835 loss_rnnt 15.025980 lr 0.00027626 rank 3
2022-12-07 01:10:46,404 DEBUG TRAIN Batch 20/10800 loss 5.437385 loss_att 481.841248 loss_ctc 15.175357 loss_rnnt 4.355388 lr 0.00027626 rank 0
2022-12-07 01:10:46,410 DEBUG TRAIN Batch 20/10800 loss 8.983525 loss_att 243.220901 loss_ctc 17.880733 loss_rnnt 7.994946 lr 0.00027626 rank 2
2022-12-07 01:10:46,418 DEBUG TRAIN Batch 20/10800 loss 12.770645 loss_att 413.853699 loss_ctc 29.381910 loss_rnnt 10.924949 lr 0.00027626 rank 1
2022-12-07 01:11:40,776 DEBUG TRAIN Batch 20/10900 loss 8.416672 loss_att 457.584473 loss_ctc 24.869404 loss_rnnt 6.588590 lr 0.00027622 rank 2
2022-12-07 01:11:40,776 DEBUG TRAIN Batch 20/10900 loss 9.262967 loss_att 352.006439 loss_ctc 14.682976 loss_rnnt 8.660744 lr 0.00027622 rank 0
2022-12-07 01:11:40,797 DEBUG TRAIN Batch 20/10900 loss 6.339202 loss_att 354.750427 loss_ctc 11.601052 loss_rnnt 5.754552 lr 0.00027622 rank 3
2022-12-07 01:11:40,799 DEBUG TRAIN Batch 20/10900 loss 10.880527 loss_att 429.371002 loss_ctc 19.175114 loss_rnnt 9.958906 lr 0.00027622 rank 1
2022-12-07 01:12:45,035 DEBUG TRAIN Batch 20/11000 loss 5.830957 loss_att 316.744446 loss_ctc 10.339194 loss_rnnt 5.330042 lr 0.00027618 rank 2
2022-12-07 01:12:45,039 DEBUG TRAIN Batch 20/11000 loss 5.317312 loss_att 336.999878 loss_ctc 10.512331 loss_rnnt 4.740088 lr 0.00027618 rank 0
2022-12-07 01:12:45,039 DEBUG TRAIN Batch 20/11000 loss 15.538847 loss_att 309.101898 loss_ctc 25.672510 loss_rnnt 14.412885 lr 0.00027618 rank 3
2022-12-07 01:12:45,056 DEBUG TRAIN Batch 20/11000 loss 5.860401 loss_att 393.055023 loss_ctc 9.386376 loss_rnnt 5.468626 lr 0.00027618 rank 1
2022-12-07 01:13:44,736 DEBUG TRAIN Batch 20/11100 loss 4.683353 loss_att 316.236603 loss_ctc 9.293565 loss_rnnt 4.171108 lr 0.00027613 rank 1
2022-12-07 01:13:44,739 DEBUG TRAIN Batch 20/11100 loss 4.638474 loss_att 385.178162 loss_ctc 11.240413 loss_rnnt 3.904924 lr 0.00027613 rank 3
2022-12-07 01:13:44,746 DEBUG TRAIN Batch 20/11100 loss 7.584082 loss_att 341.332520 loss_ctc 19.544180 loss_rnnt 6.255182 lr 0.00027613 rank 2
2022-12-07 01:13:44,762 DEBUG TRAIN Batch 20/11100 loss 4.668105 loss_att 391.593933 loss_ctc 11.859743 loss_rnnt 3.869035 lr 0.00027613 rank 0
2022-12-07 01:14:38,579 DEBUG TRAIN Batch 20/11200 loss 14.311602 loss_att 324.229156 loss_ctc 26.170210 loss_rnnt 12.993979 lr 0.00027609 rank 1
2022-12-07 01:14:38,579 DEBUG TRAIN Batch 20/11200 loss 12.995134 loss_att 390.125488 loss_ctc 26.625380 loss_rnnt 11.480662 lr 0.00027609 rank 3
2022-12-07 01:14:38,586 DEBUG TRAIN Batch 20/11200 loss 4.704291 loss_att 370.620056 loss_ctc 9.079354 loss_rnnt 4.218174 lr 0.00027609 rank 2
2022-12-07 01:14:38,592 DEBUG TRAIN Batch 20/11200 loss 13.240136 loss_att 385.837555 loss_ctc 19.546463 loss_rnnt 12.539433 lr 0.00027609 rank 0
2022-12-07 01:15:33,652 DEBUG TRAIN Batch 20/11300 loss 8.669484 loss_att 302.706970 loss_ctc 18.148355 loss_rnnt 7.616276 lr 0.00027605 rank 2
2022-12-07 01:15:33,658 DEBUG TRAIN Batch 20/11300 loss 10.075713 loss_att 361.879974 loss_ctc 24.841030 loss_rnnt 8.435122 lr 0.00027605 rank 3
2022-12-07 01:15:33,668 DEBUG TRAIN Batch 20/11300 loss 8.699239 loss_att 322.575378 loss_ctc 17.956144 loss_rnnt 7.670694 lr 0.00027605 rank 0
2022-12-07 01:15:33,701 DEBUG TRAIN Batch 20/11300 loss 3.479762 loss_att 403.373718 loss_ctc 9.333297 loss_rnnt 2.829369 lr 0.00027605 rank 1
2022-12-07 01:16:39,657 DEBUG TRAIN Batch 20/11400 loss 12.044828 loss_att 363.860596 loss_ctc 26.984171 loss_rnnt 10.384901 lr 0.00027601 rank 2
2022-12-07 01:16:39,661 DEBUG TRAIN Batch 20/11400 loss 7.259316 loss_att 398.115326 loss_ctc 13.979907 loss_rnnt 6.512584 lr 0.00027601 rank 3
2022-12-07 01:16:39,694 DEBUG TRAIN Batch 20/11400 loss 9.786201 loss_att 72.357285 loss_ctc 17.677505 loss_rnnt 8.909390 lr 0.00027601 rank 0
2022-12-07 01:16:39,703 DEBUG TRAIN Batch 20/11400 loss 16.714884 loss_att 389.067719 loss_ctc 26.267200 loss_rnnt 15.653515 lr 0.00027601 rank 1
2022-12-07 01:17:35,347 DEBUG TRAIN Batch 20/11500 loss 4.408540 loss_att 376.108643 loss_ctc 14.020572 loss_rnnt 3.340537 lr 0.00027597 rank 3
2022-12-07 01:17:35,349 DEBUG TRAIN Batch 20/11500 loss 2.012468 loss_att 337.264099 loss_ctc 6.478316 loss_rnnt 1.516262 lr 0.00027597 rank 1
2022-12-07 01:17:35,375 DEBUG TRAIN Batch 20/11500 loss 9.754560 loss_att 135.603699 loss_ctc 16.433062 loss_rnnt 9.012506 lr 0.00027597 rank 2
2022-12-07 01:17:35,380 DEBUG TRAIN Batch 20/11500 loss 3.808250 loss_att 410.666321 loss_ctc 13.369680 loss_rnnt 2.745869 lr 0.00027597 rank 0
2022-12-07 01:18:29,398 DEBUG TRAIN Batch 20/11600 loss 3.492062 loss_att 326.743683 loss_ctc 5.823873 loss_rnnt 3.232972 lr 0.00027592 rank 1
2022-12-07 01:18:29,413 DEBUG TRAIN Batch 20/11600 loss 8.797668 loss_att 383.443298 loss_ctc 17.216795 loss_rnnt 7.862210 lr 0.00027592 rank 2
2022-12-07 01:18:29,414 DEBUG TRAIN Batch 20/11600 loss 22.919083 loss_att 344.974731 loss_ctc 40.909435 loss_rnnt 20.920155 lr 0.00027592 rank 0
2022-12-07 01:18:29,418 DEBUG TRAIN Batch 20/11600 loss 9.104883 loss_att 304.204956 loss_ctc 15.655289 loss_rnnt 8.377060 lr 0.00027592 rank 3
2022-12-07 01:19:31,959 DEBUG TRAIN Batch 20/11700 loss 9.680849 loss_att 180.708893 loss_ctc 15.828228 loss_rnnt 8.997808 lr 0.00027588 rank 3
2022-12-07 01:19:31,966 DEBUG TRAIN Batch 20/11700 loss 21.395567 loss_att 384.007751 loss_ctc 40.545238 loss_rnnt 19.267826 lr 0.00027588 rank 2
2022-12-07 01:19:31,967 DEBUG TRAIN Batch 20/11700 loss 5.404403 loss_att 340.115234 loss_ctc 11.807209 loss_rnnt 4.692980 lr 0.00027588 rank 1
2022-12-07 01:19:31,969 DEBUG TRAIN Batch 20/11700 loss 12.058795 loss_att 389.435242 loss_ctc 23.651434 loss_rnnt 10.770723 lr 0.00027588 rank 0
2022-12-07 01:20:31,494 DEBUG TRAIN Batch 20/11800 loss 7.486194 loss_att 320.684265 loss_ctc 12.338041 loss_rnnt 6.947100 lr 0.00027584 rank 0
2022-12-07 01:20:31,501 DEBUG TRAIN Batch 20/11800 loss 12.501727 loss_att 378.544159 loss_ctc 17.696844 loss_rnnt 11.924492 lr 0.00027584 rank 3
2022-12-07 01:20:31,505 DEBUG TRAIN Batch 20/11800 loss 11.240036 loss_att 228.712006 loss_ctc 19.229790 loss_rnnt 10.352285 lr 0.00027584 rank 1
2022-12-07 01:20:31,509 DEBUG TRAIN Batch 20/11800 loss 10.248045 loss_att 390.513580 loss_ctc 20.944687 loss_rnnt 9.059529 lr 0.00027584 rank 2
2022-12-07 01:21:25,850 DEBUG TRAIN Batch 20/11900 loss 13.659863 loss_att 356.611298 loss_ctc 22.625439 loss_rnnt 12.663689 lr 0.00027580 rank 1
2022-12-07 01:21:25,859 DEBUG TRAIN Batch 20/11900 loss 5.765749 loss_att 425.492920 loss_ctc 16.779043 loss_rnnt 4.542049 lr 0.00027580 rank 3
2022-12-07 01:21:25,861 DEBUG TRAIN Batch 20/11900 loss 4.902911 loss_att 354.955353 loss_ctc 8.572059 loss_rnnt 4.495228 lr 0.00027580 rank 2
2022-12-07 01:21:25,871 DEBUG TRAIN Batch 20/11900 loss 3.301596 loss_att 257.473755 loss_ctc 6.777532 loss_rnnt 2.915381 lr 0.00027580 rank 0
2022-12-07 01:22:20,478 DEBUG TRAIN Batch 20/12000 loss 8.626883 loss_att 291.583588 loss_ctc 16.963430 loss_rnnt 7.700600 lr 0.00027576 rank 0
2022-12-07 01:22:20,484 DEBUG TRAIN Batch 20/12000 loss 7.564249 loss_att 391.676239 loss_ctc 16.682318 loss_rnnt 6.551130 lr 0.00027576 rank 1
2022-12-07 01:22:20,488 DEBUG TRAIN Batch 20/12000 loss 12.443751 loss_att 358.391113 loss_ctc 19.744999 loss_rnnt 11.632502 lr 0.00027576 rank 2
2022-12-07 01:22:20,490 DEBUG TRAIN Batch 20/12000 loss 1.561166 loss_att 366.072357 loss_ctc 6.445009 loss_rnnt 1.018517 lr 0.00027576 rank 3
2022-12-07 01:23:25,284 DEBUG TRAIN Batch 20/12100 loss 2.960074 loss_att 446.806488 loss_ctc 10.407024 loss_rnnt 2.132635 lr 0.00027571 rank 1
2022-12-07 01:23:25,285 DEBUG TRAIN Batch 20/12100 loss 7.780911 loss_att 360.974915 loss_ctc 17.553787 loss_rnnt 6.695036 lr 0.00027571 rank 0
2022-12-07 01:23:25,295 DEBUG TRAIN Batch 20/12100 loss 23.110027 loss_att 393.693176 loss_ctc 33.818378 loss_rnnt 21.920212 lr 0.00027571 rank 3
2022-12-07 01:23:25,299 DEBUG TRAIN Batch 20/12100 loss 8.458103 loss_att 147.287354 loss_ctc 13.430095 loss_rnnt 7.905660 lr 0.00027571 rank 2
2022-12-07 01:24:31,376 DEBUG TRAIN Batch 20/12200 loss 9.475099 loss_att 422.920746 loss_ctc 17.050705 loss_rnnt 8.633365 lr 0.00027567 rank 2
2022-12-07 01:24:31,386 DEBUG TRAIN Batch 20/12200 loss 13.875369 loss_att 385.796021 loss_ctc 21.913769 loss_rnnt 12.982214 lr 0.00027567 rank 3
2022-12-07 01:24:31,392 DEBUG TRAIN Batch 20/12200 loss 3.498120 loss_att 311.198242 loss_ctc 9.757509 loss_rnnt 2.802632 lr 0.00027567 rank 1
2022-12-07 01:24:31,395 DEBUG TRAIN Batch 20/12200 loss 9.269176 loss_att 378.793701 loss_ctc 15.035351 loss_rnnt 8.628490 lr 0.00027567 rank 0
2022-12-07 01:25:25,793 DEBUG TRAIN Batch 20/12300 loss 10.409462 loss_att 279.428802 loss_ctc 21.217436 loss_rnnt 9.208577 lr 0.00027563 rank 3
2022-12-07 01:25:25,798 DEBUG TRAIN Batch 20/12300 loss 6.495560 loss_att 361.830017 loss_ctc 14.342799 loss_rnnt 5.623645 lr 0.00027563 rank 2
2022-12-07 01:25:25,801 DEBUG TRAIN Batch 20/12300 loss 7.887967 loss_att 360.407074 loss_ctc 21.428894 loss_rnnt 6.383420 lr 0.00027563 rank 0
2022-12-07 01:25:25,810 DEBUG TRAIN Batch 20/12300 loss 12.224086 loss_att 381.804565 loss_ctc 22.623255 loss_rnnt 11.068623 lr 0.00027563 rank 1
2022-12-07 01:26:30,067 DEBUG TRAIN Batch 20/12400 loss 12.405488 loss_att 294.611084 loss_ctc 23.472374 loss_rnnt 11.175834 lr 0.00027559 rank 1
2022-12-07 01:26:30,068 DEBUG TRAIN Batch 20/12400 loss 15.374623 loss_att 424.235413 loss_ctc 33.464668 loss_rnnt 13.364618 lr 0.00027559 rank 2
2022-12-07 01:26:30,072 DEBUG TRAIN Batch 20/12400 loss 20.142090 loss_att 409.210388 loss_ctc 39.914833 loss_rnnt 17.945118 lr 0.00027559 rank 0
2022-12-07 01:26:30,080 DEBUG TRAIN Batch 20/12400 loss 8.609072 loss_att 116.776672 loss_ctc 12.014124 loss_rnnt 8.230733 lr 0.00027559 rank 3
2022-12-07 01:27:28,023 DEBUG TRAIN Batch 20/12500 loss 11.532930 loss_att 359.459167 loss_ctc 22.486904 loss_rnnt 10.315823 lr 0.00027555 rank 2
2022-12-07 01:27:28,028 DEBUG TRAIN Batch 20/12500 loss 5.397434 loss_att 383.494568 loss_ctc 9.702944 loss_rnnt 4.919044 lr 0.00027555 rank 3
2022-12-07 01:27:28,033 DEBUG TRAIN Batch 20/12500 loss 5.823510 loss_att 348.388458 loss_ctc 13.320660 loss_rnnt 4.990494 lr 0.00027555 rank 0
2022-12-07 01:27:28,037 DEBUG TRAIN Batch 20/12500 loss 5.182383 loss_att 366.637177 loss_ctc 10.537657 loss_rnnt 4.587352 lr 0.00027555 rank 1
2022-12-07 01:28:31,270 DEBUG TRAIN Batch 20/12600 loss 25.493900 loss_att 357.399292 loss_ctc 39.023048 loss_rnnt 23.990662 lr 0.00027550 rank 2
2022-12-07 01:28:31,273 DEBUG TRAIN Batch 20/12600 loss 10.589388 loss_att 320.961456 loss_ctc 15.289691 loss_rnnt 10.067133 lr 0.00027550 rank 1
2022-12-07 01:28:31,279 DEBUG TRAIN Batch 20/12600 loss 22.568359 loss_att 449.860046 loss_ctc 46.062073 loss_rnnt 19.957947 lr 0.00027550 rank 3
2022-12-07 01:28:31,300 DEBUG TRAIN Batch 20/12600 loss 8.587245 loss_att 314.258270 loss_ctc 13.316823 loss_rnnt 8.061736 lr 0.00027550 rank 0
2022-12-07 01:29:26,081 DEBUG TRAIN Batch 20/12700 loss 15.977590 loss_att 428.646301 loss_ctc 23.462591 loss_rnnt 15.145923 lr 0.00027546 rank 3
2022-12-07 01:29:26,085 DEBUG TRAIN Batch 20/12700 loss 15.595734 loss_att 408.276306 loss_ctc 25.405685 loss_rnnt 14.505739 lr 0.00027546 rank 1
2022-12-07 01:29:26,093 DEBUG TRAIN Batch 20/12700 loss 8.979118 loss_att 460.906311 loss_ctc 19.585222 loss_rnnt 7.800663 lr 0.00027546 rank 0
2022-12-07 01:29:26,108 DEBUG TRAIN Batch 20/12700 loss 9.117275 loss_att 99.717606 loss_ctc 14.953749 loss_rnnt 8.468779 lr 0.00027546 rank 2
2022-12-07 01:30:29,374 DEBUG TRAIN Batch 20/12800 loss 23.723516 loss_att 405.261169 loss_ctc 36.640575 loss_rnnt 22.288290 lr 0.00027542 rank 2
2022-12-07 01:30:29,375 DEBUG TRAIN Batch 20/12800 loss 10.631071 loss_att 361.065552 loss_ctc 16.526661 loss_rnnt 9.976006 lr 0.00027542 rank 3
2022-12-07 01:30:29,376 DEBUG TRAIN Batch 20/12800 loss 6.678129 loss_att 377.599457 loss_ctc 12.675601 loss_rnnt 6.011743 lr 0.00027542 rank 0
2022-12-07 01:30:29,413 DEBUG TRAIN Batch 20/12800 loss 9.668284 loss_att 393.833374 loss_ctc 17.457886 loss_rnnt 8.802773 lr 0.00027542 rank 1
2022-12-07 01:31:35,948 DEBUG TRAIN Batch 20/12900 loss 9.744429 loss_att 301.673248 loss_ctc 19.341084 loss_rnnt 8.678134 lr 0.00027538 rank 3
2022-12-07 01:31:35,961 DEBUG TRAIN Batch 20/12900 loss 7.180034 loss_att 334.908234 loss_ctc 11.909464 loss_rnnt 6.654541 lr 0.00027538 rank 1
2022-12-07 01:31:35,970 DEBUG TRAIN Batch 20/12900 loss 9.464558 loss_att 348.691711 loss_ctc 19.469929 loss_rnnt 8.352850 lr 0.00027538 rank 2
2022-12-07 01:31:35,986 DEBUG TRAIN Batch 20/12900 loss 12.896820 loss_att 403.864746 loss_ctc 24.290129 loss_rnnt 11.630897 lr 0.00027538 rank 0
2022-12-07 01:32:30,424 DEBUG TRAIN Batch 20/13000 loss 5.510617 loss_att 356.600342 loss_ctc 11.522580 loss_rnnt 4.842621 lr 0.00027534 rank 2
2022-12-07 01:32:30,434 DEBUG TRAIN Batch 20/13000 loss 12.179306 loss_att 390.867737 loss_ctc 21.217995 loss_rnnt 11.175008 lr 0.00027534 rank 0
2022-12-07 01:32:30,443 DEBUG TRAIN Batch 20/13000 loss 15.116053 loss_att 337.574005 loss_ctc 32.560188 loss_rnnt 13.177815 lr 0.00027534 rank 1
2022-12-07 01:32:30,444 DEBUG TRAIN Batch 20/13000 loss 6.548852 loss_att 158.335068 loss_ctc 10.818207 loss_rnnt 6.074480 lr 0.00027534 rank 3
2022-12-07 01:33:25,246 DEBUG TRAIN Batch 20/13100 loss 12.476240 loss_att 407.578979 loss_ctc 23.959499 loss_rnnt 11.200322 lr 0.00027530 rank 2
2022-12-07 01:33:25,254 DEBUG TRAIN Batch 20/13100 loss 10.770630 loss_att 431.855194 loss_ctc 21.434256 loss_rnnt 9.585782 lr 0.00027530 rank 3
2022-12-07 01:33:25,254 DEBUG TRAIN Batch 20/13100 loss 13.226904 loss_att 355.607727 loss_ctc 24.537722 loss_rnnt 11.970146 lr 0.00027530 rank 0
2022-12-07 01:33:25,260 DEBUG TRAIN Batch 20/13100 loss 7.375103 loss_att 150.216827 loss_ctc 15.090439 loss_rnnt 6.517844 lr 0.00027530 rank 1
2022-12-07 01:34:31,806 DEBUG TRAIN Batch 20/13200 loss 11.944733 loss_att 381.766785 loss_ctc 23.021652 loss_rnnt 10.713964 lr 0.00027525 rank 1
2022-12-07 01:34:31,817 DEBUG TRAIN Batch 20/13200 loss 13.674654 loss_att 399.213257 loss_ctc 19.699287 loss_rnnt 13.005251 lr 0.00027525 rank 2
2022-12-07 01:34:31,818 DEBUG TRAIN Batch 20/13200 loss 22.964760 loss_att 453.987793 loss_ctc 46.027153 loss_rnnt 20.402271 lr 0.00027525 rank 3
2022-12-07 01:34:31,824 DEBUG TRAIN Batch 20/13200 loss 5.706798 loss_att 288.927490 loss_ctc 14.682776 loss_rnnt 4.709466 lr 0.00027525 rank 0
2022-12-07 01:35:26,111 DEBUG TRAIN Batch 20/13300 loss 12.480971 loss_att 258.347107 loss_ctc 19.762451 loss_rnnt 11.671919 lr 0.00027521 rank 2
2022-12-07 01:35:26,121 DEBUG TRAIN Batch 20/13300 loss 10.401278 loss_att 76.039177 loss_ctc 14.786128 loss_rnnt 9.914073 lr 0.00027521 rank 0
2022-12-07 01:35:26,122 DEBUG TRAIN Batch 20/13300 loss 7.897443 loss_att 458.379944 loss_ctc 20.154743 loss_rnnt 6.535521 lr 0.00027521 rank 1
2022-12-07 01:35:26,134 DEBUG TRAIN Batch 20/13300 loss 10.963778 loss_att 457.368835 loss_ctc 20.010176 loss_rnnt 9.958622 lr 0.00027521 rank 3
2022-12-07 01:36:20,240 DEBUG TRAIN Batch 20/13400 loss 7.606476 loss_att 363.948273 loss_ctc 12.244754 loss_rnnt 7.091112 lr 0.00027517 rank 1
2022-12-07 01:36:20,249 DEBUG TRAIN Batch 20/13400 loss 14.389947 loss_att 396.107544 loss_ctc 32.165924 loss_rnnt 12.414839 lr 0.00027517 rank 3
2022-12-07 01:36:20,250 DEBUG TRAIN Batch 20/13400 loss 11.927622 loss_att 397.384216 loss_ctc 24.219740 loss_rnnt 10.561831 lr 0.00027517 rank 2
2022-12-07 01:36:20,260 DEBUG TRAIN Batch 20/13400 loss 11.660946 loss_att 397.037170 loss_ctc 22.674625 loss_rnnt 10.437203 lr 0.00027517 rank 0
2022-12-07 01:37:24,337 DEBUG TRAIN Batch 20/13500 loss 9.752871 loss_att 296.715302 loss_ctc 18.759899 loss_rnnt 8.752090 lr 0.00027513 rank 1
2022-12-07 01:37:24,339 DEBUG TRAIN Batch 20/13500 loss 15.163242 loss_att 415.542419 loss_ctc 29.894232 loss_rnnt 13.526466 lr 0.00027513 rank 3
2022-12-07 01:37:24,344 DEBUG TRAIN Batch 20/13500 loss 8.895055 loss_att 373.389099 loss_ctc 14.055775 loss_rnnt 8.321642 lr 0.00027513 rank 2
2022-12-07 01:37:24,361 DEBUG TRAIN Batch 20/13500 loss 4.910827 loss_att 308.735229 loss_ctc 11.946242 loss_rnnt 4.129114 lr 0.00027513 rank 0
2022-12-07 01:38:30,368 DEBUG TRAIN Batch 20/13600 loss 19.608795 loss_att 357.865936 loss_ctc 32.818527 loss_rnnt 18.141048 lr 0.00027509 rank 1
2022-12-07 01:38:30,375 DEBUG TRAIN Batch 20/13600 loss 7.042126 loss_att 413.466705 loss_ctc 14.758722 loss_rnnt 6.184727 lr 0.00027509 rank 0
2022-12-07 01:38:30,387 DEBUG TRAIN Batch 20/13600 loss 9.715857 loss_att 411.234955 loss_ctc 21.510504 loss_rnnt 8.405340 lr 0.00027509 rank 2
2022-12-07 01:38:30,387 DEBUG TRAIN Batch 20/13600 loss 17.546825 loss_att 285.960541 loss_ctc 27.249817 loss_rnnt 16.468716 lr 0.00027509 rank 3
2022-12-07 01:39:23,795 DEBUG TRAIN Batch 20/13700 loss 8.835630 loss_att 375.006439 loss_ctc 17.818897 loss_rnnt 7.837490 lr 0.00027505 rank 3
2022-12-07 01:39:23,811 DEBUG TRAIN Batch 20/13700 loss 4.214036 loss_att 345.225586 loss_ctc 8.065988 loss_rnnt 3.786042 lr 0.00027505 rank 2
2022-12-07 01:39:23,814 DEBUG TRAIN Batch 20/13700 loss 6.716377 loss_att 263.996002 loss_ctc 13.933354 loss_rnnt 5.914491 lr 0.00027505 rank 1
2022-12-07 01:39:23,816 DEBUG TRAIN Batch 20/13700 loss 6.593072 loss_att 288.457977 loss_ctc 12.732463 loss_rnnt 5.910918 lr 0.00027505 rank 0
2022-12-07 01:40:18,670 DEBUG TRAIN Batch 20/13800 loss 10.150994 loss_att 352.578125 loss_ctc 16.856012 loss_rnnt 9.405993 lr 0.00027500 rank 2
2022-12-07 01:40:18,672 DEBUG TRAIN Batch 20/13800 loss 6.689679 loss_att 393.018677 loss_ctc 14.400851 loss_rnnt 5.832882 lr 0.00027500 rank 3
2022-12-07 01:40:18,676 DEBUG TRAIN Batch 20/13800 loss 3.333651 loss_att 333.591309 loss_ctc 5.662461 loss_rnnt 3.074894 lr 0.00027500 rank 0
2022-12-07 01:40:18,688 DEBUG TRAIN Batch 20/13800 loss 7.097667 loss_att 346.800720 loss_ctc 17.666456 loss_rnnt 5.923357 lr 0.00027500 rank 1
2022-12-07 01:41:24,103 DEBUG TRAIN Batch 20/13900 loss 21.029655 loss_att 409.578064 loss_ctc 31.963461 loss_rnnt 19.814789 lr 0.00027496 rank 1
2022-12-07 01:41:24,122 DEBUG TRAIN Batch 20/13900 loss 6.609185 loss_att 272.848755 loss_ctc 12.571372 loss_rnnt 5.946720 lr 0.00027496 rank 2
2022-12-07 01:41:24,122 DEBUG TRAIN Batch 20/13900 loss 5.525970 loss_att 399.775665 loss_ctc 13.608614 loss_rnnt 4.627899 lr 0.00027496 rank 3
2022-12-07 01:41:24,130 DEBUG TRAIN Batch 20/13900 loss 7.688060 loss_att 193.237350 loss_ctc 12.262524 loss_rnnt 7.179787 lr 0.00027496 rank 0
2022-12-07 01:42:18,820 DEBUG TRAIN Batch 20/14000 loss 2.694584 loss_att 405.099304 loss_ctc 7.184169 loss_rnnt 2.195741 lr 0.00027492 rank 0
2022-12-07 01:42:18,826 DEBUG TRAIN Batch 20/14000 loss 9.681665 loss_att 359.039764 loss_ctc 20.329002 loss_rnnt 8.498628 lr 0.00027492 rank 3
2022-12-07 01:42:18,829 DEBUG TRAIN Batch 20/14000 loss 15.607170 loss_att 395.683929 loss_ctc 29.332394 loss_rnnt 14.082146 lr 0.00027492 rank 1
2022-12-07 01:42:18,837 DEBUG TRAIN Batch 20/14000 loss 8.465884 loss_att 186.545990 loss_ctc 16.146458 loss_rnnt 7.612487 lr 0.00027492 rank 2
2022-12-07 01:43:12,850 DEBUG TRAIN Batch 20/14100 loss 4.956890 loss_att 379.549042 loss_ctc 11.554302 loss_rnnt 4.223845 lr 0.00027488 rank 1
2022-12-07 01:43:12,867 DEBUG TRAIN Batch 20/14100 loss 7.361474 loss_att 429.635254 loss_ctc 20.561642 loss_rnnt 5.894789 lr 0.00027488 rank 2
2022-12-07 01:43:12,872 DEBUG TRAIN Batch 20/14100 loss 3.144424 loss_att 359.583862 loss_ctc 7.870972 loss_rnnt 2.619252 lr 0.00027488 rank 3
2022-12-07 01:43:12,878 DEBUG TRAIN Batch 20/14100 loss 6.179610 loss_att 401.970428 loss_ctc 11.261545 loss_rnnt 5.614951 lr 0.00027488 rank 0
2022-12-07 01:44:26,436 DEBUG TRAIN Batch 20/14200 loss 9.290665 loss_att 385.742676 loss_ctc 20.787838 loss_rnnt 8.013202 lr 0.00027484 rank 1
2022-12-07 01:44:26,447 DEBUG TRAIN Batch 20/14200 loss 9.340575 loss_att 486.725189 loss_ctc 21.621435 loss_rnnt 7.976036 lr 0.00027484 rank 0
2022-12-07 01:44:26,458 DEBUG TRAIN Batch 20/14200 loss 8.742429 loss_att 349.191833 loss_ctc 19.208128 loss_rnnt 7.579574 lr 0.00027484 rank 2
2022-12-07 01:44:26,459 DEBUG TRAIN Batch 20/14200 loss 7.232246 loss_att 322.041351 loss_ctc 12.447906 loss_rnnt 6.652729 lr 0.00027484 rank 3
2022-12-07 01:45:24,960 DEBUG TRAIN Batch 20/14300 loss 11.265219 loss_att 180.014984 loss_ctc 17.492943 loss_rnnt 10.573250 lr 0.00027480 rank 3
2022-12-07 01:45:24,961 DEBUG TRAIN Batch 20/14300 loss 6.892177 loss_att 302.251556 loss_ctc 11.707112 loss_rnnt 6.357184 lr 0.00027480 rank 1
2022-12-07 01:45:24,963 DEBUG TRAIN Batch 20/14300 loss 3.231688 loss_att 335.761719 loss_ctc 7.023116 loss_rnnt 2.810419 lr 0.00027480 rank 2
2022-12-07 01:45:24,967 DEBUG TRAIN Batch 20/14300 loss 18.212412 loss_att 445.469177 loss_ctc 38.636673 loss_rnnt 15.943050 lr 0.00027480 rank 0
2022-12-07 01:46:19,399 DEBUG TRAIN Batch 20/14400 loss 7.717165 loss_att 346.387329 loss_ctc 13.951304 loss_rnnt 7.024483 lr 0.00027475 rank 3
2022-12-07 01:46:19,420 DEBUG TRAIN Batch 20/14400 loss 9.191966 loss_att 336.428680 loss_ctc 13.828107 loss_rnnt 8.676840 lr 0.00027475 rank 2
2022-12-07 01:46:19,423 DEBUG TRAIN Batch 20/14400 loss 6.824498 loss_att 207.663376 loss_ctc 13.359439 loss_rnnt 6.098394 lr 0.00027475 rank 1
2022-12-07 01:46:19,432 DEBUG TRAIN Batch 20/14400 loss 4.740400 loss_att 282.912354 loss_ctc 11.883099 loss_rnnt 3.946767 lr 0.00027475 rank 0
2022-12-07 01:47:13,995 DEBUG TRAIN Batch 20/14500 loss 8.396362 loss_att 414.889984 loss_ctc 9.800550 loss_rnnt 8.240341 lr 0.00027471 rank 3
2022-12-07 01:47:13,996 DEBUG TRAIN Batch 20/14500 loss 4.545191 loss_att 307.729553 loss_ctc 8.642161 loss_rnnt 4.089972 lr 0.00027471 rank 2
2022-12-07 01:47:13,999 DEBUG TRAIN Batch 20/14500 loss 14.461601 loss_att 333.411865 loss_ctc 24.995474 loss_rnnt 13.291171 lr 0.00027471 rank 1
2022-12-07 01:47:14,003 DEBUG TRAIN Batch 20/14500 loss 12.649685 loss_att 328.098236 loss_ctc 25.170792 loss_rnnt 11.258451 lr 0.00027471 rank 0
2022-12-07 01:48:34,164 DEBUG TRAIN Batch 20/14600 loss 16.573376 loss_att 309.428436 loss_ctc 32.834797 loss_rnnt 14.766550 lr 0.00027467 rank 2
2022-12-07 01:48:34,168 DEBUG TRAIN Batch 20/14600 loss 10.246789 loss_att 372.660248 loss_ctc 17.126160 loss_rnnt 9.482414 lr 0.00027467 rank 1
2022-12-07 01:48:34,177 DEBUG TRAIN Batch 20/14600 loss 1.776125 loss_att 356.793945 loss_ctc 6.858934 loss_rnnt 1.211368 lr 0.00027467 rank 3
2022-12-07 01:48:34,199 DEBUG TRAIN Batch 20/14600 loss 9.276744 loss_att 71.462624 loss_ctc 12.666903 loss_rnnt 8.900060 lr 0.00027467 rank 0
2022-12-07 01:49:47,321 DEBUG TRAIN Batch 20/14700 loss 9.935982 loss_att 366.244873 loss_ctc 21.620497 loss_rnnt 8.637703 lr 0.00027463 rank 1
2022-12-07 01:49:47,327 DEBUG TRAIN Batch 20/14700 loss 11.135513 loss_att 365.898010 loss_ctc 16.197159 loss_rnnt 10.573109 lr 0.00027463 rank 3
2022-12-07 01:49:47,332 DEBUG TRAIN Batch 20/14700 loss 6.150802 loss_att 60.849842 loss_ctc 10.576010 loss_rnnt 5.659112 lr 0.00027463 rank 2
2022-12-07 01:49:47,338 DEBUG TRAIN Batch 20/14700 loss 8.347920 loss_att 395.243164 loss_ctc 16.792078 loss_rnnt 7.409681 lr 0.00027463 rank 0
2022-12-07 01:50:41,370 DEBUG TRAIN Batch 20/14800 loss 13.823049 loss_att 418.483398 loss_ctc 22.070280 loss_rnnt 12.906690 lr 0.00027459 rank 1
2022-12-07 01:50:41,373 DEBUG TRAIN Batch 20/14800 loss 8.383271 loss_att 426.289398 loss_ctc 16.003002 loss_rnnt 7.536634 lr 0.00027459 rank 2
2022-12-07 01:50:41,376 DEBUG TRAIN Batch 20/14800 loss 13.993832 loss_att 299.189178 loss_ctc 24.143055 loss_rnnt 12.866140 lr 0.00027459 rank 3
2022-12-07 01:50:41,407 DEBUG TRAIN Batch 20/14800 loss 4.476065 loss_att 393.492188 loss_ctc 14.894038 loss_rnnt 3.318513 lr 0.00027459 rank 0
2022-12-07 01:51:54,481 DEBUG TRAIN Batch 20/14900 loss 6.629756 loss_att 349.970947 loss_ctc 19.654907 loss_rnnt 5.182517 lr 0.00027455 rank 2
2022-12-07 01:51:54,496 DEBUG TRAIN Batch 20/14900 loss 10.572371 loss_att 287.652679 loss_ctc 17.671453 loss_rnnt 9.783584 lr 0.00027455 rank 3
2022-12-07 01:51:54,498 DEBUG TRAIN Batch 20/14900 loss 14.550180 loss_att 354.158325 loss_ctc 25.326679 loss_rnnt 13.352792 lr 0.00027455 rank 1
2022-12-07 01:51:54,503 DEBUG TRAIN Batch 20/14900 loss 11.673844 loss_att 375.456665 loss_ctc 21.965900 loss_rnnt 10.530282 lr 0.00027455 rank 0
2022-12-07 01:53:17,291 DEBUG TRAIN Batch 20/15000 loss 6.506523 loss_att 362.228271 loss_ctc 11.945189 loss_rnnt 5.902227 lr 0.00027451 rank 3
2022-12-07 01:53:17,294 DEBUG TRAIN Batch 20/15000 loss 9.764183 loss_att 409.919312 loss_ctc 16.958555 loss_rnnt 8.964809 lr 0.00027451 rank 2
2022-12-07 01:53:17,300 DEBUG TRAIN Batch 20/15000 loss 13.499146 loss_att 269.260620 loss_ctc 24.461380 loss_rnnt 12.281120 lr 0.00027451 rank 1
2022-12-07 01:53:17,320 DEBUG TRAIN Batch 20/15000 loss 11.363710 loss_att 385.817932 loss_ctc 22.938509 loss_rnnt 10.077621 lr 0.00027451 rank 0
2022-12-07 01:54:11,474 DEBUG TRAIN Batch 20/15100 loss 8.218208 loss_att 416.687836 loss_ctc 15.027644 loss_rnnt 7.461604 lr 0.00027446 rank 1
2022-12-07 01:54:11,486 DEBUG TRAIN Batch 20/15100 loss 15.838092 loss_att 421.349487 loss_ctc 32.413025 loss_rnnt 13.996433 lr 0.00027446 rank 0
2022-12-07 01:54:11,490 DEBUG TRAIN Batch 20/15100 loss 7.277327 loss_att 450.210571 loss_ctc 26.223297 loss_rnnt 5.172219 lr 0.00027446 rank 3
2022-12-07 01:54:11,495 DEBUG TRAIN Batch 20/15100 loss 13.000845 loss_att 384.676880 loss_ctc 26.434782 loss_rnnt 11.508185 lr 0.00027446 rank 2
2022-12-07 01:55:06,339 DEBUG TRAIN Batch 20/15200 loss 6.392828 loss_att 362.314026 loss_ctc 9.996856 loss_rnnt 5.992380 lr 0.00027442 rank 3
2022-12-07 01:55:06,351 DEBUG TRAIN Batch 20/15200 loss 7.437139 loss_att 235.373352 loss_ctc 14.469651 loss_rnnt 6.655749 lr 0.00027442 rank 0
2022-12-07 01:55:06,364 DEBUG TRAIN Batch 20/15200 loss 4.883073 loss_att 357.435028 loss_ctc 12.814625 loss_rnnt 4.001789 lr 0.00027442 rank 1
2022-12-07 01:55:06,367 DEBUG TRAIN Batch 20/15200 loss 9.039649 loss_att 376.017395 loss_ctc 17.281185 loss_rnnt 8.123922 lr 0.00027442 rank 2
2022-12-07 01:56:20,230 DEBUG TRAIN Batch 20/15300 loss 12.561775 loss_att 390.053314 loss_ctc 19.940727 loss_rnnt 11.741892 lr 0.00027438 rank 1
2022-12-07 01:56:20,241 DEBUG TRAIN Batch 20/15300 loss 10.099156 loss_att 369.148285 loss_ctc 16.517525 loss_rnnt 9.386004 lr 0.00027438 rank 0
2022-12-07 01:56:20,243 DEBUG TRAIN Batch 20/15300 loss 11.978456 loss_att 356.352539 loss_ctc 21.379822 loss_rnnt 10.933859 lr 0.00027438 rank 3
2022-12-07 01:56:20,249 DEBUG TRAIN Batch 20/15300 loss 5.212453 loss_att 218.633179 loss_ctc 9.790344 loss_rnnt 4.703798 lr 0.00027438 rank 2
2022-12-07 01:57:39,278 DEBUG TRAIN Batch 20/15400 loss 5.721513 loss_att 458.018494 loss_ctc 11.988940 loss_rnnt 5.025133 lr 0.00027434 rank 2
2022-12-07 01:57:39,296 DEBUG TRAIN Batch 20/15400 loss 6.724088 loss_att 327.392487 loss_ctc 12.097933 loss_rnnt 6.126994 lr 0.00027434 rank 3
2022-12-07 01:57:39,304 DEBUG TRAIN Batch 20/15400 loss 17.126907 loss_att 358.056519 loss_ctc 36.333961 loss_rnnt 14.992792 lr 0.00027434 rank 1
2022-12-07 01:57:39,306 DEBUG TRAIN Batch 20/15400 loss 1.911677 loss_att 318.756287 loss_ctc 4.303700 loss_rnnt 1.645896 lr 0.00027434 rank 0
2022-12-07 01:58:33,649 DEBUG TRAIN Batch 20/15500 loss 4.499965 loss_att 360.537231 loss_ctc 9.425461 loss_rnnt 3.952688 lr 0.00027430 rank 2
2022-12-07 01:58:33,650 DEBUG TRAIN Batch 20/15500 loss 4.013811 loss_att 325.375671 loss_ctc 8.053513 loss_rnnt 3.564955 lr 0.00027430 rank 1
2022-12-07 01:58:33,666 DEBUG TRAIN Batch 20/15500 loss 9.030989 loss_att 313.622070 loss_ctc 17.605106 loss_rnnt 8.078309 lr 0.00027430 rank 3
2022-12-07 01:58:33,676 DEBUG TRAIN Batch 20/15500 loss 10.753201 loss_att 320.160889 loss_ctc 20.913298 loss_rnnt 9.624303 lr 0.00027430 rank 0
2022-12-07 01:59:47,040 DEBUG TRAIN Batch 20/15600 loss 10.866330 loss_att 427.601349 loss_ctc 18.770700 loss_rnnt 9.988067 lr 0.00027426 rank 2
2022-12-07 01:59:47,054 DEBUG TRAIN Batch 20/15600 loss 6.149125 loss_att 271.942932 loss_ctc 13.646238 loss_rnnt 5.316112 lr 0.00027426 rank 3
2022-12-07 01:59:47,055 DEBUG TRAIN Batch 20/15600 loss 7.373025 loss_att 254.211380 loss_ctc 15.978488 loss_rnnt 6.416862 lr 0.00027426 rank 1
2022-12-07 01:59:47,066 DEBUG TRAIN Batch 20/15600 loss 12.410515 loss_att 387.101135 loss_ctc 21.495838 loss_rnnt 11.401034 lr 0.00027426 rank 0
2022-12-07 02:00:46,873 DEBUG TRAIN Batch 20/15700 loss 11.154673 loss_att 377.597290 loss_ctc 24.365238 loss_rnnt 9.686831 lr 0.00027422 rank 3
2022-12-07 02:00:46,881 DEBUG TRAIN Batch 20/15700 loss 6.512332 loss_att 333.947052 loss_ctc 10.242575 loss_rnnt 6.097860 lr 0.00027422 rank 0
2022-12-07 02:00:46,891 DEBUG TRAIN Batch 20/15700 loss 8.491497 loss_att 393.076019 loss_ctc 20.905479 loss_rnnt 7.112166 lr 0.00027422 rank 1
2022-12-07 02:00:46,901 DEBUG TRAIN Batch 20/15700 loss 8.022846 loss_att 380.073730 loss_ctc 18.024986 loss_rnnt 6.911497 lr 0.00027422 rank 2
2022-12-07 02:02:00,845 DEBUG TRAIN Batch 20/15800 loss 7.565606 loss_att 411.021179 loss_ctc 16.694235 loss_rnnt 6.551314 lr 0.00027418 rank 1
2022-12-07 02:02:00,867 DEBUG TRAIN Batch 20/15800 loss 14.245506 loss_att 477.199402 loss_ctc 34.029900 loss_rnnt 12.047240 lr 0.00027418 rank 3
2022-12-07 02:02:00,872 DEBUG TRAIN Batch 20/15800 loss 7.422763 loss_att 359.561218 loss_ctc 18.773094 loss_rnnt 6.161615 lr 0.00027418 rank 2
2022-12-07 02:02:00,876 DEBUG TRAIN Batch 20/15800 loss 9.959278 loss_att 293.187469 loss_ctc 20.724411 loss_rnnt 8.763152 lr 0.00027418 rank 0
2022-12-07 02:02:55,926 DEBUG TRAIN Batch 20/15900 loss 8.173660 loss_att 361.643646 loss_ctc 15.405346 loss_rnnt 7.370140 lr 0.00027413 rank 1
2022-12-07 02:02:55,929 DEBUG TRAIN Batch 20/15900 loss 7.985703 loss_att 271.033539 loss_ctc 11.345798 loss_rnnt 7.612359 lr 0.00027413 rank 2
2022-12-07 02:02:55,932 DEBUG TRAIN Batch 20/15900 loss 7.680281 loss_att 338.516235 loss_ctc 17.050615 loss_rnnt 6.639133 lr 0.00027413 rank 3
2022-12-07 02:02:55,938 DEBUG TRAIN Batch 20/15900 loss 3.841138 loss_att 375.006592 loss_ctc 10.133335 loss_rnnt 3.142005 lr 0.00027413 rank 0
2022-12-07 02:04:11,276 DEBUG TRAIN Batch 20/16000 loss 9.745606 loss_att 333.647522 loss_ctc 18.062836 loss_rnnt 8.821470 lr 0.00027409 rank 3
2022-12-07 02:04:11,281 DEBUG TRAIN Batch 20/16000 loss 8.524404 loss_att 272.384796 loss_ctc 21.096741 loss_rnnt 7.127478 lr 0.00027409 rank 1
2022-12-07 02:04:11,285 DEBUG TRAIN Batch 20/16000 loss 4.615551 loss_att 391.447021 loss_ctc 12.212913 loss_rnnt 3.771400 lr 0.00027409 rank 0
2022-12-07 02:04:11,286 DEBUG TRAIN Batch 20/16000 loss 8.738405 loss_att 412.571045 loss_ctc 19.150486 loss_rnnt 7.581508 lr 0.00027409 rank 2
2022-12-07 02:05:32,715 DEBUG TRAIN Batch 20/16100 loss 3.478305 loss_att 380.438049 loss_ctc 9.381346 loss_rnnt 2.822412 lr 0.00027405 rank 2
2022-12-07 02:05:32,736 DEBUG TRAIN Batch 20/16100 loss 11.015750 loss_att 293.901581 loss_ctc 23.080051 loss_rnnt 9.675272 lr 0.00027405 rank 3
2022-12-07 02:05:32,737 DEBUG TRAIN Batch 20/16100 loss 15.548505 loss_att 344.795166 loss_ctc 31.637810 loss_rnnt 13.760804 lr 0.00027405 rank 1
2022-12-07 02:05:32,740 DEBUG TRAIN Batch 20/16100 loss 9.288996 loss_att 367.212738 loss_ctc 14.625378 loss_rnnt 8.696064 lr 0.00027405 rank 0
2022-12-07 02:06:26,586 DEBUG TRAIN Batch 20/16200 loss 5.541521 loss_att 340.832703 loss_ctc 11.074397 loss_rnnt 4.926757 lr 0.00027401 rank 3
2022-12-07 02:06:26,599 DEBUG TRAIN Batch 20/16200 loss 5.023924 loss_att 390.162720 loss_ctc 14.915209 loss_rnnt 3.924892 lr 0.00027401 rank 2
2022-12-07 02:06:26,600 DEBUG TRAIN Batch 20/16200 loss 7.390000 loss_att 390.067566 loss_ctc 16.203793 loss_rnnt 6.410690 lr 0.00027401 rank 0
2022-12-07 02:06:26,609 DEBUG TRAIN Batch 20/16200 loss 11.625901 loss_att 275.335602 loss_ctc 17.413168 loss_rnnt 10.982872 lr 0.00027401 rank 1
2022-12-07 02:07:21,558 DEBUG TRAIN Batch 20/16300 loss 16.416773 loss_att 516.553162 loss_ctc 30.888535 loss_rnnt 14.808799 lr 0.00027397 rank 3
2022-12-07 02:07:21,558 DEBUG TRAIN Batch 20/16300 loss 8.394453 loss_att 403.515381 loss_ctc 17.722702 loss_rnnt 7.357981 lr 0.00027397 rank 2
2022-12-07 02:07:21,564 DEBUG TRAIN Batch 20/16300 loss 7.470613 loss_att 375.092987 loss_ctc 15.199962 loss_rnnt 6.611797 lr 0.00027397 rank 0
2022-12-07 02:07:21,570 DEBUG TRAIN Batch 20/16300 loss 4.039778 loss_att 149.573441 loss_ctc 7.582006 loss_rnnt 3.646198 lr 0.00027397 rank 1
2022-12-07 02:08:31,935 DEBUG TRAIN Batch 20/16400 loss 7.646082 loss_att 364.559265 loss_ctc 16.186602 loss_rnnt 6.697135 lr 0.00027393 rank 3
2022-12-07 02:08:31,946 DEBUG TRAIN Batch 20/16400 loss 14.820244 loss_att 340.786499 loss_ctc 23.428215 loss_rnnt 13.863804 lr 0.00027393 rank 2
2022-12-07 02:08:31,963 DEBUG TRAIN Batch 20/16400 loss 7.761531 loss_att 256.835754 loss_ctc 12.791180 loss_rnnt 7.202681 lr 0.00027393 rank 0
2022-12-07 02:08:31,971 DEBUG TRAIN Batch 20/16400 loss 6.833174 loss_att 406.352051 loss_ctc 12.185703 loss_rnnt 6.238449 lr 0.00027393 rank 1
2022-12-07 02:09:38,124 DEBUG TRAIN Batch 20/16500 loss 2.898460 loss_att 405.201843 loss_ctc 8.069502 loss_rnnt 2.323900 lr 0.00027389 rank 0
2022-12-07 02:09:38,128 DEBUG TRAIN Batch 20/16500 loss 13.699272 loss_att 440.717529 loss_ctc 25.717026 loss_rnnt 12.363966 lr 0.00027389 rank 3
2022-12-07 02:09:38,132 DEBUG TRAIN Batch 20/16500 loss 8.795955 loss_att 395.986328 loss_ctc 20.389717 loss_rnnt 7.507759 lr 0.00027389 rank 1
2022-12-07 02:09:38,136 DEBUG TRAIN Batch 20/16500 loss 13.732235 loss_att 325.172058 loss_ctc 31.763149 loss_rnnt 11.728800 lr 0.00027389 rank 2
2022-12-07 02:10:31,580 DEBUG TRAIN Batch 20/16600 loss 4.188019 loss_att 247.120071 loss_ctc 8.191515 loss_rnnt 3.743187 lr 0.00027385 rank 2
2022-12-07 02:10:31,582 DEBUG TRAIN Batch 20/16600 loss 8.685398 loss_att 363.179657 loss_ctc 12.274023 loss_rnnt 8.286662 lr 0.00027385 rank 1
2022-12-07 02:10:31,590 DEBUG TRAIN Batch 20/16600 loss 9.930340 loss_att 333.620941 loss_ctc 16.176653 loss_rnnt 9.236305 lr 0.00027385 rank 0
2022-12-07 02:10:31,597 DEBUG TRAIN Batch 20/16600 loss 20.142836 loss_att 349.241943 loss_ctc 34.955299 loss_rnnt 18.497007 lr 0.00027385 rank 3
2022-12-07 02:11:24,731 DEBUG TRAIN Batch 20/16700 loss 4.867653 loss_att 396.848572 loss_ctc 13.219099 loss_rnnt 3.939714 lr 0.00027381 rank 1
2022-12-07 02:11:24,733 DEBUG TRAIN Batch 20/16700 loss 13.822239 loss_att 456.243347 loss_ctc 29.106525 loss_rnnt 12.123985 lr 0.00027381 rank 0
2022-12-07 02:11:42,367 DEBUG CV Batch 20/0 loss 1.529269 loss_att 48.087822 loss_ctc 3.003984 loss_rnnt 1.365412 history loss 1.472629 rank 1
2022-12-07 02:11:42,375 DEBUG CV Batch 20/0 loss 1.529269 loss_att 48.087822 loss_ctc 3.003984 loss_rnnt 1.365412 history loss 1.472629 rank 2
2022-12-07 02:11:42,381 DEBUG CV Batch 20/0 loss 1.529269 loss_att 48.087822 loss_ctc 3.003984 loss_rnnt 1.365412 history loss 1.472629 rank 3
2022-12-07 02:11:42,385 DEBUG CV Batch 20/0 loss 1.529269 loss_att 48.087822 loss_ctc 3.003984 loss_rnnt 1.365412 history loss 1.472629 rank 0
2022-12-07 02:11:52,822 DEBUG CV Batch 20/100 loss 5.325890 loss_att 266.944458 loss_ctc 14.040756 loss_rnnt 4.357572 history loss 3.106487 rank 2
2022-12-07 02:11:52,911 DEBUG CV Batch 20/100 loss 5.325890 loss_att 266.944458 loss_ctc 14.040756 loss_rnnt 4.357572 history loss 3.106487 rank 1
2022-12-07 02:11:53,173 DEBUG CV Batch 20/100 loss 5.325890 loss_att 266.944458 loss_ctc 14.040756 loss_rnnt 4.357572 history loss 3.106487 rank 3
2022-12-07 02:11:53,239 DEBUG CV Batch 20/100 loss 5.325890 loss_att 266.944458 loss_ctc 14.040756 loss_rnnt 4.357572 history loss 3.106487 rank 0
2022-12-07 02:12:05,637 DEBUG CV Batch 20/200 loss 5.315404 loss_att 641.175964 loss_ctc 5.491900 loss_rnnt 5.295794 history loss 3.641688 rank 1
2022-12-07 02:12:05,666 DEBUG CV Batch 20/200 loss 5.315404 loss_att 641.175964 loss_ctc 5.491900 loss_rnnt 5.295794 history loss 3.641688 rank 2
2022-12-07 02:12:06,425 DEBUG CV Batch 20/200 loss 5.315404 loss_att 641.175964 loss_ctc 5.491900 loss_rnnt 5.295794 history loss 3.641688 rank 0
2022-12-07 02:12:06,684 DEBUG CV Batch 20/200 loss 5.315404 loss_att 641.175964 loss_ctc 5.491900 loss_rnnt 5.295794 history loss 3.641688 rank 3
2022-12-07 02:12:16,635 DEBUG CV Batch 20/300 loss 4.800680 loss_att 190.887695 loss_ctc 9.772395 loss_rnnt 4.248267 history loss 3.800300 rank 1
2022-12-07 02:12:17,035 DEBUG CV Batch 20/300 loss 4.800680 loss_att 190.887695 loss_ctc 9.772395 loss_rnnt 4.248267 history loss 3.800300 rank 2
2022-12-07 02:12:17,645 DEBUG CV Batch 20/300 loss 4.800680 loss_att 190.887695 loss_ctc 9.772395 loss_rnnt 4.248267 history loss 3.800300 rank 0
2022-12-07 02:12:17,856 DEBUG CV Batch 20/300 loss 4.800680 loss_att 190.887695 loss_ctc 9.772395 loss_rnnt 4.248267 history loss 3.800300 rank 3
2022-12-07 02:12:27,801 DEBUG CV Batch 20/400 loss 6.128024 loss_att 826.186401 loss_ctc 9.266468 loss_rnnt 5.779308 history loss 4.738025 rank 1
2022-12-07 02:12:27,984 DEBUG CV Batch 20/400 loss 6.128024 loss_att 826.186401 loss_ctc 9.266468 loss_rnnt 5.779308 history loss 4.738025 rank 2
2022-12-07 02:12:28,635 DEBUG CV Batch 20/400 loss 6.128024 loss_att 826.186401 loss_ctc 9.266468 loss_rnnt 5.779308 history loss 4.738025 rank 0
2022-12-07 02:12:28,987 DEBUG CV Batch 20/400 loss 6.128024 loss_att 826.186401 loss_ctc 9.266468 loss_rnnt 5.779308 history loss 4.738025 rank 3
2022-12-07 02:12:37,346 DEBUG CV Batch 20/500 loss 4.567469 loss_att 266.658295 loss_ctc 9.015083 loss_rnnt 4.073289 history loss 5.332318 rank 1
2022-12-07 02:12:37,550 DEBUG CV Batch 20/500 loss 4.567469 loss_att 266.658295 loss_ctc 9.015083 loss_rnnt 4.073289 history loss 5.332318 rank 2
2022-12-07 02:12:38,231 DEBUG CV Batch 20/500 loss 4.567469 loss_att 266.658295 loss_ctc 9.015083 loss_rnnt 4.073289 history loss 5.332318 rank 0
2022-12-07 02:12:39,059 DEBUG CV Batch 20/500 loss 4.567469 loss_att 266.658295 loss_ctc 9.015083 loss_rnnt 4.073289 history loss 5.332318 rank 3
2022-12-07 02:12:48,928 DEBUG CV Batch 20/600 loss 5.332422 loss_att 104.307388 loss_ctc 9.415164 loss_rnnt 4.878785 history loss 6.208542 rank 1
2022-12-07 02:12:48,960 DEBUG CV Batch 20/600 loss 5.332422 loss_att 104.307388 loss_ctc 9.415164 loss_rnnt 4.878785 history loss 6.208542 rank 2
2022-12-07 02:12:49,563 DEBUG CV Batch 20/600 loss 5.332422 loss_att 104.307388 loss_ctc 9.415164 loss_rnnt 4.878785 history loss 6.208542 rank 0
2022-12-07 02:12:51,317 DEBUG CV Batch 20/600 loss 5.332422 loss_att 104.307388 loss_ctc 9.415164 loss_rnnt 4.878785 history loss 6.208542 rank 3
2022-12-07 02:13:00,645 DEBUG CV Batch 20/700 loss 7.298242 loss_att 707.031860 loss_ctc 19.994333 loss_rnnt 5.887565 history loss 6.768826 rank 2
2022-12-07 02:13:00,663 DEBUG CV Batch 20/700 loss 7.298242 loss_att 707.031860 loss_ctc 19.994333 loss_rnnt 5.887565 history loss 6.768826 rank 1
2022-12-07 02:13:01,779 DEBUG CV Batch 20/700 loss 7.298242 loss_att 707.031860 loss_ctc 19.994333 loss_rnnt 5.887565 history loss 6.768826 rank 0
2022-12-07 02:13:03,738 DEBUG CV Batch 20/700 loss 7.298242 loss_att 707.031860 loss_ctc 19.994333 loss_rnnt 5.887565 history loss 6.768826 rank 3
2022-12-07 02:13:11,646 DEBUG CV Batch 20/800 loss 8.654943 loss_att 263.403076 loss_ctc 19.621531 loss_rnnt 7.436434 history loss 6.278446 rank 2
2022-12-07 02:13:11,762 DEBUG CV Batch 20/800 loss 8.654943 loss_att 263.403076 loss_ctc 19.621531 loss_rnnt 7.436434 history loss 6.278446 rank 1
2022-12-07 02:13:13,768 DEBUG CV Batch 20/800 loss 8.654943 loss_att 263.403076 loss_ctc 19.621531 loss_rnnt 7.436434 history loss 6.278446 rank 0
2022-12-07 02:13:15,710 DEBUG CV Batch 20/800 loss 8.654943 loss_att 263.403076 loss_ctc 19.621531 loss_rnnt 7.436434 history loss 6.278446 rank 3
2022-12-07 02:13:24,186 DEBUG CV Batch 20/900 loss 11.778131 loss_att 550.850464 loss_ctc 21.081257 loss_rnnt 10.744452 history loss 6.099903 rank 2
2022-12-07 02:13:24,456 DEBUG CV Batch 20/900 loss 11.778131 loss_att 550.850464 loss_ctc 21.081257 loss_rnnt 10.744452 history loss 6.099903 rank 1
2022-12-07 02:13:27,063 DEBUG CV Batch 20/900 loss 11.778131 loss_att 550.850464 loss_ctc 21.081257 loss_rnnt 10.744452 history loss 6.099903 rank 0
2022-12-07 02:13:29,059 DEBUG CV Batch 20/900 loss 11.778131 loss_att 550.850464 loss_ctc 21.081257 loss_rnnt 10.744452 history loss 6.099903 rank 3
2022-12-07 02:13:35,481 DEBUG CV Batch 20/1000 loss 2.066733 loss_att 176.486328 loss_ctc 4.266499 loss_rnnt 1.822314 history loss 5.894972 rank 2
2022-12-07 02:13:35,739 DEBUG CV Batch 20/1000 loss 2.066733 loss_att 176.486328 loss_ctc 4.266499 loss_rnnt 1.822314 history loss 5.894972 rank 1
2022-12-07 02:13:38,289 DEBUG CV Batch 20/1000 loss 2.066733 loss_att 176.486328 loss_ctc 4.266499 loss_rnnt 1.822314 history loss 5.894972 rank 0
2022-12-07 02:13:40,513 DEBUG CV Batch 20/1000 loss 2.066733 loss_att 176.486328 loss_ctc 4.266499 loss_rnnt 1.822314 history loss 5.894972 rank 3
2022-12-07 02:13:46,434 DEBUG CV Batch 20/1100 loss 4.567405 loss_att 60.694927 loss_ctc 8.156166 loss_rnnt 4.168653 history loss 5.890544 rank 2
2022-12-07 02:13:46,655 DEBUG CV Batch 20/1100 loss 4.567405 loss_att 60.694927 loss_ctc 8.156166 loss_rnnt 4.168653 history loss 5.890544 rank 1
2022-12-07 02:13:49,573 DEBUG CV Batch 20/1100 loss 4.567405 loss_att 60.694927 loss_ctc 8.156166 loss_rnnt 4.168653 history loss 5.890544 rank 0
2022-12-07 02:13:52,110 DEBUG CV Batch 20/1100 loss 4.567405 loss_att 60.694927 loss_ctc 8.156166 loss_rnnt 4.168653 history loss 5.890544 rank 3
2022-12-07 02:13:55,944 DEBUG CV Batch 20/1200 loss 8.210840 loss_att 280.458466 loss_ctc 10.970154 loss_rnnt 7.904250 history loss 6.166060 rank 2
2022-12-07 02:13:56,153 DEBUG CV Batch 20/1200 loss 8.210840 loss_att 280.458466 loss_ctc 10.970154 loss_rnnt 7.904250 history loss 6.166060 rank 1
2022-12-07 02:14:00,199 DEBUG CV Batch 20/1200 loss 8.210840 loss_att 280.458466 loss_ctc 10.970154 loss_rnnt 7.904250 history loss 6.166060 rank 0
2022-12-07 02:14:02,701 DEBUG CV Batch 20/1200 loss 8.210840 loss_att 280.458466 loss_ctc 10.970154 loss_rnnt 7.904250 history loss 6.166060 rank 3
2022-12-07 02:14:07,070 DEBUG CV Batch 20/1300 loss 5.527113 loss_att 105.977013 loss_ctc 8.934303 loss_rnnt 5.148537 history loss 6.461314 rank 2
2022-12-07 02:14:07,536 DEBUG CV Batch 20/1300 loss 5.527113 loss_att 105.977013 loss_ctc 8.934303 loss_rnnt 5.148537 history loss 6.461314 rank 1
2022-12-07 02:14:11,872 DEBUG CV Batch 20/1300 loss 5.527113 loss_att 105.977013 loss_ctc 8.934303 loss_rnnt 5.148537 history loss 6.461314 rank 0
2022-12-07 02:14:14,439 DEBUG CV Batch 20/1300 loss 5.527113 loss_att 105.977013 loss_ctc 8.934303 loss_rnnt 5.148537 history loss 6.461314 rank 3
2022-12-07 02:14:17,401 DEBUG CV Batch 20/1400 loss 2.522301 loss_att 562.085327 loss_ctc 6.633334 loss_rnnt 2.065520 history loss 6.743690 rank 2
2022-12-07 02:14:17,830 DEBUG CV Batch 20/1400 loss 2.522301 loss_att 562.085327 loss_ctc 6.633334 loss_rnnt 2.065520 history loss 6.743690 rank 1
2022-12-07 02:14:24,050 DEBUG CV Batch 20/1400 loss 2.522301 loss_att 562.085327 loss_ctc 6.633334 loss_rnnt 2.065520 history loss 6.743690 rank 0
2022-12-07 02:14:26,795 DEBUG CV Batch 20/1400 loss 2.522301 loss_att 562.085327 loss_ctc 6.633334 loss_rnnt 2.065520 history loss 6.743690 rank 3
2022-12-07 02:14:28,283 DEBUG CV Batch 20/1500 loss 8.162812 loss_att 275.487030 loss_ctc 7.867961 loss_rnnt 8.195574 history loss 6.610548 rank 2
2022-12-07 02:14:29,200 DEBUG CV Batch 20/1500 loss 8.162812 loss_att 275.487030 loss_ctc 7.867961 loss_rnnt 8.195574 history loss 6.610548 rank 1
2022-12-07 02:14:36,554 DEBUG CV Batch 20/1500 loss 8.162812 loss_att 275.487030 loss_ctc 7.867961 loss_rnnt 8.195574 history loss 6.610548 rank 0
2022-12-07 02:14:39,486 DEBUG CV Batch 20/1500 loss 8.162812 loss_att 275.487030 loss_ctc 7.867961 loss_rnnt 8.195574 history loss 6.610548 rank 3
2022-12-07 02:14:40,776 DEBUG CV Batch 20/1600 loss 7.608102 loss_att 593.908203 loss_ctc 16.839233 loss_rnnt 6.582421 history loss 6.567915 rank 2
2022-12-07 02:14:41,803 DEBUG CV Batch 20/1600 loss 7.608102 loss_att 593.908203 loss_ctc 16.839233 loss_rnnt 6.582421 history loss 6.567915 rank 1
2022-12-07 02:14:49,632 DEBUG CV Batch 20/1600 loss 7.608102 loss_att 593.908203 loss_ctc 16.839233 loss_rnnt 6.582421 history loss 6.567915 rank 0
2022-12-07 02:14:52,419 DEBUG CV Batch 20/1700 loss 7.492167 loss_att 210.844147 loss_ctc 14.277020 loss_rnnt 6.738295 history loss 6.502024 rank 2
2022-12-07 02:14:52,930 DEBUG CV Batch 20/1600 loss 7.608102 loss_att 593.908203 loss_ctc 16.839233 loss_rnnt 6.582421 history loss 6.567915 rank 3
2022-12-07 02:14:53,329 DEBUG CV Batch 20/1700 loss 7.492167 loss_att 210.844147 loss_ctc 14.277020 loss_rnnt 6.738295 history loss 6.502024 rank 1
2022-12-07 02:15:00,948 DEBUG CV Batch 20/1700 loss 7.492167 loss_att 210.844147 loss_ctc 14.277020 loss_rnnt 6.738295 history loss 6.502024 rank 0
2022-12-07 02:15:01,171 INFO Epoch 20 CV info cv_loss 6.487589978548484
2022-12-07 02:15:01,172 INFO Epoch 21 TRAIN info lr 0.0002738182926547801
2022-12-07 02:15:01,175 INFO using accumulate grad, new batch size is 1 times larger than before
2022-12-07 02:15:02,172 INFO Epoch 20 CV info cv_loss 6.487589978548484
2022-12-07 02:15:02,173 INFO Epoch 21 TRAIN info lr 0.0002737961230279112
2022-12-07 02:15:02,174 INFO using accumulate grad, new batch size is 1 times larger than before
2022-12-07 02:15:04,332 DEBUG CV Batch 20/1700 loss 7.492167 loss_att 210.844147 loss_ctc 14.277020 loss_rnnt 6.738295 history loss 6.502024 rank 3
2022-12-07 02:15:09,609 INFO Epoch 20 CV info cv_loss 6.487589978548484
2022-12-07 02:15:09,609 INFO Checkpoint: save to checkpoint exp/1204_encoder_bias_nobi_noatt/20.pt
2022-12-07 02:15:10,593 INFO Epoch 21 TRAIN info lr 0.0002738051544480729
2022-12-07 02:15:10,597 INFO using accumulate grad, new batch size is 1 times larger than before
2022-12-07 02:15:13,327 INFO Epoch 20 CV info cv_loss 6.487589978548484
2022-12-07 02:15:13,328 INFO Epoch 21 TRAIN info lr 0.000273842110475716
2022-12-07 02:15:13,330 INFO using accumulate grad, new batch size is 1 times larger than before
2022-12-07 02:16:14,689 DEBUG TRAIN Batch 21/0 loss 7.375243 loss_att 67.412781 loss_ctc 11.737556 loss_rnnt 6.890542 lr 0.00027380 rank 1
2022-12-07 02:16:14,695 DEBUG TRAIN Batch 21/0 loss 7.329838 loss_att 70.826042 loss_ctc 12.292371 loss_rnnt 6.778445 lr 0.00027384 rank 3
2022-12-07 02:16:14,696 DEBUG TRAIN Batch 21/0 loss 11.825825 loss_att 82.980057 loss_ctc 16.437712 loss_rnnt 11.313394 lr 0.00027382 rank 2
2022-12-07 02:16:14,718 DEBUG TRAIN Batch 21/0 loss 7.147123 loss_att 74.802155 loss_ctc 10.656761 loss_rnnt 6.757163 lr 0.00027380 rank 0
2022-12-07 02:17:08,902 DEBUG TRAIN Batch 21/100 loss 4.940973 loss_att 428.104492 loss_ctc 10.694942 loss_rnnt 4.301643 lr 0.00027375 rank 1
2022-12-07 02:17:08,920 DEBUG TRAIN Batch 21/100 loss 2.492063 loss_att 396.529388 loss_ctc 5.562843 loss_rnnt 2.150865 lr 0.00027380 rank 3
2022-12-07 02:17:08,926 DEBUG TRAIN Batch 21/100 loss 15.027023 loss_att 445.229095 loss_ctc 33.859146 loss_rnnt 12.934566 lr 0.00027376 rank 0
2022-12-07 02:17:08,927 DEBUG TRAIN Batch 21/100 loss 3.419429 loss_att 399.425140 loss_ctc 9.815145 loss_rnnt 2.708794 lr 0.00027378 rank 2
2022-12-07 02:18:02,586 DEBUG TRAIN Batch 21/200 loss 3.216561 loss_att 361.994568 loss_ctc 5.498152 loss_rnnt 2.963051 lr 0.00027372 rank 0
2022-12-07 02:18:02,588 DEBUG TRAIN Batch 21/200 loss 4.346197 loss_att 392.934753 loss_ctc 13.728334 loss_rnnt 3.303738 lr 0.00027374 rank 2
2022-12-07 02:18:02,591 DEBUG TRAIN Batch 21/200 loss 9.188143 loss_att 333.494415 loss_ctc 17.029722 loss_rnnt 8.316856 lr 0.00027376 rank 3
2022-12-07 02:18:02,592 DEBUG TRAIN Batch 21/200 loss 4.322117 loss_att 384.523987 loss_ctc 8.379530 loss_rnnt 3.871293 lr 0.00027371 rank 1
2022-12-07 02:18:56,875 DEBUG TRAIN Batch 21/300 loss 16.720980 loss_att 409.819275 loss_ctc 32.465820 loss_rnnt 14.971554 lr 0.00027367 rank 1
2022-12-07 02:18:56,876 DEBUG TRAIN Batch 21/300 loss 10.536164 loss_att 353.759430 loss_ctc 21.647358 loss_rnnt 9.301587 lr 0.00027372 rank 3
2022-12-07 02:18:56,896 DEBUG TRAIN Batch 21/300 loss 7.895546 loss_att 342.981018 loss_ctc 17.534056 loss_rnnt 6.824601 lr 0.00027368 rank 0
2022-12-07 02:18:56,900 DEBUG TRAIN Batch 21/300 loss 6.990683 loss_att 362.014954 loss_ctc 12.765110 loss_rnnt 6.349080 lr 0.00027369 rank 2
2022-12-07 02:20:03,524 DEBUG TRAIN Batch 21/400 loss 9.450050 loss_att 397.096802 loss_ctc 20.845016 loss_rnnt 8.183943 lr 0.00027363 rank 1
2022-12-07 02:20:03,530 DEBUG TRAIN Batch 21/400 loss 8.060648 loss_att 429.978699 loss_ctc 17.607182 loss_rnnt 6.999923 lr 0.00027368 rank 3
2022-12-07 02:20:03,545 DEBUG TRAIN Batch 21/400 loss 7.085136 loss_att 285.941772 loss_ctc 18.990688 loss_rnnt 5.762298 lr 0.00027365 rank 2
2022-12-07 02:20:03,551 DEBUG TRAIN Batch 21/400 loss 10.861553 loss_att 330.766602 loss_ctc 20.485682 loss_rnnt 9.792207 lr 0.00027364 rank 0
2022-12-07 02:20:57,648 DEBUG TRAIN Batch 21/500 loss 8.383422 loss_att 323.433105 loss_ctc 14.180658 loss_rnnt 7.739285 lr 0.00027359 rank 1
2022-12-07 02:20:57,650 DEBUG TRAIN Batch 21/500 loss 8.565922 loss_att 343.774994 loss_ctc 16.400562 loss_rnnt 7.695406 lr 0.00027360 rank 0
2022-12-07 02:20:57,660 DEBUG TRAIN Batch 21/500 loss 4.711203 loss_att 312.601410 loss_ctc 13.353162 loss_rnnt 3.750985 lr 0.00027364 rank 3
2022-12-07 02:20:57,662 DEBUG TRAIN Batch 21/500 loss 9.722441 loss_att 297.584229 loss_ctc 20.816181 loss_rnnt 8.489803 lr 0.00027361 rank 2
2022-12-07 02:21:51,819 DEBUG TRAIN Batch 21/600 loss 6.446293 loss_att 100.068916 loss_ctc 11.571264 loss_rnnt 5.876852 lr 0.00027360 rank 3
2022-12-07 02:21:51,820 DEBUG TRAIN Batch 21/600 loss 17.162584 loss_att 205.349274 loss_ctc 26.882143 loss_rnnt 16.082634 lr 0.00027357 rank 2
2022-12-07 02:21:51,824 DEBUG TRAIN Batch 21/600 loss 6.847905 loss_att 202.917389 loss_ctc 9.731318 loss_rnnt 6.527526 lr 0.00027355 rank 1
2022-12-07 02:21:51,829 DEBUG TRAIN Batch 21/600 loss 9.365828 loss_att 163.795029 loss_ctc 16.334351 loss_rnnt 8.591548 lr 0.00027356 rank 0
2022-12-07 02:22:46,131 DEBUG TRAIN Batch 21/700 loss 8.249682 loss_att 396.648926 loss_ctc 14.509209 loss_rnnt 7.554180 lr 0.00027355 rank 3
2022-12-07 02:22:46,147 DEBUG TRAIN Batch 21/700 loss 9.471436 loss_att 381.309906 loss_ctc 15.977592 loss_rnnt 8.748529 lr 0.00027352 rank 0
2022-12-07 02:22:46,152 DEBUG TRAIN Batch 21/700 loss 8.290606 loss_att 400.234711 loss_ctc 17.350437 loss_rnnt 7.283958 lr 0.00027351 rank 1
2022-12-07 02:22:46,156 DEBUG TRAIN Batch 21/700 loss 11.684093 loss_att 455.978699 loss_ctc 26.324919 loss_rnnt 10.057335 lr 0.00027353 rank 2
2022-12-07 02:23:51,663 DEBUG TRAIN Batch 21/800 loss 8.831853 loss_att 380.272339 loss_ctc 18.642048 loss_rnnt 7.741832 lr 0.00027347 rank 1
2022-12-07 02:23:51,664 DEBUG TRAIN Batch 21/800 loss 10.932014 loss_att 436.070251 loss_ctc 28.247967 loss_rnnt 9.008018 lr 0.00027351 rank 3
2022-12-07 02:23:51,679 DEBUG TRAIN Batch 21/800 loss 12.596631 loss_att 459.797852 loss_ctc 24.559811 loss_rnnt 11.267389 lr 0.00027349 rank 2
2022-12-07 02:23:51,688 DEBUG TRAIN Batch 21/800 loss 9.896954 loss_att 346.709320 loss_ctc 17.692915 loss_rnnt 9.030736 lr 0.00027348 rank 0
2022-12-07 02:24:45,721 DEBUG TRAIN Batch 21/900 loss 16.161036 loss_att 349.782410 loss_ctc 36.823700 loss_rnnt 13.865185 lr 0.00027343 rank 1
2022-12-07 02:24:45,729 DEBUG TRAIN Batch 21/900 loss 9.303861 loss_att 416.473694 loss_ctc 20.245605 loss_rnnt 8.088111 lr 0.00027347 rank 3
2022-12-07 02:24:45,742 DEBUG TRAIN Batch 21/900 loss 18.447247 loss_att 383.653137 loss_ctc 38.297340 loss_rnnt 16.241680 lr 0.00027345 rank 2
2022-12-07 02:24:45,754 DEBUG TRAIN Batch 21/900 loss 5.110585 loss_att 374.508545 loss_ctc 12.382277 loss_rnnt 4.302619 lr 0.00027344 rank 0
2022-12-07 02:25:39,758 DEBUG TRAIN Batch 21/1000 loss 11.187624 loss_att 372.101257 loss_ctc 27.584599 loss_rnnt 9.365738 lr 0.00027343 rank 3
2022-12-07 02:25:39,759 DEBUG TRAIN Batch 21/1000 loss 13.682161 loss_att 360.369873 loss_ctc 24.641136 loss_rnnt 12.464499 lr 0.00027341 rank 2
2022-12-07 02:25:39,776 DEBUG TRAIN Batch 21/1000 loss 10.482986 loss_att 366.963196 loss_ctc 19.065020 loss_rnnt 9.529428 lr 0.00027339 rank 1
2022-12-07 02:25:39,779 DEBUG TRAIN Batch 21/1000 loss 10.373992 loss_att 318.790833 loss_ctc 16.110897 loss_rnnt 9.736558 lr 0.00027340 rank 0
2022-12-07 02:26:44,885 DEBUG TRAIN Batch 21/1100 loss 10.872535 loss_att 369.459198 loss_ctc 22.166746 loss_rnnt 9.617622 lr 0.00027335 rank 1
2022-12-07 02:26:44,893 DEBUG TRAIN Batch 21/1100 loss 8.497784 loss_att 291.454346 loss_ctc 16.020655 loss_rnnt 7.661909 lr 0.00027335 rank 0
2022-12-07 02:26:44,904 DEBUG TRAIN Batch 21/1100 loss 12.917972 loss_att 329.863281 loss_ctc 26.495180 loss_rnnt 11.409393 lr 0.00027337 rank 2
2022-12-07 02:26:44,905 DEBUG TRAIN Batch 21/1100 loss 6.150907 loss_att 338.664734 loss_ctc 12.854089 loss_rnnt 5.406109 lr 0.00027339 rank 3
2022-12-07 02:27:39,477 DEBUG TRAIN Batch 21/1200 loss 16.401556 loss_att 296.616852 loss_ctc 28.454477 loss_rnnt 15.062342 lr 0.00027330 rank 1
2022-12-07 02:27:39,487 DEBUG TRAIN Batch 21/1200 loss 11.904934 loss_att 253.430511 loss_ctc 17.405186 loss_rnnt 11.293796 lr 0.00027331 rank 0
2022-12-07 02:27:39,493 DEBUG TRAIN Batch 21/1200 loss 13.871380 loss_att 278.613373 loss_ctc 22.140209 loss_rnnt 12.952621 lr 0.00027333 rank 2
2022-12-07 02:27:39,497 DEBUG TRAIN Batch 21/1200 loss 8.940610 loss_att 206.495132 loss_ctc 15.787524 loss_rnnt 8.179842 lr 0.00027335 rank 3
2022-12-07 02:28:33,690 DEBUG TRAIN Batch 21/1300 loss 12.072699 loss_att 161.731125 loss_ctc 23.037718 loss_rnnt 10.854363 lr 0.00027326 rank 1
2022-12-07 02:28:33,698 DEBUG TRAIN Batch 21/1300 loss 6.435147 loss_att 376.457367 loss_ctc 15.992643 loss_rnnt 5.373203 lr 0.00027331 rank 3
2022-12-07 02:28:33,706 DEBUG TRAIN Batch 21/1300 loss 4.924167 loss_att 342.816132 loss_ctc 8.940285 loss_rnnt 4.477931 lr 0.00027329 rank 2
2022-12-07 02:28:33,709 DEBUG TRAIN Batch 21/1300 loss 2.507504 loss_att 405.104828 loss_ctc 6.289705 loss_rnnt 2.087259 lr 0.00027327 rank 0
2022-12-07 02:29:28,935 DEBUG TRAIN Batch 21/1400 loss 22.521749 loss_att 430.775146 loss_ctc 44.927391 loss_rnnt 20.032234 lr 0.00027323 rank 0
2022-12-07 02:29:28,941 DEBUG TRAIN Batch 21/1400 loss 5.133162 loss_att 390.524628 loss_ctc 10.990810 loss_rnnt 4.482312 lr 0.00027327 rank 3
2022-12-07 02:29:28,947 DEBUG TRAIN Batch 21/1400 loss 11.292658 loss_att 411.628052 loss_ctc 19.685612 loss_rnnt 10.360107 lr 0.00027324 rank 2
2022-12-07 02:29:28,983 DEBUG TRAIN Batch 21/1400 loss 2.766599 loss_att 364.888763 loss_ctc 7.492209 loss_rnnt 2.241531 lr 0.00027322 rank 1
2022-12-07 02:30:33,485 DEBUG TRAIN Batch 21/1500 loss 5.291892 loss_att 377.400330 loss_ctc 9.511252 loss_rnnt 4.823074 lr 0.00027318 rank 1
2022-12-07 02:30:33,497 DEBUG TRAIN Batch 21/1500 loss 7.058989 loss_att 370.697388 loss_ctc 15.740272 loss_rnnt 6.094402 lr 0.00027323 rank 3
2022-12-07 02:30:33,508 DEBUG TRAIN Batch 21/1500 loss 13.908070 loss_att 434.336121 loss_ctc 28.353260 loss_rnnt 12.303049 lr 0.00027319 rank 0
2022-12-07 02:30:33,509 DEBUG TRAIN Batch 21/1500 loss 31.670834 loss_att 419.656006 loss_ctc 67.378876 loss_rnnt 27.703274 lr 0.00027320 rank 2
2022-12-07 02:31:27,284 DEBUG TRAIN Batch 21/1600 loss 10.608522 loss_att 416.424652 loss_ctc 25.851612 loss_rnnt 8.914846 lr 0.00027314 rank 1
2022-12-07 02:31:27,293 DEBUG TRAIN Batch 21/1600 loss 8.311196 loss_att 324.293640 loss_ctc 12.720049 loss_rnnt 7.821324 lr 0.00027319 rank 3
2022-12-07 02:31:27,301 DEBUG TRAIN Batch 21/1600 loss 5.699246 loss_att 330.835480 loss_ctc 12.538565 loss_rnnt 4.939322 lr 0.00027316 rank 2
2022-12-07 02:31:27,302 DEBUG TRAIN Batch 21/1600 loss 13.748772 loss_att 361.878113 loss_ctc 23.469975 loss_rnnt 12.668638 lr 0.00027315 rank 0
2022-12-07 02:32:21,580 DEBUG TRAIN Batch 21/1700 loss 12.425979 loss_att 387.611389 loss_ctc 25.009914 loss_rnnt 11.027763 lr 0.00027310 rank 1
2022-12-07 02:32:21,595 DEBUG TRAIN Batch 21/1700 loss 9.072842 loss_att 311.640991 loss_ctc 14.328888 loss_rnnt 8.488836 lr 0.00027315 rank 3
2022-12-07 02:32:21,603 DEBUG TRAIN Batch 21/1700 loss 9.472574 loss_att 388.371338 loss_ctc 18.452131 loss_rnnt 8.474846 lr 0.00027312 rank 2
2022-12-07 02:32:21,607 DEBUG TRAIN Batch 21/1700 loss 4.238686 loss_att 360.787933 loss_ctc 10.404962 loss_rnnt 3.553544 lr 0.00027311 rank 0
2022-12-07 02:33:28,874 DEBUG TRAIN Batch 21/1800 loss 9.872740 loss_att 223.817856 loss_ctc 18.515589 loss_rnnt 8.912424 lr 0.00027311 rank 3
2022-12-07 02:33:28,889 DEBUG TRAIN Batch 21/1800 loss 12.256734 loss_att 362.539429 loss_ctc 23.156574 loss_rnnt 11.045641 lr 0.00027306 rank 1
2022-12-07 02:33:28,894 DEBUG TRAIN Batch 21/1800 loss 12.730570 loss_att 316.631653 loss_ctc 26.865099 loss_rnnt 11.160067 lr 0.00027308 rank 2
2022-12-07 02:33:28,901 DEBUG TRAIN Batch 21/1800 loss 5.251264 loss_att 282.734100 loss_ctc 8.974120 loss_rnnt 4.837614 lr 0.00027307 rank 0
2022-12-07 02:34:23,920 DEBUG TRAIN Batch 21/1900 loss 10.103123 loss_att 222.097763 loss_ctc 15.868621 loss_rnnt 9.462512 lr 0.00027304 rank 2
2022-12-07 02:34:23,922 DEBUG TRAIN Batch 21/1900 loss 9.001900 loss_att 379.794067 loss_ctc 18.597797 loss_rnnt 7.935689 lr 0.00027306 rank 3
2022-12-07 02:34:23,937 DEBUG TRAIN Batch 21/1900 loss 10.651155 loss_att 236.774826 loss_ctc 18.037079 loss_rnnt 9.830498 lr 0.00027302 rank 1
2022-12-07 02:34:23,946 DEBUG TRAIN Batch 21/1900 loss 6.226510 loss_att 155.848114 loss_ctc 12.984960 loss_rnnt 5.475571 lr 0.00027303 rank 0
2022-12-07 02:35:18,438 DEBUG TRAIN Batch 21/2000 loss 6.894707 loss_att 438.789246 loss_ctc 13.658136 loss_rnnt 6.143215 lr 0.00027298 rank 1
2022-12-07 02:35:18,456 DEBUG TRAIN Batch 21/2000 loss 10.372870 loss_att 301.853363 loss_ctc 16.136658 loss_rnnt 9.732450 lr 0.00027300 rank 2
2022-12-07 02:35:18,457 DEBUG TRAIN Batch 21/2000 loss 15.914226 loss_att 366.226685 loss_ctc 28.035831 loss_rnnt 14.567381 lr 0.00027302 rank 3
2022-12-07 02:35:18,468 DEBUG TRAIN Batch 21/2000 loss 15.115634 loss_att 427.649719 loss_ctc 31.157280 loss_rnnt 13.333229 lr 0.00027299 rank 0
2022-12-07 02:36:12,786 DEBUG TRAIN Batch 21/2100 loss 6.364597 loss_att 427.767426 loss_ctc 11.345961 loss_rnnt 5.811113 lr 0.00027296 rank 2
2022-12-07 02:36:12,789 DEBUG TRAIN Batch 21/2100 loss 3.356296 loss_att 402.833832 loss_ctc 6.152996 loss_rnnt 3.045551 lr 0.00027298 rank 3
2022-12-07 02:36:12,811 DEBUG TRAIN Batch 21/2100 loss 5.665062 loss_att 370.420593 loss_ctc 18.082542 loss_rnnt 4.285342 lr 0.00027295 rank 0
2022-12-07 02:36:12,842 DEBUG TRAIN Batch 21/2100 loss 2.848748 loss_att 330.122284 loss_ctc 7.717983 loss_rnnt 2.307722 lr 0.00027294 rank 1
2022-12-07 02:37:18,676 DEBUG TRAIN Batch 21/2200 loss 9.949281 loss_att 420.667755 loss_ctc 24.486675 loss_rnnt 8.334015 lr 0.00027292 rank 2
2022-12-07 02:37:18,685 DEBUG TRAIN Batch 21/2200 loss 6.921278 loss_att 432.675964 loss_ctc 15.013087 loss_rnnt 6.022188 lr 0.00027294 rank 3
2022-12-07 02:37:18,687 DEBUG TRAIN Batch 21/2200 loss 1.254744 loss_att 350.541260 loss_ctc 4.262160 loss_rnnt 0.920586 lr 0.00027291 rank 0
2022-12-07 02:37:18,694 DEBUG TRAIN Batch 21/2200 loss 5.560467 loss_att 261.516418 loss_ctc 14.103011 loss_rnnt 4.611296 lr 0.00027290 rank 1
2022-12-07 02:38:12,199 DEBUG TRAIN Batch 21/2300 loss 14.021615 loss_att 356.493591 loss_ctc 22.686111 loss_rnnt 13.058894 lr 0.00027290 rank 3
2022-12-07 02:38:12,214 DEBUG TRAIN Batch 21/2300 loss 8.420992 loss_att 372.380005 loss_ctc 18.328863 loss_rnnt 7.320117 lr 0.00027287 rank 0
2022-12-07 02:38:12,221 DEBUG TRAIN Batch 21/2300 loss 7.062571 loss_att 379.097961 loss_ctc 19.843792 loss_rnnt 5.642435 lr 0.00027286 rank 1
2022-12-07 02:38:12,224 DEBUG TRAIN Batch 21/2300 loss 7.998262 loss_att 303.055084 loss_ctc 14.978552 loss_rnnt 7.222674 lr 0.00027288 rank 2
2022-12-07 02:39:06,792 DEBUG TRAIN Batch 21/2400 loss 11.401125 loss_att 273.641693 loss_ctc 21.276400 loss_rnnt 10.303873 lr 0.00027286 rank 3
2022-12-07 02:39:06,801 DEBUG TRAIN Batch 21/2400 loss 10.457765 loss_att 342.951019 loss_ctc 17.060867 loss_rnnt 9.724087 lr 0.00027282 rank 0
2022-12-07 02:39:06,805 DEBUG TRAIN Batch 21/2400 loss 5.762064 loss_att 329.023834 loss_ctc 8.114732 loss_rnnt 5.500657 lr 0.00027284 rank 2
2022-12-07 02:39:06,811 DEBUG TRAIN Batch 21/2400 loss 9.022664 loss_att 341.797424 loss_ctc 12.897469 loss_rnnt 8.592131 lr 0.00027282 rank 1
2022-12-07 02:40:13,196 DEBUG TRAIN Batch 21/2500 loss 10.392615 loss_att 217.301117 loss_ctc 18.967909 loss_rnnt 9.439806 lr 0.00027278 rank 0
2022-12-07 02:40:13,205 DEBUG TRAIN Batch 21/2500 loss 8.597780 loss_att 131.432526 loss_ctc 15.773954 loss_rnnt 7.800428 lr 0.00027280 rank 2
2022-12-07 02:40:13,206 DEBUG TRAIN Batch 21/2500 loss 4.733409 loss_att 102.147003 loss_ctc 6.918035 loss_rnnt 4.490674 lr 0.00027282 rank 3
2022-12-07 02:40:13,207 DEBUG TRAIN Batch 21/2500 loss 7.843644 loss_att 301.773041 loss_ctc 17.254364 loss_rnnt 6.798009 lr 0.00027278 rank 1
2022-12-07 02:41:06,548 DEBUG TRAIN Batch 21/2600 loss 8.204002 loss_att 167.723602 loss_ctc 12.635654 loss_rnnt 7.711597 lr 0.00027273 rank 1
2022-12-07 02:41:06,561 DEBUG TRAIN Batch 21/2600 loss 2.387743 loss_att 379.741882 loss_ctc 7.874595 loss_rnnt 1.778093 lr 0.00027278 rank 3
2022-12-07 02:41:06,563 DEBUG TRAIN Batch 21/2600 loss 5.140761 loss_att 394.291931 loss_ctc 13.762413 loss_rnnt 4.182800 lr 0.00027276 rank 2
2022-12-07 02:41:06,571 DEBUG TRAIN Batch 21/2600 loss 6.617424 loss_att 395.967285 loss_ctc 11.713389 loss_rnnt 6.051206 lr 0.00027274 rank 0
2022-12-07 02:42:00,019 DEBUG TRAIN Batch 21/2700 loss 3.563166 loss_att 398.124512 loss_ctc 7.439888 loss_rnnt 3.132419 lr 0.00027272 rank 2
2022-12-07 02:42:00,025 DEBUG TRAIN Batch 21/2700 loss 10.514383 loss_att 351.445984 loss_ctc 20.437130 loss_rnnt 9.411857 lr 0.00027269 rank 1
2022-12-07 02:42:00,028 DEBUG TRAIN Batch 21/2700 loss 8.477943 loss_att 366.287476 loss_ctc 13.884352 loss_rnnt 7.877232 lr 0.00027274 rank 3
2022-12-07 02:42:00,046 DEBUG TRAIN Batch 21/2700 loss 11.779381 loss_att 456.929016 loss_ctc 33.168777 loss_rnnt 9.402781 lr 0.00027270 rank 0
2022-12-07 02:42:54,612 DEBUG TRAIN Batch 21/2800 loss 10.946041 loss_att 376.462494 loss_ctc 16.271011 loss_rnnt 10.354378 lr 0.00027266 rank 0
2022-12-07 02:42:54,617 DEBUG TRAIN Batch 21/2800 loss 16.430031 loss_att 390.114777 loss_ctc 27.244970 loss_rnnt 15.228371 lr 0.00027270 rank 3
2022-12-07 02:42:54,623 DEBUG TRAIN Batch 21/2800 loss 6.267891 loss_att 390.896729 loss_ctc 14.067696 loss_rnnt 5.401246 lr 0.00027265 rank 1
2022-12-07 02:42:54,624 DEBUG TRAIN Batch 21/2800 loss 5.830887 loss_att 415.246033 loss_ctc 16.162392 loss_rnnt 4.682942 lr 0.00027268 rank 2
2022-12-07 02:43:59,958 DEBUG TRAIN Batch 21/2900 loss 3.381921 loss_att 345.031219 loss_ctc 6.823726 loss_rnnt 2.999498 lr 0.00027261 rank 1
2022-12-07 02:43:59,960 DEBUG TRAIN Batch 21/2900 loss 7.449817 loss_att 399.717407 loss_ctc 16.195705 loss_rnnt 6.478052 lr 0.00027266 rank 3
2022-12-07 02:43:59,973 DEBUG TRAIN Batch 21/2900 loss 9.513937 loss_att 404.099579 loss_ctc 17.539925 loss_rnnt 8.622161 lr 0.00027263 rank 2
2022-12-07 02:43:59,979 DEBUG TRAIN Batch 21/2900 loss 6.594125 loss_att 330.671021 loss_ctc 13.450735 loss_rnnt 5.832279 lr 0.00027262 rank 0
2022-12-07 02:44:53,679 DEBUG TRAIN Batch 21/3000 loss 4.416266 loss_att 318.427277 loss_ctc 8.008407 loss_rnnt 4.017140 lr 0.00027262 rank 3
2022-12-07 02:44:53,683 DEBUG TRAIN Batch 21/3000 loss 5.972311 loss_att 375.715515 loss_ctc 13.519503 loss_rnnt 5.133734 lr 0.00027257 rank 1
2022-12-07 02:44:53,686 DEBUG TRAIN Batch 21/3000 loss 7.840595 loss_att 331.837433 loss_ctc 18.534512 loss_rnnt 6.652382 lr 0.00027258 rank 0
2022-12-07 02:44:53,689 DEBUG TRAIN Batch 21/3000 loss 14.413933 loss_att 370.229126 loss_ctc 23.110580 loss_rnnt 13.447639 lr 0.00027259 rank 2
2022-12-07 02:45:47,730 DEBUG TRAIN Batch 21/3100 loss 4.716580 loss_att 296.204681 loss_ctc 8.745371 loss_rnnt 4.268937 lr 0.00027253 rank 1
2022-12-07 02:45:47,741 DEBUG TRAIN Batch 21/3100 loss 7.251256 loss_att 300.550751 loss_ctc 16.259136 loss_rnnt 6.250381 lr 0.00027255 rank 2
2022-12-07 02:45:47,745 DEBUG TRAIN Batch 21/3100 loss 9.779007 loss_att 267.984528 loss_ctc 17.054377 loss_rnnt 8.970633 lr 0.00027254 rank 0
2022-12-07 02:45:47,752 DEBUG TRAIN Batch 21/3100 loss 14.387625 loss_att 293.158295 loss_ctc 28.033808 loss_rnnt 12.871383 lr 0.00027258 rank 3
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-rnnt-runtime/wenet/utils/executor.py", line 90, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 4056) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-rnnt-runtime/wenet/utils/executor.py", line 84, in train
    loss_dict = model(feats, feats_lengths, target,
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 878, in forward
    self._sync_params()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1370, in _sync_params
    authoritative_rank = self._find_common_rank(
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1352, in _find_common_rank
    dist.all_reduce(rank_to_use, op=ReduceOp.MAX, group=self.process_group)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1292, in all_reduce
    work.wait()
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:3291
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-rnnt-runtime/wenet/utils/executor.py", line 84, in train
    loss_dict = model(feats, feats_lengths, target,
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 878, in forward
    self._sync_params()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1370, in _sync_params
    authoritative_rank = self._find_common_rank(
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1352, in _find_common_rank
    dist.all_reduce(rank_to_use, op=ReduceOp.MAX, group=self.process_group)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1292, in all_reduce
    work.wait()
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:34214
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-rnnt-runtime/wenet/utils/executor.py", line 84, in train
    loss_dict = model(feats, feats_lengths, target,
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 878, in forward
    self._sync_params()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1370, in _sync_params
    authoritative_rank = self._find_common_rank(
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1352, in _find_common_rank
    dist.all_reduce(rank_to_use, op=ReduceOp.MAX, group=self.process_group)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1292, in all_reduce
    work.wait()
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:35740
