/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_25_rnnt_bias_3word_finetune_seperate.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_25_rnnt_bias_sepeate_2_class_3word_finetune/ddp_init
2023-02-27 12:57:58,020 INFO training on multiple gpus, this gpu 4
2023-02-27 12:57:58,020 INFO training on multiple gpus, this gpu 6
2023-02-27 12:57:58,020 INFO training on multiple gpus, this gpu 3
2023-02-27 12:57:58,020 INFO training on multiple gpus, this gpu 7
2023-02-27 12:57:58,025 INFO training on multiple gpus, this gpu 5
2023-02-27 12:57:58,894 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-27 12:57:58,909 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-27 12:57:58,914 INFO Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-27 12:57:58,917 INFO Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-27 12:57:58,918 INFO Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-27 12:57:58,919 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
2023-02-27 12:57:58,919 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
2023-02-27 12:57:58,922 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
2023-02-27 12:57:58,923 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
2023-02-27 12:57:58,929 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
2023-02-27 12:58:08,625 INFO Checkpoint: loading from checkpoint exp/2_25_rnnt_bias_sepeate_2_class_3word_finetune/43.pt for GPU
2023-02-27 12:58:08,647 INFO Checkpoint: loading from checkpoint exp/2_25_rnnt_bias_sepeate_2_class_3word_finetune/43.pt for GPU
2023-02-27 12:58:08,671 INFO Checkpoint: loading from checkpoint exp/2_25_rnnt_bias_sepeate_2_class_3word_finetune/43.pt for GPU
2023-02-27 12:58:08,693 INFO Checkpoint: loading from checkpoint exp/2_25_rnnt_bias_sepeate_2_class_3word_finetune/43.pt for GPU
2023-02-27 12:58:12,573 INFO Checkpoint: loading from checkpoint exp/2_25_rnnt_bias_sepeate_2_class_3word_finetune/43.pt for GPU
2023-02-27 12:58:54,766 INFO Epoch 44 TRAIN info lr 4e-08
2023-02-27 12:58:54,768 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-27 12:58:54,778 INFO Epoch 44 TRAIN info lr 4e-08
2023-02-27 12:58:54,780 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-27 12:58:54,782 INFO Epoch 44 TRAIN info lr 4e-08
2023-02-27 12:58:54,784 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-27 12:58:54,846 INFO Epoch 44 TRAIN info lr 4e-08
2023-02-27 12:58:54,847 INFO Epoch 44 TRAIN info lr 4e-08
2023-02-27 12:58:54,848 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-27 12:58:54,849 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=256, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=256, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=256, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58499370
2023-02-27 13:00:50,100 DEBUG TRAIN Batch 44/0 loss 8.408676 loss_att 7.194131 loss_ctc 10.531454 loss_rnnt 5.643568 hw_loss 5.109337 lr 0.00029907 rank 3
2023-02-27 13:00:50,106 DEBUG TRAIN Batch 44/0 loss 8.896162 loss_att 7.386924 loss_ctc 9.728000 loss_rnnt 6.345782 hw_loss 5.139968 lr 0.00029907 rank 1
2023-02-27 13:00:50,174 DEBUG TRAIN Batch 44/0 loss 6.964941 loss_att 5.083821 loss_ctc 6.840999 loss_rnnt 4.600937 hw_loss 5.168911 lr 0.00029907 rank 2
2023-02-27 13:00:50,230 DEBUG TRAIN Batch 44/0 loss 6.467267 loss_att 4.965355 loss_ctc 7.663309 loss_rnnt 3.861490 hw_loss 5.150040 lr 0.00029907 rank 0
2023-02-27 13:00:50,285 DEBUG TRAIN Batch 44/0 loss 14.031762 loss_att 12.858135 loss_ctc 17.833828 loss_rnnt 11.029565 hw_loss 5.118713 lr 0.00029907 rank 4
2023-02-27 13:02:53,089 DEBUG TRAIN Batch 44/100 loss 6.471592 loss_att 7.964786 loss_ctc 11.090335 loss_rnnt 3.938948 hw_loss 3.034074 lr 0.00029902 rank 2
2023-02-27 13:02:53,091 DEBUG TRAIN Batch 44/100 loss 11.407245 loss_att 13.753664 loss_ctc 14.702576 loss_rnnt 8.925081 hw_loss 2.950317 lr 0.00029902 rank 0
2023-02-27 13:02:53,092 DEBUG TRAIN Batch 44/100 loss 5.732707 loss_att 8.940456 loss_ctc 7.494298 loss_rnnt 3.330709 hw_loss 2.860440 lr 0.00029902 rank 1
2023-02-27 13:02:53,094 DEBUG TRAIN Batch 44/100 loss 5.155304 loss_att 6.391402 loss_ctc 8.272635 loss_rnnt 2.908533 hw_loss 2.969825 lr 0.00029902 rank 4
2023-02-27 13:02:53,153 DEBUG TRAIN Batch 44/100 loss 3.204192 loss_att 4.146346 loss_ctc 3.318304 loss_rnnt 1.468570 hw_loss 2.872456 lr 0.00029902 rank 3
2023-02-27 13:04:18,960 DEBUG TRAIN Batch 44/200 loss 4.709579 loss_att 10.404590 loss_ctc 7.375476 loss_rnnt 2.932125 hw_loss 0.530623 lr 0.00029897 rank 3
2023-02-27 13:04:18,962 DEBUG TRAIN Batch 44/200 loss 11.033462 loss_att 13.676571 loss_ctc 15.046530 loss_rnnt 9.774345 hw_loss 0.366408 lr 0.00029897 rank 4
2023-02-27 13:04:18,962 DEBUG TRAIN Batch 44/200 loss 8.692278 loss_att 11.345240 loss_ctc 14.955104 loss_rnnt 7.035572 hw_loss 0.545758 lr 0.00029897 rank 0
2023-02-27 13:04:18,963 DEBUG TRAIN Batch 44/200 loss 8.085781 loss_att 11.352433 loss_ctc 15.129276 loss_rnnt 6.279180 hw_loss 0.401509 lr 0.00029897 rank 1
2023-02-27 13:04:18,964 DEBUG TRAIN Batch 44/200 loss 2.481561 loss_att 5.219671 loss_ctc 3.426917 loss_rnnt 1.689836 hw_loss 0.221355 lr 0.00029897 rank 2
2023-02-27 13:05:46,756 DEBUG TRAIN Batch 44/300 loss 3.815370 loss_att 6.721997 loss_ctc 8.331581 loss_rnnt 2.415532 hw_loss 0.405658 lr 0.00029891 rank 0
2023-02-27 13:05:46,759 DEBUG TRAIN Batch 44/300 loss 5.628370 loss_att 10.487219 loss_ctc 11.757517 loss_rnnt 3.657472 hw_loss 0.341078 lr 0.00029891 rank 4
2023-02-27 13:05:46,764 DEBUG TRAIN Batch 44/300 loss 7.680444 loss_att 12.185766 loss_ctc 14.080976 loss_rnnt 5.768305 hw_loss 0.295632 lr 0.00029891 rank 1
2023-02-27 13:05:46,767 DEBUG TRAIN Batch 44/300 loss 5.924177 loss_att 9.676848 loss_ctc 13.392296 loss_rnnt 4.083535 hw_loss 0.176920 lr 0.00029891 rank 3
2023-02-27 13:05:46,821 DEBUG TRAIN Batch 44/300 loss 6.667151 loss_att 12.164740 loss_ctc 17.934277 loss_rnnt 3.944324 hw_loss 0.226924 lr 0.00029891 rank 2
2023-02-27 13:07:39,982 DEBUG TRAIN Batch 44/400 loss 3.943033 loss_att 7.624933 loss_ctc 6.852476 loss_rnnt 2.667351 hw_loss 0.283829 lr 0.00029886 rank 3
2023-02-27 13:07:39,983 DEBUG TRAIN Batch 44/400 loss 16.285831 loss_att 23.605885 loss_ctc 28.660910 loss_rnnt 13.062721 hw_loss 0.204545 lr 0.00029886 rank 2
2023-02-27 13:07:39,984 DEBUG TRAIN Batch 44/400 loss 11.853297 loss_att 15.546116 loss_ctc 18.991081 loss_rnnt 10.061541 hw_loss 0.190288 lr 0.00029886 rank 1
2023-02-27 13:07:39,986 DEBUG TRAIN Batch 44/400 loss 5.773048 loss_att 9.173566 loss_ctc 9.906264 loss_rnnt 4.283744 hw_loss 0.483946 lr 0.00029886 rank 0
2023-02-27 13:07:39,988 DEBUG TRAIN Batch 44/400 loss 6.939204 loss_att 9.061662 loss_ctc 12.424428 loss_rnnt 5.662224 hw_loss 0.227109 lr 0.00029886 rank 4
2023-02-27 13:09:10,437 DEBUG TRAIN Batch 44/500 loss 13.127593 loss_att 16.159248 loss_ctc 24.108982 loss_rnnt 10.931643 hw_loss 0.235188 lr 0.00029881 rank 3
2023-02-27 13:09:10,438 DEBUG TRAIN Batch 44/500 loss 6.846345 loss_att 9.885109 loss_ctc 10.759862 loss_rnnt 5.577976 hw_loss 0.260278 lr 0.00029881 rank 2
2023-02-27 13:09:10,439 DEBUG TRAIN Batch 44/500 loss 7.500263 loss_att 8.094566 loss_ctc 13.115425 loss_rnnt 6.483592 hw_loss 0.279604 lr 0.00029881 rank 1
2023-02-27 13:09:10,442 DEBUG TRAIN Batch 44/500 loss 11.264009 loss_att 13.620502 loss_ctc 22.747625 loss_rnnt 9.108347 hw_loss 0.287278 lr 0.00029881 rank 0
2023-02-27 13:09:10,445 DEBUG TRAIN Batch 44/500 loss 8.732038 loss_att 11.762236 loss_ctc 13.128817 loss_rnnt 7.417212 hw_loss 0.229780 lr 0.00029881 rank 4
2023-02-27 13:10:35,166 DEBUG TRAIN Batch 44/600 loss 10.362006 loss_att 11.903381 loss_ctc 15.654066 loss_rnnt 9.192371 hw_loss 0.292035 lr 0.00029875 rank 0
2023-02-27 13:10:35,166 DEBUG TRAIN Batch 44/600 loss 7.240479 loss_att 7.049365 loss_ctc 10.125453 loss_rnnt 6.681290 hw_loss 0.398903 lr 0.00029875 rank 4
2023-02-27 13:10:35,167 DEBUG TRAIN Batch 44/600 loss 6.938233 loss_att 10.100534 loss_ctc 10.521898 loss_rnnt 5.652981 hw_loss 0.328069 lr 0.00029875 rank 1
2023-02-27 13:10:35,168 DEBUG TRAIN Batch 44/600 loss 9.414962 loss_att 11.288068 loss_ctc 12.976342 loss_rnnt 8.339020 hw_loss 0.424630 lr 0.00029875 rank 2
2023-02-27 13:10:35,221 DEBUG TRAIN Batch 44/600 loss 9.873072 loss_att 10.267428 loss_ctc 15.095960 loss_rnnt 8.973665 hw_loss 0.232781 lr 0.00029875 rank 3
2023-02-27 13:11:56,853 DEBUG TRAIN Batch 44/700 loss 8.112727 loss_att 10.396849 loss_ctc 14.288707 loss_rnnt 6.718490 hw_loss 0.213653 lr 0.00029870 rank 4
2023-02-27 13:11:56,858 DEBUG TRAIN Batch 44/700 loss 1.307155 loss_att 3.377038 loss_ctc 3.134581 loss_rnnt 0.519602 hw_loss 0.243599 lr 0.00029870 rank 3
2023-02-27 13:11:56,858 DEBUG TRAIN Batch 44/700 loss 3.129107 loss_att 6.554699 loss_ctc 3.581236 loss_rnnt 2.288395 hw_loss 0.178704 lr 0.00029870 rank 1
2023-02-27 13:11:56,862 DEBUG TRAIN Batch 44/700 loss 2.737005 loss_att 6.248610 loss_ctc 4.930974 loss_rnnt 1.655611 hw_loss 0.162269 lr 0.00029870 rank 0
2023-02-27 13:11:56,911 DEBUG TRAIN Batch 44/700 loss 2.480242 loss_att 6.688129 loss_ctc 2.846349 loss_rnnt 1.474728 hw_loss 0.215855 lr 0.00029870 rank 2
2023-02-27 13:13:53,538 DEBUG TRAIN Batch 44/800 loss 4.952046 loss_att 6.469886 loss_ctc 6.042990 loss_rnnt 4.342387 hw_loss 0.301183 lr 0.00029865 rank 1
2023-02-27 13:13:53,542 DEBUG TRAIN Batch 44/800 loss 4.779509 loss_att 8.353977 loss_ctc 10.173301 loss_rnnt 3.219016 hw_loss 0.237051 lr 0.00029865 rank 4
2023-02-27 13:13:53,546 DEBUG TRAIN Batch 44/800 loss 1.190021 loss_att 4.480318 loss_ctc 1.083268 loss_rnnt 0.374188 hw_loss 0.322513 lr 0.00029865 rank 3
2023-02-27 13:13:53,547 DEBUG TRAIN Batch 44/800 loss 3.970809 loss_att 6.316919 loss_ctc 8.504240 loss_rnnt 2.705801 hw_loss 0.358742 lr 0.00029865 rank 0
2023-02-27 13:13:53,620 DEBUG TRAIN Batch 44/800 loss 5.042924 loss_att 11.806557 loss_ctc 8.748674 loss_rnnt 3.122797 hw_loss 0.137437 lr 0.00029865 rank 2
2023-02-27 13:15:11,926 DEBUG TRAIN Batch 44/900 loss 3.568649 loss_att 7.792731 loss_ctc 9.533075 loss_rnnt 1.749300 hw_loss 0.336142 lr 0.00029859 rank 0
2023-02-27 13:15:11,928 DEBUG TRAIN Batch 44/900 loss 3.843959 loss_att 6.786334 loss_ctc 7.658842 loss_rnnt 2.552858 hw_loss 0.363703 lr 0.00029859 rank 2
2023-02-27 13:15:11,932 DEBUG TRAIN Batch 44/900 loss 6.474243 loss_att 7.702654 loss_ctc 7.425971 loss_rnnt 5.969129 hw_loss 0.248503 lr 0.00029859 rank 1
2023-02-27 13:15:11,935 DEBUG TRAIN Batch 44/900 loss 6.141056 loss_att 10.275873 loss_ctc 11.844045 loss_rnnt 4.391570 hw_loss 0.303982 lr 0.00029859 rank 4
2023-02-27 13:15:11,939 DEBUG TRAIN Batch 44/900 loss 10.053208 loss_att 11.425143 loss_ctc 15.250106 loss_rnnt 9.009834 hw_loss 0.142625 lr 0.00029859 rank 3
2023-02-27 13:16:33,542 DEBUG TRAIN Batch 44/1000 loss 5.431140 loss_att 8.584871 loss_ctc 11.166715 loss_rnnt 3.899014 hw_loss 0.256193 lr 0.00029854 rank 3
2023-02-27 13:16:33,545 DEBUG TRAIN Batch 44/1000 loss 4.507188 loss_att 7.092019 loss_ctc 7.804968 loss_rnnt 3.423816 hw_loss 0.237566 lr 0.00029854 rank 1
2023-02-27 13:16:33,550 DEBUG TRAIN Batch 44/1000 loss 6.003956 loss_att 11.357924 loss_ctc 13.526969 loss_rnnt 3.834519 hw_loss 0.179203 lr 0.00029854 rank 0
2023-02-27 13:16:33,551 DEBUG TRAIN Batch 44/1000 loss 9.760344 loss_att 13.905130 loss_ctc 17.064144 loss_rnnt 7.724357 hw_loss 0.437232 lr 0.00029854 rank 4
2023-02-27 13:16:33,600 DEBUG TRAIN Batch 44/1000 loss 5.507715 loss_att 10.219671 loss_ctc 12.939098 loss_rnnt 3.462886 hw_loss 0.209224 lr 0.00029854 rank 2
2023-02-27 13:18:27,298 DEBUG TRAIN Batch 44/1100 loss 12.477044 loss_att 13.254974 loss_ctc 17.519482 loss_rnnt 11.364701 hw_loss 0.533309 lr 0.00029849 rank 4
2023-02-27 13:18:27,299 DEBUG TRAIN Batch 44/1100 loss 7.382356 loss_att 10.483241 loss_ctc 15.154885 loss_rnnt 5.567176 hw_loss 0.297497 lr 0.00029849 rank 1
2023-02-27 13:18:27,299 DEBUG TRAIN Batch 44/1100 loss 10.122064 loss_att 12.818258 loss_ctc 15.840330 loss_rnnt 8.676150 hw_loss 0.270449 lr 0.00029849 rank 2
2023-02-27 13:18:27,300 DEBUG TRAIN Batch 44/1100 loss 7.308558 loss_att 9.726160 loss_ctc 10.157159 loss_rnnt 6.338632 hw_loss 0.199860 lr 0.00029849 rank 0
2023-02-27 13:18:27,304 DEBUG TRAIN Batch 44/1100 loss 7.909337 loss_att 13.959671 loss_ctc 18.108038 loss_rnnt 5.110750 hw_loss 0.428800 lr 0.00029849 rank 3
2023-02-27 13:19:46,463 DEBUG TRAIN Batch 44/1200 loss 4.929724 loss_att 7.361072 loss_ctc 10.814155 loss_rnnt 3.494806 hw_loss 0.307607 lr 0.00029843 rank 3
2023-02-27 13:19:46,469 DEBUG TRAIN Batch 44/1200 loss 8.904695 loss_att 11.352709 loss_ctc 12.438219 loss_rnnt 7.810291 hw_loss 0.250621 lr 0.00029843 rank 2
2023-02-27 13:19:46,470 DEBUG TRAIN Batch 44/1200 loss 9.060785 loss_att 10.727173 loss_ctc 10.832191 loss_rnnt 8.283415 hw_loss 0.389824 lr 0.00029843 rank 1
2023-02-27 13:19:46,481 DEBUG TRAIN Batch 44/1200 loss 8.254161 loss_att 10.254098 loss_ctc 14.386986 loss_rnnt 6.849260 hw_loss 0.351006 lr 0.00029843 rank 0
2023-02-27 13:19:46,491 DEBUG TRAIN Batch 44/1200 loss 5.068464 loss_att 6.963713 loss_ctc 7.191346 loss_rnnt 4.233427 hw_loss 0.324257 lr 0.00029843 rank 4
2023-02-27 13:21:07,128 DEBUG TRAIN Batch 44/1300 loss 9.010270 loss_att 13.207283 loss_ctc 15.408993 loss_rnnt 7.198306 hw_loss 0.223873 lr 0.00029838 rank 4
2023-02-27 13:21:07,133 DEBUG TRAIN Batch 44/1300 loss 7.937769 loss_att 10.326531 loss_ctc 14.936183 loss_rnnt 6.341930 hw_loss 0.346810 lr 0.00029838 rank 0
2023-02-27 13:21:07,133 DEBUG TRAIN Batch 44/1300 loss 1.883078 loss_att 5.876272 loss_ctc 4.624782 loss_rnnt 0.572776 hw_loss 0.273944 lr 0.00029838 rank 3
2023-02-27 13:21:07,133 DEBUG TRAIN Batch 44/1300 loss 4.509807 loss_att 7.172405 loss_ctc 9.018734 loss_rnnt 3.252187 hw_loss 0.232332 lr 0.00029838 rank 1
2023-02-27 13:21:07,234 DEBUG TRAIN Batch 44/1300 loss 2.264695 loss_att 6.674715 loss_ctc 5.164124 loss_rnnt 0.805523 hw_loss 0.357332 lr 0.00029838 rank 2
2023-02-27 13:22:29,396 DEBUG TRAIN Batch 44/1400 loss 2.722852 loss_att 6.200186 loss_ctc 5.377466 loss_rnnt 1.564246 hw_loss 0.204732 lr 0.00029833 rank 3
2023-02-27 13:22:29,397 DEBUG TRAIN Batch 44/1400 loss 2.131439 loss_att 4.839016 loss_ctc 3.105599 loss_rnnt 1.291658 hw_loss 0.315707 lr 0.00029833 rank 1
2023-02-27 13:22:29,398 DEBUG TRAIN Batch 44/1400 loss 6.825091 loss_att 9.081876 loss_ctc 10.893558 loss_rnnt 5.703004 hw_loss 0.240502 lr 0.00029833 rank 4
2023-02-27 13:22:29,400 DEBUG TRAIN Batch 44/1400 loss 6.639788 loss_att 12.184294 loss_ctc 15.413315 loss_rnnt 4.213375 hw_loss 0.276953 lr 0.00029833 rank 2
2023-02-27 13:22:29,441 DEBUG TRAIN Batch 44/1400 loss 4.319641 loss_att 8.246032 loss_ctc 7.503935 loss_rnnt 2.976500 hw_loss 0.249918 lr 0.00029833 rank 0
2023-02-27 13:24:21,901 DEBUG TRAIN Batch 44/1500 loss 5.258251 loss_att 7.346446 loss_ctc 8.741867 loss_rnnt 4.309854 hw_loss 0.124269 lr 0.00029828 rank 1
2023-02-27 13:24:21,903 DEBUG TRAIN Batch 44/1500 loss 4.597550 loss_att 8.789435 loss_ctc 8.573573 loss_rnnt 3.070218 hw_loss 0.297784 lr 0.00029828 rank 0
2023-02-27 13:24:21,906 DEBUG TRAIN Batch 44/1500 loss 6.412929 loss_att 9.044820 loss_ctc 9.623487 loss_rnnt 5.329843 hw_loss 0.241187 lr 0.00029828 rank 4
2023-02-27 13:24:21,911 DEBUG TRAIN Batch 44/1500 loss 7.312378 loss_att 8.267485 loss_ctc 13.121462 loss_rnnt 6.228854 hw_loss 0.221173 lr 0.00029828 rank 3
2023-02-27 13:24:21,912 DEBUG TRAIN Batch 44/1500 loss 2.555713 loss_att 5.534821 loss_ctc 5.786493 loss_rnnt 1.406039 hw_loss 0.230779 lr 0.00029828 rank 2
2023-02-27 13:25:47,091 DEBUG TRAIN Batch 44/1600 loss 6.483898 loss_att 9.663816 loss_ctc 11.375141 loss_rnnt 5.074940 hw_loss 0.226515 lr 0.00029822 rank 0
2023-02-27 13:25:47,097 DEBUG TRAIN Batch 44/1600 loss 8.555273 loss_att 12.519384 loss_ctc 16.011139 loss_rnnt 6.687118 hw_loss 0.152284 lr 0.00029822 rank 3
2023-02-27 13:25:47,098 DEBUG TRAIN Batch 44/1600 loss 8.743499 loss_att 10.450619 loss_ctc 15.094341 loss_rnnt 7.453261 hw_loss 0.191314 lr 0.00029822 rank 4
2023-02-27 13:25:47,103 DEBUG TRAIN Batch 44/1600 loss 8.717793 loss_att 13.570110 loss_ctc 16.685581 loss_rnnt 6.469771 hw_loss 0.403473 lr 0.00029822 rank 2
2023-02-27 13:25:47,105 DEBUG TRAIN Batch 44/1600 loss 6.030334 loss_att 9.302743 loss_ctc 6.754157 loss_rnnt 5.151541 hw_loss 0.239629 lr 0.00029822 rank 1
2023-02-27 13:27:05,584 DEBUG TRAIN Batch 44/1700 loss 6.691419 loss_att 8.287529 loss_ctc 7.746784 loss_rnnt 6.100134 hw_loss 0.246276 lr 0.00029817 rank 1
2023-02-27 13:27:05,583 DEBUG TRAIN Batch 44/1700 loss 7.734872 loss_att 9.460323 loss_ctc 9.350657 loss_rnnt 7.054972 hw_loss 0.223824 lr 0.00029817 rank 2
2023-02-27 13:27:05,588 DEBUG TRAIN Batch 44/1700 loss 5.544180 loss_att 7.520164 loss_ctc 8.564165 loss_rnnt 4.630977 hw_loss 0.216267 lr 0.00029817 rank 4
2023-02-27 13:27:05,590 DEBUG TRAIN Batch 44/1700 loss 9.151631 loss_att 11.635710 loss_ctc 15.966454 loss_rnnt 7.617899 hw_loss 0.240514 lr 0.00029817 rank 3
2023-02-27 13:27:05,591 DEBUG TRAIN Batch 44/1700 loss 10.824289 loss_att 9.979384 loss_ctc 12.204071 loss_rnnt 10.665841 hw_loss 0.268983 lr 0.00029817 rank 0
2023-02-27 13:29:00,332 DEBUG TRAIN Batch 44/1800 loss 2.542805 loss_att 4.205069 loss_ctc 2.950413 loss_rnnt 2.012188 hw_loss 0.269655 lr 0.00029812 rank 2
2023-02-27 13:29:00,341 DEBUG TRAIN Batch 44/1800 loss 9.822227 loss_att 11.887807 loss_ctc 14.471906 loss_rnnt 8.621938 hw_loss 0.313529 lr 0.00029812 rank 0
2023-02-27 13:29:00,343 DEBUG TRAIN Batch 44/1800 loss 3.342250 loss_att 4.965381 loss_ctc 5.360726 loss_rnnt 2.524999 hw_loss 0.419053 lr 0.00029812 rank 4
2023-02-27 13:29:00,344 DEBUG TRAIN Batch 44/1800 loss 8.684862 loss_att 10.409265 loss_ctc 11.273134 loss_rnnt 7.874607 hw_loss 0.225508 lr 0.00029812 rank 1
2023-02-27 13:29:00,346 DEBUG TRAIN Batch 44/1800 loss 10.142399 loss_att 11.806332 loss_ctc 14.349349 loss_rnnt 9.086505 hw_loss 0.304089 lr 0.00029812 rank 3
2023-02-27 13:30:20,806 DEBUG TRAIN Batch 44/1900 loss 11.555843 loss_att 12.349038 loss_ctc 18.701761 loss_rnnt 10.245146 hw_loss 0.373630 lr 0.00029806 rank 1
2023-02-27 13:30:20,806 DEBUG TRAIN Batch 44/1900 loss 7.862864 loss_att 12.445342 loss_ctc 15.423882 loss_rnnt 5.855059 hw_loss 0.155950 lr 0.00029806 rank 0
2023-02-27 13:30:20,807 DEBUG TRAIN Batch 44/1900 loss 9.408278 loss_att 13.914565 loss_ctc 15.658785 loss_rnnt 7.581850 hw_loss 0.172068 lr 0.00029806 rank 2
2023-02-27 13:30:20,811 DEBUG TRAIN Batch 44/1900 loss 5.598328 loss_att 9.096870 loss_ctc 12.741177 loss_rnnt 3.907842 hw_loss 0.071995 lr 0.00029806 rank 4
2023-02-27 13:30:20,866 DEBUG TRAIN Batch 44/1900 loss 7.877827 loss_att 8.272749 loss_ctc 11.248171 loss_rnnt 7.063933 hw_loss 0.535369 lr 0.00029806 rank 3
2023-02-27 13:31:42,524 DEBUG TRAIN Batch 44/2000 loss 8.897728 loss_att 11.046510 loss_ctc 15.281772 loss_rnnt 7.404532 hw_loss 0.397937 lr 0.00029801 rank 0
2023-02-27 13:31:42,527 DEBUG TRAIN Batch 44/2000 loss 11.487154 loss_att 16.468735 loss_ctc 15.777851 loss_rnnt 9.691020 hw_loss 0.426983 lr 0.00029801 rank 1
2023-02-27 13:31:42,537 DEBUG TRAIN Batch 44/2000 loss 7.703646 loss_att 8.022312 loss_ctc 10.461184 loss_rnnt 7.046219 hw_loss 0.423791 lr 0.00029801 rank 4
2023-02-27 13:31:42,549 DEBUG TRAIN Batch 44/2000 loss 6.884955 loss_att 11.221292 loss_ctc 13.474017 loss_rnnt 4.999606 hw_loss 0.261637 lr 0.00029801 rank 2
2023-02-27 13:31:42,583 DEBUG TRAIN Batch 44/2000 loss 9.088418 loss_att 14.573822 loss_ctc 18.313007 loss_rnnt 6.573614 hw_loss 0.352084 lr 0.00029801 rank 3
2023-02-27 13:33:04,568 DEBUG TRAIN Batch 44/2100 loss 3.930228 loss_att 6.292772 loss_ctc 6.364213 loss_rnnt 3.007477 hw_loss 0.235707 lr 0.00029796 rank 2
2023-02-27 13:33:04,568 DEBUG TRAIN Batch 44/2100 loss 13.399935 loss_att 18.778513 loss_ctc 27.032713 loss_rnnt 10.368601 hw_loss 0.258588 lr 0.00029796 rank 1
2023-02-27 13:33:04,569 DEBUG TRAIN Batch 44/2100 loss 5.914686 loss_att 6.993170 loss_ctc 7.316976 loss_rnnt 5.363039 hw_loss 0.279336 lr 0.00029796 rank 4
2023-02-27 13:33:04,572 DEBUG TRAIN Batch 44/2100 loss 7.235012 loss_att 7.938740 loss_ctc 7.439705 loss_rnnt 6.893431 hw_loss 0.325392 lr 0.00029796 rank 0
2023-02-27 13:33:04,626 DEBUG TRAIN Batch 44/2100 loss 6.205593 loss_att 10.384930 loss_ctc 15.283751 loss_rnnt 3.970565 hw_loss 0.353886 lr 0.00029796 rank 3
2023-02-27 13:34:56,244 DEBUG TRAIN Batch 44/2200 loss 6.920828 loss_att 9.269078 loss_ctc 12.998337 loss_rnnt 5.453172 hw_loss 0.351884 lr 0.00029790 rank 4
2023-02-27 13:34:56,245 DEBUG TRAIN Batch 44/2200 loss 6.353133 loss_att 8.526571 loss_ctc 10.623360 loss_rnnt 5.250847 hw_loss 0.184190 lr 0.00029790 rank 2
2023-02-27 13:34:56,245 DEBUG TRAIN Batch 44/2200 loss 14.784074 loss_att 17.669357 loss_ctc 27.285923 loss_rnnt 12.437908 hw_loss 0.191615 lr 0.00029790 rank 0
2023-02-27 13:34:56,247 DEBUG TRAIN Batch 44/2200 loss 5.287857 loss_att 10.190288 loss_ctc 11.321430 loss_rnnt 3.352329 hw_loss 0.282311 lr 0.00029790 rank 1
2023-02-27 13:34:56,340 DEBUG TRAIN Batch 44/2200 loss 4.414814 loss_att 8.951196 loss_ctc 7.565422 loss_rnnt 2.981343 hw_loss 0.198964 lr 0.00029790 rank 3
2023-02-27 13:36:15,656 DEBUG TRAIN Batch 44/2300 loss 3.968471 loss_att 8.440193 loss_ctc 11.860495 loss_rnnt 1.904015 hw_loss 0.220952 lr 0.00029785 rank 2
2023-02-27 13:36:15,660 DEBUG TRAIN Batch 44/2300 loss 8.958872 loss_att 12.378843 loss_ctc 15.519827 loss_rnnt 7.262960 hw_loss 0.257105 lr 0.00029785 rank 4
2023-02-27 13:36:15,660 DEBUG TRAIN Batch 44/2300 loss 4.938982 loss_att 7.937519 loss_ctc 8.339733 loss_rnnt 3.785680 hw_loss 0.187803 lr 0.00029785 rank 3
2023-02-27 13:36:15,661 DEBUG TRAIN Batch 44/2300 loss 11.975984 loss_att 17.592728 loss_ctc 27.242680 loss_rnnt 8.676655 hw_loss 0.263287 lr 0.00029785 rank 1
2023-02-27 13:36:15,663 DEBUG TRAIN Batch 44/2300 loss 5.928151 loss_att 8.632402 loss_ctc 9.172134 loss_rnnt 4.743853 hw_loss 0.395468 lr 0.00029785 rank 0
2023-02-27 13:37:34,778 DEBUG TRAIN Batch 44/2400 loss 8.217150 loss_att 11.788239 loss_ctc 14.405696 loss_rnnt 6.526313 hw_loss 0.284022 lr 0.00029780 rank 0
2023-02-27 13:37:34,780 DEBUG TRAIN Batch 44/2400 loss 6.003765 loss_att 8.211374 loss_ctc 8.407319 loss_rnnt 5.037527 hw_loss 0.382953 lr 0.00029780 rank 2
2023-02-27 13:37:34,783 DEBUG TRAIN Batch 44/2400 loss 6.416017 loss_att 9.660946 loss_ctc 12.444976 loss_rnnt 4.794337 hw_loss 0.316561 lr 0.00029780 rank 3
2023-02-27 13:37:34,785 DEBUG TRAIN Batch 44/2400 loss 7.368051 loss_att 10.387186 loss_ctc 15.089247 loss_rnnt 5.637738 hw_loss 0.181862 lr 0.00029780 rank 4
2023-02-27 13:37:34,835 DEBUG TRAIN Batch 44/2400 loss 6.423544 loss_att 10.287777 loss_ctc 10.530990 loss_rnnt 4.957454 hw_loss 0.272970 lr 0.00029780 rank 1
2023-02-27 13:39:32,237 DEBUG TRAIN Batch 44/2500 loss 8.047925 loss_att 8.678451 loss_ctc 10.784266 loss_rnnt 7.227472 hw_loss 0.617819 lr 0.00029775 rank 3
2023-02-27 13:39:32,239 DEBUG TRAIN Batch 44/2500 loss 7.539796 loss_att 10.115910 loss_ctc 14.227026 loss_rnnt 5.956800 hw_loss 0.330266 lr 0.00029775 rank 2
2023-02-27 13:39:32,240 DEBUG TRAIN Batch 44/2500 loss 6.867434 loss_att 6.780331 loss_ctc 10.375125 loss_rnnt 6.163735 hw_loss 0.475173 lr 0.00029775 rank 0
2023-02-27 13:39:32,240 DEBUG TRAIN Batch 44/2500 loss 7.051472 loss_att 7.239994 loss_ctc 10.005885 loss_rnnt 6.253746 hw_loss 0.686439 lr 0.00029775 rank 4
2023-02-27 13:39:32,241 DEBUG TRAIN Batch 44/2500 loss 6.580403 loss_att 8.087577 loss_ctc 10.810434 loss_rnnt 5.556209 hw_loss 0.297668 lr 0.00029775 rank 1
2023-02-27 13:40:51,929 DEBUG TRAIN Batch 44/2600 loss 3.952573 loss_att 6.315732 loss_ctc 5.339318 loss_rnnt 3.102206 hw_loss 0.361567 lr 0.00029769 rank 0
2023-02-27 13:40:51,931 DEBUG TRAIN Batch 44/2600 loss 5.220589 loss_att 9.353427 loss_ctc 10.042077 loss_rnnt 3.604234 hw_loss 0.275477 lr 0.00029769 rank 2
2023-02-27 13:40:51,931 DEBUG TRAIN Batch 44/2600 loss 16.856043 loss_att 16.925703 loss_ctc 22.758312 loss_rnnt 15.860667 hw_loss 0.364641 lr 0.00029769 rank 1
2023-02-27 13:40:51,939 DEBUG TRAIN Batch 44/2600 loss 7.497257 loss_att 10.016384 loss_ctc 11.949692 loss_rnnt 6.288857 hw_loss 0.207969 lr 0.00029769 rank 4
2023-02-27 13:40:52,014 DEBUG TRAIN Batch 44/2600 loss 1.694104 loss_att 4.083356 loss_ctc 4.285603 loss_rnnt 0.756056 hw_loss 0.214997 lr 0.00029769 rank 3
2023-02-27 13:42:11,170 DEBUG TRAIN Batch 44/2700 loss 4.302611 loss_att 9.642466 loss_ctc 8.990175 loss_rnnt 2.538179 hw_loss 0.133974 lr 0.00029764 rank 3
2023-02-27 13:42:11,170 DEBUG TRAIN Batch 44/2700 loss 9.583635 loss_att 10.614048 loss_ctc 9.565048 loss_rnnt 9.312550 hw_loss 0.126529 lr 0.00029764 rank 4
2023-02-27 13:42:11,171 DEBUG TRAIN Batch 44/2700 loss 3.862477 loss_att 8.078115 loss_ctc 4.565549 loss_rnnt 2.836267 hw_loss 0.167511 lr 0.00029764 rank 2
2023-02-27 13:42:11,171 DEBUG TRAIN Batch 44/2700 loss 7.634775 loss_att 11.048108 loss_ctc 16.707912 loss_rnnt 5.567440 hw_loss 0.327969 lr 0.00029764 rank 0
2023-02-27 13:42:11,174 DEBUG TRAIN Batch 44/2700 loss 6.259738 loss_att 9.139351 loss_ctc 8.835661 loss_rnnt 5.144970 hw_loss 0.366356 lr 0.00029764 rank 1
2023-02-27 13:43:31,560 DEBUG TRAIN Batch 44/2800 loss 5.514324 loss_att 8.967220 loss_ctc 10.950366 loss_rnnt 3.989976 hw_loss 0.204307 lr 0.00029759 rank 0
2023-02-27 13:43:31,565 DEBUG TRAIN Batch 44/2800 loss 8.437026 loss_att 10.634569 loss_ctc 12.916265 loss_rnnt 7.299612 hw_loss 0.188764 lr 0.00029759 rank 3
2023-02-27 13:43:31,567 DEBUG TRAIN Batch 44/2800 loss 9.892310 loss_att 15.663208 loss_ctc 22.727072 loss_rnnt 6.928230 hw_loss 0.184873 lr 0.00029759 rank 1
2023-02-27 13:43:31,573 DEBUG TRAIN Batch 44/2800 loss 9.932019 loss_att 13.696265 loss_ctc 16.357269 loss_rnnt 8.143181 hw_loss 0.336169 lr 0.00029759 rank 4
2023-02-27 13:43:31,589 DEBUG TRAIN Batch 44/2800 loss 3.779117 loss_att 6.739218 loss_ctc 7.762959 loss_rnnt 2.561175 hw_loss 0.177644 lr 0.00029759 rank 2
2023-02-27 13:45:26,390 DEBUG TRAIN Batch 44/2900 loss 2.442111 loss_att 5.835983 loss_ctc 4.740798 loss_rnnt 1.281843 hw_loss 0.328128 lr 0.00029754 rank 2
2023-02-27 13:45:26,392 DEBUG TRAIN Batch 44/2900 loss 6.246325 loss_att 7.951060 loss_ctc 8.199642 loss_rnnt 5.502757 hw_loss 0.266586 lr 0.00029754 rank 1
2023-02-27 13:45:26,393 DEBUG TRAIN Batch 44/2900 loss 14.359725 loss_att 15.902044 loss_ctc 24.514885 loss_rnnt 12.555616 hw_loss 0.265544 lr 0.00029754 rank 4
2023-02-27 13:45:26,393 DEBUG TRAIN Batch 44/2900 loss 11.947360 loss_att 14.613275 loss_ctc 19.207748 loss_rnnt 10.341328 hw_loss 0.196498 lr 0.00029754 rank 3
2023-02-27 13:45:26,399 DEBUG TRAIN Batch 44/2900 loss 4.064452 loss_att 5.432683 loss_ctc 5.782053 loss_rnnt 3.460164 hw_loss 0.190552 lr 0.00029754 rank 0
2023-02-27 13:46:45,787 DEBUG TRAIN Batch 44/3000 loss 11.401666 loss_att 14.827167 loss_ctc 21.101002 loss_rnnt 9.286860 hw_loss 0.255863 lr 0.00029748 rank 3
2023-02-27 13:46:45,791 DEBUG TRAIN Batch 44/3000 loss 7.647143 loss_att 9.950155 loss_ctc 12.686525 loss_rnnt 6.348911 hw_loss 0.310709 lr 0.00029748 rank 1
2023-02-27 13:46:45,792 DEBUG TRAIN Batch 44/3000 loss 5.749481 loss_att 8.357869 loss_ctc 10.254087 loss_rnnt 4.433548 hw_loss 0.363078 lr 0.00029748 rank 2
2023-02-27 13:46:45,795 DEBUG TRAIN Batch 44/3000 loss 5.305937 loss_att 8.871355 loss_ctc 10.980104 loss_rnnt 3.715755 hw_loss 0.226017 lr 0.00029748 rank 0
2023-02-27 13:46:45,798 DEBUG TRAIN Batch 44/3000 loss 4.517610 loss_att 7.097782 loss_ctc 9.004653 loss_rnnt 3.287528 hw_loss 0.217079 lr 0.00029748 rank 4
2023-02-27 13:48:07,971 DEBUG TRAIN Batch 44/3100 loss 8.395207 loss_att 11.234535 loss_ctc 16.475515 loss_rnnt 6.619077 hw_loss 0.245419 lr 0.00029743 rank 2
2023-02-27 13:48:07,973 DEBUG TRAIN Batch 44/3100 loss 11.697240 loss_att 14.002164 loss_ctc 18.407890 loss_rnnt 10.152505 hw_loss 0.354369 lr 0.00029743 rank 3
2023-02-27 13:48:07,976 DEBUG TRAIN Batch 44/3100 loss 7.851266 loss_att 10.537206 loss_ctc 14.469309 loss_rnnt 6.229571 hw_loss 0.378940 lr 0.00029743 rank 1
2023-02-27 13:48:07,980 DEBUG TRAIN Batch 44/3100 loss 10.812103 loss_att 11.258763 loss_ctc 15.099407 loss_rnnt 9.900188 hw_loss 0.470517 lr 0.00029743 rank 0
2023-02-27 13:48:08,018 DEBUG TRAIN Batch 44/3100 loss 8.371508 loss_att 10.029325 loss_ctc 16.531544 loss_rnnt 6.797450 hw_loss 0.289667 lr 0.00029743 rank 4
2023-02-27 13:50:01,345 DEBUG TRAIN Batch 44/3200 loss 1.302574 loss_att 3.695436 loss_ctc 2.685968 loss_rnnt 0.504522 hw_loss 0.253175 lr 0.00029738 rank 0
2023-02-27 13:50:01,346 DEBUG TRAIN Batch 44/3200 loss 3.665633 loss_att 6.154342 loss_ctc 5.045572 loss_rnnt 2.869420 hw_loss 0.214650 lr 0.00029738 rank 4
2023-02-27 13:50:01,349 DEBUG TRAIN Batch 44/3200 loss 4.292203 loss_att 6.548760 loss_ctc 7.843401 loss_rnnt 3.300114 hw_loss 0.126160 lr 0.00029738 rank 3
2023-02-27 13:50:01,411 DEBUG TRAIN Batch 44/3200 loss 7.407817 loss_att 7.242049 loss_ctc 10.685622 loss_rnnt 6.710815 hw_loss 0.549592 lr 0.00029738 rank 1
2023-02-27 13:50:01,417 DEBUG TRAIN Batch 44/3200 loss 9.104145 loss_att 12.023843 loss_ctc 15.204899 loss_rnnt 7.643836 hw_loss 0.118005 lr 0.00029738 rank 2
2023-02-27 13:51:23,976 DEBUG TRAIN Batch 44/3300 loss 5.843660 loss_att 8.990374 loss_ctc 10.721344 loss_rnnt 4.444066 hw_loss 0.224801 lr 0.00029732 rank 0
2023-02-27 13:51:23,978 DEBUG TRAIN Batch 44/3300 loss 10.159008 loss_att 12.815525 loss_ctc 15.498558 loss_rnnt 8.720076 hw_loss 0.366917 lr 0.00029732 rank 3
2023-02-27 13:51:23,979 DEBUG TRAIN Batch 44/3300 loss 4.519636 loss_att 7.592223 loss_ctc 7.091140 loss_rnnt 3.417264 hw_loss 0.271850 lr 0.00029732 rank 1
2023-02-27 13:51:23,981 DEBUG TRAIN Batch 44/3300 loss 7.221010 loss_att 10.856100 loss_ctc 16.527571 loss_rnnt 5.158400 hw_loss 0.177594 lr 0.00029732 rank 2
2023-02-27 13:51:23,981 DEBUG TRAIN Batch 44/3300 loss 2.642535 loss_att 6.105069 loss_ctc 2.660816 loss_rnnt 1.772798 hw_loss 0.327735 lr 0.00029732 rank 4
2023-02-27 13:52:44,895 DEBUG TRAIN Batch 44/3400 loss 5.969822 loss_att 9.023923 loss_ctc 9.563397 loss_rnnt 4.736515 hw_loss 0.268768 lr 0.00029727 rank 4
2023-02-27 13:52:44,895 DEBUG TRAIN Batch 44/3400 loss 8.151396 loss_att 12.544275 loss_ctc 13.482046 loss_rnnt 6.393173 hw_loss 0.316677 lr 0.00029727 rank 0
2023-02-27 13:52:44,895 DEBUG TRAIN Batch 44/3400 loss 13.210306 loss_att 18.924252 loss_ctc 29.849834 loss_rnnt 9.812377 hw_loss 0.068506 lr 0.00029727 rank 3
2023-02-27 13:52:44,896 DEBUG TRAIN Batch 44/3400 loss 9.671124 loss_att 13.661414 loss_ctc 22.392742 loss_rnnt 7.039232 hw_loss 0.258030 lr 0.00029727 rank 1
2023-02-27 13:52:44,897 DEBUG TRAIN Batch 44/3400 loss 3.387260 loss_att 9.015645 loss_ctc 9.129934 loss_rnnt 1.305443 hw_loss 0.357095 lr 0.00029727 rank 2
2023-02-27 13:54:04,986 DEBUG TRAIN Batch 44/3500 loss 3.096830 loss_att 6.362969 loss_ctc 6.088924 loss_rnnt 1.870966 hw_loss 0.325667 lr 0.00029722 rank 0
2023-02-27 13:54:04,986 DEBUG TRAIN Batch 44/3500 loss 2.799783 loss_att 5.192255 loss_ctc 5.475713 loss_rnnt 1.850003 hw_loss 0.214678 lr 0.00029722 rank 4
2023-02-27 13:54:04,987 DEBUG TRAIN Batch 44/3500 loss 4.273622 loss_att 7.298279 loss_ctc 11.584943 loss_rnnt 2.612992 hw_loss 0.151603 lr 0.00029722 rank 3
2023-02-27 13:54:04,992 DEBUG TRAIN Batch 44/3500 loss 6.351508 loss_att 8.984365 loss_ctc 8.045351 loss_rnnt 5.481001 hw_loss 0.221420 lr 0.00029722 rank 2
2023-02-27 13:54:04,992 DEBUG TRAIN Batch 44/3500 loss 3.884828 loss_att 7.375357 loss_ctc 10.754619 loss_rnnt 2.113426 hw_loss 0.294984 lr 0.00029722 rank 1
2023-02-27 13:55:59,103 DEBUG TRAIN Batch 44/3600 loss 10.508374 loss_att 11.493963 loss_ctc 18.011044 loss_rnnt 9.220016 hw_loss 0.170411 lr 0.00029717 rank 0
2023-02-27 13:55:59,107 DEBUG TRAIN Batch 44/3600 loss 6.842841 loss_att 12.581509 loss_ctc 14.408637 loss_rnnt 4.570354 hw_loss 0.217463 lr 0.00029717 rank 4
2023-02-27 13:55:59,108 DEBUG TRAIN Batch 44/3600 loss 6.213432 loss_att 7.605525 loss_ctc 8.446074 loss_rnnt 5.523959 hw_loss 0.212567 lr 0.00029717 rank 1
2023-02-27 13:55:59,108 DEBUG TRAIN Batch 44/3600 loss 11.594350 loss_att 15.674457 loss_ctc 19.355988 loss_rnnt 9.637683 hw_loss 0.198303 lr 0.00029717 rank 2
2023-02-27 13:55:59,109 DEBUG TRAIN Batch 44/3600 loss 6.267748 loss_att 7.584034 loss_ctc 12.956347 loss_rnnt 5.005531 hw_loss 0.200900 lr 0.00029717 rank 3
2023-02-27 13:57:18,967 DEBUG TRAIN Batch 44/3700 loss 4.789263 loss_att 9.009048 loss_ctc 11.180206 loss_rnnt 3.003687 hw_loss 0.167799 lr 0.00029711 rank 1
2023-02-27 13:57:18,971 DEBUG TRAIN Batch 44/3700 loss 10.473603 loss_att 13.112459 loss_ctc 18.072290 loss_rnnt 8.684630 hw_loss 0.465082 lr 0.00029711 rank 3
2023-02-27 13:57:18,972 DEBUG TRAIN Batch 44/3700 loss 7.027021 loss_att 9.978514 loss_ctc 12.696182 loss_rnnt 5.541721 hw_loss 0.260839 lr 0.00029711 rank 0
2023-02-27 13:57:18,972 DEBUG TRAIN Batch 44/3700 loss 7.031725 loss_att 8.519135 loss_ctc 9.729640 loss_rnnt 6.235460 hw_loss 0.260742 lr 0.00029711 rank 2
2023-02-27 13:57:18,975 DEBUG TRAIN Batch 44/3700 loss 7.262550 loss_att 10.419592 loss_ctc 9.846634 loss_rnnt 6.106891 hw_loss 0.336948 lr 0.00029711 rank 4
2023-02-27 13:58:38,730 DEBUG TRAIN Batch 44/3800 loss 8.302785 loss_att 8.237828 loss_ctc 11.886850 loss_rnnt 7.596776 hw_loss 0.452109 lr 0.00029706 rank 3
2023-02-27 13:58:38,737 DEBUG TRAIN Batch 44/3800 loss 5.550249 loss_att 5.496202 loss_ctc 8.634927 loss_rnnt 4.948543 hw_loss 0.377296 lr 0.00029706 rank 4
2023-02-27 13:58:38,739 DEBUG TRAIN Batch 44/3800 loss 4.615825 loss_att 9.171745 loss_ctc 9.113314 loss_rnnt 2.966252 hw_loss 0.260107 lr 0.00029706 rank 0
2023-02-27 13:58:38,753 DEBUG TRAIN Batch 44/3800 loss 4.106957 loss_att 10.413932 loss_ctc 8.535128 loss_rnnt 2.149324 hw_loss 0.198402 lr 0.00029706 rank 2
2023-02-27 13:58:38,779 DEBUG TRAIN Batch 44/3800 loss 10.340595 loss_att 10.319480 loss_ctc 13.212065 loss_rnnt 9.829730 hw_loss 0.247925 lr 0.00029706 rank 1
2023-02-27 14:00:01,328 DEBUG TRAIN Batch 44/3900 loss 5.670118 loss_att 8.477406 loss_ctc 8.387565 loss_rnnt 4.597422 hw_loss 0.279210 lr 0.00029701 rank 4
2023-02-27 14:00:01,332 DEBUG TRAIN Batch 44/3900 loss 14.091207 loss_att 17.322594 loss_ctc 21.989981 loss_rnnt 12.261522 hw_loss 0.244192 lr 0.00029701 rank 1
2023-02-27 14:00:01,333 DEBUG TRAIN Batch 44/3900 loss 9.177304 loss_att 10.827337 loss_ctc 10.677817 loss_rnnt 8.441472 hw_loss 0.385792 lr 0.00029701 rank 0
2023-02-27 14:00:01,334 DEBUG TRAIN Batch 44/3900 loss 3.482215 loss_att 5.590235 loss_ctc 3.938980 loss_rnnt 2.827989 hw_loss 0.321975 lr 0.00029701 rank 3
2023-02-27 14:00:01,335 DEBUG TRAIN Batch 44/3900 loss 5.198278 loss_att 8.163662 loss_ctc 9.067909 loss_rnnt 3.926966 hw_loss 0.304284 lr 0.00029701 rank 2
2023-02-27 14:01:53,022 DEBUG TRAIN Batch 44/4000 loss 7.259588 loss_att 9.791775 loss_ctc 11.913565 loss_rnnt 6.054587 hw_loss 0.146311 lr 0.00029696 rank 0
2023-02-27 14:01:53,023 DEBUG TRAIN Batch 44/4000 loss 6.271773 loss_att 9.107677 loss_ctc 9.260086 loss_rnnt 5.150329 hw_loss 0.292168 lr 0.00029696 rank 1
2023-02-27 14:01:53,024 DEBUG TRAIN Batch 44/4000 loss 4.294449 loss_att 8.287273 loss_ctc 8.111991 loss_rnnt 2.951492 hw_loss 0.066349 lr 0.00029696 rank 4
2023-02-27 14:01:53,025 DEBUG TRAIN Batch 44/4000 loss 4.542320 loss_att 8.565315 loss_ctc 12.597064 loss_rnnt 2.494826 hw_loss 0.316743 lr 0.00029696 rank 3
2023-02-27 14:01:53,028 DEBUG TRAIN Batch 44/4000 loss 1.472440 loss_att 5.452709 loss_ctc 1.995821 loss_rnnt 0.478314 hw_loss 0.240541 lr 0.00029696 rank 2
2023-02-27 14:03:12,010 DEBUG TRAIN Batch 44/4100 loss 9.357793 loss_att 11.969460 loss_ctc 10.592319 loss_rnnt 8.639277 hw_loss 0.059212 lr 0.00029691 rank 1
2023-02-27 14:03:12,010 DEBUG TRAIN Batch 44/4100 loss 11.794238 loss_att 12.689034 loss_ctc 13.524111 loss_rnnt 11.176961 hw_loss 0.389375 lr 0.00029691 rank 2
2023-02-27 14:03:12,014 DEBUG TRAIN Batch 44/4100 loss 5.629293 loss_att 8.140224 loss_ctc 8.938564 loss_rnnt 4.534080 hw_loss 0.284607 lr 0.00029691 rank 3
2023-02-27 14:03:12,019 DEBUG TRAIN Batch 44/4100 loss 8.616282 loss_att 12.414106 loss_ctc 16.028067 loss_rnnt 6.686852 hw_loss 0.340551 lr 0.00029691 rank 0
2023-02-27 14:03:12,145 DEBUG TRAIN Batch 44/4100 loss 11.374446 loss_att 15.339304 loss_ctc 15.353778 loss_rnnt 9.935490 hw_loss 0.216388 lr 0.00029691 rank 4
2023-02-27 14:04:33,636 DEBUG TRAIN Batch 44/4200 loss 7.292091 loss_att 10.970092 loss_ctc 12.187204 loss_rnnt 5.767732 hw_loss 0.255144 lr 0.00029685 rank 3
2023-02-27 14:04:33,638 DEBUG TRAIN Batch 44/4200 loss 6.453916 loss_att 8.726448 loss_ctc 8.556000 loss_rnnt 5.540941 hw_loss 0.334108 lr 0.00029685 rank 1
2023-02-27 14:04:33,640 DEBUG TRAIN Batch 44/4200 loss 7.515980 loss_att 10.979769 loss_ctc 12.785860 loss_rnnt 5.858830 hw_loss 0.490765 lr 0.00029685 rank 4
2023-02-27 14:04:33,641 DEBUG TRAIN Batch 44/4200 loss 12.090549 loss_att 14.857678 loss_ctc 19.250034 loss_rnnt 10.430199 hw_loss 0.285614 lr 0.00029685 rank 2
2023-02-27 14:04:33,648 DEBUG TRAIN Batch 44/4200 loss 14.191805 loss_att 18.887390 loss_ctc 18.569311 loss_rnnt 12.560082 hw_loss 0.204258 lr 0.00029685 rank 0
2023-02-27 14:06:28,894 DEBUG TRAIN Batch 44/4300 loss 5.594862 loss_att 6.139238 loss_ctc 9.604485 loss_rnnt 4.795996 hw_loss 0.291326 lr 0.00029680 rank 0
2023-02-27 14:06:28,896 DEBUG TRAIN Batch 44/4300 loss 10.148827 loss_att 14.434140 loss_ctc 20.694494 loss_rnnt 7.763304 hw_loss 0.229444 lr 0.00029680 rank 4
2023-02-27 14:06:28,899 DEBUG TRAIN Batch 44/4300 loss 4.965197 loss_att 8.500836 loss_ctc 8.346119 loss_rnnt 3.684697 hw_loss 0.229842 lr 0.00029680 rank 1
2023-02-27 14:06:28,900 DEBUG TRAIN Batch 44/4300 loss 9.854507 loss_att 12.985775 loss_ctc 15.888214 loss_rnnt 8.323224 hw_loss 0.188503 lr 0.00029680 rank 2
2023-02-27 14:06:28,955 DEBUG TRAIN Batch 44/4300 loss 6.386267 loss_att 10.281929 loss_ctc 13.010156 loss_rnnt 4.595983 hw_loss 0.239939 lr 0.00029680 rank 3
2023-02-27 14:07:48,190 DEBUG TRAIN Batch 44/4400 loss 6.594667 loss_att 8.972640 loss_ctc 9.524183 loss_rnnt 5.520451 hw_loss 0.390037 lr 0.00029675 rank 1
2023-02-27 14:07:48,191 DEBUG TRAIN Batch 44/4400 loss 6.419134 loss_att 9.334016 loss_ctc 8.617594 loss_rnnt 5.318876 hw_loss 0.420288 lr 0.00029675 rank 2
2023-02-27 14:07:48,192 DEBUG TRAIN Batch 44/4400 loss 7.664064 loss_att 11.646469 loss_ctc 11.205736 loss_rnnt 6.268876 hw_loss 0.237158 lr 0.00029675 rank 0
2023-02-27 14:07:48,196 DEBUG TRAIN Batch 44/4400 loss 4.990700 loss_att 5.972754 loss_ctc 7.625724 loss_rnnt 4.186882 hw_loss 0.480133 lr 0.00029675 rank 4
2023-02-27 14:07:48,239 DEBUG TRAIN Batch 44/4400 loss 8.435190 loss_att 10.013048 loss_ctc 14.916937 loss_rnnt 7.020023 hw_loss 0.441307 lr 0.00029675 rank 3
2023-02-27 14:09:06,529 DEBUG TRAIN Batch 44/4500 loss 3.637675 loss_att 6.900900 loss_ctc 6.048160 loss_rnnt 2.564010 hw_loss 0.186791 lr 0.00029670 rank 2
2023-02-27 14:09:06,529 DEBUG TRAIN Batch 44/4500 loss 7.466360 loss_att 10.686100 loss_ctc 10.511721 loss_rnnt 6.235467 hw_loss 0.339181 lr 0.00029670 rank 3
2023-02-27 14:09:06,532 DEBUG TRAIN Batch 44/4500 loss 7.503239 loss_att 11.487859 loss_ctc 15.723936 loss_rnnt 5.553523 hw_loss 0.106311 lr 0.00029670 rank 1
2023-02-27 14:09:06,532 DEBUG TRAIN Batch 44/4500 loss 2.269476 loss_att 6.109763 loss_ctc 4.077326 loss_rnnt 1.179733 hw_loss 0.151198 lr 0.00029670 rank 4
2023-02-27 14:09:06,533 DEBUG TRAIN Batch 44/4500 loss 7.414067 loss_att 10.994799 loss_ctc 12.034880 loss_rnnt 5.857250 hw_loss 0.421053 lr 0.00029670 rank 0
2023-02-27 14:10:28,427 DEBUG TRAIN Batch 44/4600 loss 7.791348 loss_att 7.575206 loss_ctc 9.925688 loss_rnnt 7.448501 hw_loss 0.190306 lr 0.00029664 rank 0
2023-02-27 14:10:28,430 DEBUG TRAIN Batch 44/4600 loss 4.174847 loss_att 8.467281 loss_ctc 12.567714 loss_rnnt 2.110244 hw_loss 0.163250 lr 0.00029664 rank 2
2023-02-27 14:10:28,434 DEBUG TRAIN Batch 44/4600 loss 10.712885 loss_att 12.910778 loss_ctc 12.963650 loss_rnnt 9.867926 hw_loss 0.197397 lr 0.00029664 rank 4
2023-02-27 14:10:28,436 DEBUG TRAIN Batch 44/4600 loss 11.200111 loss_att 15.587846 loss_ctc 20.511152 loss_rnnt 8.906593 hw_loss 0.327186 lr 0.00029664 rank 1
2023-02-27 14:10:28,439 DEBUG TRAIN Batch 44/4600 loss 11.274865 loss_att 12.775656 loss_ctc 14.052505 loss_rnnt 10.570561 hw_loss 0.063361 lr 0.00029664 rank 3
2023-02-27 14:12:24,660 DEBUG TRAIN Batch 44/4700 loss 5.736883 loss_att 9.001025 loss_ctc 10.848575 loss_rnnt 4.245973 hw_loss 0.293480 lr 0.00029659 rank 3
2023-02-27 14:12:24,667 DEBUG TRAIN Batch 44/4700 loss 7.409770 loss_att 9.724915 loss_ctc 15.922034 loss_rnnt 5.708685 hw_loss 0.193289 lr 0.00029659 rank 0
2023-02-27 14:12:24,667 DEBUG TRAIN Batch 44/4700 loss 4.378795 loss_att 7.259716 loss_ctc 9.086252 loss_rnnt 3.068513 hw_loss 0.199568 lr 0.00029659 rank 4
2023-02-27 14:12:24,671 DEBUG TRAIN Batch 44/4700 loss 4.994249 loss_att 8.675416 loss_ctc 11.437057 loss_rnnt 3.232722 hw_loss 0.311722 lr 0.00029659 rank 1
2023-02-27 14:12:24,671 DEBUG TRAIN Batch 44/4700 loss 6.429654 loss_att 9.221235 loss_ctc 11.406384 loss_rnnt 5.023109 hw_loss 0.346245 lr 0.00029659 rank 2
2023-02-27 14:13:44,798 DEBUG TRAIN Batch 44/4800 loss 10.202016 loss_att 11.638093 loss_ctc 14.205120 loss_rnnt 9.267624 hw_loss 0.212679 lr 0.00029654 rank 2
2023-02-27 14:13:44,798 DEBUG TRAIN Batch 44/4800 loss 6.421148 loss_att 10.539797 loss_ctc 15.065258 loss_rnnt 4.356550 hw_loss 0.165600 lr 0.00029654 rank 1
2023-02-27 14:13:44,801 DEBUG TRAIN Batch 44/4800 loss 8.184218 loss_att 10.661673 loss_ctc 12.873211 loss_rnnt 6.999276 hw_loss 0.120473 lr 0.00029654 rank 4
2023-02-27 14:13:44,802 DEBUG TRAIN Batch 44/4800 loss 4.949450 loss_att 7.831255 loss_ctc 8.258297 loss_rnnt 3.748343 hw_loss 0.344187 lr 0.00029654 rank 0
2023-02-27 14:13:44,803 DEBUG TRAIN Batch 44/4800 loss 5.297723 loss_att 8.143742 loss_ctc 8.395983 loss_rnnt 4.164922 hw_loss 0.282180 lr 0.00029654 rank 3
2023-02-27 14:15:07,074 DEBUG TRAIN Batch 44/4900 loss 9.441044 loss_att 12.625731 loss_ctc 18.961807 loss_rnnt 7.330222 hw_loss 0.383344 lr 0.00029649 rank 4
2023-02-27 14:15:07,080 DEBUG TRAIN Batch 44/4900 loss 5.328136 loss_att 9.585988 loss_ctc 6.895073 loss_rnnt 4.182608 hw_loss 0.159436 lr 0.00029649 rank 2
2023-02-27 14:15:07,088 DEBUG TRAIN Batch 44/4900 loss 12.513450 loss_att 14.518439 loss_ctc 22.173031 loss_rnnt 10.607342 hw_loss 0.407186 lr 0.00029649 rank 1
2023-02-27 14:15:07,106 DEBUG TRAIN Batch 44/4900 loss 6.449234 loss_att 7.658519 loss_ctc 10.955006 loss_rnnt 5.439812 hw_loss 0.312742 lr 0.00029649 rank 0
2023-02-27 14:15:07,141 DEBUG TRAIN Batch 44/4900 loss 3.713502 loss_att 5.688604 loss_ctc 6.637753 loss_rnnt 2.769361 hw_loss 0.298537 lr 0.00029649 rank 3
2023-02-27 14:17:04,291 DEBUG TRAIN Batch 44/5000 loss 8.789920 loss_att 10.735937 loss_ctc 13.453879 loss_rnnt 7.578934 hw_loss 0.374854 lr 0.00029644 rank 2
2023-02-27 14:17:04,293 DEBUG TRAIN Batch 44/5000 loss 15.231061 loss_att 16.685633 loss_ctc 21.958769 loss_rnnt 13.939697 hw_loss 0.193916 lr 0.00029644 rank 3
2023-02-27 14:17:04,296 DEBUG TRAIN Batch 44/5000 loss 7.109826 loss_att 8.271067 loss_ctc 11.029544 loss_rnnt 6.241431 hw_loss 0.212845 lr 0.00029644 rank 0
2023-02-27 14:17:04,298 DEBUG TRAIN Batch 44/5000 loss 5.096165 loss_att 8.891492 loss_ctc 9.568743 loss_rnnt 3.660086 hw_loss 0.151256 lr 0.00029644 rank 4
2023-02-27 14:17:04,304 DEBUG TRAIN Batch 44/5000 loss 3.178139 loss_att 5.706397 loss_ctc 8.224486 loss_rnnt 1.873389 hw_loss 0.236721 lr 0.00029644 rank 1
2023-02-27 14:18:23,361 DEBUG TRAIN Batch 44/5100 loss 5.387491 loss_att 9.004240 loss_ctc 6.991546 loss_rnnt 4.326931 hw_loss 0.231253 lr 0.00029638 rank 2
2023-02-27 14:18:23,364 DEBUG TRAIN Batch 44/5100 loss 6.306453 loss_att 6.632385 loss_ctc 10.561512 loss_rnnt 5.425850 hw_loss 0.465142 lr 0.00029638 rank 3
2023-02-27 14:18:23,365 DEBUG TRAIN Batch 44/5100 loss 6.875981 loss_att 10.070704 loss_ctc 11.595499 loss_rnnt 5.519474 hw_loss 0.165551 lr 0.00029638 rank 4
2023-02-27 14:18:23,368 DEBUG TRAIN Batch 44/5100 loss 5.110332 loss_att 6.837032 loss_ctc 8.395039 loss_rnnt 4.242353 hw_loss 0.158772 lr 0.00029638 rank 0
2023-02-27 14:18:23,370 DEBUG TRAIN Batch 44/5100 loss 2.986495 loss_att 5.539410 loss_ctc 5.314163 loss_rnnt 2.091568 hw_loss 0.138728 lr 0.00029638 rank 1
2023-02-27 14:19:41,960 DEBUG TRAIN Batch 44/5200 loss 5.020452 loss_att 11.302565 loss_ctc 11.570707 loss_rnnt 2.744466 hw_loss 0.274120 lr 0.00029633 rank 0
2023-02-27 14:19:41,965 DEBUG TRAIN Batch 44/5200 loss 6.242958 loss_att 7.896344 loss_ctc 10.257772 loss_rnnt 5.157046 hw_loss 0.412360 lr 0.00029633 rank 3
2023-02-27 14:19:41,965 DEBUG TRAIN Batch 44/5200 loss 10.527791 loss_att 9.745371 loss_ctc 14.089366 loss_rnnt 10.143291 hw_loss 0.123952 lr 0.00029633 rank 1
2023-02-27 14:19:41,965 DEBUG TRAIN Batch 44/5200 loss 5.992902 loss_att 10.869881 loss_ctc 13.809122 loss_rnnt 3.943504 hw_loss 0.059700 lr 0.00029633 rank 4
2023-02-27 14:19:41,971 DEBUG TRAIN Batch 44/5200 loss 11.132829 loss_att 13.893394 loss_ctc 16.936459 loss_rnnt 9.653053 hw_loss 0.288461 lr 0.00029633 rank 2
2023-02-27 14:21:04,978 DEBUG TRAIN Batch 44/5300 loss 12.397800 loss_att 16.101667 loss_ctc 20.452301 loss_rnnt 10.475490 hw_loss 0.201760 lr 0.00029628 rank 0
2023-02-27 14:21:04,981 DEBUG TRAIN Batch 44/5300 loss 8.193746 loss_att 11.929504 loss_ctc 15.054332 loss_rnnt 6.427564 hw_loss 0.195535 lr 0.00029628 rank 3
2023-02-27 14:21:04,989 DEBUG TRAIN Batch 44/5300 loss 4.065626 loss_att 6.087664 loss_ctc 7.541595 loss_rnnt 3.083416 hw_loss 0.214387 lr 0.00029628 rank 4
2023-02-27 14:21:04,993 DEBUG TRAIN Batch 44/5300 loss 8.125956 loss_att 12.489799 loss_ctc 15.706096 loss_rnnt 6.098000 hw_loss 0.270942 lr 0.00029628 rank 1
2023-02-27 14:21:05,018 DEBUG TRAIN Batch 44/5300 loss 5.520824 loss_att 6.503306 loss_ctc 5.537828 loss_rnnt 5.049985 hw_loss 0.510142 lr 0.00029628 rank 2
2023-02-27 14:22:56,635 DEBUG TRAIN Batch 44/5400 loss 4.446921 loss_att 6.598200 loss_ctc 6.576617 loss_rnnt 3.558730 hw_loss 0.326206 lr 0.00029623 rank 4
2023-02-27 14:22:56,636 DEBUG TRAIN Batch 44/5400 loss 4.924538 loss_att 8.790447 loss_ctc 11.536522 loss_rnnt 3.135195 hw_loss 0.252304 lr 0.00029623 rank 2
2023-02-27 14:22:56,637 DEBUG TRAIN Batch 44/5400 loss 10.381661 loss_att 14.078207 loss_ctc 24.599728 loss_rnnt 7.627574 hw_loss 0.223197 lr 0.00029623 rank 0
2023-02-27 14:22:56,638 DEBUG TRAIN Batch 44/5400 loss 10.379143 loss_att 15.354282 loss_ctc 18.212408 loss_rnnt 8.222365 hw_loss 0.219964 lr 0.00029623 rank 1
2023-02-27 14:22:56,657 DEBUG TRAIN Batch 44/5400 loss 10.737153 loss_att 14.620844 loss_ctc 15.857682 loss_rnnt 9.095565 hw_loss 0.341460 lr 0.00029623 rank 3
2023-02-27 14:24:13,716 DEBUG TRAIN Batch 44/5500 loss 4.544333 loss_att 6.757527 loss_ctc 9.933891 loss_rnnt 3.190160 hw_loss 0.361739 lr 0.00029617 rank 2
2023-02-27 14:24:13,716 DEBUG TRAIN Batch 44/5500 loss 7.655540 loss_att 10.488979 loss_ctc 10.588693 loss_rnnt 6.598051 hw_loss 0.186961 lr 0.00029617 rank 1
2023-02-27 14:24:13,717 DEBUG TRAIN Batch 44/5500 loss 8.375654 loss_att 12.819707 loss_ctc 18.212582 loss_rnnt 6.011836 hw_loss 0.306409 lr 0.00029617 rank 4
2023-02-27 14:24:13,723 DEBUG TRAIN Batch 44/5500 loss 6.364937 loss_att 7.281040 loss_ctc 8.200190 loss_rnnt 5.836422 hw_loss 0.188615 lr 0.00029617 rank 3
2023-02-27 14:24:13,730 DEBUG TRAIN Batch 44/5500 loss 2.373229 loss_att 5.503143 loss_ctc 4.489950 loss_rnnt 1.227693 hw_loss 0.444982 lr 0.00029617 rank 0
2023-02-27 14:25:33,538 DEBUG TRAIN Batch 44/5600 loss 11.076463 loss_att 14.765585 loss_ctc 17.844164 loss_rnnt 9.226454 hw_loss 0.393419 lr 0.00029612 rank 3
2023-02-27 14:25:33,540 DEBUG TRAIN Batch 44/5600 loss 8.903956 loss_att 12.268072 loss_ctc 15.385704 loss_rnnt 7.161861 hw_loss 0.384449 lr 0.00029612 rank 1
2023-02-27 14:25:33,544 DEBUG TRAIN Batch 44/5600 loss 2.449767 loss_att 4.255471 loss_ctc 5.250477 loss_rnnt 1.541913 hw_loss 0.324908 lr 0.00029612 rank 2
2023-02-27 14:25:33,544 DEBUG TRAIN Batch 44/5600 loss 6.810491 loss_att 8.926044 loss_ctc 12.988489 loss_rnnt 5.445635 hw_loss 0.221273 lr 0.00029612 rank 0
2023-02-27 14:25:33,549 DEBUG TRAIN Batch 44/5600 loss 7.974345 loss_att 9.824173 loss_ctc 14.386563 loss_rnnt 6.599579 hw_loss 0.280946 lr 0.00029612 rank 4
2023-02-27 14:27:27,652 DEBUG TRAIN Batch 44/5700 loss 8.138581 loss_att 8.333939 loss_ctc 10.572430 loss_rnnt 7.555537 hw_loss 0.411487 lr 0.00029607 rank 2
2023-02-27 14:27:27,654 DEBUG TRAIN Batch 44/5700 loss 6.322532 loss_att 8.550692 loss_ctc 11.234987 loss_rnnt 5.056667 hw_loss 0.309822 lr 0.00029607 rank 3
2023-02-27 14:27:27,662 DEBUG TRAIN Batch 44/5700 loss 9.712043 loss_att 8.967884 loss_ctc 11.487107 loss_rnnt 9.378211 hw_loss 0.461229 lr 0.00029607 rank 4
2023-02-27 14:27:27,662 DEBUG TRAIN Batch 44/5700 loss 11.149855 loss_att 11.526152 loss_ctc 16.595207 loss_rnnt 10.126143 hw_loss 0.417009 lr 0.00029607 rank 1
2023-02-27 14:27:27,721 DEBUG TRAIN Batch 44/5700 loss 2.968300 loss_att 5.917533 loss_ctc 3.522274 loss_rnnt 2.219198 hw_loss 0.160111 lr 0.00029607 rank 0
2023-02-27 14:28:50,176 DEBUG TRAIN Batch 44/5800 loss 3.243394 loss_att 5.526250 loss_ctc 4.726933 loss_rnnt 2.508745 hw_loss 0.150511 lr 0.00029602 rank 0
2023-02-27 14:28:50,177 DEBUG TRAIN Batch 44/5800 loss 6.211418 loss_att 9.667736 loss_ctc 9.758526 loss_rnnt 4.982789 hw_loss 0.120782 lr 0.00029602 rank 2
2023-02-27 14:28:50,179 DEBUG TRAIN Batch 44/5800 loss 6.273791 loss_att 8.578401 loss_ctc 6.953508 loss_rnnt 5.533821 hw_loss 0.353286 lr 0.00029602 rank 1
2023-02-27 14:28:50,180 DEBUG TRAIN Batch 44/5800 loss 7.171966 loss_att 13.034131 loss_ctc 14.215500 loss_rnnt 4.796654 hw_loss 0.494516 lr 0.00029602 rank 3
2023-02-27 14:28:50,181 DEBUG TRAIN Batch 44/5800 loss 5.561721 loss_att 8.063716 loss_ctc 7.187159 loss_rnnt 4.730398 hw_loss 0.214125 lr 0.00029602 rank 4
2023-02-27 14:30:10,933 DEBUG TRAIN Batch 44/5900 loss 8.061231 loss_att 8.442717 loss_ctc 11.154802 loss_rnnt 7.494022 hw_loss 0.147064 lr 0.00029597 rank 0
2023-02-27 14:30:10,936 DEBUG TRAIN Batch 44/5900 loss 8.775638 loss_att 12.455383 loss_ctc 19.176321 loss_rnnt 6.386592 hw_loss 0.499383 lr 0.00029597 rank 2
2023-02-27 14:30:10,936 DEBUG TRAIN Batch 44/5900 loss 7.625735 loss_att 11.248487 loss_ctc 14.398429 loss_rnnt 5.908834 hw_loss 0.167483 lr 0.00029597 rank 4
2023-02-27 14:30:10,942 DEBUG TRAIN Batch 44/5900 loss 13.107603 loss_att 19.216854 loss_ctc 26.671150 loss_rnnt 9.956879 hw_loss 0.225751 lr 0.00029597 rank 1
2023-02-27 14:30:10,990 DEBUG TRAIN Batch 44/5900 loss 7.698398 loss_att 10.350778 loss_ctc 12.391760 loss_rnnt 6.429050 hw_loss 0.212045 lr 0.00029597 rank 3
2023-02-27 14:31:34,205 DEBUG TRAIN Batch 44/6000 loss 4.669736 loss_att 5.830086 loss_ctc 5.498042 loss_rnnt 4.058611 hw_loss 0.503653 lr 0.00029592 rank 3
2023-02-27 14:31:34,211 DEBUG TRAIN Batch 44/6000 loss 7.774017 loss_att 9.671141 loss_ctc 13.123268 loss_rnnt 6.508732 hw_loss 0.323675 lr 0.00029592 rank 4
2023-02-27 14:31:34,214 DEBUG TRAIN Batch 44/6000 loss 4.978040 loss_att 7.260253 loss_ctc 6.822530 loss_rnnt 4.163754 hw_loss 0.209833 lr 0.00029592 rank 0
2023-02-27 14:31:34,219 DEBUG TRAIN Batch 44/6000 loss 6.883281 loss_att 9.945182 loss_ctc 12.213984 loss_rnnt 5.454100 hw_loss 0.198825 lr 0.00029592 rank 1
2023-02-27 14:31:34,313 DEBUG TRAIN Batch 44/6000 loss 9.262178 loss_att 14.809896 loss_ctc 17.942379 loss_rnnt 6.891035 hw_loss 0.195449 lr 0.00029592 rank 2
2023-02-27 14:33:28,270 DEBUG TRAIN Batch 44/6100 loss 14.464190 loss_att 19.556154 loss_ctc 26.084003 loss_rnnt 11.779455 hw_loss 0.219440 lr 0.00029586 rank 3
2023-02-27 14:33:28,271 DEBUG TRAIN Batch 44/6100 loss 2.833969 loss_att 6.080452 loss_ctc 4.968933 loss_rnnt 1.739635 hw_loss 0.300704 lr 0.00029586 rank 4
2023-02-27 14:33:28,272 DEBUG TRAIN Batch 44/6100 loss 4.230671 loss_att 8.905245 loss_ctc 10.501918 loss_rnnt 2.338144 hw_loss 0.227713 lr 0.00029586 rank 2
2023-02-27 14:33:28,274 DEBUG TRAIN Batch 44/6100 loss 2.906371 loss_att 5.879505 loss_ctc 4.526033 loss_rnnt 2.038133 hw_loss 0.108106 lr 0.00029586 rank 0
2023-02-27 14:33:28,277 DEBUG TRAIN Batch 44/6100 loss 8.026031 loss_att 9.567772 loss_ctc 9.590504 loss_rnnt 7.284244 hw_loss 0.421581 lr 0.00029586 rank 1
2023-02-27 14:34:49,783 DEBUG TRAIN Batch 44/6200 loss 7.218756 loss_att 9.949597 loss_ctc 11.660094 loss_rnnt 5.928520 hw_loss 0.284792 lr 0.00029581 rank 0
2023-02-27 14:34:49,790 DEBUG TRAIN Batch 44/6200 loss 7.790563 loss_att 8.330887 loss_ctc 18.378113 loss_rnnt 6.090065 hw_loss 0.338924 lr 0.00029581 rank 1
2023-02-27 14:34:49,792 DEBUG TRAIN Batch 44/6200 loss 10.758930 loss_att 14.880722 loss_ctc 15.196335 loss_rnnt 9.197817 hw_loss 0.272063 lr 0.00029581 rank 4
2023-02-27 14:34:49,810 DEBUG TRAIN Batch 44/6200 loss 9.478867 loss_att 10.908171 loss_ctc 13.507317 loss_rnnt 8.543602 hw_loss 0.210519 lr 0.00029581 rank 3
2023-02-27 14:34:49,841 DEBUG TRAIN Batch 44/6200 loss 5.483337 loss_att 7.783896 loss_ctc 10.087727 loss_rnnt 4.267071 hw_loss 0.266692 lr 0.00029581 rank 2
2023-02-27 14:36:12,205 DEBUG TRAIN Batch 44/6300 loss 8.090273 loss_att 9.863501 loss_ctc 12.053017 loss_rnnt 7.070646 hw_loss 0.256155 lr 0.00029576 rank 4
2023-02-27 14:36:12,206 DEBUG TRAIN Batch 44/6300 loss 8.000509 loss_att 9.152741 loss_ctc 9.444321 loss_rnnt 7.411624 hw_loss 0.311119 lr 0.00029576 rank 0
2023-02-27 14:36:12,208 DEBUG TRAIN Batch 44/6300 loss 8.772149 loss_att 8.884683 loss_ctc 13.086406 loss_rnnt 7.979271 hw_loss 0.365881 lr 0.00029576 rank 2
2023-02-27 14:36:12,208 DEBUG TRAIN Batch 44/6300 loss 10.655747 loss_att 14.221685 loss_ctc 18.862083 loss_rnnt 8.735404 hw_loss 0.211832 lr 0.00029576 rank 3
2023-02-27 14:36:12,267 DEBUG TRAIN Batch 44/6300 loss 6.036472 loss_att 8.464993 loss_ctc 8.909134 loss_rnnt 5.005744 hw_loss 0.303753 lr 0.00029576 rank 1
2023-02-27 14:37:37,114 DEBUG TRAIN Batch 44/6400 loss 7.472674 loss_att 9.141479 loss_ctc 12.438295 loss_rnnt 6.348963 hw_loss 0.239751 lr 0.00029571 rank 4
2023-02-27 14:37:37,116 DEBUG TRAIN Batch 44/6400 loss 6.690790 loss_att 10.181690 loss_ctc 10.489634 loss_rnnt 5.397273 hw_loss 0.166546 lr 0.00029571 rank 3
2023-02-27 14:37:37,121 DEBUG TRAIN Batch 44/6400 loss 11.485464 loss_att 12.444632 loss_ctc 14.310107 loss_rnnt 10.817657 hw_loss 0.186288 lr 0.00029571 rank 0
2023-02-27 14:37:37,183 DEBUG TRAIN Batch 44/6400 loss 10.017762 loss_att 11.899994 loss_ctc 11.915104 loss_rnnt 9.323578 hw_loss 0.121425 lr 0.00029571 rank 1
2023-02-27 14:37:37,183 DEBUG TRAIN Batch 44/6400 loss 18.344887 loss_att 17.335415 loss_ctc 32.493382 loss_rnnt 16.589432 hw_loss 0.132906 lr 0.00029571 rank 2
2023-02-27 14:39:35,409 DEBUG TRAIN Batch 44/6500 loss 5.634770 loss_att 9.028039 loss_ctc 15.487963 loss_rnnt 3.448716 hw_loss 0.363077 lr 0.00029566 rank 2
2023-02-27 14:39:35,410 DEBUG TRAIN Batch 44/6500 loss 8.994002 loss_att 11.365129 loss_ctc 11.595275 loss_rnnt 7.975223 hw_loss 0.370721 lr 0.00029566 rank 3
2023-02-27 14:39:35,414 DEBUG TRAIN Batch 44/6500 loss 11.970373 loss_att 17.628769 loss_ctc 22.493343 loss_rnnt 9.403706 hw_loss 0.059860 lr 0.00029566 rank 4
2023-02-27 14:39:35,418 DEBUG TRAIN Batch 44/6500 loss 12.094121 loss_att 16.149532 loss_ctc 20.245695 loss_rnnt 10.112082 hw_loss 0.157648 lr 0.00029566 rank 0
2023-02-27 14:39:35,474 DEBUG TRAIN Batch 44/6500 loss 10.407051 loss_att 12.729288 loss_ctc 19.572227 loss_rnnt 8.511770 hw_loss 0.391519 lr 0.00029566 rank 1
2023-02-27 14:40:59,110 DEBUG TRAIN Batch 44/6600 loss 7.575307 loss_att 13.235462 loss_ctc 12.056074 loss_rnnt 5.733766 hw_loss 0.210141 lr 0.00029561 rank 3
2023-02-27 14:40:59,118 DEBUG TRAIN Batch 44/6600 loss 4.880561 loss_att 7.761518 loss_ctc 7.853430 loss_rnnt 3.794849 hw_loss 0.212133 lr 0.00029561 rank 4
2023-02-27 14:40:59,119 DEBUG TRAIN Batch 44/6600 loss 4.533563 loss_att 7.051141 loss_ctc 8.591708 loss_rnnt 3.247146 hw_loss 0.453405 lr 0.00029561 rank 1
2023-02-27 14:40:59,120 DEBUG TRAIN Batch 44/6600 loss 12.130341 loss_att 14.662006 loss_ctc 17.565945 loss_rnnt 10.773657 hw_loss 0.235508 lr 0.00029561 rank 0
2023-02-27 14:40:59,173 DEBUG TRAIN Batch 44/6600 loss 9.899376 loss_att 11.212580 loss_ctc 15.180968 loss_rnnt 8.672457 hw_loss 0.487625 lr 0.00029561 rank 2
2023-02-27 14:42:22,005 DEBUG TRAIN Batch 44/6700 loss 7.313727 loss_att 11.914355 loss_ctc 16.180845 loss_rnnt 4.988062 hw_loss 0.418607 lr 0.00029555 rank 4
2023-02-27 14:42:22,014 DEBUG TRAIN Batch 44/6700 loss 5.548114 loss_att 10.738136 loss_ctc 10.055743 loss_rnnt 3.781019 hw_loss 0.240135 lr 0.00029555 rank 0
2023-02-27 14:42:22,016 DEBUG TRAIN Batch 44/6700 loss 8.245812 loss_att 12.194514 loss_ctc 16.929977 loss_rnnt 6.175474 hw_loss 0.230081 lr 0.00029555 rank 2
2023-02-27 14:42:22,017 DEBUG TRAIN Batch 44/6700 loss 11.004027 loss_att 14.411909 loss_ctc 19.068365 loss_rnnt 9.172336 hw_loss 0.140382 lr 0.00029555 rank 3
2023-02-27 14:42:22,064 DEBUG TRAIN Batch 44/6700 loss 17.626139 loss_att 19.323631 loss_ctc 22.556757 loss_rnnt 16.463974 hw_loss 0.309846 lr 0.00029555 rank 1
2023-02-27 14:44:20,967 DEBUG TRAIN Batch 44/6800 loss 7.001687 loss_att 8.052885 loss_ctc 10.569028 loss_rnnt 6.147151 hw_loss 0.316220 lr 0.00029550 rank 4
2023-02-27 14:44:20,968 DEBUG TRAIN Batch 44/6800 loss 3.313025 loss_att 6.747046 loss_ctc 6.680760 loss_rnnt 1.960826 hw_loss 0.405682 lr 0.00029550 rank 2
2023-02-27 14:44:20,968 DEBUG TRAIN Batch 44/6800 loss 13.141088 loss_att 19.777813 loss_ctc 18.944239 loss_rnnt 10.911196 hw_loss 0.241490 lr 0.00029550 rank 3
2023-02-27 14:44:20,970 DEBUG TRAIN Batch 44/6800 loss 10.322143 loss_att 11.071931 loss_ctc 15.149065 loss_rnnt 9.329037 hw_loss 0.374173 lr 0.00029550 rank 0
2023-02-27 14:44:20,972 DEBUG TRAIN Batch 44/6800 loss 8.751908 loss_att 10.714672 loss_ctc 14.669725 loss_rnnt 7.412329 hw_loss 0.296220 lr 0.00029550 rank 1
2023-02-27 14:45:42,737 DEBUG TRAIN Batch 44/6900 loss 5.255197 loss_att 7.174522 loss_ctc 7.908922 loss_rnnt 4.345588 hw_loss 0.322338 lr 0.00029545 rank 3
2023-02-27 14:45:42,738 DEBUG TRAIN Batch 44/6900 loss 4.103055 loss_att 6.078627 loss_ctc 6.951965 loss_rnnt 3.170480 hw_loss 0.295511 lr 0.00029545 rank 0
2023-02-27 14:45:42,740 DEBUG TRAIN Batch 44/6900 loss 4.305298 loss_att 7.473780 loss_ctc 8.532975 loss_rnnt 2.986896 hw_loss 0.226905 lr 0.00029545 rank 2
2023-02-27 14:45:42,744 DEBUG TRAIN Batch 44/6900 loss 7.560593 loss_att 10.104548 loss_ctc 10.257977 loss_rnnt 6.456172 hw_loss 0.442459 lr 0.00029545 rank 4
2023-02-27 14:45:42,746 DEBUG TRAIN Batch 44/6900 loss 10.442622 loss_att 11.121819 loss_ctc 14.900367 loss_rnnt 9.596377 hw_loss 0.217573 lr 0.00029545 rank 1
2023-02-27 14:47:03,089 DEBUG TRAIN Batch 44/7000 loss 7.451332 loss_att 7.383907 loss_ctc 12.937157 loss_rnnt 6.508760 hw_loss 0.421150 lr 0.00029540 rank 3
2023-02-27 14:47:03,093 DEBUG TRAIN Batch 44/7000 loss 8.075722 loss_att 11.267075 loss_ctc 13.638656 loss_rnnt 6.642091 hw_loss 0.100564 lr 0.00029540 rank 2
2023-02-27 14:47:03,092 DEBUG TRAIN Batch 44/7000 loss 8.570417 loss_att 8.201837 loss_ctc 10.295833 loss_rnnt 8.062071 hw_loss 0.660015 lr 0.00029540 rank 1
2023-02-27 14:47:03,094 DEBUG TRAIN Batch 44/7000 loss 9.790697 loss_att 11.613844 loss_ctc 15.606863 loss_rnnt 8.409873 hw_loss 0.451325 lr 0.00029540 rank 0
2023-02-27 14:47:03,096 DEBUG TRAIN Batch 44/7000 loss 12.599154 loss_att 17.935658 loss_ctc 19.697031 loss_rnnt 10.494239 hw_loss 0.171060 lr 0.00029540 rank 4
2023-02-27 14:48:27,206 DEBUG TRAIN Batch 44/7100 loss 2.129987 loss_att 4.438390 loss_ctc 6.297960 loss_rnnt 0.927890 hw_loss 0.346287 lr 0.00029535 rank 3
2023-02-27 14:48:27,214 DEBUG TRAIN Batch 44/7100 loss 5.107687 loss_att 10.316729 loss_ctc 9.390566 loss_rnnt 3.415375 hw_loss 0.148973 lr 0.00029535 rank 4
2023-02-27 14:48:27,214 DEBUG TRAIN Batch 44/7100 loss 8.377630 loss_att 11.156502 loss_ctc 12.569351 loss_rnnt 7.131084 hw_loss 0.247266 lr 0.00029535 rank 1
2023-02-27 14:48:27,228 DEBUG TRAIN Batch 44/7100 loss 12.443674 loss_att 14.352248 loss_ctc 16.618656 loss_rnnt 11.375692 hw_loss 0.243005 lr 0.00029535 rank 2
2023-02-27 14:48:27,270 DEBUG TRAIN Batch 44/7100 loss 4.644810 loss_att 7.639678 loss_ctc 7.786218 loss_rnnt 3.511705 hw_loss 0.216145 lr 0.00029535 rank 0
2023-02-27 14:50:23,511 DEBUG TRAIN Batch 44/7200 loss 5.259800 loss_att 8.496888 loss_ctc 5.857928 loss_rnnt 4.439407 hw_loss 0.174798 lr 0.00029530 rank 3
2023-02-27 14:50:23,512 DEBUG TRAIN Batch 44/7200 loss 3.630928 loss_att 6.247319 loss_ctc 5.985471 loss_rnnt 2.587473 hw_loss 0.386693 lr 0.00029530 rank 2
2023-02-27 14:50:23,520 DEBUG TRAIN Batch 44/7200 loss 8.282056 loss_att 13.770373 loss_ctc 19.885532 loss_rnnt 5.536820 hw_loss 0.188330 lr 0.00029530 rank 4
2023-02-27 14:50:23,526 DEBUG TRAIN Batch 44/7200 loss 3.159545 loss_att 4.801262 loss_ctc 6.630379 loss_rnnt 2.169385 hw_loss 0.373196 lr 0.00029530 rank 0
2023-02-27 14:50:23,527 DEBUG TRAIN Batch 44/7200 loss 4.429446 loss_att 9.209054 loss_ctc 8.247852 loss_rnnt 2.795869 hw_loss 0.316002 lr 0.00029530 rank 1
2023-02-27 14:51:48,314 DEBUG TRAIN Batch 44/7300 loss 8.868155 loss_att 12.602530 loss_ctc 15.977256 loss_rnnt 6.933249 hw_loss 0.450282 lr 0.00029524 rank 0
2023-02-27 14:51:48,315 DEBUG TRAIN Batch 44/7300 loss 13.391422 loss_att 15.012056 loss_ctc 28.624460 loss_rnnt 10.873602 hw_loss 0.304919 lr 0.00029524 rank 1
2023-02-27 14:51:48,315 DEBUG TRAIN Batch 44/7300 loss 9.000036 loss_att 12.839476 loss_ctc 14.021268 loss_rnnt 7.403635 hw_loss 0.298155 lr 0.00029524 rank 3
2023-02-27 14:51:48,316 DEBUG TRAIN Batch 44/7300 loss 7.343469 loss_att 9.940098 loss_ctc 11.605562 loss_rnnt 6.011550 hw_loss 0.458090 lr 0.00029524 rank 4
2023-02-27 14:51:48,319 DEBUG TRAIN Batch 44/7300 loss 7.227073 loss_att 9.114432 loss_ctc 10.025127 loss_rnnt 6.369281 hw_loss 0.201087 lr 0.00029524 rank 2
2023-02-27 14:53:15,603 DEBUG TRAIN Batch 44/7400 loss 4.832643 loss_att 8.761947 loss_ctc 7.843174 loss_rnnt 3.522982 hw_loss 0.229492 lr 0.00029519 rank 0
2023-02-27 14:53:15,611 DEBUG TRAIN Batch 44/7400 loss 7.016678 loss_att 8.843623 loss_ctc 13.330834 loss_rnnt 5.685045 hw_loss 0.233169 lr 0.00029519 rank 1
2023-02-27 14:53:15,653 DEBUG TRAIN Batch 44/7400 loss 12.700414 loss_att 15.331528 loss_ctc 20.906242 loss_rnnt 10.984580 hw_loss 0.179061 lr 0.00029519 rank 2
2023-02-27 14:53:15,740 DEBUG TRAIN Batch 44/7400 loss 8.527999 loss_att 11.896159 loss_ctc 14.354047 loss_rnnt 6.940033 hw_loss 0.257861 lr 0.00029519 rank 4
2023-02-27 14:53:15,744 DEBUG TRAIN Batch 44/7400 loss 6.149607 loss_att 11.620713 loss_ctc 9.278860 loss_rnnt 4.570290 hw_loss 0.127242 lr 0.00029519 rank 3
2023-02-27 14:55:10,961 DEBUG TRAIN Batch 44/7500 loss 12.917704 loss_att 14.837410 loss_ctc 19.695267 loss_rnnt 11.532843 hw_loss 0.182335 lr 0.00029514 rank 3
2023-02-27 14:55:10,965 DEBUG TRAIN Batch 44/7500 loss 4.822162 loss_att 6.004670 loss_ctc 5.398078 loss_rnnt 4.382236 hw_loss 0.237440 lr 0.00029514 rank 2
2023-02-27 14:55:10,967 DEBUG TRAIN Batch 44/7500 loss 2.889653 loss_att 4.872533 loss_ctc 3.721238 loss_rnnt 2.247651 hw_loss 0.252279 lr 0.00029514 rank 1
2023-02-27 14:55:10,968 DEBUG TRAIN Batch 44/7500 loss 10.255837 loss_att 12.033956 loss_ctc 15.208527 loss_rnnt 9.085834 hw_loss 0.288789 lr 0.00029514 rank 0
2023-02-27 14:55:10,977 DEBUG TRAIN Batch 44/7500 loss 6.054733 loss_att 8.292580 loss_ctc 11.127917 loss_rnnt 4.807567 hw_loss 0.230948 lr 0.00029514 rank 4
2023-02-27 14:56:33,502 DEBUG TRAIN Batch 44/7600 loss 9.571664 loss_att 11.581643 loss_ctc 16.713373 loss_rnnt 8.165028 hw_loss 0.098273 lr 0.00029509 rank 3
2023-02-27 14:56:33,507 DEBUG TRAIN Batch 44/7600 loss 9.873944 loss_att 11.633560 loss_ctc 17.212318 loss_rnnt 8.332890 hw_loss 0.395028 lr 0.00029509 rank 2
2023-02-27 14:56:33,512 DEBUG TRAIN Batch 44/7600 loss 5.974768 loss_att 7.921423 loss_ctc 9.148407 loss_rnnt 5.029052 hw_loss 0.249813 lr 0.00029509 rank 1
2023-02-27 14:56:33,515 DEBUG TRAIN Batch 44/7600 loss 12.467169 loss_att 11.807541 loss_ctc 15.489451 loss_rnnt 11.980886 hw_loss 0.403568 lr 0.00029509 rank 0
2023-02-27 14:56:33,517 DEBUG TRAIN Batch 44/7600 loss 3.899245 loss_att 6.409502 loss_ctc 7.669567 loss_rnnt 2.789654 hw_loss 0.196556 lr 0.00029509 rank 4
2023-02-27 14:57:54,744 DEBUG TRAIN Batch 44/7700 loss 4.599749 loss_att 8.420776 loss_ctc 9.948239 loss_rnnt 2.949180 hw_loss 0.324807 lr 0.00029504 rank 0
2023-02-27 14:57:54,746 DEBUG TRAIN Batch 44/7700 loss 9.786140 loss_att 12.640763 loss_ctc 16.063917 loss_rnnt 8.256672 hw_loss 0.227825 lr 0.00029504 rank 1
2023-02-27 14:57:54,747 DEBUG TRAIN Batch 44/7700 loss 10.792849 loss_att 13.225413 loss_ctc 17.573944 loss_rnnt 9.251925 hw_loss 0.281747 lr 0.00029504 rank 3
2023-02-27 14:57:54,748 DEBUG TRAIN Batch 44/7700 loss 3.491655 loss_att 7.421227 loss_ctc 6.034964 loss_rnnt 2.208469 hw_loss 0.296557 lr 0.00029504 rank 4
2023-02-27 14:57:54,749 DEBUG TRAIN Batch 44/7700 loss 2.261095 loss_att 4.121546 loss_ctc 4.987281 loss_rnnt 1.391464 hw_loss 0.251342 lr 0.00029504 rank 2
2023-02-27 14:59:19,233 DEBUG TRAIN Batch 44/7800 loss 6.698802 loss_att 10.044163 loss_ctc 12.849995 loss_rnnt 5.073734 hw_loss 0.254692 lr 0.00029499 rank 2
2023-02-27 14:59:19,240 DEBUG TRAIN Batch 44/7800 loss 5.853138 loss_att 7.939614 loss_ctc 6.999835 loss_rnnt 5.131356 hw_loss 0.284237 lr 0.00029499 rank 3
2023-02-27 14:59:19,245 DEBUG TRAIN Batch 44/7800 loss 6.863727 loss_att 9.168816 loss_ctc 9.670602 loss_rnnt 5.825912 hw_loss 0.379775 lr 0.00029499 rank 4
2023-02-27 14:59:19,247 DEBUG TRAIN Batch 44/7800 loss 4.138826 loss_att 6.925398 loss_ctc 6.242078 loss_rnnt 3.195911 hw_loss 0.197188 lr 0.00029499 rank 1
2023-02-27 14:59:19,258 DEBUG TRAIN Batch 44/7800 loss 6.851963 loss_att 8.163805 loss_ctc 12.168533 loss_rnnt 5.852002 hw_loss 0.053843 lr 0.00029499 rank 0
2023-02-27 15:01:15,026 DEBUG TRAIN Batch 44/7900 loss 7.206192 loss_att 11.780639 loss_ctc 15.049888 loss_rnnt 5.073195 hw_loss 0.323027 lr 0.00029494 rank 0
2023-02-27 15:01:15,029 DEBUG TRAIN Batch 44/7900 loss 4.774006 loss_att 5.926034 loss_ctc 10.596186 loss_rnnt 3.682551 hw_loss 0.158922 lr 0.00029494 rank 3
2023-02-27 15:01:15,030 DEBUG TRAIN Batch 44/7900 loss 4.311753 loss_att 8.257439 loss_ctc 9.469681 loss_rnnt 2.666045 hw_loss 0.316589 lr 0.00029494 rank 2
2023-02-27 15:01:15,036 DEBUG TRAIN Batch 44/7900 loss 7.926404 loss_att 13.677472 loss_ctc 13.296581 loss_rnnt 5.895844 hw_loss 0.308104 lr 0.00029494 rank 1
2023-02-27 15:01:15,039 DEBUG TRAIN Batch 44/7900 loss 8.292770 loss_att 11.589595 loss_ctc 13.002766 loss_rnnt 6.794114 hw_loss 0.396172 lr 0.00029494 rank 4
2023-02-27 15:02:38,031 DEBUG TRAIN Batch 44/8000 loss 8.269753 loss_att 10.356559 loss_ctc 13.072041 loss_rnnt 7.137432 hw_loss 0.139978 lr 0.00029488 rank 2
2023-02-27 15:02:38,041 DEBUG TRAIN Batch 44/8000 loss 11.494063 loss_att 12.515661 loss_ctc 15.531450 loss_rnnt 10.502117 hw_loss 0.467454 lr 0.00029488 rank 0
2023-02-27 15:02:38,043 DEBUG TRAIN Batch 44/8000 loss 6.255734 loss_att 7.558629 loss_ctc 7.128836 loss_rnnt 5.705661 hw_loss 0.324528 lr 0.00029488 rank 4
2023-02-27 15:02:38,043 DEBUG TRAIN Batch 44/8000 loss 5.436426 loss_att 8.154686 loss_ctc 9.705083 loss_rnnt 4.177124 hw_loss 0.274679 lr 0.00029488 rank 1
2023-02-27 15:02:38,057 DEBUG TRAIN Batch 44/8000 loss 10.429211 loss_att 12.401270 loss_ctc 15.053427 loss_rnnt 9.282211 hw_loss 0.255047 lr 0.00029488 rank 3
2023-02-27 15:04:00,676 DEBUG TRAIN Batch 44/8100 loss 7.200492 loss_att 12.111958 loss_ctc 11.343530 loss_rnnt 5.516030 hw_loss 0.280805 lr 0.00029483 rank 3
2023-02-27 15:04:00,681 DEBUG TRAIN Batch 44/8100 loss 9.927092 loss_att 12.579397 loss_ctc 18.058071 loss_rnnt 8.193710 hw_loss 0.222732 lr 0.00029483 rank 1
2023-02-27 15:04:00,681 DEBUG TRAIN Batch 44/8100 loss 11.697735 loss_att 14.675888 loss_ctc 18.477594 loss_rnnt 10.100306 hw_loss 0.183405 lr 0.00029483 rank 0
2023-02-27 15:04:00,683 DEBUG TRAIN Batch 44/8100 loss 10.992314 loss_att 13.240582 loss_ctc 18.615906 loss_rnnt 9.296452 hw_loss 0.430743 lr 0.00029483 rank 4
2023-02-27 15:04:00,687 DEBUG TRAIN Batch 44/8100 loss 7.032976 loss_att 10.432584 loss_ctc 11.396501 loss_rnnt 5.600129 hw_loss 0.320854 lr 0.00029483 rank 2
2023-02-27 15:05:59,365 DEBUG TRAIN Batch 44/8200 loss 7.454221 loss_att 8.084398 loss_ctc 12.085773 loss_rnnt 6.456771 hw_loss 0.476014 lr 0.00029478 rank 2
2023-02-27 15:05:59,367 DEBUG TRAIN Batch 44/8200 loss 4.367820 loss_att 5.733088 loss_ctc 7.406763 loss_rnnt 3.418138 hw_loss 0.508943 lr 0.00029478 rank 4
2023-02-27 15:05:59,368 DEBUG TRAIN Batch 44/8200 loss 6.947850 loss_att 8.698761 loss_ctc 11.666487 loss_rnnt 5.759818 hw_loss 0.391308 lr 0.00029478 rank 0
2023-02-27 15:05:59,368 DEBUG TRAIN Batch 44/8200 loss 9.188533 loss_att 12.990833 loss_ctc 16.818962 loss_rnnt 7.243451 hw_loss 0.313558 lr 0.00029478 rank 3
2023-02-27 15:05:59,372 DEBUG TRAIN Batch 44/8200 loss 11.488811 loss_att 14.861023 loss_ctc 27.341768 loss_rnnt 8.596655 hw_loss 0.194975 lr 0.00029478 rank 1
2023-02-27 15:07:21,499 DEBUG TRAIN Batch 44/8300 loss 3.876117 loss_att 9.249765 loss_ctc 8.350604 loss_rnnt 2.074073 hw_loss 0.245090 lr 0.00029473 rank 4
2023-02-27 15:07:21,502 DEBUG TRAIN Batch 44/8300 loss 3.857505 loss_att 8.342884 loss_ctc 8.059725 loss_rnnt 2.230150 hw_loss 0.318717 lr 0.00029473 rank 0
2023-02-27 15:07:21,503 DEBUG TRAIN Batch 44/8300 loss 7.018900 loss_att 7.878454 loss_ctc 11.955091 loss_rnnt 6.066621 hw_loss 0.229142 lr 0.00029473 rank 3
2023-02-27 15:07:21,504 DEBUG TRAIN Batch 44/8300 loss 5.806718 loss_att 8.604010 loss_ctc 12.170338 loss_rnnt 4.292039 hw_loss 0.200133 lr 0.00029473 rank 2
2023-02-27 15:07:21,512 DEBUG TRAIN Batch 44/8300 loss 6.542507 loss_att 7.134428 loss_ctc 10.741609 loss_rnnt 5.732938 hw_loss 0.246195 lr 0.00029473 rank 1
2023-02-27 15:08:43,525 DEBUG TRAIN Batch 44/8400 loss 5.059960 loss_att 8.478642 loss_ctc 11.174720 loss_rnnt 3.446690 hw_loss 0.214186 lr 0.00029468 rank 2
2023-02-27 15:08:43,526 DEBUG TRAIN Batch 44/8400 loss 2.468927 loss_att 6.120046 loss_ctc 5.227388 loss_rnnt 1.256056 hw_loss 0.215349 lr 0.00029468 rank 4
2023-02-27 15:08:43,527 DEBUG TRAIN Batch 44/8400 loss 4.963687 loss_att 8.323580 loss_ctc 8.886082 loss_rnnt 3.627951 hw_loss 0.263948 lr 0.00029468 rank 3
2023-02-27 15:08:43,532 DEBUG TRAIN Batch 44/8400 loss 5.610291 loss_att 7.526727 loss_ctc 7.546890 loss_rnnt 4.799188 hw_loss 0.318003 lr 0.00029468 rank 0
2023-02-27 15:08:43,588 DEBUG TRAIN Batch 44/8400 loss 3.609905 loss_att 6.791830 loss_ctc 5.915591 loss_rnnt 2.610886 hw_loss 0.103515 lr 0.00029468 rank 1
2023-02-27 15:10:09,066 DEBUG TRAIN Batch 44/8500 loss 7.947079 loss_att 10.018640 loss_ctc 11.886101 loss_rnnt 6.792571 hw_loss 0.403111 lr 0.00029463 rank 3
2023-02-27 15:10:09,067 DEBUG TRAIN Batch 44/8500 loss 4.220166 loss_att 7.879257 loss_ctc 7.064260 loss_rnnt 3.032566 hw_loss 0.143567 lr 0.00029463 rank 0
2023-02-27 15:10:09,073 DEBUG TRAIN Batch 44/8500 loss 7.874662 loss_att 11.620348 loss_ctc 17.101334 loss_rnnt 5.796627 hw_loss 0.185014 lr 0.00029463 rank 2
2023-02-27 15:10:09,075 DEBUG TRAIN Batch 44/8500 loss 9.141096 loss_att 12.589158 loss_ctc 16.619129 loss_rnnt 7.232795 hw_loss 0.415534 lr 0.00029463 rank 1
2023-02-27 15:10:09,081 DEBUG TRAIN Batch 44/8500 loss 12.725939 loss_att 16.239174 loss_ctc 15.246325 loss_rnnt 11.518570 hw_loss 0.316258 lr 0.00029463 rank 4
2023-02-27 15:12:05,286 DEBUG TRAIN Batch 44/8600 loss 6.089728 loss_att 10.735833 loss_ctc 13.986076 loss_rnnt 4.019919 hw_loss 0.164515 lr 0.00029458 rank 3
2023-02-27 15:12:05,290 DEBUG TRAIN Batch 44/8600 loss 14.586249 loss_att 19.234772 loss_ctc 28.281372 loss_rnnt 11.657274 hw_loss 0.324851 lr 0.00029458 rank 0
2023-02-27 15:12:05,291 DEBUG TRAIN Batch 44/8600 loss 8.078491 loss_att 9.544864 loss_ctc 14.203362 loss_rnnt 6.889429 hw_loss 0.148385 lr 0.00029458 rank 4
2023-02-27 15:12:05,293 DEBUG TRAIN Batch 44/8600 loss 4.240549 loss_att 6.493055 loss_ctc 6.060514 loss_rnnt 3.423915 hw_loss 0.231505 lr 0.00029458 rank 1
2023-02-27 15:12:05,293 DEBUG TRAIN Batch 44/8600 loss 8.824986 loss_att 12.503670 loss_ctc 14.230588 loss_rnnt 7.242503 hw_loss 0.236249 lr 0.00029458 rank 2
2023-02-27 15:13:29,638 DEBUG TRAIN Batch 44/8700 loss 13.840207 loss_att 16.189148 loss_ctc 24.396994 loss_rnnt 11.826716 hw_loss 0.255246 lr 0.00029453 rank 2
2023-02-27 15:13:29,639 DEBUG TRAIN Batch 44/8700 loss 7.470882 loss_att 9.932882 loss_ctc 11.280283 loss_rnnt 6.297736 hw_loss 0.324049 lr 0.00029453 rank 1
2023-02-27 15:13:29,640 DEBUG TRAIN Batch 44/8700 loss 5.185068 loss_att 7.339891 loss_ctc 8.767416 loss_rnnt 4.150422 hw_loss 0.236314 lr 0.00029453 rank 3
2023-02-27 15:13:29,642 DEBUG TRAIN Batch 44/8700 loss 4.453201 loss_att 6.034933 loss_ctc 8.083191 loss_rnnt 3.541606 hw_loss 0.208592 lr 0.00029453 rank 4
2023-02-27 15:13:29,648 DEBUG TRAIN Batch 44/8700 loss 18.102970 loss_att 20.314049 loss_ctc 28.109221 loss_rnnt 16.210190 hw_loss 0.218246 lr 0.00029453 rank 0
2023-02-27 15:14:52,045 DEBUG TRAIN Batch 44/8800 loss 7.012059 loss_att 10.343764 loss_ctc 11.160522 loss_rnnt 5.723724 hw_loss 0.129124 lr 0.00029448 rank 3
2023-02-27 15:14:52,050 DEBUG TRAIN Batch 44/8800 loss 7.687698 loss_att 9.284606 loss_ctc 11.105128 loss_rnnt 6.704091 hw_loss 0.391066 lr 0.00029448 rank 2
2023-02-27 15:14:52,053 DEBUG TRAIN Batch 44/8800 loss 5.001926 loss_att 7.283842 loss_ctc 7.001204 loss_rnnt 4.154322 hw_loss 0.233722 lr 0.00029448 rank 4
2023-02-27 15:14:52,055 DEBUG TRAIN Batch 44/8800 loss 3.742618 loss_att 7.321986 loss_ctc 8.105880 loss_rnnt 2.277710 hw_loss 0.313623 lr 0.00029448 rank 0
2023-02-27 15:14:52,060 DEBUG TRAIN Batch 44/8800 loss 8.338132 loss_att 10.800749 loss_ctc 12.975046 loss_rnnt 7.126994 hw_loss 0.188175 lr 0.00029448 rank 1
2023-02-27 15:16:49,769 DEBUG TRAIN Batch 44/8900 loss 8.485790 loss_att 10.997504 loss_ctc 13.446543 loss_rnnt 7.200952 hw_loss 0.226992 lr 0.00029442 rank 3
2023-02-27 15:16:49,772 DEBUG TRAIN Batch 44/8900 loss 2.472369 loss_att 4.933983 loss_ctc 6.527660 loss_rnnt 1.273466 hw_loss 0.311015 lr 0.00029442 rank 2
2023-02-27 15:16:49,773 DEBUG TRAIN Batch 44/8900 loss 3.046562 loss_att 7.092134 loss_ctc 7.577942 loss_rnnt 1.420646 hw_loss 0.398657 lr 0.00029442 rank 0
2023-02-27 15:16:49,775 DEBUG TRAIN Batch 44/8900 loss 7.112839 loss_att 9.575605 loss_ctc 16.767767 loss_rnnt 5.168532 hw_loss 0.308307 lr 0.00029442 rank 1
2023-02-27 15:16:49,780 DEBUG TRAIN Batch 44/8900 loss 6.612325 loss_att 8.443571 loss_ctc 7.844701 loss_rnnt 5.924285 hw_loss 0.295263 lr 0.00029442 rank 4
2023-02-27 15:18:11,788 DEBUG TRAIN Batch 44/9000 loss 8.744558 loss_att 11.409712 loss_ctc 14.136932 loss_rnnt 7.408097 hw_loss 0.158337 lr 0.00029437 rank 2
2023-02-27 15:18:11,791 DEBUG TRAIN Batch 44/9000 loss 8.212132 loss_att 8.668768 loss_ctc 13.812576 loss_rnnt 7.164638 hw_loss 0.392702 lr 0.00029437 rank 3
2023-02-27 15:18:11,799 DEBUG TRAIN Batch 44/9000 loss 8.171293 loss_att 12.616531 loss_ctc 18.639080 loss_rnnt 5.739736 hw_loss 0.275259 lr 0.00029437 rank 4
2023-02-27 15:18:11,951 DEBUG TRAIN Batch 44/9000 loss 6.344930 loss_att 10.796544 loss_ctc 9.530367 loss_rnnt 4.982873 hw_loss 0.088144 lr 0.00029437 rank 0
2023-02-27 15:18:11,952 DEBUG TRAIN Batch 44/9000 loss 2.985557 loss_att 5.207145 loss_ctc 4.778116 loss_rnnt 2.127175 hw_loss 0.328230 lr 0.00029437 rank 1
2023-02-27 15:19:35,770 DEBUG TRAIN Batch 44/9100 loss 6.314423 loss_att 10.068542 loss_ctc 14.027087 loss_rnnt 4.410038 hw_loss 0.234760 lr 0.00029432 rank 2
2023-02-27 15:19:35,771 DEBUG TRAIN Batch 44/9100 loss 5.706039 loss_att 7.222355 loss_ctc 9.265936 loss_rnnt 4.898616 hw_loss 0.055327 lr 0.00029432 rank 3
2023-02-27 15:19:35,774 DEBUG TRAIN Batch 44/9100 loss 4.634931 loss_att 6.687044 loss_ctc 5.991492 loss_rnnt 3.875352 hw_loss 0.315528 lr 0.00029432 rank 4
2023-02-27 15:19:35,785 DEBUG TRAIN Batch 44/9100 loss 12.899428 loss_att 19.063875 loss_ctc 25.093807 loss_rnnt 9.909261 hw_loss 0.246303 lr 0.00029432 rank 0
2023-02-27 15:19:35,835 DEBUG TRAIN Batch 44/9100 loss 12.019968 loss_att 14.168049 loss_ctc 20.856827 loss_rnnt 10.300825 hw_loss 0.208649 lr 0.00029432 rank 1
2023-02-27 15:20:59,185 DEBUG TRAIN Batch 44/9200 loss 11.932723 loss_att 16.092403 loss_ctc 17.919956 loss_rnnt 10.214951 hw_loss 0.164134 lr 0.00029427 rank 2
2023-02-27 15:20:59,189 DEBUG TRAIN Batch 44/9200 loss 20.744274 loss_att 17.122133 loss_ctc 41.670868 loss_rnnt 18.606472 hw_loss 0.135036 lr 0.00029427 rank 1
2023-02-27 15:20:59,191 DEBUG TRAIN Batch 44/9200 loss 5.999475 loss_att 7.763210 loss_ctc 8.784046 loss_rnnt 5.111503 hw_loss 0.307407 lr 0.00029427 rank 0
2023-02-27 15:20:59,192 DEBUG TRAIN Batch 44/9200 loss 6.269784 loss_att 8.789025 loss_ctc 11.270245 loss_rnnt 4.956159 hw_loss 0.268218 lr 0.00029427 rank 4
2023-02-27 15:20:59,223 DEBUG TRAIN Batch 44/9200 loss 6.830063 loss_att 11.979575 loss_ctc 12.281330 loss_rnnt 5.007109 hw_loss 0.124154 lr 0.00029427 rank 3
2023-02-27 15:23:00,112 DEBUG TRAIN Batch 44/9300 loss 9.068069 loss_att 13.922897 loss_ctc 20.825550 loss_rnnt 6.409863 hw_loss 0.224201 lr 0.00029422 rank 2
2023-02-27 15:23:00,118 DEBUG TRAIN Batch 44/9300 loss 6.922102 loss_att 11.638704 loss_ctc 10.973869 loss_rnnt 5.341259 hw_loss 0.182414 lr 0.00029422 rank 4
2023-02-27 15:23:00,118 DEBUG TRAIN Batch 44/9300 loss 10.367004 loss_att 15.171805 loss_ctc 22.640966 loss_rnnt 7.588716 hw_loss 0.338999 lr 0.00029422 rank 3
2023-02-27 15:23:00,123 DEBUG TRAIN Batch 44/9300 loss 10.423572 loss_att 14.395988 loss_ctc 15.203524 loss_rnnt 8.898518 hw_loss 0.174832 lr 0.00029422 rank 0
2023-02-27 15:23:00,127 DEBUG TRAIN Batch 44/9300 loss 5.108187 loss_att 6.675597 loss_ctc 9.504759 loss_rnnt 3.978970 hw_loss 0.430360 lr 0.00029422 rank 1
2023-02-27 15:24:22,921 DEBUG TRAIN Batch 44/9400 loss 4.075157 loss_att 7.960362 loss_ctc 6.203160 loss_rnnt 2.898921 hw_loss 0.216490 lr 0.00029417 rank 3
2023-02-27 15:24:22,922 DEBUG TRAIN Batch 44/9400 loss 5.686678 loss_att 8.676185 loss_ctc 9.346321 loss_rnnt 4.467878 hw_loss 0.249274 lr 0.00029417 rank 2
2023-02-27 15:24:22,927 DEBUG TRAIN Batch 44/9400 loss 3.221561 loss_att 5.371003 loss_ctc 6.150779 loss_rnnt 2.307514 hw_loss 0.175493 lr 0.00029417 rank 4
2023-02-27 15:24:22,933 DEBUG TRAIN Batch 44/9400 loss 5.155494 loss_att 8.465387 loss_ctc 7.572491 loss_rnnt 4.029973 hw_loss 0.264892 lr 0.00029417 rank 0
2023-02-27 15:24:22,936 DEBUG TRAIN Batch 44/9400 loss 10.448151 loss_att 12.887173 loss_ctc 16.394014 loss_rnnt 9.028309 hw_loss 0.261105 lr 0.00029417 rank 1
2023-02-27 15:25:44,907 DEBUG TRAIN Batch 44/9500 loss 7.129491 loss_att 7.556175 loss_ctc 9.751213 loss_rnnt 6.367703 hw_loss 0.612915 lr 0.00029412 rank 2
2023-02-27 15:25:44,914 DEBUG TRAIN Batch 44/9500 loss 6.966701 loss_att 7.457554 loss_ctc 9.154378 loss_rnnt 6.369448 hw_loss 0.388858 lr 0.00029412 rank 0
2023-02-27 15:25:44,917 DEBUG TRAIN Batch 44/9500 loss 9.592555 loss_att 14.783980 loss_ctc 17.286795 loss_rnnt 7.445596 hw_loss 0.155204 lr 0.00029412 rank 4
2023-02-27 15:25:44,919 DEBUG TRAIN Batch 44/9500 loss 8.828413 loss_att 11.715616 loss_ctc 15.781534 loss_rnnt 7.168337 hw_loss 0.291660 lr 0.00029412 rank 3
2023-02-27 15:25:44,923 DEBUG TRAIN Batch 44/9500 loss 8.070020 loss_att 11.120736 loss_ctc 17.558300 loss_rnnt 6.098845 hw_loss 0.179864 lr 0.00029412 rank 1
2023-02-27 15:27:45,095 DEBUG TRAIN Batch 44/9600 loss 7.641870 loss_att 9.919301 loss_ctc 10.727970 loss_rnnt 6.564713 hw_loss 0.394107 lr 0.00029407 rank 1
2023-02-27 15:27:45,097 DEBUG TRAIN Batch 44/9600 loss 2.816123 loss_att 6.042055 loss_ctc 5.507114 loss_rnnt 1.781042 hw_loss 0.058303 lr 0.00029407 rank 4
2023-02-27 15:27:45,102 DEBUG TRAIN Batch 44/9600 loss 4.878835 loss_att 7.993620 loss_ctc 7.760108 loss_rnnt 3.646105 hw_loss 0.423005 lr 0.00029407 rank 2
2023-02-27 15:27:45,101 DEBUG TRAIN Batch 44/9600 loss 8.025715 loss_att 9.193445 loss_ctc 11.871554 loss_rnnt 7.024211 hw_loss 0.478460 lr 0.00029407 rank 3
2023-02-27 15:27:45,162 DEBUG TRAIN Batch 44/9600 loss 8.176730 loss_att 12.328812 loss_ctc 13.854281 loss_rnnt 6.503590 hw_loss 0.160720 lr 0.00029407 rank 0
2023-02-27 15:29:10,511 DEBUG TRAIN Batch 44/9700 loss 6.445799 loss_att 11.016029 loss_ctc 11.313107 loss_rnnt 4.819489 hw_loss 0.118667 lr 0.00029402 rank 2
2023-02-27 15:29:10,513 DEBUG TRAIN Batch 44/9700 loss 2.541385 loss_att 6.731562 loss_ctc 6.111784 loss_rnnt 1.140192 hw_loss 0.163321 lr 0.00029402 rank 1
2023-02-27 15:29:10,517 DEBUG TRAIN Batch 44/9700 loss 4.208617 loss_att 7.167430 loss_ctc 3.858378 loss_rnnt 3.474068 hw_loss 0.355285 lr 0.00029402 rank 3
2023-02-27 15:29:10,523 DEBUG TRAIN Batch 44/9700 loss 4.524926 loss_att 8.467901 loss_ctc 7.236022 loss_rnnt 3.242714 hw_loss 0.247756 lr 0.00029402 rank 4
2023-02-27 15:29:10,570 DEBUG TRAIN Batch 44/9700 loss 8.085503 loss_att 13.062123 loss_ctc 13.914405 loss_rnnt 6.142878 hw_loss 0.318965 lr 0.00029402 rank 0
2023-02-27 15:30:33,268 DEBUG TRAIN Batch 44/9800 loss 9.954625 loss_att 13.187791 loss_ctc 15.729193 loss_rnnt 8.422390 hw_loss 0.216862 lr 0.00029397 rank 2
2023-02-27 15:30:33,271 DEBUG TRAIN Batch 44/9800 loss 7.511637 loss_att 8.027609 loss_ctc 11.024081 loss_rnnt 6.863495 hw_loss 0.143664 lr 0.00029397 rank 1
2023-02-27 15:30:33,273 DEBUG TRAIN Batch 44/9800 loss 6.983483 loss_att 10.912201 loss_ctc 11.388828 loss_rnnt 5.426207 hw_loss 0.345287 lr 0.00029397 rank 0
2023-02-27 15:30:33,275 DEBUG TRAIN Batch 44/9800 loss 12.288406 loss_att 16.611452 loss_ctc 23.385136 loss_rnnt 9.891077 hw_loss 0.099669 lr 0.00029397 rank 3
2023-02-27 15:30:33,293 DEBUG TRAIN Batch 44/9800 loss 6.512480 loss_att 9.990858 loss_ctc 12.948961 loss_rnnt 4.836236 hw_loss 0.229445 lr 0.00029397 rank 4
2023-02-27 15:31:56,679 DEBUG TRAIN Batch 44/9900 loss 6.396206 loss_att 9.500509 loss_ctc 9.325605 loss_rnnt 5.261774 hw_loss 0.230597 lr 0.00029391 rank 1
2023-02-27 15:31:56,682 DEBUG TRAIN Batch 44/9900 loss 3.858595 loss_att 6.557799 loss_ctc 7.668820 loss_rnnt 2.623697 hw_loss 0.350675 lr 0.00029391 rank 3
2023-02-27 15:31:56,685 DEBUG TRAIN Batch 44/9900 loss 6.111466 loss_att 9.317551 loss_ctc 9.386887 loss_rnnt 4.913201 hw_loss 0.225610 lr 0.00029391 rank 2
2023-02-27 15:31:56,688 DEBUG TRAIN Batch 44/9900 loss 11.689794 loss_att 15.471437 loss_ctc 16.431639 loss_rnnt 10.183661 hw_loss 0.220421 lr 0.00029391 rank 4
2023-02-27 15:31:56,688 DEBUG TRAIN Batch 44/9900 loss 9.554085 loss_att 13.214735 loss_ctc 15.133222 loss_rnnt 7.926555 hw_loss 0.284090 lr 0.00029391 rank 0
2023-02-27 15:33:51,903 DEBUG TRAIN Batch 44/10000 loss 1.347460 loss_att 3.368567 loss_ctc 2.505765 loss_rnnt 0.640260 hw_loss 0.278507 lr 0.00029386 rank 3
2023-02-27 15:33:51,904 DEBUG TRAIN Batch 44/10000 loss 12.821177 loss_att 14.424006 loss_ctc 21.222237 loss_rnnt 11.150101 hw_loss 0.431939 lr 0.00029386 rank 0
2023-02-27 15:33:51,906 DEBUG TRAIN Batch 44/10000 loss 3.975419 loss_att 7.894316 loss_ctc 7.066114 loss_rnnt 2.636574 hw_loss 0.268075 lr 0.00029386 rank 1
2023-02-27 15:33:51,908 DEBUG TRAIN Batch 44/10000 loss 9.564561 loss_att 12.470442 loss_ctc 15.425258 loss_rnnt 8.035047 hw_loss 0.312961 lr 0.00029386 rank 2
2023-02-27 15:33:51,914 DEBUG TRAIN Batch 44/10000 loss 10.763725 loss_att 15.157746 loss_ctc 20.330191 loss_rnnt 8.527655 hw_loss 0.153256 lr 0.00029386 rank 4
2023-02-27 15:35:15,065 DEBUG TRAIN Batch 44/10100 loss 6.271241 loss_att 10.137476 loss_ctc 11.264895 loss_rnnt 4.606474 hw_loss 0.423187 lr 0.00029381 rank 3
2023-02-27 15:35:15,066 DEBUG TRAIN Batch 44/10100 loss 10.654048 loss_att 12.439651 loss_ctc 20.220968 loss_rnnt 8.807451 hw_loss 0.401037 lr 0.00029381 rank 4
2023-02-27 15:35:15,068 DEBUG TRAIN Batch 44/10100 loss 12.410903 loss_att 14.776376 loss_ctc 23.056257 loss_rnnt 10.404250 hw_loss 0.214083 lr 0.00029381 rank 2
2023-02-27 15:35:15,077 DEBUG TRAIN Batch 44/10100 loss 9.211446 loss_att 11.516234 loss_ctc 12.700603 loss_rnnt 8.172036 hw_loss 0.212306 lr 0.00029381 rank 1
2023-02-27 15:35:15,084 DEBUG TRAIN Batch 44/10100 loss 3.388098 loss_att 6.197497 loss_ctc 7.039617 loss_rnnt 2.167561 hw_loss 0.322103 lr 0.00029381 rank 0
2023-02-27 15:36:37,955 DEBUG TRAIN Batch 44/10200 loss 6.856848 loss_att 8.292466 loss_ctc 10.516222 loss_rnnt 5.852495 hw_loss 0.429961 lr 0.00029376 rank 3
2023-02-27 15:36:37,957 DEBUG TRAIN Batch 44/10200 loss 4.325315 loss_att 6.012913 loss_ctc 5.862358 loss_rnnt 3.628182 hw_loss 0.290015 lr 0.00029376 rank 0
2023-02-27 15:36:37,958 DEBUG TRAIN Batch 44/10200 loss 6.362228 loss_att 8.991042 loss_ctc 7.900756 loss_rnnt 5.490499 hw_loss 0.264053 lr 0.00029376 rank 2
2023-02-27 15:36:37,958 DEBUG TRAIN Batch 44/10200 loss 5.942901 loss_att 6.769392 loss_ctc 8.944044 loss_rnnt 5.296482 hw_loss 0.151816 lr 0.00029376 rank 4
2023-02-27 15:36:37,962 DEBUG TRAIN Batch 44/10200 loss 10.647722 loss_att 13.443939 loss_ctc 16.302893 loss_rnnt 9.167430 hw_loss 0.313174 lr 0.00029376 rank 1
2023-02-27 15:38:03,665 DEBUG TRAIN Batch 44/10300 loss 12.879626 loss_att 12.564331 loss_ctc 17.747620 loss_rnnt 12.222040 hw_loss 0.134208 lr 0.00029371 rank 2
2023-02-27 15:38:03,667 DEBUG TRAIN Batch 44/10300 loss 6.041918 loss_att 11.374033 loss_ctc 8.280031 loss_rnnt 4.471300 hw_loss 0.385839 lr 0.00029371 rank 3
2023-02-27 15:38:03,670 DEBUG TRAIN Batch 44/10300 loss 2.959957 loss_att 6.117249 loss_ctc 6.164117 loss_rnnt 1.728828 hw_loss 0.323342 lr 0.00029371 rank 4
2023-02-27 15:38:03,717 DEBUG TRAIN Batch 44/10300 loss 5.083108 loss_att 7.739667 loss_ctc 8.985997 loss_rnnt 3.884077 hw_loss 0.276254 lr 0.00029371 rank 1
2023-02-27 15:38:03,724 DEBUG TRAIN Batch 44/10300 loss 9.515370 loss_att 12.555888 loss_ctc 12.500518 loss_rnnt 8.409602 hw_loss 0.186836 lr 0.00029371 rank 0
2023-02-27 15:39:56,934 DEBUG TRAIN Batch 44/10400 loss 5.564608 loss_att 8.025323 loss_ctc 9.604808 loss_rnnt 4.376824 hw_loss 0.294277 lr 0.00029366 rank 2
2023-02-27 15:39:56,936 DEBUG TRAIN Batch 44/10400 loss 6.711550 loss_att 11.128387 loss_ctc 17.633068 loss_rnnt 4.344203 hw_loss 0.052082 lr 0.00029366 rank 3
2023-02-27 15:39:56,937 DEBUG TRAIN Batch 44/10400 loss 16.544163 loss_att 20.227900 loss_ctc 31.224697 loss_rnnt 13.690784 hw_loss 0.298546 lr 0.00029366 rank 0
2023-02-27 15:39:56,940 DEBUG TRAIN Batch 44/10400 loss 1.827753 loss_att 4.302424 loss_ctc 3.120806 loss_rnnt 1.000033 hw_loss 0.300711 lr 0.00029366 rank 1
2023-02-27 15:39:56,940 DEBUG TRAIN Batch 44/10400 loss 7.157174 loss_att 9.182981 loss_ctc 13.186292 loss_rnnt 5.826531 hw_loss 0.227997 lr 0.00029366 rank 4
2023-02-27 15:41:19,824 DEBUG TRAIN Batch 44/10500 loss 3.334125 loss_att 6.682902 loss_ctc 6.133357 loss_rnnt 2.207488 hw_loss 0.156845 lr 0.00029361 rank 2
2023-02-27 15:41:19,825 DEBUG TRAIN Batch 44/10500 loss 10.330584 loss_att 9.675146 loss_ctc 13.112683 loss_rnnt 9.888905 hw_loss 0.378412 lr 0.00029361 rank 3
2023-02-27 15:41:19,826 DEBUG TRAIN Batch 44/10500 loss 12.660578 loss_att 17.327085 loss_ctc 24.615669 loss_rnnt 9.902925 hw_loss 0.431884 lr 0.00029361 rank 1
2023-02-27 15:41:19,827 DEBUG TRAIN Batch 44/10500 loss 7.246622 loss_att 10.974710 loss_ctc 10.844165 loss_rnnt 5.871139 hw_loss 0.281612 lr 0.00029361 rank 4
2023-02-27 15:41:19,828 DEBUG TRAIN Batch 44/10500 loss 8.340449 loss_att 11.354148 loss_ctc 14.013618 loss_rnnt 6.835544 hw_loss 0.273269 lr 0.00029361 rank 0
2023-02-27 15:42:43,691 DEBUG TRAIN Batch 44/10600 loss 7.641943 loss_att 10.849243 loss_ctc 11.989652 loss_rnnt 6.278115 hw_loss 0.267513 lr 0.00029356 rank 3
2023-02-27 15:42:43,694 DEBUG TRAIN Batch 44/10600 loss 5.874076 loss_att 9.639489 loss_ctc 9.477965 loss_rnnt 4.545447 hw_loss 0.178177 lr 0.00029356 rank 2
2023-02-27 15:42:43,695 DEBUG TRAIN Batch 44/10600 loss 7.211064 loss_att 8.986705 loss_ctc 10.655956 loss_rnnt 6.257994 hw_loss 0.259920 lr 0.00029356 rank 0
2023-02-27 15:42:43,700 DEBUG TRAIN Batch 44/10600 loss 6.913552 loss_att 8.427636 loss_ctc 9.741196 loss_rnnt 6.090597 hw_loss 0.268349 lr 0.00029356 rank 4
2023-02-27 15:42:43,703 DEBUG TRAIN Batch 44/10600 loss 6.372781 loss_att 11.684576 loss_ctc 12.005387 loss_rnnt 4.423108 hw_loss 0.255563 lr 0.00029356 rank 1
2023-02-27 15:44:38,303 DEBUG TRAIN Batch 44/10700 loss 7.371480 loss_att 10.635580 loss_ctc 15.812781 loss_rnnt 5.454318 hw_loss 0.260314 lr 0.00029351 rank 2
2023-02-27 15:44:38,303 DEBUG TRAIN Batch 44/10700 loss 2.911946 loss_att 5.480397 loss_ctc 5.576637 loss_rnnt 1.878351 hw_loss 0.308648 lr 0.00029351 rank 0
2023-02-27 15:44:38,304 DEBUG TRAIN Batch 44/10700 loss 4.248393 loss_att 6.952425 loss_ctc 7.242932 loss_rnnt 3.151813 hw_loss 0.293440 lr 0.00029351 rank 3
2023-02-27 15:44:38,310 DEBUG TRAIN Batch 44/10700 loss 4.268688 loss_att 7.572063 loss_ctc 7.170000 loss_rnnt 3.070739 hw_loss 0.282061 lr 0.00029351 rank 4
2023-02-27 15:44:38,313 DEBUG TRAIN Batch 44/10700 loss 3.237217 loss_att 6.802789 loss_ctc 7.677832 loss_rnnt 1.716273 hw_loss 0.404526 lr 0.00029351 rank 1
2023-02-27 15:46:01,281 DEBUG TRAIN Batch 44/10800 loss 5.378213 loss_att 9.642363 loss_ctc 11.427966 loss_rnnt 3.543235 hw_loss 0.329089 lr 0.00029346 rank 2
2023-02-27 15:46:01,286 DEBUG TRAIN Batch 44/10800 loss 13.917682 loss_att 15.837088 loss_ctc 22.885847 loss_rnnt 12.102921 hw_loss 0.440858 lr 0.00029346 rank 0
2023-02-27 15:46:01,293 DEBUG TRAIN Batch 44/10800 loss 17.716547 loss_att 15.103398 loss_ctc 32.473373 loss_rnnt 16.101480 hw_loss 0.318971 lr 0.00029346 rank 4
2023-02-27 15:46:01,295 DEBUG TRAIN Batch 44/10800 loss 9.243675 loss_att 12.147213 loss_ctc 16.245110 loss_rnnt 7.632825 hw_loss 0.181159 lr 0.00029346 rank 3
2023-02-27 15:46:01,319 DEBUG TRAIN Batch 44/10800 loss 8.838405 loss_att 11.991220 loss_ctc 14.924649 loss_rnnt 7.163284 hw_loss 0.436983 lr 0.00029346 rank 1
2023-02-27 15:47:24,085 DEBUG TRAIN Batch 44/10900 loss 2.248074 loss_att 4.943401 loss_ctc 7.767456 loss_rnnt 0.904982 hw_loss 0.127705 lr 0.00029341 rank 3
2023-02-27 15:47:24,091 DEBUG TRAIN Batch 44/10900 loss 6.008996 loss_att 7.252924 loss_ctc 11.182571 loss_rnnt 4.859346 hw_loss 0.395726 lr 0.00029341 rank 2
2023-02-27 15:47:24,092 DEBUG TRAIN Batch 44/10900 loss 7.637795 loss_att 11.178288 loss_ctc 13.929805 loss_rnnt 5.960213 hw_loss 0.244781 lr 0.00029341 rank 1
2023-02-27 15:47:24,096 DEBUG TRAIN Batch 44/10900 loss 6.806061 loss_att 11.374719 loss_ctc 9.458790 loss_rnnt 5.354968 hw_loss 0.344371 lr 0.00029341 rank 4
2023-02-27 15:47:24,100 DEBUG TRAIN Batch 44/10900 loss 7.894828 loss_att 10.915545 loss_ctc 10.480835 loss_rnnt 6.786905 hw_loss 0.298083 lr 0.00029341 rank 0
2023-02-27 15:48:48,945 DEBUG TRAIN Batch 44/11000 loss 7.278602 loss_att 8.280396 loss_ctc 12.286677 loss_rnnt 6.267694 hw_loss 0.267760 lr 0.00029336 rank 4
2023-02-27 15:48:48,955 DEBUG TRAIN Batch 44/11000 loss 8.078579 loss_att 13.312281 loss_ctc 15.612948 loss_rnnt 5.878067 hw_loss 0.279729 lr 0.00029336 rank 1
2023-02-27 15:48:49,013 DEBUG TRAIN Batch 44/11000 loss 17.605392 loss_att 18.315784 loss_ctc 33.222073 loss_rnnt 15.305710 hw_loss 0.141335 lr 0.00029336 rank 0
2023-02-27 15:48:49,014 DEBUG TRAIN Batch 44/11000 loss 5.753981 loss_att 8.982150 loss_ctc 7.490286 loss_rnnt 4.850416 hw_loss 0.049543 lr 0.00029336 rank 3
2023-02-27 15:48:49,015 DEBUG TRAIN Batch 44/11000 loss 11.062483 loss_att 16.656380 loss_ctc 19.408440 loss_rnnt 8.701744 hw_loss 0.242186 lr 0.00029336 rank 2
2023-02-27 15:50:42,665 DEBUG TRAIN Batch 44/11100 loss 13.210177 loss_att 16.360039 loss_ctc 20.436470 loss_rnnt 11.505669 hw_loss 0.208181 lr 0.00029331 rank 4
2023-02-27 15:50:42,665 DEBUG TRAIN Batch 44/11100 loss 11.923000 loss_att 14.413607 loss_ctc 13.178386 loss_rnnt 11.115223 hw_loss 0.266758 lr 0.00029331 rank 3
2023-02-27 15:50:42,667 DEBUG TRAIN Batch 44/11100 loss 13.829536 loss_att 15.180056 loss_ctc 20.537560 loss_rnnt 12.538424 hw_loss 0.237385 lr 0.00029331 rank 0
2023-02-27 15:50:42,668 DEBUG TRAIN Batch 44/11100 loss 8.041630 loss_att 9.247930 loss_ctc 15.497694 loss_rnnt 6.668914 hw_loss 0.257464 lr 0.00029331 rank 2
2023-02-27 15:50:42,679 DEBUG TRAIN Batch 44/11100 loss 4.523187 loss_att 8.932617 loss_ctc 9.579707 loss_rnnt 2.825408 hw_loss 0.265670 lr 0.00029331 rank 1
2023-02-27 15:52:04,679 DEBUG TRAIN Batch 44/11200 loss 15.109071 loss_att 18.674286 loss_ctc 28.388308 loss_rnnt 12.496169 hw_loss 0.242426 lr 0.00029326 rank 2
2023-02-27 15:52:04,681 DEBUG TRAIN Batch 44/11200 loss 6.100469 loss_att 9.714913 loss_ctc 8.132257 loss_rnnt 5.033038 hw_loss 0.138068 lr 0.00029326 rank 3
2023-02-27 15:52:04,681 DEBUG TRAIN Batch 44/11200 loss 9.066190 loss_att 12.660013 loss_ctc 14.880867 loss_rnnt 7.429708 hw_loss 0.267051 lr 0.00029326 rank 0
2023-02-27 15:52:04,683 DEBUG TRAIN Batch 44/11200 loss 15.455242 loss_att 15.620350 loss_ctc 22.845655 loss_rnnt 14.301878 hw_loss 0.253039 lr 0.00029326 rank 4
2023-02-27 15:52:04,686 DEBUG TRAIN Batch 44/11200 loss 5.971325 loss_att 7.852853 loss_ctc 11.486963 loss_rnnt 4.658617 hw_loss 0.376844 lr 0.00029326 rank 1
2023-02-27 15:53:26,434 DEBUG TRAIN Batch 44/11300 loss 3.440440 loss_att 5.974321 loss_ctc 7.388624 loss_rnnt 2.192114 hw_loss 0.403359 lr 0.00029321 rank 3
2023-02-27 15:53:26,439 DEBUG TRAIN Batch 44/11300 loss 12.930809 loss_att 16.039553 loss_ctc 16.523834 loss_rnnt 11.689840 hw_loss 0.262780 lr 0.00029321 rank 2
2023-02-27 15:53:26,446 DEBUG TRAIN Batch 44/11300 loss 9.680737 loss_att 12.180596 loss_ctc 18.744482 loss_rnnt 7.862936 hw_loss 0.204989 lr 0.00029321 rank 0
2023-02-27 15:53:26,447 DEBUG TRAIN Batch 44/11300 loss 7.142321 loss_att 9.357387 loss_ctc 12.182589 loss_rnnt 5.861072 hw_loss 0.311627 lr 0.00029321 rank 1
2023-02-27 15:53:26,448 DEBUG TRAIN Batch 44/11300 loss 4.767351 loss_att 7.830119 loss_ctc 9.328032 loss_rnnt 3.348023 hw_loss 0.372531 lr 0.00029321 rank 4
2023-02-27 15:55:25,309 DEBUG TRAIN Batch 44/11400 loss 4.167972 loss_att 6.581352 loss_ctc 7.487787 loss_rnnt 3.071074 hw_loss 0.321712 lr 0.00029316 rank 2
2023-02-27 15:55:25,309 DEBUG TRAIN Batch 44/11400 loss 12.551610 loss_att 13.981913 loss_ctc 15.402248 loss_rnnt 11.696259 hw_loss 0.354757 lr 0.00029316 rank 3
2023-02-27 15:55:25,313 DEBUG TRAIN Batch 44/11400 loss 5.659023 loss_att 6.796819 loss_ctc 9.343936 loss_rnnt 4.716835 hw_loss 0.418700 lr 0.00029316 rank 4
2023-02-27 15:55:25,316 DEBUG TRAIN Batch 44/11400 loss 15.770097 loss_att 18.100143 loss_ctc 27.680275 loss_rnnt 13.586368 hw_loss 0.243180 lr 0.00029316 rank 1
2023-02-27 15:55:25,370 DEBUG TRAIN Batch 44/11400 loss 6.063652 loss_att 9.341136 loss_ctc 14.249186 loss_rnnt 4.183136 hw_loss 0.250528 lr 0.00029316 rank 0
2023-02-27 15:56:47,989 DEBUG TRAIN Batch 44/11500 loss 13.726714 loss_att 17.908508 loss_ctc 22.794998 loss_rnnt 11.589931 hw_loss 0.171223 lr 0.00029311 rank 4
2023-02-27 15:56:47,992 DEBUG TRAIN Batch 44/11500 loss 3.900864 loss_att 5.901728 loss_ctc 4.876736 loss_rnnt 3.267623 hw_loss 0.193036 lr 0.00029311 rank 2
2023-02-27 15:56:47,993 DEBUG TRAIN Batch 44/11500 loss 12.379838 loss_att 12.845283 loss_ctc 18.860401 loss_rnnt 11.173251 hw_loss 0.467670 lr 0.00029311 rank 3
2023-02-27 15:56:47,995 DEBUG TRAIN Batch 44/11500 loss 5.278796 loss_att 11.645650 loss_ctc 12.763918 loss_rnnt 2.979549 hw_loss 0.052236 lr 0.00029311 rank 0
2023-02-27 15:56:47,997 DEBUG TRAIN Batch 44/11500 loss 5.989898 loss_att 7.666302 loss_ctc 9.554430 loss_rnnt 4.903691 hw_loss 0.516852 lr 0.00029311 rank 1
2023-02-27 15:58:11,760 DEBUG TRAIN Batch 44/11600 loss 4.211744 loss_att 6.868980 loss_ctc 9.686407 loss_rnnt 2.898331 hw_loss 0.097520 lr 0.00029306 rank 2
2023-02-27 15:58:11,761 DEBUG TRAIN Batch 44/11600 loss 3.213696 loss_att 4.926532 loss_ctc 3.409307 loss_rnnt 2.664829 hw_loss 0.337908 lr 0.00029306 rank 3
2023-02-27 15:58:11,763 DEBUG TRAIN Batch 44/11600 loss 7.318467 loss_att 11.233436 loss_ctc 10.351935 loss_rnnt 5.993214 hw_loss 0.258370 lr 0.00029306 rank 1
2023-02-27 15:58:11,764 DEBUG TRAIN Batch 44/11600 loss 3.020447 loss_att 5.689042 loss_ctc 4.349216 loss_rnnt 2.182300 hw_loss 0.238609 lr 0.00029306 rank 4
2023-02-27 15:58:11,864 DEBUG TRAIN Batch 44/11600 loss 7.881116 loss_att 10.464697 loss_ctc 11.083862 loss_rnnt 6.866748 hw_loss 0.132411 lr 0.00029306 rank 0
2023-02-27 15:59:35,176 DEBUG TRAIN Batch 44/11700 loss 11.339242 loss_att 14.261622 loss_ctc 18.136524 loss_rnnt 9.649704 hw_loss 0.372672 lr 0.00029301 rank 2
2023-02-27 15:59:35,179 DEBUG TRAIN Batch 44/11700 loss 10.215557 loss_att 12.075123 loss_ctc 12.439957 loss_rnnt 9.390224 hw_loss 0.294060 lr 0.00029301 rank 3
2023-02-27 15:59:35,184 DEBUG TRAIN Batch 44/11700 loss 8.170837 loss_att 10.560786 loss_ctc 15.077189 loss_rnnt 6.615108 hw_loss 0.294173 lr 0.00029301 rank 4
2023-02-27 15:59:35,203 DEBUG TRAIN Batch 44/11700 loss 4.487498 loss_att 7.563166 loss_ctc 7.720210 loss_rnnt 3.371814 hw_loss 0.130354 lr 0.00029301 rank 0
2023-02-27 15:59:35,226 DEBUG TRAIN Batch 44/11700 loss 4.826237 loss_att 9.395941 loss_ctc 8.088828 loss_rnnt 3.309678 hw_loss 0.314261 lr 0.00029301 rank 1
2023-02-27 16:01:28,639 DEBUG TRAIN Batch 44/11800 loss 8.963458 loss_att 11.432178 loss_ctc 15.959457 loss_rnnt 7.388321 hw_loss 0.278611 lr 0.00029295 rank 3
2023-02-27 16:01:28,642 DEBUG TRAIN Batch 44/11800 loss 9.387955 loss_att 12.433158 loss_ctc 11.813133 loss_rnnt 8.295422 hw_loss 0.300253 lr 0.00029295 rank 0
2023-02-27 16:01:28,642 DEBUG TRAIN Batch 44/11800 loss 8.334440 loss_att 13.150453 loss_ctc 16.944424 loss_rnnt 6.118424 hw_loss 0.196532 lr 0.00029295 rank 2
2023-02-27 16:01:28,649 DEBUG TRAIN Batch 44/11800 loss 10.318144 loss_att 13.013351 loss_ctc 19.149672 loss_rnnt 8.464660 hw_loss 0.256698 lr 0.00029295 rank 4
2023-02-27 16:01:28,650 DEBUG TRAIN Batch 44/11800 loss 4.631574 loss_att 8.375478 loss_ctc 10.663870 loss_rnnt 2.918795 hw_loss 0.299423 lr 0.00029295 rank 1
2023-02-27 16:02:51,788 DEBUG TRAIN Batch 44/11900 loss 6.737684 loss_att 8.712132 loss_ctc 9.380161 loss_rnnt 5.773249 hw_loss 0.407279 lr 0.00029290 rank 3
2023-02-27 16:02:51,788 DEBUG TRAIN Batch 44/11900 loss 10.881891 loss_att 15.427801 loss_ctc 17.323681 loss_rnnt 8.990521 hw_loss 0.231151 lr 0.00029290 rank 2
2023-02-27 16:02:51,788 DEBUG TRAIN Batch 44/11900 loss 8.553086 loss_att 13.105326 loss_ctc 12.149655 loss_rnnt 7.056540 hw_loss 0.199793 lr 0.00029290 rank 0
2023-02-27 16:02:51,792 DEBUG TRAIN Batch 44/11900 loss 9.704696 loss_att 10.785436 loss_ctc 14.411446 loss_rnnt 8.700089 hw_loss 0.301670 lr 0.00029290 rank 4
2023-02-27 16:02:51,794 DEBUG TRAIN Batch 44/11900 loss 8.384212 loss_att 12.190751 loss_ctc 21.176996 loss_rnnt 5.725437 hw_loss 0.359554 lr 0.00029290 rank 1
2023-02-27 16:04:13,242 DEBUG TRAIN Batch 44/12000 loss 6.771513 loss_att 9.796703 loss_ctc 11.972369 loss_rnnt 5.303804 hw_loss 0.317293 lr 0.00029285 rank 2
2023-02-27 16:04:13,246 DEBUG TRAIN Batch 44/12000 loss 12.662090 loss_att 14.281658 loss_ctc 18.943378 loss_rnnt 11.351334 hw_loss 0.280009 lr 0.00029285 rank 3
2023-02-27 16:04:13,246 DEBUG TRAIN Batch 44/12000 loss 6.422902 loss_att 9.612841 loss_ctc 12.075073 loss_rnnt 4.886241 hw_loss 0.271970 lr 0.00029285 rank 0
2023-02-27 16:04:13,248 DEBUG TRAIN Batch 44/12000 loss 10.878660 loss_att 14.041597 loss_ctc 19.611450 loss_rnnt 8.954314 hw_loss 0.238849 lr 0.00029285 rank 4
2023-02-27 16:04:13,255 DEBUG TRAIN Batch 44/12000 loss 13.036668 loss_att 18.536596 loss_ctc 25.947052 loss_rnnt 10.152946 hw_loss 0.116908 lr 0.00029285 rank 1
2023-02-27 16:06:09,009 DEBUG TRAIN Batch 44/12100 loss 7.944031 loss_att 11.221092 loss_ctc 11.482433 loss_rnnt 6.666340 hw_loss 0.282171 lr 0.00029280 rank 4
2023-02-27 16:06:09,014 DEBUG TRAIN Batch 44/12100 loss 9.156869 loss_att 12.987011 loss_ctc 22.540039 loss_rnnt 6.500506 hw_loss 0.198585 lr 0.00029280 rank 2
2023-02-27 16:06:09,015 DEBUG TRAIN Batch 44/12100 loss 7.798650 loss_att 8.465797 loss_ctc 11.186005 loss_rnnt 7.121367 hw_loss 0.172888 lr 0.00029280 rank 3
2023-02-27 16:06:09,016 DEBUG TRAIN Batch 44/12100 loss 9.198812 loss_att 11.840960 loss_ctc 18.383688 loss_rnnt 7.298871 hw_loss 0.275363 lr 0.00029280 rank 1
2023-02-27 16:06:09,017 DEBUG TRAIN Batch 44/12100 loss 9.879709 loss_att 10.727675 loss_ctc 18.068920 loss_rnnt 8.467556 hw_loss 0.282498 lr 0.00029280 rank 0
2023-02-27 16:07:32,332 DEBUG TRAIN Batch 44/12200 loss 8.467231 loss_att 12.396634 loss_ctc 12.720911 loss_rnnt 6.998466 hw_loss 0.216987 lr 0.00029275 rank 4
2023-02-27 16:07:32,336 DEBUG TRAIN Batch 44/12200 loss 8.510661 loss_att 10.610748 loss_ctc 12.995982 loss_rnnt 7.345455 hw_loss 0.275901 lr 0.00029275 rank 3
2023-02-27 16:07:32,338 DEBUG TRAIN Batch 44/12200 loss 1.986356 loss_att 4.627683 loss_ctc 2.623217 loss_rnnt 1.291760 hw_loss 0.152653 lr 0.00029275 rank 2
2023-02-27 16:07:32,343 DEBUG TRAIN Batch 44/12200 loss 7.424033 loss_att 12.649923 loss_ctc 15.310279 loss_rnnt 5.232737 hw_loss 0.177410 lr 0.00029275 rank 0
2023-02-27 16:07:32,384 DEBUG TRAIN Batch 44/12200 loss 6.727529 loss_att 6.881054 loss_ctc 8.584129 loss_rnnt 6.178477 hw_loss 0.507750 lr 0.00029275 rank 1
2023-02-27 16:08:54,086 DEBUG TRAIN Batch 44/12300 loss 6.593692 loss_att 8.387152 loss_ctc 7.127859 loss_rnnt 5.953428 hw_loss 0.394408 lr 0.00029270 rank 0
2023-02-27 16:08:54,088 DEBUG TRAIN Batch 44/12300 loss 11.908124 loss_att 15.000135 loss_ctc 22.104887 loss_rnnt 9.659357 hw_loss 0.507743 lr 0.00029270 rank 3
2023-02-27 16:08:54,091 DEBUG TRAIN Batch 44/12300 loss 9.050710 loss_att 10.901623 loss_ctc 18.407925 loss_rnnt 7.271350 hw_loss 0.302905 lr 0.00029270 rank 2
2023-02-27 16:08:54,091 DEBUG TRAIN Batch 44/12300 loss 3.271307 loss_att 4.565438 loss_ctc 6.038942 loss_rnnt 2.522730 hw_loss 0.226375 lr 0.00029270 rank 4
2023-02-27 16:08:54,094 DEBUG TRAIN Batch 44/12300 loss 2.188377 loss_att 4.902716 loss_ctc 2.903895 loss_rnnt 1.429455 hw_loss 0.226222 lr 0.00029270 rank 1
2023-02-27 16:10:19,114 DEBUG TRAIN Batch 44/12400 loss 6.301625 loss_att 11.014751 loss_ctc 11.086596 loss_rnnt 4.522295 hw_loss 0.372580 lr 0.00029265 rank 3
2023-02-27 16:10:19,118 DEBUG TRAIN Batch 44/12400 loss 9.162664 loss_att 11.994835 loss_ctc 15.493923 loss_rnnt 7.604292 hw_loss 0.277071 lr 0.00029265 rank 2
2023-02-27 16:10:19,122 DEBUG TRAIN Batch 44/12400 loss 9.996140 loss_att 13.201245 loss_ctc 16.830473 loss_rnnt 8.337352 hw_loss 0.199732 lr 0.00029265 rank 4
2023-02-27 16:10:19,135 DEBUG TRAIN Batch 44/12400 loss 6.362277 loss_att 11.164609 loss_ctc 9.782061 loss_rnnt 4.776024 hw_loss 0.318402 lr 0.00029265 rank 1
2023-02-27 16:10:19,150 DEBUG TRAIN Batch 44/12400 loss 7.723219 loss_att 11.195251 loss_ctc 11.603374 loss_rnnt 6.389320 hw_loss 0.229009 lr 0.00029265 rank 0
2023-02-27 16:12:13,651 DEBUG TRAIN Batch 44/12500 loss 4.473185 loss_att 6.937939 loss_ctc 7.002396 loss_rnnt 3.516066 hw_loss 0.238011 lr 0.00029260 rank 4
2023-02-27 16:12:13,651 DEBUG TRAIN Batch 44/12500 loss 2.581305 loss_att 5.800232 loss_ctc 3.883584 loss_rnnt 1.608063 hw_loss 0.292162 lr 0.00029260 rank 3
2023-02-27 16:12:13,654 DEBUG TRAIN Batch 44/12500 loss 9.400054 loss_att 11.707302 loss_ctc 15.665390 loss_rnnt 8.021469 hw_loss 0.153296 lr 0.00029260 rank 2
2023-02-27 16:12:13,655 DEBUG TRAIN Batch 44/12500 loss 13.839933 loss_att 16.901634 loss_ctc 26.527361 loss_rnnt 11.419790 hw_loss 0.217773 lr 0.00029260 rank 0
2023-02-27 16:12:13,662 DEBUG TRAIN Batch 44/12500 loss 6.397927 loss_att 8.570040 loss_ctc 8.409211 loss_rnnt 5.518373 hw_loss 0.331802 lr 0.00029260 rank 1
2023-02-27 16:13:34,823 DEBUG TRAIN Batch 44/12600 loss 8.781901 loss_att 12.379283 loss_ctc 16.569866 loss_rnnt 6.924976 hw_loss 0.185727 lr 0.00029255 rank 0
2023-02-27 16:13:34,825 DEBUG TRAIN Batch 44/12600 loss 7.675118 loss_att 7.660537 loss_ctc 11.860322 loss_rnnt 6.931234 hw_loss 0.353948 lr 0.00029255 rank 3
2023-02-27 16:13:34,829 DEBUG TRAIN Batch 44/12600 loss 7.033647 loss_att 8.790504 loss_ctc 11.786250 loss_rnnt 5.910465 hw_loss 0.258993 lr 0.00029255 rank 2
2023-02-27 16:13:34,831 DEBUG TRAIN Batch 44/12600 loss 9.098660 loss_att 13.380114 loss_ctc 15.588968 loss_rnnt 7.213609 hw_loss 0.306349 lr 0.00029255 rank 1
2023-02-27 16:13:34,874 DEBUG TRAIN Batch 44/12600 loss 6.556358 loss_att 8.645268 loss_ctc 11.489456 loss_rnnt 5.355811 hw_loss 0.234410 lr 0.00029255 rank 4
2023-02-27 16:14:57,298 DEBUG TRAIN Batch 44/12700 loss 7.086906 loss_att 12.851903 loss_ctc 10.548079 loss_rnnt 5.401166 hw_loss 0.133597 lr 0.00029250 rank 2
2023-02-27 16:14:57,298 DEBUG TRAIN Batch 44/12700 loss 9.050524 loss_att 9.653805 loss_ctc 13.414921 loss_rnnt 8.193291 hw_loss 0.289981 lr 0.00029250 rank 4
2023-02-27 16:14:57,301 DEBUG TRAIN Batch 44/12700 loss 7.365387 loss_att 8.782636 loss_ctc 10.003540 loss_rnnt 6.488194 hw_loss 0.453732 lr 0.00029250 rank 3
2023-02-27 16:14:57,305 DEBUG TRAIN Batch 44/12700 loss 7.561687 loss_att 9.424275 loss_ctc 14.247878 loss_rnnt 6.216424 hw_loss 0.152349 lr 0.00029250 rank 0
2023-02-27 16:14:57,330 DEBUG TRAIN Batch 44/12700 loss 6.679255 loss_att 8.132812 loss_ctc 8.235977 loss_rnnt 6.007864 hw_loss 0.324594 lr 0.00029250 rank 1
2023-02-27 16:16:55,313 DEBUG TRAIN Batch 44/12800 loss 9.669964 loss_att 10.565852 loss_ctc 13.766391 loss_rnnt 8.668317 hw_loss 0.518021 lr 0.00029245 rank 3
2023-02-27 16:16:55,313 DEBUG TRAIN Batch 44/12800 loss 12.899694 loss_att 15.886612 loss_ctc 25.400135 loss_rnnt 10.467847 hw_loss 0.314511 lr 0.00029245 rank 4
2023-02-27 16:16:55,314 DEBUG TRAIN Batch 44/12800 loss 8.378030 loss_att 10.768331 loss_ctc 13.263804 loss_rnnt 7.147857 hw_loss 0.188767 lr 0.00029245 rank 2
2023-02-27 16:16:55,318 DEBUG TRAIN Batch 44/12800 loss 4.730170 loss_att 7.031788 loss_ctc 7.582637 loss_rnnt 3.773359 hw_loss 0.217796 lr 0.00029245 rank 0
2023-02-27 16:16:55,318 DEBUG TRAIN Batch 44/12800 loss 10.347882 loss_att 10.160467 loss_ctc 15.059022 loss_rnnt 9.542281 hw_loss 0.403000 lr 0.00029245 rank 1
2023-02-27 16:18:18,222 DEBUG TRAIN Batch 44/12900 loss 10.260295 loss_att 12.945601 loss_ctc 13.123659 loss_rnnt 9.224132 hw_loss 0.219976 lr 0.00029240 rank 2
2023-02-27 16:18:18,225 DEBUG TRAIN Batch 44/12900 loss 4.331931 loss_att 8.272939 loss_ctc 7.450895 loss_rnnt 3.019113 hw_loss 0.203915 lr 0.00029240 rank 3
2023-02-27 16:18:18,230 DEBUG TRAIN Batch 44/12900 loss 6.795271 loss_att 10.261794 loss_ctc 8.898659 loss_rnnt 5.684175 hw_loss 0.257513 lr 0.00029240 rank 4
2023-02-27 16:18:18,231 DEBUG TRAIN Batch 44/12900 loss 3.779591 loss_att 6.001365 loss_ctc 7.608345 loss_rnnt 2.698993 hw_loss 0.235766 lr 0.00029240 rank 0
2023-02-27 16:18:18,234 DEBUG TRAIN Batch 44/12900 loss 7.605578 loss_att 9.759974 loss_ctc 7.878594 loss_rnnt 7.043877 hw_loss 0.177037 lr 0.00029240 rank 1
2023-02-27 16:19:45,086 DEBUG TRAIN Batch 44/13000 loss 7.315916 loss_att 11.135321 loss_ctc 12.755283 loss_rnnt 5.563543 hw_loss 0.493581 lr 0.00029235 rank 2
2023-02-27 16:19:45,088 DEBUG TRAIN Batch 44/13000 loss 9.444782 loss_att 18.593056 loss_ctc 20.844294 loss_rnnt 5.954834 hw_loss 0.263172 lr 0.00029235 rank 3
2023-02-27 16:19:45,090 DEBUG TRAIN Batch 44/13000 loss 5.193122 loss_att 7.022976 loss_ctc 9.240013 loss_rnnt 4.180070 hw_loss 0.201556 lr 0.00029235 rank 4
2023-02-27 16:19:45,091 DEBUG TRAIN Batch 44/13000 loss 6.392493 loss_att 11.854673 loss_ctc 14.358179 loss_rnnt 4.072473 hw_loss 0.310298 lr 0.00029235 rank 1
2023-02-27 16:19:45,101 DEBUG TRAIN Batch 44/13000 loss 11.435316 loss_att 14.038322 loss_ctc 18.404015 loss_rnnt 9.864341 hw_loss 0.227277 lr 0.00029235 rank 0
2023-02-27 16:21:08,405 DEBUG TRAIN Batch 44/13100 loss 10.835302 loss_att 14.083897 loss_ctc 16.312496 loss_rnnt 9.300630 hw_loss 0.289988 lr 0.00029230 rank 2
2023-02-27 16:21:08,406 DEBUG TRAIN Batch 44/13100 loss 1.971189 loss_att 3.961338 loss_ctc 2.655966 loss_rnnt 1.304294 hw_loss 0.332929 lr 0.00029230 rank 3
2023-02-27 16:21:08,409 DEBUG TRAIN Batch 44/13100 loss 8.843637 loss_att 14.291829 loss_ctc 22.978470 loss_rnnt 5.691688 hw_loss 0.333126 lr 0.00029230 rank 4
2023-02-27 16:21:08,458 DEBUG TRAIN Batch 44/13100 loss 11.438435 loss_att 13.387489 loss_ctc 19.543449 loss_rnnt 9.873112 hw_loss 0.177831 lr 0.00029230 rank 1
2023-02-27 16:21:08,473 DEBUG TRAIN Batch 44/13100 loss 4.129251 loss_att 6.320056 loss_ctc 7.603030 loss_rnnt 3.040408 hw_loss 0.351586 lr 0.00029230 rank 0
2023-02-27 16:22:32,485 DEBUG TRAIN Batch 44/13200 loss 18.677265 loss_att 18.197010 loss_ctc 28.856272 loss_rnnt 17.261436 hw_loss 0.290023 lr 0.00029225 rank 3
2023-02-27 16:22:32,492 DEBUG TRAIN Batch 44/13200 loss 15.705197 loss_att 22.558762 loss_ctc 33.213272 loss_rnnt 11.833672 hw_loss 0.312007 lr 0.00029225 rank 4
2023-02-27 16:22:32,493 DEBUG TRAIN Batch 44/13200 loss 2.504664 loss_att 6.184316 loss_ctc 4.939309 loss_rnnt 1.307791 hw_loss 0.255606 lr 0.00029225 rank 1
2023-02-27 16:22:32,494 DEBUG TRAIN Batch 44/13200 loss 11.107327 loss_att 12.533470 loss_ctc 18.807316 loss_rnnt 9.735996 hw_loss 0.111442 lr 0.00029225 rank 2
2023-02-27 16:22:32,523 DEBUG TRAIN Batch 44/13200 loss 9.140368 loss_att 11.439348 loss_ctc 13.502941 loss_rnnt 7.891515 hw_loss 0.388838 lr 0.00029225 rank 0
2023-02-27 16:23:53,932 DEBUG TRAIN Batch 44/13300 loss 3.879452 loss_att 6.919237 loss_ctc 8.460705 loss_rnnt 2.460683 hw_loss 0.374959 lr 0.00029220 rank 3
2023-02-27 16:23:53,933 DEBUG TRAIN Batch 44/13300 loss 11.218907 loss_att 12.841034 loss_ctc 16.809242 loss_rnnt 10.056809 hw_loss 0.173052 lr 0.00029220 rank 0
2023-02-27 16:23:53,935 DEBUG TRAIN Batch 44/13300 loss 8.910633 loss_att 10.376199 loss_ctc 12.230392 loss_rnnt 7.994848 hw_loss 0.337570 lr 0.00029220 rank 2
2023-02-27 16:23:53,935 DEBUG TRAIN Batch 44/13300 loss 7.828761 loss_att 10.828382 loss_ctc 10.528423 loss_rnnt 6.730382 hw_loss 0.259687 lr 0.00029220 rank 4
2023-02-27 16:23:53,936 DEBUG TRAIN Batch 44/13300 loss 10.296809 loss_att 12.261279 loss_ctc 16.389450 loss_rnnt 8.979939 hw_loss 0.209295 lr 0.00029220 rank 1
2023-02-27 16:24:46,798 DEBUG CV Batch 44/0 loss 1.520491 loss_att 1.175699 loss_ctc 1.825583 loss_rnnt 1.172816 hw_loss 0.704915 history loss 1.464177 rank 2
2023-02-27 16:24:46,798 DEBUG CV Batch 44/0 loss 1.520491 loss_att 1.175699 loss_ctc 1.825583 loss_rnnt 1.172816 hw_loss 0.704915 history loss 1.464177 rank 4
2023-02-27 16:24:46,799 DEBUG CV Batch 44/0 loss 1.520491 loss_att 1.175699 loss_ctc 1.825583 loss_rnnt 1.172816 hw_loss 0.704915 history loss 1.464177 rank 3
2023-02-27 16:24:46,851 DEBUG CV Batch 44/0 loss 1.520491 loss_att 1.175699 loss_ctc 1.825583 loss_rnnt 1.172816 hw_loss 0.704915 history loss 1.464177 rank 0
2023-02-27 16:24:46,858 DEBUG CV Batch 44/0 loss 1.520491 loss_att 1.175699 loss_ctc 1.825583 loss_rnnt 1.172816 hw_loss 0.704915 history loss 1.464177 rank 1
2023-02-27 16:24:59,757 DEBUG CV Batch 44/100 loss 3.232772 loss_att 4.215149 loss_ctc 7.427677 loss_rnnt 2.300075 hw_loss 0.331691 history loss 3.308817 rank 4
2023-02-27 16:24:59,787 DEBUG CV Batch 44/100 loss 3.232772 loss_att 4.215149 loss_ctc 7.427677 loss_rnnt 2.300075 hw_loss 0.331691 history loss 3.308817 rank 3
2023-02-27 16:25:00,945 DEBUG CV Batch 44/100 loss 3.232772 loss_att 4.215149 loss_ctc 7.427677 loss_rnnt 2.300075 hw_loss 0.331691 history loss 3.308817 rank 2
2023-02-27 16:25:01,263 DEBUG CV Batch 44/100 loss 3.232772 loss_att 4.215149 loss_ctc 7.427677 loss_rnnt 2.300075 hw_loss 0.331691 history loss 3.308817 rank 1
2023-02-27 16:25:02,492 DEBUG CV Batch 44/100 loss 3.232772 loss_att 4.215149 loss_ctc 7.427677 loss_rnnt 2.300075 hw_loss 0.331691 history loss 3.308817 rank 0
2023-02-27 16:25:16,415 DEBUG CV Batch 44/200 loss 8.854305 loss_att 13.438847 loss_ctc 11.411633 loss_rnnt 7.511045 hw_loss 0.160079 history loss 3.874270 rank 4
2023-02-27 16:25:16,434 DEBUG CV Batch 44/200 loss 8.854305 loss_att 13.438847 loss_ctc 11.411633 loss_rnnt 7.511045 hw_loss 0.160079 history loss 3.874270 rank 3
2023-02-27 16:25:16,453 DEBUG CV Batch 44/200 loss 8.854305 loss_att 13.438847 loss_ctc 11.411633 loss_rnnt 7.511045 hw_loss 0.160079 history loss 3.874270 rank 2
2023-02-27 16:25:16,842 DEBUG CV Batch 44/200 loss 8.854305 loss_att 13.438847 loss_ctc 11.411633 loss_rnnt 7.511045 hw_loss 0.160079 history loss 3.874270 rank 1
2023-02-27 16:25:18,589 DEBUG CV Batch 44/200 loss 8.854305 loss_att 13.438847 loss_ctc 11.411633 loss_rnnt 7.511045 hw_loss 0.160079 history loss 3.874270 rank 0
2023-02-27 16:25:30,906 DEBUG CV Batch 44/300 loss 3.757882 loss_att 4.724778 loss_ctc 7.134568 loss_rnnt 2.891371 hw_loss 0.417950 history loss 3.978679 rank 3
2023-02-27 16:25:31,171 DEBUG CV Batch 44/300 loss 3.757882 loss_att 4.724778 loss_ctc 7.134568 loss_rnnt 2.891371 hw_loss 0.417950 history loss 3.978679 rank 2
2023-02-27 16:25:31,172 DEBUG CV Batch 44/300 loss 3.757882 loss_att 4.724778 loss_ctc 7.134568 loss_rnnt 2.891371 hw_loss 0.417950 history loss 3.978679 rank 4
2023-02-27 16:25:31,376 DEBUG CV Batch 44/300 loss 3.757882 loss_att 4.724778 loss_ctc 7.134568 loss_rnnt 2.891371 hw_loss 0.417950 history loss 3.978679 rank 0
2023-02-27 16:25:31,632 DEBUG CV Batch 44/300 loss 3.757882 loss_att 4.724778 loss_ctc 7.134568 loss_rnnt 2.891371 hw_loss 0.417950 history loss 3.978679 rank 1
2023-02-27 16:25:47,239 DEBUG CV Batch 44/400 loss 14.371237 loss_att 49.652782 loss_ctc 9.539248 loss_rnnt 7.848115 hw_loss 0.208271 history loss 4.789673 rank 3
2023-02-27 16:25:47,247 DEBUG CV Batch 44/400 loss 14.371237 loss_att 49.652782 loss_ctc 9.539248 loss_rnnt 7.848115 hw_loss 0.208271 history loss 4.789673 rank 2
2023-02-27 16:25:47,247 DEBUG CV Batch 44/400 loss 14.371237 loss_att 49.652782 loss_ctc 9.539248 loss_rnnt 7.848115 hw_loss 0.208271 history loss 4.789673 rank 4
2023-02-27 16:25:47,256 DEBUG CV Batch 44/400 loss 14.371237 loss_att 49.652782 loss_ctc 9.539248 loss_rnnt 7.848115 hw_loss 0.208271 history loss 4.789673 rank 1
2023-02-27 16:25:47,265 DEBUG CV Batch 44/400 loss 14.371237 loss_att 49.652782 loss_ctc 9.539248 loss_rnnt 7.848115 hw_loss 0.208271 history loss 4.789673 rank 0
2023-02-27 16:26:00,644 DEBUG CV Batch 44/500 loss 5.752732 loss_att 4.978155 loss_ctc 6.665664 loss_rnnt 5.707505 hw_loss 0.147035 history loss 5.403991 rank 2
2023-02-27 16:26:00,784 DEBUG CV Batch 44/500 loss 5.752732 loss_att 4.978155 loss_ctc 6.665664 loss_rnnt 5.707505 hw_loss 0.147035 history loss 5.403991 rank 4
2023-02-27 16:26:00,794 DEBUG CV Batch 44/500 loss 5.752732 loss_att 4.978155 loss_ctc 6.665664 loss_rnnt 5.707505 hw_loss 0.147035 history loss 5.403991 rank 0
2023-02-27 16:26:00,823 DEBUG CV Batch 44/500 loss 5.752732 loss_att 4.978155 loss_ctc 6.665664 loss_rnnt 5.707505 hw_loss 0.147035 history loss 5.403991 rank 3
2023-02-27 16:26:01,408 DEBUG CV Batch 44/500 loss 5.752732 loss_att 4.978155 loss_ctc 6.665664 loss_rnnt 5.707505 hw_loss 0.147035 history loss 5.403991 rank 1
2023-02-27 16:26:16,950 DEBUG CV Batch 44/600 loss 7.866440 loss_att 7.420015 loss_ctc 10.981737 loss_rnnt 7.244250 hw_loss 0.555192 history loss 6.292585 rank 4
2023-02-27 16:26:16,952 DEBUG CV Batch 44/600 loss 7.866440 loss_att 7.420015 loss_ctc 10.981737 loss_rnnt 7.244250 hw_loss 0.555192 history loss 6.292585 rank 2
2023-02-27 16:26:16,981 DEBUG CV Batch 44/600 loss 7.866440 loss_att 7.420015 loss_ctc 10.981737 loss_rnnt 7.244250 hw_loss 0.555192 history loss 6.292585 rank 3
2023-02-27 16:26:16,988 DEBUG CV Batch 44/600 loss 7.866440 loss_att 7.420015 loss_ctc 10.981737 loss_rnnt 7.244250 hw_loss 0.555192 history loss 6.292585 rank 0
2023-02-27 16:26:17,126 DEBUG CV Batch 44/600 loss 7.866440 loss_att 7.420015 loss_ctc 10.981737 loss_rnnt 7.244250 hw_loss 0.555192 history loss 6.292585 rank 1
2023-02-27 16:26:32,376 DEBUG CV Batch 44/700 loss 8.433457 loss_att 20.902279 loss_ctc 10.678271 loss_rnnt 5.573067 hw_loss 0.126218 history loss 6.825994 rank 4
2023-02-27 16:26:32,397 DEBUG CV Batch 44/700 loss 8.433457 loss_att 20.902279 loss_ctc 10.678271 loss_rnnt 5.573067 hw_loss 0.126218 history loss 6.825994 rank 2
2023-02-27 16:26:32,445 DEBUG CV Batch 44/700 loss 8.433457 loss_att 20.902279 loss_ctc 10.678271 loss_rnnt 5.573067 hw_loss 0.126218 history loss 6.825994 rank 3
2023-02-27 16:26:32,707 DEBUG CV Batch 44/700 loss 8.433457 loss_att 20.902279 loss_ctc 10.678271 loss_rnnt 5.573067 hw_loss 0.126218 history loss 6.825994 rank 0
2023-02-27 16:26:32,921 DEBUG CV Batch 44/700 loss 8.433457 loss_att 20.902279 loss_ctc 10.678271 loss_rnnt 5.573067 hw_loss 0.126218 history loss 6.825994 rank 1
2023-02-27 16:26:44,439 DEBUG CV Batch 44/800 loss 6.421852 loss_att 6.985758 loss_ctc 13.747909 loss_rnnt 5.168615 hw_loss 0.306838 history loss 6.363708 rank 4
2023-02-27 16:26:44,755 DEBUG CV Batch 44/800 loss 6.421852 loss_att 6.985758 loss_ctc 13.747909 loss_rnnt 5.168615 hw_loss 0.306838 history loss 6.363708 rank 2
2023-02-27 16:26:44,854 DEBUG CV Batch 44/800 loss 6.421852 loss_att 6.985758 loss_ctc 13.747909 loss_rnnt 5.168615 hw_loss 0.306838 history loss 6.363708 rank 3
2023-02-27 16:26:45,219 DEBUG CV Batch 44/800 loss 6.421852 loss_att 6.985758 loss_ctc 13.747909 loss_rnnt 5.168615 hw_loss 0.306838 history loss 6.363708 rank 0
2023-02-27 16:26:45,653 DEBUG CV Batch 44/800 loss 6.421852 loss_att 6.985758 loss_ctc 13.747909 loss_rnnt 5.168615 hw_loss 0.306838 history loss 6.363708 rank 1
2023-02-27 16:26:58,411 DEBUG CV Batch 44/900 loss 11.267843 loss_att 14.079775 loss_ctc 18.664099 loss_rnnt 9.668321 hw_loss 0.095564 history loss 6.194171 rank 4
2023-02-27 16:26:58,839 DEBUG CV Batch 44/900 loss 11.267843 loss_att 14.079775 loss_ctc 18.664099 loss_rnnt 9.668321 hw_loss 0.095564 history loss 6.194171 rank 2
2023-02-27 16:26:59,064 DEBUG CV Batch 44/900 loss 11.267843 loss_att 14.079775 loss_ctc 18.664099 loss_rnnt 9.668321 hw_loss 0.095564 history loss 6.194171 rank 3
2023-02-27 16:26:59,509 DEBUG CV Batch 44/900 loss 11.267843 loss_att 14.079775 loss_ctc 18.664099 loss_rnnt 9.668321 hw_loss 0.095564 history loss 6.194171 rank 0
2023-02-27 16:27:00,644 DEBUG CV Batch 44/900 loss 11.267843 loss_att 14.079775 loss_ctc 18.664099 loss_rnnt 9.668321 hw_loss 0.095564 history loss 6.194171 rank 1
2023-02-27 16:27:11,748 DEBUG CV Batch 44/1000 loss 4.307255 loss_att 4.464930 loss_ctc 5.672874 loss_rnnt 3.938135 hw_loss 0.291566 history loss 5.995872 rank 2
2023-02-27 16:27:12,084 DEBUG CV Batch 44/1000 loss 4.307255 loss_att 4.464930 loss_ctc 5.672874 loss_rnnt 3.938135 hw_loss 0.291566 history loss 5.995872 rank 3
2023-02-27 16:27:12,551 DEBUG CV Batch 44/1000 loss 4.307255 loss_att 4.464930 loss_ctc 5.672874 loss_rnnt 3.938135 hw_loss 0.291566 history loss 5.995872 rank 0
2023-02-27 16:27:13,074 DEBUG CV Batch 44/1000 loss 4.307255 loss_att 4.464930 loss_ctc 5.672874 loss_rnnt 3.938135 hw_loss 0.291566 history loss 5.995872 rank 4
2023-02-27 16:27:15,556 DEBUG CV Batch 44/1000 loss 4.307255 loss_att 4.464930 loss_ctc 5.672874 loss_rnnt 3.938135 hw_loss 0.291566 history loss 5.995872 rank 1
2023-02-27 16:27:24,423 DEBUG CV Batch 44/1100 loss 5.260081 loss_att 4.723185 loss_ctc 8.582328 loss_rnnt 4.650288 hw_loss 0.514135 history loss 5.967942 rank 2
2023-02-27 16:27:24,900 DEBUG CV Batch 44/1100 loss 5.260081 loss_att 4.723185 loss_ctc 8.582328 loss_rnnt 4.650288 hw_loss 0.514135 history loss 5.967942 rank 3
2023-02-27 16:27:25,526 DEBUG CV Batch 44/1100 loss 5.260081 loss_att 4.723185 loss_ctc 8.582328 loss_rnnt 4.650288 hw_loss 0.514135 history loss 5.967942 rank 0
2023-02-27 16:27:25,549 DEBUG CV Batch 44/1100 loss 5.260081 loss_att 4.723185 loss_ctc 8.582328 loss_rnnt 4.650288 hw_loss 0.514135 history loss 5.967942 rank 4
2023-02-27 16:27:28,967 DEBUG CV Batch 44/1100 loss 5.260081 loss_att 4.723185 loss_ctc 8.582328 loss_rnnt 4.650288 hw_loss 0.514135 history loss 5.967942 rank 1
2023-02-27 16:27:35,573 DEBUG CV Batch 44/1200 loss 7.084435 loss_att 6.814049 loss_ctc 7.632334 loss_rnnt 6.952062 hw_loss 0.212621 history loss 6.240189 rank 2
2023-02-27 16:27:36,297 DEBUG CV Batch 44/1200 loss 7.084435 loss_att 6.814049 loss_ctc 7.632334 loss_rnnt 6.952062 hw_loss 0.212621 history loss 6.240189 rank 3
2023-02-27 16:27:36,873 DEBUG CV Batch 44/1200 loss 7.084435 loss_att 6.814049 loss_ctc 7.632334 loss_rnnt 6.952062 hw_loss 0.212621 history loss 6.240189 rank 4
2023-02-27 16:27:37,085 DEBUG CV Batch 44/1200 loss 7.084435 loss_att 6.814049 loss_ctc 7.632334 loss_rnnt 6.952062 hw_loss 0.212621 history loss 6.240189 rank 0
2023-02-27 16:27:41,044 DEBUG CV Batch 44/1200 loss 7.084435 loss_att 6.814049 loss_ctc 7.632334 loss_rnnt 6.952062 hw_loss 0.212621 history loss 6.240189 rank 1
2023-02-27 16:27:48,286 DEBUG CV Batch 44/1300 loss 4.901299 loss_att 4.837461 loss_ctc 7.096201 loss_rnnt 4.388947 hw_loss 0.435875 history loss 6.531530 rank 2
2023-02-27 16:27:48,967 DEBUG CV Batch 44/1300 loss 4.901299 loss_att 4.837461 loss_ctc 7.096201 loss_rnnt 4.388947 hw_loss 0.435875 history loss 6.531530 rank 3
2023-02-27 16:27:49,646 DEBUG CV Batch 44/1300 loss 4.901299 loss_att 4.837461 loss_ctc 7.096201 loss_rnnt 4.388947 hw_loss 0.435875 history loss 6.531530 rank 4
2023-02-27 16:27:50,923 DEBUG CV Batch 44/1300 loss 4.901299 loss_att 4.837461 loss_ctc 7.096201 loss_rnnt 4.388947 hw_loss 0.435875 history loss 6.531530 rank 0
2023-02-27 16:27:54,562 DEBUG CV Batch 44/1300 loss 4.901299 loss_att 4.837461 loss_ctc 7.096201 loss_rnnt 4.388947 hw_loss 0.435875 history loss 6.531530 rank 1
2023-02-27 16:28:00,257 DEBUG CV Batch 44/1400 loss 3.512416 loss_att 13.678860 loss_ctc 3.552166 loss_rnnt 1.256832 hw_loss 0.406868 history loss 6.804807 rank 2
2023-02-27 16:28:01,102 DEBUG CV Batch 44/1400 loss 3.512416 loss_att 13.678860 loss_ctc 3.552166 loss_rnnt 1.256832 hw_loss 0.406868 history loss 6.804807 rank 3
2023-02-27 16:28:01,490 DEBUG CV Batch 44/1400 loss 3.512416 loss_att 13.678860 loss_ctc 3.552166 loss_rnnt 1.256832 hw_loss 0.406868 history loss 6.804807 rank 4
2023-02-27 16:28:03,410 DEBUG CV Batch 44/1400 loss 3.512416 loss_att 13.678860 loss_ctc 3.552166 loss_rnnt 1.256832 hw_loss 0.406868 history loss 6.804807 rank 0
2023-02-27 16:28:07,244 DEBUG CV Batch 44/1400 loss 3.512416 loss_att 13.678860 loss_ctc 3.552166 loss_rnnt 1.256832 hw_loss 0.406868 history loss 6.804807 rank 1
2023-02-27 16:28:12,395 DEBUG CV Batch 44/1500 loss 6.113186 loss_att 6.565515 loss_ctc 5.604217 loss_rnnt 5.903288 hw_loss 0.351179 history loss 6.666837 rank 2
2023-02-27 16:28:13,210 DEBUG CV Batch 44/1500 loss 6.113186 loss_att 6.565515 loss_ctc 5.604217 loss_rnnt 5.903288 hw_loss 0.351179 history loss 6.666837 rank 3
2023-02-27 16:28:13,777 DEBUG CV Batch 44/1500 loss 6.113186 loss_att 6.565515 loss_ctc 5.604217 loss_rnnt 5.903288 hw_loss 0.351179 history loss 6.666837 rank 4
2023-02-27 16:28:16,500 DEBUG CV Batch 44/1500 loss 6.113186 loss_att 6.565515 loss_ctc 5.604217 loss_rnnt 5.903288 hw_loss 0.351179 history loss 6.666837 rank 0
2023-02-27 16:28:20,089 DEBUG CV Batch 44/1500 loss 6.113186 loss_att 6.565515 loss_ctc 5.604217 loss_rnnt 5.903288 hw_loss 0.351179 history loss 6.666837 rank 1
2023-02-27 16:28:26,342 DEBUG CV Batch 44/1600 loss 9.045783 loss_att 14.642690 loss_ctc 9.782265 loss_rnnt 7.681855 hw_loss 0.274403 history loss 6.622535 rank 2
2023-02-27 16:28:27,122 DEBUG CV Batch 44/1600 loss 9.045783 loss_att 14.642690 loss_ctc 9.782265 loss_rnnt 7.681855 hw_loss 0.274403 history loss 6.622535 rank 3
2023-02-27 16:28:27,578 DEBUG CV Batch 44/1600 loss 9.045783 loss_att 14.642690 loss_ctc 9.782265 loss_rnnt 7.681855 hw_loss 0.274403 history loss 6.622535 rank 4
2023-02-27 16:28:30,959 DEBUG CV Batch 44/1600 loss 9.045783 loss_att 14.642690 loss_ctc 9.782265 loss_rnnt 7.681855 hw_loss 0.274403 history loss 6.622535 rank 0
2023-02-27 16:28:34,284 DEBUG CV Batch 44/1600 loss 9.045783 loss_att 14.642690 loss_ctc 9.782265 loss_rnnt 7.681855 hw_loss 0.274403 history loss 6.622535 rank 1
2023-02-27 16:28:40,331 DEBUG CV Batch 44/1700 loss 7.995524 loss_att 7.621180 loss_ctc 14.907373 loss_rnnt 6.921612 hw_loss 0.426002 history loss 6.550807 rank 2
2023-02-27 16:28:40,895 DEBUG CV Batch 44/1700 loss 7.995524 loss_att 7.621180 loss_ctc 14.907373 loss_rnnt 6.921612 hw_loss 0.426002 history loss 6.550807 rank 4
2023-02-27 16:28:43,445 DEBUG CV Batch 44/1700 loss 7.995524 loss_att 7.621180 loss_ctc 14.907373 loss_rnnt 6.921612 hw_loss 0.426002 history loss 6.550807 rank 3
2023-02-27 16:28:46,691 DEBUG CV Batch 44/1700 loss 7.995524 loss_att 7.621180 loss_ctc 14.907373 loss_rnnt 6.921612 hw_loss 0.426002 history loss 6.550807 rank 0
2023-02-27 16:28:47,914 DEBUG CV Batch 44/1700 loss 7.995524 loss_att 7.621180 loss_ctc 14.907373 loss_rnnt 6.921612 hw_loss 0.426002 history loss 6.550807 rank 1
2023-02-27 16:28:50,321 INFO Epoch 44 CV info cv_loss 6.528030974949555
2023-02-27 16:28:50,322 INFO Epoch 45 TRAIN info lr 0.00029217642151641075
2023-02-27 16:28:50,325 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-27 16:28:50,732 INFO Epoch 44 CV info cv_loss 6.528030973943797
2023-02-27 16:28:50,733 INFO Epoch 45 TRAIN info lr 0.00029218041235697296
2023-02-27 16:28:50,736 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-27 16:28:53,463 INFO Epoch 44 CV info cv_loss 6.528030974682502
2023-02-27 16:28:53,464 INFO Epoch 45 TRAIN info lr 0.000292189392346188
2023-02-27 16:28:53,467 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-27 16:28:56,850 INFO Epoch 44 CV info cv_loss 6.528030970668083
2023-02-27 16:28:56,851 INFO Checkpoint: save to checkpoint exp/2_25_rnnt_bias_sepeate_2_class_3word_finetune/44.pt
2023-02-27 16:28:57,538 INFO Epoch 45 TRAIN info lr 0.00029218390447661617
2023-02-27 16:28:57,542 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-27 16:28:58,029 INFO Epoch 44 CV info cv_loss 6.528030972378088
2023-02-27 16:28:58,031 INFO Epoch 45 TRAIN info lr 0.000292189392346188
2023-02-27 16:28:58,036 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-27 16:30:15,295 DEBUG TRAIN Batch 45/0 loss 4.839887 loss_att 5.296367 loss_ctc 8.468034 loss_rnnt 4.053450 hw_loss 0.396352 lr 0.00029219 rank 3
2023-02-27 16:30:15,298 DEBUG TRAIN Batch 45/0 loss 9.967089 loss_att 9.787321 loss_ctc 13.904692 loss_rnnt 9.127766 hw_loss 0.656742 lr 0.00029218 rank 0
2023-02-27 16:30:15,300 DEBUG TRAIN Batch 45/0 loss 7.317831 loss_att 7.218284 loss_ctc 10.174921 loss_rnnt 6.628300 hw_loss 0.615927 lr 0.00029218 rank 2
2023-02-27 16:30:15,300 DEBUG TRAIN Batch 45/0 loss 5.999944 loss_att 6.191671 loss_ctc 9.597610 loss_rnnt 5.174365 hw_loss 0.576647 lr 0.00029218 rank 4
2023-02-27 16:30:15,391 DEBUG TRAIN Batch 45/0 loss 8.588432 loss_att 7.876078 loss_ctc 12.187051 loss_rnnt 7.959925 hw_loss 0.545932 lr 0.00029219 rank 1
2023-02-27 16:31:39,065 DEBUG TRAIN Batch 45/100 loss 10.564404 loss_att 14.459793 loss_ctc 15.718225 loss_rnnt 8.994797 hw_loss 0.193787 lr 0.00029213 rank 2
2023-02-27 16:31:39,070 DEBUG TRAIN Batch 45/100 loss 5.741124 loss_att 8.781852 loss_ctc 10.063613 loss_rnnt 4.478184 hw_loss 0.147117 lr 0.00029214 rank 1
2023-02-27 16:31:39,071 DEBUG TRAIN Batch 45/100 loss 1.920834 loss_att 4.492987 loss_ctc 3.250764 loss_rnnt 1.070125 hw_loss 0.298040 lr 0.00029213 rank 4
2023-02-27 16:31:39,072 DEBUG TRAIN Batch 45/100 loss 3.583911 loss_att 6.986537 loss_ctc 7.032719 loss_rnnt 2.287430 hw_loss 0.292714 lr 0.00029213 rank 0
2023-02-27 16:31:39,074 DEBUG TRAIN Batch 45/100 loss 3.742884 loss_att 5.845443 loss_ctc 5.792996 loss_rnnt 2.861396 hw_loss 0.351804 lr 0.00029214 rank 3
2023-02-27 16:33:02,893 DEBUG TRAIN Batch 45/200 loss 8.622935 loss_att 12.023948 loss_ctc 10.563982 loss_rnnt 7.555543 hw_loss 0.240720 lr 0.00029208 rank 2
2023-02-27 16:33:02,897 DEBUG TRAIN Batch 45/200 loss 4.348395 loss_att 7.348146 loss_ctc 11.237272 loss_rnnt 2.733079 hw_loss 0.181592 lr 0.00029209 rank 3
2023-02-27 16:33:02,899 DEBUG TRAIN Batch 45/200 loss 3.236809 loss_att 6.543331 loss_ctc 6.850801 loss_rnnt 1.881260 hw_loss 0.398211 lr 0.00029208 rank 4
2023-02-27 16:33:02,899 DEBUG TRAIN Batch 45/200 loss 13.897820 loss_att 16.796608 loss_ctc 18.045019 loss_rnnt 12.559828 hw_loss 0.384893 lr 0.00029208 rank 0
2023-02-27 16:33:02,900 DEBUG TRAIN Batch 45/200 loss 5.282723 loss_att 9.635538 loss_ctc 8.838438 loss_rnnt 3.866095 hw_loss 0.134944 lr 0.00029209 rank 1
2023-02-27 16:34:27,133 DEBUG TRAIN Batch 45/300 loss 4.197662 loss_att 7.731108 loss_ctc 8.219667 loss_rnnt 2.879537 hw_loss 0.140941 lr 0.00029204 rank 3
2023-02-27 16:34:27,135 DEBUG TRAIN Batch 45/300 loss 6.327888 loss_att 8.805267 loss_ctc 8.991430 loss_rnnt 5.397419 hw_loss 0.149726 lr 0.00029203 rank 4
2023-02-27 16:34:27,138 DEBUG TRAIN Batch 45/300 loss 7.403785 loss_att 9.843568 loss_ctc 14.826996 loss_rnnt 5.771906 hw_loss 0.289050 lr 0.00029203 rank 2
2023-02-27 16:34:27,138 DEBUG TRAIN Batch 45/300 loss 6.014899 loss_att 11.278127 loss_ctc 15.336697 loss_rnnt 3.532777 hw_loss 0.349818 lr 0.00029203 rank 0
2023-02-27 16:34:27,142 DEBUG TRAIN Batch 45/300 loss 6.016455 loss_att 8.012650 loss_ctc 14.353443 loss_rnnt 4.372365 hw_loss 0.249848 lr 0.00029204 rank 1
2023-02-27 16:35:53,686 DEBUG TRAIN Batch 45/400 loss 10.949621 loss_att 11.890825 loss_ctc 14.928075 loss_rnnt 10.130152 hw_loss 0.188940 lr 0.00029198 rank 2
2023-02-27 16:35:53,687 DEBUG TRAIN Batch 45/400 loss 5.908075 loss_att 10.312988 loss_ctc 13.163458 loss_rnnt 3.930077 hw_loss 0.243058 lr 0.00029198 rank 4
2023-02-27 16:35:53,689 DEBUG TRAIN Batch 45/400 loss 5.172312 loss_att 8.448925 loss_ctc 7.200793 loss_rnnt 4.198354 hw_loss 0.090322 lr 0.00029199 rank 3
2023-02-27 16:35:53,690 DEBUG TRAIN Batch 45/400 loss 7.919507 loss_att 11.693350 loss_ctc 14.344231 loss_rnnt 6.091569 hw_loss 0.406010 lr 0.00029199 rank 1
2023-02-27 16:35:53,692 DEBUG TRAIN Batch 45/400 loss 3.805482 loss_att 6.291033 loss_ctc 8.951135 loss_rnnt 2.468333 hw_loss 0.288658 lr 0.00029198 rank 0
2023-02-27 16:37:16,809 DEBUG TRAIN Batch 45/500 loss 5.663304 loss_att 7.749269 loss_ctc 9.685572 loss_rnnt 4.533605 hw_loss 0.330383 lr 0.00029194 rank 3
2023-02-27 16:37:16,810 DEBUG TRAIN Batch 45/500 loss 10.789718 loss_att 16.595644 loss_ctc 19.960762 loss_rnnt 8.262637 hw_loss 0.268294 lr 0.00029193 rank 2
2023-02-27 16:37:16,811 DEBUG TRAIN Batch 45/500 loss 5.011833 loss_att 7.858903 loss_ctc 8.984528 loss_rnnt 3.799251 hw_loss 0.212765 lr 0.00029194 rank 1
2023-02-27 16:37:16,817 DEBUG TRAIN Batch 45/500 loss 9.110875 loss_att 10.336253 loss_ctc 14.689053 loss_rnnt 7.907090 hw_loss 0.403035 lr 0.00029193 rank 0
2023-02-27 16:37:16,820 DEBUG TRAIN Batch 45/500 loss 8.465333 loss_att 9.586891 loss_ctc 12.596077 loss_rnnt 7.570264 hw_loss 0.224984 lr 0.00029193 rank 4
2023-02-27 16:38:39,853 DEBUG TRAIN Batch 45/600 loss 7.595692 loss_att 8.621222 loss_ctc 9.981249 loss_rnnt 6.858236 hw_loss 0.401765 lr 0.00029188 rank 0
2023-02-27 16:38:39,854 DEBUG TRAIN Batch 45/600 loss 6.121940 loss_att 8.866278 loss_ctc 8.986458 loss_rnnt 5.108264 hw_loss 0.155385 lr 0.00029188 rank 4
2023-02-27 16:38:39,854 DEBUG TRAIN Batch 45/600 loss 7.532244 loss_att 9.097227 loss_ctc 11.543372 loss_rnnt 6.495035 hw_loss 0.355117 lr 0.00029188 rank 2
2023-02-27 16:38:39,855 DEBUG TRAIN Batch 45/600 loss 11.169765 loss_att 11.969908 loss_ctc 14.555772 loss_rnnt 10.299286 hw_loss 0.485591 lr 0.00029189 rank 1
2023-02-27 16:38:39,861 DEBUG TRAIN Batch 45/600 loss 6.041131 loss_att 6.299280 loss_ctc 7.553752 loss_rnnt 5.651680 hw_loss 0.255260 lr 0.00029189 rank 3
2023-02-27 16:40:05,172 DEBUG TRAIN Batch 45/700 loss 7.663972 loss_att 10.644269 loss_ctc 8.907462 loss_rnnt 6.639729 hw_loss 0.491971 lr 0.00029183 rank 2
2023-02-27 16:40:05,173 DEBUG TRAIN Batch 45/700 loss 9.506275 loss_att 11.642492 loss_ctc 17.235912 loss_rnnt 7.886417 hw_loss 0.303743 lr 0.00029184 rank 3
2023-02-27 16:40:05,173 DEBUG TRAIN Batch 45/700 loss 9.878559 loss_att 13.779870 loss_ctc 15.219673 loss_rnnt 8.082760 hw_loss 0.568853 lr 0.00029183 rank 0
2023-02-27 16:40:05,178 DEBUG TRAIN Batch 45/700 loss 8.531186 loss_att 11.134301 loss_ctc 17.583401 loss_rnnt 6.657577 hw_loss 0.273796 lr 0.00029183 rank 4
2023-02-27 16:40:05,190 DEBUG TRAIN Batch 45/700 loss 6.963748 loss_att 9.909382 loss_ctc 10.677711 loss_rnnt 5.751340 hw_loss 0.240161 lr 0.00029184 rank 1
2023-02-27 16:41:26,637 DEBUG TRAIN Batch 45/800 loss 11.892912 loss_att 16.242008 loss_ctc 18.568571 loss_rnnt 9.978408 hw_loss 0.289870 lr 0.00029178 rank 2
2023-02-27 16:41:26,637 DEBUG TRAIN Batch 45/800 loss 6.813137 loss_att 9.696012 loss_ctc 8.975067 loss_rnnt 5.903584 hw_loss 0.083852 lr 0.00029179 rank 3
2023-02-27 16:41:26,641 DEBUG TRAIN Batch 45/800 loss 7.807084 loss_att 9.744020 loss_ctc 14.539477 loss_rnnt 6.358108 hw_loss 0.307380 lr 0.00029178 rank 4
2023-02-27 16:41:26,646 DEBUG TRAIN Batch 45/800 loss 10.648544 loss_att 17.550449 loss_ctc 19.852018 loss_rnnt 7.904276 hw_loss 0.256420 lr 0.00029179 rank 1
2023-02-27 16:41:26,664 DEBUG TRAIN Batch 45/800 loss 7.126849 loss_att 8.747644 loss_ctc 12.059052 loss_rnnt 5.944189 hw_loss 0.376637 lr 0.00029179 rank 0
2023-02-27 16:42:48,004 DEBUG TRAIN Batch 45/900 loss 10.242908 loss_att 10.708679 loss_ctc 11.207127 loss_rnnt 9.951963 hw_loss 0.129803 lr 0.00029173 rank 4
2023-02-27 16:42:48,005 DEBUG TRAIN Batch 45/900 loss 15.020222 loss_att 16.791145 loss_ctc 21.704592 loss_rnnt 13.624226 hw_loss 0.282303 lr 0.00029173 rank 2
2023-02-27 16:42:48,007 DEBUG TRAIN Batch 45/900 loss 8.250878 loss_att 8.946835 loss_ctc 14.738871 loss_rnnt 7.121525 hw_loss 0.234553 lr 0.00029174 rank 3
2023-02-27 16:42:48,007 DEBUG TRAIN Batch 45/900 loss 10.308020 loss_att 12.095530 loss_ctc 20.118504 loss_rnnt 8.438725 hw_loss 0.381990 lr 0.00029174 rank 0
2023-02-27 16:42:48,009 DEBUG TRAIN Batch 45/900 loss 9.631328 loss_att 14.473797 loss_ctc 16.865938 loss_rnnt 7.533166 hw_loss 0.309475 lr 0.00029174 rank 1
2023-02-27 16:44:10,828 DEBUG TRAIN Batch 45/1000 loss 8.676212 loss_att 12.802862 loss_ctc 14.580257 loss_rnnt 6.866241 hw_loss 0.370191 lr 0.00029168 rank 4
2023-02-27 16:44:10,830 DEBUG TRAIN Batch 45/1000 loss 14.108026 loss_att 18.048826 loss_ctc 19.881687 loss_rnnt 12.444036 hw_loss 0.198765 lr 0.00029168 rank 2
2023-02-27 16:44:10,833 DEBUG TRAIN Batch 45/1000 loss 4.436616 loss_att 7.944026 loss_ctc 8.045017 loss_rnnt 3.110535 hw_loss 0.269023 lr 0.00029169 rank 3
2023-02-27 16:44:10,832 DEBUG TRAIN Batch 45/1000 loss 4.536595 loss_att 6.719750 loss_ctc 8.618093 loss_rnnt 3.439032 hw_loss 0.218874 lr 0.00029169 rank 0
2023-02-27 16:44:10,834 DEBUG TRAIN Batch 45/1000 loss 6.892685 loss_att 9.726828 loss_ctc 10.035517 loss_rnnt 5.782302 hw_loss 0.233459 lr 0.00029169 rank 1
2023-02-27 16:45:35,911 DEBUG TRAIN Batch 45/1100 loss 4.248436 loss_att 7.637015 loss_ctc 7.025795 loss_rnnt 3.107803 hw_loss 0.173630 lr 0.00029163 rank 2
2023-02-27 16:45:35,911 DEBUG TRAIN Batch 45/1100 loss 3.875628 loss_att 6.104387 loss_ctc 5.339935 loss_rnnt 3.083266 hw_loss 0.283817 lr 0.00029164 rank 3
2023-02-27 16:45:35,914 DEBUG TRAIN Batch 45/1100 loss 2.572189 loss_att 5.953671 loss_ctc 4.308775 loss_rnnt 1.549715 hw_loss 0.214937 lr 0.00029164 rank 1
2023-02-27 16:45:35,918 DEBUG TRAIN Batch 45/1100 loss 7.252085 loss_att 10.505979 loss_ctc 13.937529 loss_rnnt 5.527889 hw_loss 0.341297 lr 0.00029163 rank 4
2023-02-27 16:45:35,920 DEBUG TRAIN Batch 45/1100 loss 8.946867 loss_att 11.449203 loss_ctc 12.379857 loss_rnnt 7.798695 hw_loss 0.356197 lr 0.00029164 rank 0
2023-02-27 16:46:57,171 DEBUG TRAIN Batch 45/1200 loss 3.603292 loss_att 5.354960 loss_ctc 5.755293 loss_rnnt 2.718280 hw_loss 0.464522 lr 0.00029159 rank 3
2023-02-27 16:46:57,172 DEBUG TRAIN Batch 45/1200 loss 10.151806 loss_att 11.413410 loss_ctc 11.714090 loss_rnnt 9.563516 hw_loss 0.239373 lr 0.00029158 rank 4
2023-02-27 16:46:57,172 DEBUG TRAIN Batch 45/1200 loss 5.646161 loss_att 8.408435 loss_ctc 9.484193 loss_rnnt 4.424984 hw_loss 0.294348 lr 0.00029158 rank 2
2023-02-27 16:46:57,173 DEBUG TRAIN Batch 45/1200 loss 7.599311 loss_att 10.588171 loss_ctc 11.577729 loss_rnnt 6.313473 hw_loss 0.295519 lr 0.00029159 rank 1
2023-02-27 16:46:57,175 DEBUG TRAIN Batch 45/1200 loss 13.203883 loss_att 14.875257 loss_ctc 18.015820 loss_rnnt 12.144223 hw_loss 0.157110 lr 0.00029159 rank 0
2023-02-27 16:48:19,479 DEBUG TRAIN Batch 45/1300 loss 8.129459 loss_att 16.287537 loss_ctc 11.040085 loss_rnnt 5.944715 hw_loss 0.309458 lr 0.00029153 rank 2
2023-02-27 16:48:19,482 DEBUG TRAIN Batch 45/1300 loss 9.861890 loss_att 16.316465 loss_ctc 19.182198 loss_rnnt 7.182838 hw_loss 0.272680 lr 0.00029154 rank 1
2023-02-27 16:48:19,483 DEBUG TRAIN Batch 45/1300 loss 6.769065 loss_att 9.894800 loss_ctc 11.140997 loss_rnnt 5.478703 hw_loss 0.154296 lr 0.00029154 rank 3
2023-02-27 16:48:19,485 DEBUG TRAIN Batch 45/1300 loss 5.492257 loss_att 6.260833 loss_ctc 6.795634 loss_rnnt 4.979204 hw_loss 0.347914 lr 0.00029153 rank 4
2023-02-27 16:48:19,485 DEBUG TRAIN Batch 45/1300 loss 6.916771 loss_att 9.888252 loss_ctc 11.097748 loss_rnnt 5.672153 hw_loss 0.174108 lr 0.00029154 rank 0
2023-02-27 16:49:45,601 DEBUG TRAIN Batch 45/1400 loss 6.803180 loss_att 9.675222 loss_ctc 12.867335 loss_rnnt 5.365968 hw_loss 0.101716 lr 0.00029149 rank 3
2023-02-27 16:49:45,603 DEBUG TRAIN Batch 45/1400 loss 7.164231 loss_att 11.309119 loss_ctc 10.733185 loss_rnnt 5.730683 hw_loss 0.241330 lr 0.00029149 rank 1
2023-02-27 16:49:45,605 DEBUG TRAIN Batch 45/1400 loss 5.258295 loss_att 7.882476 loss_ctc 8.465548 loss_rnnt 4.190025 hw_loss 0.217123 lr 0.00029149 rank 0
2023-02-27 16:49:45,606 DEBUG TRAIN Batch 45/1400 loss 21.642151 loss_att 22.462463 loss_ctc 29.921392 loss_rnnt 20.230663 hw_loss 0.269111 lr 0.00029148 rank 2
2023-02-27 16:49:45,612 DEBUG TRAIN Batch 45/1400 loss 7.894290 loss_att 9.999707 loss_ctc 7.673301 loss_rnnt 7.380220 hw_loss 0.229596 lr 0.00029148 rank 4
2023-02-27 16:51:08,905 DEBUG TRAIN Batch 45/1500 loss 12.198580 loss_att 17.159302 loss_ctc 21.068857 loss_rnnt 9.891598 hw_loss 0.247750 lr 0.00029144 rank 1
2023-02-27 16:51:08,905 DEBUG TRAIN Batch 45/1500 loss 7.138574 loss_att 11.524668 loss_ctc 14.374010 loss_rnnt 5.075813 hw_loss 0.414032 lr 0.00029144 rank 3
2023-02-27 16:51:08,907 DEBUG TRAIN Batch 45/1500 loss 16.594521 loss_att 19.746420 loss_ctc 31.479740 loss_rnnt 13.925727 hw_loss 0.100724 lr 0.00029143 rank 4
2023-02-27 16:51:08,908 DEBUG TRAIN Batch 45/1500 loss 2.147647 loss_att 4.492450 loss_ctc 2.976924 loss_rnnt 1.490450 hw_loss 0.145624 lr 0.00029144 rank 0
2023-02-27 16:51:08,910 DEBUG TRAIN Batch 45/1500 loss 6.666687 loss_att 9.600374 loss_ctc 12.182741 loss_rnnt 5.169975 hw_loss 0.327190 lr 0.00029143 rank 2
2023-02-27 16:52:30,076 DEBUG TRAIN Batch 45/1600 loss 9.869780 loss_att 13.696420 loss_ctc 16.297436 loss_rnnt 8.083093 hw_loss 0.308132 lr 0.00029139 rank 3
2023-02-27 16:52:30,081 DEBUG TRAIN Batch 45/1600 loss 4.428326 loss_att 6.599300 loss_ctc 6.298029 loss_rnnt 3.627419 hw_loss 0.220160 lr 0.00029139 rank 0
2023-02-27 16:52:30,082 DEBUG TRAIN Batch 45/1600 loss 4.567952 loss_att 8.626284 loss_ctc 7.787808 loss_rnnt 3.142071 hw_loss 0.346689 lr 0.00029139 rank 1
2023-02-27 16:52:30,082 DEBUG TRAIN Batch 45/1600 loss 10.132243 loss_att 11.257249 loss_ctc 14.699621 loss_rnnt 9.166370 hw_loss 0.247291 lr 0.00029138 rank 2
2023-02-27 16:52:30,084 DEBUG TRAIN Batch 45/1600 loss 6.786790 loss_att 7.083423 loss_ctc 6.437099 loss_rnnt 6.668535 hw_loss 0.197916 lr 0.00029138 rank 4
2023-02-27 16:53:53,632 DEBUG TRAIN Batch 45/1700 loss 5.654609 loss_att 8.477551 loss_ctc 9.011634 loss_rnnt 4.425475 hw_loss 0.406767 lr 0.00029133 rank 2
2023-02-27 16:53:53,636 DEBUG TRAIN Batch 45/1700 loss 8.392498 loss_att 10.594610 loss_ctc 17.562603 loss_rnnt 6.608111 hw_loss 0.227406 lr 0.00029134 rank 3
2023-02-27 16:53:53,636 DEBUG TRAIN Batch 45/1700 loss 9.547119 loss_att 11.136144 loss_ctc 11.623011 loss_rnnt 8.775378 hw_loss 0.332158 lr 0.00029134 rank 0
2023-02-27 16:53:53,655 DEBUG TRAIN Batch 45/1700 loss 2.333511 loss_att 3.328237 loss_ctc 6.093689 loss_rnnt 1.512852 hw_loss 0.225668 lr 0.00029134 rank 1
2023-02-27 16:53:53,662 DEBUG TRAIN Batch 45/1700 loss 3.943556 loss_att 7.948102 loss_ctc 8.615047 loss_rnnt 2.312870 hw_loss 0.387958 lr 0.00029134 rank 4
Terminated
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [192.168.0.18]:4526: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [192.168.0.18]:17308: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.18]:44429
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.18]:40111
