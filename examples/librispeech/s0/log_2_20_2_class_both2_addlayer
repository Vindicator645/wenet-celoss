/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_20_rnnt_bias_both_2_class_more_layers_fintune.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/ddp_init
2023-02-20 19:13:10,885 INFO training on multiple gpus, this gpu 1
2023-02-20 19:13:10,885 INFO training on multiple gpus, this gpu 5
2023-02-20 19:13:10,887 INFO training on multiple gpus, this gpu 7
2023-02-20 19:13:10,887 INFO training on multiple gpus, this gpu 3
2023-02-20 19:13:10,887 INFO training on multiple gpus, this gpu 6
2023-02-20 19:13:10,888 INFO training on multiple gpus, this gpu 0
2023-02-20 19:13:10,889 INFO training on multiple gpus, this gpu 2
2023-02-20 19:13:10,892 INFO training on multiple gpus, this gpu 4
2023-02-20 19:13:10,992 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-20 19:13:11,003 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-20 19:13:11,013 INFO Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-20 19:13:11,022 INFO Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-20 19:13:11,025 INFO Added key: store_based_barrier_key:1 to store for rank: 5
2023-02-20 19:13:11,027 INFO Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-20 19:13:11,036 INFO Added key: store_based_barrier_key:1 to store for rank: 6
2023-02-20 19:13:11,036 INFO Added key: store_based_barrier_key:1 to store for rank: 7
2023-02-20 19:13:11,037 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 19:13:11,038 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 19:13:11,040 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 19:13:11,041 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 19:13:11,041 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 19:13:11,044 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 19:13:11,045 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 19:13:11,046 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-20 19:13:23,383 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/10.pt for GPU
2023-02-20 19:13:23,410 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/10.pt for GPU
2023-02-20 19:13:23,436 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/10.pt for GPU
2023-02-20 19:13:23,461 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/10.pt for GPU
2023-02-20 19:13:23,486 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/10.pt for GPU
2023-02-20 19:13:23,510 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/10.pt for GPU
2023-02-20 19:13:23,536 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/10.pt for GPU
2023-02-20 19:13:23,567 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/10.pt for GPU
2023-02-20 19:13:38,019 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-20 19:13:38,020 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-20 19:13:38,021 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-20 19:13:38,023 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-20 19:13:38,023 INFO Epoch 11 TRAIN info lr 4e-08
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-20 19:13:38,025 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-20 19:13:38,076 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-20 19:13:38,078 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-20 19:13:38,245 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-20 19:13:38,247 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-20 19:13:38,274 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-20 19:13:38,276 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-20 19:13:38,418 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-20 19:13:38,420 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-20 19:13:38,490 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-20 19:13:38,493 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-20 19:14:51,971 DEBUG TRAIN Batch 11/0 loss 491.351746 loss_att 71.860268 loss_ctc 895.497437 loss_rnnt 520.801941 hw_loss 1.053758 lr 0.00052195 rank 6
2023-02-20 19:14:51,984 DEBUG TRAIN Batch 11/0 loss 632.099487 loss_att 83.636375 loss_ctc 871.550171 loss_rnnt 709.503479 hw_loss 0.678402 lr 0.00052195 rank 4
2023-02-20 19:14:52,006 DEBUG TRAIN Batch 11/0 loss 214.821777 loss_att 77.522469 loss_ctc 278.056335 loss_rnnt 233.470444 hw_loss 0.712348 lr 0.00052195 rank 1
2023-02-20 19:14:52,007 DEBUG TRAIN Batch 11/0 loss 198.188995 loss_att 84.111115 loss_ctc 300.616272 loss_rnnt 206.905731 hw_loss 0.828501 lr 0.00052195 rank 0
2023-02-20 19:14:52,009 DEBUG TRAIN Batch 11/0 loss 361.187866 loss_att 79.753059 loss_ctc 1021.896606 loss_rnnt 328.940552 hw_loss 0.824554 lr 0.00052195 rank 5
2023-02-20 19:14:52,010 DEBUG TRAIN Batch 11/0 loss 227.790497 loss_att 70.399452 loss_ctc 600.552612 loss_rnnt 209.195099 hw_loss 0.697448 lr 0.00052195 rank 3
2023-02-20 19:14:52,019 DEBUG TRAIN Batch 11/0 loss 403.472778 loss_att 78.294724 loss_ctc 690.224426 loss_rnnt 429.853149 hw_loss 0.790676 lr 0.00052195 rank 7
2023-02-20 19:14:52,130 DEBUG TRAIN Batch 11/0 loss 394.896484 loss_att 82.172195 loss_ctc 479.727295 loss_rnnt 445.655640 hw_loss 0.890500 lr 0.00052195 rank 2
2023-02-20 19:16:15,731 DEBUG TRAIN Batch 11/100 loss 245.600388 loss_att 310.733490 loss_ctc 231.198608 loss_rnnt 234.444626 hw_loss 0.092585 lr 0.00052167 rank 0
2023-02-20 19:16:15,732 DEBUG TRAIN Batch 11/100 loss 294.120514 loss_att 333.481445 loss_ctc 284.133057 loss_rnnt 287.466858 hw_loss 0.212077 lr 0.00052167 rank 4
2023-02-20 19:16:15,732 DEBUG TRAIN Batch 11/100 loss 194.965347 loss_att 239.247818 loss_ctc 185.444397 loss_rnnt 187.352753 hw_loss 0.047961 lr 0.00052167 rank 1
2023-02-20 19:16:15,735 DEBUG TRAIN Batch 11/100 loss 202.320053 loss_att 256.568970 loss_ctc 183.810654 loss_rnnt 193.843689 hw_loss 0.177186 lr 0.00052167 rank 2
2023-02-20 19:16:15,738 DEBUG TRAIN Batch 11/100 loss 259.941589 loss_att 296.076721 loss_ctc 252.846893 loss_rnnt 253.614258 hw_loss 0.086749 lr 0.00052167 rank 6
2023-02-20 19:16:15,738 DEBUG TRAIN Batch 11/100 loss 226.318436 loss_att 269.683411 loss_ctc 211.007416 loss_rnnt 219.661118 hw_loss 0.048363 lr 0.00052167 rank 7
2023-02-20 19:16:15,739 DEBUG TRAIN Batch 11/100 loss 267.839325 loss_att 319.678589 loss_ctc 251.498581 loss_rnnt 259.535645 hw_loss 0.214848 lr 0.00052167 rank 5
2023-02-20 19:16:15,740 DEBUG TRAIN Batch 11/100 loss 224.822052 loss_att 291.489197 loss_ctc 207.108017 loss_rnnt 213.763550 hw_loss 0.163056 lr 0.00052167 rank 3
2023-02-20 19:17:39,057 DEBUG TRAIN Batch 11/200 loss 138.905365 loss_att 252.406082 loss_ctc 115.873962 loss_rnnt 119.261780 hw_loss 0.026803 lr 0.00052139 rank 7
2023-02-20 19:17:39,057 DEBUG TRAIN Batch 11/200 loss 180.963226 loss_att 273.735657 loss_ctc 153.396469 loss_rnnt 166.047531 hw_loss 0.068979 lr 0.00052139 rank 4
2023-02-20 19:17:39,057 DEBUG TRAIN Batch 11/200 loss 168.561981 loss_att 289.210083 loss_ctc 149.163925 loss_rnnt 146.967499 hw_loss 0.096143 lr 0.00052139 rank 1
2023-02-20 19:17:39,057 DEBUG TRAIN Batch 11/200 loss 146.647446 loss_att 287.172058 loss_ctc 134.259262 loss_rnnt 120.157883 hw_loss 0.068251 lr 0.00052139 rank 5
2023-02-20 19:17:39,060 DEBUG TRAIN Batch 11/200 loss 131.495895 loss_att 255.772995 loss_ctc 112.971825 loss_rnnt 109.045776 hw_loss 0.121075 lr 0.00052139 rank 0
2023-02-20 19:17:39,062 DEBUG TRAIN Batch 11/200 loss 204.054306 loss_att 290.105164 loss_ctc 194.200912 loss_rnnt 188.132568 hw_loss 0.047521 lr 0.00052139 rank 2
2023-02-20 19:17:39,064 DEBUG TRAIN Batch 11/200 loss 135.988739 loss_att 234.020752 loss_ctc 117.382477 loss_rnnt 118.837204 hw_loss 0.048659 lr 0.00052139 rank 3
2023-02-20 19:17:39,106 DEBUG TRAIN Batch 11/200 loss 191.880798 loss_att 312.913757 loss_ctc 166.497345 loss_rnnt 170.970032 hw_loss 0.166151 lr 0.00052139 rank 6
2023-02-20 19:19:03,669 DEBUG TRAIN Batch 11/300 loss 131.839981 loss_att 255.998962 loss_ctc 108.749390 loss_rnnt 109.985504 hw_loss 0.190177 lr 0.00052110 rank 7
2023-02-20 19:19:03,670 DEBUG TRAIN Batch 11/300 loss 135.194473 loss_att 276.468018 loss_ctc 119.581093 loss_rnnt 108.955383 hw_loss 0.124069 lr 0.00052110 rank 4
2023-02-20 19:19:03,674 DEBUG TRAIN Batch 11/300 loss 120.148438 loss_att 244.783844 loss_ctc 104.117386 loss_rnnt 97.292007 hw_loss 0.125281 lr 0.00052110 rank 1
2023-02-20 19:19:03,674 DEBUG TRAIN Batch 11/300 loss 151.676819 loss_att 292.895691 loss_ctc 137.362366 loss_rnnt 125.274673 hw_loss 0.125526 lr 0.00052110 rank 0
2023-02-20 19:19:03,676 DEBUG TRAIN Batch 11/300 loss 133.170914 loss_att 274.275391 loss_ctc 119.071991 loss_rnnt 106.773636 hw_loss 0.105434 lr 0.00052110 rank 5
2023-02-20 19:19:03,676 DEBUG TRAIN Batch 11/300 loss 128.862442 loss_att 256.661407 loss_ctc 110.691986 loss_rnnt 105.540802 hw_loss 0.346064 lr 0.00052110 rank 2
2023-02-20 19:19:03,677 DEBUG TRAIN Batch 11/300 loss 112.362411 loss_att 246.694977 loss_ctc 88.606094 loss_rnnt 88.529297 hw_loss 0.251436 lr 0.00052110 rank 6
2023-02-20 19:19:03,679 DEBUG TRAIN Batch 11/300 loss 151.823486 loss_att 300.983521 loss_ctc 122.916504 loss_rnnt 125.774147 hw_loss 0.134221 lr 0.00052110 rank 3
2023-02-20 19:20:26,146 DEBUG TRAIN Batch 11/400 loss 110.068649 loss_att 232.203537 loss_ctc 102.985664 loss_rnnt 86.495377 hw_loss 0.170016 lr 0.00052082 rank 7
2023-02-20 19:20:26,149 DEBUG TRAIN Batch 11/400 loss 92.909065 loss_att 242.450638 loss_ctc 78.304398 loss_rnnt 64.915764 hw_loss 0.060522 lr 0.00052082 rank 2
2023-02-20 19:20:26,150 DEBUG TRAIN Batch 11/400 loss 130.810089 loss_att 270.539978 loss_ctc 113.064102 loss_rnnt 105.193695 hw_loss 0.068497 lr 0.00052082 rank 6
2023-02-20 19:20:26,153 DEBUG TRAIN Batch 11/400 loss 101.426643 loss_att 244.654419 loss_ctc 87.057663 loss_rnnt 74.604561 hw_loss 0.173216 lr 0.00052082 rank 0
2023-02-20 19:20:26,157 DEBUG TRAIN Batch 11/400 loss 106.744049 loss_att 239.403839 loss_ctc 95.673218 loss_rnnt 81.571358 hw_loss 0.219080 lr 0.00052082 rank 5
2023-02-20 19:20:26,180 DEBUG TRAIN Batch 11/400 loss 97.724739 loss_att 226.310089 loss_ctc 85.695740 loss_rnnt 73.537308 hw_loss 0.139177 lr 0.00052082 rank 4
2023-02-20 19:20:26,185 DEBUG TRAIN Batch 11/400 loss 99.648369 loss_att 242.703278 loss_ctc 72.263809 loss_rnnt 74.631943 hw_loss 0.106334 lr 0.00052082 rank 1
2023-02-20 19:20:26,204 DEBUG TRAIN Batch 11/400 loss 117.183685 loss_att 242.448700 loss_ctc 103.113319 loss_rnnt 93.920746 hw_loss 0.161209 lr 0.00052082 rank 3
2023-02-20 19:21:48,467 DEBUG TRAIN Batch 11/500 loss 63.011814 loss_att 174.051559 loss_ctc 53.174477 loss_rnnt 42.057354 hw_loss 0.109041 lr 0.00052054 rank 4
2023-02-20 19:21:48,468 DEBUG TRAIN Batch 11/500 loss 85.303009 loss_att 197.066528 loss_ctc 74.090652 loss_rnnt 64.400558 hw_loss 0.083874 lr 0.00052054 rank 2
2023-02-20 19:21:48,469 DEBUG TRAIN Batch 11/500 loss 78.870262 loss_att 206.764542 loss_ctc 60.964241 loss_rnnt 55.496944 hw_loss 0.341134 lr 0.00052054 rank 7
2023-02-20 19:21:48,475 DEBUG TRAIN Batch 11/500 loss 105.522316 loss_att 228.687775 loss_ctc 89.689850 loss_rnnt 82.892372 hw_loss 0.202191 lr 0.00052054 rank 6
2023-02-20 19:21:48,477 DEBUG TRAIN Batch 11/500 loss 87.491226 loss_att 221.373749 loss_ctc 80.499344 loss_rnnt 61.597404 hw_loss 0.092936 lr 0.00052054 rank 0
2023-02-20 19:21:48,484 DEBUG TRAIN Batch 11/500 loss 74.128250 loss_att 182.475098 loss_ctc 64.965271 loss_rnnt 53.564880 hw_loss 0.216973 lr 0.00052054 rank 5
2023-02-20 19:21:48,486 DEBUG TRAIN Batch 11/500 loss 80.445793 loss_att 184.974670 loss_ctc 71.405266 loss_rnnt 60.695850 hw_loss 0.092936 lr 0.00052054 rank 3
2023-02-20 19:21:48,525 DEBUG TRAIN Batch 11/500 loss 102.902245 loss_att 244.782074 loss_ctc 84.511551 loss_rnnt 76.932693 hw_loss 0.085662 lr 0.00052054 rank 1
2023-02-20 19:23:08,426 DEBUG TRAIN Batch 11/600 loss 66.014992 loss_att 155.314728 loss_ctc 52.852028 loss_rnnt 49.836334 hw_loss 0.138322 lr 0.00052026 rank 7
2023-02-20 19:23:08,429 DEBUG TRAIN Batch 11/600 loss 53.592018 loss_att 118.175369 loss_ctc 48.258553 loss_rnnt 41.327095 hw_loss 0.111346 lr 0.00052026 rank 0
2023-02-20 19:23:08,430 DEBUG TRAIN Batch 11/600 loss 45.680618 loss_att 85.588593 loss_ctc 42.488934 loss_rnnt 38.018394 hw_loss 0.199109 lr 0.00052026 rank 3
2023-02-20 19:23:08,431 DEBUG TRAIN Batch 11/600 loss 47.187363 loss_att 109.476135 loss_ctc 42.446487 loss_rnnt 35.222763 hw_loss 0.260557 lr 0.00052026 rank 2
2023-02-20 19:23:08,431 DEBUG TRAIN Batch 11/600 loss 42.249706 loss_att 92.329926 loss_ctc 32.204113 loss_rnnt 33.473595 hw_loss 0.186528 lr 0.00052026 rank 4
2023-02-20 19:23:08,431 DEBUG TRAIN Batch 11/600 loss 69.604591 loss_att 162.887222 loss_ctc 66.597954 loss_rnnt 51.265320 hw_loss 0.156797 lr 0.00052026 rank 1
2023-02-20 19:23:08,433 DEBUG TRAIN Batch 11/600 loss 68.398972 loss_att 128.466614 loss_ctc 63.475250 loss_rnnt 56.969074 hw_loss 0.136604 lr 0.00052026 rank 5
2023-02-20 19:23:08,435 DEBUG TRAIN Batch 11/600 loss 71.570847 loss_att 151.738037 loss_ctc 65.525177 loss_rnnt 56.171215 hw_loss 0.323030 lr 0.00052026 rank 6
2023-02-20 19:24:31,134 DEBUG TRAIN Batch 11/700 loss 68.497520 loss_att 166.316864 loss_ctc 54.144066 loss_rnnt 50.772316 hw_loss 0.140865 lr 0.00051997 rank 2
2023-02-20 19:24:31,140 DEBUG TRAIN Batch 11/700 loss 76.354774 loss_att 159.382202 loss_ctc 75.024429 loss_rnnt 59.880219 hw_loss 0.087098 lr 0.00051997 rank 0
2023-02-20 19:24:31,141 DEBUG TRAIN Batch 11/700 loss 87.077065 loss_att 172.209793 loss_ctc 77.763824 loss_rnnt 71.203819 hw_loss 0.165881 lr 0.00051997 rank 6
2023-02-20 19:24:31,145 DEBUG TRAIN Batch 11/700 loss 93.886681 loss_att 221.083984 loss_ctc 84.215698 loss_rnnt 69.657532 hw_loss 0.148403 lr 0.00051997 rank 5
2023-02-20 19:24:31,146 DEBUG TRAIN Batch 11/700 loss 79.907768 loss_att 180.301971 loss_ctc 67.967316 loss_rnnt 61.303276 hw_loss 0.220709 lr 0.00051997 rank 1
2023-02-20 19:24:31,187 DEBUG TRAIN Batch 11/700 loss 65.773140 loss_att 158.021530 loss_ctc 64.784889 loss_rnnt 47.364258 hw_loss 0.170582 lr 0.00051997 rank 7
2023-02-20 19:24:31,189 DEBUG TRAIN Batch 11/700 loss 84.924377 loss_att 183.464020 loss_ctc 68.122604 loss_rnnt 67.381561 hw_loss 0.140863 lr 0.00051997 rank 3
2023-02-20 19:24:31,205 DEBUG TRAIN Batch 11/700 loss 57.245995 loss_att 130.764069 loss_ctc 48.801449 loss_rnnt 43.579842 hw_loss 0.165882 lr 0.00051997 rank 4
2023-02-20 19:25:49,437 DEBUG TRAIN Batch 11/800 loss 56.072113 loss_att 124.709137 loss_ctc 55.655762 loss_rnnt 42.334141 hw_loss 0.123899 lr 0.00051969 rank 6
2023-02-20 19:25:49,440 DEBUG TRAIN Batch 11/800 loss 60.077293 loss_att 159.059296 loss_ctc 55.260674 loss_rnnt 40.898796 hw_loss 0.045590 lr 0.00051969 rank 7
2023-02-20 19:25:49,441 DEBUG TRAIN Batch 11/800 loss 72.120537 loss_att 157.808350 loss_ctc 75.655861 loss_rnnt 54.457470 hw_loss 0.101490 lr 0.00051969 rank 4
2023-02-20 19:25:49,443 DEBUG TRAIN Batch 11/800 loss 51.090729 loss_att 123.063484 loss_ctc 47.370392 loss_rnnt 37.143337 hw_loss 0.091665 lr 0.00051969 rank 0
2023-02-20 19:25:49,444 DEBUG TRAIN Batch 11/800 loss 53.902935 loss_att 127.568764 loss_ctc 43.484127 loss_rnnt 40.443771 hw_loss 0.215947 lr 0.00051969 rank 2
2023-02-20 19:25:49,444 DEBUG TRAIN Batch 11/800 loss 98.732117 loss_att 204.567535 loss_ctc 85.320457 loss_rnnt 79.312469 hw_loss 0.076482 lr 0.00051969 rank 3
2023-02-20 19:25:49,445 DEBUG TRAIN Batch 11/800 loss 63.845184 loss_att 167.792435 loss_ctc 45.766857 loss_rnnt 45.405487 hw_loss 0.113797 lr 0.00051969 rank 1
2023-02-20 19:25:49,453 DEBUG TRAIN Batch 11/800 loss 54.036930 loss_att 154.310898 loss_ctc 39.249886 loss_rnnt 35.880905 hw_loss 0.136579 lr 0.00051969 rank 5
2023-02-20 19:27:08,837 DEBUG TRAIN Batch 11/900 loss 75.020882 loss_att 136.308960 loss_ctc 70.796967 loss_rnnt 63.252827 hw_loss 0.138047 lr 0.00051941 rank 2
2023-02-20 19:27:08,838 DEBUG TRAIN Batch 11/900 loss 62.868874 loss_att 130.544464 loss_ctc 53.756294 loss_rnnt 50.468224 hw_loss 0.151028 lr 0.00051941 rank 5
2023-02-20 19:27:08,841 DEBUG TRAIN Batch 11/900 loss 61.320427 loss_att 134.769928 loss_ctc 57.281410 loss_rnnt 47.089600 hw_loss 0.148982 lr 0.00051941 rank 6
2023-02-20 19:27:08,841 DEBUG TRAIN Batch 11/900 loss 143.653809 loss_att 206.642181 loss_ctc 142.251678 loss_rnnt 131.220917 hw_loss 0.041597 lr 0.00051941 rank 7
2023-02-20 19:27:08,841 DEBUG TRAIN Batch 11/900 loss 75.537125 loss_att 135.740784 loss_ctc 74.865761 loss_rnnt 63.514080 hw_loss 0.134680 lr 0.00051941 rank 4
2023-02-20 19:27:08,843 DEBUG TRAIN Batch 11/900 loss 69.969086 loss_att 127.972748 loss_ctc 72.386490 loss_rnnt 57.991135 hw_loss 0.102934 lr 0.00051941 rank 3
2023-02-20 19:27:08,847 DEBUG TRAIN Batch 11/900 loss 80.373688 loss_att 152.186859 loss_ctc 77.786591 loss_rnnt 66.268967 hw_loss 0.163195 lr 0.00051941 rank 0
2023-02-20 19:27:08,855 DEBUG TRAIN Batch 11/900 loss 59.126328 loss_att 119.329613 loss_ctc 57.225227 loss_rnnt 47.239624 hw_loss 0.186613 lr 0.00051941 rank 1
2023-02-20 19:28:28,937 DEBUG TRAIN Batch 11/1000 loss 55.008896 loss_att 99.180763 loss_ctc 48.026371 loss_rnnt 46.963493 hw_loss 0.266325 lr 0.00051913 rank 4
2023-02-20 19:28:28,937 DEBUG TRAIN Batch 11/1000 loss 60.003132 loss_att 114.005081 loss_ctc 56.100548 loss_rnnt 49.675186 hw_loss 0.089804 lr 0.00051913 rank 2
2023-02-20 19:28:28,938 DEBUG TRAIN Batch 11/1000 loss 45.247055 loss_att 71.455681 loss_ctc 49.751045 loss_rnnt 39.328125 hw_loss 0.143760 lr 0.00051913 rank 6
2023-02-20 19:28:28,943 DEBUG TRAIN Batch 11/1000 loss 50.988766 loss_att 99.952049 loss_ctc 49.063950 loss_rnnt 41.348328 hw_loss 0.195790 lr 0.00051913 rank 3
2023-02-20 19:28:28,944 DEBUG TRAIN Batch 11/1000 loss 50.043598 loss_att 100.335876 loss_ctc 47.170006 loss_rnnt 40.224709 hw_loss 0.269216 lr 0.00051913 rank 1
2023-02-20 19:28:28,945 DEBUG TRAIN Batch 11/1000 loss 44.066353 loss_att 89.662003 loss_ctc 43.147709 loss_rnnt 34.975567 hw_loss 0.176516 lr 0.00051913 rank 7
2023-02-20 19:28:28,949 DEBUG TRAIN Batch 11/1000 loss 50.106064 loss_att 104.667740 loss_ctc 48.684334 loss_rnnt 39.289291 hw_loss 0.176250 lr 0.00051913 rank 5
2023-02-20 19:28:28,952 DEBUG TRAIN Batch 11/1000 loss 68.781311 loss_att 135.333893 loss_ctc 72.974968 loss_rnnt 54.886494 hw_loss 0.047142 lr 0.00051913 rank 0
2023-02-20 19:29:51,793 DEBUG TRAIN Batch 11/1100 loss 62.171398 loss_att 111.396370 loss_ctc 59.779633 loss_rnnt 52.609280 hw_loss 0.067547 lr 0.00051885 rank 4
2023-02-20 19:29:51,796 DEBUG TRAIN Batch 11/1100 loss 28.461864 loss_att 54.748367 loss_ctc 31.309435 loss_rnnt 22.773327 hw_loss 0.096678 lr 0.00051885 rank 1
2023-02-20 19:29:51,799 DEBUG TRAIN Batch 11/1100 loss 34.444530 loss_att 76.516846 loss_ctc 29.266430 loss_rnnt 26.639256 hw_loss 0.152296 lr 0.00051885 rank 0
2023-02-20 19:29:51,801 DEBUG TRAIN Batch 11/1100 loss 43.974098 loss_att 80.302055 loss_ctc 44.025238 loss_rnnt 36.667427 hw_loss 0.064237 lr 0.00051885 rank 7
2023-02-20 19:29:51,802 DEBUG TRAIN Batch 11/1100 loss 31.370823 loss_att 74.894455 loss_ctc 31.657127 loss_rnnt 22.585653 hw_loss 0.079254 lr 0.00051885 rank 2
2023-02-20 19:29:51,805 DEBUG TRAIN Batch 11/1100 loss 27.669992 loss_att 60.864773 loss_ctc 28.667492 loss_rnnt 20.780975 hw_loss 0.219494 lr 0.00051885 rank 3
2023-02-20 19:29:51,808 DEBUG TRAIN Batch 11/1100 loss 32.792587 loss_att 53.598232 loss_ctc 32.724220 loss_rnnt 28.565533 hw_loss 0.140705 lr 0.00051885 rank 5
2023-02-20 19:29:51,813 DEBUG TRAIN Batch 11/1100 loss 58.907185 loss_att 101.324158 loss_ctc 60.876347 loss_rnnt 50.149223 hw_loss 0.022527 lr 0.00051885 rank 6
2023-02-20 19:31:14,704 DEBUG TRAIN Batch 11/1200 loss 61.885693 loss_att 79.461288 loss_ctc 71.557610 loss_rnnt 56.948994 hw_loss 0.247479 lr 0.00051857 rank 4
2023-02-20 19:31:14,710 DEBUG TRAIN Batch 11/1200 loss 60.476166 loss_att 90.394005 loss_ctc 61.528400 loss_rnnt 54.303169 hw_loss 0.092115 lr 0.00051857 rank 3
2023-02-20 19:31:14,711 DEBUG TRAIN Batch 11/1200 loss 49.116249 loss_att 78.635910 loss_ctc 48.137085 loss_rnnt 43.269863 hw_loss 0.136886 lr 0.00051857 rank 2
2023-02-20 19:31:14,714 DEBUG TRAIN Batch 11/1200 loss 29.367357 loss_att 52.123859 loss_ctc 34.336575 loss_rnnt 24.010939 hw_loss 0.267295 lr 0.00051857 rank 7
2023-02-20 19:31:14,718 DEBUG TRAIN Batch 11/1200 loss 41.534847 loss_att 69.732132 loss_ctc 47.940628 loss_rnnt 34.986069 hw_loss 0.103535 lr 0.00051857 rank 5
2023-02-20 19:31:14,718 DEBUG TRAIN Batch 11/1200 loss 46.269688 loss_att 80.494797 loss_ctc 54.727943 loss_rnnt 38.152878 hw_loss 0.270046 lr 0.00051857 rank 0
2023-02-20 19:31:14,736 DEBUG TRAIN Batch 11/1200 loss 38.225937 loss_att 70.559494 loss_ctc 36.564957 loss_rnnt 31.820520 hw_loss 0.300314 lr 0.00051857 rank 6
2023-02-20 19:31:14,743 DEBUG TRAIN Batch 11/1200 loss 51.078491 loss_att 83.446373 loss_ctc 55.925854 loss_rnnt 43.890404 hw_loss 0.127872 lr 0.00051857 rank 1
2023-02-20 19:32:38,484 DEBUG TRAIN Batch 11/1300 loss 65.393295 loss_att 113.060379 loss_ctc 58.248184 loss_rnnt 56.758484 hw_loss 0.101386 lr 0.00051829 rank 7
2023-02-20 19:32:38,485 DEBUG TRAIN Batch 11/1300 loss 37.528591 loss_att 82.778656 loss_ctc 39.219204 loss_rnnt 28.240669 hw_loss 0.023420 lr 0.00051829 rank 2
2023-02-20 19:32:38,490 DEBUG TRAIN Batch 11/1300 loss 36.202061 loss_att 48.457981 loss_ctc 37.816746 loss_rnnt 33.454605 hw_loss 0.151838 lr 0.00051829 rank 1
2023-02-20 19:32:38,493 DEBUG TRAIN Batch 11/1300 loss 60.017021 loss_att 96.739899 loss_ctc 68.312157 loss_rnnt 51.553932 hw_loss 0.023420 lr 0.00051829 rank 0
2023-02-20 19:32:38,493 DEBUG TRAIN Batch 11/1300 loss 21.926838 loss_att 24.575798 loss_ctc 25.302263 loss_rnnt 20.713333 hw_loss 0.438110 lr 0.00051829 rank 4
2023-02-20 19:32:38,493 DEBUG TRAIN Batch 11/1300 loss 34.644661 loss_att 68.789520 loss_ctc 29.662910 loss_rnnt 28.429770 hw_loss 0.094031 lr 0.00051829 rank 3
2023-02-20 19:32:38,498 DEBUG TRAIN Batch 11/1300 loss 28.233759 loss_att 43.032684 loss_ctc 29.888508 loss_rnnt 24.976692 hw_loss 0.143712 lr 0.00051829 rank 6
2023-02-20 19:32:38,500 DEBUG TRAIN Batch 11/1300 loss 55.806744 loss_att 88.943184 loss_ctc 54.332745 loss_rnnt 49.317619 hw_loss 0.109454 lr 0.00051829 rank 5
2023-02-20 19:34:01,412 DEBUG TRAIN Batch 11/1400 loss 51.609402 loss_att 93.679382 loss_ctc 59.488060 loss_rnnt 42.102310 hw_loss 0.079887 lr 0.00051802 rank 7
2023-02-20 19:34:01,415 DEBUG TRAIN Batch 11/1400 loss 65.801819 loss_att 102.996544 loss_ctc 69.648514 loss_rnnt 57.639427 hw_loss 0.394789 lr 0.00051802 rank 6
2023-02-20 19:34:01,415 DEBUG TRAIN Batch 11/1400 loss 27.990669 loss_att 55.690025 loss_ctc 21.545815 loss_rnnt 23.272038 hw_loss 0.071385 lr 0.00051802 rank 1
2023-02-20 19:34:01,418 DEBUG TRAIN Batch 11/1400 loss 50.333546 loss_att 77.708092 loss_ctc 50.289131 loss_rnnt 44.761894 hw_loss 0.192494 lr 0.00051802 rank 0
2023-02-20 19:34:01,419 DEBUG TRAIN Batch 11/1400 loss 36.343727 loss_att 64.841248 loss_ctc 34.115089 loss_rnnt 30.927013 hw_loss 0.026930 lr 0.00051802 rank 4
2023-02-20 19:34:01,422 DEBUG TRAIN Batch 11/1400 loss 50.645058 loss_att 78.172844 loss_ctc 41.590477 loss_rnnt 46.301437 hw_loss 0.085009 lr 0.00051802 rank 3
2023-02-20 19:34:01,428 DEBUG TRAIN Batch 11/1400 loss 47.847046 loss_att 77.690216 loss_ctc 44.359112 loss_rnnt 42.230614 hw_loss 0.211605 lr 0.00051802 rank 5
2023-02-20 19:34:01,433 DEBUG TRAIN Batch 11/1400 loss 30.392702 loss_att 56.760586 loss_ctc 30.494156 loss_rnnt 25.052044 hw_loss 0.100416 lr 0.00051802 rank 2
2023-02-20 19:35:24,343 DEBUG TRAIN Batch 11/1500 loss 34.602539 loss_att 54.231304 loss_ctc 35.717255 loss_rnnt 30.477962 hw_loss 0.094123 lr 0.00051774 rank 7
2023-02-20 19:35:24,343 DEBUG TRAIN Batch 11/1500 loss 31.238544 loss_att 53.705715 loss_ctc 34.046886 loss_rnnt 26.258667 hw_loss 0.209997 lr 0.00051774 rank 3
2023-02-20 19:35:24,346 DEBUG TRAIN Batch 11/1500 loss 29.526176 loss_att 63.777031 loss_ctc 34.994411 loss_rnnt 21.901077 hw_loss 0.085931 lr 0.00051774 rank 2
2023-02-20 19:35:24,350 DEBUG TRAIN Batch 11/1500 loss 37.561958 loss_att 61.879471 loss_ctc 38.351479 loss_rnnt 32.525299 hw_loss 0.127286 lr 0.00051774 rank 0
2023-02-20 19:35:24,352 DEBUG TRAIN Batch 11/1500 loss 44.164421 loss_att 80.361603 loss_ctc 47.636745 loss_rnnt 36.423813 hw_loss 0.071614 lr 0.00051774 rank 4
2023-02-20 19:35:24,352 DEBUG TRAIN Batch 11/1500 loss 28.522898 loss_att 50.025200 loss_ctc 25.200817 loss_rnnt 24.652634 hw_loss 0.023898 lr 0.00051774 rank 5
2023-02-20 19:35:24,355 DEBUG TRAIN Batch 11/1500 loss 45.838505 loss_att 63.925797 loss_ctc 54.649376 loss_rnnt 41.033516 hw_loss 0.023898 lr 0.00051774 rank 1
2023-02-20 19:35:24,401 DEBUG TRAIN Batch 11/1500 loss 46.024902 loss_att 87.337334 loss_ctc 44.320110 loss_rnnt 37.888748 hw_loss 0.189321 lr 0.00051774 rank 6
2023-02-20 19:36:45,741 DEBUG TRAIN Batch 11/1600 loss 36.439182 loss_att 68.874573 loss_ctc 40.864670 loss_rnnt 29.338146 hw_loss 0.044794 lr 0.00051746 rank 1
2023-02-20 19:36:45,747 DEBUG TRAIN Batch 11/1600 loss 42.220108 loss_att 58.483624 loss_ctc 44.164001 loss_rnnt 38.597466 hw_loss 0.207661 lr 0.00051746 rank 4
2023-02-20 19:36:45,749 DEBUG TRAIN Batch 11/1600 loss 33.646870 loss_att 58.241066 loss_ctc 36.557240 loss_rnnt 28.271233 hw_loss 0.128905 lr 0.00051746 rank 0
2023-02-20 19:36:45,749 DEBUG TRAIN Batch 11/1600 loss 38.780201 loss_att 54.461639 loss_ctc 40.193680 loss_rnnt 35.373428 hw_loss 0.153785 lr 0.00051746 rank 2
2023-02-20 19:36:45,751 DEBUG TRAIN Batch 11/1600 loss 28.124226 loss_att 49.147919 loss_ctc 27.337292 loss_rnnt 24.000576 hw_loss 0.044693 lr 0.00051746 rank 5
2023-02-20 19:36:45,751 DEBUG TRAIN Batch 11/1600 loss 40.286907 loss_att 66.519470 loss_ctc 41.247704 loss_rnnt 34.834854 hw_loss 0.145185 lr 0.00051746 rank 3
2023-02-20 19:36:45,754 DEBUG TRAIN Batch 11/1600 loss 42.468472 loss_att 65.142067 loss_ctc 52.535507 loss_rnnt 36.553715 hw_loss 0.070807 lr 0.00051746 rank 6
2023-02-20 19:36:45,754 DEBUG TRAIN Batch 11/1600 loss 24.060635 loss_att 42.821232 loss_ctc 28.351238 loss_rnnt 19.612896 hw_loss 0.231635 lr 0.00051746 rank 7
2023-02-20 19:38:09,239 DEBUG TRAIN Batch 11/1700 loss 35.190975 loss_att 56.223927 loss_ctc 35.530029 loss_rnnt 30.845964 hw_loss 0.174767 lr 0.00051718 rank 2
2023-02-20 19:38:09,241 DEBUG TRAIN Batch 11/1700 loss 21.760685 loss_att 37.173771 loss_ctc 23.110826 loss_rnnt 18.425997 hw_loss 0.135098 lr 0.00051718 rank 7
2023-02-20 19:38:09,245 DEBUG TRAIN Batch 11/1700 loss 29.623028 loss_att 46.830353 loss_ctc 26.730740 loss_rnnt 26.544670 hw_loss 0.042241 lr 0.00051718 rank 1
2023-02-20 19:38:09,249 DEBUG TRAIN Batch 11/1700 loss 36.594471 loss_att 58.529831 loss_ctc 39.694313 loss_rnnt 31.699404 hw_loss 0.177522 lr 0.00051718 rank 6
2023-02-20 19:38:09,250 DEBUG TRAIN Batch 11/1700 loss 22.052975 loss_att 42.584038 loss_ctc 22.109392 loss_rnnt 17.925369 hw_loss 0.026007 lr 0.00051718 rank 4
2023-02-20 19:38:09,252 DEBUG TRAIN Batch 11/1700 loss 44.892704 loss_att 61.632797 loss_ctc 43.073887 loss_rnnt 41.672699 hw_loss 0.214685 lr 0.00051718 rank 0
2023-02-20 19:38:09,255 DEBUG TRAIN Batch 11/1700 loss 18.088655 loss_att 35.848137 loss_ctc 15.343730 loss_rnnt 14.782047 hw_loss 0.226315 lr 0.00051718 rank 5
2023-02-20 19:38:09,259 DEBUG TRAIN Batch 11/1700 loss 16.996225 loss_att 31.995556 loss_ctc 15.843067 loss_rnnt 14.006163 hw_loss 0.269908 lr 0.00051718 rank 3
2023-02-20 19:39:34,642 DEBUG TRAIN Batch 11/1800 loss 23.612549 loss_att 34.866882 loss_ctc 23.360630 loss_rnnt 21.299536 hw_loss 0.179505 lr 0.00051691 rank 4
2023-02-20 19:39:34,643 DEBUG TRAIN Batch 11/1800 loss 37.287182 loss_att 49.432526 loss_ctc 43.188332 loss_rnnt 34.017242 hw_loss 0.101349 lr 0.00051691 rank 7
2023-02-20 19:39:34,643 DEBUG TRAIN Batch 11/1800 loss 43.491211 loss_att 55.577579 loss_ctc 49.257542 loss_rnnt 40.246731 hw_loss 0.109422 lr 0.00051691 rank 2
2023-02-20 19:39:34,645 DEBUG TRAIN Batch 11/1800 loss 20.575535 loss_att 34.460838 loss_ctc 18.810095 loss_rnnt 18.009417 hw_loss 0.045841 lr 0.00051691 rank 0
2023-02-20 19:39:34,649 DEBUG TRAIN Batch 11/1800 loss 25.804739 loss_att 48.402298 loss_ctc 29.185225 loss_rnnt 20.771917 hw_loss 0.117335 lr 0.00051691 rank 3
2023-02-20 19:39:34,651 DEBUG TRAIN Batch 11/1800 loss 31.114145 loss_att 44.753151 loss_ctc 36.925808 loss_rnnt 27.525227 hw_loss 0.161674 lr 0.00051691 rank 5
2023-02-20 19:39:34,650 DEBUG TRAIN Batch 11/1800 loss 37.230450 loss_att 58.970009 loss_ctc 34.057697 loss_rnnt 33.238773 hw_loss 0.125248 lr 0.00051691 rank 6
2023-02-20 19:39:34,652 DEBUG TRAIN Batch 11/1800 loss 57.646824 loss_att 73.036316 loss_ctc 57.534760 loss_rnnt 54.553642 hw_loss 0.056674 lr 0.00051691 rank 1
2023-02-20 19:40:57,878 DEBUG TRAIN Batch 11/1900 loss 23.046263 loss_att 31.322630 loss_ctc 24.127502 loss_rnnt 21.176313 hw_loss 0.132209 lr 0.00051663 rank 1
2023-02-20 19:40:57,883 DEBUG TRAIN Batch 11/1900 loss 18.033855 loss_att 22.824389 loss_ctc 19.927618 loss_rnnt 16.711828 hw_loss 0.208911 lr 0.00051663 rank 2
2023-02-20 19:40:57,884 DEBUG TRAIN Batch 11/1900 loss 28.131538 loss_att 33.225296 loss_ctc 27.977274 loss_rnnt 27.006020 hw_loss 0.238754 lr 0.00051663 rank 4
2023-02-20 19:40:57,886 DEBUG TRAIN Batch 11/1900 loss 22.405727 loss_att 30.900719 loss_ctc 26.851027 loss_rnnt 19.999588 hw_loss 0.214566 lr 0.00051663 rank 6
2023-02-20 19:40:57,886 DEBUG TRAIN Batch 11/1900 loss 21.022881 loss_att 24.856197 loss_ctc 23.020561 loss_rnnt 19.930765 hw_loss 0.110802 lr 0.00051663 rank 5
2023-02-20 19:40:57,888 DEBUG TRAIN Batch 11/1900 loss 15.468048 loss_att 21.984056 loss_ctc 17.629019 loss_rnnt 13.759031 hw_loss 0.220661 lr 0.00051663 rank 3
2023-02-20 19:40:57,889 DEBUG TRAIN Batch 11/1900 loss 50.231590 loss_att 78.075157 loss_ctc 52.083549 loss_rnnt 44.351265 hw_loss 0.121279 lr 0.00051663 rank 7
2023-02-20 19:40:57,936 DEBUG TRAIN Batch 11/1900 loss 27.729469 loss_att 36.934086 loss_ctc 26.724049 loss_rnnt 25.945671 hw_loss 0.144244 lr 0.00051663 rank 0
2023-02-20 19:42:20,417 DEBUG TRAIN Batch 11/2000 loss 35.540394 loss_att 60.313492 loss_ctc 27.702206 loss_rnnt 31.618628 hw_loss 0.022951 lr 0.00051636 rank 2
2023-02-20 19:42:20,417 DEBUG TRAIN Batch 11/2000 loss 25.650742 loss_att 37.870197 loss_ctc 27.746613 loss_rnnt 22.892485 hw_loss 0.065470 lr 0.00051636 rank 3
2023-02-20 19:42:20,420 DEBUG TRAIN Batch 11/2000 loss 62.548115 loss_att 86.054901 loss_ctc 69.306839 loss_rnnt 56.913673 hw_loss 0.059847 lr 0.00051636 rank 4
2023-02-20 19:42:20,421 DEBUG TRAIN Batch 11/2000 loss 12.623069 loss_att 36.668335 loss_ctc 9.588085 loss_rnnt 8.158657 hw_loss 0.112544 lr 0.00051636 rank 0
2023-02-20 19:42:20,421 DEBUG TRAIN Batch 11/2000 loss 40.301552 loss_att 54.359505 loss_ctc 42.130318 loss_rnnt 37.095490 hw_loss 0.282443 lr 0.00051636 rank 7
2023-02-20 19:42:20,427 DEBUG TRAIN Batch 11/2000 loss 48.235294 loss_att 75.150223 loss_ctc 44.188591 loss_rnnt 43.297298 hw_loss 0.177324 lr 0.00051636 rank 1
2023-02-20 19:42:20,427 DEBUG TRAIN Batch 11/2000 loss 27.563496 loss_att 41.272594 loss_ctc 30.784676 loss_rnnt 24.304213 hw_loss 0.164948 lr 0.00051636 rank 5
2023-02-20 19:42:20,475 DEBUG TRAIN Batch 11/2000 loss 25.582874 loss_att 40.409790 loss_ctc 27.171415 loss_rnnt 22.350285 hw_loss 0.103874 lr 0.00051636 rank 6
2023-02-20 19:43:45,820 DEBUG TRAIN Batch 11/2100 loss 40.813171 loss_att 60.862286 loss_ctc 46.537975 loss_rnnt 35.980019 hw_loss 0.112546 lr 0.00051608 rank 3
2023-02-20 19:43:45,824 DEBUG TRAIN Batch 11/2100 loss 17.624582 loss_att 24.972673 loss_ctc 22.132225 loss_rnnt 15.461401 hw_loss 0.173520 lr 0.00051608 rank 0
2023-02-20 19:43:45,841 DEBUG TRAIN Batch 11/2100 loss 23.210917 loss_att 38.770576 loss_ctc 24.199545 loss_rnnt 19.943989 hw_loss 0.043461 lr 0.00051608 rank 5
2023-02-20 19:43:45,851 DEBUG TRAIN Batch 11/2100 loss 23.577394 loss_att 39.247086 loss_ctc 26.552786 loss_rnnt 19.905785 hw_loss 0.264289 lr 0.00051608 rank 1
2023-02-20 19:43:45,854 DEBUG TRAIN Batch 11/2100 loss 23.051163 loss_att 38.025276 loss_ctc 23.658440 loss_rnnt 19.862108 hw_loss 0.212363 lr 0.00051608 rank 4
2023-02-20 19:43:45,855 DEBUG TRAIN Batch 11/2100 loss 17.723139 loss_att 29.298628 loss_ctc 15.955855 loss_rnnt 15.618602 hw_loss 0.047022 lr 0.00051608 rank 7
2023-02-20 19:43:45,861 DEBUG TRAIN Batch 11/2100 loss 17.614445 loss_att 30.787189 loss_ctc 16.799501 loss_rnnt 15.017622 hw_loss 0.132997 lr 0.00051608 rank 2
2023-02-20 19:43:45,879 DEBUG TRAIN Batch 11/2100 loss 27.710552 loss_att 39.103920 loss_ctc 28.184202 loss_rnnt 25.251501 hw_loss 0.219794 lr 0.00051608 rank 6
2023-02-20 19:45:09,085 DEBUG TRAIN Batch 11/2200 loss 33.571011 loss_att 46.785561 loss_ctc 36.773743 loss_rnnt 30.417850 hw_loss 0.156031 lr 0.00051581 rank 2
2023-02-20 19:45:09,088 DEBUG TRAIN Batch 11/2200 loss 27.862774 loss_att 40.684002 loss_ctc 32.834572 loss_rnnt 24.549831 hw_loss 0.160860 lr 0.00051581 rank 1
2023-02-20 19:45:09,091 DEBUG TRAIN Batch 11/2200 loss 21.789518 loss_att 44.158180 loss_ctc 24.097389 loss_rnnt 16.968781 hw_loss 0.073668 lr 0.00051581 rank 7
2023-02-20 19:45:09,094 DEBUG TRAIN Batch 11/2200 loss 24.516056 loss_att 37.013802 loss_ctc 27.476250 loss_rnnt 21.582067 hw_loss 0.074525 lr 0.00051581 rank 4
2023-02-20 19:45:09,094 DEBUG TRAIN Batch 11/2200 loss 15.114300 loss_att 33.664474 loss_ctc 15.343917 loss_rnnt 11.277465 hw_loss 0.180347 lr 0.00051581 rank 0
2023-02-20 19:45:09,098 DEBUG TRAIN Batch 11/2200 loss 27.342590 loss_att 38.196903 loss_ctc 33.269794 loss_rnnt 24.319082 hw_loss 0.116909 lr 0.00051581 rank 5
2023-02-20 19:45:09,101 DEBUG TRAIN Batch 11/2200 loss 10.585195 loss_att 20.380905 loss_ctc 7.644294 loss_rnnt 8.914969 hw_loss 0.193505 lr 0.00051581 rank 6
2023-02-20 19:45:09,150 DEBUG TRAIN Batch 11/2200 loss 46.098358 loss_att 66.151085 loss_ctc 53.068443 loss_rnnt 41.106052 hw_loss 0.098287 lr 0.00051581 rank 3
2023-02-20 19:46:30,879 DEBUG TRAIN Batch 11/2300 loss 28.814758 loss_att 43.119106 loss_ctc 34.211342 loss_rnnt 25.174221 hw_loss 0.112729 lr 0.00051553 rank 2
2023-02-20 19:46:30,886 DEBUG TRAIN Batch 11/2300 loss 37.966709 loss_att 52.495277 loss_ctc 40.598675 loss_rnnt 34.652012 hw_loss 0.108851 lr 0.00051553 rank 1
2023-02-20 19:46:30,888 DEBUG TRAIN Batch 11/2300 loss 33.064426 loss_att 48.882290 loss_ctc 28.029564 loss_rnnt 30.513451 hw_loss 0.110101 lr 0.00051553 rank 6
2023-02-20 19:46:30,889 DEBUG TRAIN Batch 11/2300 loss 24.399313 loss_att 29.228836 loss_ctc 27.019516 loss_rnnt 23.016346 hw_loss 0.126941 lr 0.00051553 rank 0
2023-02-20 19:46:30,889 DEBUG TRAIN Batch 11/2300 loss 41.948898 loss_att 57.122871 loss_ctc 42.837414 loss_rnnt 38.763306 hw_loss 0.060619 lr 0.00051553 rank 4
2023-02-20 19:46:30,889 DEBUG TRAIN Batch 11/2300 loss 26.403786 loss_att 36.621178 loss_ctc 25.205036 loss_rnnt 24.477404 hw_loss 0.080126 lr 0.00051553 rank 3
2023-02-20 19:46:30,891 DEBUG TRAIN Batch 11/2300 loss 25.014969 loss_att 41.940639 loss_ctc 26.652714 loss_rnnt 21.317242 hw_loss 0.176676 lr 0.00051553 rank 5
2023-02-20 19:46:30,941 DEBUG TRAIN Batch 11/2300 loss 29.949535 loss_att 39.284546 loss_ctc 32.740692 loss_rnnt 27.624615 hw_loss 0.160807 lr 0.00051553 rank 7
2023-02-20 19:47:53,186 DEBUG TRAIN Batch 11/2400 loss 26.424868 loss_att 42.582973 loss_ctc 29.089321 loss_rnnt 22.771946 hw_loss 0.123822 lr 0.00051526 rank 7
2023-02-20 19:47:53,186 DEBUG TRAIN Batch 11/2400 loss 28.176819 loss_att 42.850845 loss_ctc 26.118616 loss_rnnt 25.418964 hw_loss 0.182764 lr 0.00051526 rank 3
2023-02-20 19:47:53,188 DEBUG TRAIN Batch 11/2400 loss 27.850922 loss_att 30.881948 loss_ctc 27.276278 loss_rnnt 27.273304 hw_loss 0.090060 lr 0.00051526 rank 4
2023-02-20 19:47:53,190 DEBUG TRAIN Batch 11/2400 loss 57.453529 loss_att 69.370735 loss_ctc 66.043427 loss_rnnt 53.881241 hw_loss 0.081618 lr 0.00051526 rank 1
2023-02-20 19:47:53,191 DEBUG TRAIN Batch 11/2400 loss 21.420517 loss_att 34.536430 loss_ctc 26.093224 loss_rnnt 18.098122 hw_loss 0.142853 lr 0.00051526 rank 2
2023-02-20 19:47:53,196 DEBUG TRAIN Batch 11/2400 loss 33.698090 loss_att 45.647728 loss_ctc 42.154636 loss_rnnt 30.150269 hw_loss 0.056911 lr 0.00051526 rank 0
2023-02-20 19:47:53,198 DEBUG TRAIN Batch 11/2400 loss 28.196226 loss_att 39.494198 loss_ctc 25.485415 loss_rnnt 26.185215 hw_loss 0.211605 lr 0.00051526 rank 5
2023-02-20 19:47:53,201 DEBUG TRAIN Batch 11/2400 loss 25.530844 loss_att 36.958973 loss_ctc 30.712654 loss_rnnt 22.475700 hw_loss 0.147392 lr 0.00051526 rank 6
2023-02-20 19:49:17,318 DEBUG TRAIN Batch 11/2500 loss 29.710293 loss_att 34.854675 loss_ctc 32.956638 loss_rnnt 28.199339 hw_loss 0.092312 lr 0.00051499 rank 1
2023-02-20 19:49:17,322 DEBUG TRAIN Batch 11/2500 loss 26.560213 loss_att 34.061916 loss_ctc 25.771793 loss_rnnt 25.092489 hw_loss 0.135952 lr 0.00051499 rank 0
2023-02-20 19:49:17,322 DEBUG TRAIN Batch 11/2500 loss 16.601908 loss_att 22.810024 loss_ctc 17.644192 loss_rnnt 15.154612 hw_loss 0.125063 lr 0.00051499 rank 3
2023-02-20 19:49:17,322 DEBUG TRAIN Batch 11/2500 loss 26.015810 loss_att 34.734863 loss_ctc 30.299465 loss_rnnt 23.613447 hw_loss 0.163870 lr 0.00051499 rank 2
2023-02-20 19:49:17,323 DEBUG TRAIN Batch 11/2500 loss 30.471914 loss_att 28.533878 loss_ctc 31.515774 loss_rnnt 30.664469 hw_loss 0.104756 lr 0.00051499 rank 7
2023-02-20 19:49:17,324 DEBUG TRAIN Batch 11/2500 loss 32.225292 loss_att 36.101635 loss_ctc 31.681950 loss_rnnt 31.435724 hw_loss 0.162640 lr 0.00051499 rank 4
2023-02-20 19:49:17,328 DEBUG TRAIN Batch 11/2500 loss 32.885185 loss_att 34.896255 loss_ctc 34.337494 loss_rnnt 32.170002 hw_loss 0.223731 lr 0.00051499 rank 6
2023-02-20 19:49:17,330 DEBUG TRAIN Batch 11/2500 loss 22.910883 loss_att 23.218674 loss_ctc 25.876760 loss_rnnt 22.395218 hw_loss 0.109977 lr 0.00051499 rank 5
2023-02-20 19:50:40,300 DEBUG TRAIN Batch 11/2600 loss 17.064802 loss_att 23.491186 loss_ctc 17.762892 loss_rnnt 15.604988 hw_loss 0.152734 lr 0.00051471 rank 2
2023-02-20 19:50:40,302 DEBUG TRAIN Batch 11/2600 loss 34.850124 loss_att 45.118698 loss_ctc 35.606445 loss_rnnt 32.641735 hw_loss 0.100940 lr 0.00051471 rank 4
2023-02-20 19:50:40,302 DEBUG TRAIN Batch 11/2600 loss 17.773800 loss_att 38.645065 loss_ctc 20.912273 loss_rnnt 13.145741 hw_loss 0.066268 lr 0.00051471 rank 6
2023-02-20 19:50:40,302 DEBUG TRAIN Batch 11/2600 loss 20.176659 loss_att 34.707577 loss_ctc 21.794247 loss_rnnt 16.924370 hw_loss 0.244552 lr 0.00051471 rank 7
2023-02-20 19:50:40,302 DEBUG TRAIN Batch 11/2600 loss 24.650286 loss_att 25.473040 loss_ctc 26.164667 loss_rnnt 24.111576 hw_loss 0.322950 lr 0.00051471 rank 0
2023-02-20 19:50:40,306 DEBUG TRAIN Batch 11/2600 loss 34.985909 loss_att 49.310936 loss_ctc 27.383205 loss_rnnt 33.087799 hw_loss 0.087737 lr 0.00051471 rank 1
2023-02-20 19:50:40,308 DEBUG TRAIN Batch 11/2600 loss 18.623463 loss_att 28.687809 loss_ctc 18.576336 loss_rnnt 16.561619 hw_loss 0.103606 lr 0.00051471 rank 3
2023-02-20 19:50:40,308 DEBUG TRAIN Batch 11/2600 loss 21.347719 loss_att 40.226418 loss_ctc 25.787258 loss_rnnt 16.925407 hw_loss 0.102438 lr 0.00051471 rank 5
2023-02-20 19:52:02,994 DEBUG TRAIN Batch 11/2700 loss 15.659211 loss_att 25.733171 loss_ctc 10.212779 loss_rnnt 14.235203 hw_loss 0.253890 lr 0.00051444 rank 6
2023-02-20 19:52:02,996 DEBUG TRAIN Batch 11/2700 loss 28.865166 loss_att 35.051270 loss_ctc 31.879578 loss_rnnt 27.179548 hw_loss 0.087140 lr 0.00051444 rank 4
2023-02-20 19:52:02,996 DEBUG TRAIN Batch 11/2700 loss 21.971483 loss_att 31.251209 loss_ctc 21.612995 loss_rnnt 20.072666 hw_loss 0.170008 lr 0.00051444 rank 2
2023-02-20 19:52:02,997 DEBUG TRAIN Batch 11/2700 loss 26.448301 loss_att 35.485386 loss_ctc 29.408386 loss_rnnt 24.231571 hw_loss 0.027441 lr 0.00051444 rank 5
2023-02-20 19:52:02,998 DEBUG TRAIN Batch 11/2700 loss 30.068470 loss_att 42.384880 loss_ctc 29.019041 loss_rnnt 27.673447 hw_loss 0.134367 lr 0.00051444 rank 1
2023-02-20 19:52:02,999 DEBUG TRAIN Batch 11/2700 loss 20.777573 loss_att 29.777245 loss_ctc 22.435463 loss_rnnt 18.721411 hw_loss 0.065957 lr 0.00051444 rank 0
2023-02-20 19:52:03,001 DEBUG TRAIN Batch 11/2700 loss 15.024111 loss_att 24.031902 loss_ctc 15.915579 loss_rnnt 13.089056 hw_loss 0.027441 lr 0.00051444 rank 3
2023-02-20 19:52:03,003 DEBUG TRAIN Batch 11/2700 loss 26.656124 loss_att 47.562542 loss_ctc 21.966557 loss_rnnt 23.024252 hw_loss 0.142246 lr 0.00051444 rank 7
2023-02-20 19:53:28,030 DEBUG TRAIN Batch 11/2800 loss 43.637077 loss_att 49.110107 loss_ctc 43.334927 loss_rnnt 42.556526 hw_loss 0.049177 lr 0.00051417 rank 4
2023-02-20 19:53:28,030 DEBUG TRAIN Batch 11/2800 loss 30.831039 loss_att 57.559380 loss_ctc 33.061310 loss_rnnt 25.130785 hw_loss 0.107279 lr 0.00051417 rank 7
2023-02-20 19:53:28,031 DEBUG TRAIN Batch 11/2800 loss 42.429886 loss_att 52.991734 loss_ctc 46.545662 loss_rnnt 39.689747 hw_loss 0.148112 lr 0.00051417 rank 3
2023-02-20 19:53:28,036 DEBUG TRAIN Batch 11/2800 loss 52.366985 loss_att 79.230003 loss_ctc 56.048222 loss_rnnt 46.394348 hw_loss 0.204762 lr 0.00051417 rank 2
2023-02-20 19:53:28,036 DEBUG TRAIN Batch 11/2800 loss 12.951296 loss_att 20.604246 loss_ctc 9.203721 loss_rnnt 11.817180 hw_loss 0.193506 lr 0.00051417 rank 1
2023-02-20 19:53:28,037 DEBUG TRAIN Batch 11/2800 loss 19.641741 loss_att 33.435097 loss_ctc 21.295540 loss_rnnt 16.646706 hw_loss 0.029733 lr 0.00051417 rank 0
2023-02-20 19:53:28,049 DEBUG TRAIN Batch 11/2800 loss 25.683168 loss_att 38.576862 loss_ctc 28.556822 loss_rnnt 22.650505 hw_loss 0.132692 lr 0.00051417 rank 5
2023-02-20 19:53:28,054 DEBUG TRAIN Batch 11/2800 loss 12.430504 loss_att 26.361094 loss_ctc 9.407215 loss_rnnt 9.999443 hw_loss 0.090089 lr 0.00051417 rank 6
2023-02-20 19:54:51,186 DEBUG TRAIN Batch 11/2900 loss 17.440979 loss_att 27.843412 loss_ctc 18.495716 loss_rnnt 15.176863 hw_loss 0.080625 lr 0.00051390 rank 2
2023-02-20 19:54:51,187 DEBUG TRAIN Batch 11/2900 loss 40.915741 loss_att 47.560562 loss_ctc 49.867165 loss_rnnt 38.311966 hw_loss 0.152418 lr 0.00051390 rank 4
2023-02-20 19:54:51,187 DEBUG TRAIN Batch 11/2900 loss 26.690987 loss_att 34.306343 loss_ctc 29.902439 loss_rnnt 24.664310 hw_loss 0.141394 lr 0.00051390 rank 1
2023-02-20 19:54:51,192 DEBUG TRAIN Batch 11/2900 loss 21.502010 loss_att 28.953354 loss_ctc 21.062725 loss_rnnt 19.950047 hw_loss 0.225497 lr 0.00051390 rank 7
2023-02-20 19:54:51,195 DEBUG TRAIN Batch 11/2900 loss 16.159975 loss_att 26.142071 loss_ctc 13.274027 loss_rnnt 14.515818 hw_loss 0.060992 lr 0.00051390 rank 6
2023-02-20 19:54:51,196 DEBUG TRAIN Batch 11/2900 loss 24.101917 loss_att 45.939613 loss_ctc 27.392561 loss_rnnt 19.255390 hw_loss 0.075443 lr 0.00051390 rank 0
2023-02-20 19:54:51,197 DEBUG TRAIN Batch 11/2900 loss 31.095285 loss_att 43.112362 loss_ctc 33.032196 loss_rnnt 28.343813 hw_loss 0.168381 lr 0.00051390 rank 5
2023-02-20 19:54:51,238 DEBUG TRAIN Batch 11/2900 loss 27.260914 loss_att 33.021790 loss_ctc 23.710579 loss_rnnt 26.488640 hw_loss 0.175272 lr 0.00051390 rank 3
2023-02-20 19:56:13,136 DEBUG TRAIN Batch 11/3000 loss 40.247921 loss_att 56.122105 loss_ctc 43.525375 loss_rnnt 36.571098 hw_loss 0.121864 lr 0.00051362 rank 1
2023-02-20 19:56:13,138 DEBUG TRAIN Batch 11/3000 loss 28.535439 loss_att 30.431194 loss_ctc 29.399572 loss_rnnt 27.943527 hw_loss 0.182893 lr 0.00051362 rank 7
2023-02-20 19:56:13,143 DEBUG TRAIN Batch 11/3000 loss 41.241974 loss_att 49.898670 loss_ctc 53.823738 loss_rnnt 37.742722 hw_loss 0.169394 lr 0.00051362 rank 2
2023-02-20 19:56:13,144 DEBUG TRAIN Batch 11/3000 loss 33.856441 loss_att 36.929012 loss_ctc 40.350555 loss_rnnt 32.336266 hw_loss 0.074586 lr 0.00051362 rank 4
2023-02-20 19:56:13,144 DEBUG TRAIN Batch 11/3000 loss 28.410423 loss_att 41.236893 loss_ctc 27.991787 loss_rnnt 25.872250 hw_loss 0.053809 lr 0.00051362 rank 0
2023-02-20 19:56:13,144 DEBUG TRAIN Batch 11/3000 loss 14.771591 loss_att 16.202246 loss_ctc 18.968573 loss_rnnt 13.821885 hw_loss 0.194958 lr 0.00051362 rank 3
2023-02-20 19:56:13,144 DEBUG TRAIN Batch 11/3000 loss 20.592287 loss_att 25.917864 loss_ctc 19.157656 loss_rnnt 19.644741 hw_loss 0.138218 lr 0.00051362 rank 6
2023-02-20 19:56:13,149 DEBUG TRAIN Batch 11/3000 loss 31.700741 loss_att 43.308399 loss_ctc 34.672775 loss_rnnt 28.953136 hw_loss 0.055882 lr 0.00051362 rank 5
2023-02-20 19:57:35,554 DEBUG TRAIN Batch 11/3100 loss 35.454975 loss_att 47.708347 loss_ctc 41.529861 loss_rnnt 32.106583 hw_loss 0.164503 lr 0.00051335 rank 3
2023-02-20 19:57:35,560 DEBUG TRAIN Batch 11/3100 loss 24.197912 loss_att 28.097820 loss_ctc 24.189781 loss_rnnt 23.360632 hw_loss 0.109467 lr 0.00051335 rank 4
2023-02-20 19:57:35,560 DEBUG TRAIN Batch 11/3100 loss 18.440823 loss_att 32.261475 loss_ctc 21.911982 loss_rnnt 15.153055 hw_loss 0.114028 lr 0.00051335 rank 1
2023-02-20 19:57:35,562 DEBUG TRAIN Batch 11/3100 loss 26.174603 loss_att 36.647663 loss_ctc 29.977711 loss_rnnt 23.488045 hw_loss 0.159121 lr 0.00051335 rank 6
2023-02-20 19:57:35,566 DEBUG TRAIN Batch 11/3100 loss 29.076727 loss_att 36.490471 loss_ctc 32.186699 loss_rnnt 27.146971 hw_loss 0.060648 lr 0.00051335 rank 0
2023-02-20 19:57:35,570 DEBUG TRAIN Batch 11/3100 loss 25.324350 loss_att 27.127972 loss_ctc 24.227682 loss_rnnt 25.024704 hw_loss 0.159649 lr 0.00051335 rank 2
2023-02-20 19:57:35,570 DEBUG TRAIN Batch 11/3100 loss 36.200016 loss_att 37.327728 loss_ctc 43.545780 loss_rnnt 34.888298 hw_loss 0.200134 lr 0.00051335 rank 7
2023-02-20 19:57:35,572 DEBUG TRAIN Batch 11/3100 loss 20.229090 loss_att 26.050461 loss_ctc 26.173508 loss_rnnt 18.226574 hw_loss 0.085597 lr 0.00051335 rank 5
2023-02-20 19:59:01,729 DEBUG TRAIN Batch 11/3200 loss 18.961912 loss_att 18.705883 loss_ctc 23.190413 loss_rnnt 18.376602 hw_loss 0.136343 lr 0.00051308 rank 1
2023-02-20 19:59:01,730 DEBUG TRAIN Batch 11/3200 loss 16.342937 loss_att 16.610149 loss_ctc 17.922180 loss_rnnt 15.919812 hw_loss 0.298341 lr 0.00051308 rank 4
2023-02-20 19:59:01,731 DEBUG TRAIN Batch 11/3200 loss 22.289738 loss_att 23.716387 loss_ctc 24.557987 loss_rnnt 21.556097 hw_loss 0.273516 lr 0.00051308 rank 2
2023-02-20 19:59:01,731 DEBUG TRAIN Batch 11/3200 loss 19.893520 loss_att 23.177637 loss_ctc 22.667507 loss_rnnt 18.851585 hw_loss 0.028588 lr 0.00051308 rank 0
2023-02-20 19:59:01,732 DEBUG TRAIN Batch 11/3200 loss 50.533058 loss_att 57.015911 loss_ctc 45.908325 loss_rnnt 49.818203 hw_loss 0.065466 lr 0.00051308 rank 6
2023-02-20 19:59:01,738 DEBUG TRAIN Batch 11/3200 loss 15.242652 loss_att 25.828856 loss_ctc 19.020172 loss_rnnt 12.510483 hw_loss 0.208609 lr 0.00051308 rank 5
2023-02-20 19:59:01,738 DEBUG TRAIN Batch 11/3200 loss 22.621513 loss_att 28.290470 loss_ctc 25.202686 loss_rnnt 21.071098 hw_loss 0.135875 lr 0.00051308 rank 3
2023-02-20 19:59:01,744 DEBUG TRAIN Batch 11/3200 loss 36.771908 loss_att 53.487946 loss_ctc 35.703026 loss_rnnt 33.545303 hw_loss 0.048592 lr 0.00051308 rank 7
2023-02-20 20:00:23,608 DEBUG TRAIN Batch 11/3300 loss 35.503510 loss_att 39.342903 loss_ctc 39.544899 loss_rnnt 34.097244 hw_loss 0.186621 lr 0.00051281 rank 1
2023-02-20 20:00:23,610 DEBUG TRAIN Batch 11/3300 loss 29.573662 loss_att 45.181198 loss_ctc 34.432590 loss_rnnt 25.757183 hw_loss 0.088343 lr 0.00051281 rank 2
2023-02-20 20:00:23,610 DEBUG TRAIN Batch 11/3300 loss 18.648726 loss_att 26.352425 loss_ctc 23.049574 loss_rnnt 16.481995 hw_loss 0.073517 lr 0.00051281 rank 7
2023-02-20 20:00:23,610 DEBUG TRAIN Batch 11/3300 loss 44.449039 loss_att 47.885391 loss_ctc 44.742279 loss_rnnt 43.681160 hw_loss 0.077834 lr 0.00051281 rank 3
2023-02-20 20:00:23,614 DEBUG TRAIN Batch 11/3300 loss 46.556942 loss_att 53.169075 loss_ctc 52.553688 loss_rnnt 44.356033 hw_loss 0.147970 lr 0.00051281 rank 5
2023-02-20 20:00:23,614 DEBUG TRAIN Batch 11/3300 loss 21.780668 loss_att 27.064770 loss_ctc 22.704336 loss_rnnt 20.524319 hw_loss 0.143198 lr 0.00051281 rank 6
2023-02-20 20:00:23,614 DEBUG TRAIN Batch 11/3300 loss 36.461147 loss_att 52.378456 loss_ctc 38.369179 loss_rnnt 33.010544 hw_loss 0.023883 lr 0.00051281 rank 4
2023-02-20 20:00:23,619 DEBUG TRAIN Batch 11/3300 loss 45.589390 loss_att 58.488976 loss_ctc 46.078197 loss_rnnt 42.907055 hw_loss 0.069841 lr 0.00051281 rank 0
2023-02-20 20:01:46,545 DEBUG TRAIN Batch 11/3400 loss 23.340767 loss_att 36.688595 loss_ctc 20.708502 loss_rnnt 21.007311 hw_loss 0.027856 lr 0.00051254 rank 4
2023-02-20 20:01:46,546 DEBUG TRAIN Batch 11/3400 loss 16.625887 loss_att 27.676807 loss_ctc 13.845839 loss_rnnt 14.771521 hw_loss 0.027856 lr 0.00051254 rank 2
2023-02-20 20:01:46,547 DEBUG TRAIN Batch 11/3400 loss 18.566053 loss_att 24.166815 loss_ctc 21.703634 loss_rnnt 16.931400 hw_loss 0.180291 lr 0.00051254 rank 3
2023-02-20 20:01:46,547 DEBUG TRAIN Batch 11/3400 loss 23.212687 loss_att 29.667545 loss_ctc 28.452415 loss_rnnt 21.102543 hw_loss 0.226010 lr 0.00051254 rank 7
2023-02-20 20:01:46,547 DEBUG TRAIN Batch 11/3400 loss 31.703424 loss_att 45.255226 loss_ctc 33.126450 loss_rnnt 28.735632 hw_loss 0.126930 lr 0.00051254 rank 1
2023-02-20 20:01:46,548 DEBUG TRAIN Batch 11/3400 loss 35.056515 loss_att 47.158600 loss_ctc 42.583843 loss_rnnt 31.567274 hw_loss 0.122213 lr 0.00051254 rank 0
2023-02-20 20:01:46,550 DEBUG TRAIN Batch 11/3400 loss 27.452116 loss_att 33.122864 loss_ctc 34.545105 loss_rnnt 25.332016 hw_loss 0.075412 lr 0.00051254 rank 5
2023-02-20 20:01:46,587 DEBUG TRAIN Batch 11/3400 loss 37.344151 loss_att 46.334366 loss_ctc 43.089653 loss_rnnt 34.701771 hw_loss 0.146749 lr 0.00051254 rank 6
2023-02-20 20:03:10,235 DEBUG TRAIN Batch 11/3500 loss 19.235294 loss_att 26.373487 loss_ctc 19.902227 loss_rnnt 17.665100 hw_loss 0.100554 lr 0.00051228 rank 7
2023-02-20 20:03:10,238 DEBUG TRAIN Batch 11/3500 loss 30.006821 loss_att 39.999668 loss_ctc 24.597797 loss_rnnt 28.673183 hw_loss 0.105511 lr 0.00051228 rank 1
2023-02-20 20:03:10,239 DEBUG TRAIN Batch 11/3500 loss 28.949177 loss_att 43.495094 loss_ctc 27.561047 loss_rnnt 26.193520 hw_loss 0.059165 lr 0.00051228 rank 4
2023-02-20 20:03:10,239 DEBUG TRAIN Batch 11/3500 loss 25.275564 loss_att 34.240814 loss_ctc 30.840055 loss_rnnt 22.708708 hw_loss 0.059767 lr 0.00051228 rank 6
2023-02-20 20:03:10,244 DEBUG TRAIN Batch 11/3500 loss 37.767750 loss_att 45.536343 loss_ctc 42.509552 loss_rnnt 35.560036 hw_loss 0.040789 lr 0.00051228 rank 3
2023-02-20 20:03:10,245 DEBUG TRAIN Batch 11/3500 loss 32.403831 loss_att 42.250854 loss_ctc 29.824041 loss_rnnt 30.715427 hw_loss 0.118062 lr 0.00051228 rank 0
2023-02-20 20:03:10,254 DEBUG TRAIN Batch 11/3500 loss 27.457144 loss_att 36.257729 loss_ctc 34.219986 loss_rnnt 24.740341 hw_loss 0.103075 lr 0.00051228 rank 5
2023-02-20 20:03:10,302 DEBUG TRAIN Batch 11/3500 loss 20.809177 loss_att 28.585878 loss_ctc 24.032166 loss_rnnt 18.743382 hw_loss 0.151357 lr 0.00051228 rank 2
2023-02-20 20:04:31,532 DEBUG TRAIN Batch 11/3600 loss 25.307800 loss_att 36.335926 loss_ctc 24.523674 loss_rnnt 23.185976 hw_loss 0.038899 lr 0.00051201 rank 1
2023-02-20 20:04:31,534 DEBUG TRAIN Batch 11/3600 loss 28.736834 loss_att 35.649090 loss_ctc 32.715176 loss_rnnt 26.752230 hw_loss 0.134453 lr 0.00051201 rank 4
2023-02-20 20:04:31,538 DEBUG TRAIN Batch 11/3600 loss 29.126167 loss_att 35.479126 loss_ctc 30.259361 loss_rnnt 27.649281 hw_loss 0.103503 lr 0.00051201 rank 3
2023-02-20 20:04:31,540 DEBUG TRAIN Batch 11/3600 loss 26.610430 loss_att 36.634148 loss_ctc 30.473705 loss_rnnt 24.057087 hw_loss 0.062798 lr 0.00051201 rank 7
2023-02-20 20:04:31,545 DEBUG TRAIN Batch 11/3600 loss 35.097416 loss_att 45.986229 loss_ctc 39.200783 loss_rnnt 32.298138 hw_loss 0.139501 lr 0.00051201 rank 5
2023-02-20 20:04:31,546 DEBUG TRAIN Batch 11/3600 loss 23.906837 loss_att 30.191467 loss_ctc 23.918953 loss_rnnt 22.573895 hw_loss 0.139503 lr 0.00051201 rank 2
2023-02-20 20:04:31,546 DEBUG TRAIN Batch 11/3600 loss 22.739880 loss_att 25.686987 loss_ctc 25.375061 loss_rnnt 21.704645 hw_loss 0.177103 lr 0.00051201 rank 6
2023-02-20 20:04:31,592 DEBUG TRAIN Batch 11/3600 loss 12.949767 loss_att 21.949568 loss_ctc 11.930558 loss_rnnt 11.160394 hw_loss 0.234950 lr 0.00051201 rank 0
2023-02-20 20:05:53,355 DEBUG TRAIN Batch 11/3700 loss 25.233763 loss_att 29.561819 loss_ctc 32.432922 loss_rnnt 23.369249 hw_loss 0.073151 lr 0.00051174 rank 7
2023-02-20 20:05:53,364 DEBUG TRAIN Batch 11/3700 loss 27.299641 loss_att 35.729221 loss_ctc 28.640793 loss_rnnt 25.333134 hw_loss 0.190821 lr 0.00051174 rank 3
2023-02-20 20:05:53,365 DEBUG TRAIN Batch 11/3700 loss 28.956596 loss_att 37.938778 loss_ctc 34.633545 loss_rnnt 26.364603 hw_loss 0.072431 lr 0.00051174 rank 1
2023-02-20 20:05:53,366 DEBUG TRAIN Batch 11/3700 loss 13.596241 loss_att 17.584759 loss_ctc 19.581663 loss_rnnt 11.948117 hw_loss 0.098180 lr 0.00051174 rank 2
2023-02-20 20:05:53,368 DEBUG TRAIN Batch 11/3700 loss 20.858366 loss_att 26.201036 loss_ctc 21.427589 loss_rnnt 19.657215 hw_loss 0.106356 lr 0.00051174 rank 4
2023-02-20 20:05:53,371 DEBUG TRAIN Batch 11/3700 loss 18.809483 loss_att 28.987286 loss_ctc 18.687460 loss_rnnt 16.719418 hw_loss 0.132702 lr 0.00051174 rank 0
2023-02-20 20:05:53,374 DEBUG TRAIN Batch 11/3700 loss 28.579208 loss_att 36.608265 loss_ctc 37.089046 loss_rnnt 25.763121 hw_loss 0.141813 lr 0.00051174 rank 5
2023-02-20 20:05:53,419 DEBUG TRAIN Batch 11/3700 loss 20.094812 loss_att 29.609371 loss_ctc 21.121201 loss_rnnt 17.956657 hw_loss 0.184483 lr 0.00051174 rank 6
2023-02-20 20:07:15,030 DEBUG TRAIN Batch 11/3800 loss 29.699923 loss_att 27.867359 loss_ctc 35.159409 loss_rnnt 29.209007 hw_loss 0.242801 lr 0.00051147 rank 7
2023-02-20 20:07:15,033 DEBUG TRAIN Batch 11/3800 loss 17.224409 loss_att 20.465755 loss_ctc 20.568308 loss_rnnt 16.073420 hw_loss 0.106621 lr 0.00051147 rank 4
2023-02-20 20:07:15,036 DEBUG TRAIN Batch 11/3800 loss 16.985445 loss_att 20.795630 loss_ctc 15.654482 loss_rnnt 16.281971 hw_loss 0.222936 lr 0.00051147 rank 1
2023-02-20 20:07:15,037 DEBUG TRAIN Batch 11/3800 loss 15.431024 loss_att 14.706654 loss_ctc 16.889410 loss_rnnt 15.256606 hw_loss 0.234074 lr 0.00051147 rank 2
2023-02-20 20:07:15,038 DEBUG TRAIN Batch 11/3800 loss 21.016872 loss_att 20.986275 loss_ctc 24.644924 loss_rnnt 20.431713 hw_loss 0.201634 lr 0.00051147 rank 6
2023-02-20 20:07:15,040 DEBUG TRAIN Batch 11/3800 loss 32.303722 loss_att 37.218899 loss_ctc 36.612507 loss_rnnt 30.649206 hw_loss 0.181824 lr 0.00051147 rank 0
2023-02-20 20:07:15,041 DEBUG TRAIN Batch 11/3800 loss 20.263279 loss_att 30.726143 loss_ctc 21.482296 loss_rnnt 17.913536 hw_loss 0.177441 lr 0.00051147 rank 5
2023-02-20 20:07:15,089 DEBUG TRAIN Batch 11/3800 loss 17.544479 loss_att 25.727337 loss_ctc 17.053070 loss_rnnt 15.856769 hw_loss 0.218738 lr 0.00051147 rank 3
2023-02-20 20:08:39,027 DEBUG TRAIN Batch 11/3900 loss 17.180637 loss_att 24.299545 loss_ctc 19.160475 loss_rnnt 15.467029 hw_loss 0.048466 lr 0.00051120 rank 7
2023-02-20 20:08:39,028 DEBUG TRAIN Batch 11/3900 loss 22.633303 loss_att 36.401489 loss_ctc 25.507437 loss_rnnt 19.471115 hw_loss 0.047497 lr 0.00051120 rank 4
2023-02-20 20:08:39,028 DEBUG TRAIN Batch 11/3900 loss 8.794864 loss_att 14.958939 loss_ctc 10.106025 loss_rnnt 7.350326 hw_loss 0.069191 lr 0.00051120 rank 6
2023-02-20 20:08:39,031 DEBUG TRAIN Batch 11/3900 loss 10.723022 loss_att 20.310093 loss_ctc 10.890391 loss_rnnt 8.736073 hw_loss 0.088533 lr 0.00051120 rank 1
2023-02-20 20:08:39,032 DEBUG TRAIN Batch 11/3900 loss 9.741684 loss_att 16.768402 loss_ctc 14.113948 loss_rnnt 7.697969 hw_loss 0.103879 lr 0.00051120 rank 2
2023-02-20 20:08:39,038 DEBUG TRAIN Batch 11/3900 loss 19.953211 loss_att 29.794445 loss_ctc 19.028732 loss_rnnt 18.001554 hw_loss 0.200012 lr 0.00051120 rank 0
2023-02-20 20:08:39,045 DEBUG TRAIN Batch 11/3900 loss 21.081228 loss_att 23.172573 loss_ctc 26.498741 loss_rnnt 19.867048 hw_loss 0.137961 lr 0.00051120 rank 3
2023-02-20 20:08:39,085 DEBUG TRAIN Batch 11/3900 loss 16.406443 loss_att 27.186831 loss_ctc 17.483765 loss_rnnt 14.059502 hw_loss 0.088534 lr 0.00051120 rank 5
2023-02-20 20:09:57,752 DEBUG TRAIN Batch 11/4000 loss 18.934269 loss_att 25.111399 loss_ctc 23.354134 loss_rnnt 17.085466 hw_loss 0.045112 lr 0.00051094 rank 2
2023-02-20 20:09:57,753 DEBUG TRAIN Batch 11/4000 loss 14.778981 loss_att 19.655792 loss_ctc 15.448737 loss_rnnt 13.609060 hw_loss 0.197357 lr 0.00051094 rank 1
2023-02-20 20:09:57,754 DEBUG TRAIN Batch 11/4000 loss 22.725229 loss_att 28.368633 loss_ctc 23.706409 loss_rnnt 21.386860 hw_loss 0.147867 lr 0.00051094 rank 4
2023-02-20 20:09:57,761 DEBUG TRAIN Batch 11/4000 loss 14.059598 loss_att 21.568317 loss_ctc 19.304327 loss_rnnt 11.774620 hw_loss 0.157384 lr 0.00051094 rank 7
2023-02-20 20:09:57,762 DEBUG TRAIN Batch 11/4000 loss 22.615145 loss_att 29.684719 loss_ctc 26.937775 loss_rnnt 20.601694 hw_loss 0.043474 lr 0.00051094 rank 0
2023-02-20 20:09:57,763 DEBUG TRAIN Batch 11/4000 loss 30.281044 loss_att 41.637741 loss_ctc 29.235485 loss_rnnt 28.115292 hw_loss 0.063419 lr 0.00051094 rank 5
2023-02-20 20:09:57,765 DEBUG TRAIN Batch 11/4000 loss 28.179176 loss_att 35.498177 loss_ctc 32.875076 loss_rnnt 25.941717 hw_loss 0.276639 lr 0.00051094 rank 6
2023-02-20 20:09:57,810 DEBUG TRAIN Batch 11/4000 loss 35.984024 loss_att 52.934898 loss_ctc 36.741970 loss_rnnt 32.415009 hw_loss 0.145839 lr 0.00051094 rank 3
2023-02-20 20:11:16,906 DEBUG TRAIN Batch 11/4100 loss 46.344868 loss_att 45.780487 loss_ctc 55.114342 loss_rnnt 45.233562 hw_loss 0.102973 lr 0.00051067 rank 0
2023-02-20 20:11:16,907 DEBUG TRAIN Batch 11/4100 loss 35.685211 loss_att 39.161476 loss_ctc 37.707207 loss_rnnt 34.637215 hw_loss 0.155883 lr 0.00051067 rank 7
2023-02-20 20:11:16,908 DEBUG TRAIN Batch 11/4100 loss 25.064774 loss_att 34.782722 loss_ctc 27.767239 loss_rnnt 22.719444 hw_loss 0.077641 lr 0.00051067 rank 1
2023-02-20 20:11:16,909 DEBUG TRAIN Batch 11/4100 loss 26.308630 loss_att 38.099724 loss_ctc 36.212589 loss_rnnt 22.557724 hw_loss 0.135299 lr 0.00051067 rank 2
2023-02-20 20:11:16,910 DEBUG TRAIN Batch 11/4100 loss 22.429693 loss_att 22.802494 loss_ctc 24.604834 loss_rnnt 22.053946 hw_loss 0.020945 lr 0.00051067 rank 3
2023-02-20 20:11:16,913 DEBUG TRAIN Batch 11/4100 loss 15.600250 loss_att 29.703701 loss_ctc 16.250389 loss_rnnt 12.672523 hw_loss 0.038156 lr 0.00051067 rank 4
2023-02-20 20:11:16,919 DEBUG TRAIN Batch 11/4100 loss 17.200550 loss_att 26.153969 loss_ctc 17.148647 loss_rnnt 15.304642 hw_loss 0.210270 lr 0.00051067 rank 5
2023-02-20 20:11:16,961 DEBUG TRAIN Batch 11/4100 loss 15.328443 loss_att 22.468191 loss_ctc 13.320042 loss_rnnt 14.157108 hw_loss 0.020946 lr 0.00051067 rank 6
2023-02-20 20:12:37,187 DEBUG TRAIN Batch 11/4200 loss 29.515764 loss_att 39.744724 loss_ctc 29.277596 loss_rnnt 27.458622 hw_loss 0.080819 lr 0.00051040 rank 6
2023-02-20 20:12:37,187 DEBUG TRAIN Batch 11/4200 loss 15.576203 loss_att 23.402596 loss_ctc 15.435169 loss_rnnt 13.962025 hw_loss 0.126946 lr 0.00051040 rank 1
2023-02-20 20:12:37,188 DEBUG TRAIN Batch 11/4200 loss 26.976637 loss_att 37.027012 loss_ctc 35.958511 loss_rnnt 23.729256 hw_loss 0.074476 lr 0.00051040 rank 2
2023-02-20 20:12:37,191 DEBUG TRAIN Batch 11/4200 loss 21.848997 loss_att 25.221302 loss_ctc 23.090036 loss_rnnt 20.952553 hw_loss 0.105958 lr 0.00051040 rank 7
2023-02-20 20:12:37,191 DEBUG TRAIN Batch 11/4200 loss 21.134222 loss_att 29.015942 loss_ctc 20.431026 loss_rnnt 19.593603 hw_loss 0.108820 lr 0.00051040 rank 4
2023-02-20 20:12:37,192 DEBUG TRAIN Batch 11/4200 loss 24.309090 loss_att 31.172424 loss_ctc 27.894962 loss_rnnt 22.435795 hw_loss 0.042207 lr 0.00051040 rank 0
2023-02-20 20:12:37,196 DEBUG TRAIN Batch 11/4200 loss 11.853236 loss_att 15.468256 loss_ctc 15.283087 loss_rnnt 10.590024 hw_loss 0.155428 lr 0.00051040 rank 5
2023-02-20 20:12:37,202 DEBUG TRAIN Batch 11/4200 loss 13.929293 loss_att 22.521769 loss_ctc 17.562073 loss_rnnt 11.677711 hw_loss 0.091341 lr 0.00051040 rank 3
2023-02-20 20:13:57,499 DEBUG TRAIN Batch 11/4300 loss 13.492700 loss_att 22.440895 loss_ctc 16.209057 loss_rnnt 11.303854 hw_loss 0.069422 lr 0.00051014 rank 3
2023-02-20 20:13:57,502 DEBUG TRAIN Batch 11/4300 loss 26.092970 loss_att 34.828419 loss_ctc 26.772270 loss_rnnt 24.188354 hw_loss 0.125538 lr 0.00051014 rank 0
2023-02-20 20:13:57,502 DEBUG TRAIN Batch 11/4300 loss 15.629355 loss_att 21.338623 loss_ctc 21.555668 loss_rnnt 13.553188 hw_loss 0.270259 lr 0.00051014 rank 7
2023-02-20 20:13:57,504 DEBUG TRAIN Batch 11/4300 loss 19.357096 loss_att 27.806702 loss_ctc 22.073318 loss_rnnt 17.221632 hw_loss 0.156342 lr 0.00051014 rank 4
2023-02-20 20:13:57,506 DEBUG TRAIN Batch 11/4300 loss 26.407448 loss_att 31.193798 loss_ctc 31.989985 loss_rnnt 24.692558 hw_loss 0.024901 lr 0.00051014 rank 1
2023-02-20 20:13:57,509 DEBUG TRAIN Batch 11/4300 loss 12.781744 loss_att 17.406414 loss_ctc 11.433664 loss_rnnt 11.950185 hw_loss 0.161941 lr 0.00051014 rank 6
2023-02-20 20:13:57,510 DEBUG TRAIN Batch 11/4300 loss 24.129904 loss_att 27.317238 loss_ctc 29.516073 loss_rnnt 22.720108 hw_loss 0.101575 lr 0.00051014 rank 5
2023-02-20 20:13:57,512 DEBUG TRAIN Batch 11/4300 loss 20.925051 loss_att 27.310055 loss_ctc 23.151114 loss_rnnt 19.287197 hw_loss 0.120083 lr 0.00051014 rank 2
2023-02-20 20:15:17,095 DEBUG TRAIN Batch 11/4400 loss 17.821095 loss_att 25.517635 loss_ctc 20.849695 loss_rnnt 15.834452 hw_loss 0.081603 lr 0.00050987 rank 4
2023-02-20 20:15:17,096 DEBUG TRAIN Batch 11/4400 loss 17.701715 loss_att 21.081673 loss_ctc 19.766441 loss_rnnt 16.668022 hw_loss 0.154510 lr 0.00050987 rank 7
2023-02-20 20:15:17,098 DEBUG TRAIN Batch 11/4400 loss 21.385540 loss_att 26.993935 loss_ctc 20.835638 loss_rnnt 20.232487 hw_loss 0.196302 lr 0.00050987 rank 1
2023-02-20 20:15:17,100 DEBUG TRAIN Batch 11/4400 loss 30.754261 loss_att 40.325348 loss_ctc 31.935188 loss_rnnt 28.645309 hw_loss 0.069894 lr 0.00050987 rank 3
2023-02-20 20:15:17,102 DEBUG TRAIN Batch 11/4400 loss 26.850348 loss_att 30.441494 loss_ctc 28.013035 loss_rnnt 25.865683 hw_loss 0.208897 lr 0.00050987 rank 2
2023-02-20 20:15:17,104 DEBUG TRAIN Batch 11/4400 loss 26.708454 loss_att 29.916306 loss_ctc 30.816570 loss_rnnt 25.462366 hw_loss 0.106437 lr 0.00050987 rank 0
2023-02-20 20:15:17,107 DEBUG TRAIN Batch 11/4400 loss 14.368315 loss_att 16.016491 loss_ctc 17.883263 loss_rnnt 13.495092 hw_loss 0.140491 lr 0.00050987 rank 5
2023-02-20 20:15:17,157 DEBUG TRAIN Batch 11/4400 loss 11.539417 loss_att 18.750240 loss_ctc 12.322844 loss_rnnt 9.914378 hw_loss 0.147032 lr 0.00050987 rank 6
2023-02-20 20:16:36,848 DEBUG TRAIN Batch 11/4500 loss 11.877014 loss_att 23.511467 loss_ctc 10.756789 loss_rnnt 9.576132 hw_loss 0.231290 lr 0.00050961 rank 7
2023-02-20 20:16:36,849 DEBUG TRAIN Batch 11/4500 loss 22.015753 loss_att 26.816011 loss_ctc 27.437880 loss_rnnt 20.248863 hw_loss 0.157290 lr 0.00050961 rank 3
2023-02-20 20:16:36,854 DEBUG TRAIN Batch 11/4500 loss 17.495665 loss_att 35.185555 loss_ctc 13.385455 loss_rnnt 14.459483 hw_loss 0.086683 lr 0.00050961 rank 4
2023-02-20 20:16:36,857 DEBUG TRAIN Batch 11/4500 loss 17.230997 loss_att 24.203207 loss_ctc 17.894209 loss_rnnt 15.667784 hw_loss 0.150641 lr 0.00050961 rank 2
2023-02-20 20:16:36,858 DEBUG TRAIN Batch 11/4500 loss 17.717728 loss_att 19.685242 loss_ctc 23.223961 loss_rnnt 16.453520 hw_loss 0.256013 lr 0.00050961 rank 1
2023-02-20 20:16:36,859 DEBUG TRAIN Batch 11/4500 loss 9.605085 loss_att 23.489149 loss_ctc 9.546068 loss_rnnt 6.753954 hw_loss 0.154102 lr 0.00050961 rank 5
2023-02-20 20:16:36,863 DEBUG TRAIN Batch 11/4500 loss 15.685810 loss_att 16.264488 loss_ctc 18.535254 loss_rnnt 15.151062 hw_loss 0.073288 lr 0.00050961 rank 0
2023-02-20 20:16:36,911 DEBUG TRAIN Batch 11/4500 loss 25.284254 loss_att 26.239765 loss_ctc 27.095028 loss_rnnt 24.826092 hw_loss 0.048044 lr 0.00050961 rank 6
2023-02-20 20:18:00,086 DEBUG TRAIN Batch 11/4600 loss 20.740013 loss_att 28.799423 loss_ctc 26.022619 loss_rnnt 18.338398 hw_loss 0.160100 lr 0.00050934 rank 1
2023-02-20 20:18:00,087 DEBUG TRAIN Batch 11/4600 loss 14.651191 loss_att 21.602882 loss_ctc 15.275084 loss_rnnt 13.131187 hw_loss 0.087148 lr 0.00050934 rank 2
2023-02-20 20:18:00,087 DEBUG TRAIN Batch 11/4600 loss 19.276037 loss_att 30.626236 loss_ctc 22.604324 loss_rnnt 16.507156 hw_loss 0.103254 lr 0.00050934 rank 4
2023-02-20 20:18:00,089 DEBUG TRAIN Batch 11/4600 loss 18.227613 loss_att 26.375481 loss_ctc 19.277252 loss_rnnt 16.443445 hw_loss 0.027457 lr 0.00050934 rank 0
2023-02-20 20:18:00,094 DEBUG TRAIN Batch 11/4600 loss 22.280176 loss_att 24.648155 loss_ctc 22.250326 loss_rnnt 21.735851 hw_loss 0.140078 lr 0.00050934 rank 7
2023-02-20 20:18:00,096 DEBUG TRAIN Batch 11/4600 loss 27.685308 loss_att 31.566246 loss_ctc 32.995872 loss_rnnt 26.098583 hw_loss 0.192117 lr 0.00050934 rank 6
2023-02-20 20:18:00,103 DEBUG TRAIN Batch 11/4600 loss 21.522011 loss_att 32.487030 loss_ctc 22.283066 loss_rnnt 19.212887 hw_loss 0.027459 lr 0.00050934 rank 3
2023-02-20 20:18:00,143 DEBUG TRAIN Batch 11/4600 loss 23.764860 loss_att 31.403484 loss_ctc 23.754265 loss_rnnt 22.167057 hw_loss 0.134045 lr 0.00050934 rank 5
2023-02-20 20:19:19,158 DEBUG TRAIN Batch 11/4700 loss 25.377987 loss_att 32.268440 loss_ctc 27.220051 loss_rnnt 23.682850 hw_loss 0.133945 lr 0.00050908 rank 3
2023-02-20 20:19:19,158 DEBUG TRAIN Batch 11/4700 loss 27.256241 loss_att 49.849197 loss_ctc 32.208717 loss_rnnt 22.026062 hw_loss 0.096111 lr 0.00050908 rank 1
2023-02-20 20:19:19,162 DEBUG TRAIN Batch 11/4700 loss 18.628082 loss_att 20.594198 loss_ctc 21.150051 loss_rnnt 17.841358 hw_loss 0.107321 lr 0.00050908 rank 6
2023-02-20 20:19:19,163 DEBUG TRAIN Batch 11/4700 loss 21.563536 loss_att 26.001095 loss_ctc 24.307526 loss_rnnt 20.216303 hw_loss 0.175983 lr 0.00050908 rank 2
2023-02-20 20:19:19,163 DEBUG TRAIN Batch 11/4700 loss 18.690687 loss_att 31.117760 loss_ctc 23.527601 loss_rnnt 15.492649 hw_loss 0.126938 lr 0.00050908 rank 7
2023-02-20 20:19:19,167 DEBUG TRAIN Batch 11/4700 loss 27.186811 loss_att 32.931839 loss_ctc 34.322128 loss_rnnt 25.037415 hw_loss 0.091906 lr 0.00050908 rank 0
2023-02-20 20:19:19,167 DEBUG TRAIN Batch 11/4700 loss 24.981167 loss_att 29.867865 loss_ctc 27.938641 loss_rnnt 23.551552 hw_loss 0.108650 lr 0.00050908 rank 5
2023-02-20 20:19:19,202 DEBUG TRAIN Batch 11/4700 loss 24.172684 loss_att 28.237968 loss_ctc 24.297255 loss_rnnt 23.293999 hw_loss 0.091909 lr 0.00050908 rank 4
2023-02-20 20:20:38,541 DEBUG TRAIN Batch 11/4800 loss 16.923374 loss_att 24.673464 loss_ctc 23.120199 loss_rnnt 14.453176 hw_loss 0.176130 lr 0.00050882 rank 7
2023-02-20 20:20:38,542 DEBUG TRAIN Batch 11/4800 loss 44.192791 loss_att 49.560417 loss_ctc 50.339912 loss_rnnt 42.178684 hw_loss 0.226798 lr 0.00050882 rank 2
2023-02-20 20:20:38,544 DEBUG TRAIN Batch 11/4800 loss 29.199188 loss_att 37.771179 loss_ctc 31.950520 loss_rnnt 27.078712 hw_loss 0.073567 lr 0.00050882 rank 4
2023-02-20 20:20:38,545 DEBUG TRAIN Batch 11/4800 loss 19.945660 loss_att 30.186680 loss_ctc 17.964602 loss_rnnt 18.113234 hw_loss 0.090682 lr 0.00050882 rank 3
2023-02-20 20:20:38,546 DEBUG TRAIN Batch 11/4800 loss 20.303715 loss_att 27.631376 loss_ctc 20.446281 loss_rnnt 18.753708 hw_loss 0.122747 lr 0.00050882 rank 1
2023-02-20 20:20:38,549 DEBUG TRAIN Batch 11/4800 loss 32.396984 loss_att 37.460072 loss_ctc 34.636539 loss_rnnt 31.028864 hw_loss 0.106675 lr 0.00050882 rank 0
2023-02-20 20:20:38,551 DEBUG TRAIN Batch 11/4800 loss 27.251554 loss_att 31.839165 loss_ctc 29.605291 loss_rnnt 25.969336 hw_loss 0.095372 lr 0.00050882 rank 5
2023-02-20 20:20:38,551 DEBUG TRAIN Batch 11/4800 loss 23.072010 loss_att 31.234188 loss_ctc 21.894653 loss_rnnt 21.565590 hw_loss 0.058057 lr 0.00050882 rank 6
2023-02-20 20:21:59,313 DEBUG TRAIN Batch 11/4900 loss 15.807628 loss_att 18.263115 loss_ctc 21.071377 loss_rnnt 14.554030 hw_loss 0.113750 lr 0.00050855 rank 7
2023-02-20 20:21:59,319 DEBUG TRAIN Batch 11/4900 loss 25.482187 loss_att 29.727413 loss_ctc 26.046968 loss_rnnt 24.496288 hw_loss 0.115410 lr 0.00050855 rank 2
2023-02-20 20:21:59,321 DEBUG TRAIN Batch 11/4900 loss 24.512495 loss_att 28.549065 loss_ctc 23.595484 loss_rnnt 23.683729 hw_loss 0.269476 lr 0.00050855 rank 1
2023-02-20 20:21:59,321 DEBUG TRAIN Batch 11/4900 loss 27.582846 loss_att 37.775826 loss_ctc 31.358019 loss_rnnt 24.963768 hw_loss 0.144613 lr 0.00050855 rank 5
2023-02-20 20:21:59,322 DEBUG TRAIN Batch 11/4900 loss 24.159830 loss_att 35.914242 loss_ctc 26.636110 loss_rnnt 21.395061 hw_loss 0.156971 lr 0.00050855 rank 4
2023-02-20 20:21:59,323 DEBUG TRAIN Batch 11/4900 loss 28.386169 loss_att 27.836802 loss_ctc 25.704399 loss_rnnt 28.794079 hw_loss 0.111629 lr 0.00050855 rank 0
2023-02-20 20:21:59,327 DEBUG TRAIN Batch 11/4900 loss 25.841734 loss_att 30.560152 loss_ctc 31.389364 loss_rnnt 24.118597 hw_loss 0.074566 lr 0.00050855 rank 3
2023-02-20 20:21:59,333 DEBUG TRAIN Batch 11/4900 loss 16.902603 loss_att 17.730547 loss_ctc 18.994030 loss_rnnt 16.263979 hw_loss 0.364086 lr 0.00050855 rank 6
2023-02-20 20:23:20,307 DEBUG TRAIN Batch 11/5000 loss 21.356882 loss_att 21.142824 loss_ctc 23.658979 loss_rnnt 20.932589 hw_loss 0.300296 lr 0.00050829 rank 3
2023-02-20 20:23:20,310 DEBUG TRAIN Batch 11/5000 loss 14.566894 loss_att 23.480698 loss_ctc 18.725086 loss_rnnt 12.150477 hw_loss 0.148557 lr 0.00050829 rank 7
2023-02-20 20:23:20,313 DEBUG TRAIN Batch 11/5000 loss 20.798941 loss_att 23.413967 loss_ctc 23.680861 loss_rnnt 19.872396 hw_loss 0.036159 lr 0.00050829 rank 2
2023-02-20 20:23:20,313 DEBUG TRAIN Batch 11/5000 loss 15.953546 loss_att 24.536949 loss_ctc 17.630852 loss_rnnt 13.920568 hw_loss 0.173729 lr 0.00050829 rank 1
2023-02-20 20:23:20,317 DEBUG TRAIN Batch 11/5000 loss 17.578779 loss_att 19.309048 loss_ctc 22.675606 loss_rnnt 16.533863 hw_loss 0.036159 lr 0.00050829 rank 0
2023-02-20 20:23:20,317 DEBUG TRAIN Batch 11/5000 loss 18.595448 loss_att 24.634909 loss_ctc 21.176723 loss_rnnt 16.987415 hw_loss 0.104944 lr 0.00050829 rank 4
2023-02-20 20:23:20,318 DEBUG TRAIN Batch 11/5000 loss 24.768789 loss_att 31.092930 loss_ctc 31.568308 loss_rnnt 22.529734 hw_loss 0.126794 lr 0.00050829 rank 5
2023-02-20 20:23:20,364 DEBUG TRAIN Batch 11/5000 loss 25.147141 loss_att 25.547390 loss_ctc 26.379242 loss_rnnt 24.809813 hw_loss 0.174370 lr 0.00050829 rank 6
2023-02-20 20:24:40,338 DEBUG TRAIN Batch 11/5100 loss 16.716326 loss_att 18.875835 loss_ctc 19.429880 loss_rnnt 15.883716 hw_loss 0.072938 lr 0.00050803 rank 1
2023-02-20 20:24:40,338 DEBUG TRAIN Batch 11/5100 loss 30.759542 loss_att 42.903725 loss_ctc 27.846367 loss_rnnt 28.620544 hw_loss 0.184848 lr 0.00050803 rank 7
2023-02-20 20:24:40,341 DEBUG TRAIN Batch 11/5100 loss 14.862951 loss_att 15.486940 loss_ctc 17.622030 loss_rnnt 14.192277 hw_loss 0.333748 lr 0.00050803 rank 2
2023-02-20 20:24:40,344 DEBUG TRAIN Batch 11/5100 loss 15.287505 loss_att 19.082321 loss_ctc 15.385266 loss_rnnt 14.336252 hw_loss 0.336102 lr 0.00050803 rank 0
2023-02-20 20:24:40,346 DEBUG TRAIN Batch 11/5100 loss 12.319097 loss_att 14.048330 loss_ctc 14.221697 loss_rnnt 11.571680 hw_loss 0.277292 lr 0.00050803 rank 4
2023-02-20 20:24:40,348 DEBUG TRAIN Batch 11/5100 loss 26.647911 loss_att 34.237885 loss_ctc 32.944012 loss_rnnt 24.206829 hw_loss 0.156766 lr 0.00050803 rank 6
2023-02-20 20:24:40,350 DEBUG TRAIN Batch 11/5100 loss 29.255404 loss_att 29.589771 loss_ctc 28.999266 loss_rnnt 29.179337 hw_loss 0.081267 lr 0.00050803 rank 3
2023-02-20 20:24:40,358 DEBUG TRAIN Batch 11/5100 loss 15.977090 loss_att 15.110249 loss_ctc 18.095953 loss_rnnt 15.765816 hw_loss 0.191488 lr 0.00050803 rank 5
2023-02-20 20:25:59,879 DEBUG TRAIN Batch 11/5200 loss 19.543758 loss_att 27.049580 loss_ctc 16.907253 loss_rnnt 18.315739 hw_loss 0.146980 lr 0.00050776 rank 7
2023-02-20 20:25:59,879 DEBUG TRAIN Batch 11/5200 loss 22.775249 loss_att 30.699024 loss_ctc 26.759541 loss_rnnt 20.580864 hw_loss 0.146979 lr 0.00050776 rank 3
2023-02-20 20:25:59,885 DEBUG TRAIN Batch 11/5200 loss 13.919505 loss_att 20.176121 loss_ctc 13.759315 loss_rnnt 12.580699 hw_loss 0.204079 lr 0.00050776 rank 4
2023-02-20 20:25:59,889 DEBUG TRAIN Batch 11/5200 loss 24.468607 loss_att 39.152107 loss_ctc 30.642967 loss_rnnt 20.682240 hw_loss 0.049532 lr 0.00050776 rank 5
2023-02-20 20:25:59,889 DEBUG TRAIN Batch 11/5200 loss 13.671667 loss_att 25.716841 loss_ctc 16.276691 loss_rnnt 10.842731 hw_loss 0.136060 lr 0.00050776 rank 1
2023-02-20 20:25:59,890 DEBUG TRAIN Batch 11/5200 loss 10.117373 loss_att 16.696457 loss_ctc 12.233723 loss_rnnt 8.437616 hw_loss 0.153301 lr 0.00050776 rank 0
2023-02-20 20:25:59,891 DEBUG TRAIN Batch 11/5200 loss 11.782291 loss_att 22.366577 loss_ctc 13.620625 loss_rnnt 9.359406 hw_loss 0.114222 lr 0.00050776 rank 6
2023-02-20 20:25:59,944 DEBUG TRAIN Batch 11/5200 loss 23.054855 loss_att 32.314072 loss_ctc 25.263769 loss_rnnt 20.839476 hw_loss 0.129402 lr 0.00050776 rank 2
2023-02-20 20:27:21,960 DEBUG TRAIN Batch 11/5300 loss 22.493961 loss_att 32.296562 loss_ctc 28.519405 loss_rnnt 19.695187 hw_loss 0.065367 lr 0.00050750 rank 7
2023-02-20 20:27:21,962 DEBUG TRAIN Batch 11/5300 loss 26.683496 loss_att 31.264206 loss_ctc 28.152836 loss_rnnt 25.500341 hw_loss 0.133317 lr 0.00050750 rank 2
2023-02-20 20:27:21,964 DEBUG TRAIN Batch 11/5300 loss 14.093755 loss_att 22.311581 loss_ctc 14.431847 loss_rnnt 12.331321 hw_loss 0.138357 lr 0.00050750 rank 1
2023-02-20 20:27:21,967 DEBUG TRAIN Batch 11/5300 loss 11.972123 loss_att 18.579735 loss_ctc 11.769960 loss_rnnt 10.628025 hw_loss 0.092870 lr 0.00050750 rank 5
2023-02-20 20:27:21,968 DEBUG TRAIN Batch 11/5300 loss 15.156407 loss_att 16.572701 loss_ctc 14.999886 loss_rnnt 14.843795 hw_loss 0.094168 lr 0.00050750 rank 3
2023-02-20 20:27:21,969 DEBUG TRAIN Batch 11/5300 loss 12.387082 loss_att 24.115955 loss_ctc 13.591055 loss_rnnt 9.867918 hw_loss 0.024111 lr 0.00050750 rank 0
2023-02-20 20:27:21,969 DEBUG TRAIN Batch 11/5300 loss 11.255346 loss_att 20.933384 loss_ctc 10.910449 loss_rnnt 9.352866 hw_loss 0.024111 lr 0.00050750 rank 6
2023-02-20 20:27:21,970 DEBUG TRAIN Batch 11/5300 loss 18.883345 loss_att 20.878674 loss_ctc 26.035275 loss_rnnt 17.517830 hw_loss 0.024111 lr 0.00050750 rank 4
2023-02-20 20:28:41,541 DEBUG TRAIN Batch 11/5400 loss 15.017355 loss_att 17.682468 loss_ctc 14.438777 loss_rnnt 14.521208 hw_loss 0.075502 lr 0.00050724 rank 3
2023-02-20 20:28:41,545 DEBUG TRAIN Batch 11/5400 loss 20.508224 loss_att 25.804014 loss_ctc 20.953228 loss_rnnt 19.333141 hw_loss 0.106108 lr 0.00050724 rank 2
2023-02-20 20:28:41,546 DEBUG TRAIN Batch 11/5400 loss 25.811037 loss_att 34.039566 loss_ctc 28.869591 loss_rnnt 23.696545 hw_loss 0.114343 lr 0.00050724 rank 0
2023-02-20 20:28:41,546 DEBUG TRAIN Batch 11/5400 loss 11.709987 loss_att 18.976538 loss_ctc 14.161514 loss_rnnt 9.854573 hw_loss 0.141059 lr 0.00050724 rank 5
2023-02-20 20:28:41,546 DEBUG TRAIN Batch 11/5400 loss 21.339590 loss_att 26.585464 loss_ctc 22.965267 loss_rnnt 19.992081 hw_loss 0.152958 lr 0.00050724 rank 6
2023-02-20 20:28:41,563 DEBUG TRAIN Batch 11/5400 loss 28.152950 loss_att 28.626381 loss_ctc 26.583973 loss_rnnt 28.180855 hw_loss 0.162386 lr 0.00050724 rank 4
2023-02-20 20:28:41,575 DEBUG TRAIN Batch 11/5400 loss 26.726727 loss_att 37.526337 loss_ctc 31.912289 loss_rnnt 23.831234 hw_loss 0.082807 lr 0.00050724 rank 1
2023-02-20 20:28:41,601 DEBUG TRAIN Batch 11/5400 loss 27.911280 loss_att 37.760532 loss_ctc 31.419304 loss_rnnt 25.434605 hw_loss 0.073289 lr 0.00050724 rank 7
2023-02-20 20:30:01,414 DEBUG TRAIN Batch 11/5500 loss 27.439579 loss_att 34.581757 loss_ctc 36.492634 loss_rnnt 24.756840 hw_loss 0.088554 lr 0.00050698 rank 7
2023-02-20 20:30:01,415 DEBUG TRAIN Batch 11/5500 loss 23.701939 loss_att 28.659563 loss_ctc 26.919571 loss_rnnt 22.220474 hw_loss 0.114229 lr 0.00050698 rank 1
2023-02-20 20:30:01,417 DEBUG TRAIN Batch 11/5500 loss 19.254656 loss_att 24.940229 loss_ctc 24.706238 loss_rnnt 17.350157 hw_loss 0.075949 lr 0.00050698 rank 2
2023-02-20 20:30:01,422 DEBUG TRAIN Batch 11/5500 loss 21.267311 loss_att 28.089121 loss_ctc 25.480619 loss_rnnt 19.275354 hw_loss 0.123417 lr 0.00050698 rank 3
2023-02-20 20:30:01,422 DEBUG TRAIN Batch 11/5500 loss 24.159834 loss_att 28.316990 loss_ctc 26.714840 loss_rnnt 22.938177 hw_loss 0.092922 lr 0.00050698 rank 4
2023-02-20 20:30:01,426 DEBUG TRAIN Batch 11/5500 loss 22.822571 loss_att 28.524067 loss_ctc 28.401592 loss_rnnt 20.821949 hw_loss 0.218347 lr 0.00050698 rank 0
2023-02-20 20:30:01,427 DEBUG TRAIN Batch 11/5500 loss 19.065271 loss_att 24.266836 loss_ctc 24.285027 loss_rnnt 17.295267 hw_loss 0.063230 lr 0.00050698 rank 5
2023-02-20 20:30:01,429 DEBUG TRAIN Batch 11/5500 loss 19.926485 loss_att 27.110088 loss_ctc 21.048256 loss_rnnt 18.295467 hw_loss 0.083861 lr 0.00050698 rank 6
2023-02-20 20:31:21,608 DEBUG TRAIN Batch 11/5600 loss 12.749968 loss_att 18.491175 loss_ctc 13.614364 loss_rnnt 11.391315 hw_loss 0.178423 lr 0.00050672 rank 7
2023-02-20 20:31:21,609 DEBUG TRAIN Batch 11/5600 loss 6.000242 loss_att 10.295094 loss_ctc 7.482742 loss_rnnt 4.910043 hw_loss 0.062928 lr 0.00050672 rank 3
2023-02-20 20:31:21,613 DEBUG TRAIN Batch 11/5600 loss 26.914524 loss_att 30.752613 loss_ctc 31.602057 loss_rnnt 25.468895 hw_loss 0.099385 lr 0.00050672 rank 1
2023-02-20 20:31:21,613 DEBUG TRAIN Batch 11/5600 loss 19.870033 loss_att 25.850025 loss_ctc 22.291676 loss_rnnt 18.325510 hw_loss 0.048070 lr 0.00050672 rank 4
2023-02-20 20:31:21,614 DEBUG TRAIN Batch 11/5600 loss 20.006420 loss_att 23.454693 loss_ctc 18.422016 loss_rnnt 19.468481 hw_loss 0.111639 lr 0.00050672 rank 2
2023-02-20 20:31:21,618 DEBUG TRAIN Batch 11/5600 loss 24.880182 loss_att 25.483433 loss_ctc 28.058760 loss_rnnt 24.279562 hw_loss 0.105299 lr 0.00050672 rank 6
2023-02-20 20:31:21,618 DEBUG TRAIN Batch 11/5600 loss 4.523351 loss_att 9.217130 loss_ctc 5.585836 loss_rnnt 3.368748 hw_loss 0.139092 lr 0.00050672 rank 5
2023-02-20 20:31:21,657 DEBUG TRAIN Batch 11/5600 loss 7.485997 loss_att 15.213444 loss_ctc 6.979568 loss_rnnt 5.941916 hw_loss 0.123966 lr 0.00050672 rank 0
2023-02-20 20:32:44,662 DEBUG TRAIN Batch 11/5700 loss 18.760887 loss_att 28.706533 loss_ctc 27.553808 loss_rnnt 15.537097 hw_loss 0.116760 lr 0.00050646 rank 7
2023-02-20 20:32:44,663 DEBUG TRAIN Batch 11/5700 loss 9.109000 loss_att 10.781895 loss_ctc 9.792786 loss_rnnt 8.582867 hw_loss 0.188221 lr 0.00050646 rank 4
2023-02-20 20:32:44,665 DEBUG TRAIN Batch 11/5700 loss 9.339194 loss_att 13.289219 loss_ctc 9.460034 loss_rnnt 8.433176 hw_loss 0.187314 lr 0.00050646 rank 2
2023-02-20 20:32:44,666 DEBUG TRAIN Batch 11/5700 loss 24.460695 loss_att 32.433243 loss_ctc 27.993540 loss_rnnt 22.348949 hw_loss 0.086607 lr 0.00050646 rank 1
2023-02-20 20:32:44,668 DEBUG TRAIN Batch 11/5700 loss 18.858524 loss_att 25.083305 loss_ctc 26.025826 loss_rnnt 16.597584 hw_loss 0.113145 lr 0.00050646 rank 6
2023-02-20 20:32:44,670 DEBUG TRAIN Batch 11/5700 loss 19.189995 loss_att 22.928349 loss_ctc 20.952600 loss_rnnt 18.136944 hw_loss 0.131937 lr 0.00050646 rank 3
2023-02-20 20:32:44,672 DEBUG TRAIN Batch 11/5700 loss 11.594534 loss_att 13.168495 loss_ctc 11.473530 loss_rnnt 11.180582 hw_loss 0.216174 lr 0.00050646 rank 5
2023-02-20 20:32:44,678 DEBUG TRAIN Batch 11/5700 loss 8.177547 loss_att 13.946682 loss_ctc 9.434280 loss_rnnt 6.776460 hw_loss 0.149432 lr 0.00050646 rank 0
2023-02-20 20:34:04,300 DEBUG TRAIN Batch 11/5800 loss 7.031499 loss_att 16.099552 loss_ctc 9.212983 loss_rnnt 4.878494 hw_loss 0.090993 lr 0.00050620 rank 2
2023-02-20 20:34:04,301 DEBUG TRAIN Batch 11/5800 loss 18.506872 loss_att 23.494371 loss_ctc 20.358038 loss_rnnt 17.211540 hw_loss 0.095645 lr 0.00050620 rank 6
2023-02-20 20:34:04,305 DEBUG TRAIN Batch 11/5800 loss 15.991049 loss_att 16.590601 loss_ctc 17.200796 loss_rnnt 15.604946 hw_loss 0.196675 lr 0.00050620 rank 3
2023-02-20 20:34:04,305 DEBUG TRAIN Batch 11/5800 loss 20.513916 loss_att 27.937126 loss_ctc 23.649368 loss_rnnt 18.558270 hw_loss 0.099274 lr 0.00050620 rank 7
2023-02-20 20:34:04,305 DEBUG TRAIN Batch 11/5800 loss 11.390468 loss_att 11.541100 loss_ctc 13.838876 loss_rnnt 10.832205 hw_loss 0.378155 lr 0.00050620 rank 4
2023-02-20 20:34:04,307 DEBUG TRAIN Batch 11/5800 loss 15.472067 loss_att 16.039484 loss_ctc 17.105955 loss_rnnt 14.940511 hw_loss 0.375413 lr 0.00050620 rank 0
2023-02-20 20:34:04,308 DEBUG TRAIN Batch 11/5800 loss 13.146298 loss_att 14.350908 loss_ctc 15.204554 loss_rnnt 12.463711 hw_loss 0.313560 lr 0.00050620 rank 1
2023-02-20 20:34:04,355 DEBUG TRAIN Batch 11/5800 loss 7.648806 loss_att 15.591270 loss_ctc 9.323526 loss_rnnt 5.790131 hw_loss 0.087910 lr 0.00050620 rank 5
2023-02-20 20:35:24,310 DEBUG TRAIN Batch 11/5900 loss 12.967150 loss_att 15.715275 loss_ctc 16.880543 loss_rnnt 11.882395 hw_loss 0.025020 lr 0.00050594 rank 2
2023-02-20 20:35:24,310 DEBUG TRAIN Batch 11/5900 loss 18.882694 loss_att 31.983252 loss_ctc 19.219633 loss_rnnt 16.192848 hw_loss 0.046515 lr 0.00050594 rank 3
2023-02-20 20:35:24,313 DEBUG TRAIN Batch 11/5900 loss 14.081905 loss_att 22.015865 loss_ctc 13.262518 loss_rnnt 12.591022 hw_loss 0.025020 lr 0.00050594 rank 0
2023-02-20 20:35:24,315 DEBUG TRAIN Batch 11/5900 loss 29.960234 loss_att 31.003162 loss_ctc 31.318159 loss_rnnt 29.544920 hw_loss 0.048135 lr 0.00050594 rank 7
2023-02-20 20:35:24,315 DEBUG TRAIN Batch 11/5900 loss 20.036243 loss_att 26.427122 loss_ctc 30.471861 loss_rnnt 17.322191 hw_loss 0.083362 lr 0.00050594 rank 4
2023-02-20 20:35:24,318 DEBUG TRAIN Batch 11/5900 loss 14.316261 loss_att 24.957897 loss_ctc 15.124812 loss_rnnt 12.066783 hw_loss 0.025020 lr 0.00050594 rank 1
2023-02-20 20:35:24,318 DEBUG TRAIN Batch 11/5900 loss 21.756927 loss_att 26.087246 loss_ctc 27.721060 loss_rnnt 20.082300 hw_loss 0.025021 lr 0.00050594 rank 6
2023-02-20 20:35:24,320 DEBUG TRAIN Batch 11/5900 loss 19.080492 loss_att 26.562576 loss_ctc 20.874144 loss_rnnt 17.331575 hw_loss 0.025020 lr 0.00050594 rank 5
2023-02-20 20:36:46,312 DEBUG TRAIN Batch 11/6000 loss 23.869261 loss_att 28.477642 loss_ctc 24.725960 loss_rnnt 22.818943 hw_loss 0.027026 lr 0.00050568 rank 1
2023-02-20 20:36:46,313 DEBUG TRAIN Batch 11/6000 loss 31.815596 loss_att 39.969036 loss_ctc 32.738514 loss_rnnt 29.880283 hw_loss 0.340438 lr 0.00050568 rank 7
2023-02-20 20:36:46,313 DEBUG TRAIN Batch 11/6000 loss 21.305443 loss_att 23.711143 loss_ctc 19.290695 loss_rnnt 20.996880 hw_loss 0.180107 lr 0.00050568 rank 4
2023-02-20 20:36:46,313 DEBUG TRAIN Batch 11/6000 loss 11.450906 loss_att 17.216105 loss_ctc 14.766171 loss_rnnt 9.841417 hw_loss 0.027026 lr 0.00050568 rank 2
2023-02-20 20:36:46,315 DEBUG TRAIN Batch 11/6000 loss 11.984711 loss_att 20.337784 loss_ctc 12.092252 loss_rnnt 10.285343 hw_loss 0.027026 lr 0.00050568 rank 6
2023-02-20 20:36:46,316 DEBUG TRAIN Batch 11/6000 loss 22.073109 loss_att 26.907658 loss_ctc 24.725327 loss_rnnt 20.716476 hw_loss 0.067675 lr 0.00050568 rank 3
2023-02-20 20:36:46,316 DEBUG TRAIN Batch 11/6000 loss 16.854906 loss_att 18.902746 loss_ctc 14.977237 loss_rnnt 16.614662 hw_loss 0.151936 lr 0.00050568 rank 0
2023-02-20 20:36:46,324 DEBUG TRAIN Batch 11/6000 loss 26.527447 loss_att 32.426224 loss_ctc 31.939468 loss_rnnt 24.583868 hw_loss 0.079162 lr 0.00050568 rank 5
2023-02-20 20:38:07,170 DEBUG TRAIN Batch 11/6100 loss 18.487667 loss_att 23.994015 loss_ctc 17.107468 loss_rnnt 17.426903 hw_loss 0.269103 lr 0.00050542 rank 7
2023-02-20 20:38:07,174 DEBUG TRAIN Batch 11/6100 loss 19.987642 loss_att 21.447292 loss_ctc 16.793446 loss_rnnt 20.005642 hw_loss 0.217427 lr 0.00050542 rank 2
2023-02-20 20:38:07,176 DEBUG TRAIN Batch 11/6100 loss 18.538851 loss_att 23.538416 loss_ctc 19.463341 loss_rnnt 17.364761 hw_loss 0.095460 lr 0.00050542 rank 4
2023-02-20 20:38:07,177 DEBUG TRAIN Batch 11/6100 loss 20.009787 loss_att 23.692011 loss_ctc 22.355570 loss_rnnt 18.858427 hw_loss 0.191517 lr 0.00050542 rank 6
2023-02-20 20:38:07,178 DEBUG TRAIN Batch 11/6100 loss 15.221197 loss_att 22.587402 loss_ctc 18.877617 loss_rnnt 13.245484 hw_loss 0.028028 lr 0.00050542 rank 1
2023-02-20 20:38:07,180 DEBUG TRAIN Batch 11/6100 loss 30.474136 loss_att 38.057705 loss_ctc 35.276497 loss_rnnt 28.223038 hw_loss 0.176383 lr 0.00050542 rank 3
2023-02-20 20:38:07,228 DEBUG TRAIN Batch 11/6100 loss 15.599064 loss_att 22.466076 loss_ctc 20.429148 loss_rnnt 13.533960 hw_loss 0.089416 lr 0.00050542 rank 0
2023-02-20 20:38:07,233 DEBUG TRAIN Batch 11/6100 loss 9.980282 loss_att 17.620220 loss_ctc 9.229769 loss_rnnt 8.465756 hw_loss 0.162388 lr 0.00050542 rank 5
2023-02-20 20:39:26,825 DEBUG TRAIN Batch 11/6200 loss 15.152452 loss_att 23.505619 loss_ctc 17.099352 loss_rnnt 13.154524 hw_loss 0.126956 lr 0.00050517 rank 1
2023-02-20 20:39:26,827 DEBUG TRAIN Batch 11/6200 loss 6.843299 loss_att 11.665283 loss_ctc 6.828825 loss_rnnt 5.748083 hw_loss 0.248905 lr 0.00050517 rank 7
2023-02-20 20:39:26,829 DEBUG TRAIN Batch 11/6200 loss 19.927391 loss_att 26.817974 loss_ctc 22.464407 loss_rnnt 18.156303 hw_loss 0.102566 lr 0.00050517 rank 2
2023-02-20 20:39:26,831 DEBUG TRAIN Batch 11/6200 loss 14.909950 loss_att 19.559774 loss_ctc 16.434736 loss_rnnt 13.697504 hw_loss 0.148453 lr 0.00050517 rank 4
2023-02-20 20:39:26,831 DEBUG TRAIN Batch 11/6200 loss 20.909721 loss_att 24.455208 loss_ctc 19.779745 loss_rnnt 20.267319 hw_loss 0.157444 lr 0.00050517 rank 6
2023-02-20 20:39:26,835 DEBUG TRAIN Batch 11/6200 loss 10.967887 loss_att 19.448988 loss_ctc 15.548243 loss_rnnt 8.571817 hw_loss 0.167129 lr 0.00050517 rank 0
2023-02-20 20:39:26,840 DEBUG TRAIN Batch 11/6200 loss 22.653778 loss_att 26.183533 loss_ctc 26.366222 loss_rnnt 21.411615 hw_loss 0.077287 lr 0.00050517 rank 5
2023-02-20 20:39:26,879 DEBUG TRAIN Batch 11/6200 loss 18.661852 loss_att 23.298786 loss_ctc 20.374542 loss_rnnt 17.428577 hw_loss 0.145364 lr 0.00050517 rank 3
2023-02-20 20:40:46,952 DEBUG TRAIN Batch 11/6300 loss 15.526175 loss_att 16.571562 loss_ctc 19.293972 loss_rnnt 14.648813 hw_loss 0.311079 lr 0.00050491 rank 7
2023-02-20 20:40:46,955 DEBUG TRAIN Batch 11/6300 loss 10.727625 loss_att 12.879128 loss_ctc 11.359138 loss_rnnt 10.128337 hw_loss 0.158972 lr 0.00050491 rank 2
2023-02-20 20:40:46,956 DEBUG TRAIN Batch 11/6300 loss 20.193243 loss_att 23.676485 loss_ctc 21.039680 loss_rnnt 19.343143 hw_loss 0.076107 lr 0.00050491 rank 1
2023-02-20 20:40:46,959 DEBUG TRAIN Batch 11/6300 loss 21.547422 loss_att 22.736347 loss_ctc 23.793898 loss_rnnt 20.932045 hw_loss 0.146372 lr 0.00050491 rank 4
2023-02-20 20:40:46,961 DEBUG TRAIN Batch 11/6300 loss 23.871305 loss_att 31.607952 loss_ctc 30.245939 loss_rnnt 21.403744 hw_loss 0.131785 lr 0.00050491 rank 3
2023-02-20 20:40:46,962 DEBUG TRAIN Batch 11/6300 loss 12.182138 loss_att 18.231762 loss_ctc 11.912853 loss_rnnt 10.928354 hw_loss 0.149558 lr 0.00050491 rank 0
2023-02-20 20:40:46,965 DEBUG TRAIN Batch 11/6300 loss 15.605627 loss_att 17.512844 loss_ctc 17.144592 loss_rnnt 14.998715 hw_loss 0.038011 lr 0.00050491 rank 5
2023-02-20 20:40:46,967 DEBUG TRAIN Batch 11/6300 loss 15.671638 loss_att 14.903856 loss_ctc 18.468077 loss_rnnt 15.272025 hw_loss 0.338086 lr 0.00050491 rank 6
2023-02-20 20:42:09,060 DEBUG TRAIN Batch 11/6400 loss 18.291046 loss_att 17.695244 loss_ctc 21.394606 loss_rnnt 17.809641 hw_loss 0.350168 lr 0.00050465 rank 4
2023-02-20 20:42:09,065 DEBUG TRAIN Batch 11/6400 loss 17.119612 loss_att 24.630138 loss_ctc 20.231247 loss_rnnt 15.187836 hw_loss 0.027723 lr 0.00050465 rank 2
2023-02-20 20:42:09,069 DEBUG TRAIN Batch 11/6400 loss 19.032761 loss_att 28.144829 loss_ctc 25.671467 loss_rnnt 16.310402 hw_loss 0.027723 lr 0.00050465 rank 7
2023-02-20 20:42:09,070 DEBUG TRAIN Batch 11/6400 loss 15.981762 loss_att 19.956818 loss_ctc 20.915110 loss_rnnt 14.480412 hw_loss 0.091050 lr 0.00050465 rank 3
2023-02-20 20:42:09,071 DEBUG TRAIN Batch 11/6400 loss 24.408516 loss_att 25.507992 loss_ctc 25.622917 loss_rnnt 23.906090 hw_loss 0.226145 lr 0.00050465 rank 1
2023-02-20 20:42:09,072 DEBUG TRAIN Batch 11/6400 loss 23.946157 loss_att 26.994839 loss_ctc 27.407259 loss_rnnt 22.784565 hw_loss 0.169452 lr 0.00050465 rank 0
2023-02-20 20:42:09,075 DEBUG TRAIN Batch 11/6400 loss 40.837086 loss_att 53.193886 loss_ctc 44.201962 loss_rnnt 37.850803 hw_loss 0.124253 lr 0.00050465 rank 5
2023-02-20 20:42:09,075 DEBUG TRAIN Batch 11/6400 loss 10.082832 loss_att 15.441644 loss_ctc 11.005522 loss_rnnt 8.773005 hw_loss 0.215700 lr 0.00050465 rank 6
2023-02-20 20:43:28,492 DEBUG TRAIN Batch 11/6500 loss 13.543735 loss_att 17.685257 loss_ctc 18.837065 loss_rnnt 11.962665 hw_loss 0.088102 lr 0.00050439 rank 4
2023-02-20 20:43:28,494 DEBUG TRAIN Batch 11/6500 loss 18.638819 loss_att 22.549679 loss_ctc 22.541159 loss_rnnt 17.237339 hw_loss 0.185618 lr 0.00050439 rank 7
2023-02-20 20:43:28,499 DEBUG TRAIN Batch 11/6500 loss 29.004766 loss_att 31.457218 loss_ctc 35.930267 loss_rnnt 27.506901 hw_loss 0.157446 lr 0.00050439 rank 1
2023-02-20 20:43:28,499 DEBUG TRAIN Batch 11/6500 loss 32.982464 loss_att 40.389080 loss_ctc 42.646286 loss_rnnt 30.082592 hw_loss 0.243827 lr 0.00050439 rank 5
2023-02-20 20:43:28,499 DEBUG TRAIN Batch 11/6500 loss 5.276701 loss_att 12.126702 loss_ctc 7.882487 loss_rnnt 3.527017 hw_loss 0.060461 lr 0.00050439 rank 0
2023-02-20 20:43:28,500 DEBUG TRAIN Batch 11/6500 loss 27.854120 loss_att 31.910315 loss_ctc 29.592419 loss_rnnt 26.779942 hw_loss 0.058437 lr 0.00050439 rank 2
2023-02-20 20:43:28,505 DEBUG TRAIN Batch 11/6500 loss 20.959057 loss_att 26.243742 loss_ctc 25.421696 loss_rnnt 19.284124 hw_loss 0.043082 lr 0.00050439 rank 6
2023-02-20 20:43:28,551 DEBUG TRAIN Batch 11/6500 loss 12.119426 loss_att 21.090651 loss_ctc 12.897451 loss_rnnt 10.132025 hw_loss 0.167660 lr 0.00050439 rank 3
2023-02-20 20:44:47,182 DEBUG TRAIN Batch 11/6600 loss 41.890034 loss_att 45.410393 loss_ctc 48.232056 loss_rnnt 40.328690 hw_loss 0.021890 lr 0.00050414 rank 2
2023-02-20 20:44:47,184 DEBUG TRAIN Batch 11/6600 loss 44.153706 loss_att 43.445892 loss_ctc 67.742577 loss_rnnt 41.068329 hw_loss 0.153295 lr 0.00050414 rank 1
2023-02-20 20:44:47,186 DEBUG TRAIN Batch 11/6600 loss 37.984047 loss_att 36.023403 loss_ctc 40.951435 loss_rnnt 37.968853 hw_loss 0.021890 lr 0.00050414 rank 4
2023-02-20 20:44:47,188 DEBUG TRAIN Batch 11/6600 loss 48.697224 loss_att 56.842018 loss_ctc 55.806931 loss_rnnt 46.047043 hw_loss 0.137367 lr 0.00050414 rank 3
2023-02-20 20:44:47,192 DEBUG TRAIN Batch 11/6600 loss 13.131486 loss_att 22.104004 loss_ctc 17.057316 loss_rnnt 10.764912 hw_loss 0.091177 lr 0.00050414 rank 7
2023-02-20 20:44:47,194 DEBUG TRAIN Batch 11/6600 loss 6.623183 loss_att 14.817467 loss_ctc 8.001547 loss_rnnt 4.749019 hw_loss 0.096611 lr 0.00050414 rank 0
2023-02-20 20:44:47,195 DEBUG TRAIN Batch 11/6600 loss 33.074066 loss_att 34.452011 loss_ctc 37.542908 loss_rnnt 32.190956 hw_loss 0.021890 lr 0.00050414 rank 6
2023-02-20 20:44:47,198 DEBUG TRAIN Batch 11/6600 loss 18.179058 loss_att 23.526369 loss_ctc 18.058784 loss_rnnt 17.113956 hw_loss 0.021890 lr 0.00050414 rank 5
2023-02-20 20:46:09,020 DEBUG TRAIN Batch 11/6700 loss 31.116346 loss_att 36.369537 loss_ctc 31.887344 loss_rnnt 29.926855 hw_loss 0.067603 lr 0.00050388 rank 3
2023-02-20 20:46:09,020 DEBUG TRAIN Batch 11/6700 loss 14.102870 loss_att 18.238489 loss_ctc 15.928534 loss_rnnt 12.995282 hw_loss 0.069453 lr 0.00050388 rank 6
2023-02-20 20:46:09,020 DEBUG TRAIN Batch 11/6700 loss 14.078438 loss_att 18.584150 loss_ctc 10.826467 loss_rnnt 13.530933 hw_loss 0.149922 lr 0.00050388 rank 1
2023-02-20 20:46:09,021 DEBUG TRAIN Batch 11/6700 loss 15.069937 loss_att 22.881870 loss_ctc 21.163851 loss_rnnt 12.635464 hw_loss 0.111683 lr 0.00050388 rank 2
2023-02-20 20:46:09,024 DEBUG TRAIN Batch 11/6700 loss 21.525112 loss_att 26.566826 loss_ctc 18.902960 loss_rnnt 20.809521 hw_loss 0.106631 lr 0.00050388 rank 0
2023-02-20 20:46:09,027 DEBUG TRAIN Batch 11/6700 loss 30.088032 loss_att 32.941124 loss_ctc 33.433517 loss_rnnt 28.992508 hw_loss 0.147827 lr 0.00050388 rank 5
2023-02-20 20:46:09,051 DEBUG TRAIN Batch 11/6700 loss 13.363947 loss_att 14.608521 loss_ctc 15.807581 loss_rnnt 12.756901 hw_loss 0.060588 lr 0.00050388 rank 7
2023-02-20 20:46:09,062 DEBUG TRAIN Batch 11/6700 loss 18.554018 loss_att 22.758451 loss_ctc 21.473608 loss_rnnt 17.310925 hw_loss 0.024238 lr 0.00050388 rank 4
2023-02-20 20:47:29,965 DEBUG TRAIN Batch 11/6800 loss 25.198465 loss_att 32.601082 loss_ctc 29.897446 loss_rnnt 22.994345 hw_loss 0.182004 lr 0.00050363 rank 0
2023-02-20 20:47:29,966 DEBUG TRAIN Batch 11/6800 loss 9.317211 loss_att 13.540319 loss_ctc 8.379272 loss_rnnt 8.528937 hw_loss 0.128831 lr 0.00050363 rank 2
2023-02-20 20:47:29,968 DEBUG TRAIN Batch 11/6800 loss 19.107794 loss_att 28.557426 loss_ctc 23.892941 loss_rnnt 16.505489 hw_loss 0.139422 lr 0.00050363 rank 7
2023-02-20 20:47:29,969 DEBUG TRAIN Batch 11/6800 loss 11.257539 loss_att 16.297491 loss_ctc 11.518147 loss_rnnt 10.121793 hw_loss 0.174388 lr 0.00050363 rank 3
2023-02-20 20:47:29,970 DEBUG TRAIN Batch 11/6800 loss 19.505535 loss_att 25.270397 loss_ctc 28.002567 loss_rnnt 17.180363 hw_loss 0.073615 lr 0.00050363 rank 4
2023-02-20 20:47:29,970 DEBUG TRAIN Batch 11/6800 loss 13.631211 loss_att 19.153049 loss_ctc 13.376531 loss_rnnt 12.485353 hw_loss 0.141463 lr 0.00050363 rank 1
2023-02-20 20:47:29,976 DEBUG TRAIN Batch 11/6800 loss 7.219794 loss_att 12.522006 loss_ctc 6.683017 loss_rnnt 6.188261 hw_loss 0.079990 lr 0.00050363 rank 5
2023-02-20 20:47:29,977 DEBUG TRAIN Batch 11/6800 loss 14.617641 loss_att 16.143085 loss_ctc 20.205944 loss_rnnt 13.448834 hw_loss 0.222396 lr 0.00050363 rank 6
2023-02-20 20:48:50,332 DEBUG TRAIN Batch 11/6900 loss 19.165764 loss_att 20.699463 loss_ctc 22.681503 loss_rnnt 18.300997 hw_loss 0.167364 lr 0.00050337 rank 7
2023-02-20 20:48:50,334 DEBUG TRAIN Batch 11/6900 loss 32.272182 loss_att 32.615440 loss_ctc 36.257137 loss_rnnt 31.612209 hw_loss 0.112495 lr 0.00050337 rank 2
2023-02-20 20:48:50,339 DEBUG TRAIN Batch 11/6900 loss 7.166493 loss_att 12.628286 loss_ctc 5.845237 loss_rnnt 6.160475 hw_loss 0.168425 lr 0.00050337 rank 4
2023-02-20 20:48:50,339 DEBUG TRAIN Batch 11/6900 loss 20.958488 loss_att 23.411568 loss_ctc 28.081074 loss_rnnt 19.462769 hw_loss 0.103923 lr 0.00050337 rank 5
2023-02-20 20:48:50,340 DEBUG TRAIN Batch 11/6900 loss 26.216282 loss_att 28.037777 loss_ctc 32.143494 loss_rnnt 25.032761 hw_loss 0.054235 lr 0.00050337 rank 0
2023-02-20 20:48:50,341 DEBUG TRAIN Batch 11/6900 loss 12.910872 loss_att 17.949181 loss_ctc 13.969351 loss_rnnt 11.660004 hw_loss 0.191389 lr 0.00050337 rank 1
2023-02-20 20:48:50,345 DEBUG TRAIN Batch 11/6900 loss 13.629051 loss_att 14.703563 loss_ctc 17.362453 loss_rnnt 12.830820 hw_loss 0.160388 lr 0.00050337 rank 6
2023-02-20 20:48:50,395 DEBUG TRAIN Batch 11/6900 loss 9.065195 loss_att 12.645634 loss_ctc 9.394464 loss_rnnt 8.209181 hw_loss 0.180043 lr 0.00050337 rank 3
2023-02-20 20:50:11,376 DEBUG TRAIN Batch 11/7000 loss 13.942534 loss_att 17.848471 loss_ctc 15.546851 loss_rnnt 12.843683 hw_loss 0.194537 lr 0.00050312 rank 1
2023-02-20 20:50:11,381 DEBUG TRAIN Batch 11/7000 loss 13.087149 loss_att 20.764309 loss_ctc 17.217140 loss_rnnt 10.954641 hw_loss 0.087019 lr 0.00050312 rank 5
2023-02-20 20:50:11,382 DEBUG TRAIN Batch 11/7000 loss 22.138426 loss_att 28.678837 loss_ctc 27.937962 loss_rnnt 20.040268 hw_loss 0.031507 lr 0.00050312 rank 6
2023-02-20 20:50:11,383 DEBUG TRAIN Batch 11/7000 loss 23.207561 loss_att 25.205229 loss_ctc 28.100006 loss_rnnt 22.131554 hw_loss 0.045275 lr 0.00050312 rank 0
2023-02-20 20:50:11,383 DEBUG TRAIN Batch 11/7000 loss 18.252159 loss_att 28.934299 loss_ctc 18.889921 loss_rnnt 15.977901 hw_loss 0.098992 lr 0.00050312 rank 2
2023-02-20 20:50:11,382 DEBUG TRAIN Batch 11/7000 loss 19.457300 loss_att 23.226967 loss_ctc 24.739086 loss_rnnt 17.982325 hw_loss 0.031507 lr 0.00050312 rank 4
2023-02-20 20:50:11,386 DEBUG TRAIN Batch 11/7000 loss 16.356571 loss_att 18.286247 loss_ctc 22.328703 loss_rnnt 15.102869 hw_loss 0.134027 lr 0.00050312 rank 3
2023-02-20 20:50:11,392 DEBUG TRAIN Batch 11/7000 loss 38.689770 loss_att 40.932144 loss_ctc 40.304192 loss_rnnt 37.962170 hw_loss 0.119757 lr 0.00050312 rank 7
2023-02-20 20:51:33,862 DEBUG TRAIN Batch 11/7100 loss 29.765663 loss_att 36.952560 loss_ctc 32.770145 loss_rnnt 27.913391 hw_loss 0.026804 lr 0.00050286 rank 7
2023-02-20 20:51:33,864 DEBUG TRAIN Batch 11/7100 loss 33.605465 loss_att 44.858833 loss_ctc 40.948448 loss_rnnt 30.329906 hw_loss 0.085916 lr 0.00050286 rank 1
2023-02-20 20:51:33,868 DEBUG TRAIN Batch 11/7100 loss 9.593365 loss_att 13.249894 loss_ctc 10.560032 loss_rnnt 8.686822 hw_loss 0.086900 lr 0.00050286 rank 4
2023-02-20 20:51:33,869 DEBUG TRAIN Batch 11/7100 loss 11.773608 loss_att 18.262077 loss_ctc 14.307673 loss_rnnt 10.100432 hw_loss 0.070513 lr 0.00050286 rank 2
2023-02-20 20:51:33,870 DEBUG TRAIN Batch 11/7100 loss 11.686307 loss_att 10.822884 loss_ctc 12.676191 loss_rnnt 11.534950 hw_loss 0.360106 lr 0.00050286 rank 3
2023-02-20 20:51:33,876 DEBUG TRAIN Batch 11/7100 loss 16.052168 loss_att 19.949865 loss_ctc 20.162266 loss_rnnt 14.670256 hw_loss 0.101924 lr 0.00050286 rank 5
2023-02-20 20:51:33,877 DEBUG TRAIN Batch 11/7100 loss 8.956203 loss_att 16.134266 loss_ctc 14.533088 loss_rnnt 6.704433 hw_loss 0.136070 lr 0.00050286 rank 0
2023-02-20 20:51:33,877 DEBUG TRAIN Batch 11/7100 loss 16.541548 loss_att 27.408474 loss_ctc 17.863363 loss_rnnt 14.142013 hw_loss 0.093580 lr 0.00050286 rank 6
2023-02-20 20:52:53,615 DEBUG TRAIN Batch 11/7200 loss 31.938387 loss_att 47.335056 loss_ctc 39.072014 loss_rnnt 27.839249 hw_loss 0.128723 lr 0.00050261 rank 1
2023-02-20 20:52:53,617 DEBUG TRAIN Batch 11/7200 loss 16.950676 loss_att 26.276594 loss_ctc 19.698959 loss_rnnt 14.664134 hw_loss 0.102977 lr 0.00050261 rank 2
2023-02-20 20:52:53,622 DEBUG TRAIN Batch 11/7200 loss 17.343218 loss_att 29.575851 loss_ctc 19.231714 loss_rnnt 14.554983 hw_loss 0.168580 lr 0.00050261 rank 7
2023-02-20 20:52:53,623 DEBUG TRAIN Batch 11/7200 loss 19.693657 loss_att 31.783480 loss_ctc 20.609449 loss_rnnt 17.139685 hw_loss 0.026062 lr 0.00050261 rank 4
2023-02-20 20:52:53,627 DEBUG TRAIN Batch 11/7200 loss 11.939550 loss_att 19.575428 loss_ctc 13.319644 loss_rnnt 10.061153 hw_loss 0.313516 lr 0.00050261 rank 5
2023-02-20 20:52:53,651 DEBUG TRAIN Batch 11/7200 loss 14.947667 loss_att 19.984791 loss_ctc 17.889027 loss_rnnt 13.534163 hw_loss 0.026061 lr 0.00050261 rank 6
2023-02-20 20:52:53,662 DEBUG TRAIN Batch 11/7200 loss 12.736679 loss_att 15.890150 loss_ctc 10.918058 loss_rnnt 12.321899 hw_loss 0.049815 lr 0.00050261 rank 3
2023-02-20 20:52:53,664 DEBUG TRAIN Batch 11/7200 loss 8.468173 loss_att 17.751045 loss_ctc 8.369219 loss_rnnt 6.610893 hw_loss 0.026062 lr 0.00050261 rank 0
2023-02-20 20:54:14,009 DEBUG TRAIN Batch 11/7300 loss 19.332327 loss_att 19.854013 loss_ctc 19.099041 loss_rnnt 19.176231 hw_loss 0.155369 lr 0.00050235 rank 3
2023-02-20 20:54:14,012 DEBUG TRAIN Batch 11/7300 loss 16.446180 loss_att 22.558516 loss_ctc 22.475733 loss_rnnt 14.347824 hw_loss 0.134900 lr 0.00050235 rank 4
2023-02-20 20:54:14,012 DEBUG TRAIN Batch 11/7300 loss 14.266747 loss_att 24.129744 loss_ctc 16.139805 loss_rnnt 11.999923 hw_loss 0.083408 lr 0.00050235 rank 5
2023-02-20 20:54:14,013 DEBUG TRAIN Batch 11/7300 loss 26.527590 loss_att 28.967176 loss_ctc 31.033335 loss_rnnt 25.350792 hw_loss 0.165210 lr 0.00050235 rank 2
2023-02-20 20:54:14,014 DEBUG TRAIN Batch 11/7300 loss 11.828243 loss_att 17.101616 loss_ctc 15.180470 loss_rnnt 10.311972 hw_loss 0.027438 lr 0.00050235 rank 0
2023-02-20 20:54:14,016 DEBUG TRAIN Batch 11/7300 loss 25.923269 loss_att 29.735374 loss_ctc 34.508469 loss_rnnt 23.961721 hw_loss 0.102065 lr 0.00050235 rank 7
2023-02-20 20:54:14,016 DEBUG TRAIN Batch 11/7300 loss 18.627892 loss_att 25.299023 loss_ctc 23.055786 loss_rnnt 16.668747 hw_loss 0.064751 lr 0.00050235 rank 1
2023-02-20 20:54:14,019 DEBUG TRAIN Batch 11/7300 loss 26.506836 loss_att 36.062607 loss_ctc 29.129669 loss_rnnt 24.135813 hw_loss 0.206542 lr 0.00050235 rank 6
2023-02-20 20:55:33,893 DEBUG TRAIN Batch 11/7400 loss 22.728472 loss_att 29.187702 loss_ctc 25.102329 loss_rnnt 21.080643 hw_loss 0.073999 lr 0.00050210 rank 4
2023-02-20 20:55:33,896 DEBUG TRAIN Batch 11/7400 loss 35.131332 loss_att 45.138649 loss_ctc 39.020695 loss_rnnt 32.524757 hw_loss 0.162234 lr 0.00050210 rank 1
2023-02-20 20:55:33,897 DEBUG TRAIN Batch 11/7400 loss 10.600657 loss_att 15.135489 loss_ctc 12.415249 loss_rnnt 9.437714 hw_loss 0.026308 lr 0.00050210 rank 2
2023-02-20 20:55:33,899 DEBUG TRAIN Batch 11/7400 loss 11.164278 loss_att 15.348445 loss_ctc 9.520935 loss_rnnt 10.515569 hw_loss 0.058103 lr 0.00050210 rank 0
2023-02-20 20:55:33,900 DEBUG TRAIN Batch 11/7400 loss 23.322071 loss_att 26.276030 loss_ctc 29.413586 loss_rnnt 21.846718 hw_loss 0.135674 lr 0.00050210 rank 7
2023-02-20 20:55:33,901 DEBUG TRAIN Batch 11/7400 loss 14.953216 loss_att 19.380737 loss_ctc 16.760252 loss_rnnt 13.781052 hw_loss 0.085727 lr 0.00050210 rank 6
2023-02-20 20:55:33,901 DEBUG TRAIN Batch 11/7400 loss 30.678669 loss_att 31.097519 loss_ctc 34.709610 loss_rnnt 29.998100 hw_loss 0.111264 lr 0.00050210 rank 5
2023-02-20 20:55:33,904 DEBUG TRAIN Batch 11/7400 loss 10.024857 loss_att 17.180508 loss_ctc 11.936484 loss_rnnt 8.300648 hw_loss 0.071617 lr 0.00050210 rank 3
2023-02-20 20:56:55,687 DEBUG TRAIN Batch 11/7500 loss 25.913198 loss_att 27.679937 loss_ctc 28.073204 loss_rnnt 25.183582 hw_loss 0.165502 lr 0.00050185 rank 1
2023-02-20 20:56:55,689 DEBUG TRAIN Batch 11/7500 loss 11.295276 loss_att 21.023766 loss_ctc 15.157372 loss_rnnt 8.727123 hw_loss 0.201575 lr 0.00050185 rank 4
2023-02-20 20:56:55,691 DEBUG TRAIN Batch 11/7500 loss 21.752207 loss_att 24.806763 loss_ctc 25.323141 loss_rnnt 20.613161 hw_loss 0.097518 lr 0.00050185 rank 7
2023-02-20 20:56:55,692 DEBUG TRAIN Batch 11/7500 loss 18.482092 loss_att 23.811092 loss_ctc 20.672604 loss_rnnt 17.022665 hw_loss 0.190422 lr 0.00050185 rank 5
2023-02-20 20:56:55,693 DEBUG TRAIN Batch 11/7500 loss 19.315037 loss_att 22.911631 loss_ctc 22.008982 loss_rnnt 18.082748 hw_loss 0.288333 lr 0.00050185 rank 2
2023-02-20 20:56:55,697 DEBUG TRAIN Batch 11/7500 loss 22.475149 loss_att 30.210461 loss_ctc 26.629513 loss_rnnt 20.293941 hw_loss 0.150427 lr 0.00050185 rank 0
2023-02-20 20:56:55,702 DEBUG TRAIN Batch 11/7500 loss 15.477245 loss_att 16.793724 loss_ctc 16.999508 loss_rnnt 14.969047 hw_loss 0.078627 lr 0.00050185 rank 6
2023-02-20 20:56:55,706 DEBUG TRAIN Batch 11/7500 loss 18.203074 loss_att 22.096401 loss_ctc 17.815647 loss_rnnt 17.392328 hw_loss 0.157005 lr 0.00050185 rank 3
2023-02-20 20:58:17,168 DEBUG TRAIN Batch 11/7600 loss 15.285928 loss_att 19.484306 loss_ctc 19.694534 loss_rnnt 13.752665 hw_loss 0.198324 lr 0.00050160 rank 3
2023-02-20 20:58:17,170 DEBUG TRAIN Batch 11/7600 loss 27.387533 loss_att 29.530449 loss_ctc 32.705734 loss_rnnt 26.211742 hw_loss 0.071466 lr 0.00050160 rank 2
2023-02-20 20:58:17,172 DEBUG TRAIN Batch 11/7600 loss 23.286163 loss_att 26.016062 loss_ctc 29.489246 loss_rnnt 21.876278 hw_loss 0.069056 lr 0.00050160 rank 1
2023-02-20 20:58:17,174 DEBUG TRAIN Batch 11/7600 loss 18.174049 loss_att 20.576124 loss_ctc 22.247623 loss_rnnt 17.061089 hw_loss 0.167630 lr 0.00050160 rank 0
2023-02-20 20:58:17,176 DEBUG TRAIN Batch 11/7600 loss 19.532333 loss_att 19.304050 loss_ctc 21.205444 loss_rnnt 19.292858 hw_loss 0.116350 lr 0.00050160 rank 6
2023-02-20 20:58:17,178 DEBUG TRAIN Batch 11/7600 loss 15.552547 loss_att 16.183861 loss_ctc 17.534622 loss_rnnt 15.143321 hw_loss 0.035038 lr 0.00050160 rank 5
2023-02-20 20:58:17,178 DEBUG TRAIN Batch 11/7600 loss 20.945467 loss_att 22.672281 loss_ctc 27.031996 loss_rnnt 19.704565 hw_loss 0.157502 lr 0.00050160 rank 4
2023-02-20 20:58:17,179 DEBUG TRAIN Batch 11/7600 loss 14.019122 loss_att 14.400665 loss_ctc 16.555098 loss_rnnt 13.505179 hw_loss 0.186570 lr 0.00050160 rank 7
2023-02-20 20:59:37,548 DEBUG TRAIN Batch 11/7700 loss 16.472099 loss_att 19.344835 loss_ctc 20.047791 loss_rnnt 15.290751 hw_loss 0.243828 lr 0.00050134 rank 1
2023-02-20 20:59:37,550 DEBUG TRAIN Batch 11/7700 loss 13.683783 loss_att 12.365123 loss_ctc 15.019835 loss_rnnt 13.561896 hw_loss 0.389022 lr 0.00050134 rank 4
2023-02-20 20:59:37,551 DEBUG TRAIN Batch 11/7700 loss 12.614944 loss_att 15.486897 loss_ctc 12.688536 loss_rnnt 11.994086 hw_loss 0.068731 lr 0.00050134 rank 7
2023-02-20 20:59:37,553 DEBUG TRAIN Batch 11/7700 loss 14.888761 loss_att 15.015011 loss_ctc 16.281847 loss_rnnt 14.510581 hw_loss 0.313473 lr 0.00050134 rank 3
2023-02-20 20:59:37,554 DEBUG TRAIN Batch 11/7700 loss 12.297708 loss_att 15.341925 loss_ctc 9.850843 loss_rnnt 11.942656 hw_loss 0.135858 lr 0.00050134 rank 2
2023-02-20 20:59:37,556 DEBUG TRAIN Batch 11/7700 loss 19.890970 loss_att 20.338566 loss_ctc 24.049673 loss_rnnt 19.162136 hw_loss 0.159042 lr 0.00050134 rank 0
2023-02-20 20:59:37,559 DEBUG TRAIN Batch 11/7700 loss 22.049259 loss_att 23.028017 loss_ctc 23.708656 loss_rnnt 21.578243 hw_loss 0.101276 lr 0.00050134 rank 5
2023-02-20 20:59:37,604 DEBUG TRAIN Batch 11/7700 loss 23.331961 loss_att 26.885447 loss_ctc 23.832975 loss_rnnt 22.500448 hw_loss 0.101276 lr 0.00050134 rank 6
2023-02-20 21:01:00,207 DEBUG TRAIN Batch 11/7800 loss 18.606104 loss_att 23.127163 loss_ctc 21.891834 loss_rnnt 17.207935 hw_loss 0.104739 lr 0.00050109 rank 4
2023-02-20 21:01:00,207 DEBUG TRAIN Batch 11/7800 loss 20.026592 loss_att 27.813457 loss_ctc 24.522095 loss_rnnt 17.813957 hw_loss 0.104741 lr 0.00050109 rank 3
2023-02-20 21:01:00,208 DEBUG TRAIN Batch 11/7800 loss 33.597191 loss_att 46.937347 loss_ctc 41.692413 loss_rnnt 29.835852 hw_loss 0.026153 lr 0.00050109 rank 0
2023-02-20 21:01:00,208 DEBUG TRAIN Batch 11/7800 loss 25.178619 loss_att 28.230469 loss_ctc 28.801495 loss_rnnt 24.044350 hw_loss 0.076590 lr 0.00050109 rank 2
2023-02-20 21:01:00,209 DEBUG TRAIN Batch 11/7800 loss 10.658229 loss_att 18.484417 loss_ctc 12.047804 loss_rnnt 8.815866 hw_loss 0.172215 lr 0.00050109 rank 7
2023-02-20 21:01:00,210 DEBUG TRAIN Batch 11/7800 loss 9.409751 loss_att 13.948627 loss_ctc 10.441848 loss_rnnt 8.279818 hw_loss 0.158522 lr 0.00050109 rank 6
2023-02-20 21:01:00,215 DEBUG TRAIN Batch 11/7800 loss 8.587686 loss_att 13.035258 loss_ctc 8.587540 loss_rnnt 7.658422 hw_loss 0.074565 lr 0.00050109 rank 1
2023-02-20 21:01:00,219 DEBUG TRAIN Batch 11/7800 loss 12.441680 loss_att 13.856909 loss_ctc 13.482880 loss_rnnt 11.991514 hw_loss 0.053050 lr 0.00050109 rank 5
2023-02-20 21:02:19,898 DEBUG TRAIN Batch 11/7900 loss 21.734985 loss_att 26.987522 loss_ctc 24.719833 loss_rnnt 20.216982 hw_loss 0.130344 lr 0.00050084 rank 3
2023-02-20 21:02:19,900 DEBUG TRAIN Batch 11/7900 loss 19.953257 loss_att 27.868845 loss_ctc 21.235672 loss_rnnt 18.115280 hw_loss 0.157258 lr 0.00050084 rank 2
2023-02-20 21:02:19,901 DEBUG TRAIN Batch 11/7900 loss 16.166843 loss_att 21.731058 loss_ctc 22.112354 loss_rnnt 14.168570 hw_loss 0.173807 lr 0.00050084 rank 0
2023-02-20 21:02:19,901 DEBUG TRAIN Batch 11/7900 loss 6.821574 loss_att 9.966253 loss_ctc 9.178148 loss_rnnt 5.852201 hw_loss 0.049176 lr 0.00050084 rank 4
2023-02-20 21:02:19,902 DEBUG TRAIN Batch 11/7900 loss 10.973729 loss_att 13.737960 loss_ctc 12.578005 loss_rnnt 10.178294 hw_loss 0.053786 lr 0.00050084 rank 7
2023-02-20 21:02:19,905 DEBUG TRAIN Batch 11/7900 loss 22.706017 loss_att 28.537846 loss_ctc 27.104191 loss_rnnt 20.859144 hw_loss 0.176412 lr 0.00050084 rank 1
2023-02-20 21:02:19,911 DEBUG TRAIN Batch 11/7900 loss 25.735495 loss_att 27.083054 loss_ctc 26.904907 loss_rnnt 25.252918 hw_loss 0.107139 lr 0.00050084 rank 5
2023-02-20 21:02:19,950 DEBUG TRAIN Batch 11/7900 loss 25.065161 loss_att 31.065031 loss_ctc 33.244934 loss_rnnt 22.720049 hw_loss 0.102190 lr 0.00050084 rank 6
2023-02-20 21:03:40,166 DEBUG TRAIN Batch 11/8000 loss 25.356958 loss_att 29.210747 loss_ctc 30.048634 loss_rnnt 23.889486 hw_loss 0.133421 lr 0.00050059 rank 2
2023-02-20 21:03:40,169 DEBUG TRAIN Batch 11/8000 loss 16.468428 loss_att 22.517466 loss_ctc 15.427584 loss_rnnt 15.384768 hw_loss 0.023686 lr 0.00050059 rank 4
2023-02-20 21:03:40,169 DEBUG TRAIN Batch 11/8000 loss 16.933041 loss_att 21.732563 loss_ctc 17.887836 loss_rnnt 15.702908 hw_loss 0.267977 lr 0.00050059 rank 0
2023-02-20 21:03:40,171 DEBUG TRAIN Batch 11/8000 loss 16.595346 loss_att 17.981047 loss_ctc 18.953485 loss_rnnt 15.942380 hw_loss 0.115135 lr 0.00050059 rank 7
2023-02-20 21:03:40,173 DEBUG TRAIN Batch 11/8000 loss 15.631108 loss_att 19.582575 loss_ctc 19.989105 loss_rnnt 14.194194 hw_loss 0.122914 lr 0.00050059 rank 3
2023-02-20 21:03:40,175 DEBUG TRAIN Batch 11/8000 loss 23.422335 loss_att 29.295677 loss_ctc 25.788971 loss_rnnt 21.825134 hw_loss 0.200587 lr 0.00050059 rank 1
2023-02-20 21:03:40,176 DEBUG TRAIN Batch 11/8000 loss 19.600574 loss_att 25.431835 loss_ctc 17.029043 loss_rnnt 18.756784 hw_loss 0.038261 lr 0.00050059 rank 6
2023-02-20 21:03:40,178 DEBUG TRAIN Batch 11/8000 loss 16.201952 loss_att 21.422932 loss_ctc 18.932213 loss_rnnt 14.738930 hw_loss 0.102733 lr 0.00050059 rank 5
2023-02-20 21:05:00,011 DEBUG TRAIN Batch 11/8100 loss 7.219790 loss_att 10.473406 loss_ctc 8.176186 loss_rnnt 6.408992 hw_loss 0.061044 lr 0.00050034 rank 4
2023-02-20 21:05:00,012 DEBUG TRAIN Batch 11/8100 loss 15.683684 loss_att 19.963093 loss_ctc 17.318127 loss_rnnt 14.575831 hw_loss 0.063836 lr 0.00050034 rank 7
2023-02-20 21:05:00,013 DEBUG TRAIN Batch 11/8100 loss 27.775106 loss_att 34.199772 loss_ctc 35.738663 loss_rnnt 25.374140 hw_loss 0.101677 lr 0.00050034 rank 2
2023-02-20 21:05:00,017 DEBUG TRAIN Batch 11/8100 loss 12.105598 loss_att 19.281397 loss_ctc 16.756203 loss_rnnt 9.954393 hw_loss 0.179935 lr 0.00050034 rank 1
2023-02-20 21:05:00,019 DEBUG TRAIN Batch 11/8100 loss 18.360973 loss_att 24.584930 loss_ctc 16.545612 loss_rnnt 17.325109 hw_loss 0.062101 lr 0.00050034 rank 0
2023-02-20 21:05:00,021 DEBUG TRAIN Batch 11/8100 loss 11.544925 loss_att 14.608183 loss_ctc 17.774853 loss_rnnt 10.014956 hw_loss 0.162490 lr 0.00050034 rank 6
2023-02-20 21:05:00,026 DEBUG TRAIN Batch 11/8100 loss 9.162422 loss_att 13.161583 loss_ctc 14.955111 loss_rnnt 7.542366 hw_loss 0.089749 lr 0.00050034 rank 5
2023-02-20 21:05:00,070 DEBUG TRAIN Batch 11/8100 loss 13.571393 loss_att 19.476294 loss_ctc 16.725384 loss_rnnt 11.936762 hw_loss 0.062096 lr 0.00050034 rank 3
2023-02-20 21:06:20,405 DEBUG TRAIN Batch 11/8200 loss 18.487442 loss_att 17.740482 loss_ctc 20.980797 loss_rnnt 18.177633 hw_loss 0.237664 lr 0.00050009 rank 7
2023-02-20 21:06:20,407 DEBUG TRAIN Batch 11/8200 loss 12.026190 loss_att 11.319439 loss_ctc 13.519840 loss_rnnt 11.829171 hw_loss 0.261027 lr 0.00050009 rank 2
2023-02-20 21:06:20,407 DEBUG TRAIN Batch 11/8200 loss 9.774067 loss_att 11.954368 loss_ctc 12.984088 loss_rnnt 8.756629 hw_loss 0.287578 lr 0.00050009 rank 6
2023-02-20 21:06:20,409 DEBUG TRAIN Batch 11/8200 loss 13.161068 loss_att 14.478849 loss_ctc 15.383835 loss_rnnt 12.551341 hw_loss 0.093380 lr 0.00050009 rank 4
2023-02-20 21:06:20,412 DEBUG TRAIN Batch 11/8200 loss 25.140251 loss_att 25.424654 loss_ctc 25.767656 loss_rnnt 24.929707 hw_loss 0.131269 lr 0.00050009 rank 0
2023-02-20 21:06:20,413 DEBUG TRAIN Batch 11/8200 loss 20.035179 loss_att 25.730816 loss_ctc 25.770472 loss_rnnt 18.060766 hw_loss 0.132334 lr 0.00050009 rank 5
2023-02-20 21:06:20,413 DEBUG TRAIN Batch 11/8200 loss 11.973061 loss_att 14.977315 loss_ctc 14.105208 loss_rnnt 11.009394 hw_loss 0.147242 lr 0.00050009 rank 1
2023-02-20 21:06:20,416 DEBUG TRAIN Batch 11/8200 loss 23.004890 loss_att 25.817179 loss_ctc 25.393543 loss_rnnt 22.003796 hw_loss 0.225283 lr 0.00050009 rank 3
2023-02-20 21:07:38,913 DEBUG TRAIN Batch 11/8300 loss 11.058685 loss_att 14.183751 loss_ctc 10.618043 loss_rnnt 10.372459 hw_loss 0.224935 lr 0.00049984 rank 7
2023-02-20 21:07:38,914 DEBUG TRAIN Batch 11/8300 loss 21.957262 loss_att 31.621536 loss_ctc 31.065786 loss_rnnt 18.787050 hw_loss 0.042910 lr 0.00049984 rank 2
2023-02-20 21:07:38,916 DEBUG TRAIN Batch 11/8300 loss 14.033704 loss_att 15.859404 loss_ctc 15.289065 loss_rnnt 13.426404 hw_loss 0.140207 lr 0.00049984 rank 4
2023-02-20 21:07:38,918 DEBUG TRAIN Batch 11/8300 loss 17.488914 loss_att 22.762350 loss_ctc 23.582924 loss_rnnt 15.513735 hw_loss 0.202418 lr 0.00049984 rank 0
2023-02-20 21:07:38,918 DEBUG TRAIN Batch 11/8300 loss 19.129625 loss_att 22.323826 loss_ctc 19.303333 loss_rnnt 18.347660 hw_loss 0.224933 lr 0.00049984 rank 1
2023-02-20 21:07:38,920 DEBUG TRAIN Batch 11/8300 loss 35.091515 loss_att 43.139343 loss_ctc 39.707123 loss_rnnt 32.827271 hw_loss 0.073610 lr 0.00049984 rank 3
2023-02-20 21:07:38,926 DEBUG TRAIN Batch 11/8300 loss 15.370871 loss_att 25.886244 loss_ctc 17.338140 loss_rnnt 12.932025 hw_loss 0.137754 lr 0.00049984 rank 5
2023-02-20 21:07:38,965 DEBUG TRAIN Batch 11/8300 loss 15.082962 loss_att 17.192532 loss_ctc 13.883623 loss_rnnt 14.807361 hw_loss 0.025501 lr 0.00049984 rank 6
2023-02-20 21:08:28,906 DEBUG CV Batch 11/0 loss 3.000121 loss_att 2.721104 loss_ctc 3.447050 loss_rnnt 2.780683 hw_loss 0.404345 history loss 2.889005 rank 6
2023-02-20 21:08:28,906 DEBUG CV Batch 11/0 loss 3.000121 loss_att 2.721104 loss_ctc 3.447050 loss_rnnt 2.780683 hw_loss 0.404345 history loss 2.889005 rank 5
2023-02-20 21:08:28,911 DEBUG CV Batch 11/0 loss 3.000121 loss_att 2.721104 loss_ctc 3.447050 loss_rnnt 2.780683 hw_loss 0.404345 history loss 2.889005 rank 0
2023-02-20 21:08:28,918 DEBUG CV Batch 11/0 loss 3.000121 loss_att 2.721104 loss_ctc 3.447050 loss_rnnt 2.780683 hw_loss 0.404345 history loss 2.889005 rank 4
2023-02-20 21:08:28,918 DEBUG CV Batch 11/0 loss 3.000121 loss_att 2.721104 loss_ctc 3.447050 loss_rnnt 2.780683 hw_loss 0.404345 history loss 2.889005 rank 2
2023-02-20 21:08:28,922 DEBUG CV Batch 11/0 loss 3.000121 loss_att 2.721104 loss_ctc 3.447050 loss_rnnt 2.780683 hw_loss 0.404345 history loss 2.889005 rank 3
2023-02-20 21:08:28,939 DEBUG CV Batch 11/0 loss 3.000121 loss_att 2.721104 loss_ctc 3.447050 loss_rnnt 2.780683 hw_loss 0.404345 history loss 2.889005 rank 1
2023-02-20 21:08:28,943 DEBUG CV Batch 11/0 loss 3.000121 loss_att 2.721104 loss_ctc 3.447050 loss_rnnt 2.780683 hw_loss 0.404345 history loss 2.889005 rank 7
2023-02-20 21:08:40,138 DEBUG CV Batch 11/100 loss 11.054573 loss_att 13.896238 loss_ctc 14.568960 loss_rnnt 10.003910 hw_loss 0.025774 history loss 5.629948 rank 0
2023-02-20 21:08:40,142 DEBUG CV Batch 11/100 loss 11.054573 loss_att 13.896238 loss_ctc 14.568960 loss_rnnt 10.003910 hw_loss 0.025774 history loss 5.629948 rank 5
2023-02-20 21:08:40,162 DEBUG CV Batch 11/100 loss 11.054573 loss_att 13.896238 loss_ctc 14.568960 loss_rnnt 10.003910 hw_loss 0.025774 history loss 5.629948 rank 4
2023-02-20 21:08:40,172 DEBUG CV Batch 11/100 loss 11.054573 loss_att 13.896238 loss_ctc 14.568960 loss_rnnt 10.003910 hw_loss 0.025774 history loss 5.629948 rank 3
2023-02-20 21:08:40,230 DEBUG CV Batch 11/100 loss 11.054573 loss_att 13.896238 loss_ctc 14.568960 loss_rnnt 10.003910 hw_loss 0.025774 history loss 5.629948 rank 1
2023-02-20 21:08:40,433 DEBUG CV Batch 11/100 loss 11.054573 loss_att 13.896238 loss_ctc 14.568960 loss_rnnt 10.003910 hw_loss 0.025774 history loss 5.629948 rank 2
2023-02-20 21:08:40,833 DEBUG CV Batch 11/100 loss 11.054573 loss_att 13.896238 loss_ctc 14.568960 loss_rnnt 10.003910 hw_loss 0.025774 history loss 5.629948 rank 7
2023-02-20 21:08:41,644 DEBUG CV Batch 11/100 loss 11.054573 loss_att 13.896238 loss_ctc 14.568960 loss_rnnt 10.003910 hw_loss 0.025774 history loss 5.629948 rank 6
2023-02-20 21:08:53,480 DEBUG CV Batch 11/200 loss 9.314108 loss_att 24.364046 loss_ctc 8.983002 loss_rnnt 6.267479 hw_loss 0.151480 history loss 6.321782 rank 5
2023-02-20 21:08:53,503 DEBUG CV Batch 11/200 loss 9.314108 loss_att 24.364046 loss_ctc 8.983002 loss_rnnt 6.267479 hw_loss 0.151480 history loss 6.321782 rank 4
2023-02-20 21:08:53,610 DEBUG CV Batch 11/200 loss 9.314108 loss_att 24.364046 loss_ctc 8.983002 loss_rnnt 6.267479 hw_loss 0.151480 history loss 6.321782 rank 3
2023-02-20 21:08:53,629 DEBUG CV Batch 11/200 loss 9.314108 loss_att 24.364046 loss_ctc 8.983002 loss_rnnt 6.267479 hw_loss 0.151480 history loss 6.321782 rank 1
2023-02-20 21:08:53,762 DEBUG CV Batch 11/200 loss 9.314108 loss_att 24.364046 loss_ctc 8.983002 loss_rnnt 6.267479 hw_loss 0.151480 history loss 6.321782 rank 0
2023-02-20 21:08:53,840 DEBUG CV Batch 11/200 loss 9.314108 loss_att 24.364046 loss_ctc 8.983002 loss_rnnt 6.267479 hw_loss 0.151480 history loss 6.321782 rank 2
2023-02-20 21:08:54,322 DEBUG CV Batch 11/200 loss 9.314108 loss_att 24.364046 loss_ctc 8.983002 loss_rnnt 6.267479 hw_loss 0.151480 history loss 6.321782 rank 7
2023-02-20 21:08:55,403 DEBUG CV Batch 11/200 loss 9.314108 loss_att 24.364046 loss_ctc 8.983002 loss_rnnt 6.267479 hw_loss 0.151480 history loss 6.321782 rank 6
2023-02-20 21:09:05,720 DEBUG CV Batch 11/300 loss 7.400262 loss_att 8.293311 loss_ctc 9.474464 loss_rnnt 6.878185 hw_loss 0.125452 history loss 6.555183 rank 5
2023-02-20 21:09:05,889 DEBUG CV Batch 11/300 loss 7.400262 loss_att 8.293311 loss_ctc 9.474464 loss_rnnt 6.878185 hw_loss 0.125452 history loss 6.555183 rank 3
2023-02-20 21:09:05,919 DEBUG CV Batch 11/300 loss 7.400262 loss_att 8.293311 loss_ctc 9.474464 loss_rnnt 6.878185 hw_loss 0.125452 history loss 6.555183 rank 4
2023-02-20 21:09:06,046 DEBUG CV Batch 11/300 loss 7.400262 loss_att 8.293311 loss_ctc 9.474464 loss_rnnt 6.878185 hw_loss 0.125452 history loss 6.555183 rank 2
2023-02-20 21:09:06,087 DEBUG CV Batch 11/300 loss 7.400262 loss_att 8.293311 loss_ctc 9.474464 loss_rnnt 6.878185 hw_loss 0.125452 history loss 6.555183 rank 1
2023-02-20 21:09:06,647 DEBUG CV Batch 11/300 loss 7.400262 loss_att 8.293311 loss_ctc 9.474464 loss_rnnt 6.878185 hw_loss 0.125452 history loss 6.555183 rank 0
2023-02-20 21:09:07,517 DEBUG CV Batch 11/300 loss 7.400262 loss_att 8.293311 loss_ctc 9.474464 loss_rnnt 6.878185 hw_loss 0.125452 history loss 6.555183 rank 7
2023-02-20 21:09:07,844 DEBUG CV Batch 11/300 loss 7.400262 loss_att 8.293311 loss_ctc 9.474464 loss_rnnt 6.878185 hw_loss 0.125452 history loss 6.555183 rank 6
2023-02-20 21:09:17,726 DEBUG CV Batch 11/400 loss 34.178741 loss_att 151.637268 loss_ctc 16.450497 loss_rnnt 13.001383 hw_loss 0.092664 history loss 7.909955 rank 5
2023-02-20 21:09:17,845 DEBUG CV Batch 11/400 loss 34.178741 loss_att 151.637268 loss_ctc 16.450497 loss_rnnt 13.001383 hw_loss 0.092664 history loss 7.909955 rank 4
2023-02-20 21:09:17,862 DEBUG CV Batch 11/400 loss 34.178741 loss_att 151.637268 loss_ctc 16.450497 loss_rnnt 13.001383 hw_loss 0.092664 history loss 7.909955 rank 3
2023-02-20 21:09:17,981 DEBUG CV Batch 11/400 loss 34.178741 loss_att 151.637268 loss_ctc 16.450497 loss_rnnt 13.001383 hw_loss 0.092664 history loss 7.909955 rank 2
2023-02-20 21:09:18,062 DEBUG CV Batch 11/400 loss 34.178741 loss_att 151.637268 loss_ctc 16.450497 loss_rnnt 13.001383 hw_loss 0.092664 history loss 7.909955 rank 1
2023-02-20 21:09:18,717 DEBUG CV Batch 11/400 loss 34.178741 loss_att 151.637268 loss_ctc 16.450497 loss_rnnt 13.001383 hw_loss 0.092664 history loss 7.909955 rank 0
2023-02-20 21:09:19,790 DEBUG CV Batch 11/400 loss 34.178741 loss_att 151.637268 loss_ctc 16.450497 loss_rnnt 13.001383 hw_loss 0.092664 history loss 7.909955 rank 7
2023-02-20 21:09:19,960 DEBUG CV Batch 11/400 loss 34.178741 loss_att 151.637268 loss_ctc 16.450497 loss_rnnt 13.001383 hw_loss 0.092664 history loss 7.909955 rank 6
2023-02-20 21:09:28,285 DEBUG CV Batch 11/500 loss 6.451890 loss_att 7.993452 loss_ctc 7.866096 loss_rnnt 5.898067 hw_loss 0.106784 history loss 9.165724 rank 5
2023-02-20 21:09:28,364 DEBUG CV Batch 11/500 loss 6.451890 loss_att 7.993452 loss_ctc 7.866096 loss_rnnt 5.898067 hw_loss 0.106784 history loss 9.165724 rank 4
2023-02-20 21:09:28,515 DEBUG CV Batch 11/500 loss 6.451890 loss_att 7.993452 loss_ctc 7.866096 loss_rnnt 5.898067 hw_loss 0.106784 history loss 9.165724 rank 3
2023-02-20 21:09:28,549 DEBUG CV Batch 11/500 loss 6.451890 loss_att 7.993452 loss_ctc 7.866096 loss_rnnt 5.898067 hw_loss 0.106784 history loss 9.165724 rank 2
2023-02-20 21:09:28,649 DEBUG CV Batch 11/500 loss 6.451890 loss_att 7.993452 loss_ctc 7.866096 loss_rnnt 5.898067 hw_loss 0.106784 history loss 9.165724 rank 1
2023-02-20 21:09:30,579 DEBUG CV Batch 11/500 loss 6.451890 loss_att 7.993452 loss_ctc 7.866096 loss_rnnt 5.898067 hw_loss 0.106784 history loss 9.165724 rank 0
2023-02-20 21:09:30,587 DEBUG CV Batch 11/500 loss 6.451890 loss_att 7.993452 loss_ctc 7.866096 loss_rnnt 5.898067 hw_loss 0.106784 history loss 9.165724 rank 7
2023-02-20 21:09:30,675 DEBUG CV Batch 11/500 loss 6.451890 loss_att 7.993452 loss_ctc 7.866096 loss_rnnt 5.898067 hw_loss 0.106784 history loss 9.165724 rank 6
2023-02-20 21:09:40,357 DEBUG CV Batch 11/600 loss 10.499044 loss_att 10.201136 loss_ctc 11.872231 loss_rnnt 10.210567 hw_loss 0.309313 history loss 10.254858 rank 4
2023-02-20 21:09:40,513 DEBUG CV Batch 11/600 loss 10.499044 loss_att 10.201136 loss_ctc 11.872231 loss_rnnt 10.210567 hw_loss 0.309313 history loss 10.254858 rank 2
2023-02-20 21:09:40,626 DEBUG CV Batch 11/600 loss 10.499044 loss_att 10.201136 loss_ctc 11.872231 loss_rnnt 10.210567 hw_loss 0.309313 history loss 10.254858 rank 1
2023-02-20 21:09:40,710 DEBUG CV Batch 11/600 loss 10.499044 loss_att 10.201136 loss_ctc 11.872231 loss_rnnt 10.210567 hw_loss 0.309313 history loss 10.254858 rank 5
2023-02-20 21:09:40,961 DEBUG CV Batch 11/600 loss 10.499044 loss_att 10.201136 loss_ctc 11.872231 loss_rnnt 10.210567 hw_loss 0.309313 history loss 10.254858 rank 3
2023-02-20 21:09:42,845 DEBUG CV Batch 11/600 loss 10.499044 loss_att 10.201136 loss_ctc 11.872231 loss_rnnt 10.210567 hw_loss 0.309313 history loss 10.254858 rank 6
2023-02-20 21:09:42,967 DEBUG CV Batch 11/600 loss 10.499044 loss_att 10.201136 loss_ctc 11.872231 loss_rnnt 10.210567 hw_loss 0.309313 history loss 10.254858 rank 7
2023-02-20 21:09:43,062 DEBUG CV Batch 11/600 loss 10.499044 loss_att 10.201136 loss_ctc 11.872231 loss_rnnt 10.210567 hw_loss 0.309313 history loss 10.254858 rank 0
2023-02-20 21:09:51,567 DEBUG CV Batch 11/700 loss 26.617052 loss_att 71.777435 loss_ctc 27.162397 loss_rnnt 17.457150 hw_loss 0.103337 history loss 11.161989 rank 4
2023-02-20 21:09:51,803 DEBUG CV Batch 11/700 loss 26.617052 loss_att 71.777435 loss_ctc 27.162397 loss_rnnt 17.457150 hw_loss 0.103337 history loss 11.161989 rank 2
2023-02-20 21:09:51,868 DEBUG CV Batch 11/700 loss 26.617052 loss_att 71.777435 loss_ctc 27.162397 loss_rnnt 17.457150 hw_loss 0.103337 history loss 11.161989 rank 1
2023-02-20 21:09:52,037 DEBUG CV Batch 11/700 loss 26.617052 loss_att 71.777435 loss_ctc 27.162397 loss_rnnt 17.457150 hw_loss 0.103337 history loss 11.161989 rank 5
2023-02-20 21:09:52,289 DEBUG CV Batch 11/700 loss 26.617052 loss_att 71.777435 loss_ctc 27.162397 loss_rnnt 17.457150 hw_loss 0.103337 history loss 11.161989 rank 3
2023-02-20 21:09:54,284 DEBUG CV Batch 11/700 loss 26.617052 loss_att 71.777435 loss_ctc 27.162397 loss_rnnt 17.457150 hw_loss 0.103337 history loss 11.161989 rank 6
2023-02-20 21:09:54,542 DEBUG CV Batch 11/700 loss 26.617052 loss_att 71.777435 loss_ctc 27.162397 loss_rnnt 17.457150 hw_loss 0.103337 history loss 11.161989 rank 7
2023-02-20 21:09:55,041 DEBUG CV Batch 11/700 loss 26.617052 loss_att 71.777435 loss_ctc 27.162397 loss_rnnt 17.457150 hw_loss 0.103337 history loss 11.161989 rank 0
2023-02-20 21:10:02,811 DEBUG CV Batch 11/800 loss 16.028009 loss_att 16.935757 loss_ctc 19.357887 loss_rnnt 15.285980 hw_loss 0.218431 history loss 10.417391 rank 4
2023-02-20 21:10:03,083 DEBUG CV Batch 11/800 loss 16.028009 loss_att 16.935757 loss_ctc 19.357887 loss_rnnt 15.285980 hw_loss 0.218431 history loss 10.417391 rank 2
2023-02-20 21:10:03,210 DEBUG CV Batch 11/800 loss 16.028009 loss_att 16.935757 loss_ctc 19.357887 loss_rnnt 15.285980 hw_loss 0.218431 history loss 10.417391 rank 1
2023-02-20 21:10:03,256 DEBUG CV Batch 11/800 loss 16.028009 loss_att 16.935757 loss_ctc 19.357887 loss_rnnt 15.285980 hw_loss 0.218431 history loss 10.417391 rank 5
2023-02-20 21:10:03,626 DEBUG CV Batch 11/800 loss 16.028009 loss_att 16.935757 loss_ctc 19.357887 loss_rnnt 15.285980 hw_loss 0.218431 history loss 10.417391 rank 3
2023-02-20 21:10:05,575 DEBUG CV Batch 11/800 loss 16.028009 loss_att 16.935757 loss_ctc 19.357887 loss_rnnt 15.285980 hw_loss 0.218431 history loss 10.417391 rank 6
2023-02-20 21:10:05,952 DEBUG CV Batch 11/800 loss 16.028009 loss_att 16.935757 loss_ctc 19.357887 loss_rnnt 15.285980 hw_loss 0.218431 history loss 10.417391 rank 7
2023-02-20 21:10:07,508 DEBUG CV Batch 11/800 loss 16.028009 loss_att 16.935757 loss_ctc 19.357887 loss_rnnt 15.285980 hw_loss 0.218431 history loss 10.417391 rank 0
2023-02-20 21:10:15,870 DEBUG CV Batch 11/900 loss 20.915224 loss_att 37.534206 loss_ctc 20.273735 loss_rnnt 17.619026 hw_loss 0.108626 history loss 10.121669 rank 4
2023-02-20 21:10:16,243 DEBUG CV Batch 11/900 loss 20.915224 loss_att 37.534206 loss_ctc 20.273735 loss_rnnt 17.619026 hw_loss 0.108626 history loss 10.121669 rank 2
2023-02-20 21:10:16,385 DEBUG CV Batch 11/900 loss 20.915224 loss_att 37.534206 loss_ctc 20.273735 loss_rnnt 17.619026 hw_loss 0.108626 history loss 10.121669 rank 1
2023-02-20 21:10:16,568 DEBUG CV Batch 11/900 loss 20.915224 loss_att 37.534206 loss_ctc 20.273735 loss_rnnt 17.619026 hw_loss 0.108626 history loss 10.121669 rank 5
2023-02-20 21:10:16,871 DEBUG CV Batch 11/900 loss 20.915224 loss_att 37.534206 loss_ctc 20.273735 loss_rnnt 17.619026 hw_loss 0.108626 history loss 10.121669 rank 3
2023-02-20 21:10:18,855 DEBUG CV Batch 11/900 loss 20.915224 loss_att 37.534206 loss_ctc 20.273735 loss_rnnt 17.619026 hw_loss 0.108626 history loss 10.121669 rank 6
2023-02-20 21:10:19,315 DEBUG CV Batch 11/900 loss 20.915224 loss_att 37.534206 loss_ctc 20.273735 loss_rnnt 17.619026 hw_loss 0.108626 history loss 10.121669 rank 7
2023-02-20 21:10:20,906 DEBUG CV Batch 11/900 loss 20.915224 loss_att 37.534206 loss_ctc 20.273735 loss_rnnt 17.619026 hw_loss 0.108626 history loss 10.121669 rank 0
2023-02-20 21:10:28,181 DEBUG CV Batch 11/1000 loss 6.700348 loss_att 7.108925 loss_ctc 6.761902 loss_rnnt 6.565320 hw_loss 0.084572 history loss 9.804717 rank 4
2023-02-20 21:10:28,393 DEBUG CV Batch 11/1000 loss 6.700348 loss_att 7.108925 loss_ctc 6.761902 loss_rnnt 6.565320 hw_loss 0.084572 history loss 9.804717 rank 2
2023-02-20 21:10:28,565 DEBUG CV Batch 11/1000 loss 6.700348 loss_att 7.108925 loss_ctc 6.761902 loss_rnnt 6.565320 hw_loss 0.084572 history loss 9.804717 rank 1
2023-02-20 21:10:28,723 DEBUG CV Batch 11/1000 loss 6.700348 loss_att 7.108925 loss_ctc 6.761902 loss_rnnt 6.565320 hw_loss 0.084572 history loss 9.804717 rank 5
2023-02-20 21:10:29,140 DEBUG CV Batch 11/1000 loss 6.700348 loss_att 7.108925 loss_ctc 6.761902 loss_rnnt 6.565320 hw_loss 0.084572 history loss 9.804717 rank 3
2023-02-20 21:10:31,265 DEBUG CV Batch 11/1000 loss 6.700348 loss_att 7.108925 loss_ctc 6.761902 loss_rnnt 6.565320 hw_loss 0.084572 history loss 9.804717 rank 6
2023-02-20 21:10:31,782 DEBUG CV Batch 11/1000 loss 6.700348 loss_att 7.108925 loss_ctc 6.761902 loss_rnnt 6.565320 hw_loss 0.084572 history loss 9.804717 rank 7
2023-02-20 21:10:33,282 DEBUG CV Batch 11/1000 loss 6.700348 loss_att 7.108925 loss_ctc 6.761902 loss_rnnt 6.565320 hw_loss 0.084572 history loss 9.804717 rank 0
2023-02-20 21:10:40,066 DEBUG CV Batch 11/1100 loss 8.893342 loss_att 8.047882 loss_ctc 10.391470 loss_rnnt 8.578902 hw_loss 0.532090 history loss 9.783699 rank 4
2023-02-20 21:10:40,240 DEBUG CV Batch 11/1100 loss 8.893342 loss_att 8.047882 loss_ctc 10.391470 loss_rnnt 8.578902 hw_loss 0.532090 history loss 9.783699 rank 2
2023-02-20 21:10:40,294 DEBUG CV Batch 11/1100 loss 8.893342 loss_att 8.047882 loss_ctc 10.391470 loss_rnnt 8.578902 hw_loss 0.532090 history loss 9.783699 rank 1
2023-02-20 21:10:40,651 DEBUG CV Batch 11/1100 loss 8.893342 loss_att 8.047882 loss_ctc 10.391470 loss_rnnt 8.578902 hw_loss 0.532090 history loss 9.783699 rank 5
2023-02-20 21:10:41,039 DEBUG CV Batch 11/1100 loss 8.893342 loss_att 8.047882 loss_ctc 10.391470 loss_rnnt 8.578902 hw_loss 0.532090 history loss 9.783699 rank 3
2023-02-20 21:10:43,313 DEBUG CV Batch 11/1100 loss 8.893342 loss_att 8.047882 loss_ctc 10.391470 loss_rnnt 8.578902 hw_loss 0.532090 history loss 9.783699 rank 6
2023-02-20 21:10:43,766 DEBUG CV Batch 11/1100 loss 8.893342 loss_att 8.047882 loss_ctc 10.391470 loss_rnnt 8.578902 hw_loss 0.532090 history loss 9.783699 rank 7
2023-02-20 21:10:45,199 DEBUG CV Batch 11/1100 loss 8.893342 loss_att 8.047882 loss_ctc 10.391470 loss_rnnt 8.578902 hw_loss 0.532090 history loss 9.783699 rank 0
2023-02-20 21:10:50,668 DEBUG CV Batch 11/1200 loss 10.344274 loss_att 13.650545 loss_ctc 11.285047 loss_rnnt 9.517916 hw_loss 0.074377 history loss 10.324365 rank 4
2023-02-20 21:10:50,869 DEBUG CV Batch 11/1200 loss 10.344274 loss_att 13.650545 loss_ctc 11.285047 loss_rnnt 9.517916 hw_loss 0.074377 history loss 10.324365 rank 2
2023-02-20 21:10:50,900 DEBUG CV Batch 11/1200 loss 10.344274 loss_att 13.650545 loss_ctc 11.285047 loss_rnnt 9.517916 hw_loss 0.074377 history loss 10.324365 rank 1
2023-02-20 21:10:51,266 DEBUG CV Batch 11/1200 loss 10.344274 loss_att 13.650545 loss_ctc 11.285047 loss_rnnt 9.517916 hw_loss 0.074377 history loss 10.324365 rank 5
2023-02-20 21:10:51,650 DEBUG CV Batch 11/1200 loss 10.344274 loss_att 13.650545 loss_ctc 11.285047 loss_rnnt 9.517916 hw_loss 0.074377 history loss 10.324365 rank 3
2023-02-20 21:10:54,090 DEBUG CV Batch 11/1200 loss 10.344274 loss_att 13.650545 loss_ctc 11.285047 loss_rnnt 9.517916 hw_loss 0.074377 history loss 10.324365 rank 6
2023-02-20 21:10:54,524 DEBUG CV Batch 11/1200 loss 10.344274 loss_att 13.650545 loss_ctc 11.285047 loss_rnnt 9.517916 hw_loss 0.074377 history loss 10.324365 rank 7
2023-02-20 21:10:55,823 DEBUG CV Batch 11/1200 loss 10.344274 loss_att 13.650545 loss_ctc 11.285047 loss_rnnt 9.517916 hw_loss 0.074377 history loss 10.324365 rank 0
2023-02-20 21:11:02,815 DEBUG CV Batch 11/1300 loss 8.407234 loss_att 7.383344 loss_ctc 9.611199 loss_rnnt 8.260984 hw_loss 0.357184 history loss 10.671927 rank 4
2023-02-20 21:11:02,851 DEBUG CV Batch 11/1300 loss 8.407234 loss_att 7.383344 loss_ctc 9.611199 loss_rnnt 8.260984 hw_loss 0.357184 history loss 10.671927 rank 2
2023-02-20 21:11:02,851 DEBUG CV Batch 11/1300 loss 8.407234 loss_att 7.383344 loss_ctc 9.611199 loss_rnnt 8.260984 hw_loss 0.357184 history loss 10.671927 rank 1
2023-02-20 21:11:03,290 DEBUG CV Batch 11/1300 loss 8.407234 loss_att 7.383344 loss_ctc 9.611199 loss_rnnt 8.260984 hw_loss 0.357184 history loss 10.671927 rank 5
2023-02-20 21:11:03,608 DEBUG CV Batch 11/1300 loss 8.407234 loss_att 7.383344 loss_ctc 9.611199 loss_rnnt 8.260984 hw_loss 0.357184 history loss 10.671927 rank 3
2023-02-20 21:11:06,150 DEBUG CV Batch 11/1300 loss 8.407234 loss_att 7.383344 loss_ctc 9.611199 loss_rnnt 8.260984 hw_loss 0.357184 history loss 10.671927 rank 6
2023-02-20 21:11:06,818 DEBUG CV Batch 11/1300 loss 8.407234 loss_att 7.383344 loss_ctc 9.611199 loss_rnnt 8.260984 hw_loss 0.357184 history loss 10.671927 rank 7
2023-02-20 21:11:07,950 DEBUG CV Batch 11/1300 loss 8.407234 loss_att 7.383344 loss_ctc 9.611199 loss_rnnt 8.260984 hw_loss 0.357184 history loss 10.671927 rank 0
2023-02-20 21:11:13,942 DEBUG CV Batch 11/1400 loss 20.414837 loss_att 63.411556 loss_ctc 16.564316 loss_rnnt 12.315149 hw_loss 0.025774 history loss 11.110600 rank 1
2023-02-20 21:11:13,948 DEBUG CV Batch 11/1400 loss 20.414837 loss_att 63.411556 loss_ctc 16.564316 loss_rnnt 12.315149 hw_loss 0.025774 history loss 11.110600 rank 2
2023-02-20 21:11:14,085 DEBUG CV Batch 11/1400 loss 20.414837 loss_att 63.411556 loss_ctc 16.564316 loss_rnnt 12.315149 hw_loss 0.025774 history loss 11.110600 rank 4
2023-02-20 21:11:14,607 DEBUG CV Batch 11/1400 loss 20.414837 loss_att 63.411556 loss_ctc 16.564316 loss_rnnt 12.315149 hw_loss 0.025774 history loss 11.110600 rank 5
2023-02-20 21:11:14,849 DEBUG CV Batch 11/1400 loss 20.414837 loss_att 63.411556 loss_ctc 16.564316 loss_rnnt 12.315149 hw_loss 0.025774 history loss 11.110600 rank 3
2023-02-20 21:11:17,394 DEBUG CV Batch 11/1400 loss 20.414837 loss_att 63.411556 loss_ctc 16.564316 loss_rnnt 12.315149 hw_loss 0.025774 history loss 11.110600 rank 6
2023-02-20 21:11:18,716 DEBUG CV Batch 11/1400 loss 20.414837 loss_att 63.411556 loss_ctc 16.564316 loss_rnnt 12.315149 hw_loss 0.025774 history loss 11.110600 rank 7
2023-02-20 21:11:19,170 DEBUG CV Batch 11/1400 loss 20.414837 loss_att 63.411556 loss_ctc 16.564316 loss_rnnt 12.315149 hw_loss 0.025774 history loss 11.110600 rank 0
2023-02-20 21:11:25,352 DEBUG CV Batch 11/1500 loss 10.258746 loss_att 13.505094 loss_ctc 9.443310 loss_rnnt 9.608713 hw_loss 0.205291 history loss 10.858034 rank 1
2023-02-20 21:11:25,390 DEBUG CV Batch 11/1500 loss 10.258746 loss_att 13.505094 loss_ctc 9.443310 loss_rnnt 9.608713 hw_loss 0.205291 history loss 10.858034 rank 2
2023-02-20 21:11:25,506 DEBUG CV Batch 11/1500 loss 10.258746 loss_att 13.505094 loss_ctc 9.443310 loss_rnnt 9.608713 hw_loss 0.205291 history loss 10.858034 rank 4
2023-02-20 21:11:26,204 DEBUG CV Batch 11/1500 loss 10.258746 loss_att 13.505094 loss_ctc 9.443310 loss_rnnt 9.608713 hw_loss 0.205291 history loss 10.858034 rank 3
2023-02-20 21:11:27,096 DEBUG CV Batch 11/1500 loss 10.258746 loss_att 13.505094 loss_ctc 9.443310 loss_rnnt 9.608713 hw_loss 0.205291 history loss 10.858034 rank 5
2023-02-20 21:11:28,873 DEBUG CV Batch 11/1500 loss 10.258746 loss_att 13.505094 loss_ctc 9.443310 loss_rnnt 9.608713 hw_loss 0.205291 history loss 10.858034 rank 6
2023-02-20 21:11:30,324 DEBUG CV Batch 11/1500 loss 10.258746 loss_att 13.505094 loss_ctc 9.443310 loss_rnnt 9.608713 hw_loss 0.205291 history loss 10.858034 rank 7
2023-02-20 21:11:30,734 DEBUG CV Batch 11/1500 loss 10.258746 loss_att 13.505094 loss_ctc 9.443310 loss_rnnt 9.608713 hw_loss 0.205291 history loss 10.858034 rank 0
2023-02-20 21:11:38,213 DEBUG CV Batch 11/1600 loss 11.232387 loss_att 27.503273 loss_ctc 13.832521 loss_rnnt 7.520566 hw_loss 0.208048 history loss 10.741226 rank 1
2023-02-20 21:11:38,363 DEBUG CV Batch 11/1600 loss 11.232387 loss_att 27.503273 loss_ctc 13.832521 loss_rnnt 7.520566 hw_loss 0.208048 history loss 10.741226 rank 2
2023-02-20 21:11:38,395 DEBUG CV Batch 11/1600 loss 11.232387 loss_att 27.503273 loss_ctc 13.832521 loss_rnnt 7.520566 hw_loss 0.208048 history loss 10.741226 rank 4
2023-02-20 21:11:39,190 DEBUG CV Batch 11/1600 loss 11.232387 loss_att 27.503273 loss_ctc 13.832521 loss_rnnt 7.520566 hw_loss 0.208048 history loss 10.741226 rank 3
2023-02-20 21:11:40,072 DEBUG CV Batch 11/1600 loss 11.232387 loss_att 27.503273 loss_ctc 13.832521 loss_rnnt 7.520566 hw_loss 0.208048 history loss 10.741226 rank 5
2023-02-20 21:11:41,835 DEBUG CV Batch 11/1600 loss 11.232387 loss_att 27.503273 loss_ctc 13.832521 loss_rnnt 7.520566 hw_loss 0.208048 history loss 10.741226 rank 6
2023-02-20 21:11:43,316 DEBUG CV Batch 11/1600 loss 11.232387 loss_att 27.503273 loss_ctc 13.832521 loss_rnnt 7.520566 hw_loss 0.208048 history loss 10.741226 rank 7
2023-02-20 21:11:43,758 DEBUG CV Batch 11/1600 loss 11.232387 loss_att 27.503273 loss_ctc 13.832521 loss_rnnt 7.520566 hw_loss 0.208048 history loss 10.741226 rank 0
2023-02-20 21:11:50,555 DEBUG CV Batch 11/1700 loss 16.733032 loss_att 16.032560 loss_ctc 21.098251 loss_rnnt 16.235687 hw_loss 0.103891 history loss 10.607856 rank 1
2023-02-20 21:11:50,769 DEBUG CV Batch 11/1700 loss 16.733032 loss_att 16.032560 loss_ctc 21.098251 loss_rnnt 16.235687 hw_loss 0.103891 history loss 10.607856 rank 2
2023-02-20 21:11:50,874 DEBUG CV Batch 11/1700 loss 16.733032 loss_att 16.032560 loss_ctc 21.098251 loss_rnnt 16.235687 hw_loss 0.103891 history loss 10.607856 rank 4
2023-02-20 21:11:51,528 DEBUG CV Batch 11/1700 loss 16.733032 loss_att 16.032560 loss_ctc 21.098251 loss_rnnt 16.235687 hw_loss 0.103891 history loss 10.607856 rank 3
2023-02-20 21:11:52,374 DEBUG CV Batch 11/1700 loss 16.733032 loss_att 16.032560 loss_ctc 21.098251 loss_rnnt 16.235687 hw_loss 0.103891 history loss 10.607856 rank 5
2023-02-20 21:11:54,148 DEBUG CV Batch 11/1700 loss 16.733032 loss_att 16.032560 loss_ctc 21.098251 loss_rnnt 16.235687 hw_loss 0.103891 history loss 10.607856 rank 6
2023-02-20 21:11:55,482 DEBUG CV Batch 11/1700 loss 16.733032 loss_att 16.032560 loss_ctc 21.098251 loss_rnnt 16.235687 hw_loss 0.103891 history loss 10.607856 rank 7
2023-02-20 21:11:55,922 DEBUG CV Batch 11/1700 loss 16.733032 loss_att 16.032560 loss_ctc 21.098251 loss_rnnt 16.235687 hw_loss 0.103891 history loss 10.607856 rank 0
2023-02-20 21:11:59,511 INFO Epoch 11 CV info cv_loss 10.552359261063417
2023-02-20 21:11:59,512 INFO Epoch 12 TRAIN info lr 0.0004997751517611834
2023-02-20 21:11:59,516 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-20 21:11:59,793 INFO Epoch 11 CV info cv_loss 10.552359260873896
2023-02-20 21:11:59,794 INFO Epoch 12 TRAIN info lr 0.0004997002697302832
2023-02-20 21:11:59,803 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-20 21:11:59,867 INFO Epoch 11 CV info cv_loss 10.552359260133036
2023-02-20 21:11:59,868 INFO Epoch 12 TRAIN info lr 0.0004997551799280648
2023-02-20 21:11:59,871 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-20 21:12:00,510 INFO Epoch 11 CV info cv_loss 10.552359260960042
2023-02-20 21:12:00,512 INFO Epoch 12 TRAIN info lr 0.0004997426987481652
2023-02-20 21:12:00,517 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-20 21:12:01,315 INFO Epoch 11 CV info cv_loss 10.552359259288803
2023-02-20 21:12:01,316 INFO Epoch 12 TRAIN info lr 0.0004997377065380372
2023-02-20 21:12:01,320 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-20 21:12:03,179 INFO Epoch 11 CV info cv_loss 10.552359259271574
2023-02-20 21:12:03,181 INFO Epoch 12 TRAIN info lr 0.000499722730805265
2023-02-20 21:12:03,185 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-20 21:12:04,510 INFO Epoch 11 CV info cv_loss 10.552359260684373
2023-02-20 21:12:04,511 INFO Epoch 12 TRAIN info lr 0.0004997077564187539
2023-02-20 21:12:04,515 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-20 21:12:04,988 INFO Epoch 11 CV info cv_loss 10.552359257514189
2023-02-20 21:12:04,989 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/11.pt
2023-02-20 21:12:05,816 INFO Epoch 12 TRAIN info lr 0.0004997951259889105
2023-02-20 21:12:05,821 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-20 21:13:18,005 DEBUG TRAIN Batch 12/0 loss 13.436082 loss_att 13.220562 loss_ctc 16.547705 loss_rnnt 12.871473 hw_loss 0.361554 lr 0.00049977 rank 1
2023-02-20 21:13:18,013 DEBUG TRAIN Batch 12/0 loss 14.446595 loss_att 13.119420 loss_ctc 15.172967 loss_rnnt 14.426941 hw_loss 0.352951 lr 0.00049974 rank 3
2023-02-20 21:13:18,015 DEBUG TRAIN Batch 12/0 loss 16.259129 loss_att 14.679642 loss_ctc 17.047434 loss_rnnt 16.189737 hw_loss 0.525339 lr 0.00049974 rank 5
2023-02-20 21:13:18,015 DEBUG TRAIN Batch 12/0 loss 18.634331 loss_att 16.465801 loss_ctc 21.765490 loss_rnnt 18.423317 hw_loss 0.426061 lr 0.00049970 rank 2
2023-02-20 21:13:18,018 DEBUG TRAIN Batch 12/0 loss 14.458677 loss_att 13.195999 loss_ctc 15.926691 loss_rnnt 14.286471 hw_loss 0.429388 lr 0.00049972 rank 6
2023-02-20 21:13:18,018 DEBUG TRAIN Batch 12/0 loss 12.902020 loss_att 11.619491 loss_ctc 14.015361 loss_rnnt 12.744724 hw_loss 0.497545 lr 0.00049975 rank 4
2023-02-20 21:13:18,046 DEBUG TRAIN Batch 12/0 loss 11.377643 loss_att 10.594126 loss_ctc 12.658801 loss_rnnt 11.143203 hw_loss 0.413106 lr 0.00049979 rank 0
2023-02-20 21:13:18,053 DEBUG TRAIN Batch 12/0 loss 17.810942 loss_att 16.660545 loss_ctc 20.336906 loss_rnnt 17.552029 hw_loss 0.285367 lr 0.00049971 rank 7
2023-02-20 21:14:33,378 DEBUG TRAIN Batch 12/100 loss 11.570433 loss_att 17.778900 loss_ctc 16.251427 loss_rnnt 9.655162 hw_loss 0.092709 lr 0.00049945 rank 2
2023-02-20 21:14:33,380 DEBUG TRAIN Batch 12/100 loss 14.311692 loss_att 17.228867 loss_ctc 17.803265 loss_rnnt 13.215832 hw_loss 0.087903 lr 0.00049949 rank 3
2023-02-20 21:14:33,382 DEBUG TRAIN Batch 12/100 loss 20.402428 loss_att 23.078119 loss_ctc 25.658598 loss_rnnt 19.124716 hw_loss 0.078282 lr 0.00049947 rank 6
2023-02-20 21:14:33,382 DEBUG TRAIN Batch 12/100 loss 15.812223 loss_att 20.012661 loss_ctc 21.617682 loss_rnnt 14.171476 hw_loss 0.049872 lr 0.00049952 rank 1
2023-02-20 21:14:33,385 DEBUG TRAIN Batch 12/100 loss 15.302633 loss_att 20.572739 loss_ctc 17.515036 loss_rnnt 13.913035 hw_loss 0.076106 lr 0.00049954 rank 0
2023-02-20 21:14:33,385 DEBUG TRAIN Batch 12/100 loss 29.903042 loss_att 36.954281 loss_ctc 36.320908 loss_rnnt 27.624405 hw_loss 0.023761 lr 0.00049950 rank 4
2023-02-20 21:14:33,388 DEBUG TRAIN Batch 12/100 loss 31.010334 loss_att 35.670441 loss_ctc 40.184212 loss_rnnt 28.802320 hw_loss 0.099015 lr 0.00049949 rank 5
2023-02-20 21:14:33,389 DEBUG TRAIN Batch 12/100 loss 18.354712 loss_att 23.954811 loss_ctc 23.554014 loss_rnnt 16.485346 hw_loss 0.105195 lr 0.00049946 rank 7
2023-02-20 21:15:49,868 DEBUG TRAIN Batch 12/200 loss 12.918407 loss_att 17.430954 loss_ctc 19.170948 loss_rnnt 11.111247 hw_loss 0.133084 lr 0.00049920 rank 2
2023-02-20 21:15:49,870 DEBUG TRAIN Batch 12/200 loss 20.170290 loss_att 29.392361 loss_ctc 21.409248 loss_rnnt 18.097530 hw_loss 0.118403 lr 0.00049927 rank 1
2023-02-20 21:15:49,874 DEBUG TRAIN Batch 12/200 loss 19.093399 loss_att 20.744944 loss_ctc 24.496143 loss_rnnt 17.988024 hw_loss 0.102563 lr 0.00049924 rank 3
2023-02-20 21:15:49,875 DEBUG TRAIN Batch 12/200 loss 22.399763 loss_att 30.885235 loss_ctc 26.821880 loss_rnnt 20.082075 hw_loss 0.058086 lr 0.00049924 rank 5
2023-02-20 21:15:49,875 DEBUG TRAIN Batch 12/200 loss 5.811491 loss_att 10.792909 loss_ctc 7.343959 loss_rnnt 4.483022 hw_loss 0.239730 lr 0.00049925 rank 4
2023-02-20 21:15:49,876 DEBUG TRAIN Batch 12/200 loss 19.488573 loss_att 27.222897 loss_ctc 24.669386 loss_rnnt 17.142487 hw_loss 0.203338 lr 0.00049921 rank 7
2023-02-20 21:15:49,878 DEBUG TRAIN Batch 12/200 loss 22.673592 loss_att 30.633808 loss_ctc 26.557644 loss_rnnt 20.481800 hw_loss 0.153511 lr 0.00049929 rank 0
2023-02-20 21:15:49,922 DEBUG TRAIN Batch 12/200 loss 17.482351 loss_att 23.282921 loss_ctc 18.872265 loss_rnnt 16.090773 hw_loss 0.086522 lr 0.00049922 rank 6
2023-02-20 21:17:07,537 DEBUG TRAIN Batch 12/300 loss 19.654711 loss_att 25.026129 loss_ctc 22.270370 loss_rnnt 18.152885 hw_loss 0.147725 lr 0.00049901 rank 4
2023-02-20 21:17:07,538 DEBUG TRAIN Batch 12/300 loss 20.217833 loss_att 24.582201 loss_ctc 27.322443 loss_rnnt 18.300552 hw_loss 0.182110 lr 0.00049896 rank 7
2023-02-20 21:17:07,538 DEBUG TRAIN Batch 12/300 loss 20.281309 loss_att 31.205420 loss_ctc 25.713800 loss_rnnt 17.322334 hw_loss 0.093412 lr 0.00049903 rank 1
2023-02-20 21:17:07,539 DEBUG TRAIN Batch 12/300 loss 20.023022 loss_att 23.202019 loss_ctc 18.385647 loss_rnnt 19.549833 hw_loss 0.104453 lr 0.00049899 rank 5
2023-02-20 21:17:07,548 DEBUG TRAIN Batch 12/300 loss 16.389242 loss_att 19.625685 loss_ctc 18.054829 loss_rnnt 15.387604 hw_loss 0.248009 lr 0.00049899 rank 3
2023-02-20 21:17:07,548 DEBUG TRAIN Batch 12/300 loss 26.430691 loss_att 32.344845 loss_ctc 37.338402 loss_rnnt 23.769608 hw_loss 0.044794 lr 0.00049905 rank 0
2023-02-20 21:17:07,556 DEBUG TRAIN Batch 12/300 loss 24.004911 loss_att 29.737709 loss_ctc 32.675678 loss_rnnt 21.616615 hw_loss 0.160562 lr 0.00049895 rank 2
2023-02-20 21:17:07,557 DEBUG TRAIN Batch 12/300 loss 9.117542 loss_att 12.168379 loss_ctc 9.160471 loss_rnnt 8.439925 hw_loss 0.115736 lr 0.00049897 rank 6
2023-02-20 21:18:25,608 DEBUG TRAIN Batch 12/400 loss 19.039818 loss_att 24.534534 loss_ctc 20.446686 loss_rnnt 17.643988 hw_loss 0.204952 lr 0.00049876 rank 4
2023-02-20 21:18:25,616 DEBUG TRAIN Batch 12/400 loss 20.795231 loss_att 21.485947 loss_ctc 25.431561 loss_rnnt 19.900820 hw_loss 0.258917 lr 0.00049880 rank 0
2023-02-20 21:18:25,617 DEBUG TRAIN Batch 12/400 loss 18.418077 loss_att 23.481926 loss_ctc 20.001120 loss_rnnt 17.108110 hw_loss 0.161480 lr 0.00049872 rank 6
2023-02-20 21:18:25,617 DEBUG TRAIN Batch 12/400 loss 16.849298 loss_att 22.290121 loss_ctc 19.918962 loss_rnnt 15.283300 hw_loss 0.128520 lr 0.00049874 rank 3
2023-02-20 21:18:25,618 DEBUG TRAIN Batch 12/400 loss 19.717760 loss_att 22.407238 loss_ctc 20.998203 loss_rnnt 18.951717 hw_loss 0.107667 lr 0.00049874 rank 5
2023-02-20 21:18:25,622 DEBUG TRAIN Batch 12/400 loss 33.076908 loss_att 31.322927 loss_ctc 43.746449 loss_rnnt 31.949923 hw_loss 0.103454 lr 0.00049871 rank 7
2023-02-20 21:18:25,634 DEBUG TRAIN Batch 12/400 loss 23.530779 loss_att 28.834614 loss_ctc 26.175049 loss_rnnt 22.074661 hw_loss 0.080216 lr 0.00049878 rank 1
2023-02-20 21:18:25,644 DEBUG TRAIN Batch 12/400 loss 13.850973 loss_att 15.093412 loss_ctc 14.378304 loss_rnnt 13.456865 hw_loss 0.141206 lr 0.00049870 rank 2
2023-02-20 21:19:41,793 DEBUG TRAIN Batch 12/500 loss 4.230711 loss_att 8.253649 loss_ctc 4.731123 loss_rnnt 3.312253 hw_loss 0.088405 lr 0.00049845 rank 2
2023-02-20 21:19:41,794 DEBUG TRAIN Batch 12/500 loss 19.418806 loss_att 23.375546 loss_ctc 23.692854 loss_rnnt 18.021673 hw_loss 0.067332 lr 0.00049851 rank 4
2023-02-20 21:19:41,796 DEBUG TRAIN Batch 12/500 loss 19.290308 loss_att 18.357880 loss_ctc 22.401814 loss_rnnt 18.969467 hw_loss 0.173362 lr 0.00049855 rank 0
2023-02-20 21:19:41,797 DEBUG TRAIN Batch 12/500 loss 19.309443 loss_att 19.578335 loss_ctc 20.676447 loss_rnnt 18.992170 hw_loss 0.152300 lr 0.00049850 rank 3
2023-02-20 21:19:41,800 DEBUG TRAIN Batch 12/500 loss 17.940521 loss_att 23.497932 loss_ctc 23.393700 loss_rnnt 16.033352 hw_loss 0.128619 lr 0.00049853 rank 1
2023-02-20 21:19:41,802 DEBUG TRAIN Batch 12/500 loss 13.130184 loss_att 18.668114 loss_ctc 14.510395 loss_rnnt 11.735204 hw_loss 0.193810 lr 0.00049848 rank 6
2023-02-20 21:19:41,806 DEBUG TRAIN Batch 12/500 loss 12.267518 loss_att 15.396025 loss_ctc 16.338171 loss_rnnt 11.016278 hw_loss 0.155222 lr 0.00049849 rank 5
2023-02-20 21:19:41,808 DEBUG TRAIN Batch 12/500 loss 21.757870 loss_att 26.944124 loss_ctc 25.754951 loss_rnnt 20.166550 hw_loss 0.039612 lr 0.00049846 rank 7
2023-02-20 21:20:58,617 DEBUG TRAIN Batch 12/600 loss 17.806219 loss_att 15.931780 loss_ctc 20.026188 loss_rnnt 17.734907 hw_loss 0.281636 lr 0.00049826 rank 4
2023-02-20 21:20:58,619 DEBUG TRAIN Batch 12/600 loss 18.219183 loss_att 18.103624 loss_ctc 21.212799 loss_rnnt 17.738331 hw_loss 0.196528 lr 0.00049825 rank 3
2023-02-20 21:20:58,622 DEBUG TRAIN Batch 12/600 loss 16.291855 loss_att 18.111795 loss_ctc 17.987724 loss_rnnt 15.601133 hw_loss 0.188655 lr 0.00049830 rank 0
2023-02-20 21:20:58,624 DEBUG TRAIN Batch 12/600 loss 32.256836 loss_att 30.271019 loss_ctc 39.422348 loss_rnnt 31.589149 hw_loss 0.205214 lr 0.00049828 rank 1
2023-02-20 21:20:58,623 DEBUG TRAIN Batch 12/600 loss 25.497526 loss_att 28.852480 loss_ctc 28.814592 loss_rnnt 24.345638 hw_loss 0.072415 lr 0.00049821 rank 2
2023-02-20 21:20:58,625 DEBUG TRAIN Batch 12/600 loss 11.900990 loss_att 11.752783 loss_ctc 11.303200 loss_rnnt 11.941626 hw_loss 0.128834 lr 0.00049823 rank 6
2023-02-20 21:20:58,628 DEBUG TRAIN Batch 12/600 loss 17.435957 loss_att 20.344364 loss_ctc 20.348110 loss_rnnt 16.335165 hw_loss 0.245297 lr 0.00049821 rank 7
2023-02-20 21:20:58,634 DEBUG TRAIN Batch 12/600 loss 13.329429 loss_att 12.336709 loss_ctc 15.924809 loss_rnnt 12.996325 hw_loss 0.347993 lr 0.00049824 rank 5
2023-02-20 21:22:18,607 DEBUG TRAIN Batch 12/700 loss 29.828623 loss_att 33.249603 loss_ctc 33.507282 loss_rnnt 28.595482 hw_loss 0.109604 lr 0.00049796 rank 2
2023-02-20 21:22:18,608 DEBUG TRAIN Batch 12/700 loss 11.751328 loss_att 22.794983 loss_ctc 8.971008 loss_rnnt 9.877351 hw_loss 0.067418 lr 0.00049797 rank 7
2023-02-20 21:22:18,609 DEBUG TRAIN Batch 12/700 loss 10.063806 loss_att 16.211929 loss_ctc 8.674539 loss_rnnt 8.960148 hw_loss 0.111129 lr 0.00049801 rank 4
2023-02-20 21:22:18,612 DEBUG TRAIN Batch 12/700 loss 11.851799 loss_att 13.941657 loss_ctc 12.049225 loss_rnnt 11.350540 hw_loss 0.106808 lr 0.00049800 rank 3
2023-02-20 21:22:18,614 DEBUG TRAIN Batch 12/700 loss 28.136675 loss_att 36.041718 loss_ctc 32.113590 loss_rnnt 25.980946 hw_loss 0.083371 lr 0.00049803 rank 1
2023-02-20 21:22:18,613 DEBUG TRAIN Batch 12/700 loss 29.444963 loss_att 35.003220 loss_ctc 33.840744 loss_rnnt 27.664665 hw_loss 0.154767 lr 0.00049798 rank 6
2023-02-20 21:22:18,615 DEBUG TRAIN Batch 12/700 loss 26.740696 loss_att 26.175304 loss_ctc 34.043579 loss_rnnt 25.815483 hw_loss 0.121074 lr 0.00049800 rank 5
2023-02-20 21:22:18,655 DEBUG TRAIN Batch 12/700 loss 16.671537 loss_att 20.512966 loss_ctc 17.557693 loss_rnnt 15.707680 hw_loss 0.145159 lr 0.00049805 rank 0
2023-02-20 21:23:36,313 DEBUG TRAIN Batch 12/800 loss 24.703882 loss_att 30.676199 loss_ctc 30.666077 loss_rnnt 22.700888 hw_loss 0.025450 lr 0.00049771 rank 2
2023-02-20 21:23:36,313 DEBUG TRAIN Batch 12/800 loss 16.471563 loss_att 22.709728 loss_ctc 22.598516 loss_rnnt 14.364513 hw_loss 0.079669 lr 0.00049779 rank 1
2023-02-20 21:23:36,313 DEBUG TRAIN Batch 12/800 loss 22.445597 loss_att 28.870659 loss_ctc 29.939064 loss_rnnt 20.079403 hw_loss 0.153845 lr 0.00049776 rank 3
2023-02-20 21:23:36,316 DEBUG TRAIN Batch 12/800 loss 20.132524 loss_att 24.810949 loss_ctc 19.230408 loss_rnnt 19.262848 hw_loss 0.101764 lr 0.00049777 rank 4
2023-02-20 21:23:36,316 DEBUG TRAIN Batch 12/800 loss 20.759743 loss_att 22.601868 loss_ctc 25.357580 loss_rnnt 19.722052 hw_loss 0.105415 lr 0.00049775 rank 5
2023-02-20 21:23:36,318 DEBUG TRAIN Batch 12/800 loss 17.020571 loss_att 21.433506 loss_ctc 29.768942 loss_rnnt 14.303772 hw_loss 0.252049 lr 0.00049774 rank 6
2023-02-20 21:23:36,319 DEBUG TRAIN Batch 12/800 loss 6.766156 loss_att 13.183945 loss_ctc 7.503555 loss_rnnt 5.272758 hw_loss 0.209102 lr 0.00049772 rank 7
2023-02-20 21:23:36,368 DEBUG TRAIN Batch 12/800 loss 14.818881 loss_att 22.616787 loss_ctc 16.502789 loss_rnnt 12.998275 hw_loss 0.068446 lr 0.00049781 rank 0
2023-02-20 21:24:52,485 DEBUG TRAIN Batch 12/900 loss 14.413346 loss_att 18.481045 loss_ctc 14.159874 loss_rnnt 13.587302 hw_loss 0.086815 lr 0.00049754 rank 1
2023-02-20 21:24:52,486 DEBUG TRAIN Batch 12/900 loss 12.524381 loss_att 17.019817 loss_ctc 13.987906 loss_rnnt 11.313653 hw_loss 0.218444 lr 0.00049752 rank 4
2023-02-20 21:24:52,491 DEBUG TRAIN Batch 12/900 loss 19.385263 loss_att 24.705641 loss_ctc 19.134068 loss_rnnt 18.299860 hw_loss 0.102785 lr 0.00049747 rank 2
2023-02-20 21:24:52,491 DEBUG TRAIN Batch 12/900 loss 32.947235 loss_att 40.045212 loss_ctc 45.649197 loss_rnnt 29.778059 hw_loss 0.104967 lr 0.00049749 rank 6
2023-02-20 21:24:52,495 DEBUG TRAIN Batch 12/900 loss 32.845898 loss_att 39.861259 loss_ctc 35.802132 loss_rnnt 30.988388 hw_loss 0.113006 lr 0.00049747 rank 7
2023-02-20 21:24:52,495 DEBUG TRAIN Batch 12/900 loss 14.299450 loss_att 14.860630 loss_ctc 15.550892 loss_rnnt 13.968742 hw_loss 0.096773 lr 0.00049756 rank 0
2023-02-20 21:24:52,495 DEBUG TRAIN Batch 12/900 loss 7.004526 loss_att 13.977856 loss_ctc 10.548231 loss_rnnt 5.052404 hw_loss 0.159303 lr 0.00049751 rank 3
2023-02-20 21:24:52,540 DEBUG TRAIN Batch 12/900 loss 35.371490 loss_att 41.937279 loss_ctc 39.428528 loss_rnnt 33.387302 hw_loss 0.243923 lr 0.00049750 rank 5
2023-02-20 21:26:09,208 DEBUG TRAIN Batch 12/1000 loss 8.555090 loss_att 14.696566 loss_ctc 10.191143 loss_rnnt 7.057522 hw_loss 0.095872 lr 0.00049724 rank 6
2023-02-20 21:26:09,208 DEBUG TRAIN Batch 12/1000 loss 12.831346 loss_att 17.342384 loss_ctc 14.491631 loss_rnnt 11.639046 hw_loss 0.128850 lr 0.00049727 rank 4
2023-02-20 21:26:09,209 DEBUG TRAIN Batch 12/1000 loss 17.501717 loss_att 28.105366 loss_ctc 19.933758 loss_rnnt 15.015728 hw_loss 0.076847 lr 0.00049731 rank 0
2023-02-20 21:26:09,213 DEBUG TRAIN Batch 12/1000 loss 11.947481 loss_att 17.482527 loss_ctc 11.131718 loss_rnnt 10.875951 hw_loss 0.137417 lr 0.00049723 rank 7
2023-02-20 21:26:09,213 DEBUG TRAIN Batch 12/1000 loss 12.510564 loss_att 16.641523 loss_ctc 12.559099 loss_rnnt 11.623609 hw_loss 0.101797 lr 0.00049726 rank 3
2023-02-20 21:26:09,215 DEBUG TRAIN Batch 12/1000 loss 16.440184 loss_att 19.000734 loss_ctc 22.653126 loss_rnnt 15.069283 hw_loss 0.056995 lr 0.00049722 rank 2
2023-02-20 21:26:09,216 DEBUG TRAIN Batch 12/1000 loss 21.198980 loss_att 25.201761 loss_ctc 24.929176 loss_rnnt 19.707722 hw_loss 0.362520 lr 0.00049726 rank 5
2023-02-20 21:26:09,217 DEBUG TRAIN Batch 12/1000 loss 14.556420 loss_att 17.491186 loss_ctc 13.017940 loss_rnnt 14.135840 hw_loss 0.072670 lr 0.00049729 rank 1
2023-02-20 21:27:28,468 DEBUG TRAIN Batch 12/1100 loss 14.639318 loss_att 17.274418 loss_ctc 16.858143 loss_rnnt 13.734261 hw_loss 0.154113 lr 0.00049698 rank 2
2023-02-20 21:27:28,468 DEBUG TRAIN Batch 12/1100 loss 9.075164 loss_att 13.068098 loss_ctc 9.554808 loss_rnnt 8.126822 hw_loss 0.160879 lr 0.00049702 rank 3
2023-02-20 21:27:28,471 DEBUG TRAIN Batch 12/1100 loss 23.456526 loss_att 26.821186 loss_ctc 27.988525 loss_rnnt 22.135967 hw_loss 0.081301 lr 0.00049705 rank 1
2023-02-20 21:27:28,474 DEBUG TRAIN Batch 12/1100 loss 31.846643 loss_att 39.774696 loss_ctc 42.260883 loss_rnnt 28.838562 hw_loss 0.063567 lr 0.00049700 rank 6
2023-02-20 21:27:28,476 DEBUG TRAIN Batch 12/1100 loss 16.386667 loss_att 22.918867 loss_ctc 15.758183 loss_rnnt 15.065672 hw_loss 0.184414 lr 0.00049707 rank 0
2023-02-20 21:27:28,476 DEBUG TRAIN Batch 12/1100 loss 28.291643 loss_att 36.089661 loss_ctc 35.488052 loss_rnnt 25.622940 hw_loss 0.280460 lr 0.00049703 rank 4
2023-02-20 21:27:28,485 DEBUG TRAIN Batch 12/1100 loss 13.518456 loss_att 15.889219 loss_ctc 14.721382 loss_rnnt 12.807633 hw_loss 0.143025 lr 0.00049698 rank 7
2023-02-20 21:27:28,526 DEBUG TRAIN Batch 12/1100 loss 19.211504 loss_att 22.755316 loss_ctc 20.989964 loss_rnnt 18.227695 hw_loss 0.071099 lr 0.00049701 rank 5
2023-02-20 21:28:45,583 DEBUG TRAIN Batch 12/1200 loss 15.856166 loss_att 20.455248 loss_ctc 19.299519 loss_rnnt 14.425030 hw_loss 0.097887 lr 0.00049677 rank 3
2023-02-20 21:28:45,586 DEBUG TRAIN Batch 12/1200 loss 41.921795 loss_att 52.771706 loss_ctc 51.313534 loss_rnnt 38.432281 hw_loss 0.126177 lr 0.00049680 rank 1
2023-02-20 21:28:45,586 DEBUG TRAIN Batch 12/1200 loss 17.210672 loss_att 20.215929 loss_ctc 20.954491 loss_rnnt 16.053995 hw_loss 0.105845 lr 0.00049678 rank 4
2023-02-20 21:28:45,587 DEBUG TRAIN Batch 12/1200 loss 16.516281 loss_att 18.538395 loss_ctc 18.066221 loss_rnnt 15.808838 hw_loss 0.180680 lr 0.00049677 rank 5
2023-02-20 21:28:45,588 DEBUG TRAIN Batch 12/1200 loss 12.472037 loss_att 15.191124 loss_ctc 13.001952 loss_rnnt 11.810770 hw_loss 0.087739 lr 0.00049673 rank 2
2023-02-20 21:28:45,589 DEBUG TRAIN Batch 12/1200 loss 23.683640 loss_att 29.986176 loss_ctc 29.091660 loss_rnnt 21.614498 hw_loss 0.164186 lr 0.00049675 rank 6
2023-02-20 21:28:45,589 DEBUG TRAIN Batch 12/1200 loss 21.368477 loss_att 25.487486 loss_ctc 25.174877 loss_rnnt 19.962368 hw_loss 0.140229 lr 0.00049682 rank 0
2023-02-20 21:28:45,642 DEBUG TRAIN Batch 12/1200 loss 13.553592 loss_att 18.847811 loss_ctc 17.017479 loss_rnnt 11.957243 hw_loss 0.141853 lr 0.00049674 rank 7
2023-02-20 21:30:01,814 DEBUG TRAIN Batch 12/1300 loss 8.925640 loss_att 14.120644 loss_ctc 11.238883 loss_rnnt 7.530426 hw_loss 0.089588 lr 0.00049654 rank 4
2023-02-20 21:30:01,815 DEBUG TRAIN Batch 12/1300 loss 9.366992 loss_att 14.410721 loss_ctc 10.429414 loss_rnnt 8.147068 hw_loss 0.130353 lr 0.00049653 rank 3
2023-02-20 21:30:01,818 DEBUG TRAIN Batch 12/1300 loss 9.828760 loss_att 11.834274 loss_ctc 11.483375 loss_rnnt 9.105376 hw_loss 0.190624 lr 0.00049656 rank 1
2023-02-20 21:30:01,821 DEBUG TRAIN Batch 12/1300 loss 19.689091 loss_att 24.415390 loss_ctc 22.351843 loss_rnnt 18.357023 hw_loss 0.059578 lr 0.00049652 rank 5
2023-02-20 21:30:01,822 DEBUG TRAIN Batch 12/1300 loss 16.910295 loss_att 20.921665 loss_ctc 22.105350 loss_rnnt 15.380363 hw_loss 0.065592 lr 0.00049648 rank 2
2023-02-20 21:30:01,825 DEBUG TRAIN Batch 12/1300 loss 12.089554 loss_att 11.180099 loss_ctc 13.403109 loss_rnnt 11.922044 hw_loss 0.326740 lr 0.00049649 rank 7
2023-02-20 21:30:01,827 DEBUG TRAIN Batch 12/1300 loss 19.428225 loss_att 25.195997 loss_ctc 22.143410 loss_rnnt 17.878304 hw_loss 0.064393 lr 0.00049658 rank 0
2023-02-20 21:30:01,870 DEBUG TRAIN Batch 12/1300 loss 12.148606 loss_att 12.758112 loss_ctc 15.158216 loss_rnnt 11.557719 hw_loss 0.126946 lr 0.00049651 rank 6
2023-02-20 21:31:20,639 DEBUG TRAIN Batch 12/1400 loss 17.358599 loss_att 19.422680 loss_ctc 18.667294 loss_rnnt 16.708632 hw_loss 0.117483 lr 0.00049624 rank 2
2023-02-20 21:31:20,643 DEBUG TRAIN Batch 12/1400 loss 27.220879 loss_att 30.298323 loss_ctc 29.072891 loss_rnnt 26.277760 hw_loss 0.151299 lr 0.00049631 rank 1
2023-02-20 21:31:20,646 DEBUG TRAIN Batch 12/1400 loss 11.343665 loss_att 15.564734 loss_ctc 10.329689 loss_rnnt 10.621370 hw_loss 0.024896 lr 0.00049625 rank 7
2023-02-20 21:31:20,646 DEBUG TRAIN Batch 12/1400 loss 31.777361 loss_att 35.379162 loss_ctc 37.520760 loss_rnnt 30.277622 hw_loss 0.025488 lr 0.00049628 rank 3
2023-02-20 21:31:20,646 DEBUG TRAIN Batch 12/1400 loss 18.607668 loss_att 27.356695 loss_ctc 26.808126 loss_rnnt 15.740126 hw_loss 0.045643 lr 0.00049629 rank 4
2023-02-20 21:31:20,646 DEBUG TRAIN Batch 12/1400 loss 28.095747 loss_att 28.449619 loss_ctc 30.467003 loss_rnnt 27.647942 hw_loss 0.114120 lr 0.00049633 rank 0
2023-02-20 21:31:20,652 DEBUG TRAIN Batch 12/1400 loss 35.213760 loss_att 38.548645 loss_ctc 38.048042 loss_rnnt 34.119225 hw_loss 0.093099 lr 0.00049626 rank 6
2023-02-20 21:31:20,656 DEBUG TRAIN Batch 12/1400 loss 11.008079 loss_att 20.347271 loss_ctc 12.131063 loss_rnnt 8.876973 hw_loss 0.212879 lr 0.00049628 rank 5
2023-02-20 21:32:37,830 DEBUG TRAIN Batch 12/1500 loss 9.013052 loss_att 16.801594 loss_ctc 11.562369 loss_rnnt 7.077293 hw_loss 0.071514 lr 0.00049604 rank 3
2023-02-20 21:32:37,831 DEBUG TRAIN Batch 12/1500 loss 13.292401 loss_att 20.360819 loss_ctc 11.411371 loss_rnnt 12.053798 hw_loss 0.141983 lr 0.00049607 rank 1
2023-02-20 21:32:37,832 DEBUG TRAIN Batch 12/1500 loss 15.769796 loss_att 22.965775 loss_ctc 20.726894 loss_rnnt 13.623538 hw_loss 0.086469 lr 0.00049605 rank 4
2023-02-20 21:32:37,833 DEBUG TRAIN Batch 12/1500 loss 8.422823 loss_att 12.453884 loss_ctc 10.504173 loss_rnnt 7.210753 hw_loss 0.240646 lr 0.00049602 rank 6
2023-02-20 21:32:37,835 DEBUG TRAIN Batch 12/1500 loss 8.646399 loss_att 12.156221 loss_ctc 9.836685 loss_rnnt 7.772367 hw_loss 0.025054 lr 0.00049603 rank 5
2023-02-20 21:32:37,836 DEBUG TRAIN Batch 12/1500 loss 29.049246 loss_att 32.985622 loss_ctc 36.145790 loss_rnnt 27.275190 hw_loss 0.076080 lr 0.00049600 rank 2
2023-02-20 21:32:37,839 DEBUG TRAIN Batch 12/1500 loss 18.922596 loss_att 21.601933 loss_ctc 24.367971 loss_rnnt 17.598928 hw_loss 0.115783 lr 0.00049600 rank 7
2023-02-20 21:32:37,842 DEBUG TRAIN Batch 12/1500 loss 20.214479 loss_att 26.849884 loss_ctc 27.236557 loss_rnnt 17.822908 hw_loss 0.240397 lr 0.00049609 rank 0
2023-02-20 21:33:54,052 DEBUG TRAIN Batch 12/1600 loss 16.720537 loss_att 25.542128 loss_ctc 23.160444 loss_rnnt 14.018013 hw_loss 0.149161 lr 0.00049581 rank 4
2023-02-20 21:33:54,055 DEBUG TRAIN Batch 12/1600 loss 21.527750 loss_att 26.945015 loss_ctc 30.574791 loss_rnnt 19.123283 hw_loss 0.215140 lr 0.00049583 rank 1
2023-02-20 21:33:54,055 DEBUG TRAIN Batch 12/1600 loss 23.250177 loss_att 26.955528 loss_ctc 30.340139 loss_rnnt 21.494362 hw_loss 0.130159 lr 0.00049579 rank 3
2023-02-20 21:33:54,056 DEBUG TRAIN Batch 12/1600 loss 13.712137 loss_att 22.511215 loss_ctc 18.598759 loss_rnnt 11.171361 hw_loss 0.242646 lr 0.00049575 rank 2
2023-02-20 21:33:54,059 DEBUG TRAIN Batch 12/1600 loss 18.802582 loss_att 26.024488 loss_ctc 22.406118 loss_rnnt 16.823856 hw_loss 0.101008 lr 0.00049577 rank 6
2023-02-20 21:33:54,059 DEBUG TRAIN Batch 12/1600 loss 14.350327 loss_att 17.959890 loss_ctc 15.285921 loss_rnnt 13.456556 hw_loss 0.088334 lr 0.00049576 rank 7
2023-02-20 21:33:54,060 DEBUG TRAIN Batch 12/1600 loss 19.265186 loss_att 24.502571 loss_ctc 19.420351 loss_rnnt 18.161039 hw_loss 0.067468 lr 0.00049584 rank 0
2023-02-20 21:33:54,103 DEBUG TRAIN Batch 12/1600 loss 7.866929 loss_att 12.438675 loss_ctc 8.896959 loss_rnnt 6.776759 hw_loss 0.072156 lr 0.00049579 rank 5
2023-02-20 21:35:12,347 DEBUG TRAIN Batch 12/1700 loss 13.985785 loss_att 16.384470 loss_ctc 18.978998 loss_rnnt 12.805151 hw_loss 0.065882 lr 0.00049551 rank 2
2023-02-20 21:35:12,352 DEBUG TRAIN Batch 12/1700 loss 18.611000 loss_att 22.181458 loss_ctc 23.043356 loss_rnnt 17.244259 hw_loss 0.115630 lr 0.00049558 rank 1
2023-02-20 21:35:12,353 DEBUG TRAIN Batch 12/1700 loss 8.063344 loss_att 13.759226 loss_ctc 8.324522 loss_rnnt 6.825142 hw_loss 0.120377 lr 0.00049552 rank 7
2023-02-20 21:35:12,354 DEBUG TRAIN Batch 12/1700 loss 13.401550 loss_att 16.718893 loss_ctc 17.368591 loss_rnnt 12.192015 hw_loss 0.032114 lr 0.00049556 rank 4
2023-02-20 21:35:12,354 DEBUG TRAIN Batch 12/1700 loss 8.327103 loss_att 11.424276 loss_ctc 6.971686 loss_rnnt 7.834880 hw_loss 0.100331 lr 0.00049555 rank 5
2023-02-20 21:35:12,359 DEBUG TRAIN Batch 12/1700 loss 9.664948 loss_att 14.700056 loss_ctc 11.003801 loss_rnnt 8.322227 hw_loss 0.294720 lr 0.00049560 rank 0
2023-02-20 21:35:12,359 DEBUG TRAIN Batch 12/1700 loss 13.161279 loss_att 14.569357 loss_ctc 16.209248 loss_rnnt 12.440426 hw_loss 0.061576 lr 0.00049555 rank 3
2023-02-20 21:35:12,359 DEBUG TRAIN Batch 12/1700 loss 30.190660 loss_att 31.458408 loss_ctc 33.164101 loss_rnnt 29.393242 hw_loss 0.276393 lr 0.00049553 rank 6
2023-02-20 21:36:31,689 DEBUG TRAIN Batch 12/1800 loss 9.245789 loss_att 10.425190 loss_ctc 7.275757 loss_rnnt 9.181473 hw_loss 0.170825 lr 0.00049527 rank 2
2023-02-20 21:36:31,689 DEBUG TRAIN Batch 12/1800 loss 12.928346 loss_att 15.818174 loss_ctc 12.996276 loss_rnnt 12.250483 hw_loss 0.170323 lr 0.00049532 rank 4
2023-02-20 21:36:31,689 DEBUG TRAIN Batch 12/1800 loss 9.756763 loss_att 12.569988 loss_ctc 10.996653 loss_rnnt 8.937848 hw_loss 0.170533 lr 0.00049531 rank 3
2023-02-20 21:36:31,690 DEBUG TRAIN Batch 12/1800 loss 13.306910 loss_att 14.354994 loss_ctc 11.286731 loss_rnnt 13.250015 hw_loss 0.218688 lr 0.00049529 rank 6
2023-02-20 21:36:31,691 DEBUG TRAIN Batch 12/1800 loss 12.162868 loss_att 18.435408 loss_ctc 17.898548 loss_rnnt 10.071305 hw_loss 0.135555 lr 0.00049534 rank 1
2023-02-20 21:36:31,692 DEBUG TRAIN Batch 12/1800 loss 23.685017 loss_att 26.939804 loss_ctc 28.901957 loss_rnnt 22.285789 hw_loss 0.098770 lr 0.00049536 rank 0
2023-02-20 21:36:31,697 DEBUG TRAIN Batch 12/1800 loss 11.795861 loss_att 14.633411 loss_ctc 13.415099 loss_rnnt 10.941019 hw_loss 0.133937 lr 0.00049530 rank 5
2023-02-20 21:36:31,742 DEBUG TRAIN Batch 12/1800 loss 15.781527 loss_att 15.876467 loss_ctc 15.192446 loss_rnnt 15.720819 hw_loss 0.225493 lr 0.00049527 rank 7
2023-02-20 21:37:48,565 DEBUG TRAIN Batch 12/1900 loss 12.298080 loss_att 11.829820 loss_ctc 15.421564 loss_rnnt 11.831301 hw_loss 0.269938 lr 0.00049502 rank 2
2023-02-20 21:37:48,570 DEBUG TRAIN Batch 12/1900 loss 12.853024 loss_att 16.604706 loss_ctc 11.877645 loss_rnnt 12.150749 hw_loss 0.153727 lr 0.00049503 rank 7
2023-02-20 21:37:48,573 DEBUG TRAIN Batch 12/1900 loss 6.775529 loss_att 12.843300 loss_ctc 8.431353 loss_rnnt 5.323344 hw_loss 0.033476 lr 0.00049506 rank 3
2023-02-20 21:37:48,574 DEBUG TRAIN Batch 12/1900 loss 14.181922 loss_att 16.052448 loss_ctc 16.972919 loss_rnnt 13.404781 hw_loss 0.057940 lr 0.00049510 rank 1
2023-02-20 21:37:48,575 DEBUG TRAIN Batch 12/1900 loss 20.976017 loss_att 22.423981 loss_ctc 20.475010 loss_rnnt 20.643164 hw_loss 0.206365 lr 0.00049504 rank 6
2023-02-20 21:37:48,576 DEBUG TRAIN Batch 12/1900 loss 12.560240 loss_att 13.694027 loss_ctc 17.670431 loss_rnnt 11.545753 hw_loss 0.199448 lr 0.00049512 rank 0
2023-02-20 21:37:48,578 DEBUG TRAIN Batch 12/1900 loss 19.141977 loss_att 19.095953 loss_ctc 24.751617 loss_rnnt 18.334433 hw_loss 0.128995 lr 0.00049508 rank 4
2023-02-20 21:37:48,578 DEBUG TRAIN Batch 12/1900 loss 20.571754 loss_att 27.007156 loss_ctc 27.784300 loss_rnnt 18.236202 hw_loss 0.162750 lr 0.00049506 rank 5
2023-02-20 21:39:05,003 DEBUG TRAIN Batch 12/2000 loss 18.084097 loss_att 23.662600 loss_ctc 15.183073 loss_rnnt 17.321316 hw_loss 0.063528 lr 0.00049487 rank 0
2023-02-20 21:39:05,004 DEBUG TRAIN Batch 12/2000 loss 20.085203 loss_att 28.945002 loss_ctc 22.598610 loss_rnnt 17.934200 hw_loss 0.082352 lr 0.00049483 rank 4
2023-02-20 21:39:05,006 DEBUG TRAIN Batch 12/2000 loss 16.365582 loss_att 19.168360 loss_ctc 16.373306 loss_rnnt 15.733440 hw_loss 0.132287 lr 0.00049482 rank 3
2023-02-20 21:39:05,006 DEBUG TRAIN Batch 12/2000 loss 19.011122 loss_att 26.517921 loss_ctc 26.898350 loss_rnnt 16.407335 hw_loss 0.095243 lr 0.00049480 rank 6
2023-02-20 21:39:05,007 DEBUG TRAIN Batch 12/2000 loss 11.212971 loss_att 11.956960 loss_ctc 13.079748 loss_rnnt 10.776894 hw_loss 0.071955 lr 0.00049478 rank 2
2023-02-20 21:39:05,007 DEBUG TRAIN Batch 12/2000 loss 9.756550 loss_att 19.980253 loss_ctc 8.960962 loss_rnnt 7.748253 hw_loss 0.130566 lr 0.00049485 rank 1
2023-02-20 21:39:05,010 DEBUG TRAIN Batch 12/2000 loss 14.925410 loss_att 18.762556 loss_ctc 20.053215 loss_rnnt 13.460559 hw_loss 0.025715 lr 0.00049479 rank 7
2023-02-20 21:39:05,011 DEBUG TRAIN Batch 12/2000 loss 25.849920 loss_att 28.870129 loss_ctc 27.769751 loss_rnnt 24.962921 hw_loss 0.050589 lr 0.00049482 rank 5
2023-02-20 21:40:23,700 DEBUG TRAIN Batch 12/2100 loss 7.285411 loss_att 12.575539 loss_ctc 8.320314 loss_rnnt 6.000577 hw_loss 0.166540 lr 0.00049459 rank 4
2023-02-20 21:40:23,702 DEBUG TRAIN Batch 12/2100 loss 9.315759 loss_att 12.125336 loss_ctc 12.439917 loss_rnnt 8.277781 hw_loss 0.111578 lr 0.00049461 rank 1
2023-02-20 21:40:23,703 DEBUG TRAIN Batch 12/2100 loss 14.446198 loss_att 17.589930 loss_ctc 20.075001 loss_rnnt 13.019786 hw_loss 0.088425 lr 0.00049458 rank 3
2023-02-20 21:40:23,704 DEBUG TRAIN Batch 12/2100 loss 10.810195 loss_att 14.316973 loss_ctc 14.666250 loss_rnnt 9.486472 hw_loss 0.202925 lr 0.00049454 rank 2
2023-02-20 21:40:23,705 DEBUG TRAIN Batch 12/2100 loss 39.308537 loss_att 39.400543 loss_ctc 37.938614 loss_rnnt 39.457096 hw_loss 0.029417 lr 0.00049463 rank 0
2023-02-20 21:40:23,704 DEBUG TRAIN Batch 12/2100 loss 9.398642 loss_att 12.898187 loss_ctc 10.011726 loss_rnnt 8.588900 hw_loss 0.052666 lr 0.00049455 rank 7
2023-02-20 21:40:23,705 DEBUG TRAIN Batch 12/2100 loss 3.864914 loss_att 8.601674 loss_ctc 4.490812 loss_rnnt 2.818093 hw_loss 0.030029 lr 0.00049456 rank 6
2023-02-20 21:40:23,713 DEBUG TRAIN Batch 12/2100 loss 20.177662 loss_att 23.524754 loss_ctc 27.534332 loss_rnnt 18.411520 hw_loss 0.217188 lr 0.00049457 rank 5
2023-02-20 21:41:41,218 DEBUG TRAIN Batch 12/2200 loss 17.310150 loss_att 25.468565 loss_ctc 20.295212 loss_rnnt 15.230249 hw_loss 0.094141 lr 0.00049434 rank 3
2023-02-20 21:41:41,218 DEBUG TRAIN Batch 12/2200 loss 14.371127 loss_att 20.267824 loss_ctc 18.133730 loss_rnnt 12.649325 hw_loss 0.076468 lr 0.00049430 rank 2
2023-02-20 21:41:41,222 DEBUG TRAIN Batch 12/2200 loss 11.937590 loss_att 15.656893 loss_ctc 13.496106 loss_rnnt 10.957529 hw_loss 0.053243 lr 0.00049437 rank 1
2023-02-20 21:41:41,223 DEBUG TRAIN Batch 12/2200 loss 14.191545 loss_att 19.481926 loss_ctc 16.386631 loss_rnnt 12.765265 hw_loss 0.141610 lr 0.00049435 rank 4
2023-02-20 21:41:41,223 DEBUG TRAIN Batch 12/2200 loss 16.351944 loss_att 21.086441 loss_ctc 24.382414 loss_rnnt 14.273451 hw_loss 0.114121 lr 0.00049439 rank 0
2023-02-20 21:41:41,223 DEBUG TRAIN Batch 12/2200 loss 15.904316 loss_att 18.026144 loss_ctc 17.872713 loss_rnnt 15.139490 hw_loss 0.146261 lr 0.00049432 rank 6
2023-02-20 21:41:41,226 DEBUG TRAIN Batch 12/2200 loss 13.802904 loss_att 20.465956 loss_ctc 15.981613 loss_rnnt 12.141296 hw_loss 0.072190 lr 0.00049433 rank 5
2023-02-20 21:41:41,230 DEBUG TRAIN Batch 12/2200 loss 12.185086 loss_att 13.589476 loss_ctc 19.187988 loss_rnnt 10.911761 hw_loss 0.110114 lr 0.00049430 rank 7
2023-02-20 21:42:57,386 DEBUG TRAIN Batch 12/2300 loss 20.676720 loss_att 25.446854 loss_ctc 24.855968 loss_rnnt 19.091215 hw_loss 0.139209 lr 0.00049406 rank 2
2023-02-20 21:42:57,386 DEBUG TRAIN Batch 12/2300 loss 18.247101 loss_att 18.205778 loss_ctc 17.645775 loss_rnnt 18.311447 hw_loss 0.045175 lr 0.00049413 rank 1
2023-02-20 21:42:57,390 DEBUG TRAIN Batch 12/2300 loss 15.302326 loss_att 20.053442 loss_ctc 23.133875 loss_rnnt 13.274476 hw_loss 0.062661 lr 0.00049410 rank 3
2023-02-20 21:42:57,390 DEBUG TRAIN Batch 12/2300 loss 14.934841 loss_att 17.848879 loss_ctc 13.615399 loss_rnnt 14.437006 hw_loss 0.170536 lr 0.00049411 rank 4
2023-02-20 21:42:57,392 DEBUG TRAIN Batch 12/2300 loss 15.337999 loss_att 21.063553 loss_ctc 16.238382 loss_rnnt 14.042761 hw_loss 0.056395 lr 0.00049408 rank 6
2023-02-20 21:42:57,393 DEBUG TRAIN Batch 12/2300 loss 19.460339 loss_att 24.906157 loss_ctc 26.218548 loss_rnnt 17.341793 hw_loss 0.240539 lr 0.00049415 rank 0
2023-02-20 21:42:57,394 DEBUG TRAIN Batch 12/2300 loss 18.482430 loss_att 25.492630 loss_ctc 22.558081 loss_rnnt 16.507648 hw_loss 0.054973 lr 0.00049406 rank 7
2023-02-20 21:42:57,396 DEBUG TRAIN Batch 12/2300 loss 28.877316 loss_att 33.144035 loss_ctc 31.331621 loss_rnnt 27.661762 hw_loss 0.065567 lr 0.00049409 rank 5
2023-02-20 21:44:13,581 DEBUG TRAIN Batch 12/2400 loss 7.666241 loss_att 10.527267 loss_ctc 8.207751 loss_rnnt 6.993220 hw_loss 0.053652 lr 0.00049387 rank 4
2023-02-20 21:44:13,583 DEBUG TRAIN Batch 12/2400 loss 12.992775 loss_att 18.211344 loss_ctc 11.947894 loss_rnnt 12.022481 hw_loss 0.123555 lr 0.00049386 rank 3
2023-02-20 21:44:13,585 DEBUG TRAIN Batch 12/2400 loss 12.987556 loss_att 17.097044 loss_ctc 15.392870 loss_rnnt 11.825725 hw_loss 0.036048 lr 0.00049382 rank 7
2023-02-20 21:44:13,586 DEBUG TRAIN Batch 12/2400 loss 8.908340 loss_att 14.396817 loss_ctc 9.355133 loss_rnnt 7.671363 hw_loss 0.149456 lr 0.00049384 rank 6
2023-02-20 21:44:13,586 DEBUG TRAIN Batch 12/2400 loss 16.337875 loss_att 15.809738 loss_ctc 17.999487 loss_rnnt 16.185272 hw_loss 0.068777 lr 0.00049389 rank 1
2023-02-20 21:44:13,588 DEBUG TRAIN Batch 12/2400 loss 14.538536 loss_att 18.507030 loss_ctc 16.955591 loss_rnnt 13.329978 hw_loss 0.173597 lr 0.00049385 rank 5
2023-02-20 21:44:13,589 DEBUG TRAIN Batch 12/2400 loss 21.925743 loss_att 29.221870 loss_ctc 25.989902 loss_rnnt 19.846619 hw_loss 0.146272 lr 0.00049381 rank 2
2023-02-20 21:44:13,635 DEBUG TRAIN Batch 12/2400 loss 29.954618 loss_att 34.483593 loss_ctc 39.102604 loss_rnnt 27.703871 hw_loss 0.234790 lr 0.00049391 rank 0
2023-02-20 21:45:34,105 DEBUG TRAIN Batch 12/2500 loss 11.631452 loss_att 13.414677 loss_ctc 15.672016 loss_rnnt 10.655628 hw_loss 0.150819 lr 0.00049365 rank 1
2023-02-20 21:45:34,107 DEBUG TRAIN Batch 12/2500 loss 18.220150 loss_att 20.586172 loss_ctc 20.603313 loss_rnnt 17.345974 hw_loss 0.156033 lr 0.00049357 rank 2
2023-02-20 21:45:34,109 DEBUG TRAIN Batch 12/2500 loss 21.069229 loss_att 27.456722 loss_ctc 28.445965 loss_rnnt 18.669060 hw_loss 0.260822 lr 0.00049360 rank 6
2023-02-20 21:45:34,110 DEBUG TRAIN Batch 12/2500 loss 9.612669 loss_att 19.781672 loss_ctc 9.026251 loss_rnnt 7.638493 hw_loss 0.034809 lr 0.00049361 rank 3
2023-02-20 21:45:34,110 DEBUG TRAIN Batch 12/2500 loss 9.178441 loss_att 11.898912 loss_ctc 12.501284 loss_rnnt 8.048636 hw_loss 0.267496 lr 0.00049363 rank 4
2023-02-20 21:45:34,115 DEBUG TRAIN Batch 12/2500 loss 20.617111 loss_att 24.689259 loss_ctc 27.184326 loss_rnnt 18.882380 hw_loss 0.083761 lr 0.00049358 rank 7
2023-02-20 21:45:34,135 DEBUG TRAIN Batch 12/2500 loss 9.813018 loss_att 10.790694 loss_ctc 10.267042 loss_rnnt 9.443137 hw_loss 0.213393 lr 0.00049366 rank 0
2023-02-20 21:45:34,143 DEBUG TRAIN Batch 12/2500 loss 12.502753 loss_att 12.594615 loss_ctc 13.839093 loss_rnnt 12.241939 hw_loss 0.120495 lr 0.00049361 rank 5
2023-02-20 21:46:50,368 DEBUG TRAIN Batch 12/2600 loss 17.853863 loss_att 25.859257 loss_ctc 15.809494 loss_rnnt 16.502512 hw_loss 0.042850 lr 0.00049339 rank 4
2023-02-20 21:46:50,370 DEBUG TRAIN Batch 12/2600 loss 12.379387 loss_att 14.975411 loss_ctc 14.329176 loss_rnnt 11.551841 hw_loss 0.090692 lr 0.00049337 rank 3
2023-02-20 21:46:50,370 DEBUG TRAIN Batch 12/2600 loss 12.656768 loss_att 13.110807 loss_ctc 12.149067 loss_rnnt 12.432970 hw_loss 0.376278 lr 0.00049335 rank 6
2023-02-20 21:46:50,371 DEBUG TRAIN Batch 12/2600 loss 19.044163 loss_att 24.128519 loss_ctc 23.562971 loss_rnnt 17.366398 hw_loss 0.109472 lr 0.00049341 rank 1
2023-02-20 21:46:50,372 DEBUG TRAIN Batch 12/2600 loss 38.339817 loss_att 44.058025 loss_ctc 43.867645 loss_rnnt 36.379299 hw_loss 0.149686 lr 0.00049342 rank 0
2023-02-20 21:46:50,373 DEBUG TRAIN Batch 12/2600 loss 9.655210 loss_att 14.273752 loss_ctc 8.487236 loss_rnnt 8.823628 hw_loss 0.119257 lr 0.00049333 rank 2
2023-02-20 21:46:50,376 DEBUG TRAIN Batch 12/2600 loss 18.465706 loss_att 26.125257 loss_ctc 28.911434 loss_rnnt 15.449356 hw_loss 0.171892 lr 0.00049337 rank 5
2023-02-20 21:46:50,421 DEBUG TRAIN Batch 12/2600 loss 20.368294 loss_att 25.092829 loss_ctc 27.106224 loss_rnnt 18.501919 hw_loss 0.043275 lr 0.00049334 rank 7
2023-02-20 21:48:07,074 DEBUG TRAIN Batch 12/2700 loss 9.927402 loss_att 16.128607 loss_ctc 7.740026 loss_rnnt 8.942827 hw_loss 0.067469 lr 0.00049315 rank 4
2023-02-20 21:48:07,074 DEBUG TRAIN Batch 12/2700 loss 27.298222 loss_att 32.126160 loss_ctc 33.562576 loss_rnnt 25.354248 hw_loss 0.268384 lr 0.00049309 rank 2
2023-02-20 21:48:07,074 DEBUG TRAIN Batch 12/2700 loss 26.354721 loss_att 30.445679 loss_ctc 31.370611 loss_rnnt 24.823914 hw_loss 0.082176 lr 0.00049317 rank 1
2023-02-20 21:48:07,075 DEBUG TRAIN Batch 12/2700 loss 25.329912 loss_att 28.315849 loss_ctc 28.764267 loss_rnnt 24.197582 hw_loss 0.144806 lr 0.00049313 rank 5
2023-02-20 21:48:07,076 DEBUG TRAIN Batch 12/2700 loss 11.549400 loss_att 14.911972 loss_ctc 14.389153 loss_rnnt 10.467779 hw_loss 0.057140 lr 0.00049313 rank 3
2023-02-20 21:48:07,076 DEBUG TRAIN Batch 12/2700 loss 28.478844 loss_att 34.346230 loss_ctc 33.118431 loss_rnnt 26.654026 hw_loss 0.061358 lr 0.00049311 rank 6
2023-02-20 21:48:07,080 DEBUG TRAIN Batch 12/2700 loss 17.944071 loss_att 18.442795 loss_ctc 20.854126 loss_rnnt 17.356504 hw_loss 0.187154 lr 0.00049310 rank 7
2023-02-20 21:48:07,087 DEBUG TRAIN Batch 12/2700 loss 15.950519 loss_att 20.175575 loss_ctc 17.801889 loss_rnnt 14.776607 hw_loss 0.153847 lr 0.00049318 rank 0
2023-02-20 21:49:26,520 DEBUG TRAIN Batch 12/2800 loss 26.037060 loss_att 28.505264 loss_ctc 38.922874 loss_rnnt 23.750214 hw_loss 0.140804 lr 0.00049285 rank 2
2023-02-20 21:49:26,519 DEBUG TRAIN Batch 12/2800 loss 11.761343 loss_att 15.958330 loss_ctc 12.946537 loss_rnnt 10.730886 hw_loss 0.061937 lr 0.00049294 rank 0
2023-02-20 21:49:26,522 DEBUG TRAIN Batch 12/2800 loss 18.875273 loss_att 22.460983 loss_ctc 23.519268 loss_rnnt 17.412266 hw_loss 0.237496 lr 0.00049286 rank 7
2023-02-20 21:49:26,524 DEBUG TRAIN Batch 12/2800 loss 17.570049 loss_att 21.202223 loss_ctc 21.628185 loss_rnnt 16.234514 hw_loss 0.127523 lr 0.00049291 rank 4
2023-02-20 21:49:26,525 DEBUG TRAIN Batch 12/2800 loss 9.008329 loss_att 11.622876 loss_ctc 8.062823 loss_rnnt 8.518668 hw_loss 0.174037 lr 0.00049289 rank 3
2023-02-20 21:49:26,527 DEBUG TRAIN Batch 12/2800 loss 9.648244 loss_att 16.393356 loss_ctc 14.401515 loss_rnnt 7.654010 hw_loss 0.021456 lr 0.00049293 rank 1
2023-02-20 21:49:26,531 DEBUG TRAIN Batch 12/2800 loss 15.502040 loss_att 19.943668 loss_ctc 19.626041 loss_rnnt 13.990669 hw_loss 0.137207 lr 0.00049289 rank 5
2023-02-20 21:49:26,567 DEBUG TRAIN Batch 12/2800 loss 21.367500 loss_att 23.565802 loss_ctc 23.069609 loss_rnnt 20.676912 hw_loss 0.044960 lr 0.00049288 rank 6
2023-02-20 21:50:43,731 DEBUG TRAIN Batch 12/2900 loss 20.766670 loss_att 21.813711 loss_ctc 25.671211 loss_rnnt 19.829689 hw_loss 0.138064 lr 0.00049267 rank 4
2023-02-20 21:50:43,734 DEBUG TRAIN Batch 12/2900 loss 12.949760 loss_att 16.816399 loss_ctc 15.630691 loss_rnnt 11.726759 hw_loss 0.172906 lr 0.00049261 rank 2
2023-02-20 21:50:43,735 DEBUG TRAIN Batch 12/2900 loss 16.132063 loss_att 20.708471 loss_ctc 19.567492 loss_rnnt 14.681519 hw_loss 0.144762 lr 0.00049264 rank 6
2023-02-20 21:50:43,735 DEBUG TRAIN Batch 12/2900 loss 13.749040 loss_att 24.176411 loss_ctc 18.666374 loss_rnnt 10.978437 hw_loss 0.055282 lr 0.00049266 rank 3
2023-02-20 21:50:43,741 DEBUG TRAIN Batch 12/2900 loss 17.965357 loss_att 23.831169 loss_ctc 23.925320 loss_rnnt 15.884951 hw_loss 0.211090 lr 0.00049271 rank 0
2023-02-20 21:50:43,741 DEBUG TRAIN Batch 12/2900 loss 17.294924 loss_att 20.576437 loss_ctc 20.372772 loss_rnnt 16.190296 hw_loss 0.071147 lr 0.00049265 rank 5
2023-02-20 21:50:43,743 DEBUG TRAIN Batch 12/2900 loss 8.522739 loss_att 12.999277 loss_ctc 9.630678 loss_rnnt 7.448878 hw_loss 0.057801 lr 0.00049269 rank 1
2023-02-20 21:50:43,749 DEBUG TRAIN Batch 12/2900 loss 21.929428 loss_att 31.398115 loss_ctc 27.734144 loss_rnnt 19.181725 hw_loss 0.150010 lr 0.00049262 rank 7
2023-02-20 21:52:00,588 DEBUG TRAIN Batch 12/3000 loss 13.731597 loss_att 15.920361 loss_ctc 18.037096 loss_rnnt 12.655559 hw_loss 0.120412 lr 0.00049242 rank 3
2023-02-20 21:52:00,592 DEBUG TRAIN Batch 12/3000 loss 14.553576 loss_att 19.171051 loss_ctc 18.592442 loss_rnnt 13.065063 hw_loss 0.049692 lr 0.00049238 rank 2
2023-02-20 21:52:00,592 DEBUG TRAIN Batch 12/3000 loss 16.370096 loss_att 21.012764 loss_ctc 19.937614 loss_rnnt 14.883638 hw_loss 0.154228 lr 0.00049247 rank 0
2023-02-20 21:52:00,593 DEBUG TRAIN Batch 12/3000 loss 19.056170 loss_att 21.908754 loss_ctc 23.082235 loss_rnnt 17.903984 hw_loss 0.084107 lr 0.00049245 rank 1
2023-02-20 21:52:00,593 DEBUG TRAIN Batch 12/3000 loss 23.293650 loss_att 21.930964 loss_ctc 28.579744 loss_rnnt 22.796343 hw_loss 0.121931 lr 0.00049243 rank 4
2023-02-20 21:52:00,594 DEBUG TRAIN Batch 12/3000 loss 28.919140 loss_att 27.481052 loss_ctc 29.910137 loss_rnnt 29.047676 hw_loss 0.050526 lr 0.00049241 rank 5
2023-02-20 21:52:00,594 DEBUG TRAIN Batch 12/3000 loss 17.302469 loss_att 22.263935 loss_ctc 20.592564 loss_rnnt 15.785777 hw_loss 0.160723 lr 0.00049240 rank 6
2023-02-20 21:52:00,642 DEBUG TRAIN Batch 12/3000 loss 25.282782 loss_att 27.735653 loss_ctc 32.680096 loss_rnnt 23.690548 hw_loss 0.216290 lr 0.00049238 rank 7
2023-02-20 21:53:20,645 DEBUG TRAIN Batch 12/3100 loss 17.467199 loss_att 20.916157 loss_ctc 23.851645 loss_rnnt 15.842606 hw_loss 0.156640 lr 0.00049221 rank 1
2023-02-20 21:53:20,648 DEBUG TRAIN Batch 12/3100 loss 21.632746 loss_att 23.766314 loss_ctc 26.052593 loss_rnnt 20.539841 hw_loss 0.144148 lr 0.00049223 rank 0
2023-02-20 21:53:20,649 DEBUG TRAIN Batch 12/3100 loss 16.679586 loss_att 20.140863 loss_ctc 21.473501 loss_rnnt 15.240470 hw_loss 0.201890 lr 0.00049214 rank 2
2023-02-20 21:53:20,649 DEBUG TRAIN Batch 12/3100 loss 26.013981 loss_att 24.913109 loss_ctc 25.988091 loss_rnnt 26.180218 hw_loss 0.107608 lr 0.00049216 rank 6
2023-02-20 21:53:20,650 DEBUG TRAIN Batch 12/3100 loss 13.073359 loss_att 14.037163 loss_ctc 13.720279 loss_rnnt 12.715591 hw_loss 0.147656 lr 0.00049217 rank 5
2023-02-20 21:53:20,649 DEBUG TRAIN Batch 12/3100 loss 12.888179 loss_att 15.674902 loss_ctc 20.453154 loss_rnnt 11.298513 hw_loss 0.044358 lr 0.00049214 rank 7
2023-02-20 21:53:20,650 DEBUG TRAIN Batch 12/3100 loss 9.959244 loss_att 11.906150 loss_ctc 10.865077 loss_rnnt 9.399569 hw_loss 0.092844 lr 0.00049219 rank 4
2023-02-20 21:53:20,699 DEBUG TRAIN Batch 12/3100 loss 16.629337 loss_att 23.653214 loss_ctc 18.346823 loss_rnnt 14.945503 hw_loss 0.093859 lr 0.00049218 rank 3
2023-02-20 21:54:43,929 DEBUG TRAIN Batch 12/3200 loss 7.780304 loss_att 14.973770 loss_ctc 8.234628 loss_rnnt 6.212595 hw_loss 0.128325 lr 0.00049194 rank 3
2023-02-20 21:54:43,929 DEBUG TRAIN Batch 12/3200 loss 14.124131 loss_att 16.383789 loss_ctc 20.461767 loss_rnnt 12.707695 hw_loss 0.224037 lr 0.00049195 rank 4
2023-02-20 21:54:43,930 DEBUG TRAIN Batch 12/3200 loss 21.276741 loss_att 23.720013 loss_ctc 26.599957 loss_rnnt 19.964054 hw_loss 0.214259 lr 0.00049191 rank 7
2023-02-20 21:54:43,930 DEBUG TRAIN Batch 12/3200 loss 11.211848 loss_att 10.832441 loss_ctc 15.062334 loss_rnnt 10.646486 hw_loss 0.239708 lr 0.00049199 rank 0
2023-02-20 21:54:43,934 DEBUG TRAIN Batch 12/3200 loss 16.311529 loss_att 16.980043 loss_ctc 14.669952 loss_rnnt 16.341396 hw_loss 0.103699 lr 0.00049193 rank 5
2023-02-20 21:54:43,949 DEBUG TRAIN Batch 12/3200 loss 11.807922 loss_att 14.363737 loss_ctc 13.487602 loss_rnnt 10.997077 hw_loss 0.141982 lr 0.00049190 rank 2
2023-02-20 21:54:43,958 DEBUG TRAIN Batch 12/3200 loss 14.023611 loss_att 13.906591 loss_ctc 16.512243 loss_rnnt 13.522111 hw_loss 0.362037 lr 0.00049197 rank 1
2023-02-20 21:54:43,979 DEBUG TRAIN Batch 12/3200 loss 9.630494 loss_att 12.634465 loss_ctc 12.738627 loss_rnnt 8.486811 hw_loss 0.240883 lr 0.00049192 rank 6
2023-02-20 21:56:05,922 DEBUG TRAIN Batch 12/3300 loss 32.793064 loss_att 37.769970 loss_ctc 36.444672 loss_rnnt 31.264568 hw_loss 0.086689 lr 0.00049168 rank 6
2023-02-20 21:56:05,923 DEBUG TRAIN Batch 12/3300 loss 15.128910 loss_att 19.051235 loss_ctc 16.645720 loss_rnnt 14.051985 hw_loss 0.169163 lr 0.00049171 rank 4
2023-02-20 21:56:05,925 DEBUG TRAIN Batch 12/3300 loss 16.933956 loss_att 20.943064 loss_ctc 17.320938 loss_rnnt 16.033329 hw_loss 0.088517 lr 0.00049175 rank 0
2023-02-20 21:56:05,925 DEBUG TRAIN Batch 12/3300 loss 17.299812 loss_att 22.344221 loss_ctc 23.512638 loss_rnnt 15.416485 hw_loss 0.086379 lr 0.00049170 rank 3
2023-02-20 21:56:05,928 DEBUG TRAIN Batch 12/3300 loss 9.888840 loss_att 15.136406 loss_ctc 14.203382 loss_rnnt 8.232115 hw_loss 0.059887 lr 0.00049167 rank 7
2023-02-20 21:56:05,930 DEBUG TRAIN Batch 12/3300 loss 8.135384 loss_att 14.945896 loss_ctc 9.615486 loss_rnnt 6.544391 hw_loss 0.059142 lr 0.00049173 rank 1
2023-02-20 21:56:05,933 DEBUG TRAIN Batch 12/3300 loss 15.787327 loss_att 18.994314 loss_ctc 17.784706 loss_rnnt 14.868139 hw_loss 0.021510 lr 0.00049166 rank 2
2023-02-20 21:56:05,939 DEBUG TRAIN Batch 12/3300 loss 9.150061 loss_att 11.654856 loss_ctc 10.879344 loss_rnnt 8.381377 hw_loss 0.069663 lr 0.00049170 rank 5
2023-02-20 21:57:25,441 DEBUG TRAIN Batch 12/3400 loss 25.824831 loss_att 27.672651 loss_ctc 26.351339 loss_rnnt 25.266500 hw_loss 0.222311 lr 0.00049149 rank 1
2023-02-20 21:57:25,444 DEBUG TRAIN Batch 12/3400 loss 20.362677 loss_att 24.534447 loss_ctc 21.019432 loss_rnnt 19.383850 hw_loss 0.106691 lr 0.00049143 rank 7
2023-02-20 21:57:25,445 DEBUG TRAIN Batch 12/3400 loss 20.731735 loss_att 22.996614 loss_ctc 28.383038 loss_rnnt 19.198566 hw_loss 0.112537 lr 0.00049144 rank 6
2023-02-20 21:57:25,446 DEBUG TRAIN Batch 12/3400 loss 17.742685 loss_att 21.384098 loss_ctc 21.045910 loss_rnnt 16.516676 hw_loss 0.107432 lr 0.00049148 rank 4
2023-02-20 21:57:25,448 DEBUG TRAIN Batch 12/3400 loss 15.147529 loss_att 17.609684 loss_ctc 12.956295 loss_rnnt 14.900184 hw_loss 0.088272 lr 0.00049151 rank 0
2023-02-20 21:57:25,449 DEBUG TRAIN Batch 12/3400 loss 16.782116 loss_att 20.815620 loss_ctc 24.552650 loss_rnnt 14.894773 hw_loss 0.083570 lr 0.00049142 rank 2
2023-02-20 21:57:25,452 DEBUG TRAIN Batch 12/3400 loss 15.959513 loss_att 22.967924 loss_ctc 17.420385 loss_rnnt 14.199465 hw_loss 0.306716 lr 0.00049146 rank 3
2023-02-20 21:57:25,455 DEBUG TRAIN Batch 12/3400 loss 8.800110 loss_att 12.728094 loss_ctc 8.867805 loss_rnnt 7.850185 hw_loss 0.291191 lr 0.00049146 rank 5
2023-02-20 21:58:47,018 DEBUG TRAIN Batch 12/3500 loss 18.830818 loss_att 27.048378 loss_ctc 23.600067 loss_rnnt 16.495861 hw_loss 0.104146 lr 0.00049119 rank 7
2023-02-20 21:58:47,025 DEBUG TRAIN Batch 12/3500 loss 26.992754 loss_att 31.939281 loss_ctc 32.954147 loss_rnnt 25.093721 hw_loss 0.215393 lr 0.00049123 rank 3
2023-02-20 21:58:47,026 DEBUG TRAIN Batch 12/3500 loss 14.690385 loss_att 22.240429 loss_ctc 26.066906 loss_rnnt 11.514380 hw_loss 0.279614 lr 0.00049121 rank 6
2023-02-20 21:58:47,027 DEBUG TRAIN Batch 12/3500 loss 19.368412 loss_att 21.928463 loss_ctc 22.822733 loss_rnnt 18.368919 hw_loss 0.050444 lr 0.00049128 rank 0
2023-02-20 21:58:47,027 DEBUG TRAIN Batch 12/3500 loss 26.035133 loss_att 31.241405 loss_ctc 33.904549 loss_rnnt 23.931097 hw_loss 0.025364 lr 0.00049124 rank 4
2023-02-20 21:58:47,031 DEBUG TRAIN Batch 12/3500 loss 20.053341 loss_att 24.427574 loss_ctc 27.894274 loss_rnnt 18.054325 hw_loss 0.147581 lr 0.00049119 rank 2
2023-02-20 21:58:47,030 DEBUG TRAIN Batch 12/3500 loss 16.759972 loss_att 22.164944 loss_ctc 22.410637 loss_rnnt 14.897032 hw_loss 0.053484 lr 0.00049126 rank 1
2023-02-20 21:58:47,047 DEBUG TRAIN Batch 12/3500 loss 14.345659 loss_att 18.083220 loss_ctc 17.936558 loss_rnnt 13.080709 hw_loss 0.072475 lr 0.00049122 rank 5
2023-02-20 22:00:07,788 DEBUG TRAIN Batch 12/3600 loss 6.413651 loss_att 11.378778 loss_ctc 5.775180 loss_rnnt 5.430601 hw_loss 0.140915 lr 0.00049097 rank 6
2023-02-20 22:00:07,789 DEBUG TRAIN Batch 12/3600 loss 23.522457 loss_att 24.586239 loss_ctc 27.511852 loss_rnnt 22.599218 hw_loss 0.334806 lr 0.00049100 rank 4
2023-02-20 22:00:07,791 DEBUG TRAIN Batch 12/3600 loss 22.116068 loss_att 23.009222 loss_ctc 29.861038 loss_rnnt 20.837070 hw_loss 0.126942 lr 0.00049096 rank 7
2023-02-20 22:00:07,796 DEBUG TRAIN Batch 12/3600 loss 18.327381 loss_att 20.356741 loss_ctc 22.097263 loss_rnnt 17.343596 hw_loss 0.141122 lr 0.00049099 rank 3
2023-02-20 22:00:07,796 DEBUG TRAIN Batch 12/3600 loss 13.224747 loss_att 14.258550 loss_ctc 15.252316 loss_rnnt 12.696550 hw_loss 0.095800 lr 0.00049098 rank 5
2023-02-20 22:00:07,820 DEBUG TRAIN Batch 12/3600 loss 19.346836 loss_att 26.982635 loss_ctc 25.928635 loss_rnnt 16.855358 hw_loss 0.162646 lr 0.00049102 rank 1
2023-02-20 22:00:07,829 DEBUG TRAIN Batch 12/3600 loss 25.517710 loss_att 28.266895 loss_ctc 34.470600 loss_rnnt 23.732727 hw_loss 0.077678 lr 0.00049095 rank 2
2023-02-20 22:00:07,840 DEBUG TRAIN Batch 12/3600 loss 13.252665 loss_att 19.365864 loss_ctc 14.528008 loss_rnnt 11.746399 hw_loss 0.212963 lr 0.00049104 rank 0
2023-02-20 22:01:29,010 DEBUG TRAIN Batch 12/3700 loss 22.193701 loss_att 21.520220 loss_ctc 25.209923 loss_rnnt 21.819263 hw_loss 0.200567 lr 0.00049080 rank 0
2023-02-20 22:01:29,013 DEBUG TRAIN Batch 12/3700 loss 19.319889 loss_att 21.062336 loss_ctc 21.078001 loss_rnnt 18.697510 hw_loss 0.074013 lr 0.00049076 rank 4
2023-02-20 22:01:29,018 DEBUG TRAIN Batch 12/3700 loss 12.029566 loss_att 16.442631 loss_ctc 13.556004 loss_rnnt 10.838216 hw_loss 0.197272 lr 0.00049073 rank 6
2023-02-20 22:01:29,018 DEBUG TRAIN Batch 12/3700 loss 18.758371 loss_att 21.144878 loss_ctc 22.034285 loss_rnnt 17.804325 hw_loss 0.074916 lr 0.00049078 rank 1
2023-02-20 22:01:29,019 DEBUG TRAIN Batch 12/3700 loss 19.488865 loss_att 19.989244 loss_ctc 26.058710 loss_rnnt 18.467533 hw_loss 0.084896 lr 0.00049072 rank 7
2023-02-20 22:01:29,020 DEBUG TRAIN Batch 12/3700 loss 11.836657 loss_att 14.395302 loss_ctc 16.854652 loss_rnnt 10.588582 hw_loss 0.126150 lr 0.00049075 rank 3
2023-02-20 22:01:29,022 DEBUG TRAIN Batch 12/3700 loss 17.574450 loss_att 21.438498 loss_ctc 24.943262 loss_rnnt 15.748972 hw_loss 0.131545 lr 0.00049075 rank 5
2023-02-20 22:01:29,073 DEBUG TRAIN Batch 12/3700 loss 10.910368 loss_att 14.726291 loss_ctc 12.577777 loss_rnnt 9.843717 hw_loss 0.152147 lr 0.00049071 rank 2
2023-02-20 22:02:50,770 DEBUG TRAIN Batch 12/3800 loss 10.347454 loss_att 12.374725 loss_ctc 14.806987 loss_rnnt 9.213379 hw_loss 0.251282 lr 0.00049053 rank 4
2023-02-20 22:02:50,772 DEBUG TRAIN Batch 12/3800 loss 7.739657 loss_att 9.285637 loss_ctc 8.285216 loss_rnnt 7.307091 hw_loss 0.094930 lr 0.00049048 rank 7
2023-02-20 22:02:50,775 DEBUG TRAIN Batch 12/3800 loss 23.486795 loss_att 23.096075 loss_ctc 24.965631 loss_rnnt 23.296429 hw_loss 0.133755 lr 0.00049050 rank 6
2023-02-20 22:02:50,779 DEBUG TRAIN Batch 12/3800 loss 11.574780 loss_att 12.814483 loss_ctc 14.555904 loss_rnnt 10.836277 hw_loss 0.174523 lr 0.00049057 rank 0
2023-02-20 22:02:50,781 DEBUG TRAIN Batch 12/3800 loss 15.102960 loss_att 13.609632 loss_ctc 18.004978 loss_rnnt 14.849833 hw_loss 0.309105 lr 0.00049051 rank 5
2023-02-20 22:02:50,783 DEBUG TRAIN Batch 12/3800 loss 9.651866 loss_att 12.314346 loss_ctc 12.903897 loss_rnnt 8.542171 hw_loss 0.269239 lr 0.00049055 rank 1
2023-02-20 22:02:50,783 DEBUG TRAIN Batch 12/3800 loss 17.026585 loss_att 17.758678 loss_ctc 20.247078 loss_rnnt 16.257481 hw_loss 0.362413 lr 0.00049048 rank 2
2023-02-20 22:02:50,783 DEBUG TRAIN Batch 12/3800 loss 19.388929 loss_att 22.954861 loss_ctc 23.127312 loss_rnnt 18.115799 hw_loss 0.115298 lr 0.00049052 rank 3
2023-02-20 22:04:12,558 DEBUG TRAIN Batch 12/3900 loss 40.544441 loss_att 39.203747 loss_ctc 42.022362 loss_rnnt 40.600525 hw_loss 0.028126 lr 0.00049029 rank 4
2023-02-20 22:04:12,564 DEBUG TRAIN Batch 12/3900 loss 12.926116 loss_att 13.264796 loss_ctc 15.549583 loss_rnnt 12.385866 hw_loss 0.230094 lr 0.00049026 rank 6
2023-02-20 22:04:12,565 DEBUG TRAIN Batch 12/3900 loss 12.333964 loss_att 12.509476 loss_ctc 14.703215 loss_rnnt 11.802291 hw_loss 0.338757 lr 0.00049025 rank 7
2023-02-20 22:04:12,566 DEBUG TRAIN Batch 12/3900 loss 10.477192 loss_att 15.971146 loss_ctc 12.785159 loss_rnnt 9.043026 hw_loss 0.051837 lr 0.00049028 rank 3
2023-02-20 22:04:12,566 DEBUG TRAIN Batch 12/3900 loss 15.978384 loss_att 21.575476 loss_ctc 17.465984 loss_rnnt 14.637256 hw_loss 0.043806 lr 0.00049031 rank 1
2023-02-20 22:04:12,569 DEBUG TRAIN Batch 12/3900 loss 17.167589 loss_att 19.002634 loss_ctc 19.164139 loss_rnnt 16.425846 hw_loss 0.203492 lr 0.00049028 rank 5
2023-02-20 22:04:12,570 DEBUG TRAIN Batch 12/3900 loss 17.207747 loss_att 22.477650 loss_ctc 17.267107 loss_rnnt 16.006222 hw_loss 0.261807 lr 0.00049024 rank 2
2023-02-20 22:04:12,572 DEBUG TRAIN Batch 12/3900 loss 31.094610 loss_att 38.497940 loss_ctc 36.434906 loss_rnnt 28.841841 hw_loss 0.112624 lr 0.00049033 rank 0
2023-02-20 22:05:32,294 DEBUG TRAIN Batch 12/4000 loss 16.456005 loss_att 18.618082 loss_ctc 19.256859 loss_rnnt 15.603931 hw_loss 0.086646 lr 0.00049001 rank 7
2023-02-20 22:05:32,295 DEBUG TRAIN Batch 12/4000 loss 10.310998 loss_att 18.456747 loss_ctc 16.624329 loss_rnnt 7.754333 hw_loss 0.160759 lr 0.00049008 rank 1
2023-02-20 22:05:32,297 DEBUG TRAIN Batch 12/4000 loss 11.703172 loss_att 17.346840 loss_ctc 13.561052 loss_rnnt 10.275863 hw_loss 0.095359 lr 0.00049001 rank 2
2023-02-20 22:05:32,299 DEBUG TRAIN Batch 12/4000 loss 25.112265 loss_att 30.366302 loss_ctc 33.819866 loss_rnnt 22.823624 hw_loss 0.144040 lr 0.00049006 rank 4
2023-02-20 22:05:32,301 DEBUG TRAIN Batch 12/4000 loss 12.201196 loss_att 18.231277 loss_ctc 17.043522 loss_rnnt 10.325609 hw_loss 0.044863 lr 0.00049009 rank 0
2023-02-20 22:05:32,301 DEBUG TRAIN Batch 12/4000 loss 40.341434 loss_att 42.936440 loss_ctc 49.106804 loss_rnnt 38.567219 hw_loss 0.162184 lr 0.00049005 rank 3
2023-02-20 22:05:32,304 DEBUG TRAIN Batch 12/4000 loss 14.903601 loss_att 20.223217 loss_ctc 15.787677 loss_rnnt 13.709404 hw_loss 0.023243 lr 0.00049004 rank 5
2023-02-20 22:05:32,347 DEBUG TRAIN Batch 12/4000 loss 23.944225 loss_att 21.995003 loss_ctc 24.103117 loss_rnnt 24.221205 hw_loss 0.171897 lr 0.00049003 rank 6
2023-02-20 22:06:50,652 DEBUG TRAIN Batch 12/4100 loss 24.906776 loss_att 28.885845 loss_ctc 26.537010 loss_rnnt 23.826752 hw_loss 0.125333 lr 0.00048986 rank 0
2023-02-20 22:06:50,653 DEBUG TRAIN Batch 12/4100 loss 16.054487 loss_att 23.002489 loss_ctc 21.996611 loss_rnnt 13.840635 hw_loss 0.059945 lr 0.00048981 rank 5
2023-02-20 22:06:50,654 DEBUG TRAIN Batch 12/4100 loss 21.160038 loss_att 22.549608 loss_ctc 25.048065 loss_rnnt 20.328674 hw_loss 0.065714 lr 0.00048978 rank 7
2023-02-20 22:06:50,654 DEBUG TRAIN Batch 12/4100 loss 7.463428 loss_att 8.878691 loss_ctc 7.365032 loss_rnnt 7.134759 hw_loss 0.110128 lr 0.00048977 rank 2
2023-02-20 22:06:50,657 DEBUG TRAIN Batch 12/4100 loss 21.990564 loss_att 28.757931 loss_ctc 24.230612 loss_rnnt 20.293549 hw_loss 0.084130 lr 0.00048984 rank 1
2023-02-20 22:06:50,657 DEBUG TRAIN Batch 12/4100 loss 14.922956 loss_att 17.947910 loss_ctc 17.620501 loss_rnnt 13.925631 hw_loss 0.061240 lr 0.00048981 rank 3
2023-02-20 22:06:50,658 DEBUG TRAIN Batch 12/4100 loss 12.831924 loss_att 15.864972 loss_ctc 19.965185 loss_rnnt 11.227489 hw_loss 0.087609 lr 0.00048982 rank 4
2023-02-20 22:06:50,701 DEBUG TRAIN Batch 12/4100 loss 21.792101 loss_att 29.359673 loss_ctc 27.101233 loss_rnnt 19.529766 hw_loss 0.076751 lr 0.00048979 rank 6
2023-02-20 22:08:09,780 DEBUG TRAIN Batch 12/4200 loss 18.332016 loss_att 20.294304 loss_ctc 24.609520 loss_rnnt 17.020588 hw_loss 0.153696 lr 0.00048961 rank 1
2023-02-20 22:08:09,781 DEBUG TRAIN Batch 12/4200 loss 10.275761 loss_att 16.812174 loss_ctc 14.009645 loss_rnnt 8.456781 hw_loss 0.025959 lr 0.00048954 rank 7
2023-02-20 22:08:09,782 DEBUG TRAIN Batch 12/4200 loss 7.770367 loss_att 11.654729 loss_ctc 8.214620 loss_rnnt 6.885026 hw_loss 0.092313 lr 0.00048954 rank 2
2023-02-20 22:08:09,783 DEBUG TRAIN Batch 12/4200 loss 20.512342 loss_att 23.845154 loss_ctc 26.673201 loss_rnnt 18.981050 hw_loss 0.081155 lr 0.00048956 rank 6
2023-02-20 22:08:09,783 DEBUG TRAIN Batch 12/4200 loss 7.888463 loss_att 11.402105 loss_ctc 10.864968 loss_rnnt 6.704417 hw_loss 0.158345 lr 0.00048958 rank 3
2023-02-20 22:08:09,784 DEBUG TRAIN Batch 12/4200 loss 15.666936 loss_att 18.484615 loss_ctc 15.517578 loss_rnnt 15.034517 hw_loss 0.166493 lr 0.00048959 rank 4
2023-02-20 22:08:09,787 DEBUG TRAIN Batch 12/4200 loss 18.291168 loss_att 23.396896 loss_ctc 23.800797 loss_rnnt 16.411507 hw_loss 0.232312 lr 0.00048962 rank 0
2023-02-20 22:08:09,790 DEBUG TRAIN Batch 12/4200 loss 15.396922 loss_att 20.284636 loss_ctc 19.214651 loss_rnnt 13.841104 hw_loss 0.129836 lr 0.00048957 rank 5
2023-02-20 22:09:31,492 DEBUG TRAIN Batch 12/4300 loss 11.522002 loss_att 15.876671 loss_ctc 19.151115 loss_rnnt 9.551678 hw_loss 0.154078 lr 0.00048935 rank 4
2023-02-20 22:09:31,494 DEBUG TRAIN Batch 12/4300 loss 8.754608 loss_att 12.541874 loss_ctc 12.976392 loss_rnnt 7.367485 hw_loss 0.125185 lr 0.00048934 rank 3
2023-02-20 22:09:31,495 DEBUG TRAIN Batch 12/4300 loss 7.176519 loss_att 12.181670 loss_ctc 8.166725 loss_rnnt 6.024578 hw_loss 0.035407 lr 0.00048930 rank 2
2023-02-20 22:09:31,497 DEBUG TRAIN Batch 12/4300 loss 8.334894 loss_att 11.632348 loss_ctc 8.158148 loss_rnnt 7.610517 hw_loss 0.165849 lr 0.00048931 rank 7
2023-02-20 22:09:31,498 DEBUG TRAIN Batch 12/4300 loss 17.566362 loss_att 19.856558 loss_ctc 21.371674 loss_rnnt 16.440254 hw_loss 0.301304 lr 0.00048934 rank 5
2023-02-20 22:09:31,501 DEBUG TRAIN Batch 12/4300 loss 22.374083 loss_att 29.970028 loss_ctc 25.620571 loss_rnnt 20.343071 hw_loss 0.148044 lr 0.00048937 rank 1
2023-02-20 22:09:31,502 DEBUG TRAIN Batch 12/4300 loss 17.493271 loss_att 24.439541 loss_ctc 20.961893 loss_rnnt 15.569260 hw_loss 0.135510 lr 0.00048939 rank 0
2023-02-20 22:09:31,545 DEBUG TRAIN Batch 12/4300 loss 11.143193 loss_att 14.466562 loss_ctc 14.665546 loss_rnnt 9.960634 hw_loss 0.090443 lr 0.00048932 rank 6
2023-02-20 22:10:52,370 DEBUG TRAIN Batch 12/4400 loss 7.918067 loss_att 11.127003 loss_ctc 10.994285 loss_rnnt 6.783851 hw_loss 0.154251 lr 0.00048914 rank 1
2023-02-20 22:10:52,372 DEBUG TRAIN Batch 12/4400 loss 8.662922 loss_att 16.265495 loss_ctc 11.944777 loss_rnnt 6.642541 hw_loss 0.116784 lr 0.00048911 rank 3
2023-02-20 22:10:52,372 DEBUG TRAIN Batch 12/4400 loss 17.180044 loss_att 17.059250 loss_ctc 17.229725 loss_rnnt 17.090466 hw_loss 0.200839 lr 0.00048907 rank 2
2023-02-20 22:10:52,374 DEBUG TRAIN Batch 12/4400 loss 20.364874 loss_att 19.732178 loss_ctc 26.308014 loss_rnnt 19.608503 hw_loss 0.169667 lr 0.00048912 rank 4
2023-02-20 22:10:52,375 DEBUG TRAIN Batch 12/4400 loss 15.991259 loss_att 19.604507 loss_ctc 22.912313 loss_rnnt 14.282526 hw_loss 0.118642 lr 0.00048907 rank 7
2023-02-20 22:10:52,377 DEBUG TRAIN Batch 12/4400 loss 17.846479 loss_att 22.511858 loss_ctc 24.035606 loss_rnnt 16.044027 hw_loss 0.082797 lr 0.00048909 rank 6
2023-02-20 22:10:52,378 DEBUG TRAIN Batch 12/4400 loss 12.883883 loss_att 15.898537 loss_ctc 11.194898 loss_rnnt 12.455843 hw_loss 0.094326 lr 0.00048916 rank 0
2023-02-20 22:10:52,381 DEBUG TRAIN Batch 12/4400 loss 4.688369 loss_att 9.283298 loss_ctc 6.419031 loss_rnnt 3.450517 hw_loss 0.165208 lr 0.00048910 rank 5
2023-02-20 22:12:12,728 DEBUG TRAIN Batch 12/4500 loss 6.122844 loss_att 9.827261 loss_ctc 5.652241 loss_rnnt 5.368874 hw_loss 0.142188 lr 0.00048887 rank 3
2023-02-20 22:12:12,729 DEBUG TRAIN Batch 12/4500 loss 10.462358 loss_att 12.813065 loss_ctc 14.642815 loss_rnnt 9.332724 hw_loss 0.191435 lr 0.00048888 rank 4
2023-02-20 22:12:12,730 DEBUG TRAIN Batch 12/4500 loss 15.783119 loss_att 18.265020 loss_ctc 17.019176 loss_rnnt 14.924903 hw_loss 0.369430 lr 0.00048884 rank 7
2023-02-20 22:12:12,730 DEBUG TRAIN Batch 12/4500 loss 27.590170 loss_att 33.370117 loss_ctc 32.014919 loss_rnnt 25.829880 hw_loss 0.026873 lr 0.00048892 rank 0
2023-02-20 22:12:12,734 DEBUG TRAIN Batch 12/4500 loss 12.295344 loss_att 16.702581 loss_ctc 20.314121 loss_rnnt 10.295286 hw_loss 0.092703 lr 0.00048883 rank 2
2023-02-20 22:12:12,735 DEBUG TRAIN Batch 12/4500 loss 16.093801 loss_att 17.039606 loss_ctc 16.386139 loss_rnnt 15.726520 hw_loss 0.260892 lr 0.00048885 rank 6
2023-02-20 22:12:12,736 DEBUG TRAIN Batch 12/4500 loss 12.588693 loss_att 12.994440 loss_ctc 15.067230 loss_rnnt 12.017478 hw_loss 0.299238 lr 0.00048890 rank 1
2023-02-20 22:12:12,739 DEBUG TRAIN Batch 12/4500 loss 12.670218 loss_att 16.243055 loss_ctc 16.797216 loss_rnnt 11.361258 hw_loss 0.082738 lr 0.00048887 rank 5
2023-02-20 22:13:34,027 DEBUG TRAIN Batch 12/4600 loss 24.462080 loss_att 28.473186 loss_ctc 32.244930 loss_rnnt 22.583746 hw_loss 0.071996 lr 0.00048860 rank 2
2023-02-20 22:13:34,028 DEBUG TRAIN Batch 12/4600 loss 13.241884 loss_att 23.019917 loss_ctc 15.072825 loss_rnnt 11.029200 hw_loss 0.024286 lr 0.00048862 rank 6
2023-02-20 22:13:34,029 DEBUG TRAIN Batch 12/4600 loss 16.234043 loss_att 23.261005 loss_ctc 21.043310 loss_rnnt 14.174545 hw_loss 0.024128 lr 0.00048861 rank 7
2023-02-20 22:13:34,029 DEBUG TRAIN Batch 12/4600 loss 12.698903 loss_att 18.568348 loss_ctc 15.081591 loss_rnnt 11.140427 hw_loss 0.125429 lr 0.00048865 rank 4
2023-02-20 22:13:34,033 DEBUG TRAIN Batch 12/4600 loss 21.717415 loss_att 27.053192 loss_ctc 31.938589 loss_rnnt 19.265133 hw_loss 0.041819 lr 0.00048867 rank 1
2023-02-20 22:13:34,039 DEBUG TRAIN Batch 12/4600 loss 13.823768 loss_att 19.906843 loss_ctc 20.545912 loss_rnnt 11.665976 hw_loss 0.084172 lr 0.00048864 rank 3
2023-02-20 22:13:34,043 DEBUG TRAIN Batch 12/4600 loss 6.581134 loss_att 10.286617 loss_ctc 7.806526 loss_rnnt 5.546415 hw_loss 0.244193 lr 0.00048863 rank 5
2023-02-20 22:13:34,080 DEBUG TRAIN Batch 12/4600 loss 11.327985 loss_att 18.606636 loss_ctc 16.544796 loss_rnnt 9.100878 hw_loss 0.142130 lr 0.00048869 rank 0
2023-02-20 22:14:56,348 DEBUG TRAIN Batch 12/4700 loss 11.688635 loss_att 15.904198 loss_ctc 12.923227 loss_rnnt 10.633593 hw_loss 0.088720 lr 0.00048844 rank 1
2023-02-20 22:14:56,350 DEBUG TRAIN Batch 12/4700 loss 14.664907 loss_att 17.193829 loss_ctc 19.832916 loss_rnnt 13.389802 hw_loss 0.150473 lr 0.00048841 rank 3
2023-02-20 22:14:56,351 DEBUG TRAIN Batch 12/4700 loss 12.095483 loss_att 14.092228 loss_ctc 16.692558 loss_rnnt 10.993965 hw_loss 0.167295 lr 0.00048846 rank 0
2023-02-20 22:14:56,352 DEBUG TRAIN Batch 12/4700 loss 8.379913 loss_att 12.040055 loss_ctc 11.920997 loss_rnnt 7.134755 hw_loss 0.076846 lr 0.00048837 rank 7
2023-02-20 22:14:56,354 DEBUG TRAIN Batch 12/4700 loss 9.648559 loss_att 11.653129 loss_ctc 12.283117 loss_rnnt 8.880602 hw_loss 0.029563 lr 0.00048837 rank 2
2023-02-20 22:14:56,355 DEBUG TRAIN Batch 12/4700 loss 5.470212 loss_att 13.384936 loss_ctc 6.027391 loss_rnnt 3.783314 hw_loss 0.055617 lr 0.00048842 rank 4
2023-02-20 22:14:56,355 DEBUG TRAIN Batch 12/4700 loss 18.556816 loss_att 21.300640 loss_ctc 18.719494 loss_rnnt 17.876274 hw_loss 0.206415 lr 0.00048839 rank 6
2023-02-20 22:14:56,359 DEBUG TRAIN Batch 12/4700 loss 27.141693 loss_att 27.552979 loss_ctc 31.624304 loss_rnnt 26.436035 hw_loss 0.048229 lr 0.00048840 rank 5
2023-02-20 22:16:18,211 DEBUG TRAIN Batch 12/4800 loss 14.969478 loss_att 19.252962 loss_ctc 17.408646 loss_rnnt 13.749208 hw_loss 0.071905 lr 0.00048820 rank 1
2023-02-20 22:16:18,213 DEBUG TRAIN Batch 12/4800 loss 10.478313 loss_att 17.532742 loss_ctc 13.887857 loss_rnnt 8.599758 hw_loss 0.024495 lr 0.00048817 rank 3
2023-02-20 22:16:18,213 DEBUG TRAIN Batch 12/4800 loss 19.070505 loss_att 20.835327 loss_ctc 29.304886 loss_rnnt 17.321812 hw_loss 0.058400 lr 0.00048818 rank 4
2023-02-20 22:16:18,213 DEBUG TRAIN Batch 12/4800 loss 14.047782 loss_att 20.978928 loss_ctc 16.922102 loss_rnnt 12.223539 hw_loss 0.102698 lr 0.00048815 rank 6
2023-02-20 22:16:18,219 DEBUG TRAIN Batch 12/4800 loss 24.447647 loss_att 25.975845 loss_ctc 26.070522 loss_rnnt 23.862432 hw_loss 0.118483 lr 0.00048813 rank 2
2023-02-20 22:16:18,219 DEBUG TRAIN Batch 12/4800 loss 3.814416 loss_att 9.983235 loss_ctc 4.882451 loss_rnnt 2.371569 hw_loss 0.125021 lr 0.00048814 rank 7
2023-02-20 22:16:18,221 DEBUG TRAIN Batch 12/4800 loss 7.384819 loss_att 11.972596 loss_ctc 8.779295 loss_rnnt 6.240347 hw_loss 0.076848 lr 0.00048817 rank 5
2023-02-20 22:16:18,226 DEBUG TRAIN Batch 12/4800 loss 14.937290 loss_att 20.284893 loss_ctc 18.433037 loss_rnnt 13.389153 hw_loss 0.023467 lr 0.00048822 rank 0
2023-02-20 22:22:26,269 DEBUG TRAIN Batch 12/4900 loss 12.367844 loss_att 15.914360 loss_ctc 14.231860 loss_rnnt 11.322509 hw_loss 0.164056 lr 0.00048799 rank 0
2023-02-20 22:22:26,343 DEBUG TRAIN Batch 12/4900 loss 7.739432 loss_att 12.774403 loss_ctc 11.365640 loss_rnnt 6.164073 hw_loss 0.159132 lr 0.00048794 rank 5
2023-02-20 22:22:26,382 DEBUG TRAIN Batch 12/4900 loss 14.441812 loss_att 19.151997 loss_ctc 14.550556 loss_rnnt 13.395229 hw_loss 0.168837 lr 0.00048792 rank 6
2023-02-20 22:22:26,471 DEBUG TRAIN Batch 12/4900 loss 13.188493 loss_att 21.305569 loss_ctc 15.825137 loss_rnnt 11.160329 hw_loss 0.099743 lr 0.00048797 rank 1
2023-02-20 22:22:26,495 DEBUG TRAIN Batch 12/4900 loss 12.325416 loss_att 17.971523 loss_ctc 17.651281 loss_rnnt 10.430590 hw_loss 0.104044 lr 0.00048791 rank 7
2023-02-20 22:22:26,510 DEBUG TRAIN Batch 12/4900 loss 19.400602 loss_att 24.229996 loss_ctc 20.485340 loss_rnnt 18.277538 hw_loss 0.023537 lr 0.00048790 rank 2
2023-02-20 22:22:26,516 DEBUG TRAIN Batch 12/4900 loss 23.389725 loss_att 24.701366 loss_ctc 27.446825 loss_rnnt 22.559626 hw_loss 0.050292 lr 0.00048795 rank 4
2023-02-20 22:22:26,643 DEBUG TRAIN Batch 12/4900 loss 20.283146 loss_att 24.476429 loss_ctc 20.225483 loss_rnnt 19.387714 hw_loss 0.120868 lr 0.00048794 rank 3
2023-02-20 22:30:41,732 DEBUG TRAIN Batch 12/5000 loss 17.505835 loss_att 20.682755 loss_ctc 23.776566 loss_rnnt 15.994362 hw_loss 0.074980 lr 0.00048774 rank 1
2023-02-20 22:30:41,797 DEBUG TRAIN Batch 12/5000 loss 15.189125 loss_att 18.052067 loss_ctc 16.225758 loss_rnnt 14.447759 hw_loss 0.057299 lr 0.00048772 rank 4
2023-02-20 22:30:41,959 DEBUG TRAIN Batch 12/5000 loss 11.930997 loss_att 12.497224 loss_ctc 15.658389 loss_rnnt 11.257113 hw_loss 0.119349 lr 0.00048776 rank 0
2023-02-20 22:30:42,009 DEBUG TRAIN Batch 12/5000 loss 19.926544 loss_att 21.787199 loss_ctc 27.650473 loss_rnnt 18.498663 hw_loss 0.048550 lr 0.00048768 rank 7
2023-02-20 22:30:42,175 DEBUG TRAIN Batch 12/5000 loss 22.250471 loss_att 24.584887 loss_ctc 24.560692 loss_rnnt 21.427544 hw_loss 0.090028 lr 0.00048767 rank 2
2023-02-20 22:30:42,175 DEBUG TRAIN Batch 12/5000 loss 10.384586 loss_att 12.515299 loss_ctc 12.236228 loss_rnnt 9.604735 hw_loss 0.200295 lr 0.00048770 rank 5
2023-02-20 22:30:42,203 DEBUG TRAIN Batch 12/5000 loss 7.516800 loss_att 14.428310 loss_ctc 10.892841 loss_rnnt 5.641801 hw_loss 0.079796 lr 0.00048769 rank 6
2023-02-20 22:30:42,228 DEBUG TRAIN Batch 12/5000 loss 15.855783 loss_att 17.795765 loss_ctc 22.541780 loss_rnnt 14.469604 hw_loss 0.200093 lr 0.00048771 rank 3
2023-02-20 22:38:29,171 DEBUG TRAIN Batch 12/5100 loss 11.064583 loss_att 10.937576 loss_ctc 13.121297 loss_rnnt 10.695043 hw_loss 0.226335 lr 0.00048744 rank 7
2023-02-20 22:38:29,234 DEBUG TRAIN Batch 12/5100 loss 33.752579 loss_att 33.082481 loss_ctc 33.829159 loss_rnnt 33.799175 hw_loss 0.144774 lr 0.00048748 rank 3
2023-02-20 22:38:29,281 DEBUG TRAIN Batch 12/5100 loss 12.882562 loss_att 15.207821 loss_ctc 12.650151 loss_rnnt 12.432939 hw_loss 0.029170 lr 0.00048746 rank 6
2023-02-20 22:38:29,323 DEBUG TRAIN Batch 12/5100 loss 11.749324 loss_att 12.673926 loss_ctc 16.430344 loss_rnnt 10.888138 hw_loss 0.097745 lr 0.00048753 rank 0
2023-02-20 22:38:29,435 DEBUG TRAIN Batch 12/5100 loss 11.600113 loss_att 11.656751 loss_ctc 12.942186 loss_rnnt 11.251838 hw_loss 0.296258 lr 0.00048747 rank 5
2023-02-20 22:38:29,510 DEBUG TRAIN Batch 12/5100 loss 4.992663 loss_att 9.010063 loss_ctc 4.728112 loss_rnnt 4.155859 hw_loss 0.128620 lr 0.00048744 rank 2
2023-02-20 22:38:29,704 DEBUG TRAIN Batch 12/5100 loss 16.537609 loss_att 16.956398 loss_ctc 18.377481 loss_rnnt 16.086271 hw_loss 0.229245 lr 0.00048751 rank 1
2023-02-20 22:38:30,780 DEBUG TRAIN Batch 12/5100 loss 14.825681 loss_att 13.325430 loss_ctc 16.732557 loss_rnnt 14.766991 hw_loss 0.195918 lr 0.00048749 rank 4
2023-02-20 22:46:23,881 DEBUG TRAIN Batch 12/5200 loss 7.190148 loss_att 12.167122 loss_ctc 10.396038 loss_rnnt 5.712317 hw_loss 0.103095 lr 0.00048726 rank 4
2023-02-20 22:46:23,922 DEBUG TRAIN Batch 12/5200 loss 24.876289 loss_att 28.780857 loss_ctc 29.477898 loss_rnnt 23.400312 hw_loss 0.152843 lr 0.00048728 rank 1
2023-02-20 22:46:23,967 DEBUG TRAIN Batch 12/5200 loss 7.915379 loss_att 14.366975 loss_ctc 10.590269 loss_rnnt 6.201392 hw_loss 0.125655 lr 0.00048724 rank 5
2023-02-20 22:46:24,019 DEBUG TRAIN Batch 12/5200 loss 18.745935 loss_att 21.629007 loss_ctc 24.395605 loss_rnnt 17.376198 hw_loss 0.074685 lr 0.00048725 rank 3
2023-02-20 22:46:24,047 DEBUG TRAIN Batch 12/5200 loss 11.394913 loss_att 17.410608 loss_ctc 16.285347 loss_rnnt 9.480536 hw_loss 0.110962 lr 0.00048721 rank 2
2023-02-20 22:46:24,062 DEBUG TRAIN Batch 12/5200 loss 26.187689 loss_att 30.065882 loss_ctc 30.910810 loss_rnnt 24.740559 hw_loss 0.078265 lr 0.00048721 rank 7
2023-02-20 22:46:24,125 DEBUG TRAIN Batch 12/5200 loss 17.165300 loss_att 25.985697 loss_ctc 16.014278 loss_rnnt 15.542625 hw_loss 0.022622 lr 0.00048729 rank 0
2023-02-20 22:46:24,163 DEBUG TRAIN Batch 12/5200 loss 11.434143 loss_att 11.730444 loss_ctc 14.437921 loss_rnnt 10.830104 hw_loss 0.270516 lr 0.00048723 rank 6
2023-02-20 22:54:28,051 DEBUG TRAIN Batch 12/5300 loss 14.187685 loss_att 19.026907 loss_ctc 18.368227 loss_rnnt 12.602295 hw_loss 0.112761 lr 0.00048697 rank 2
2023-02-20 22:54:28,077 DEBUG TRAIN Batch 12/5300 loss 19.373508 loss_att 24.428625 loss_ctc 27.822422 loss_rnnt 17.198338 hw_loss 0.070547 lr 0.00048703 rank 4
2023-02-20 22:54:28,107 DEBUG TRAIN Batch 12/5300 loss 14.818047 loss_att 23.017710 loss_ctc 19.009449 loss_rnnt 12.539839 hw_loss 0.148916 lr 0.00048700 rank 6
2023-02-20 22:54:28,205 DEBUG TRAIN Batch 12/5300 loss 19.127853 loss_att 25.432680 loss_ctc 27.265211 loss_rnnt 16.756868 hw_loss 0.046943 lr 0.00048701 rank 5
2023-02-20 22:54:28,305 DEBUG TRAIN Batch 12/5300 loss 14.091379 loss_att 18.955729 loss_ctc 23.344597 loss_rnnt 11.809483 hw_loss 0.141119 lr 0.00048701 rank 3
2023-02-20 22:54:28,462 DEBUG TRAIN Batch 12/5300 loss 16.925499 loss_att 19.257889 loss_ctc 19.239655 loss_rnnt 16.102390 hw_loss 0.090140 lr 0.00048698 rank 7
2023-02-20 22:54:28,464 DEBUG TRAIN Batch 12/5300 loss 12.418821 loss_att 18.666573 loss_ctc 12.268790 loss_rnnt 11.094803 hw_loss 0.177135 lr 0.00048706 rank 0
2023-02-20 22:54:28,502 DEBUG TRAIN Batch 12/5300 loss 15.821766 loss_att 17.091877 loss_ctc 20.443207 loss_rnnt 14.860321 hw_loss 0.171056 lr 0.00048704 rank 1
2023-02-20 23:02:13,210 DEBUG TRAIN Batch 12/5400 loss 23.977205 loss_att 30.400211 loss_ctc 30.647018 loss_rnnt 21.758062 hw_loss 0.084815 lr 0.00048683 rank 0
2023-02-20 23:02:13,226 DEBUG TRAIN Batch 12/5400 loss 9.146932 loss_att 13.675770 loss_ctc 12.831676 loss_rnnt 7.711878 hw_loss 0.071223 lr 0.00048675 rank 7
2023-02-20 23:02:13,348 DEBUG TRAIN Batch 12/5400 loss 16.106462 loss_att 19.112234 loss_ctc 20.848038 loss_rnnt 14.780822 hw_loss 0.173023 lr 0.00048674 rank 2
2023-02-20 23:02:13,370 DEBUG TRAIN Batch 12/5400 loss 28.513559 loss_att 32.080875 loss_ctc 36.806812 loss_rnnt 26.674036 hw_loss 0.038045 lr 0.00048681 rank 1
2023-02-20 23:02:13,390 DEBUG TRAIN Batch 12/5400 loss 26.832199 loss_att 25.330582 loss_ctc 29.290674 loss_rnnt 26.776276 hw_loss 0.053344 lr 0.00048678 rank 3
2023-02-20 23:02:13,427 DEBUG TRAIN Batch 12/5400 loss 6.402104 loss_att 10.940687 loss_ctc 6.872005 loss_rnnt 5.330851 hw_loss 0.189156 lr 0.00048676 rank 6
2023-02-20 23:02:13,530 DEBUG TRAIN Batch 12/5400 loss 11.589430 loss_att 16.676025 loss_ctc 15.715632 loss_rnnt 9.958599 hw_loss 0.118784 lr 0.00048679 rank 4
2023-02-20 23:02:13,602 DEBUG TRAIN Batch 12/5400 loss 10.518682 loss_att 14.175039 loss_ctc 13.599491 loss_rnnt 9.341650 hw_loss 0.065599 lr 0.00048678 rank 5
2023-02-20 23:10:08,694 DEBUG TRAIN Batch 12/5500 loss 8.270956 loss_att 11.002099 loss_ctc 8.814937 loss_rnnt 7.601029 hw_loss 0.095938 lr 0.00048655 rank 5
2023-02-20 23:10:08,729 DEBUG TRAIN Batch 12/5500 loss 18.662008 loss_att 21.384064 loss_ctc 20.579250 loss_rnnt 17.702921 hw_loss 0.298202 lr 0.00048655 rank 3
2023-02-20 23:10:08,789 DEBUG TRAIN Batch 12/5500 loss 23.030561 loss_att 30.182478 loss_ctc 28.314445 loss_rnnt 20.852425 hw_loss 0.081069 lr 0.00048651 rank 2
2023-02-20 23:10:08,826 DEBUG TRAIN Batch 12/5500 loss 14.460926 loss_att 20.323181 loss_ctc 17.228699 loss_rnnt 12.878808 hw_loss 0.076181 lr 0.00048653 rank 6
2023-02-20 23:10:08,830 DEBUG TRAIN Batch 12/5500 loss 17.188366 loss_att 24.354248 loss_ctc 18.588671 loss_rnnt 15.520952 hw_loss 0.089114 lr 0.00048660 rank 0
2023-02-20 23:10:08,916 DEBUG TRAIN Batch 12/5500 loss 20.290079 loss_att 24.225582 loss_ctc 23.512001 loss_rnnt 18.979439 hw_loss 0.176160 lr 0.00048658 rank 1
2023-02-20 23:10:08,932 DEBUG TRAIN Batch 12/5500 loss 16.153618 loss_att 21.928249 loss_ctc 23.157005 loss_rnnt 14.034996 hw_loss 0.056079 lr 0.00048656 rank 4
2023-02-20 23:10:09,189 DEBUG TRAIN Batch 12/5500 loss 17.487543 loss_att 19.170843 loss_ctc 21.003258 loss_rnnt 16.612144 hw_loss 0.131206 lr 0.00048652 rank 7
2023-02-20 23:17:47,103 DEBUG TRAIN Batch 12/5600 loss 9.285788 loss_att 14.179222 loss_ctc 12.068207 loss_rnnt 7.860584 hw_loss 0.141614 lr 0.00048632 rank 3
2023-02-20 23:17:47,143 DEBUG TRAIN Batch 12/5600 loss 12.021926 loss_att 14.583763 loss_ctc 14.115879 loss_rnnt 11.211339 hw_loss 0.035673 lr 0.00048637 rank 0
2023-02-20 23:17:47,175 DEBUG TRAIN Batch 12/5600 loss 21.430609 loss_att 27.844179 loss_ctc 22.917744 loss_rnnt 19.929863 hw_loss 0.037027 lr 0.00048632 rank 5
2023-02-20 23:17:47,235 DEBUG TRAIN Batch 12/5600 loss 22.978617 loss_att 24.006266 loss_ctc 26.233418 loss_rnnt 22.246449 hw_loss 0.173741 lr 0.00048629 rank 7
2023-02-20 23:17:47,267 DEBUG TRAIN Batch 12/5600 loss 8.501327 loss_att 11.687065 loss_ctc 9.496170 loss_rnnt 7.648218 hw_loss 0.156216 lr 0.00048630 rank 6
2023-02-20 23:17:47,238 DEBUG TRAIN Batch 12/5600 loss 19.104315 loss_att 20.271547 loss_ctc 21.393484 loss_rnnt 18.455252 hw_loss 0.206990 lr 0.00048628 rank 2
2023-02-20 23:17:47,350 DEBUG TRAIN Batch 12/5600 loss 14.368120 loss_att 16.333267 loss_ctc 16.671629 loss_rnnt 13.613767 hw_loss 0.101605 lr 0.00048633 rank 4
2023-02-20 23:17:47,840 DEBUG TRAIN Batch 12/5600 loss 14.436493 loss_att 15.669064 loss_ctc 19.600868 loss_rnnt 13.436623 hw_loss 0.121447 lr 0.00048635 rank 1
2023-02-20 23:25:38,550 DEBUG TRAIN Batch 12/5700 loss 13.034913 loss_att 16.038355 loss_ctc 14.808560 loss_rnnt 12.082780 hw_loss 0.215547 lr 0.00048614 rank 0
2023-02-20 23:25:38,624 DEBUG TRAIN Batch 12/5700 loss 20.080715 loss_att 21.779917 loss_ctc 26.238672 loss_rnnt 18.785606 hw_loss 0.251639 lr 0.00048609 rank 5
2023-02-20 23:25:38,711 DEBUG TRAIN Batch 12/5700 loss 16.960438 loss_att 20.952110 loss_ctc 23.307323 loss_rnnt 15.240718 hw_loss 0.140876 lr 0.00048606 rank 7
2023-02-20 23:25:38,745 DEBUG TRAIN Batch 12/5700 loss 18.882362 loss_att 22.541855 loss_ctc 25.277870 loss_rnnt 17.205114 hw_loss 0.173653 lr 0.00048610 rank 4
2023-02-20 23:25:38,813 DEBUG TRAIN Batch 12/5700 loss 13.021357 loss_att 15.553827 loss_ctc 15.406902 loss_rnnt 12.056712 hw_loss 0.262647 lr 0.00048612 rank 1
2023-02-20 23:25:38,831 DEBUG TRAIN Batch 12/5700 loss 25.920574 loss_att 29.343113 loss_ctc 32.952866 loss_rnnt 24.259426 hw_loss 0.073125 lr 0.00048607 rank 6
2023-02-20 23:25:38,859 DEBUG TRAIN Batch 12/5700 loss 8.410187 loss_att 10.112049 loss_ctc 11.264835 loss_rnnt 7.603870 hw_loss 0.159983 lr 0.00048605 rank 2
2023-02-20 23:25:39,001 DEBUG TRAIN Batch 12/5700 loss 16.304268 loss_att 17.303347 loss_ctc 17.777306 loss_rnnt 15.846886 hw_loss 0.114674 lr 0.00048609 rank 3
2023-02-20 23:33:21,252 DEBUG TRAIN Batch 12/5800 loss 11.258346 loss_att 17.897881 loss_ctc 15.554289 loss_rnnt 9.279098 hw_loss 0.147281 lr 0.00048586 rank 3
2023-02-20 23:33:21,321 DEBUG TRAIN Batch 12/5800 loss 11.392427 loss_att 12.195774 loss_ctc 16.441196 loss_rnnt 10.423585 hw_loss 0.253135 lr 0.00048584 rank 6
2023-02-20 23:33:21,335 DEBUG TRAIN Batch 12/5800 loss 15.054961 loss_att 20.032995 loss_ctc 20.111538 loss_rnnt 13.309573 hw_loss 0.141694 lr 0.00048589 rank 1
2023-02-20 23:33:21,373 DEBUG TRAIN Batch 12/5800 loss 6.216143 loss_att 10.050087 loss_ctc 5.946969 loss_rnnt 5.471184 hw_loss 0.026363 lr 0.00048587 rank 4
2023-02-20 23:33:21,422 DEBUG TRAIN Batch 12/5800 loss 18.023016 loss_att 23.795017 loss_ctc 27.137243 loss_rnnt 15.555719 hw_loss 0.183127 lr 0.00048591 rank 0
2023-02-20 23:33:21,424 DEBUG TRAIN Batch 12/5800 loss 11.211230 loss_att 10.478935 loss_ctc 12.573465 loss_rnnt 10.910550 hw_loss 0.497828 lr 0.00048586 rank 5
2023-02-20 23:33:21,431 DEBUG TRAIN Batch 12/5800 loss 7.349045 loss_att 11.752580 loss_ctc 10.752144 loss_rnnt 6.001127 hw_loss 0.025245 lr 0.00048583 rank 7
2023-02-20 23:33:21,480 DEBUG TRAIN Batch 12/5800 loss 14.988249 loss_att 20.246807 loss_ctc 18.203964 loss_rnnt 13.455133 hw_loss 0.098701 lr 0.00048582 rank 2
2023-02-20 23:41:21,881 DEBUG TRAIN Batch 12/5900 loss 16.349573 loss_att 25.825165 loss_ctc 21.820761 loss_rnnt 13.662234 hw_loss 0.117618 lr 0.00048565 rank 4
2023-02-20 23:41:21,888 DEBUG TRAIN Batch 12/5900 loss 16.906939 loss_att 22.814266 loss_ctc 20.631058 loss_rnnt 15.186941 hw_loss 0.078719 lr 0.00048559 rank 2
2023-02-20 23:41:21,904 DEBUG TRAIN Batch 12/5900 loss 3.654899 loss_att 9.977312 loss_ctc 4.617106 loss_rnnt 2.175181 hw_loss 0.163015 lr 0.00048563 rank 5
2023-02-20 23:41:21,906 DEBUG TRAIN Batch 12/5900 loss 15.652426 loss_att 19.358170 loss_ctc 19.220474 loss_rnnt 14.411434 hw_loss 0.045192 lr 0.00048560 rank 7
2023-02-20 23:41:21,999 DEBUG TRAIN Batch 12/5900 loss 21.991365 loss_att 34.312096 loss_ctc 24.558104 loss_rnnt 19.129999 hw_loss 0.103100 lr 0.00048566 rank 1
2023-02-20 23:41:22,022 DEBUG TRAIN Batch 12/5900 loss 8.556514 loss_att 9.126381 loss_ctc 9.773386 loss_rnnt 8.172686 hw_loss 0.201760 lr 0.00048562 rank 6
2023-02-20 23:41:22,196 DEBUG TRAIN Batch 12/5900 loss 20.727108 loss_att 22.037001 loss_ctc 25.783646 loss_rnnt 19.724094 hw_loss 0.125308 lr 0.00048563 rank 3
2023-02-20 23:41:22,321 DEBUG TRAIN Batch 12/5900 loss 25.956673 loss_att 23.327440 loss_ctc 28.062889 loss_rnnt 26.126320 hw_loss 0.141315 lr 0.00048568 rank 0
2023-02-20 23:49:19,235 DEBUG TRAIN Batch 12/6000 loss 9.705061 loss_att 14.563589 loss_ctc 16.850548 loss_rnnt 7.732721 hw_loss 0.089816 lr 0.00048540 rank 3
2023-02-20 23:49:19,402 DEBUG TRAIN Batch 12/6000 loss 12.234811 loss_att 17.340267 loss_ctc 20.462067 loss_rnnt 10.093791 hw_loss 0.043055 lr 0.00048542 rank 4
2023-02-20 23:49:19,603 DEBUG TRAIN Batch 12/6000 loss 14.955091 loss_att 17.457962 loss_ctc 18.171459 loss_rnnt 13.961055 hw_loss 0.121150 lr 0.00048545 rank 0
2023-02-20 23:49:19,623 DEBUG TRAIN Batch 12/6000 loss 24.869835 loss_att 28.051510 loss_ctc 26.272663 loss_rnnt 23.992243 hw_loss 0.101651 lr 0.00048537 rank 2
2023-02-20 23:49:19,644 DEBUG TRAIN Batch 12/6000 loss 37.178089 loss_att 39.096016 loss_ctc 40.313366 loss_rnnt 36.268131 hw_loss 0.203119 lr 0.00048537 rank 7
2023-02-20 23:49:19,658 DEBUG TRAIN Batch 12/6000 loss 19.654869 loss_att 19.538507 loss_ctc 24.810759 loss_rnnt 18.953905 hw_loss 0.068971 lr 0.00048540 rank 5
2023-02-20 23:49:19,725 DEBUG TRAIN Batch 12/6000 loss 19.885109 loss_att 25.381264 loss_ctc 26.169039 loss_rnnt 17.917633 hw_loss 0.056975 lr 0.00048543 rank 1
2023-02-20 23:49:19,759 DEBUG TRAIN Batch 12/6000 loss 18.774700 loss_att 25.150764 loss_ctc 19.732939 loss_rnnt 17.296131 hw_loss 0.141730 lr 0.00048539 rank 6
2023-02-20 23:57:17,345 DEBUG TRAIN Batch 12/6100 loss 20.445763 loss_att 25.091740 loss_ctc 20.848938 loss_rnnt 19.418213 hw_loss 0.083617 lr 0.00048519 rank 4
2023-02-20 23:57:17,354 DEBUG TRAIN Batch 12/6100 loss 8.386966 loss_att 12.471058 loss_ctc 12.784523 loss_rnnt 6.918335 hw_loss 0.122758 lr 0.00048516 rank 6
2023-02-20 23:57:17,363 DEBUG TRAIN Batch 12/6100 loss 8.143946 loss_att 12.928083 loss_ctc 10.455830 loss_rnnt 6.822611 hw_loss 0.105478 lr 0.00048521 rank 1
2023-02-20 23:57:17,464 DEBUG TRAIN Batch 12/6100 loss 11.515800 loss_att 13.111549 loss_ctc 12.128079 loss_rnnt 11.053286 hw_loss 0.115736 lr 0.00048514 rank 2
2023-02-20 23:57:17,569 DEBUG TRAIN Batch 12/6100 loss 4.981043 loss_att 10.120358 loss_ctc 5.295589 loss_rnnt 3.862144 hw_loss 0.092056 lr 0.00048517 rank 5
2023-02-20 23:57:17,617 DEBUG TRAIN Batch 12/6100 loss 10.856594 loss_att 15.869002 loss_ctc 13.176834 loss_rnnt 9.488818 hw_loss 0.104869 lr 0.00048514 rank 7
2023-02-20 23:57:17,702 DEBUG TRAIN Batch 12/6100 loss 14.490192 loss_att 17.708410 loss_ctc 21.235077 loss_rnnt 12.873008 hw_loss 0.139170 lr 0.00048518 rank 3
2023-02-20 23:57:17,772 DEBUG TRAIN Batch 12/6100 loss 17.745056 loss_att 20.997528 loss_ctc 22.815561 loss_rnnt 16.311974 hw_loss 0.199729 lr 0.00048522 rank 0
2023-02-21 00:05:10,975 DEBUG TRAIN Batch 12/6200 loss 8.513268 loss_att 10.746059 loss_ctc 10.614292 loss_rnnt 7.745534 hw_loss 0.076951 lr 0.00048500 rank 0
2023-02-21 00:05:11,007 DEBUG TRAIN Batch 12/6200 loss 39.085117 loss_att 43.181267 loss_ctc 45.889381 loss_rnnt 37.314323 hw_loss 0.083109 lr 0.00048496 rank 4
2023-02-21 00:05:11,017 DEBUG TRAIN Batch 12/6200 loss 14.490695 loss_att 15.196704 loss_ctc 18.840363 loss_rnnt 13.725206 hw_loss 0.083121 lr 0.00048493 rank 6
2023-02-21 00:05:11,064 DEBUG TRAIN Batch 12/6200 loss 17.770767 loss_att 23.045286 loss_ctc 21.671541 loss_rnnt 16.059200 hw_loss 0.256046 lr 0.00048495 rank 3
2023-02-21 00:05:11,039 DEBUG TRAIN Batch 12/6200 loss 16.392000 loss_att 20.038126 loss_ctc 23.745964 loss_rnnt 14.518909 hw_loss 0.306260 lr 0.00048492 rank 7
2023-02-21 00:05:11,165 DEBUG TRAIN Batch 12/6200 loss 15.185441 loss_att 19.537325 loss_ctc 17.095463 loss_rnnt 13.989626 hw_loss 0.132691 lr 0.00048494 rank 5
2023-02-21 00:05:11,179 DEBUG TRAIN Batch 12/6200 loss 5.429085 loss_att 9.760925 loss_ctc 7.948107 loss_rnnt 4.108603 hw_loss 0.221708 lr 0.00048491 rank 2
2023-02-21 00:05:11,471 DEBUG TRAIN Batch 12/6200 loss 14.968196 loss_att 15.832334 loss_ctc 18.823330 loss_rnnt 14.231718 hw_loss 0.093063 lr 0.00048498 rank 1
2023-02-21 00:12:56,015 DEBUG TRAIN Batch 12/6300 loss 10.081614 loss_att 11.002079 loss_ctc 10.664364 loss_rnnt 9.766930 hw_loss 0.099169 lr 0.00048468 rank 2
2023-02-21 00:12:56,182 DEBUG TRAIN Batch 12/6300 loss 29.373264 loss_att 31.160568 loss_ctc 33.357040 loss_rnnt 28.428761 hw_loss 0.104756 lr 0.00048475 rank 1
2023-02-21 00:12:56,186 DEBUG TRAIN Batch 12/6300 loss 9.924298 loss_att 14.510699 loss_ctc 12.292521 loss_rnnt 8.607891 hw_loss 0.156307 lr 0.00048473 rank 4
2023-02-21 00:12:56,219 DEBUG TRAIN Batch 12/6300 loss 13.590416 loss_att 14.966615 loss_ctc 15.101375 loss_rnnt 13.060843 hw_loss 0.099135 lr 0.00048469 rank 7
2023-02-21 00:12:56,234 DEBUG TRAIN Batch 12/6300 loss 22.316805 loss_att 23.549095 loss_ctc 24.858923 loss_rnnt 21.709295 hw_loss 0.041440 lr 0.00048477 rank 0
2023-02-21 00:12:56,290 DEBUG TRAIN Batch 12/6300 loss 5.146033 loss_att 7.736874 loss_ctc 5.175729 loss_rnnt 4.517605 hw_loss 0.199313 lr 0.00048472 rank 3
2023-02-21 00:12:56,282 DEBUG TRAIN Batch 12/6300 loss 10.057800 loss_att 13.328302 loss_ctc 11.384550 loss_rnnt 9.151809 hw_loss 0.140608 lr 0.00048472 rank 5
2023-02-21 00:12:56,392 DEBUG TRAIN Batch 12/6300 loss 24.215366 loss_att 26.327843 loss_ctc 27.246788 loss_rnnt 23.345610 hw_loss 0.080762 lr 0.00048470 rank 6
2023-02-21 00:20:59,310 DEBUG TRAIN Batch 12/6400 loss 15.898878 loss_att 17.830383 loss_ctc 19.602528 loss_rnnt 14.950216 hw_loss 0.128515 lr 0.00048452 rank 1
2023-02-21 00:20:59,323 DEBUG TRAIN Batch 12/6400 loss 21.500685 loss_att 22.831638 loss_ctc 22.514896 loss_rnnt 21.025242 hw_loss 0.138796 lr 0.00048449 rank 5
2023-02-21 00:20:59,361 DEBUG TRAIN Batch 12/6400 loss 8.766169 loss_att 15.615337 loss_ctc 10.556057 loss_rnnt 7.073070 hw_loss 0.158650 lr 0.00048454 rank 0
2023-02-21 00:20:59,314 DEBUG TRAIN Batch 12/6400 loss 18.689491 loss_att 25.588997 loss_ctc 31.624975 loss_rnnt 15.550245 hw_loss 0.064903 lr 0.00048450 rank 4
2023-02-21 00:20:59,536 DEBUG TRAIN Batch 12/6400 loss 16.366827 loss_att 24.372990 loss_ctc 13.829563 loss_rnnt 15.071335 hw_loss 0.061048 lr 0.00048446 rank 7
2023-02-21 00:20:59,609 DEBUG TRAIN Batch 12/6400 loss 6.295292 loss_att 12.827249 loss_ctc 7.198730 loss_rnnt 4.845141 hw_loss 0.043689 lr 0.00048445 rank 2
2023-02-21 00:20:59,665 DEBUG TRAIN Batch 12/6400 loss 23.009378 loss_att 22.811255 loss_ctc 29.259314 loss_rnnt 22.118574 hw_loss 0.182071 lr 0.00048447 rank 6
2023-02-21 00:20:59,800 DEBUG TRAIN Batch 12/6400 loss 7.011178 loss_att 12.173717 loss_ctc 7.701418 loss_rnnt 5.807094 hw_loss 0.149146 lr 0.00048449 rank 3
2023-02-21 00:28:47,180 DEBUG TRAIN Batch 12/6500 loss 15.574521 loss_att 22.106171 loss_ctc 23.700676 loss_rnnt 13.148394 hw_loss 0.068081 lr 0.00048428 rank 4
2023-02-21 00:28:47,291 DEBUG TRAIN Batch 12/6500 loss 8.826997 loss_att 9.089997 loss_ctc 10.753983 loss_rnnt 8.380581 hw_loss 0.256658 lr 0.00048426 rank 5
2023-02-21 00:28:47,315 DEBUG TRAIN Batch 12/6500 loss 17.063551 loss_att 21.403072 loss_ctc 21.298836 loss_rnnt 15.547855 hw_loss 0.155788 lr 0.00048431 rank 0
2023-02-21 00:28:47,360 DEBUG TRAIN Batch 12/6500 loss 7.171442 loss_att 14.163173 loss_ctc 9.920863 loss_rnnt 5.392267 hw_loss 0.026699 lr 0.00048425 rank 6
2023-02-21 00:28:47,413 DEBUG TRAIN Batch 12/6500 loss 17.916645 loss_att 23.632626 loss_ctc 23.708778 loss_rnnt 15.986701 hw_loss 0.027119 lr 0.00048427 rank 3
2023-02-21 00:28:47,377 DEBUG TRAIN Batch 12/6500 loss 15.758961 loss_att 19.876936 loss_ctc 19.326046 loss_rnnt 14.415033 hw_loss 0.083850 lr 0.00048429 rank 1
2023-02-21 00:28:47,512 DEBUG TRAIN Batch 12/6500 loss 13.822743 loss_att 16.754389 loss_ctc 15.282597 loss_rnnt 13.027605 hw_loss 0.026554 lr 0.00048423 rank 7
2023-02-21 00:28:47,670 DEBUG TRAIN Batch 12/6500 loss 15.022183 loss_att 19.800501 loss_ctc 19.653719 loss_rnnt 13.377475 hw_loss 0.134075 lr 0.00048423 rank 2
2023-02-21 00:36:30,188 DEBUG TRAIN Batch 12/6600 loss 12.105859 loss_att 14.631167 loss_ctc 13.696307 loss_rnnt 11.247043 hw_loss 0.265677 lr 0.00048402 rank 6
2023-02-21 00:36:30,192 DEBUG TRAIN Batch 12/6600 loss 20.412308 loss_att 21.115652 loss_ctc 21.371527 loss_rnnt 20.064476 hw_loss 0.148630 lr 0.00048401 rank 7
2023-02-21 00:36:30,281 DEBUG TRAIN Batch 12/6600 loss 16.934397 loss_att 23.200832 loss_ctc 18.549990 loss_rnnt 15.356128 hw_loss 0.205448 lr 0.00048404 rank 3
2023-02-21 00:36:30,361 DEBUG TRAIN Batch 12/6600 loss 17.192978 loss_att 21.120945 loss_ctc 20.912926 loss_rnnt 15.865246 hw_loss 0.086524 lr 0.00048409 rank 0
2023-02-21 00:36:30,395 DEBUG TRAIN Batch 12/6600 loss 14.570349 loss_att 18.246433 loss_ctc 16.834797 loss_rnnt 13.461062 hw_loss 0.135270 lr 0.00048400 rank 2
2023-02-21 00:36:30,403 DEBUG TRAIN Batch 12/6600 loss 13.246000 loss_att 16.711943 loss_ctc 18.945938 loss_rnnt 11.780559 hw_loss 0.022988 lr 0.00048403 rank 5
2023-02-21 00:36:30,345 DEBUG TRAIN Batch 12/6600 loss 13.524153 loss_att 19.611895 loss_ctc 17.375078 loss_rnnt 11.771904 hw_loss 0.039830 lr 0.00048405 rank 4
2023-02-21 00:36:30,606 DEBUG TRAIN Batch 12/6600 loss 12.535639 loss_att 12.610386 loss_ctc 16.577343 loss_rnnt 11.969963 hw_loss 0.022188 lr 0.00048407 rank 1
2023-02-21 00:44:15,752 DEBUG TRAIN Batch 12/6700 loss 27.121691 loss_att 29.269270 loss_ctc 31.636662 loss_rnnt 26.038357 hw_loss 0.097168 lr 0.00048381 rank 5
2023-02-21 00:44:15,875 DEBUG TRAIN Batch 12/6700 loss 20.018820 loss_att 24.303181 loss_ctc 26.827805 loss_rnnt 18.165428 hw_loss 0.166227 lr 0.00048381 rank 3
2023-02-21 00:44:15,922 DEBUG TRAIN Batch 12/6700 loss 11.771937 loss_att 18.039940 loss_ctc 16.264835 loss_rnnt 9.898412 hw_loss 0.039135 lr 0.00048378 rank 7
2023-02-21 00:44:16,200 DEBUG TRAIN Batch 12/6700 loss 11.247143 loss_att 13.968358 loss_ctc 13.997944 loss_rnnt 10.301815 hw_loss 0.064331 lr 0.00048379 rank 6
2023-02-21 00:44:16,230 DEBUG TRAIN Batch 12/6700 loss 13.751069 loss_att 18.327618 loss_ctc 14.945824 loss_rnnt 12.646095 hw_loss 0.056932 lr 0.00048386 rank 0
2023-02-21 00:44:16,253 DEBUG TRAIN Batch 12/6700 loss 16.844156 loss_att 22.185577 loss_ctc 25.190903 loss_rnnt 14.605032 hw_loss 0.108636 lr 0.00048382 rank 4
2023-02-21 00:44:16,277 DEBUG TRAIN Batch 12/6700 loss 13.674809 loss_att 14.845587 loss_ctc 16.685158 loss_rnnt 12.898777 hw_loss 0.263431 lr 0.00048377 rank 2
2023-02-21 00:44:16,390 DEBUG TRAIN Batch 12/6700 loss 15.994551 loss_att 20.275007 loss_ctc 20.599392 loss_rnnt 14.494387 hw_loss 0.056425 lr 0.00048384 rank 1
2023-02-21 00:52:21,103 DEBUG TRAIN Batch 12/6800 loss 21.227840 loss_att 22.103039 loss_ctc 34.341782 loss_rnnt 19.215591 hw_loss 0.166281 lr 0.00048360 rank 4
2023-02-21 00:52:21,196 DEBUG TRAIN Batch 12/6800 loss 10.631898 loss_att 15.036869 loss_ctc 15.529381 loss_rnnt 9.058679 hw_loss 0.073550 lr 0.00048363 rank 0
2023-02-21 00:52:21,262 DEBUG TRAIN Batch 12/6800 loss 11.263480 loss_att 16.354637 loss_ctc 13.246178 loss_rnnt 9.942996 hw_loss 0.071050 lr 0.00048359 rank 3
2023-02-21 00:52:21,277 DEBUG TRAIN Batch 12/6800 loss 18.646275 loss_att 23.730671 loss_ctc 25.369457 loss_rnnt 16.674065 hw_loss 0.110451 lr 0.00048358 rank 5
2023-02-21 00:52:21,353 DEBUG TRAIN Batch 12/6800 loss 14.397604 loss_att 18.608246 loss_ctc 14.708525 loss_rnnt 13.449114 hw_loss 0.121696 lr 0.00048361 rank 1
2023-02-21 00:52:21,442 DEBUG TRAIN Batch 12/6800 loss 23.003567 loss_att 24.406647 loss_ctc 28.645658 loss_rnnt 21.921942 hw_loss 0.091366 lr 0.00048355 rank 7
2023-02-21 00:52:21,418 DEBUG TRAIN Batch 12/6800 loss 9.762519 loss_att 15.419490 loss_ctc 14.904470 loss_rnnt 7.885836 hw_loss 0.111929 lr 0.00048357 rank 6
2023-02-21 00:52:21,640 DEBUG TRAIN Batch 12/6800 loss 7.358969 loss_att 11.488915 loss_ctc 9.214130 loss_rnnt 6.179368 hw_loss 0.199233 lr 0.00048355 rank 2
2023-02-21 00:59:56,912 DEBUG TRAIN Batch 12/6900 loss 10.895190 loss_att 14.747519 loss_ctc 13.380230 loss_rnnt 9.774305 hw_loss 0.035777 lr 0.00048339 rank 1
2023-02-21 00:59:56,990 DEBUG TRAIN Batch 12/6900 loss 18.231121 loss_att 21.879177 loss_ctc 22.382484 loss_rnnt 16.887926 hw_loss 0.112625 lr 0.00048335 rank 5
2023-02-21 00:59:57,014 DEBUG TRAIN Batch 12/6900 loss 22.216646 loss_att 27.599964 loss_ctc 25.265211 loss_rnnt 20.706623 hw_loss 0.050409 lr 0.00048337 rank 4
2023-02-21 00:59:57,049 DEBUG TRAIN Batch 12/6900 loss 20.689137 loss_att 22.292088 loss_ctc 22.447079 loss_rnnt 20.067816 hw_loss 0.124389 lr 0.00048332 rank 2
2023-02-21 00:59:57,108 DEBUG TRAIN Batch 12/6900 loss 9.077220 loss_att 12.388306 loss_ctc 10.088363 loss_rnnt 8.254193 hw_loss 0.048730 lr 0.00048333 rank 7
2023-02-21 00:59:57,165 DEBUG TRAIN Batch 12/6900 loss 16.947697 loss_att 19.267471 loss_ctc 21.524181 loss_rnnt 15.786558 hw_loss 0.163099 lr 0.00048336 rank 3
2023-02-21 00:59:57,325 DEBUG TRAIN Batch 12/6900 loss 27.463854 loss_att 27.025743 loss_ctc 28.065063 loss_rnnt 27.395855 hw_loss 0.141486 lr 0.00048341 rank 0
2023-02-21 00:59:57,406 DEBUG TRAIN Batch 12/6900 loss 18.304392 loss_att 21.846809 loss_ctc 22.924530 loss_rnnt 16.960560 hw_loss 0.036243 lr 0.00048334 rank 6
2023-02-21 01:07:53,062 DEBUG TRAIN Batch 12/7000 loss 10.716689 loss_att 14.257732 loss_ctc 10.840156 loss_rnnt 9.823923 hw_loss 0.315177 lr 0.00048318 rank 0
2023-02-21 01:07:53,159 DEBUG TRAIN Batch 12/7000 loss 16.823830 loss_att 23.337534 loss_ctc 24.368055 loss_rnnt 14.496292 hw_loss 0.035439 lr 0.00048314 rank 4
2023-02-21 01:07:53,316 DEBUG TRAIN Batch 12/7000 loss 13.523339 loss_att 22.537148 loss_ctc 16.536091 loss_rnnt 11.278702 hw_loss 0.075327 lr 0.00048310 rank 2
2023-02-21 01:07:53,383 DEBUG TRAIN Batch 12/7000 loss 23.501968 loss_att 21.880199 loss_ctc 28.727341 loss_rnnt 23.050123 hw_loss 0.149031 lr 0.00048316 rank 1
2023-02-21 01:07:53,395 DEBUG TRAIN Batch 12/7000 loss 14.959420 loss_att 15.757267 loss_ctc 19.181055 loss_rnnt 14.146344 hw_loss 0.169917 lr 0.00048310 rank 7
2023-02-21 01:07:53,370 DEBUG TRAIN Batch 12/7000 loss 17.630207 loss_att 18.067707 loss_ctc 19.813866 loss_rnnt 17.173601 hw_loss 0.146159 lr 0.00048313 rank 5
2023-02-21 01:07:53,408 DEBUG TRAIN Batch 12/7000 loss 6.343720 loss_att 10.029114 loss_ctc 7.357030 loss_rnnt 5.374944 hw_loss 0.181104 lr 0.00048312 rank 6
2023-02-21 01:07:53,417 DEBUG TRAIN Batch 12/7000 loss 21.285713 loss_att 22.792078 loss_ctc 26.663509 loss_rnnt 20.196095 hw_loss 0.133698 lr 0.00048313 rank 3
2023-02-21 01:16:09,307 DEBUG TRAIN Batch 12/7100 loss 14.720793 loss_att 20.911255 loss_ctc 20.075634 loss_rnnt 12.755341 hw_loss 0.025088 lr 0.00048289 rank 6
2023-02-21 01:16:09,326 DEBUG TRAIN Batch 12/7100 loss 18.798643 loss_att 20.049891 loss_ctc 23.270443 loss_rnnt 17.845720 hw_loss 0.199560 lr 0.00048290 rank 5
2023-02-21 01:16:09,361 DEBUG TRAIN Batch 12/7100 loss 27.672596 loss_att 32.092236 loss_ctc 38.677025 loss_rnnt 25.186172 hw_loss 0.253571 lr 0.00048288 rank 7
2023-02-21 01:16:09,371 DEBUG TRAIN Batch 12/7100 loss 23.492987 loss_att 28.055674 loss_ctc 36.161160 loss_rnnt 20.837200 hw_loss 0.101546 lr 0.00048296 rank 0
2023-02-21 01:16:09,429 DEBUG TRAIN Batch 12/7100 loss 18.831581 loss_att 17.017681 loss_ctc 25.839001 loss_rnnt 18.246136 hw_loss 0.026069 lr 0.00048292 rank 4
2023-02-21 01:16:09,527 DEBUG TRAIN Batch 12/7100 loss 10.989610 loss_att 13.165537 loss_ctc 11.251570 loss_rnnt 10.467662 hw_loss 0.097190 lr 0.00048291 rank 3
2023-02-21 01:16:09,586 DEBUG TRAIN Batch 12/7100 loss 7.866152 loss_att 10.714609 loss_ctc 6.884458 loss_rnnt 7.413728 hw_loss 0.025548 lr 0.00048287 rank 2
2023-02-21 01:16:09,741 DEBUG TRAIN Batch 12/7100 loss 11.226868 loss_att 17.755768 loss_ctc 12.784384 loss_rnnt 9.619736 hw_loss 0.175655 lr 0.00048294 rank 1
2023-02-21 01:23:52,269 DEBUG TRAIN Batch 12/7200 loss 8.166615 loss_att 12.973738 loss_ctc 14.816379 loss_rnnt 6.267100 hw_loss 0.096476 lr 0.00048273 rank 0
2023-02-21 01:23:52,434 DEBUG TRAIN Batch 12/7200 loss 8.620061 loss_att 14.320000 loss_ctc 11.042807 loss_rnnt 7.042603 hw_loss 0.214571 lr 0.00048268 rank 5
2023-02-21 01:23:52,570 DEBUG TRAIN Batch 12/7200 loss 15.602135 loss_att 21.976114 loss_ctc 24.019375 loss_rnnt 13.179600 hw_loss 0.047700 lr 0.00048265 rank 7
2023-02-21 01:23:52,503 DEBUG TRAIN Batch 12/7200 loss 20.202183 loss_att 22.024096 loss_ctc 26.307205 loss_rnnt 18.962198 hw_loss 0.115498 lr 0.00048269 rank 4
2023-02-21 01:23:52,558 DEBUG TRAIN Batch 12/7200 loss 16.049620 loss_att 25.848312 loss_ctc 15.962901 loss_rnnt 14.067442 hw_loss 0.063752 lr 0.00048267 rank 6
2023-02-21 01:23:52,517 DEBUG TRAIN Batch 12/7200 loss 11.704160 loss_att 15.585512 loss_ctc 16.582764 loss_rnnt 10.244072 hw_loss 0.062507 lr 0.00048268 rank 3
2023-02-21 01:23:52,725 DEBUG TRAIN Batch 12/7200 loss 17.642332 loss_att 19.793987 loss_ctc 24.924881 loss_rnnt 16.135633 hw_loss 0.197555 lr 0.00048271 rank 1
2023-02-21 01:23:52,846 DEBUG TRAIN Batch 12/7200 loss 9.857500 loss_att 15.605745 loss_ctc 14.894168 loss_rnnt 8.001856 hw_loss 0.064574 lr 0.00048264 rank 2
2023-02-21 01:31:43,600 DEBUG TRAIN Batch 12/7300 loss 22.035185 loss_att 22.292067 loss_ctc 27.502872 loss_rnnt 21.189558 hw_loss 0.122294 lr 0.00048249 rank 1
2023-02-21 01:31:43,715 DEBUG TRAIN Batch 12/7300 loss 14.438272 loss_att 17.864258 loss_ctc 18.899654 loss_rnnt 13.144723 hw_loss 0.025316 lr 0.00048246 rank 3
2023-02-21 01:31:43,739 DEBUG TRAIN Batch 12/7300 loss 12.651864 loss_att 13.762141 loss_ctc 18.638845 loss_rnnt 11.504995 hw_loss 0.237278 lr 0.00048247 rank 4
2023-02-21 01:31:43,697 DEBUG TRAIN Batch 12/7300 loss 11.554005 loss_att 18.795372 loss_ctc 13.818192 loss_rnnt 9.790306 hw_loss 0.025377 lr 0.00048242 rank 2
2023-02-21 01:31:43,778 DEBUG TRAIN Batch 12/7300 loss 23.632351 loss_att 24.906347 loss_ctc 23.438942 loss_rnnt 23.389799 hw_loss 0.025386 lr 0.00048245 rank 5
2023-02-21 01:31:43,789 DEBUG TRAIN Batch 12/7300 loss 25.820866 loss_att 29.940853 loss_ctc 26.830418 loss_rnnt 24.849243 hw_loss 0.024404 lr 0.00048251 rank 0
2023-02-21 01:31:43,850 DEBUG TRAIN Batch 12/7300 loss 45.979733 loss_att 50.285576 loss_ctc 59.761711 loss_rnnt 43.238815 hw_loss 0.079042 lr 0.00048244 rank 6
2023-02-21 01:31:43,870 DEBUG TRAIN Batch 12/7300 loss 14.992421 loss_att 19.037661 loss_ctc 18.618088 loss_rnnt 13.609156 hw_loss 0.170241 lr 0.00048243 rank 7
2023-02-21 01:39:45,667 DEBUG TRAIN Batch 12/7400 loss 10.147183 loss_att 14.707470 loss_ctc 12.588062 loss_rnnt 8.817301 hw_loss 0.173203 lr 0.00048220 rank 7
2023-02-21 01:39:45,667 DEBUG TRAIN Batch 12/7400 loss 19.083126 loss_att 23.394154 loss_ctc 25.233643 loss_rnnt 17.267893 hw_loss 0.249294 lr 0.00048223 rank 5
2023-02-21 01:39:45,908 DEBUG TRAIN Batch 12/7400 loss 18.727509 loss_att 22.799564 loss_ctc 25.350636 loss_rnnt 16.995373 hw_loss 0.064952 lr 0.00048223 rank 3
2023-02-21 01:39:45,925 DEBUG TRAIN Batch 12/7400 loss 12.608233 loss_att 20.588575 loss_ctc 17.764656 loss_rnnt 10.264440 hw_loss 0.112879 lr 0.00048222 rank 6
2023-02-21 01:39:45,932 DEBUG TRAIN Batch 12/7400 loss 13.248837 loss_att 16.920179 loss_ctc 17.525146 loss_rnnt 11.887056 hw_loss 0.107505 lr 0.00048228 rank 0
2023-02-21 01:39:45,968 DEBUG TRAIN Batch 12/7400 loss 12.112861 loss_att 15.054208 loss_ctc 16.648109 loss_rnnt 10.751163 hw_loss 0.316365 lr 0.00048225 rank 4
2023-02-21 01:39:46,075 DEBUG TRAIN Batch 12/7400 loss 9.123494 loss_att 11.257742 loss_ctc 9.292500 loss_rnnt 8.593696 hw_loss 0.150777 lr 0.00048226 rank 1
2023-02-21 01:39:46,080 DEBUG TRAIN Batch 12/7400 loss 4.672321 loss_att 7.292989 loss_ctc 3.089540 loss_rnnt 4.297890 hw_loss 0.115004 lr 0.00048220 rank 2
2023-02-21 01:47:47,385 DEBUG TRAIN Batch 12/7500 loss 14.467936 loss_att 18.733072 loss_ctc 15.412111 loss_rnnt 13.308310 hw_loss 0.338830 lr 0.00048201 rank 5
2023-02-21 01:47:47,399 DEBUG TRAIN Batch 12/7500 loss 6.259682 loss_att 8.396024 loss_ctc 8.801028 loss_rnnt 5.446008 hw_loss 0.089175 lr 0.00048202 rank 4
2023-02-21 01:47:47,411 DEBUG TRAIN Batch 12/7500 loss 17.254013 loss_att 24.285006 loss_ctc 23.560795 loss_rnnt 14.964946 hw_loss 0.078682 lr 0.00048204 rank 1
2023-02-21 01:47:47,456 DEBUG TRAIN Batch 12/7500 loss 9.664151 loss_att 12.714568 loss_ctc 7.360283 loss_rnnt 9.341545 hw_loss 0.036948 lr 0.00048201 rank 3
2023-02-21 01:47:47,546 DEBUG TRAIN Batch 12/7500 loss 22.379166 loss_att 26.161819 loss_ctc 29.149090 loss_rnnt 20.700535 hw_loss 0.036458 lr 0.00048199 rank 6
2023-02-21 01:47:47,829 DEBUG TRAIN Batch 12/7500 loss 13.331946 loss_att 18.086432 loss_ctc 14.246076 loss_rnnt 12.158963 hw_loss 0.187879 lr 0.00048206 rank 0
2023-02-21 01:47:47,806 DEBUG TRAIN Batch 12/7500 loss 16.321981 loss_att 17.746204 loss_ctc 20.884695 loss_rnnt 15.323033 hw_loss 0.198266 lr 0.00048198 rank 7
2023-02-21 01:47:47,930 DEBUG TRAIN Batch 12/7500 loss 9.003686 loss_att 12.991799 loss_ctc 12.118720 loss_rnnt 7.676859 hw_loss 0.213498 lr 0.00048197 rank 2
2023-02-21 01:55:37,203 DEBUG TRAIN Batch 12/7600 loss 19.228945 loss_att 20.371698 loss_ctc 24.834057 loss_rnnt 18.189388 hw_loss 0.119359 lr 0.00048178 rank 5
2023-02-21 01:55:37,248 DEBUG TRAIN Batch 12/7600 loss 16.212303 loss_att 16.307138 loss_ctc 18.895555 loss_rnnt 15.817569 hw_loss 0.033751 lr 0.00048180 rank 4
2023-02-21 01:55:37,330 DEBUG TRAIN Batch 12/7600 loss 15.104609 loss_att 14.169416 loss_ctc 14.193662 loss_rnnt 15.328961 hw_loss 0.157774 lr 0.00048175 rank 7
2023-02-21 01:55:37,474 DEBUG TRAIN Batch 12/7600 loss 18.789761 loss_att 20.506922 loss_ctc 29.261839 loss_rnnt 16.945562 hw_loss 0.195916 lr 0.00048183 rank 0
2023-02-21 01:55:37,479 DEBUG TRAIN Batch 12/7600 loss 12.724746 loss_att 15.963539 loss_ctc 16.829275 loss_rnnt 11.460443 hw_loss 0.129889 lr 0.00048181 rank 1
2023-02-21 01:55:37,499 DEBUG TRAIN Batch 12/7600 loss 14.019562 loss_att 15.844717 loss_ctc 17.742493 loss_rnnt 13.104306 hw_loss 0.100936 lr 0.00048179 rank 3
2023-02-21 01:55:37,592 DEBUG TRAIN Batch 12/7600 loss 17.115614 loss_att 16.579998 loss_ctc 18.961475 loss_rnnt 16.795235 hw_loss 0.340099 lr 0.00048175 rank 2
2023-02-21 01:55:37,678 DEBUG TRAIN Batch 12/7600 loss 22.950180 loss_att 21.018318 loss_ctc 27.602278 loss_rnnt 22.658493 hw_loss 0.108339 lr 0.00048177 rank 6
2023-02-21 02:03:27,699 DEBUG TRAIN Batch 12/7700 loss 5.331516 loss_att 11.333887 loss_ctc 6.342097 loss_rnnt 3.981945 hw_loss 0.026910 lr 0.00048154 rank 6
2023-02-21 02:03:27,761 DEBUG TRAIN Batch 12/7700 loss 9.515400 loss_att 16.350361 loss_ctc 13.228518 loss_rnnt 7.611028 hw_loss 0.079307 lr 0.00048152 rank 2
2023-02-21 02:03:27,840 DEBUG TRAIN Batch 12/7700 loss 16.401588 loss_att 19.672831 loss_ctc 18.368801 loss_rnnt 15.461177 hw_loss 0.044757 lr 0.00048161 rank 0
2023-02-21 02:03:27,874 DEBUG TRAIN Batch 12/7700 loss 13.195843 loss_att 19.511486 loss_ctc 17.571507 loss_rnnt 11.294940 hw_loss 0.101911 lr 0.00048153 rank 7
2023-02-21 02:03:27,909 DEBUG TRAIN Batch 12/7700 loss 20.909639 loss_att 23.412544 loss_ctc 25.514952 loss_rnnt 19.677969 hw_loss 0.219467 lr 0.00048156 rank 3
2023-02-21 02:03:28,021 DEBUG TRAIN Batch 12/7700 loss 15.867291 loss_att 18.185837 loss_ctc 20.999485 loss_rnnt 14.693683 hw_loss 0.048013 lr 0.00048157 rank 4
2023-02-21 02:03:28,075 DEBUG TRAIN Batch 12/7700 loss 14.052182 loss_att 20.308094 loss_ctc 15.230537 loss_rnnt 12.580284 hw_loss 0.119253 lr 0.00048159 rank 1
2023-02-21 02:03:28,365 DEBUG TRAIN Batch 12/7700 loss 22.085035 loss_att 27.084232 loss_ctc 29.121901 loss_rnnt 20.073021 hw_loss 0.138610 lr 0.00048156 rank 5
2023-02-21 02:11:35,516 DEBUG TRAIN Batch 12/7800 loss 13.303394 loss_att 17.648500 loss_ctc 15.244887 loss_rnnt 12.077713 hw_loss 0.183365 lr 0.00048133 rank 5
2023-02-21 02:11:35,538 DEBUG TRAIN Batch 12/7800 loss 10.966901 loss_att 14.451457 loss_ctc 14.004801 loss_rnnt 9.823008 hw_loss 0.078617 lr 0.00048132 rank 6
2023-02-21 02:11:35,559 DEBUG TRAIN Batch 12/7800 loss 12.612914 loss_att 21.455158 loss_ctc 13.587852 loss_rnnt 10.663193 hw_loss 0.096149 lr 0.00048130 rank 2
2023-02-21 02:11:35,592 DEBUG TRAIN Batch 12/7800 loss 17.305666 loss_att 21.740522 loss_ctc 23.037666 loss_rnnt 15.605617 hw_loss 0.091523 lr 0.00048134 rank 3
2023-02-21 02:11:35,745 DEBUG TRAIN Batch 12/7800 loss 8.714516 loss_att 15.300931 loss_ctc 12.220035 loss_rnnt 6.914078 hw_loss 0.029535 lr 0.00048139 rank 0
2023-02-21 02:11:35,820 DEBUG TRAIN Batch 12/7800 loss 9.018995 loss_att 13.547703 loss_ctc 11.653912 loss_rnnt 7.692388 hw_loss 0.130396 lr 0.00048135 rank 4
2023-02-21 02:11:35,891 DEBUG TRAIN Batch 12/7800 loss 11.325878 loss_att 13.011608 loss_ctc 12.351222 loss_rnnt 10.790905 hw_loss 0.114592 lr 0.00048137 rank 1
2023-02-21 02:11:36,033 DEBUG TRAIN Batch 12/7800 loss 12.430170 loss_att 16.753700 loss_ctc 17.373692 loss_rnnt 10.794035 hw_loss 0.210549 lr 0.00048131 rank 7
2023-02-21 02:19:38,556 DEBUG TRAIN Batch 12/7900 loss 11.796050 loss_att 15.201027 loss_ctc 14.278351 loss_rnnt 10.736935 hw_loss 0.088398 lr 0.00048111 rank 5
2023-02-21 02:19:38,575 DEBUG TRAIN Batch 12/7900 loss 15.464662 loss_att 18.538559 loss_ctc 16.664621 loss_rnnt 14.633522 hw_loss 0.105685 lr 0.00048113 rank 4
2023-02-21 02:19:38,607 DEBUG TRAIN Batch 12/7900 loss 19.872974 loss_att 26.251961 loss_ctc 22.844997 loss_rnnt 18.138042 hw_loss 0.117870 lr 0.00048116 rank 0
2023-02-21 02:19:38,654 DEBUG TRAIN Batch 12/7900 loss 17.802044 loss_att 19.554789 loss_ctc 21.753857 loss_rnnt 16.911224 hw_loss 0.025051 lr 0.00048108 rank 2
2023-02-21 02:19:38,681 DEBUG TRAIN Batch 12/7900 loss 5.500876 loss_att 8.340660 loss_ctc 7.779210 loss_rnnt 4.506847 hw_loss 0.229302 lr 0.00048112 rank 3
2023-02-21 02:19:38,756 DEBUG TRAIN Batch 12/7900 loss 14.865463 loss_att 14.745924 loss_ctc 14.939776 loss_rnnt 14.829891 hw_loss 0.092944 lr 0.00048109 rank 7
2023-02-21 02:19:38,874 DEBUG TRAIN Batch 12/7900 loss 16.766975 loss_att 20.463221 loss_ctc 21.397459 loss_rnnt 15.345091 hw_loss 0.122321 lr 0.00048115 rank 1
2023-02-21 02:19:38,899 DEBUG TRAIN Batch 12/7900 loss 38.600033 loss_att 46.101109 loss_ctc 49.713131 loss_rnnt 35.543404 hw_loss 0.139999 lr 0.00048110 rank 6
2023-02-21 02:27:56,796 DEBUG TRAIN Batch 12/8000 loss 16.138212 loss_att 16.786242 loss_ctc 25.317261 loss_rnnt 14.717961 hw_loss 0.125198 lr 0.00048086 rank 2
2023-02-21 02:27:56,980 DEBUG TRAIN Batch 12/8000 loss 8.102287 loss_att 13.165384 loss_ctc 10.930365 loss_rnnt 6.696757 hw_loss 0.029688 lr 0.00048089 rank 5
2023-02-21 02:27:56,989 DEBUG TRAIN Batch 12/8000 loss 15.620135 loss_att 22.893751 loss_ctc 20.697781 loss_rnnt 13.427429 hw_loss 0.114305 lr 0.00048086 rank 7
2023-02-21 02:27:57,107 DEBUG TRAIN Batch 12/8000 loss 14.618663 loss_att 18.150322 loss_ctc 20.344151 loss_rnnt 13.075932 hw_loss 0.136877 lr 0.00048090 rank 4
2023-02-21 02:27:57,143 DEBUG TRAIN Batch 12/8000 loss 5.937671 loss_att 11.697941 loss_ctc 6.407589 loss_rnnt 4.669552 hw_loss 0.100142 lr 0.00048088 rank 6
2023-02-21 02:27:57,114 DEBUG TRAIN Batch 12/8000 loss 10.223936 loss_att 15.213091 loss_ctc 15.097786 loss_rnnt 8.505940 hw_loss 0.131846 lr 0.00048092 rank 1
2023-02-21 02:27:57,216 DEBUG TRAIN Batch 12/8000 loss 19.841848 loss_att 21.239365 loss_ctc 25.889698 loss_rnnt 18.732578 hw_loss 0.043848 lr 0.00048094 rank 0
2023-02-21 02:27:57,483 DEBUG TRAIN Batch 12/8000 loss 16.466446 loss_att 22.916229 loss_ctc 18.934727 loss_rnnt 14.760305 hw_loss 0.163276 lr 0.00048089 rank 3
2023-02-21 02:36:31,832 DEBUG TRAIN Batch 12/8100 loss 16.689875 loss_att 22.352209 loss_ctc 27.143572 loss_rnnt 14.093851 hw_loss 0.130744 lr 0.00048064 rank 7
2023-02-21 02:36:31,870 DEBUG TRAIN Batch 12/8100 loss 11.053009 loss_att 13.293076 loss_ctc 15.642256 loss_rnnt 9.955305 hw_loss 0.070858 lr 0.00048070 rank 1
2023-02-21 02:36:32,038 DEBUG TRAIN Batch 12/8100 loss 6.651371 loss_att 11.197860 loss_ctc 7.717892 loss_rnnt 5.521063 hw_loss 0.147766 lr 0.00048067 rank 3
2023-02-21 02:36:32,105 DEBUG TRAIN Batch 12/8100 loss 11.518898 loss_att 14.796970 loss_ctc 16.485958 loss_rnnt 10.185523 hw_loss 0.029038 lr 0.00048068 rank 4
2023-02-21 02:36:32,132 DEBUG TRAIN Batch 12/8100 loss 15.546628 loss_att 19.402237 loss_ctc 20.127415 loss_rnnt 14.118032 hw_loss 0.087568 lr 0.00048067 rank 5
2023-02-21 02:36:32,134 DEBUG TRAIN Batch 12/8100 loss 16.940292 loss_att 19.208565 loss_ctc 20.478588 loss_rnnt 15.947162 hw_loss 0.126942 lr 0.00048072 rank 0
2023-02-21 02:36:32,249 DEBUG TRAIN Batch 12/8100 loss 18.173262 loss_att 22.404844 loss_ctc 22.777760 loss_rnnt 16.697811 hw_loss 0.028501 lr 0.00048063 rank 2
2023-02-21 02:36:32,323 DEBUG TRAIN Batch 12/8100 loss 21.783102 loss_att 26.681358 loss_ctc 27.796684 loss_rnnt 19.986687 hw_loss 0.028035 lr 0.00048065 rank 6
2023-02-21 02:45:05,837 DEBUG TRAIN Batch 12/8200 loss 11.805436 loss_att 15.073427 loss_ctc 17.143188 loss_rnnt 10.382730 hw_loss 0.107638 lr 0.00048042 rank 7
2023-02-21 02:45:05,868 DEBUG TRAIN Batch 12/8200 loss 12.800023 loss_att 14.890873 loss_ctc 17.028790 loss_rnnt 11.712988 hw_loss 0.196932 lr 0.00048046 rank 4
2023-02-21 02:45:05,902 DEBUG TRAIN Batch 12/8200 loss 11.783173 loss_att 16.613281 loss_ctc 15.711195 loss_rnnt 10.227406 hw_loss 0.123768 lr 0.00048048 rank 1
2023-02-21 02:45:05,904 DEBUG TRAIN Batch 12/8200 loss 16.491058 loss_att 18.188229 loss_ctc 18.174492 loss_rnnt 15.890330 hw_loss 0.069064 lr 0.00048045 rank 5
2023-02-21 02:45:06,002 DEBUG TRAIN Batch 12/8200 loss 6.814815 loss_att 8.715410 loss_ctc 8.581522 loss_rnnt 6.148340 hw_loss 0.095241 lr 0.00048043 rank 6
2023-02-21 02:45:06,016 DEBUG TRAIN Batch 12/8200 loss 20.073051 loss_att 21.385056 loss_ctc 27.339497 loss_rnnt 18.800016 hw_loss 0.078332 lr 0.00048041 rank 2
2023-02-21 02:45:06,123 DEBUG TRAIN Batch 12/8200 loss 8.271161 loss_att 11.968840 loss_ctc 10.741194 loss_rnnt 7.152561 hw_loss 0.093236 lr 0.00048045 rank 3
2023-02-21 02:45:06,193 DEBUG TRAIN Batch 12/8200 loss 20.175440 loss_att 22.596186 loss_ctc 27.963144 loss_rnnt 18.542473 hw_loss 0.207109 lr 0.00048050 rank 0
2023-02-21 02:53:41,247 DEBUG TRAIN Batch 12/8300 loss 7.968456 loss_att 12.385173 loss_ctc 12.021727 loss_rnnt 6.489913 hw_loss 0.102681 lr 0.00048024 rank 4
2023-02-21 02:53:41,278 DEBUG TRAIN Batch 12/8300 loss 10.098905 loss_att 11.603905 loss_ctc 13.563684 loss_rnnt 9.240546 hw_loss 0.178856 lr 0.00048021 rank 6
2023-02-21 02:53:41,311 DEBUG TRAIN Batch 12/8300 loss 23.201075 loss_att 31.291622 loss_ctc 30.804701 loss_rnnt 20.499952 hw_loss 0.129743 lr 0.00048020 rank 7
2023-02-21 02:53:41,368 DEBUG TRAIN Batch 12/8300 loss 14.683518 loss_att 16.247059 loss_ctc 18.875435 loss_rnnt 13.745147 hw_loss 0.125139 lr 0.00048026 rank 1
2023-02-21 02:53:41,369 DEBUG TRAIN Batch 12/8300 loss 10.089333 loss_att 10.208606 loss_ctc 11.092405 loss_rnnt 9.864038 hw_loss 0.126931 lr 0.00048027 rank 0
2023-02-21 02:53:41,641 DEBUG TRAIN Batch 12/8300 loss 15.510512 loss_att 21.176073 loss_ctc 22.819710 loss_rnnt 13.372343 hw_loss 0.057186 lr 0.00048022 rank 5
2023-02-21 02:53:41,642 DEBUG TRAIN Batch 12/8300 loss 11.177393 loss_att 12.800331 loss_ctc 16.540161 loss_rnnt 10.065893 hw_loss 0.134769 lr 0.00048023 rank 3
2023-02-21 02:53:41,662 DEBUG TRAIN Batch 12/8300 loss 25.541960 loss_att 33.063713 loss_ctc 34.035442 loss_rnnt 22.849205 hw_loss 0.104892 lr 0.00048019 rank 2
2023-02-21 02:58:23,179 DEBUG CV Batch 12/0 loss 3.229741 loss_att 2.943549 loss_ctc 3.509380 loss_rnnt 2.872106 hw_loss 0.707977 history loss 3.110121 rank 5
2023-02-21 02:58:23,244 DEBUG CV Batch 12/0 loss 3.229741 loss_att 2.943549 loss_ctc 3.509380 loss_rnnt 2.872106 hw_loss 0.707977 history loss 3.110121 rank 0
2023-02-21 02:58:23,252 DEBUG CV Batch 12/0 loss 3.229741 loss_att 2.943549 loss_ctc 3.509380 loss_rnnt 2.872106 hw_loss 0.707977 history loss 3.110121 rank 2
2023-02-21 02:58:23,271 DEBUG CV Batch 12/0 loss 3.229741 loss_att 2.943549 loss_ctc 3.509380 loss_rnnt 2.872106 hw_loss 0.707977 history loss 3.110121 rank 1
2023-02-21 02:58:23,288 DEBUG CV Batch 12/0 loss 3.229741 loss_att 2.943549 loss_ctc 3.509380 loss_rnnt 2.872106 hw_loss 0.707977 history loss 3.110121 rank 6
2023-02-21 02:58:23,418 DEBUG CV Batch 12/0 loss 3.229741 loss_att 2.943549 loss_ctc 3.509380 loss_rnnt 2.872106 hw_loss 0.707977 history loss 3.110121 rank 7
2023-02-21 02:58:23,479 DEBUG CV Batch 12/0 loss 3.229741 loss_att 2.943549 loss_ctc 3.509380 loss_rnnt 2.872106 hw_loss 0.707977 history loss 3.110121 rank 4
2023-02-21 02:58:23,574 DEBUG CV Batch 12/0 loss 3.229741 loss_att 2.943549 loss_ctc 3.509380 loss_rnnt 2.872106 hw_loss 0.707977 history loss 3.110121 rank 3
2023-02-21 02:59:09,540 DEBUG CV Batch 12/100 loss 11.017905 loss_att 11.439777 loss_ctc 15.084135 loss_rnnt 10.330177 hw_loss 0.114730 history loss 4.840996 rank 6
2023-02-21 02:59:09,754 DEBUG CV Batch 12/100 loss 11.017905 loss_att 11.439777 loss_ctc 15.084135 loss_rnnt 10.330177 hw_loss 0.114730 history loss 4.840996 rank 7
2023-02-21 02:59:10,815 DEBUG CV Batch 12/100 loss 11.017905 loss_att 11.439777 loss_ctc 15.084135 loss_rnnt 10.330177 hw_loss 0.114730 history loss 4.840996 rank 4
2023-02-21 02:59:10,941 DEBUG CV Batch 12/100 loss 11.017905 loss_att 11.439777 loss_ctc 15.084135 loss_rnnt 10.330177 hw_loss 0.114730 history loss 4.840996 rank 5
2023-02-21 02:59:11,931 DEBUG CV Batch 12/100 loss 11.017905 loss_att 11.439777 loss_ctc 15.084135 loss_rnnt 10.330177 hw_loss 0.114730 history loss 4.840996 rank 3
2023-02-21 02:59:13,341 DEBUG CV Batch 12/100 loss 11.017905 loss_att 11.439777 loss_ctc 15.084135 loss_rnnt 10.330177 hw_loss 0.114730 history loss 4.840996 rank 0
2023-02-21 02:59:13,486 DEBUG CV Batch 12/100 loss 11.017905 loss_att 11.439777 loss_ctc 15.084135 loss_rnnt 10.330177 hw_loss 0.114730 history loss 4.840996 rank 1
2023-02-21 02:59:18,021 DEBUG CV Batch 12/100 loss 11.017905 loss_att 11.439777 loss_ctc 15.084135 loss_rnnt 10.330177 hw_loss 0.114730 history loss 4.840996 rank 2
2023-02-21 03:00:04,831 DEBUG CV Batch 12/200 loss 9.992164 loss_att 19.333143 loss_ctc 13.652479 loss_rnnt 7.566356 hw_loss 0.130442 history loss 5.631343 rank 7
2023-02-21 03:00:10,069 DEBUG CV Batch 12/200 loss 9.992164 loss_att 19.333143 loss_ctc 13.652479 loss_rnnt 7.566356 hw_loss 0.130442 history loss 5.631343 rank 6
2023-02-21 03:00:11,110 DEBUG CV Batch 12/200 loss 9.992164 loss_att 19.333143 loss_ctc 13.652479 loss_rnnt 7.566356 hw_loss 0.130442 history loss 5.631343 rank 1
2023-02-21 03:00:11,582 DEBUG CV Batch 12/200 loss 9.992164 loss_att 19.333143 loss_ctc 13.652479 loss_rnnt 7.566356 hw_loss 0.130442 history loss 5.631343 rank 4
2023-02-21 03:00:13,246 DEBUG CV Batch 12/200 loss 9.992164 loss_att 19.333143 loss_ctc 13.652479 loss_rnnt 7.566356 hw_loss 0.130442 history loss 5.631343 rank 3
2023-02-21 03:00:14,146 DEBUG CV Batch 12/200 loss 9.992164 loss_att 19.333143 loss_ctc 13.652479 loss_rnnt 7.566356 hw_loss 0.130442 history loss 5.631343 rank 5
2023-02-21 03:00:15,318 DEBUG CV Batch 12/200 loss 9.992164 loss_att 19.333143 loss_ctc 13.652479 loss_rnnt 7.566356 hw_loss 0.130442 history loss 5.631343 rank 2
2023-02-21 03:00:16,544 DEBUG CV Batch 12/200 loss 9.992164 loss_att 19.333143 loss_ctc 13.652479 loss_rnnt 7.566356 hw_loss 0.130442 history loss 5.631343 rank 0
2023-02-21 03:01:09,602 DEBUG CV Batch 12/300 loss 5.882412 loss_att 6.187806 loss_ctc 7.632892 loss_rnnt 5.536117 hw_loss 0.097161 history loss 5.794996 rank 7
2023-02-21 03:01:11,389 DEBUG CV Batch 12/300 loss 5.882412 loss_att 6.187806 loss_ctc 7.632892 loss_rnnt 5.536117 hw_loss 0.097161 history loss 5.794996 rank 5
2023-02-21 03:01:12,064 DEBUG CV Batch 12/300 loss 5.882412 loss_att 6.187806 loss_ctc 7.632892 loss_rnnt 5.536117 hw_loss 0.097161 history loss 5.794996 rank 3
2023-02-21 03:01:12,275 DEBUG CV Batch 12/300 loss 5.882412 loss_att 6.187806 loss_ctc 7.632892 loss_rnnt 5.536117 hw_loss 0.097161 history loss 5.794996 rank 4
2023-02-21 03:01:13,901 DEBUG CV Batch 12/300 loss 5.882412 loss_att 6.187806 loss_ctc 7.632892 loss_rnnt 5.536117 hw_loss 0.097161 history loss 5.794996 rank 1
2023-02-21 03:01:14,761 DEBUG CV Batch 12/300 loss 5.882412 loss_att 6.187806 loss_ctc 7.632892 loss_rnnt 5.536117 hw_loss 0.097161 history loss 5.794996 rank 6
2023-02-21 03:01:15,328 DEBUG CV Batch 12/300 loss 5.882412 loss_att 6.187806 loss_ctc 7.632892 loss_rnnt 5.536117 hw_loss 0.097161 history loss 5.794996 rank 2
2023-02-21 03:01:19,721 DEBUG CV Batch 12/300 loss 5.882412 loss_att 6.187806 loss_ctc 7.632892 loss_rnnt 5.536117 hw_loss 0.097161 history loss 5.794996 rank 0
2023-02-21 03:02:22,564 DEBUG CV Batch 12/400 loss 20.581396 loss_att 88.828575 loss_ctc 9.661308 loss_rnnt 8.320741 hw_loss 0.126057 history loss 6.956248 rank 2
2023-02-21 03:02:22,581 DEBUG CV Batch 12/400 loss 20.581396 loss_att 88.828575 loss_ctc 9.661308 loss_rnnt 8.320741 hw_loss 0.126057 history loss 6.956248 rank 1
2023-02-21 03:02:22,730 DEBUG CV Batch 12/400 loss 20.581396 loss_att 88.828575 loss_ctc 9.661308 loss_rnnt 8.320741 hw_loss 0.126057 history loss 6.956248 rank 6
2023-02-21 03:02:22,924 DEBUG CV Batch 12/400 loss 20.581396 loss_att 88.828575 loss_ctc 9.661308 loss_rnnt 8.320741 hw_loss 0.126057 history loss 6.956248 rank 3
2023-02-21 03:02:23,371 DEBUG CV Batch 12/400 loss 20.581396 loss_att 88.828575 loss_ctc 9.661308 loss_rnnt 8.320741 hw_loss 0.126057 history loss 6.956248 rank 7
2023-02-21 03:02:27,351 DEBUG CV Batch 12/400 loss 20.581396 loss_att 88.828575 loss_ctc 9.661308 loss_rnnt 8.320741 hw_loss 0.126057 history loss 6.956248 rank 5
2023-02-21 03:02:29,374 DEBUG CV Batch 12/400 loss 20.581396 loss_att 88.828575 loss_ctc 9.661308 loss_rnnt 8.320741 hw_loss 0.126057 history loss 6.956248 rank 4
2023-02-21 03:02:32,641 DEBUG CV Batch 12/400 loss 20.581396 loss_att 88.828575 loss_ctc 9.661308 loss_rnnt 8.320741 hw_loss 0.126057 history loss 6.956248 rank 0
2023-02-21 03:03:09,092 DEBUG CV Batch 12/500 loss 5.749290 loss_att 6.876513 loss_ctc 6.842206 loss_rnnt 5.278261 hw_loss 0.187242 history loss 7.987581 rank 7
2023-02-21 03:03:11,713 DEBUG CV Batch 12/500 loss 5.749290 loss_att 6.876513 loss_ctc 6.842206 loss_rnnt 5.278261 hw_loss 0.187242 history loss 7.987581 rank 2
2023-02-21 03:03:12,271 DEBUG CV Batch 12/500 loss 5.749290 loss_att 6.876513 loss_ctc 6.842206 loss_rnnt 5.278261 hw_loss 0.187242 history loss 7.987581 rank 5
2023-02-21 03:03:13,068 DEBUG CV Batch 12/500 loss 5.749290 loss_att 6.876513 loss_ctc 6.842206 loss_rnnt 5.278261 hw_loss 0.187242 history loss 7.987581 rank 3
2023-02-21 03:03:14,145 DEBUG CV Batch 12/500 loss 5.749290 loss_att 6.876513 loss_ctc 6.842206 loss_rnnt 5.278261 hw_loss 0.187242 history loss 7.987581 rank 1
2023-02-21 03:03:17,201 DEBUG CV Batch 12/500 loss 5.749290 loss_att 6.876513 loss_ctc 6.842206 loss_rnnt 5.278261 hw_loss 0.187242 history loss 7.987581 rank 6
2023-02-21 03:03:21,032 DEBUG CV Batch 12/500 loss 5.749290 loss_att 6.876513 loss_ctc 6.842206 loss_rnnt 5.278261 hw_loss 0.187242 history loss 7.987581 rank 4
2023-02-21 03:03:22,595 DEBUG CV Batch 12/500 loss 5.749290 loss_att 6.876513 loss_ctc 6.842206 loss_rnnt 5.278261 hw_loss 0.187242 history loss 7.987581 rank 0
2023-02-21 03:04:16,380 DEBUG CV Batch 12/600 loss 9.788594 loss_att 9.193486 loss_ctc 12.303940 loss_rnnt 9.365239 hw_loss 0.388119 history loss 9.091598 rank 7
2023-02-21 03:04:18,790 DEBUG CV Batch 12/600 loss 9.788594 loss_att 9.193486 loss_ctc 12.303940 loss_rnnt 9.365239 hw_loss 0.388119 history loss 9.091598 rank 2
2023-02-21 03:04:19,396 DEBUG CV Batch 12/600 loss 9.788594 loss_att 9.193486 loss_ctc 12.303940 loss_rnnt 9.365239 hw_loss 0.388119 history loss 9.091598 rank 1
2023-02-21 03:04:22,738 DEBUG CV Batch 12/600 loss 9.788594 loss_att 9.193486 loss_ctc 12.303940 loss_rnnt 9.365239 hw_loss 0.388119 history loss 9.091598 rank 5
2023-02-21 03:04:24,704 DEBUG CV Batch 12/600 loss 9.788594 loss_att 9.193486 loss_ctc 12.303940 loss_rnnt 9.365239 hw_loss 0.388119 history loss 9.091598 rank 3
2023-02-21 03:04:25,302 DEBUG CV Batch 12/600 loss 9.788594 loss_att 9.193486 loss_ctc 12.303940 loss_rnnt 9.365239 hw_loss 0.388119 history loss 9.091598 rank 6
2023-02-21 03:04:26,868 DEBUG CV Batch 12/600 loss 9.788594 loss_att 9.193486 loss_ctc 12.303940 loss_rnnt 9.365239 hw_loss 0.388119 history loss 9.091598 rank 4
2023-02-21 03:04:30,051 DEBUG CV Batch 12/600 loss 9.788594 loss_att 9.193486 loss_ctc 12.303940 loss_rnnt 9.365239 hw_loss 0.388119 history loss 9.091598 rank 0
2023-02-21 03:05:07,556 DEBUG CV Batch 12/700 loss 20.550655 loss_att 46.563656 loss_ctc 31.332514 loss_rnnt 13.896320 hw_loss 0.026540 history loss 10.062979 rank 7
2023-02-21 03:05:08,450 DEBUG CV Batch 12/700 loss 20.550655 loss_att 46.563656 loss_ctc 31.332514 loss_rnnt 13.896320 hw_loss 0.026540 history loss 10.062979 rank 6
2023-02-21 03:05:10,796 DEBUG CV Batch 12/700 loss 20.550655 loss_att 46.563656 loss_ctc 31.332514 loss_rnnt 13.896320 hw_loss 0.026540 history loss 10.062979 rank 5
2023-02-21 03:05:15,363 DEBUG CV Batch 12/700 loss 20.550655 loss_att 46.563656 loss_ctc 31.332514 loss_rnnt 13.896320 hw_loss 0.026540 history loss 10.062979 rank 4
2023-02-21 03:05:15,757 DEBUG CV Batch 12/700 loss 20.550655 loss_att 46.563656 loss_ctc 31.332514 loss_rnnt 13.896320 hw_loss 0.026540 history loss 10.062979 rank 3
2023-02-21 03:05:16,727 DEBUG CV Batch 12/700 loss 20.550655 loss_att 46.563656 loss_ctc 31.332514 loss_rnnt 13.896320 hw_loss 0.026540 history loss 10.062979 rank 2
2023-02-21 03:05:17,259 DEBUG CV Batch 12/700 loss 20.550655 loss_att 46.563656 loss_ctc 31.332514 loss_rnnt 13.896320 hw_loss 0.026540 history loss 10.062979 rank 1
2023-02-21 03:05:20,037 DEBUG CV Batch 12/700 loss 20.550655 loss_att 46.563656 loss_ctc 31.332514 loss_rnnt 13.896320 hw_loss 0.026540 history loss 10.062979 rank 0
2023-02-21 03:06:16,049 DEBUG CV Batch 12/800 loss 15.529026 loss_att 13.662409 loss_ctc 19.926235 loss_rnnt 15.184314 hw_loss 0.247015 history loss 9.362638 rank 7
2023-02-21 03:06:17,402 DEBUG CV Batch 12/800 loss 15.529026 loss_att 13.662409 loss_ctc 19.926235 loss_rnnt 15.184314 hw_loss 0.247015 history loss 9.362638 rank 6
2023-02-21 03:06:19,735 DEBUG CV Batch 12/800 loss 15.529026 loss_att 13.662409 loss_ctc 19.926235 loss_rnnt 15.184314 hw_loss 0.247015 history loss 9.362638 rank 5
2023-02-21 03:06:22,967 DEBUG CV Batch 12/800 loss 15.529026 loss_att 13.662409 loss_ctc 19.926235 loss_rnnt 15.184314 hw_loss 0.247015 history loss 9.362638 rank 2
2023-02-21 03:06:25,485 DEBUG CV Batch 12/800 loss 15.529026 loss_att 13.662409 loss_ctc 19.926235 loss_rnnt 15.184314 hw_loss 0.247015 history loss 9.362638 rank 1
2023-02-21 03:06:26,951 DEBUG CV Batch 12/800 loss 15.529026 loss_att 13.662409 loss_ctc 19.926235 loss_rnnt 15.184314 hw_loss 0.247015 history loss 9.362638 rank 3
2023-02-21 03:06:27,162 DEBUG CV Batch 12/800 loss 15.529026 loss_att 13.662409 loss_ctc 19.926235 loss_rnnt 15.184314 hw_loss 0.247015 history loss 9.362638 rank 4
2023-02-21 03:06:30,598 DEBUG CV Batch 12/800 loss 15.529026 loss_att 13.662409 loss_ctc 19.926235 loss_rnnt 15.184314 hw_loss 0.247015 history loss 9.362638 rank 0
2023-02-21 03:07:04,162 DEBUG CV Batch 12/900 loss 24.458786 loss_att 31.576176 loss_ctc 28.675331 loss_rnnt 22.458948 hw_loss 0.026540 history loss 9.107946 rank 7
2023-02-21 03:07:08,469 DEBUG CV Batch 12/900 loss 24.458786 loss_att 31.576176 loss_ctc 28.675331 loss_rnnt 22.458948 hw_loss 0.026540 history loss 9.107946 rank 6
2023-02-21 03:07:09,983 DEBUG CV Batch 12/900 loss 24.458786 loss_att 31.576176 loss_ctc 28.675331 loss_rnnt 22.458948 hw_loss 0.026540 history loss 9.107946 rank 5
2023-02-21 03:07:19,106 DEBUG CV Batch 12/900 loss 24.458786 loss_att 31.576176 loss_ctc 28.675331 loss_rnnt 22.458948 hw_loss 0.026540 history loss 9.107946 rank 4
2023-02-21 03:07:22,196 DEBUG CV Batch 12/900 loss 24.458786 loss_att 31.576176 loss_ctc 28.675331 loss_rnnt 22.458948 hw_loss 0.026540 history loss 9.107946 rank 0
2023-02-21 03:07:26,427 DEBUG CV Batch 12/900 loss 24.458786 loss_att 31.576176 loss_ctc 28.675331 loss_rnnt 22.458948 hw_loss 0.026540 history loss 9.107946 rank 3
2023-02-21 03:07:26,858 DEBUG CV Batch 12/900 loss 24.458786 loss_att 31.576176 loss_ctc 28.675331 loss_rnnt 22.458948 hw_loss 0.026540 history loss 9.107946 rank 2
2023-02-21 03:07:28,608 DEBUG CV Batch 12/900 loss 24.458786 loss_att 31.576176 loss_ctc 28.675331 loss_rnnt 22.458948 hw_loss 0.026540 history loss 9.107946 rank 1
2023-02-21 03:08:02,104 DEBUG CV Batch 12/1000 loss 6.232658 loss_att 6.360357 loss_ctc 7.122850 loss_rnnt 5.973185 hw_loss 0.216078 history loss 8.809754 rank 7
2023-02-21 03:08:11,759 DEBUG CV Batch 12/1000 loss 6.232658 loss_att 6.360357 loss_ctc 7.122850 loss_rnnt 5.973185 hw_loss 0.216078 history loss 8.809754 rank 6
2023-02-21 03:08:18,056 DEBUG CV Batch 12/1000 loss 6.232658 loss_att 6.360357 loss_ctc 7.122850 loss_rnnt 5.973185 hw_loss 0.216078 history loss 8.809754 rank 0
2023-02-21 03:08:18,603 DEBUG CV Batch 12/1000 loss 6.232658 loss_att 6.360357 loss_ctc 7.122850 loss_rnnt 5.973185 hw_loss 0.216078 history loss 8.809754 rank 4
2023-02-21 03:08:19,671 DEBUG CV Batch 12/1000 loss 6.232658 loss_att 6.360357 loss_ctc 7.122850 loss_rnnt 5.973185 hw_loss 0.216078 history loss 8.809754 rank 5
2023-02-21 03:08:35,661 DEBUG CV Batch 12/1000 loss 6.232658 loss_att 6.360357 loss_ctc 7.122850 loss_rnnt 5.973185 hw_loss 0.216078 history loss 8.809754 rank 3
2023-02-21 03:08:37,378 DEBUG CV Batch 12/1000 loss 6.232658 loss_att 6.360357 loss_ctc 7.122850 loss_rnnt 5.973185 hw_loss 0.216078 history loss 8.809754 rank 2
2023-02-21 03:08:37,491 DEBUG CV Batch 12/1000 loss 6.232658 loss_att 6.360357 loss_ctc 7.122850 loss_rnnt 5.973185 hw_loss 0.216078 history loss 8.809754 rank 1
2023-02-21 03:09:02,047 DEBUG CV Batch 12/1100 loss 8.788512 loss_att 7.613853 loss_ctc 10.915337 loss_rnnt 8.397614 hw_loss 0.641726 history loss 8.777563 rank 7
2023-02-21 03:09:13,705 DEBUG CV Batch 12/1100 loss 8.788512 loss_att 7.613853 loss_ctc 10.915337 loss_rnnt 8.397614 hw_loss 0.641726 history loss 8.777563 rank 6
2023-02-21 03:09:14,472 DEBUG CV Batch 12/1100 loss 8.788512 loss_att 7.613853 loss_ctc 10.915337 loss_rnnt 8.397614 hw_loss 0.641726 history loss 8.777563 rank 4
2023-02-21 03:09:18,203 DEBUG CV Batch 12/1100 loss 8.788512 loss_att 7.613853 loss_ctc 10.915337 loss_rnnt 8.397614 hw_loss 0.641726 history loss 8.777563 rank 0
2023-02-21 03:09:20,813 DEBUG CV Batch 12/1100 loss 8.788512 loss_att 7.613853 loss_ctc 10.915337 loss_rnnt 8.397614 hw_loss 0.641726 history loss 8.777563 rank 5
2023-02-21 03:09:53,608 DEBUG CV Batch 12/1100 loss 8.788512 loss_att 7.613853 loss_ctc 10.915337 loss_rnnt 8.397614 hw_loss 0.641726 history loss 8.777563 rank 2
2023-02-21 03:09:53,797 DEBUG CV Batch 12/1100 loss 8.788512 loss_att 7.613853 loss_ctc 10.915337 loss_rnnt 8.397614 hw_loss 0.641726 history loss 8.777563 rank 1
2023-02-21 03:09:55,482 DEBUG CV Batch 12/1200 loss 10.097230 loss_att 11.958098 loss_ctc 11.262364 loss_rnnt 9.521268 hw_loss 0.090821 history loss 9.193217 rank 7
2023-02-21 03:09:55,508 DEBUG CV Batch 12/1100 loss 8.788512 loss_att 7.613853 loss_ctc 10.915337 loss_rnnt 8.397614 hw_loss 0.641726 history loss 8.777563 rank 3
2023-02-21 03:10:00,937 DEBUG CV Batch 12/1200 loss 10.097230 loss_att 11.958098 loss_ctc 11.262364 loss_rnnt 9.521268 hw_loss 0.090821 history loss 9.193217 rank 6
2023-02-21 03:10:02,682 DEBUG CV Batch 12/1200 loss 10.097230 loss_att 11.958098 loss_ctc 11.262364 loss_rnnt 9.521268 hw_loss 0.090821 history loss 9.193217 rank 4
2023-02-21 03:10:05,176 DEBUG CV Batch 12/1200 loss 10.097230 loss_att 11.958098 loss_ctc 11.262364 loss_rnnt 9.521268 hw_loss 0.090821 history loss 9.193217 rank 0
2023-02-21 03:10:07,118 DEBUG CV Batch 12/1200 loss 10.097230 loss_att 11.958098 loss_ctc 11.262364 loss_rnnt 9.521268 hw_loss 0.090821 history loss 9.193217 rank 5
2023-02-21 03:10:45,940 DEBUG CV Batch 12/1200 loss 10.097230 loss_att 11.958098 loss_ctc 11.262364 loss_rnnt 9.521268 hw_loss 0.090821 history loss 9.193217 rank 3
2023-02-21 03:10:47,018 DEBUG CV Batch 12/1300 loss 7.886721 loss_att 6.903615 loss_ctc 10.646686 loss_rnnt 7.629972 hw_loss 0.160078 history loss 9.551163 rank 6
2023-02-21 03:10:47,363 DEBUG CV Batch 12/1300 loss 7.886721 loss_att 6.903615 loss_ctc 10.646686 loss_rnnt 7.629972 hw_loss 0.160078 history loss 9.551163 rank 7
2023-02-21 03:10:47,575 DEBUG CV Batch 12/1200 loss 10.097230 loss_att 11.958098 loss_ctc 11.262364 loss_rnnt 9.521268 hw_loss 0.090821 history loss 9.193217 rank 1
2023-02-21 03:10:47,870 DEBUG CV Batch 12/1200 loss 10.097230 loss_att 11.958098 loss_ctc 11.262364 loss_rnnt 9.521268 hw_loss 0.090821 history loss 9.193217 rank 2
2023-02-21 03:10:56,981 DEBUG CV Batch 12/1300 loss 7.886721 loss_att 6.903615 loss_ctc 10.646686 loss_rnnt 7.629972 hw_loss 0.160078 history loss 9.551163 rank 0
2023-02-21 03:10:58,251 DEBUG CV Batch 12/1300 loss 7.886721 loss_att 6.903615 loss_ctc 10.646686 loss_rnnt 7.629972 hw_loss 0.160078 history loss 9.551163 rank 4
2023-02-21 03:11:05,501 DEBUG CV Batch 12/1300 loss 7.886721 loss_att 6.903615 loss_ctc 10.646686 loss_rnnt 7.629972 hw_loss 0.160078 history loss 9.551163 rank 5
2023-02-21 03:11:40,409 DEBUG CV Batch 12/1400 loss 12.887704 loss_att 39.407261 loss_ctc 11.117378 loss_rnnt 7.722353 hw_loss 0.182778 history loss 10.013183 rank 7
2023-02-21 03:11:40,622 DEBUG CV Batch 12/1400 loss 12.887704 loss_att 39.407261 loss_ctc 11.117378 loss_rnnt 7.722353 hw_loss 0.182778 history loss 10.013183 rank 6
2023-02-21 03:11:45,995 DEBUG CV Batch 12/1400 loss 12.887704 loss_att 39.407261 loss_ctc 11.117378 loss_rnnt 7.722353 hw_loss 0.182778 history loss 10.013183 rank 0
2023-02-21 03:11:48,862 DEBUG CV Batch 12/1300 loss 7.886721 loss_att 6.903615 loss_ctc 10.646686 loss_rnnt 7.629972 hw_loss 0.160078 history loss 9.551163 rank 3
2023-02-21 03:11:49,142 DEBUG CV Batch 12/1400 loss 12.887704 loss_att 39.407261 loss_ctc 11.117378 loss_rnnt 7.722353 hw_loss 0.182778 history loss 10.013183 rank 4
2023-02-21 03:11:52,514 DEBUG CV Batch 12/1300 loss 7.886721 loss_att 6.903615 loss_ctc 10.646686 loss_rnnt 7.629972 hw_loss 0.160078 history loss 9.551163 rank 2
2023-02-21 03:11:53,631 DEBUG CV Batch 12/1300 loss 7.886721 loss_att 6.903615 loss_ctc 10.646686 loss_rnnt 7.629972 hw_loss 0.160078 history loss 9.551163 rank 1
2023-02-21 03:12:00,960 DEBUG CV Batch 12/1400 loss 12.887704 loss_att 39.407261 loss_ctc 11.117378 loss_rnnt 7.722353 hw_loss 0.182778 history loss 10.013183 rank 5
2023-02-21 03:12:39,504 DEBUG CV Batch 12/1500 loss 7.349988 loss_att 8.598097 loss_ctc 6.183088 loss_rnnt 7.212580 hw_loss 0.081325 history loss 9.765553 rank 7
2023-02-21 03:12:41,045 DEBUG CV Batch 12/1400 loss 12.887704 loss_att 39.407261 loss_ctc 11.117378 loss_rnnt 7.722353 hw_loss 0.182778 history loss 10.013183 rank 2
2023-02-21 03:12:41,638 DEBUG CV Batch 12/1500 loss 7.349988 loss_att 8.598097 loss_ctc 6.183088 loss_rnnt 7.212580 hw_loss 0.081325 history loss 9.765553 rank 0
2023-02-21 03:12:42,127 DEBUG CV Batch 12/1400 loss 12.887704 loss_att 39.407261 loss_ctc 11.117378 loss_rnnt 7.722353 hw_loss 0.182778 history loss 10.013183 rank 3
2023-02-21 03:12:45,656 DEBUG CV Batch 12/1400 loss 12.887704 loss_att 39.407261 loss_ctc 11.117378 loss_rnnt 7.722353 hw_loss 0.182778 history loss 10.013183 rank 1
2023-02-21 03:12:46,600 DEBUG CV Batch 12/1500 loss 7.349988 loss_att 8.598097 loss_ctc 6.183088 loss_rnnt 7.212580 hw_loss 0.081325 history loss 9.765553 rank 6
2023-02-21 03:12:46,739 DEBUG CV Batch 12/1500 loss 7.349988 loss_att 8.598097 loss_ctc 6.183088 loss_rnnt 7.212580 hw_loss 0.081325 history loss 9.765553 rank 4
2023-02-21 03:13:00,491 DEBUG CV Batch 12/1500 loss 7.349988 loss_att 8.598097 loss_ctc 6.183088 loss_rnnt 7.212580 hw_loss 0.081325 history loss 9.765553 rank 5
2023-02-21 03:13:28,789 DEBUG CV Batch 12/1600 loss 10.304539 loss_att 16.699648 loss_ctc 13.702293 loss_rnnt 8.522170 hw_loss 0.094336 history loss 9.654282 rank 7
2023-02-21 03:13:33,181 DEBUG CV Batch 12/1600 loss 10.304539 loss_att 16.699648 loss_ctc 13.702293 loss_rnnt 8.522170 hw_loss 0.094336 history loss 9.654282 rank 0
2023-02-21 03:13:39,143 DEBUG CV Batch 12/1600 loss 10.304539 loss_att 16.699648 loss_ctc 13.702293 loss_rnnt 8.522170 hw_loss 0.094336 history loss 9.654282 rank 4
2023-02-21 03:13:41,217 DEBUG CV Batch 12/1600 loss 10.304539 loss_att 16.699648 loss_ctc 13.702293 loss_rnnt 8.522170 hw_loss 0.094336 history loss 9.654282 rank 6
2023-02-21 03:13:49,047 DEBUG CV Batch 12/1500 loss 7.349988 loss_att 8.598097 loss_ctc 6.183088 loss_rnnt 7.212580 hw_loss 0.081325 history loss 9.765553 rank 3
2023-02-21 03:13:55,191 DEBUG CV Batch 12/1500 loss 7.349988 loss_att 8.598097 loss_ctc 6.183088 loss_rnnt 7.212580 hw_loss 0.081325 history loss 9.765553 rank 2
2023-02-21 03:13:57,344 DEBUG CV Batch 12/1600 loss 10.304539 loss_att 16.699648 loss_ctc 13.702293 loss_rnnt 8.522170 hw_loss 0.094336 history loss 9.654282 rank 5
2023-02-21 03:14:03,680 DEBUG CV Batch 12/1500 loss 7.349988 loss_att 8.598097 loss_ctc 6.183088 loss_rnnt 7.212580 hw_loss 0.081325 history loss 9.765553 rank 1
2023-02-21 03:14:24,426 DEBUG CV Batch 12/1700 loss 12.006710 loss_att 12.238509 loss_ctc 18.079662 loss_rnnt 11.074484 hw_loss 0.142762 history loss 9.518575 rank 0
2023-02-21 03:14:27,566 DEBUG CV Batch 12/1700 loss 12.006710 loss_att 12.238509 loss_ctc 18.079662 loss_rnnt 11.074484 hw_loss 0.142762 history loss 9.518575 rank 4
2023-02-21 03:14:27,803 DEBUG CV Batch 12/1700 loss 12.006710 loss_att 12.238509 loss_ctc 18.079662 loss_rnnt 11.074484 hw_loss 0.142762 history loss 9.518575 rank 7
2023-02-21 03:14:37,494 DEBUG CV Batch 12/1600 loss 10.304539 loss_att 16.699648 loss_ctc 13.702293 loss_rnnt 8.522170 hw_loss 0.094336 history loss 9.654282 rank 3
2023-02-21 03:14:37,553 DEBUG CV Batch 12/1700 loss 12.006710 loss_att 12.238509 loss_ctc 18.079662 loss_rnnt 11.074484 hw_loss 0.142762 history loss 9.518575 rank 6
2023-02-21 03:14:44,329 DEBUG CV Batch 12/1600 loss 10.304539 loss_att 16.699648 loss_ctc 13.702293 loss_rnnt 8.522170 hw_loss 0.094336 history loss 9.654282 rank 2
2023-02-21 03:14:48,841 DEBUG CV Batch 12/1700 loss 12.006710 loss_att 12.238509 loss_ctc 18.079662 loss_rnnt 11.074484 hw_loss 0.142762 history loss 9.518575 rank 5
2023-02-21 03:14:55,051 INFO Epoch 12 CV info cv_loss 9.452899538894785
2023-02-21 03:14:55,052 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/12.pt
2023-02-21 03:14:56,183 INFO Epoch 12 CV info cv_loss 9.452899540660784
2023-02-21 03:14:56,185 INFO Epoch 13 TRAIN info lr 0.00048012600159639786
2023-02-21 03:14:56,188 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 03:14:57,294 INFO Epoch 13 TRAIN info lr 0.0004802079251046646
2023-02-21 03:14:57,297 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 03:15:03,438 DEBUG CV Batch 12/1600 loss 10.304539 loss_att 16.699648 loss_ctc 13.702293 loss_rnnt 8.522170 hw_loss 0.094336 history loss 9.654282 rank 1
2023-02-21 03:15:04,696 INFO Epoch 12 CV info cv_loss 9.452899536465457
2023-02-21 03:15:04,697 INFO Epoch 13 TRAIN info lr 0.00048012821519400966
2023-02-21 03:15:04,701 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 03:15:08,277 INFO Epoch 12 CV info cv_loss 9.45289953899816
2023-02-21 03:15:08,278 INFO Epoch 13 TRAIN info lr 0.00048013928364134265
2023-02-21 03:15:08,283 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 03:15:24,032 INFO Epoch 12 CV info cv_loss 9.452899539575341
2023-02-21 03:15:24,033 INFO Epoch 13 TRAIN info lr 0.00048018577948066545
2023-02-21 03:15:24,046 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 03:15:38,127 DEBUG CV Batch 12/1700 loss 12.006710 loss_att 12.238509 loss_ctc 18.079662 loss_rnnt 11.074484 hw_loss 0.142762 history loss 9.518575 rank 3
2023-02-21 03:15:43,771 DEBUG CV Batch 12/1700 loss 12.006710 loss_att 12.238509 loss_ctc 18.079662 loss_rnnt 11.074484 hw_loss 0.142762 history loss 9.518575 rank 2
2023-02-21 03:16:01,030 DEBUG CV Batch 12/1700 loss 12.006710 loss_att 12.238509 loss_ctc 18.079662 loss_rnnt 11.074484 hw_loss 0.142762 history loss 9.518575 rank 1
2023-02-21 03:16:13,075 INFO Epoch 12 CV info cv_loss 9.452899539790707
2023-02-21 03:16:13,077 INFO Epoch 13 TRAIN info lr 0.00048015256678863867
2023-02-21 03:16:13,080 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 03:16:20,679 INFO Epoch 12 CV info cv_loss 9.452899539618414
2023-02-21 03:16:20,681 INFO Epoch 13 TRAIN info lr 0.00048009501444425785
2023-02-21 03:16:20,685 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 03:16:36,867 INFO Epoch 12 CV info cv_loss 9.45289953955811
2023-02-21 03:16:36,868 INFO Epoch 13 TRAIN info lr 0.0004801968519096747
2023-02-21 03:16:36,871 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 03:24:49,056 DEBUG TRAIN Batch 13/0 loss 12.626439 loss_att 11.941556 loss_ctc 13.311380 loss_rnnt 12.596717 hw_loss 0.141327 lr 0.00048021 rank 0
2023-02-21 03:24:49,083 DEBUG TRAIN Batch 13/0 loss 8.509536 loss_att 7.754143 loss_ctc 9.900689 loss_rnnt 8.345596 hw_loss 0.242869 lr 0.00048015 rank 3
2023-02-21 03:24:49,092 DEBUG TRAIN Batch 13/0 loss 10.531758 loss_att 9.852298 loss_ctc 12.527719 loss_rnnt 10.248763 hw_loss 0.286425 lr 0.00048018 rank 5
2023-02-21 03:24:49,177 DEBUG TRAIN Batch 13/0 loss 10.795921 loss_att 10.287703 loss_ctc 12.525739 loss_rnnt 10.524205 hw_loss 0.267593 lr 0.00048013 rank 4
2023-02-21 03:24:49,249 DEBUG TRAIN Batch 13/0 loss 11.191848 loss_att 9.899027 loss_ctc 12.803866 loss_rnnt 11.076327 hw_loss 0.298404 lr 0.00048014 rank 6
2023-02-21 03:24:49,254 DEBUG TRAIN Batch 13/0 loss 12.857170 loss_att 10.939034 loss_ctc 14.527064 loss_rnnt 12.883463 hw_loss 0.252527 lr 0.00048012 rank 7
2023-02-21 03:24:49,483 DEBUG TRAIN Batch 13/0 loss 12.157245 loss_att 12.040133 loss_ctc 14.846132 loss_rnnt 11.593724 hw_loss 0.428294 lr 0.00048009 rank 2
2023-02-21 03:24:49,706 DEBUG TRAIN Batch 13/0 loss 16.053043 loss_att 14.196671 loss_ctc 17.728851 loss_rnnt 15.972453 hw_loss 0.428294 lr 0.00048019 rank 1
2023-02-21 03:32:46,721 DEBUG TRAIN Batch 13/100 loss 9.403686 loss_att 11.930226 loss_ctc 13.756507 loss_rnnt 8.302289 hw_loss 0.029460 lr 0.00047998 rank 0
2023-02-21 03:32:46,851 DEBUG TRAIN Batch 13/100 loss 14.756034 loss_att 21.261528 loss_ctc 22.094168 loss_rnnt 12.433359 hw_loss 0.080919 lr 0.00047990 rank 7
2023-02-21 03:32:46,916 DEBUG TRAIN Batch 13/100 loss 7.123522 loss_att 10.994017 loss_ctc 8.987246 loss_rnnt 6.038940 hw_loss 0.116224 lr 0.00047987 rank 2
2023-02-21 03:32:46,945 DEBUG TRAIN Batch 13/100 loss 21.366089 loss_att 24.748020 loss_ctc 24.025494 loss_rnnt 20.228848 hw_loss 0.199249 lr 0.00047993 rank 3
2023-02-21 03:32:46,947 DEBUG TRAIN Batch 13/100 loss 18.805937 loss_att 22.165571 loss_ctc 20.500183 loss_rnnt 17.838606 hw_loss 0.130323 lr 0.00047990 rank 4
2023-02-21 03:32:46,984 DEBUG TRAIN Batch 13/100 loss 20.028215 loss_att 26.736113 loss_ctc 26.556343 loss_rnnt 17.718796 hw_loss 0.182671 lr 0.00047997 rank 1
2023-02-21 03:32:47,063 DEBUG TRAIN Batch 13/100 loss 22.106409 loss_att 21.543434 loss_ctc 25.063269 loss_rnnt 21.756134 hw_loss 0.128665 lr 0.00047996 rank 5
2023-02-21 03:32:47,400 DEBUG TRAIN Batch 13/100 loss 6.765197 loss_att 14.596785 loss_ctc 8.538591 loss_rnnt 4.903416 hw_loss 0.110645 lr 0.00047992 rank 6
2023-02-21 03:40:42,538 DEBUG TRAIN Batch 13/200 loss 12.323605 loss_att 16.606289 loss_ctc 18.086853 loss_rnnt 10.682211 hw_loss 0.030795 lr 0.00047968 rank 7
2023-02-21 03:40:42,577 DEBUG TRAIN Batch 13/200 loss 13.053891 loss_att 13.580921 loss_ctc 14.439460 loss_rnnt 12.686976 hw_loss 0.143938 lr 0.00047974 rank 5
2023-02-21 03:40:42,637 DEBUG TRAIN Batch 13/200 loss 23.685946 loss_att 30.137745 loss_ctc 31.818966 loss_rnnt 21.294437 hw_loss 0.031397 lr 0.00047976 rank 0
2023-02-21 03:40:42,701 DEBUG TRAIN Batch 13/200 loss 24.481730 loss_att 25.436985 loss_ctc 30.220097 loss_rnnt 23.421141 hw_loss 0.195793 lr 0.00047969 rank 6
2023-02-21 03:40:42,821 DEBUG TRAIN Batch 13/200 loss 10.527768 loss_att 15.738970 loss_ctc 13.183191 loss_rnnt 9.114651 hw_loss 0.031538 lr 0.00047971 rank 3
2023-02-21 03:40:42,827 DEBUG TRAIN Batch 13/200 loss 15.748293 loss_att 19.880161 loss_ctc 21.879742 loss_rnnt 14.077701 hw_loss 0.050048 lr 0.00047968 rank 4
2023-02-21 03:40:42,829 DEBUG TRAIN Batch 13/200 loss 17.398840 loss_att 24.547560 loss_ctc 21.553482 loss_rnnt 15.344313 hw_loss 0.132808 lr 0.00047965 rank 2
2023-02-21 03:40:43,001 DEBUG TRAIN Batch 13/200 loss 17.093128 loss_att 19.391298 loss_ctc 17.342703 loss_rnnt 16.493351 hw_loss 0.200378 lr 0.00047975 rank 1
2023-02-21 03:48:45,371 DEBUG TRAIN Batch 13/300 loss 12.712601 loss_att 13.212474 loss_ctc 14.839476 loss_rnnt 12.206644 hw_loss 0.229498 lr 0.00047946 rank 4
2023-02-21 03:48:45,372 DEBUG TRAIN Batch 13/300 loss 17.748793 loss_att 19.143875 loss_ctc 26.616383 loss_rnnt 16.151981 hw_loss 0.253969 lr 0.00047953 rank 1
2023-02-21 03:48:45,400 DEBUG TRAIN Batch 13/300 loss 10.562547 loss_att 17.133036 loss_ctc 15.285553 loss_rnnt 8.493940 hw_loss 0.233954 lr 0.00047946 rank 7
2023-02-21 03:48:45,475 DEBUG TRAIN Batch 13/300 loss 12.915863 loss_att 17.129396 loss_ctc 13.224627 loss_rnnt 11.985095 hw_loss 0.087924 lr 0.00047952 rank 5
2023-02-21 03:48:45,490 DEBUG TRAIN Batch 13/300 loss 24.866394 loss_att 30.665630 loss_ctc 28.477146 loss_rnnt 23.164980 hw_loss 0.112741 lr 0.00047947 rank 6
2023-02-21 03:48:45,572 DEBUG TRAIN Batch 13/300 loss 9.850522 loss_att 15.237421 loss_ctc 13.167250 loss_rnnt 8.292014 hw_loss 0.072935 lr 0.00047943 rank 2
2023-02-21 03:48:45,717 DEBUG TRAIN Batch 13/300 loss 17.909948 loss_att 24.240662 loss_ctc 23.400925 loss_rnnt 15.843128 hw_loss 0.128525 lr 0.00047949 rank 3
2023-02-21 03:48:45,875 DEBUG TRAIN Batch 13/300 loss 15.553112 loss_att 19.392040 loss_ctc 21.645393 loss_rnnt 13.871735 hw_loss 0.189915 lr 0.00047954 rank 0
2023-02-21 03:56:59,764 DEBUG TRAIN Batch 13/400 loss 21.139339 loss_att 24.451233 loss_ctc 21.358906 loss_rnnt 20.433901 hw_loss 0.025847 lr 0.00047924 rank 7
2023-02-21 03:56:59,796 DEBUG TRAIN Batch 13/400 loss 18.237492 loss_att 21.705614 loss_ctc 25.645313 loss_rnnt 16.533525 hw_loss 0.042433 lr 0.00047932 rank 0
2023-02-21 03:56:59,889 DEBUG TRAIN Batch 13/400 loss 24.542303 loss_att 26.578827 loss_ctc 28.829510 loss_rnnt 23.441971 hw_loss 0.227623 lr 0.00047924 rank 4
2023-02-21 03:57:00,066 DEBUG TRAIN Batch 13/400 loss 15.902136 loss_att 17.224586 loss_ctc 16.027908 loss_rnnt 15.548903 hw_loss 0.134949 lr 0.00047921 rank 2
2023-02-21 03:57:00,104 DEBUG TRAIN Batch 13/400 loss 13.574895 loss_att 17.801432 loss_ctc 18.990423 loss_rnnt 11.944342 hw_loss 0.118456 lr 0.00047925 rank 6
2023-02-21 03:57:00,139 DEBUG TRAIN Batch 13/400 loss 18.195353 loss_att 21.595415 loss_ctc 25.725367 loss_rnnt 16.453268 hw_loss 0.108881 lr 0.00047930 rank 5
2023-02-21 03:57:00,236 DEBUG TRAIN Batch 13/400 loss 14.029771 loss_att 15.637865 loss_ctc 18.563560 loss_rnnt 13.006136 hw_loss 0.182832 lr 0.00047931 rank 1
2023-02-21 03:57:00,369 DEBUG TRAIN Batch 13/400 loss 12.035460 loss_att 18.029747 loss_ctc 13.864550 loss_rnnt 10.552324 hw_loss 0.075747 lr 0.00047927 rank 3
2023-02-21 04:05:02,034 DEBUG TRAIN Batch 13/500 loss 11.466073 loss_att 13.783617 loss_ctc 13.882209 loss_rnnt 10.599133 hw_loss 0.152399 lr 0.00047905 rank 3
2023-02-21 04:05:02,033 DEBUG TRAIN Batch 13/500 loss 12.972545 loss_att 16.124840 loss_ctc 11.742537 loss_rnnt 12.459278 hw_loss 0.087766 lr 0.00047908 rank 5
2023-02-21 04:05:02,044 DEBUG TRAIN Batch 13/500 loss 9.243474 loss_att 12.482870 loss_ctc 13.112968 loss_rnnt 8.022180 hw_loss 0.107780 lr 0.00047903 rank 6
2023-02-21 04:05:02,090 DEBUG TRAIN Batch 13/500 loss 27.514059 loss_att 25.607136 loss_ctc 29.709555 loss_rnnt 27.558861 hw_loss 0.082221 lr 0.00047902 rank 4
2023-02-21 04:05:02,141 DEBUG TRAIN Batch 13/500 loss 15.630808 loss_att 19.926580 loss_ctc 21.217384 loss_rnnt 13.978018 hw_loss 0.091423 lr 0.00047902 rank 7
2023-02-21 04:05:02,206 DEBUG TRAIN Batch 13/500 loss 8.450814 loss_att 10.520923 loss_ctc 9.211686 loss_rnnt 7.891133 hw_loss 0.082891 lr 0.00047899 rank 2
2023-02-21 04:05:02,228 DEBUG TRAIN Batch 13/500 loss 8.596464 loss_att 13.412701 loss_ctc 10.737827 loss_rnnt 7.233982 hw_loss 0.213223 lr 0.00047909 rank 1
2023-02-21 04:05:02,266 DEBUG TRAIN Batch 13/500 loss 14.238031 loss_att 17.791468 loss_ctc 22.544050 loss_rnnt 12.351749 hw_loss 0.127736 lr 0.00047910 rank 0
2023-02-21 04:12:57,303 DEBUG TRAIN Batch 13/600 loss 14.874406 loss_att 15.909784 loss_ctc 15.717464 loss_rnnt 14.483881 hw_loss 0.133199 lr 0.00047883 rank 3
2023-02-21 04:12:57,334 DEBUG TRAIN Batch 13/600 loss 9.887018 loss_att 10.585838 loss_ctc 14.544875 loss_rnnt 9.045717 hw_loss 0.150920 lr 0.00047888 rank 0
2023-02-21 04:12:57,459 DEBUG TRAIN Batch 13/600 loss 21.510342 loss_att 21.860497 loss_ctc 25.731234 loss_rnnt 20.762215 hw_loss 0.216208 lr 0.00047881 rank 6
2023-02-21 04:12:57,461 DEBUG TRAIN Batch 13/600 loss 10.083179 loss_att 10.319656 loss_ctc 11.017975 loss_rnnt 9.780684 hw_loss 0.244802 lr 0.00047887 rank 1
2023-02-21 04:12:57,398 DEBUG TRAIN Batch 13/600 loss 12.716304 loss_att 11.959223 loss_ctc 15.850986 loss_rnnt 12.286394 hw_loss 0.306315 lr 0.00047877 rank 2
2023-02-21 04:12:57,546 DEBUG TRAIN Batch 13/600 loss 12.721374 loss_att 13.140162 loss_ctc 14.326214 loss_rnnt 12.219001 hw_loss 0.383695 lr 0.00047886 rank 5
2023-02-21 04:12:57,550 DEBUG TRAIN Batch 13/600 loss 12.741685 loss_att 14.343201 loss_ctc 15.631063 loss_rnnt 11.946838 hw_loss 0.167425 lr 0.00047880 rank 7
2023-02-21 04:12:57,551 DEBUG TRAIN Batch 13/600 loss 15.589542 loss_att 20.042498 loss_ctc 18.406181 loss_rnnt 14.279861 hw_loss 0.081635 lr 0.00047880 rank 4
2023-02-21 04:21:05,543 DEBUG TRAIN Batch 13/700 loss 15.273198 loss_att 18.930122 loss_ctc 21.671738 loss_rnnt 13.652014 hw_loss 0.068740 lr 0.00047864 rank 5
2023-02-21 04:21:05,644 DEBUG TRAIN Batch 13/700 loss 11.514985 loss_att 18.451614 loss_ctc 16.703499 loss_rnnt 9.388259 hw_loss 0.089247 lr 0.00047861 rank 3
2023-02-21 04:21:05,749 DEBUG TRAIN Batch 13/700 loss 15.479137 loss_att 18.236303 loss_ctc 15.921202 loss_rnnt 14.783489 hw_loss 0.159887 lr 0.00047865 rank 1
2023-02-21 04:21:05,775 DEBUG TRAIN Batch 13/700 loss 13.815311 loss_att 23.014479 loss_ctc 17.213329 loss_rnnt 11.491141 hw_loss 0.058627 lr 0.00047855 rank 2
2023-02-21 04:21:05,876 DEBUG TRAIN Batch 13/700 loss 4.651060 loss_att 10.067675 loss_ctc 4.831974 loss_rnnt 3.426428 hw_loss 0.219727 lr 0.00047859 rank 6
2023-02-21 04:21:05,881 DEBUG TRAIN Batch 13/700 loss 8.948091 loss_att 13.643232 loss_ctc 12.302562 loss_rnnt 7.527572 hw_loss 0.064178 lr 0.00047866 rank 0
2023-02-21 04:21:05,835 DEBUG TRAIN Batch 13/700 loss 10.472810 loss_att 16.911436 loss_ctc 12.355766 loss_rnnt 8.848661 hw_loss 0.160053 lr 0.00047858 rank 4
2023-02-21 04:21:05,890 DEBUG TRAIN Batch 13/700 loss 20.758137 loss_att 24.323721 loss_ctc 30.218763 loss_rnnt 18.708284 hw_loss 0.141226 lr 0.00047858 rank 7
2023-02-21 04:29:09,167 DEBUG TRAIN Batch 13/800 loss 14.679841 loss_att 20.093964 loss_ctc 16.318741 loss_rnnt 13.309775 hw_loss 0.128854 lr 0.00047839 rank 3
2023-02-21 04:29:09,222 DEBUG TRAIN Batch 13/800 loss 9.374475 loss_att 13.365556 loss_ctc 12.857546 loss_rnnt 8.071404 hw_loss 0.075836 lr 0.00047838 rank 6
2023-02-21 04:29:09,319 DEBUG TRAIN Batch 13/800 loss 4.284753 loss_att 8.635609 loss_ctc 6.777795 loss_rnnt 2.979085 hw_loss 0.193297 lr 0.00047844 rank 0
2023-02-21 04:29:09,447 DEBUG TRAIN Batch 13/800 loss 10.574069 loss_att 15.221516 loss_ctc 11.756030 loss_rnnt 9.473930 hw_loss 0.024479 lr 0.00047836 rank 7
2023-02-21 04:29:09,466 DEBUG TRAIN Batch 13/800 loss 17.287582 loss_att 18.818802 loss_ctc 21.398092 loss_rnnt 16.419617 hw_loss 0.025599 lr 0.00047833 rank 2
2023-02-21 04:29:09,477 DEBUG TRAIN Batch 13/800 loss 8.693180 loss_att 15.278706 loss_ctc 11.994896 loss_rnnt 6.887442 hw_loss 0.090759 lr 0.00047843 rank 1
2023-02-21 04:29:09,495 DEBUG TRAIN Batch 13/800 loss 13.835963 loss_att 18.790838 loss_ctc 16.616240 loss_rnnt 12.439517 hw_loss 0.065192 lr 0.00047836 rank 4
2023-02-21 04:29:09,678 DEBUG TRAIN Batch 13/800 loss 9.050559 loss_att 13.805990 loss_ctc 13.107635 loss_rnnt 7.486776 hw_loss 0.134538 lr 0.00047842 rank 5
2023-02-21 04:37:07,248 DEBUG TRAIN Batch 13/900 loss 14.168786 loss_att 20.449001 loss_ctc 21.668343 loss_rnnt 11.834509 hw_loss 0.146802 lr 0.00047814 rank 7
2023-02-21 04:37:07,252 DEBUG TRAIN Batch 13/900 loss 27.543076 loss_att 31.179230 loss_ctc 30.012888 loss_rnnt 26.378448 hw_loss 0.202666 lr 0.00047820 rank 5
2023-02-21 04:37:07,379 DEBUG TRAIN Batch 13/900 loss 21.052778 loss_att 23.918314 loss_ctc 24.332554 loss_rnnt 19.957716 hw_loss 0.158719 lr 0.00047822 rank 0
2023-02-21 04:37:07,382 DEBUG TRAIN Batch 13/900 loss 15.325670 loss_att 17.320557 loss_ctc 18.562382 loss_rnnt 14.481768 hw_loss 0.025055 lr 0.00047815 rank 4
2023-02-21 04:37:07,433 DEBUG TRAIN Batch 13/900 loss 5.288304 loss_att 11.747607 loss_ctc 6.107769 loss_rnnt 3.818620 hw_loss 0.128554 lr 0.00047817 rank 3
2023-02-21 04:37:07,498 DEBUG TRAIN Batch 13/900 loss 11.132651 loss_att 13.403904 loss_ctc 14.182130 loss_rnnt 10.167885 hw_loss 0.194847 lr 0.00047821 rank 1
2023-02-21 04:37:07,519 DEBUG TRAIN Batch 13/900 loss 9.712344 loss_att 12.769789 loss_ctc 12.043894 loss_rnnt 8.683054 hw_loss 0.200490 lr 0.00047816 rank 6
2023-02-21 04:37:07,541 DEBUG TRAIN Batch 13/900 loss 7.165984 loss_att 10.451402 loss_ctc 11.016712 loss_rnnt 5.939834 hw_loss 0.104317 lr 0.00047811 rank 2
2023-02-21 04:45:20,731 DEBUG TRAIN Batch 13/1000 loss 10.478407 loss_att 13.868383 loss_ctc 12.494686 loss_rnnt 9.447939 hw_loss 0.156817 lr 0.00047795 rank 3
2023-02-21 04:45:20,770 DEBUG TRAIN Batch 13/1000 loss 13.569081 loss_att 18.091488 loss_ctc 18.347179 loss_rnnt 11.938179 hw_loss 0.167516 lr 0.00047793 rank 7
2023-02-21 04:45:20,810 DEBUG TRAIN Batch 13/1000 loss 11.425759 loss_att 14.307640 loss_ctc 11.943243 loss_rnnt 10.764353 hw_loss 0.030059 lr 0.00047800 rank 1
2023-02-21 04:45:20,921 DEBUG TRAIN Batch 13/1000 loss 10.251060 loss_att 18.560034 loss_ctc 14.026569 loss_rnnt 8.030917 hw_loss 0.103024 lr 0.00047798 rank 5
2023-02-21 04:45:21,023 DEBUG TRAIN Batch 13/1000 loss 13.693907 loss_att 20.555376 loss_ctc 19.029528 loss_rnnt 11.594030 hw_loss 0.030312 lr 0.00047789 rank 2
2023-02-21 04:45:21,040 DEBUG TRAIN Batch 13/1000 loss 14.540279 loss_att 15.813377 loss_ctc 22.508339 loss_rnnt 13.135563 hw_loss 0.164417 lr 0.00047793 rank 4
2023-02-21 04:45:21,104 DEBUG TRAIN Batch 13/1000 loss 17.726259 loss_att 20.685186 loss_ctc 23.279104 loss_rnnt 16.307737 hw_loss 0.161917 lr 0.00047801 rank 0
2023-02-21 04:45:21,394 DEBUG TRAIN Batch 13/1000 loss 13.516316 loss_att 19.685169 loss_ctc 16.789200 loss_rnnt 11.808305 hw_loss 0.070980 lr 0.00047794 rank 6
2023-02-21 04:53:35,617 DEBUG TRAIN Batch 13/1100 loss 11.724310 loss_att 14.326644 loss_ctc 17.991203 loss_rnnt 10.322099 hw_loss 0.086549 lr 0.00047773 rank 3
2023-02-21 04:53:35,663 DEBUG TRAIN Batch 13/1100 loss 25.552555 loss_att 29.803612 loss_ctc 32.068069 loss_rnnt 23.741026 hw_loss 0.173599 lr 0.00047772 rank 6
2023-02-21 04:53:35,754 DEBUG TRAIN Batch 13/1100 loss 13.489637 loss_att 14.366096 loss_ctc 14.664494 loss_rnnt 13.103521 hw_loss 0.101580 lr 0.00047778 rank 1
2023-02-21 04:53:35,820 DEBUG TRAIN Batch 13/1100 loss 10.876268 loss_att 13.579683 loss_ctc 15.255163 loss_rnnt 9.666894 hw_loss 0.159072 lr 0.00047777 rank 5
2023-02-21 04:53:35,954 DEBUG TRAIN Batch 13/1100 loss 14.207535 loss_att 16.197628 loss_ctc 19.029385 loss_rnnt 13.070251 hw_loss 0.180658 lr 0.00047771 rank 4
2023-02-21 04:53:35,981 DEBUG TRAIN Batch 13/1100 loss 16.293879 loss_att 18.457645 loss_ctc 17.791393 loss_rnnt 15.596122 hw_loss 0.122506 lr 0.00047779 rank 0
2023-02-21 04:53:36,009 DEBUG TRAIN Batch 13/1100 loss 17.668133 loss_att 16.575918 loss_ctc 18.570774 loss_rnnt 17.641956 hw_loss 0.233002 lr 0.00047771 rank 7
2023-02-21 04:53:36,075 DEBUG TRAIN Batch 13/1100 loss 9.091138 loss_att 12.042904 loss_ctc 11.002843 loss_rnnt 8.143900 hw_loss 0.191234 lr 0.00047768 rank 2
2023-02-21 05:01:32,607 DEBUG TRAIN Batch 13/1200 loss 12.467516 loss_att 16.386955 loss_ctc 15.097647 loss_rnnt 11.220978 hw_loss 0.209938 lr 0.00047757 rank 0
2023-02-21 05:01:32,651 DEBUG TRAIN Batch 13/1200 loss 16.018181 loss_att 18.107998 loss_ctc 18.995800 loss_rnnt 15.114485 hw_loss 0.166346 lr 0.00047755 rank 5
2023-02-21 05:01:32,658 DEBUG TRAIN Batch 13/1200 loss 16.003120 loss_att 17.222532 loss_ctc 19.781250 loss_rnnt 15.178658 hw_loss 0.144059 lr 0.00047746 rank 2
2023-02-21 05:01:32,769 DEBUG TRAIN Batch 13/1200 loss 15.411675 loss_att 17.586786 loss_ctc 18.617437 loss_rnnt 14.465891 hw_loss 0.156237 lr 0.00047749 rank 7
2023-02-21 05:01:32,770 DEBUG TRAIN Batch 13/1200 loss 12.549543 loss_att 16.516527 loss_ctc 14.248018 loss_rnnt 11.460018 hw_loss 0.130623 lr 0.00047756 rank 1
2023-02-21 05:01:32,803 DEBUG TRAIN Batch 13/1200 loss 12.607523 loss_att 13.808569 loss_ctc 15.790171 loss_rnnt 11.843271 hw_loss 0.186919 lr 0.00047750 rank 6
2023-02-21 05:01:32,918 DEBUG TRAIN Batch 13/1200 loss 6.941585 loss_att 10.157572 loss_ctc 8.050627 loss_rnnt 6.072528 hw_loss 0.146226 lr 0.00047749 rank 4
2023-02-21 05:01:33,082 DEBUG TRAIN Batch 13/1200 loss 8.775069 loss_att 14.286541 loss_ctc 11.094808 loss_rnnt 7.270830 hw_loss 0.173712 lr 0.00047752 rank 3
2023-02-21 05:09:35,452 DEBUG TRAIN Batch 13/1300 loss 18.080563 loss_att 21.567154 loss_ctc 22.510832 loss_rnnt 16.779932 hw_loss 0.023643 lr 0.00047730 rank 3
2023-02-21 05:09:35,569 DEBUG TRAIN Batch 13/1300 loss 10.493622 loss_att 14.273498 loss_ctc 16.946093 loss_rnnt 8.864756 hw_loss 0.023553 lr 0.00047735 rank 0
2023-02-21 05:09:35,594 DEBUG TRAIN Batch 13/1300 loss 7.998401 loss_att 13.010235 loss_ctc 9.146681 loss_rnnt 6.790649 hw_loss 0.098027 lr 0.00047724 rank 2
2023-02-21 05:09:35,635 DEBUG TRAIN Batch 13/1300 loss 9.218758 loss_att 10.739161 loss_ctc 10.248518 loss_rnnt 8.648529 hw_loss 0.241587 lr 0.00047733 rank 5
2023-02-21 05:09:35,642 DEBUG TRAIN Batch 13/1300 loss 11.696289 loss_att 11.353472 loss_ctc 12.388050 loss_rnnt 11.613397 hw_loss 0.111039 lr 0.00047728 rank 6
2023-02-21 05:09:35,723 DEBUG TRAIN Batch 13/1300 loss 11.445846 loss_att 12.205944 loss_ctc 18.304382 loss_rnnt 10.347922 hw_loss 0.058934 lr 0.00047727 rank 7
2023-02-21 05:09:35,828 DEBUG TRAIN Batch 13/1300 loss 8.164602 loss_att 12.322156 loss_ctc 11.905325 loss_rnnt 6.768655 hw_loss 0.123135 lr 0.00047734 rank 1
2023-02-21 05:09:36,176 DEBUG TRAIN Batch 13/1300 loss 11.408780 loss_att 17.530334 loss_ctc 14.905678 loss_rnnt 9.705599 hw_loss 0.023658 lr 0.00047727 rank 4
2023-02-21 05:18:01,757 DEBUG TRAIN Batch 13/1400 loss 6.929034 loss_att 12.210917 loss_ctc 11.924865 loss_rnnt 5.129993 hw_loss 0.143540 lr 0.00047712 rank 1
2023-02-21 05:18:01,867 DEBUG TRAIN Batch 13/1400 loss 12.653161 loss_att 18.995371 loss_ctc 16.019339 loss_rnnt 10.894178 hw_loss 0.078223 lr 0.00047706 rank 4
2023-02-21 05:18:01,926 DEBUG TRAIN Batch 13/1400 loss 21.925182 loss_att 22.633583 loss_ctc 26.095863 loss_rnnt 21.155647 hw_loss 0.134556 lr 0.00047707 rank 6
2023-02-21 05:18:01,949 DEBUG TRAIN Batch 13/1400 loss 7.466218 loss_att 11.989250 loss_ctc 8.318611 loss_rnnt 6.393792 hw_loss 0.101563 lr 0.00047708 rank 3
2023-02-21 05:18:01,969 DEBUG TRAIN Batch 13/1400 loss 10.344599 loss_att 12.172972 loss_ctc 12.539478 loss_rnnt 9.643950 hw_loss 0.079358 lr 0.00047713 rank 0
2023-02-21 05:18:01,971 DEBUG TRAIN Batch 13/1400 loss 24.985809 loss_att 23.587284 loss_ctc 32.718735 loss_rnnt 24.175245 hw_loss 0.111020 lr 0.00047702 rank 2
2023-02-21 05:18:02,017 DEBUG TRAIN Batch 13/1400 loss 23.840525 loss_att 27.530180 loss_ctc 32.719318 loss_rnnt 21.831444 hw_loss 0.163704 lr 0.00047705 rank 7
2023-02-21 05:18:02,403 DEBUG TRAIN Batch 13/1400 loss 10.477939 loss_att 16.446281 loss_ctc 9.285617 loss_rnnt 9.429429 hw_loss 0.025910 lr 0.00047711 rank 5
2023-02-21 05:26:03,798 DEBUG TRAIN Batch 13/1500 loss 8.514910 loss_att 11.503746 loss_ctc 7.992061 loss_rnnt 7.916405 hw_loss 0.132096 lr 0.00047681 rank 2
2023-02-21 05:26:03,907 DEBUG TRAIN Batch 13/1500 loss 9.905477 loss_att 12.582455 loss_ctc 11.321486 loss_rnnt 9.080538 hw_loss 0.188890 lr 0.00047690 rank 5
2023-02-21 05:26:03,936 DEBUG TRAIN Batch 13/1500 loss 7.001715 loss_att 10.268925 loss_ctc 7.558761 loss_rnnt 6.218904 hw_loss 0.103304 lr 0.00047692 rank 0
2023-02-21 05:26:04,096 DEBUG TRAIN Batch 13/1500 loss 9.800999 loss_att 13.252704 loss_ctc 10.768432 loss_rnnt 8.919780 hw_loss 0.116039 lr 0.00047691 rank 1
2023-02-21 05:26:04,215 DEBUG TRAIN Batch 13/1500 loss 5.691067 loss_att 9.210112 loss_ctc 7.473960 loss_rnnt 4.734391 hw_loss 0.028402 lr 0.00047684 rank 4
2023-02-21 05:26:04,254 DEBUG TRAIN Batch 13/1500 loss 12.263594 loss_att 23.322382 loss_ctc 16.335241 loss_rnnt 9.464374 hw_loss 0.083579 lr 0.00047685 rank 6
2023-02-21 05:26:04,400 DEBUG TRAIN Batch 13/1500 loss 4.340080 loss_att 8.765965 loss_ctc 4.539393 loss_rnnt 3.313998 hw_loss 0.214368 lr 0.00047684 rank 7
2023-02-21 05:26:04,423 DEBUG TRAIN Batch 13/1500 loss 8.669554 loss_att 14.489308 loss_ctc 11.017010 loss_rnnt 7.101348 hw_loss 0.171115 lr 0.00047686 rank 3
2023-02-21 05:34:05,126 DEBUG TRAIN Batch 13/1600 loss 10.476274 loss_att 16.394533 loss_ctc 14.876618 loss_rnnt 8.666272 hw_loss 0.074317 lr 0.00047662 rank 4
2023-02-21 05:34:05,193 DEBUG TRAIN Batch 13/1600 loss 15.282888 loss_att 25.184181 loss_ctc 19.614937 loss_rnnt 12.678020 hw_loss 0.088135 lr 0.00047665 rank 3
2023-02-21 05:34:05,211 DEBUG TRAIN Batch 13/1600 loss 15.679710 loss_att 16.327053 loss_ctc 18.861637 loss_rnnt 15.061783 hw_loss 0.120378 lr 0.00047659 rank 2
2023-02-21 05:34:05,311 DEBUG TRAIN Batch 13/1600 loss 22.729141 loss_att 26.344660 loss_ctc 27.740677 loss_rnnt 21.322182 hw_loss 0.029348 lr 0.00047662 rank 7
2023-02-21 05:34:05,297 DEBUG TRAIN Batch 13/1600 loss 15.330657 loss_att 20.220886 loss_ctc 20.157454 loss_rnnt 13.627737 hw_loss 0.152443 lr 0.00047669 rank 1
2023-02-21 05:34:05,346 DEBUG TRAIN Batch 13/1600 loss 17.476721 loss_att 22.474674 loss_ctc 22.212542 loss_rnnt 15.785837 hw_loss 0.112221 lr 0.00047663 rank 6
2023-02-21 05:34:05,638 DEBUG TRAIN Batch 13/1600 loss 14.204973 loss_att 19.465923 loss_ctc 17.485451 loss_rnnt 12.691083 hw_loss 0.045570 lr 0.00047668 rank 5
2023-02-21 05:34:05,782 DEBUG TRAIN Batch 13/1600 loss 12.519168 loss_att 17.444761 loss_ctc 15.761272 loss_rnnt 11.042694 hw_loss 0.110764 lr 0.00047670 rank 0
2023-02-21 05:42:15,397 DEBUG TRAIN Batch 13/1700 loss 10.449921 loss_att 14.793419 loss_ctc 13.419035 loss_rnnt 9.125217 hw_loss 0.112727 lr 0.00047637 rank 2
2023-02-21 05:42:15,423 DEBUG TRAIN Batch 13/1700 loss 23.289307 loss_att 24.969383 loss_ctc 30.472080 loss_rnnt 21.936949 hw_loss 0.109949 lr 0.00047646 rank 5
2023-02-21 05:42:15,468 DEBUG TRAIN Batch 13/1700 loss 13.519329 loss_att 18.520626 loss_ctc 16.734064 loss_rnnt 12.046676 hw_loss 0.082054 lr 0.00047647 rank 1
2023-02-21 05:42:15,472 DEBUG TRAIN Batch 13/1700 loss 13.772161 loss_att 17.730436 loss_ctc 16.821150 loss_rnnt 12.483597 hw_loss 0.169455 lr 0.00047642 rank 6
2023-02-21 05:42:15,502 DEBUG TRAIN Batch 13/1700 loss 12.330408 loss_att 15.596895 loss_ctc 15.922741 loss_rnnt 11.127528 hw_loss 0.132383 lr 0.00047641 rank 4
2023-02-21 05:42:15,632 DEBUG TRAIN Batch 13/1700 loss 12.426229 loss_att 16.403261 loss_ctc 15.290196 loss_rnnt 11.180817 hw_loss 0.127770 lr 0.00047643 rank 3
2023-02-21 05:42:15,735 DEBUG TRAIN Batch 13/1700 loss 13.714811 loss_att 18.455660 loss_ctc 17.070551 loss_rnnt 12.279337 hw_loss 0.074758 lr 0.00047640 rank 7
2023-02-21 05:42:15,806 DEBUG TRAIN Batch 13/1700 loss 15.152820 loss_att 17.605186 loss_ctc 19.636278 loss_rnnt 14.002769 hw_loss 0.115845 lr 0.00047648 rank 0
2023-02-21 05:50:29,146 DEBUG TRAIN Batch 13/1800 loss 15.008629 loss_att 16.847118 loss_ctc 16.072998 loss_rnnt 14.392055 hw_loss 0.200551 lr 0.00047625 rank 5
2023-02-21 05:50:29,162 DEBUG TRAIN Batch 13/1800 loss 18.801346 loss_att 21.518486 loss_ctc 21.852839 loss_rnnt 17.802986 hw_loss 0.090121 lr 0.00047626 rank 1
2023-02-21 05:50:29,301 DEBUG TRAIN Batch 13/1800 loss 9.795190 loss_att 12.436907 loss_ctc 11.784038 loss_rnnt 8.912094 hw_loss 0.167950 lr 0.00047619 rank 7
2023-02-21 05:50:29,360 DEBUG TRAIN Batch 13/1800 loss 14.948936 loss_att 18.190319 loss_ctc 18.080091 loss_rnnt 13.816202 hw_loss 0.125566 lr 0.00047621 rank 3
2023-02-21 05:50:29,369 DEBUG TRAIN Batch 13/1800 loss 17.021257 loss_att 20.990448 loss_ctc 14.253839 loss_rnnt 16.542023 hw_loss 0.101970 lr 0.00047619 rank 4
2023-02-21 05:50:29,378 DEBUG TRAIN Batch 13/1800 loss 13.484326 loss_att 16.939606 loss_ctc 16.129995 loss_rnnt 12.387640 hw_loss 0.099139 lr 0.00047616 rank 2
2023-02-21 05:50:29,385 DEBUG TRAIN Batch 13/1800 loss 10.599899 loss_att 10.579761 loss_ctc 11.509404 loss_rnnt 10.444415 hw_loss 0.071711 lr 0.00047627 rank 0
2023-02-21 05:50:29,424 DEBUG TRAIN Batch 13/1800 loss 16.231482 loss_att 18.519547 loss_ctc 18.729715 loss_rnnt 15.356877 hw_loss 0.157299 lr 0.00047620 rank 6
2023-02-21 05:58:23,376 DEBUG TRAIN Batch 13/1900 loss 13.654586 loss_att 14.337065 loss_ctc 17.016222 loss_rnnt 12.967734 hw_loss 0.191509 lr 0.00047599 rank 6
2023-02-21 05:58:23,407 DEBUG TRAIN Batch 13/1900 loss 18.711756 loss_att 20.055344 loss_ctc 27.445456 loss_rnnt 17.251348 hw_loss 0.050995 lr 0.00047603 rank 5
2023-02-21 05:58:23,444 DEBUG TRAIN Batch 13/1900 loss 21.241560 loss_att 21.285637 loss_ctc 24.769039 loss_rnnt 20.704779 hw_loss 0.108064 lr 0.00047594 rank 2
2023-02-21 05:58:23,592 DEBUG TRAIN Batch 13/1900 loss 11.494980 loss_att 12.624537 loss_ctc 13.700156 loss_rnnt 10.847571 hw_loss 0.239016 lr 0.00047597 rank 7
2023-02-21 05:58:23,681 DEBUG TRAIN Batch 13/1900 loss 6.998621 loss_att 6.300014 loss_ctc 7.935169 loss_rnnt 6.841940 hw_loss 0.321616 lr 0.00047600 rank 3
2023-02-21 05:58:23,815 DEBUG TRAIN Batch 13/1900 loss 6.180131 loss_att 9.517425 loss_ctc 10.101856 loss_rnnt 4.928253 hw_loss 0.115355 lr 0.00047597 rank 4
2023-02-21 05:58:23,961 DEBUG TRAIN Batch 13/1900 loss 14.799632 loss_att 17.602711 loss_ctc 15.585707 loss_rnnt 14.060289 hw_loss 0.138592 lr 0.00047605 rank 0
2023-02-21 05:58:24,046 DEBUG TRAIN Batch 13/1900 loss 8.213827 loss_att 8.613915 loss_ctc 8.614206 loss_rnnt 7.954836 hw_loss 0.235481 lr 0.00047604 rank 1
2023-02-21 06:06:27,677 DEBUG TRAIN Batch 13/2000 loss 19.944546 loss_att 23.176825 loss_ctc 22.642294 loss_rnnt 18.924107 hw_loss 0.026780 lr 0.00047584 rank 0
2023-02-21 06:06:27,821 DEBUG TRAIN Batch 13/2000 loss 12.364853 loss_att 13.822224 loss_ctc 14.767885 loss_rnnt 11.716792 hw_loss 0.067842 lr 0.00047583 rank 1
2023-02-21 06:06:27,856 DEBUG TRAIN Batch 13/2000 loss 11.068686 loss_att 16.007275 loss_ctc 12.993561 loss_rnnt 9.749885 hw_loss 0.139560 lr 0.00047576 rank 7
2023-02-21 06:06:27,992 DEBUG TRAIN Batch 13/2000 loss 10.736079 loss_att 12.007052 loss_ctc 11.957260 loss_rnnt 10.304380 hw_loss 0.027526 lr 0.00047573 rank 2
2023-02-21 06:06:28,001 DEBUG TRAIN Batch 13/2000 loss 14.435268 loss_att 17.524059 loss_ctc 17.121820 loss_rnnt 13.444967 hw_loss 0.026881 lr 0.00047578 rank 3
2023-02-21 06:06:27,967 DEBUG TRAIN Batch 13/2000 loss 5.998429 loss_att 11.099504 loss_ctc 5.584409 loss_rnnt 4.973255 hw_loss 0.112804 lr 0.00047582 rank 5
2023-02-21 06:06:28,000 DEBUG TRAIN Batch 13/2000 loss 8.564755 loss_att 13.825436 loss_ctc 11.751274 loss_rnnt 7.007076 hw_loss 0.151264 lr 0.00047577 rank 6
2023-02-21 06:06:28,102 DEBUG TRAIN Batch 13/2000 loss 21.319468 loss_att 27.732563 loss_ctc 32.362701 loss_rnnt 18.513475 hw_loss 0.095517 lr 0.00047576 rank 4
2023-02-21 06:14:39,435 DEBUG TRAIN Batch 13/2100 loss 10.237885 loss_att 16.033760 loss_ctc 8.728222 loss_rnnt 9.240461 hw_loss 0.074130 lr 0.00047561 rank 1
2023-02-21 06:14:39,518 DEBUG TRAIN Batch 13/2100 loss 3.329741 loss_att 9.496090 loss_ctc 5.406356 loss_rnnt 1.793855 hw_loss 0.048252 lr 0.00047554 rank 4
2023-02-21 06:14:39,666 DEBUG TRAIN Batch 13/2100 loss 11.398205 loss_att 20.536755 loss_ctc 16.083227 loss_rnnt 8.875786 hw_loss 0.131324 lr 0.00047557 rank 3
2023-02-21 06:14:39,628 DEBUG TRAIN Batch 13/2100 loss 5.070643 loss_att 9.324859 loss_ctc 5.138017 loss_rnnt 4.137114 hw_loss 0.138194 lr 0.00047554 rank 7
2023-02-21 06:14:39,732 DEBUG TRAIN Batch 13/2100 loss 11.253338 loss_att 14.386510 loss_ctc 15.469802 loss_rnnt 10.030034 hw_loss 0.064639 lr 0.00047555 rank 6
2023-02-21 06:14:39,830 DEBUG TRAIN Batch 13/2100 loss 8.874010 loss_att 12.968517 loss_ctc 12.101291 loss_rnnt 7.587674 hw_loss 0.069618 lr 0.00047560 rank 5
2023-02-21 06:14:40,060 DEBUG TRAIN Batch 13/2100 loss 5.683526 loss_att 8.580303 loss_ctc 8.543247 loss_rnnt 4.661101 hw_loss 0.115825 lr 0.00047562 rank 0
2023-02-21 06:14:40,226 DEBUG TRAIN Batch 13/2100 loss 20.039562 loss_att 22.797966 loss_ctc 27.279631 loss_rnnt 18.462421 hw_loss 0.112719 lr 0.00047551 rank 2
2023-02-21 06:22:46,097 DEBUG TRAIN Batch 13/2200 loss 13.863596 loss_att 15.219099 loss_ctc 16.111620 loss_rnnt 13.279109 hw_loss 0.025592 lr 0.00047533 rank 7
2023-02-21 06:22:46,152 DEBUG TRAIN Batch 13/2200 loss 12.331837 loss_att 18.439495 loss_ctc 13.951795 loss_rnnt 10.772739 hw_loss 0.227945 lr 0.00047533 rank 4
2023-02-21 06:22:46,162 DEBUG TRAIN Batch 13/2200 loss 9.504340 loss_att 13.681513 loss_ctc 12.953843 loss_rnnt 8.178408 hw_loss 0.057310 lr 0.00047541 rank 0
2023-02-21 06:22:46,261 DEBUG TRAIN Batch 13/2200 loss 15.724768 loss_att 20.378553 loss_ctc 24.392464 loss_rnnt 13.519773 hw_loss 0.222273 lr 0.00047534 rank 6
2023-02-21 06:22:46,287 DEBUG TRAIN Batch 13/2200 loss 6.863705 loss_att 9.549532 loss_ctc 7.447494 loss_rnnt 6.212402 hw_loss 0.068062 lr 0.00047540 rank 1
2023-02-21 06:22:46,398 DEBUG TRAIN Batch 13/2200 loss 10.535204 loss_att 16.410891 loss_ctc 15.034725 loss_rnnt 8.661213 hw_loss 0.185472 lr 0.00047535 rank 3
2023-02-21 06:22:46,424 DEBUG TRAIN Batch 13/2200 loss 10.649336 loss_att 13.927282 loss_ctc 15.113086 loss_rnnt 9.351054 hw_loss 0.089112 lr 0.00047538 rank 5
2023-02-21 06:22:46,425 DEBUG TRAIN Batch 13/2200 loss 13.907829 loss_att 18.086351 loss_ctc 18.166246 loss_rnnt 12.456324 hw_loss 0.090022 lr 0.00047530 rank 2
2023-02-21 06:30:54,751 DEBUG TRAIN Batch 13/2300 loss 6.875791 loss_att 11.267938 loss_ctc 10.150282 loss_rnnt 5.544175 hw_loss 0.031101 lr 0.00047508 rank 2
2023-02-21 06:30:54,761 DEBUG TRAIN Batch 13/2300 loss 12.206928 loss_att 14.617916 loss_ctc 15.175304 loss_rnnt 11.293361 hw_loss 0.066724 lr 0.00047517 rank 5
2023-02-21 06:30:54,773 DEBUG TRAIN Batch 13/2300 loss 10.001173 loss_att 15.529337 loss_ctc 13.504362 loss_rnnt 8.363420 hw_loss 0.121928 lr 0.00047511 rank 4
2023-02-21 06:30:54,807 DEBUG TRAIN Batch 13/2300 loss 21.258108 loss_att 21.252365 loss_ctc 25.623539 loss_rnnt 20.590157 hw_loss 0.163207 lr 0.00047513 rank 6
2023-02-21 06:30:54,968 DEBUG TRAIN Batch 13/2300 loss 16.010921 loss_att 20.679525 loss_ctc 21.565636 loss_rnnt 14.268835 hw_loss 0.127008 lr 0.00047514 rank 3
2023-02-21 06:30:54,994 DEBUG TRAIN Batch 13/2300 loss 23.700180 loss_att 25.967070 loss_ctc 30.314564 loss_rnnt 22.317890 hw_loss 0.088110 lr 0.00047511 rank 7
2023-02-21 06:30:55,073 DEBUG TRAIN Batch 13/2300 loss 24.236456 loss_att 26.844433 loss_ctc 30.606174 loss_rnnt 22.849430 hw_loss 0.030249 lr 0.00047519 rank 0
2023-02-21 06:30:54,976 DEBUG TRAIN Batch 13/2300 loss 7.944419 loss_att 13.933910 loss_ctc 8.815948 loss_rnnt 6.573721 hw_loss 0.106119 lr 0.00047518 rank 1
2023-02-21 06:39:15,915 DEBUG TRAIN Batch 13/2400 loss 10.741545 loss_att 12.018173 loss_ctc 17.806366 loss_rnnt 9.452631 hw_loss 0.171774 lr 0.00047496 rank 5
2023-02-21 06:39:15,922 DEBUG TRAIN Batch 13/2400 loss 13.011081 loss_att 19.569227 loss_ctc 13.305131 loss_rnnt 11.605284 hw_loss 0.103051 lr 0.00047497 rank 1
2023-02-21 06:39:15,980 DEBUG TRAIN Batch 13/2400 loss 19.645067 loss_att 20.926310 loss_ctc 25.991266 loss_rnnt 18.494688 hw_loss 0.089944 lr 0.00047492 rank 3
2023-02-21 06:39:15,999 DEBUG TRAIN Batch 13/2400 loss 15.354076 loss_att 15.967337 loss_ctc 17.207558 loss_rnnt 14.929435 hw_loss 0.102861 lr 0.00047490 rank 4
2023-02-21 06:39:16,161 DEBUG TRAIN Batch 13/2400 loss 10.934641 loss_att 12.986208 loss_ctc 14.605774 loss_rnnt 9.958280 hw_loss 0.143556 lr 0.00047487 rank 2
2023-02-21 06:39:16,175 DEBUG TRAIN Batch 13/2400 loss 16.839947 loss_att 18.400421 loss_ctc 21.563210 loss_rnnt 15.812600 hw_loss 0.160279 lr 0.00047490 rank 7
2023-02-21 06:39:16,358 DEBUG TRAIN Batch 13/2400 loss 12.663631 loss_att 15.186533 loss_ctc 15.613420 loss_rnnt 11.662780 hw_loss 0.193062 lr 0.00047491 rank 6
2023-02-21 06:39:16,469 DEBUG TRAIN Batch 13/2400 loss 12.702242 loss_att 12.008062 loss_ctc 17.201933 loss_rnnt 12.152930 hw_loss 0.165354 lr 0.00047498 rank 0
2023-02-21 06:47:46,387 DEBUG TRAIN Batch 13/2500 loss 9.867412 loss_att 12.525711 loss_ctc 13.616067 loss_rnnt 8.684357 hw_loss 0.284203 lr 0.00047471 rank 3
2023-02-21 06:47:46,569 DEBUG TRAIN Batch 13/2500 loss 13.945939 loss_att 14.941437 loss_ctc 13.993843 loss_rnnt 13.644866 hw_loss 0.179225 lr 0.00047468 rank 7
2023-02-21 06:47:46,648 DEBUG TRAIN Batch 13/2500 loss 17.678993 loss_att 25.466457 loss_ctc 19.730309 loss_rnnt 15.807857 hw_loss 0.075253 lr 0.00047474 rank 5
2023-02-21 06:47:46,695 DEBUG TRAIN Batch 13/2500 loss 14.477518 loss_att 18.306240 loss_ctc 17.602961 loss_rnnt 13.279065 hw_loss 0.029968 lr 0.00047470 rank 6
2023-02-21 06:47:46,721 DEBUG TRAIN Batch 13/2500 loss 7.609020 loss_att 7.670724 loss_ctc 8.966092 loss_rnnt 7.225106 hw_loss 0.357431 lr 0.00047476 rank 0
2023-02-21 06:47:46,758 DEBUG TRAIN Batch 13/2500 loss 23.102676 loss_att 22.181889 loss_ctc 26.465088 loss_rnnt 22.686857 hw_loss 0.284351 lr 0.00047469 rank 4
2023-02-21 06:47:46,793 DEBUG TRAIN Batch 13/2500 loss 15.282383 loss_att 14.303896 loss_ctc 16.784231 loss_rnnt 15.188529 hw_loss 0.167446 lr 0.00047465 rank 2
2023-02-21 06:47:47,076 DEBUG TRAIN Batch 13/2500 loss 14.189753 loss_att 14.951893 loss_ctc 16.320438 loss_rnnt 13.721015 hw_loss 0.060410 lr 0.00047475 rank 1
2023-02-21 06:55:54,450 DEBUG TRAIN Batch 13/2600 loss 11.356648 loss_att 13.158356 loss_ctc 14.037085 loss_rnnt 10.556647 hw_loss 0.154255 lr 0.00047448 rank 6
2023-02-21 06:55:54,469 DEBUG TRAIN Batch 13/2600 loss 30.164865 loss_att 34.061272 loss_ctc 39.173775 loss_rnnt 28.130524 hw_loss 0.101008 lr 0.00047454 rank 1
2023-02-21 06:55:54,573 DEBUG TRAIN Batch 13/2600 loss 7.099443 loss_att 12.426289 loss_ctc 8.735648 loss_rnnt 5.801578 hw_loss 0.026882 lr 0.00047447 rank 7
2023-02-21 06:55:54,593 DEBUG TRAIN Batch 13/2600 loss 23.461226 loss_att 25.944809 loss_ctc 32.059799 loss_rnnt 21.770781 hw_loss 0.088597 lr 0.00047453 rank 5
2023-02-21 06:55:54,615 DEBUG TRAIN Batch 13/2600 loss 12.795598 loss_att 13.698471 loss_ctc 16.062601 loss_rnnt 12.098453 hw_loss 0.151820 lr 0.00047447 rank 4
2023-02-21 06:55:54,753 DEBUG TRAIN Batch 13/2600 loss 25.207394 loss_att 25.770302 loss_ctc 33.870518 loss_rnnt 23.903687 hw_loss 0.067579 lr 0.00047444 rank 2
2023-02-21 06:55:54,797 DEBUG TRAIN Batch 13/2600 loss 16.001085 loss_att 20.055902 loss_ctc 21.998755 loss_rnnt 14.267828 hw_loss 0.229884 lr 0.00047450 rank 3
2023-02-21 06:55:54,829 DEBUG TRAIN Batch 13/2600 loss 5.169751 loss_att 11.316858 loss_ctc 9.414598 loss_rnnt 3.328938 hw_loss 0.085147 lr 0.00047455 rank 0
2023-02-21 07:04:09,305 DEBUG TRAIN Batch 13/2700 loss 20.936409 loss_att 21.597019 loss_ctc 29.551626 loss_rnnt 19.581303 hw_loss 0.139289 lr 0.00047432 rank 1
2023-02-21 07:04:09,410 DEBUG TRAIN Batch 13/2700 loss 8.587960 loss_att 13.572046 loss_ctc 11.025943 loss_rnnt 7.219649 hw_loss 0.087055 lr 0.00047427 rank 6
2023-02-21 07:04:09,422 DEBUG TRAIN Batch 13/2700 loss 9.707072 loss_att 12.365542 loss_ctc 10.809813 loss_rnnt 8.891014 hw_loss 0.257497 lr 0.00047428 rank 3
2023-02-21 07:04:09,484 DEBUG TRAIN Batch 13/2700 loss 19.311430 loss_att 21.089920 loss_ctc 26.496735 loss_rnnt 17.888294 hw_loss 0.205114 lr 0.00047426 rank 7
2023-02-21 07:04:09,490 DEBUG TRAIN Batch 13/2700 loss 20.825445 loss_att 27.458805 loss_ctc 26.153389 loss_rnnt 18.773001 hw_loss 0.028832 lr 0.00047431 rank 5
2023-02-21 07:04:09,530 DEBUG TRAIN Batch 13/2700 loss 23.471100 loss_att 24.616905 loss_ctc 31.412949 loss_rnnt 22.116194 hw_loss 0.125314 lr 0.00047423 rank 2
2023-02-21 07:04:09,697 DEBUG TRAIN Batch 13/2700 loss 6.219116 loss_att 9.835520 loss_ctc 5.738705 loss_rnnt 5.530727 hw_loss 0.054679 lr 0.00047434 rank 0
2023-02-21 07:04:09,716 DEBUG TRAIN Batch 13/2700 loss 11.523193 loss_att 14.686132 loss_ctc 14.673803 loss_rnnt 10.400042 hw_loss 0.132155 lr 0.00047426 rank 4
2023-02-21 07:12:37,521 DEBUG TRAIN Batch 13/2800 loss 12.671937 loss_att 16.226303 loss_ctc 18.812592 loss_rnnt 11.096468 hw_loss 0.085952 lr 0.00047405 rank 4
2023-02-21 07:12:37,641 DEBUG TRAIN Batch 13/2800 loss 15.174688 loss_att 18.862747 loss_ctc 21.519836 loss_rnnt 13.479445 hw_loss 0.209275 lr 0.00047410 rank 5
2023-02-21 07:12:37,653 DEBUG TRAIN Batch 13/2800 loss 3.440810 loss_att 7.614354 loss_ctc 4.777378 loss_rnnt 2.314156 hw_loss 0.213255 lr 0.00047407 rank 3
2023-02-21 07:12:37,635 DEBUG TRAIN Batch 13/2800 loss 16.316137 loss_att 20.234879 loss_ctc 19.659811 loss_rnnt 15.040834 hw_loss 0.085744 lr 0.00047406 rank 6
2023-02-21 07:12:37,680 DEBUG TRAIN Batch 13/2800 loss 23.594292 loss_att 28.980042 loss_ctc 34.600552 loss_rnnt 20.953011 hw_loss 0.181184 lr 0.00047404 rank 7
2023-02-21 07:12:37,688 DEBUG TRAIN Batch 13/2800 loss 6.921520 loss_att 8.366800 loss_ctc 5.633466 loss_rnnt 6.720748 hw_loss 0.156481 lr 0.00047401 rank 2
2023-02-21 07:12:37,721 DEBUG TRAIN Batch 13/2800 loss 14.020216 loss_att 19.503300 loss_ctc 20.234623 loss_rnnt 12.000572 hw_loss 0.177074 lr 0.00047412 rank 0
2023-02-21 07:12:37,875 DEBUG TRAIN Batch 13/2800 loss 14.933902 loss_att 21.241135 loss_ctc 18.815805 loss_rnnt 13.010424 hw_loss 0.270832 lr 0.00047411 rank 1
2023-02-21 07:21:09,735 DEBUG TRAIN Batch 13/2900 loss 22.742685 loss_att 29.325098 loss_ctc 25.401871 loss_rnnt 21.007383 hw_loss 0.120487 lr 0.00047386 rank 3
2023-02-21 07:21:09,741 DEBUG TRAIN Batch 13/2900 loss 16.363335 loss_att 23.120581 loss_ctc 17.116474 loss_rnnt 14.878895 hw_loss 0.061071 lr 0.00047390 rank 1
2023-02-21 07:21:09,850 DEBUG TRAIN Batch 13/2900 loss 14.802900 loss_att 17.469589 loss_ctc 17.211548 loss_rnnt 13.889840 hw_loss 0.109815 lr 0.00047383 rank 7
2023-02-21 07:21:09,885 DEBUG TRAIN Batch 13/2900 loss 10.328371 loss_att 15.483208 loss_ctc 14.039367 loss_rnnt 8.789295 hw_loss 0.024952 lr 0.00047391 rank 0
2023-02-21 07:21:09,913 DEBUG TRAIN Batch 13/2900 loss 11.304893 loss_att 15.854092 loss_ctc 13.128198 loss_rnnt 10.106005 hw_loss 0.086142 lr 0.00047384 rank 6
2023-02-21 07:21:09,930 DEBUG TRAIN Batch 13/2900 loss 8.148265 loss_att 12.017179 loss_ctc 9.748641 loss_rnnt 7.069969 hw_loss 0.170867 lr 0.00047389 rank 5
2023-02-21 07:21:10,012 DEBUG TRAIN Batch 13/2900 loss 14.513412 loss_att 18.686207 loss_ctc 19.807610 loss_rnnt 12.842093 hw_loss 0.245381 lr 0.00047380 rank 2
2023-02-21 07:21:10,037 DEBUG TRAIN Batch 13/2900 loss 10.925939 loss_att 15.371793 loss_ctc 15.242359 loss_rnnt 9.447865 hw_loss 0.025085 lr 0.00047383 rank 4
2023-02-21 07:29:34,493 DEBUG TRAIN Batch 13/3000 loss 13.390699 loss_att 17.765257 loss_ctc 15.982599 loss_rnnt 12.151941 hw_loss 0.034238 lr 0.00047364 rank 3
2023-02-21 07:29:34,522 DEBUG TRAIN Batch 13/3000 loss 16.287357 loss_att 18.924400 loss_ctc 20.944887 loss_rnnt 15.099582 hw_loss 0.073805 lr 0.00047363 rank 6
2023-02-21 07:29:34,580 DEBUG TRAIN Batch 13/3000 loss 19.449001 loss_att 25.124468 loss_ctc 28.247150 loss_rnnt 17.096983 hw_loss 0.082199 lr 0.00047359 rank 2
2023-02-21 07:29:34,636 DEBUG TRAIN Batch 13/3000 loss 15.466173 loss_att 21.202568 loss_ctc 24.178581 loss_rnnt 13.091570 hw_loss 0.123128 lr 0.00047362 rank 7
2023-02-21 07:29:34,706 DEBUG TRAIN Batch 13/3000 loss 14.382637 loss_att 16.150940 loss_ctc 15.868800 loss_rnnt 13.764614 hw_loss 0.124139 lr 0.00047370 rank 0
2023-02-21 07:29:34,705 DEBUG TRAIN Batch 13/3000 loss 18.447596 loss_att 18.769892 loss_ctc 26.496876 loss_rnnt 17.276136 hw_loss 0.063303 lr 0.00047368 rank 5
2023-02-21 07:29:34,810 DEBUG TRAIN Batch 13/3000 loss 19.752165 loss_att 22.468807 loss_ctc 27.014507 loss_rnnt 18.170572 hw_loss 0.131157 lr 0.00047369 rank 1
2023-02-21 07:29:34,893 DEBUG TRAIN Batch 13/3000 loss 14.286022 loss_att 17.883221 loss_ctc 19.102272 loss_rnnt 12.884125 hw_loss 0.075545 lr 0.00047362 rank 4
2023-02-21 07:37:58,126 DEBUG TRAIN Batch 13/3100 loss 12.705440 loss_att 13.590029 loss_ctc 14.999609 loss_rnnt 12.148067 hw_loss 0.139811 lr 0.00047348 rank 0
2023-02-21 07:37:58,323 DEBUG TRAIN Batch 13/3100 loss 7.426251 loss_att 10.014702 loss_ctc 10.636871 loss_rnnt 6.433016 hw_loss 0.088991 lr 0.00047341 rank 7
2023-02-21 07:37:58,345 DEBUG TRAIN Batch 13/3100 loss 13.526567 loss_att 19.120041 loss_ctc 17.262508 loss_rnnt 11.822375 hw_loss 0.163821 lr 0.00047342 rank 6
2023-02-21 07:37:58,381 DEBUG TRAIN Batch 13/3100 loss 8.351110 loss_att 8.625353 loss_ctc 10.531453 loss_rnnt 7.946455 hw_loss 0.110804 lr 0.00047346 rank 5
2023-02-21 07:37:58,511 DEBUG TRAIN Batch 13/3100 loss 14.264288 loss_att 17.063152 loss_ctc 17.377655 loss_rnnt 13.179020 hw_loss 0.206961 lr 0.00047341 rank 4
2023-02-21 07:37:58,813 DEBUG TRAIN Batch 13/3100 loss 13.971943 loss_att 17.426662 loss_ctc 18.302155 loss_rnnt 12.627090 hw_loss 0.143525 lr 0.00047338 rank 2
2023-02-21 07:37:58,926 DEBUG TRAIN Batch 13/3100 loss 15.298928 loss_att 16.863953 loss_ctc 17.340107 loss_rnnt 14.683749 hw_loss 0.056283 lr 0.00047347 rank 1
2023-02-21 07:37:59,170 DEBUG TRAIN Batch 13/3100 loss 23.289112 loss_att 23.485579 loss_ctc 28.845211 loss_rnnt 22.477303 hw_loss 0.059441 lr 0.00047343 rank 3
2023-02-21 07:46:29,898 DEBUG TRAIN Batch 13/3200 loss 7.370074 loss_att 9.011235 loss_ctc 6.962203 loss_rnnt 7.031290 hw_loss 0.121753 lr 0.00047325 rank 5
2023-02-21 07:46:29,913 DEBUG TRAIN Batch 13/3200 loss 7.629591 loss_att 8.742518 loss_ctc 9.883262 loss_rnnt 6.981225 hw_loss 0.234922 lr 0.00047319 rank 7
2023-02-21 07:46:30,036 DEBUG TRAIN Batch 13/3200 loss 16.068193 loss_att 15.262520 loss_ctc 20.902945 loss_rnnt 15.419253 hw_loss 0.310203 lr 0.00047320 rank 4
2023-02-21 07:46:30,039 DEBUG TRAIN Batch 13/3200 loss 8.231546 loss_att 10.906948 loss_ctc 10.342180 loss_rnnt 7.341017 hw_loss 0.138807 lr 0.00047327 rank 0
2023-02-21 07:46:30,096 DEBUG TRAIN Batch 13/3200 loss 14.490734 loss_att 23.683498 loss_ctc 19.229534 loss_rnnt 11.977972 hw_loss 0.079443 lr 0.00047322 rank 3
2023-02-21 07:46:30,123 DEBUG TRAIN Batch 13/3200 loss 7.514479 loss_att 14.115462 loss_ctc 5.297976 loss_rnnt 6.444602 hw_loss 0.084777 lr 0.00047321 rank 6
2023-02-21 07:46:30,274 DEBUG TRAIN Batch 13/3200 loss 13.988276 loss_att 12.388605 loss_ctc 15.569850 loss_rnnt 13.850223 hw_loss 0.463333 lr 0.00047326 rank 1
2023-02-21 07:46:30,449 DEBUG TRAIN Batch 13/3200 loss 13.525047 loss_att 13.826607 loss_ctc 16.565023 loss_rnnt 12.920822 hw_loss 0.259843 lr 0.00047316 rank 2
2023-02-21 07:54:43,771 DEBUG TRAIN Batch 13/3300 loss 18.822466 loss_att 23.782833 loss_ctc 24.108969 loss_rnnt 17.065329 hw_loss 0.112867 lr 0.00047306 rank 0
2023-02-21 07:54:43,774 DEBUG TRAIN Batch 13/3300 loss 10.319164 loss_att 14.011112 loss_ctc 12.058009 loss_rnnt 9.287939 hw_loss 0.114355 lr 0.00047299 rank 6
2023-02-21 07:54:43,785 DEBUG TRAIN Batch 13/3300 loss 17.976004 loss_att 21.136179 loss_ctc 21.266151 loss_rnnt 16.853001 hw_loss 0.098029 lr 0.00047298 rank 4
2023-02-21 07:54:43,791 DEBUG TRAIN Batch 13/3300 loss 4.800246 loss_att 12.263422 loss_ctc 4.958607 loss_rnnt 3.187527 hw_loss 0.185566 lr 0.00047295 rank 2
2023-02-21 07:54:43,966 DEBUG TRAIN Batch 13/3300 loss 12.559141 loss_att 14.437911 loss_ctc 12.193304 loss_rnnt 12.182348 hw_loss 0.093408 lr 0.00047304 rank 5
2023-02-21 07:54:44,012 DEBUG TRAIN Batch 13/3300 loss 18.060469 loss_att 21.472725 loss_ctc 23.997213 loss_rnnt 16.571205 hw_loss 0.028589 lr 0.00047301 rank 3
2023-02-21 07:54:44,007 DEBUG TRAIN Batch 13/3300 loss 9.474611 loss_att 13.120135 loss_ctc 14.389613 loss_rnnt 8.024584 hw_loss 0.122978 lr 0.00047298 rank 7
2023-02-21 07:54:44,438 DEBUG TRAIN Batch 13/3300 loss 8.989553 loss_att 12.857669 loss_ctc 11.535862 loss_rnnt 7.759574 hw_loss 0.219090 lr 0.00047305 rank 1
2023-02-21 08:02:54,208 DEBUG TRAIN Batch 13/3400 loss 3.310391 loss_att 6.454708 loss_ctc 4.662789 loss_rnnt 2.417402 hw_loss 0.157137 lr 0.00047277 rank 7
2023-02-21 08:02:54,218 DEBUG TRAIN Batch 13/3400 loss 13.774542 loss_att 19.757660 loss_ctc 20.782604 loss_rnnt 11.545670 hw_loss 0.183450 lr 0.00047283 rank 5
2023-02-21 08:02:54,181 DEBUG TRAIN Batch 13/3400 loss 10.455658 loss_att 13.225874 loss_ctc 13.012046 loss_rnnt 9.447618 hw_loss 0.212146 lr 0.00047285 rank 0
2023-02-21 08:02:54,287 DEBUG TRAIN Batch 13/3400 loss 9.346357 loss_att 10.763400 loss_ctc 11.003684 loss_rnnt 8.756729 hw_loss 0.159832 lr 0.00047277 rank 4
2023-02-21 08:02:54,587 DEBUG TRAIN Batch 13/3400 loss 4.703619 loss_att 7.332128 loss_ctc 6.432197 loss_rnnt 3.865971 hw_loss 0.152754 lr 0.00047274 rank 2
2023-02-21 08:02:54,601 DEBUG TRAIN Batch 13/3400 loss 15.103519 loss_att 15.066229 loss_ctc 12.592089 loss_rnnt 15.329231 hw_loss 0.218632 lr 0.00047280 rank 3
2023-02-21 08:02:54,626 DEBUG TRAIN Batch 13/3400 loss 5.907159 loss_att 9.970806 loss_ctc 10.034616 loss_rnnt 4.481351 hw_loss 0.117657 lr 0.00047284 rank 1
2023-02-21 08:02:54,722 DEBUG TRAIN Batch 13/3400 loss 5.361940 loss_att 8.705194 loss_ctc 9.388199 loss_rnnt 4.128170 hw_loss 0.053034 lr 0.00047278 rank 6
2023-02-21 08:11:13,038 DEBUG TRAIN Batch 13/3500 loss 16.660181 loss_att 19.061380 loss_ctc 20.443016 loss_rnnt 15.645099 hw_loss 0.057120 lr 0.00047253 rank 2
2023-02-21 08:11:13,227 DEBUG TRAIN Batch 13/3500 loss 18.367512 loss_att 20.516525 loss_ctc 20.362892 loss_rnnt 17.641544 hw_loss 0.056462 lr 0.00047263 rank 1
2023-02-21 08:11:13,233 DEBUG TRAIN Batch 13/3500 loss 21.215185 loss_att 20.779850 loss_ctc 22.017561 loss_rnnt 21.140129 hw_loss 0.103387 lr 0.00047262 rank 5
2023-02-21 08:11:13,305 DEBUG TRAIN Batch 13/3500 loss 10.911305 loss_att 12.400841 loss_ctc 10.870064 loss_rnnt 10.559260 hw_loss 0.111819 lr 0.00047258 rank 3
2023-02-21 08:11:13,340 DEBUG TRAIN Batch 13/3500 loss 21.952372 loss_att 20.977589 loss_ctc 30.462912 loss_rnnt 20.915585 hw_loss 0.181884 lr 0.00047256 rank 4
2023-02-21 08:11:13,532 DEBUG TRAIN Batch 13/3500 loss 21.873518 loss_att 28.268948 loss_ctc 27.774494 loss_rnnt 19.793741 hw_loss 0.026049 lr 0.00047256 rank 7
2023-02-21 08:11:13,701 DEBUG TRAIN Batch 13/3500 loss 12.741168 loss_att 16.313402 loss_ctc 15.989561 loss_rnnt 11.529819 hw_loss 0.119592 lr 0.00047264 rank 0
2023-02-21 08:11:13,818 DEBUG TRAIN Batch 13/3500 loss 19.181927 loss_att 26.143593 loss_ctc 24.435120 loss_rnnt 16.998013 hw_loss 0.170916 lr 0.00047257 rank 6
2023-02-21 08:19:31,414 DEBUG TRAIN Batch 13/3600 loss 14.280372 loss_att 14.340836 loss_ctc 16.403412 loss_rnnt 13.889470 hw_loss 0.179508 lr 0.00047240 rank 5
2023-02-21 08:19:31,455 DEBUG TRAIN Batch 13/3600 loss 14.690729 loss_att 17.198193 loss_ctc 17.736654 loss_rnnt 13.747281 hw_loss 0.067184 lr 0.00047235 rank 7
2023-02-21 08:19:31,651 DEBUG TRAIN Batch 13/3600 loss 8.386049 loss_att 12.345221 loss_ctc 10.670617 loss_rnnt 7.228116 hw_loss 0.115293 lr 0.00047232 rank 2
2023-02-21 08:19:31,707 DEBUG TRAIN Batch 13/3600 loss 11.238580 loss_att 12.787268 loss_ctc 15.133990 loss_rnnt 10.327190 hw_loss 0.154245 lr 0.00047235 rank 4
2023-02-21 08:19:31,707 DEBUG TRAIN Batch 13/3600 loss 10.266046 loss_att 11.157818 loss_ctc 11.854903 loss_rnnt 9.771513 hw_loss 0.195620 lr 0.00047243 rank 0
2023-02-21 08:19:31,736 DEBUG TRAIN Batch 13/3600 loss 15.200534 loss_att 18.471788 loss_ctc 17.757034 loss_rnnt 14.157541 hw_loss 0.089765 lr 0.00047242 rank 1
2023-02-21 08:19:31,837 DEBUG TRAIN Batch 13/3600 loss 14.094862 loss_att 16.595634 loss_ctc 18.413269 loss_rnnt 12.956137 hw_loss 0.117719 lr 0.00047236 rank 6
2023-02-21 08:19:31,982 DEBUG TRAIN Batch 13/3600 loss 10.855769 loss_att 11.892631 loss_ctc 14.138784 loss_rnnt 10.167465 hw_loss 0.080994 lr 0.00047237 rank 3
2023-02-21 08:27:37,640 DEBUG TRAIN Batch 13/3700 loss 10.274350 loss_att 11.703879 loss_ctc 12.493635 loss_rnnt 9.567122 hw_loss 0.235158 lr 0.00047222 rank 0
2023-02-21 08:27:37,701 DEBUG TRAIN Batch 13/3700 loss 18.534657 loss_att 22.481245 loss_ctc 25.169577 loss_rnnt 16.732553 hw_loss 0.240242 lr 0.00047215 rank 6
2023-02-21 08:27:37,714 DEBUG TRAIN Batch 13/3700 loss 21.108582 loss_att 23.782093 loss_ctc 26.945715 loss_rnnt 19.722744 hw_loss 0.136596 lr 0.00047219 rank 5
2023-02-21 08:27:37,729 DEBUG TRAIN Batch 13/3700 loss 18.278194 loss_att 23.738033 loss_ctc 24.362743 loss_rnnt 16.318581 hw_loss 0.105694 lr 0.00047216 rank 3
2023-02-21 08:27:37,749 DEBUG TRAIN Batch 13/3700 loss 10.722293 loss_att 11.635273 loss_ctc 15.090801 loss_rnnt 9.912075 hw_loss 0.084665 lr 0.00047214 rank 4
2023-02-21 08:27:37,754 DEBUG TRAIN Batch 13/3700 loss 12.826051 loss_att 16.444096 loss_ctc 18.215057 loss_rnnt 11.214478 hw_loss 0.317680 lr 0.00047211 rank 2
2023-02-21 08:27:37,817 DEBUG TRAIN Batch 13/3700 loss 15.468587 loss_att 15.474225 loss_ctc 19.257998 loss_rnnt 14.923791 hw_loss 0.072026 lr 0.00047214 rank 7
2023-02-21 08:27:37,842 DEBUG TRAIN Batch 13/3700 loss 19.030098 loss_att 20.661760 loss_ctc 26.990936 loss_rnnt 17.489334 hw_loss 0.286848 lr 0.00047220 rank 1
2023-02-21 08:35:57,329 DEBUG TRAIN Batch 13/3800 loss 14.980172 loss_att 18.911743 loss_ctc 18.520748 loss_rnnt 13.668051 hw_loss 0.100746 lr 0.00047200 rank 0
2023-02-21 08:35:57,373 DEBUG TRAIN Batch 13/3800 loss 7.460403 loss_att 7.672204 loss_ctc 7.528748 loss_rnnt 7.302789 hw_loss 0.199013 lr 0.00047190 rank 2
2023-02-21 08:35:57,410 DEBUG TRAIN Batch 13/3800 loss 13.721384 loss_att 14.374500 loss_ctc 15.947245 loss_rnnt 13.191895 hw_loss 0.191408 lr 0.00047194 rank 6
2023-02-21 08:35:57,490 DEBUG TRAIN Batch 13/3800 loss 14.484304 loss_att 17.280895 loss_ctc 18.118750 loss_rnnt 13.301303 hw_loss 0.260794 lr 0.00047199 rank 1
2023-02-21 08:35:57,513 DEBUG TRAIN Batch 13/3800 loss 5.914700 loss_att 12.954144 loss_ctc 6.631747 loss_rnnt 4.371707 hw_loss 0.074058 lr 0.00047198 rank 5
2023-02-21 08:35:57,562 DEBUG TRAIN Batch 13/3800 loss 11.198482 loss_att 12.476914 loss_ctc 15.139699 loss_rnnt 10.268710 hw_loss 0.278606 lr 0.00047195 rank 3
2023-02-21 08:35:57,553 DEBUG TRAIN Batch 13/3800 loss 8.309159 loss_att 9.848121 loss_ctc 9.095638 loss_rnnt 7.841944 hw_loss 0.102297 lr 0.00047193 rank 4
2023-02-21 08:35:57,744 DEBUG TRAIN Batch 13/3800 loss 8.756125 loss_att 9.826080 loss_ctc 10.692822 loss_rnnt 8.226069 hw_loss 0.108446 lr 0.00047193 rank 7
2023-02-21 08:44:33,345 DEBUG TRAIN Batch 13/3900 loss 22.483852 loss_att 24.582579 loss_ctc 32.780907 loss_rnnt 20.677055 hw_loss 0.026453 lr 0.00047178 rank 1
2023-02-21 08:44:33,461 DEBUG TRAIN Batch 13/3900 loss 25.167326 loss_att 27.309805 loss_ctc 34.003185 loss_rnnt 23.513248 hw_loss 0.089001 lr 0.00047172 rank 4
2023-02-21 08:44:33,509 DEBUG TRAIN Batch 13/3900 loss 15.202119 loss_att 19.937716 loss_ctc 22.359173 loss_rnnt 13.245035 hw_loss 0.104420 lr 0.00047177 rank 5
2023-02-21 08:44:33,530 DEBUG TRAIN Batch 13/3900 loss 13.553065 loss_att 17.473068 loss_ctc 15.736690 loss_rnnt 12.464010 hw_loss 0.026071 lr 0.00047174 rank 3
2023-02-21 08:44:33,544 DEBUG TRAIN Batch 13/3900 loss 19.469883 loss_att 28.081413 loss_ctc 26.417311 loss_rnnt 16.751163 hw_loss 0.131415 lr 0.00047179 rank 0
2023-02-21 08:44:33,565 DEBUG TRAIN Batch 13/3900 loss 25.649048 loss_att 26.845554 loss_ctc 34.325462 loss_rnnt 24.220051 hw_loss 0.061567 lr 0.00047169 rank 2
2023-02-21 08:44:33,755 DEBUG TRAIN Batch 13/3900 loss 7.042931 loss_att 10.700170 loss_ctc 10.866991 loss_rnnt 5.708454 hw_loss 0.174664 lr 0.00047173 rank 6
2023-02-21 08:44:33,743 DEBUG TRAIN Batch 13/3900 loss 19.633554 loss_att 23.377995 loss_ctc 29.477184 loss_rnnt 17.501575 hw_loss 0.132391 lr 0.00047172 rank 7
2023-02-21 08:52:41,834 DEBUG TRAIN Batch 13/4000 loss 26.307457 loss_att 32.283028 loss_ctc 36.595764 loss_rnnt 23.707596 hw_loss 0.061824 lr 0.00047151 rank 4
2023-02-21 08:52:41,869 DEBUG TRAIN Batch 13/4000 loss 17.174538 loss_att 21.815331 loss_ctc 22.127331 loss_rnnt 15.551505 hw_loss 0.064687 lr 0.00047152 rank 6
2023-02-21 08:52:41,885 DEBUG TRAIN Batch 13/4000 loss 8.777137 loss_att 11.553642 loss_ctc 11.106475 loss_rnnt 7.803011 hw_loss 0.202962 lr 0.00047157 rank 1
2023-02-21 08:52:41,912 DEBUG TRAIN Batch 13/4000 loss 18.278515 loss_att 24.286606 loss_ctc 25.630447 loss_rnnt 16.057676 hw_loss 0.073056 lr 0.00047148 rank 2
2023-02-21 08:52:41,929 DEBUG TRAIN Batch 13/4000 loss 16.100128 loss_att 18.630783 loss_ctc 23.025875 loss_rnnt 14.592279 hw_loss 0.146779 lr 0.00047158 rank 0
2023-02-21 08:52:41,940 DEBUG TRAIN Batch 13/4000 loss 12.209419 loss_att 14.527176 loss_ctc 15.154903 loss_rnnt 11.263927 hw_loss 0.167271 lr 0.00047156 rank 5
2023-02-21 08:52:42,107 DEBUG TRAIN Batch 13/4000 loss 4.413612 loss_att 7.284666 loss_ctc 8.058512 loss_rnnt 3.285711 hw_loss 0.126943 lr 0.00047151 rank 7
2023-02-21 08:52:42,108 DEBUG TRAIN Batch 13/4000 loss 5.921159 loss_att 10.962013 loss_ctc 8.689901 loss_rnnt 4.475229 hw_loss 0.128613 lr 0.00047153 rank 3
2023-02-21 09:01:01,809 DEBUG TRAIN Batch 13/4100 loss 16.357473 loss_att 25.742134 loss_ctc 23.393087 loss_rnnt 13.526911 hw_loss 0.029156 lr 0.00047136 rank 1
2023-02-21 09:01:01,945 DEBUG TRAIN Batch 13/4100 loss 11.790290 loss_att 14.800322 loss_ctc 13.120831 loss_rnnt 10.995522 hw_loss 0.028793 lr 0.00047130 rank 4
2023-02-21 09:01:01,948 DEBUG TRAIN Batch 13/4100 loss 14.159660 loss_att 15.561243 loss_ctc 17.532675 loss_rnnt 13.338871 hw_loss 0.170133 lr 0.00047132 rank 3
2023-02-21 09:01:01,951 DEBUG TRAIN Batch 13/4100 loss 14.359077 loss_att 19.095886 loss_ctc 16.646646 loss_rnnt 13.076846 hw_loss 0.055991 lr 0.00047131 rank 6
2023-02-21 09:01:02,032 DEBUG TRAIN Batch 13/4100 loss 17.727262 loss_att 18.942373 loss_ctc 22.000181 loss_rnnt 16.826885 hw_loss 0.164312 lr 0.00047135 rank 5
2023-02-21 09:01:02,092 DEBUG TRAIN Batch 13/4100 loss 18.882076 loss_att 20.697054 loss_ctc 22.368484 loss_rnnt 18.000961 hw_loss 0.099868 lr 0.00047130 rank 7
2023-02-21 09:01:02,003 DEBUG TRAIN Batch 13/4100 loss 22.074934 loss_att 20.868530 loss_ctc 26.313910 loss_rnnt 21.697874 hw_loss 0.099644 lr 0.00047127 rank 2
2023-02-21 09:01:02,304 DEBUG TRAIN Batch 13/4100 loss 28.384813 loss_att 29.424877 loss_ctc 39.137096 loss_rnnt 26.651789 hw_loss 0.171321 lr 0.00047138 rank 0
2023-02-21 09:09:20,328 DEBUG TRAIN Batch 13/4200 loss 4.353355 loss_att 7.839358 loss_ctc 5.607741 loss_rnnt 3.361442 hw_loss 0.238990 lr 0.00047111 rank 3
2023-02-21 09:09:20,363 DEBUG TRAIN Batch 13/4200 loss 13.760686 loss_att 16.008961 loss_ctc 18.369448 loss_rnnt 12.657202 hw_loss 0.073739 lr 0.00047117 rank 0
2023-02-21 09:09:20,443 DEBUG TRAIN Batch 13/4200 loss 19.467686 loss_att 23.329109 loss_ctc 25.649279 loss_rnnt 17.769657 hw_loss 0.190373 lr 0.00047110 rank 6
2023-02-21 09:09:20,450 DEBUG TRAIN Batch 13/4200 loss 6.486534 loss_att 10.205042 loss_ctc 11.105488 loss_rnnt 5.112981 hw_loss 0.026232 lr 0.00047116 rank 1
2023-02-21 09:09:20,578 DEBUG TRAIN Batch 13/4200 loss 12.059133 loss_att 16.305424 loss_ctc 16.512346 loss_rnnt 10.563828 hw_loss 0.098035 lr 0.00047109 rank 7
2023-02-21 09:09:20,590 DEBUG TRAIN Batch 13/4200 loss 19.092470 loss_att 22.280239 loss_ctc 24.606812 loss_rnnt 17.637920 hw_loss 0.153285 lr 0.00047114 rank 5
2023-02-21 09:09:20,614 DEBUG TRAIN Batch 13/4200 loss 14.848310 loss_att 16.116917 loss_ctc 16.936502 loss_rnnt 14.252135 hw_loss 0.120052 lr 0.00047109 rank 4
2023-02-21 09:09:20,696 DEBUG TRAIN Batch 13/4200 loss 12.625229 loss_att 15.860815 loss_ctc 14.700919 loss_rnnt 11.566578 hw_loss 0.252704 lr 0.00047106 rank 2
2023-02-21 09:17:51,595 DEBUG TRAIN Batch 13/4300 loss 13.620473 loss_att 17.478743 loss_ctc 16.896629 loss_rnnt 12.272715 hw_loss 0.261156 lr 0.00047095 rank 1
2023-02-21 09:17:51,606 DEBUG TRAIN Batch 13/4300 loss 30.610580 loss_att 28.264709 loss_ctc 30.050606 loss_rnnt 31.052973 hw_loss 0.190212 lr 0.00047088 rank 4
2023-02-21 09:17:51,648 DEBUG TRAIN Batch 13/4300 loss 23.667534 loss_att 24.747375 loss_ctc 31.488026 loss_rnnt 22.302746 hw_loss 0.198919 lr 0.00047096 rank 0
2023-02-21 09:17:51,666 DEBUG TRAIN Batch 13/4300 loss 18.102604 loss_att 17.994143 loss_ctc 22.378525 loss_rnnt 17.400236 hw_loss 0.288633 lr 0.00047085 rank 2
2023-02-21 09:17:51,713 DEBUG TRAIN Batch 13/4300 loss 14.514175 loss_att 15.756489 loss_ctc 20.061735 loss_rnnt 13.478661 hw_loss 0.088829 lr 0.00047089 rank 6
2023-02-21 09:17:51,761 DEBUG TRAIN Batch 13/4300 loss 19.721889 loss_att 21.363356 loss_ctc 24.341797 loss_rnnt 18.679939 hw_loss 0.183129 lr 0.00047088 rank 7
2023-02-21 09:17:51,798 DEBUG TRAIN Batch 13/4300 loss 6.604942 loss_att 8.936606 loss_ctc 7.716573 loss_rnnt 5.820863 hw_loss 0.317867 lr 0.00047090 rank 3
2023-02-21 09:17:51,986 DEBUG TRAIN Batch 13/4300 loss 9.361232 loss_att 10.523837 loss_ctc 11.100746 loss_rnnt 8.826652 hw_loss 0.131483 lr 0.00047094 rank 5
2023-02-21 09:26:02,233 DEBUG TRAIN Batch 13/4400 loss 9.249001 loss_att 13.219255 loss_ctc 11.224585 loss_rnnt 8.121080 hw_loss 0.132107 lr 0.00047068 rank 6
2023-02-21 09:26:02,264 DEBUG TRAIN Batch 13/4400 loss 8.018847 loss_att 12.417490 loss_ctc 12.333018 loss_rnnt 6.519907 hw_loss 0.082478 lr 0.00047073 rank 5
2023-02-21 09:26:02,346 DEBUG TRAIN Batch 13/4400 loss 16.465307 loss_att 18.108479 loss_ctc 21.795675 loss_rnnt 15.396419 hw_loss 0.055385 lr 0.00047070 rank 3
2023-02-21 09:26:02,377 DEBUG TRAIN Batch 13/4400 loss 5.867244 loss_att 7.564367 loss_ctc 7.264638 loss_rnnt 5.253252 hw_loss 0.165467 lr 0.00047064 rank 2
2023-02-21 09:26:02,423 DEBUG TRAIN Batch 13/4400 loss 13.100899 loss_att 14.107791 loss_ctc 14.874472 loss_rnnt 12.502388 hw_loss 0.301230 lr 0.00047067 rank 7
2023-02-21 09:26:02,465 DEBUG TRAIN Batch 13/4400 loss 16.739918 loss_att 19.848507 loss_ctc 23.606722 loss_rnnt 15.012720 hw_loss 0.356073 lr 0.00047067 rank 4
2023-02-21 09:26:02,634 DEBUG TRAIN Batch 13/4400 loss 14.198633 loss_att 14.354631 loss_ctc 16.002157 loss_rnnt 13.850223 hw_loss 0.143890 lr 0.00047074 rank 1
2023-02-21 09:26:02,570 DEBUG TRAIN Batch 13/4400 loss 13.730444 loss_att 15.631990 loss_ctc 22.956610 loss_rnnt 12.080385 hw_loss 0.074238 lr 0.00047075 rank 0
2023-02-21 09:34:20,634 DEBUG TRAIN Batch 13/4500 loss 14.464353 loss_att 17.661270 loss_ctc 20.288471 loss_rnnt 13.005213 hw_loss 0.081012 lr 0.00047053 rank 1
2023-02-21 09:34:20,641 DEBUG TRAIN Batch 13/4500 loss 14.767195 loss_att 15.279053 loss_ctc 18.735626 loss_rnnt 13.992151 hw_loss 0.269150 lr 0.00047049 rank 3
2023-02-21 09:34:20,706 DEBUG TRAIN Batch 13/4500 loss 9.263391 loss_att 12.152252 loss_ctc 12.963898 loss_rnnt 8.095353 hw_loss 0.181621 lr 0.00047052 rank 5
2023-02-21 09:34:20,879 DEBUG TRAIN Batch 13/4500 loss 30.739986 loss_att 34.951645 loss_ctc 43.408813 loss_rnnt 28.196354 hw_loss 0.022729 lr 0.00047047 rank 6
2023-02-21 09:34:20,913 DEBUG TRAIN Batch 13/4500 loss 14.470716 loss_att 18.290731 loss_ctc 20.922611 loss_rnnt 12.773528 hw_loss 0.136746 lr 0.00047043 rank 2
2023-02-21 09:34:20,875 DEBUG TRAIN Batch 13/4500 loss 10.869781 loss_att 10.174824 loss_ctc 12.096047 loss_rnnt 10.578670 hw_loss 0.499877 lr 0.00047046 rank 4
2023-02-21 09:34:20,936 DEBUG TRAIN Batch 13/4500 loss 13.322329 loss_att 15.994837 loss_ctc 15.484665 loss_rnnt 12.464572 hw_loss 0.065517 lr 0.00047054 rank 0
2023-02-21 09:34:20,941 DEBUG TRAIN Batch 13/4500 loss 6.939822 loss_att 9.867764 loss_ctc 9.305121 loss_rnnt 6.005104 hw_loss 0.063296 lr 0.00047046 rank 7
2023-02-21 09:39:43,640 DEBUG TRAIN Batch 13/4600 loss 14.090900 loss_att 16.929773 loss_ctc 16.095125 loss_rnnt 13.195164 hw_loss 0.113873 lr 0.00047027 rank 6
2023-02-21 09:39:43,642 DEBUG TRAIN Batch 13/4600 loss 11.110652 loss_att 22.628262 loss_ctc 15.191837 loss_rnnt 8.248304 hw_loss 0.027502 lr 0.00047023 rank 2
2023-02-21 09:39:43,644 DEBUG TRAIN Batch 13/4600 loss 9.510813 loss_att 12.515010 loss_ctc 10.703056 loss_rnnt 8.713064 hw_loss 0.071143 lr 0.00047025 rank 7
2023-02-21 09:39:43,644 DEBUG TRAIN Batch 13/4600 loss 21.984457 loss_att 26.521416 loss_ctc 31.374725 loss_rnnt 19.799625 hw_loss 0.047632 lr 0.00047026 rank 4
2023-02-21 09:39:43,644 DEBUG TRAIN Batch 13/4600 loss 41.306267 loss_att 40.358662 loss_ctc 49.629250 loss_rnnt 40.313564 hw_loss 0.135924 lr 0.00047031 rank 5
2023-02-21 09:39:43,647 DEBUG TRAIN Batch 13/4600 loss 6.144697 loss_att 10.378788 loss_ctc 10.982363 loss_rnnt 4.570457 hw_loss 0.154498 lr 0.00047032 rank 1
2023-02-21 09:39:43,646 DEBUG TRAIN Batch 13/4600 loss 13.393965 loss_att 16.499743 loss_ctc 17.017834 loss_rnnt 12.236870 hw_loss 0.098919 lr 0.00047033 rank 0
2023-02-21 09:39:43,665 DEBUG TRAIN Batch 13/4600 loss 19.029318 loss_att 25.244152 loss_ctc 26.694885 loss_rnnt 16.669880 hw_loss 0.176987 lr 0.00047028 rank 3
2023-02-21 09:41:02,056 DEBUG TRAIN Batch 13/4700 loss 16.158798 loss_att 23.645708 loss_ctc 20.592388 loss_rnnt 13.883495 hw_loss 0.350204 lr 0.00047002 rank 2
2023-02-21 09:41:02,061 DEBUG TRAIN Batch 13/4700 loss 13.620650 loss_att 15.530861 loss_ctc 18.545967 loss_rnnt 12.492270 hw_loss 0.168055 lr 0.00047005 rank 4
2023-02-21 09:41:02,062 DEBUG TRAIN Batch 13/4700 loss 19.218018 loss_att 23.463272 loss_ctc 26.810570 loss_rnnt 17.307394 hw_loss 0.092311 lr 0.00047006 rank 6
2023-02-21 09:41:02,064 DEBUG TRAIN Batch 13/4700 loss 13.369058 loss_att 15.475367 loss_ctc 17.297216 loss_rnnt 12.337328 hw_loss 0.162585 lr 0.00047011 rank 1
2023-02-21 09:41:02,068 DEBUG TRAIN Batch 13/4700 loss 9.655821 loss_att 15.764509 loss_ctc 16.518503 loss_rnnt 7.470758 hw_loss 0.090563 lr 0.00047010 rank 5
2023-02-21 09:41:02,069 DEBUG TRAIN Batch 13/4700 loss 17.973434 loss_att 18.330500 loss_ctc 18.165737 loss_rnnt 17.753616 hw_loss 0.230185 lr 0.00047007 rank 3
2023-02-21 09:41:02,094 DEBUG TRAIN Batch 13/4700 loss 22.548050 loss_att 26.140602 loss_ctc 23.639269 loss_rnnt 21.608574 hw_loss 0.141501 lr 0.00047012 rank 0
2023-02-21 09:41:02,115 DEBUG TRAIN Batch 13/4700 loss 22.283297 loss_att 30.571564 loss_ctc 28.772675 loss_rnnt 19.685028 hw_loss 0.141307 lr 0.00047005 rank 7
2023-02-21 09:42:20,210 DEBUG TRAIN Batch 13/4800 loss 11.810897 loss_att 15.699003 loss_ctc 12.054996 loss_rnnt 10.910721 hw_loss 0.168764 lr 0.00046981 rank 2
2023-02-21 09:42:20,213 DEBUG TRAIN Batch 13/4800 loss 26.622335 loss_att 28.832840 loss_ctc 33.291451 loss_rnnt 25.228224 hw_loss 0.117739 lr 0.00046984 rank 4
2023-02-21 09:42:20,215 DEBUG TRAIN Batch 13/4800 loss 10.183794 loss_att 11.656348 loss_ctc 13.131050 loss_rnnt 9.471004 hw_loss 0.047459 lr 0.00046985 rank 6
2023-02-21 09:42:20,215 DEBUG TRAIN Batch 13/4800 loss 9.594665 loss_att 13.455679 loss_ctc 11.253039 loss_rnnt 8.523383 hw_loss 0.146177 lr 0.00046986 rank 3
2023-02-21 09:42:20,217 DEBUG TRAIN Batch 13/4800 loss 18.531387 loss_att 20.125072 loss_ctc 23.177139 loss_rnnt 17.529713 hw_loss 0.119073 lr 0.00046991 rank 1
2023-02-21 09:42:20,217 DEBUG TRAIN Batch 13/4800 loss 12.426818 loss_att 15.630477 loss_ctc 13.524185 loss_rnnt 11.540030 hw_loss 0.187013 lr 0.00046989 rank 5
2023-02-21 09:42:20,223 DEBUG TRAIN Batch 13/4800 loss 11.585899 loss_att 14.711861 loss_ctc 12.169438 loss_rnnt 10.805509 hw_loss 0.145110 lr 0.00046992 rank 0
2023-02-21 09:42:20,262 DEBUG TRAIN Batch 13/4800 loss 13.477303 loss_att 16.182663 loss_ctc 17.895977 loss_rnnt 12.266752 hw_loss 0.150605 lr 0.00046984 rank 7
2023-02-21 09:43:39,626 DEBUG TRAIN Batch 13/4900 loss 19.441040 loss_att 23.249866 loss_ctc 22.307037 loss_rnnt 18.278713 hw_loss 0.034554 lr 0.00046960 rank 2
2023-02-21 09:43:39,627 DEBUG TRAIN Batch 13/4900 loss 9.471270 loss_att 11.733214 loss_ctc 13.834839 loss_rnnt 8.385006 hw_loss 0.097623 lr 0.00046970 rank 1
2023-02-21 09:43:39,628 DEBUG TRAIN Batch 13/4900 loss 9.551508 loss_att 13.117382 loss_ctc 13.181607 loss_rnnt 8.320210 hw_loss 0.063957 lr 0.00046963 rank 4
2023-02-21 09:43:39,632 DEBUG TRAIN Batch 13/4900 loss 18.552864 loss_att 20.693443 loss_ctc 21.626692 loss_rnnt 17.627083 hw_loss 0.164666 lr 0.00046969 rank 5
2023-02-21 09:43:39,641 DEBUG TRAIN Batch 13/4900 loss 8.927956 loss_att 15.601673 loss_ctc 13.008931 loss_rnnt 6.999512 hw_loss 0.092946 lr 0.00046966 rank 3
2023-02-21 09:43:39,640 DEBUG TRAIN Batch 13/4900 loss 13.066270 loss_att 14.129660 loss_ctc 19.433056 loss_rnnt 11.977912 hw_loss 0.050205 lr 0.00046964 rank 6
2023-02-21 09:43:39,643 DEBUG TRAIN Batch 13/4900 loss 7.561341 loss_att 12.367188 loss_ctc 11.490314 loss_rnnt 5.999670 hw_loss 0.143697 lr 0.00046963 rank 7
2023-02-21 09:43:39,680 DEBUG TRAIN Batch 13/4900 loss 11.228780 loss_att 15.135550 loss_ctc 16.280844 loss_rnnt 9.663981 hw_loss 0.205944 lr 0.00046971 rank 0
2023-02-21 09:45:00,951 DEBUG TRAIN Batch 13/5000 loss 15.772124 loss_att 16.756134 loss_ctc 21.707239 loss_rnnt 14.732071 hw_loss 0.097319 lr 0.00046940 rank 2
2023-02-21 09:45:00,956 DEBUG TRAIN Batch 13/5000 loss 13.342111 loss_att 15.921261 loss_ctc 18.090435 loss_rnnt 12.098724 hw_loss 0.177086 lr 0.00046949 rank 1
2023-02-21 09:45:00,956 DEBUG TRAIN Batch 13/5000 loss 17.472784 loss_att 17.541079 loss_ctc 23.236639 loss_rnnt 16.608667 hw_loss 0.153643 lr 0.00046942 rank 7
2023-02-21 09:45:00,956 DEBUG TRAIN Batch 13/5000 loss 23.426592 loss_att 23.468372 loss_ctc 28.513458 loss_rnnt 22.705187 hw_loss 0.065250 lr 0.00046943 rank 4
2023-02-21 09:45:00,960 DEBUG TRAIN Batch 13/5000 loss 19.632105 loss_att 20.451313 loss_ctc 26.099268 loss_rnnt 18.526636 hw_loss 0.148758 lr 0.00046944 rank 6
2023-02-21 09:45:00,965 DEBUG TRAIN Batch 13/5000 loss 11.842326 loss_att 12.926440 loss_ctc 17.183834 loss_rnnt 10.857210 hw_loss 0.105170 lr 0.00046948 rank 5
2023-02-21 09:45:00,969 DEBUG TRAIN Batch 13/5000 loss 8.471045 loss_att 12.180790 loss_ctc 11.433325 loss_rnnt 7.313029 hw_loss 0.039553 lr 0.00046945 rank 3
2023-02-21 09:45:01,001 DEBUG TRAIN Batch 13/5000 loss 9.190389 loss_att 8.991169 loss_ctc 11.550306 loss_rnnt 8.740330 hw_loss 0.328590 lr 0.00046950 rank 0
2023-02-21 09:46:20,391 DEBUG TRAIN Batch 13/5100 loss 11.292976 loss_att 13.181451 loss_ctc 16.023941 loss_rnnt 10.232086 hw_loss 0.098249 lr 0.00046929 rank 0
2023-02-21 09:46:20,392 DEBUG TRAIN Batch 13/5100 loss 9.398250 loss_att 12.071752 loss_ctc 10.787483 loss_rnnt 8.605023 hw_loss 0.137426 lr 0.00046919 rank 2
2023-02-21 09:46:20,395 DEBUG TRAIN Batch 13/5100 loss 13.514047 loss_att 13.918883 loss_ctc 16.448223 loss_rnnt 12.883911 hw_loss 0.296147 lr 0.00046922 rank 7
2023-02-21 09:46:20,398 DEBUG TRAIN Batch 13/5100 loss 12.356513 loss_att 11.100953 loss_ctc 13.773139 loss_rnnt 12.242616 hw_loss 0.330237 lr 0.00046922 rank 4
2023-02-21 09:46:20,400 DEBUG TRAIN Batch 13/5100 loss 8.481275 loss_att 10.561930 loss_ctc 11.997342 loss_rnnt 7.494253 hw_loss 0.191403 lr 0.00046928 rank 1
2023-02-21 09:46:20,401 DEBUG TRAIN Batch 13/5100 loss 9.931889 loss_att 9.323345 loss_ctc 11.475329 loss_rnnt 9.706958 hw_loss 0.264087 lr 0.00046923 rank 6
2023-02-21 09:46:20,402 DEBUG TRAIN Batch 13/5100 loss 9.812778 loss_att 14.826965 loss_ctc 9.649044 loss_rnnt 8.743473 hw_loss 0.165562 lr 0.00046927 rank 5
2023-02-21 09:46:20,403 DEBUG TRAIN Batch 13/5100 loss 9.346780 loss_att 10.350189 loss_ctc 11.354527 loss_rnnt 8.818113 hw_loss 0.113034 lr 0.00046924 rank 3
2023-02-21 09:47:38,895 DEBUG TRAIN Batch 13/5200 loss 13.536467 loss_att 16.462631 loss_ctc 13.664558 loss_rnnt 12.898425 hw_loss 0.066994 lr 0.00046901 rank 7
2023-02-21 09:47:38,900 DEBUG TRAIN Batch 13/5200 loss 24.911169 loss_att 30.938927 loss_ctc 27.747372 loss_rnnt 23.289602 hw_loss 0.070983 lr 0.00046909 rank 0
2023-02-21 09:47:38,902 DEBUG TRAIN Batch 13/5200 loss 11.123252 loss_att 16.010582 loss_ctc 15.613371 loss_rnnt 9.455379 hw_loss 0.171984 lr 0.00046908 rank 1
2023-02-21 09:47:38,902 DEBUG TRAIN Batch 13/5200 loss 26.668423 loss_att 44.674141 loss_ctc 34.122856 loss_rnnt 22.014544 hw_loss 0.110271 lr 0.00046898 rank 2
2023-02-21 09:47:38,906 DEBUG TRAIN Batch 13/5200 loss 10.963644 loss_att 11.476959 loss_ctc 13.017946 loss_rnnt 10.507287 hw_loss 0.149597 lr 0.00046901 rank 4
2023-02-21 09:47:38,907 DEBUG TRAIN Batch 13/5200 loss 7.444774 loss_att 12.188138 loss_ctc 12.594732 loss_rnnt 5.739854 hw_loss 0.130472 lr 0.00046904 rank 3
2023-02-21 09:47:38,913 DEBUG TRAIN Batch 13/5200 loss 14.244608 loss_att 17.906937 loss_ctc 14.775574 loss_rnnt 13.426826 hw_loss 0.027229 lr 0.00046907 rank 5
2023-02-21 09:47:38,947 DEBUG TRAIN Batch 13/5200 loss 10.362292 loss_att 12.185226 loss_ctc 16.629314 loss_rnnt 9.125603 hw_loss 0.068438 lr 0.00046902 rank 6
2023-02-21 09:48:59,135 DEBUG TRAIN Batch 13/5300 loss 2.180608 loss_att 6.919225 loss_ctc 2.594860 loss_rnnt 1.102811 hw_loss 0.140323 lr 0.00046881 rank 7
2023-02-21 09:48:59,137 DEBUG TRAIN Batch 13/5300 loss 6.475771 loss_att 9.095059 loss_ctc 8.513502 loss_rnnt 5.625201 hw_loss 0.103153 lr 0.00046887 rank 1
2023-02-21 09:48:59,142 DEBUG TRAIN Batch 13/5300 loss 22.119268 loss_att 23.966103 loss_ctc 28.855515 loss_rnnt 20.794718 hw_loss 0.106905 lr 0.00046881 rank 4
2023-02-21 09:48:59,141 DEBUG TRAIN Batch 13/5300 loss 15.964409 loss_att 18.830257 loss_ctc 21.189529 loss_rnnt 14.613820 hw_loss 0.151381 lr 0.00046886 rank 5
2023-02-21 09:48:59,142 DEBUG TRAIN Batch 13/5300 loss 11.589665 loss_att 14.816462 loss_ctc 12.168312 loss_rnnt 10.852640 hw_loss 0.027210 lr 0.00046878 rank 2
2023-02-21 09:48:59,143 DEBUG TRAIN Batch 13/5300 loss 12.697416 loss_att 16.579954 loss_ctc 13.276514 loss_rnnt 11.750816 hw_loss 0.174149 lr 0.00046888 rank 0
2023-02-21 09:48:59,142 DEBUG TRAIN Batch 13/5300 loss 15.193821 loss_att 20.670942 loss_ctc 22.073666 loss_rnnt 13.127414 hw_loss 0.100631 lr 0.00046882 rank 6
2023-02-21 09:48:59,150 DEBUG TRAIN Batch 13/5300 loss 18.135935 loss_att 19.508451 loss_ctc 22.466459 loss_rnnt 17.186556 hw_loss 0.182757 lr 0.00046883 rank 3
2023-02-21 09:50:18,313 DEBUG TRAIN Batch 13/5400 loss 15.054687 loss_att 17.055925 loss_ctc 18.845413 loss_rnnt 14.063574 hw_loss 0.160190 lr 0.00046857 rank 2
2023-02-21 09:50:18,314 DEBUG TRAIN Batch 13/5400 loss 11.181180 loss_att 13.212954 loss_ctc 14.617931 loss_rnnt 10.240503 hw_loss 0.142665 lr 0.00046867 rank 1
2023-02-21 09:50:18,314 DEBUG TRAIN Batch 13/5400 loss 8.981630 loss_att 12.058676 loss_ctc 12.982215 loss_rnnt 7.718583 hw_loss 0.214174 lr 0.00046868 rank 0
2023-02-21 09:50:18,317 DEBUG TRAIN Batch 13/5400 loss 19.561399 loss_att 18.347712 loss_ctc 17.032257 loss_rnnt 20.033707 hw_loss 0.201839 lr 0.00046860 rank 4
2023-02-21 09:50:18,316 DEBUG TRAIN Batch 13/5400 loss 11.943350 loss_att 15.507724 loss_ctc 13.642481 loss_rnnt 10.927402 hw_loss 0.143478 lr 0.00046865 rank 5
2023-02-21 09:50:18,318 DEBUG TRAIN Batch 13/5400 loss 16.825035 loss_att 19.766838 loss_ctc 20.374212 loss_rnnt 15.717225 hw_loss 0.086669 lr 0.00046860 rank 7
2023-02-21 09:50:18,319 DEBUG TRAIN Batch 13/5400 loss 18.508631 loss_att 21.480139 loss_ctc 20.408192 loss_rnnt 17.515091 hw_loss 0.273678 lr 0.00046861 rank 6
2023-02-21 09:50:18,364 DEBUG TRAIN Batch 13/5400 loss 25.107346 loss_att 24.526482 loss_ctc 29.702286 loss_rnnt 24.526848 hw_loss 0.157522 lr 0.00046862 rank 3
2023-02-21 09:51:38,038 DEBUG TRAIN Batch 13/5500 loss 9.198017 loss_att 13.653492 loss_ctc 12.338184 loss_rnnt 7.847977 hw_loss 0.075481 lr 0.00046839 rank 7
2023-02-21 09:51:38,040 DEBUG TRAIN Batch 13/5500 loss 11.918065 loss_att 11.903997 loss_ctc 15.413976 loss_rnnt 11.417480 hw_loss 0.069894 lr 0.00046836 rank 2
2023-02-21 09:51:38,043 DEBUG TRAIN Batch 13/5500 loss 8.248229 loss_att 10.812511 loss_ctc 9.269156 loss_rnnt 7.507088 hw_loss 0.172802 lr 0.00046846 rank 1
2023-02-21 09:51:38,044 DEBUG TRAIN Batch 13/5500 loss 16.052504 loss_att 19.085453 loss_ctc 21.703718 loss_rnnt 14.660194 hw_loss 0.060422 lr 0.00046840 rank 4
2023-02-21 09:51:38,047 DEBUG TRAIN Batch 13/5500 loss 12.810486 loss_att 16.832703 loss_ctc 16.925428 loss_rnnt 11.421251 hw_loss 0.067746 lr 0.00046842 rank 3
2023-02-21 09:51:38,048 DEBUG TRAIN Batch 13/5500 loss 9.169589 loss_att 10.708797 loss_ctc 11.120440 loss_rnnt 8.474364 hw_loss 0.238630 lr 0.00046845 rank 5
2023-02-21 09:51:38,048 DEBUG TRAIN Batch 13/5500 loss 15.124271 loss_att 15.691740 loss_ctc 17.207468 loss_rnnt 14.659710 hw_loss 0.137452 lr 0.00046847 rank 0
2023-02-21 09:51:38,090 DEBUG TRAIN Batch 13/5500 loss 13.346347 loss_att 16.133358 loss_ctc 18.424595 loss_rnnt 12.088198 hw_loss 0.044335 lr 0.00046841 rank 6
2023-02-21 09:52:57,311 DEBUG TRAIN Batch 13/5600 loss 7.295504 loss_att 10.388227 loss_ctc 9.581425 loss_rnnt 6.308372 hw_loss 0.119622 lr 0.00046816 rank 2
2023-02-21 09:52:57,312 DEBUG TRAIN Batch 13/5600 loss 15.630610 loss_att 19.510942 loss_ctc 20.984247 loss_rnnt 14.096302 hw_loss 0.083292 lr 0.00046819 rank 4
2023-02-21 09:52:57,312 DEBUG TRAIN Batch 13/5600 loss 17.889572 loss_att 19.866301 loss_ctc 22.507875 loss_rnnt 16.762676 hw_loss 0.217080 lr 0.00046819 rank 7
2023-02-21 09:52:57,317 DEBUG TRAIN Batch 13/5600 loss 13.246253 loss_att 13.229892 loss_ctc 12.861432 loss_rnnt 13.251891 hw_loss 0.091769 lr 0.00046826 rank 0
2023-02-21 09:52:57,318 DEBUG TRAIN Batch 13/5600 loss 24.788223 loss_att 31.138277 loss_ctc 30.000681 loss_rnnt 22.727467 hw_loss 0.179528 lr 0.00046825 rank 1
2023-02-21 09:52:57,319 DEBUG TRAIN Batch 13/5600 loss 14.669245 loss_att 18.626036 loss_ctc 17.471582 loss_rnnt 13.444723 hw_loss 0.111598 lr 0.00046820 rank 6
2023-02-21 09:52:57,322 DEBUG TRAIN Batch 13/5600 loss 15.212749 loss_att 16.675394 loss_ctc 18.837631 loss_rnnt 14.393435 hw_loss 0.081499 lr 0.00046824 rank 5
2023-02-21 09:52:57,325 DEBUG TRAIN Batch 13/5600 loss 10.178108 loss_att 14.130302 loss_ctc 11.715243 loss_rnnt 9.148113 hw_loss 0.064883 lr 0.00046821 rank 3
2023-02-21 09:54:19,094 DEBUG TRAIN Batch 13/5700 loss 12.027378 loss_att 13.038551 loss_ctc 16.282499 loss_rnnt 11.165051 hw_loss 0.173893 lr 0.00046798 rank 7
2023-02-21 09:54:19,094 DEBUG TRAIN Batch 13/5700 loss 18.824329 loss_att 21.201283 loss_ctc 25.934986 loss_rnnt 17.366037 hw_loss 0.065277 lr 0.00046799 rank 4
2023-02-21 09:54:19,099 DEBUG TRAIN Batch 13/5700 loss 8.977755 loss_att 13.143885 loss_ctc 12.143413 loss_rnnt 7.658689 hw_loss 0.119536 lr 0.00046805 rank 1
2023-02-21 09:54:19,102 DEBUG TRAIN Batch 13/5700 loss 11.786572 loss_att 13.793812 loss_ctc 15.795877 loss_rnnt 10.735164 hw_loss 0.216348 lr 0.00046795 rank 2
2023-02-21 09:54:19,104 DEBUG TRAIN Batch 13/5700 loss 13.190997 loss_att 11.368676 loss_ctc 16.467272 loss_rnnt 12.947327 hw_loss 0.321184 lr 0.00046804 rank 5
2023-02-21 09:54:19,105 DEBUG TRAIN Batch 13/5700 loss 10.820408 loss_att 13.352120 loss_ctc 13.705863 loss_rnnt 9.856055 hw_loss 0.137406 lr 0.00046800 rank 6
2023-02-21 09:54:19,107 DEBUG TRAIN Batch 13/5700 loss 8.019313 loss_att 9.336700 loss_ctc 9.158743 loss_rnnt 7.513407 hw_loss 0.169695 lr 0.00046801 rank 3
2023-02-21 09:54:19,157 DEBUG TRAIN Batch 13/5700 loss 12.373539 loss_att 19.420500 loss_ctc 12.195224 loss_rnnt 10.904216 hw_loss 0.156951 lr 0.00046806 rank 0
2023-02-21 09:55:39,104 DEBUG TRAIN Batch 13/5800 loss 11.410233 loss_att 13.849228 loss_ctc 11.642302 loss_rnnt 10.775287 hw_loss 0.217887 lr 0.00046784 rank 1
2023-02-21 09:55:39,103 DEBUG TRAIN Batch 13/5800 loss 9.086834 loss_att 11.282066 loss_ctc 12.600248 loss_rnnt 8.133240 hw_loss 0.086422 lr 0.00046775 rank 2
2023-02-21 09:55:39,104 DEBUG TRAIN Batch 13/5800 loss 5.978588 loss_att 7.424202 loss_ctc 6.826790 loss_rnnt 5.509746 hw_loss 0.124923 lr 0.00046785 rank 0
2023-02-21 09:55:39,105 DEBUG TRAIN Batch 13/5800 loss 22.197746 loss_att 22.852016 loss_ctc 26.670677 loss_rnnt 21.455236 hw_loss 0.028621 lr 0.00046778 rank 4
2023-02-21 09:55:39,106 DEBUG TRAIN Batch 13/5800 loss 5.985220 loss_att 9.342530 loss_ctc 8.060927 loss_rnnt 4.931659 hw_loss 0.197509 lr 0.00046778 rank 7
2023-02-21 09:55:39,110 DEBUG TRAIN Batch 13/5800 loss 19.067411 loss_att 19.072321 loss_ctc 22.656012 loss_rnnt 18.496986 hw_loss 0.170559 lr 0.00046783 rank 5
2023-02-21 09:55:39,111 DEBUG TRAIN Batch 13/5800 loss 6.772339 loss_att 10.585610 loss_ctc 10.449448 loss_rnnt 5.492057 hw_loss 0.051275 lr 0.00046780 rank 3
2023-02-21 09:55:39,152 DEBUG TRAIN Batch 13/5800 loss 11.192216 loss_att 14.212545 loss_ctc 16.748541 loss_rnnt 9.821016 hw_loss 0.049294 lr 0.00046779 rank 6
2023-02-21 09:56:58,173 DEBUG TRAIN Batch 13/5900 loss 19.326536 loss_att 22.969330 loss_ctc 23.503441 loss_rnnt 17.986094 hw_loss 0.103055 lr 0.00046765 rank 0
2023-02-21 09:56:58,174 DEBUG TRAIN Batch 13/5900 loss 11.136588 loss_att 16.862988 loss_ctc 12.939011 loss_rnnt 9.678394 hw_loss 0.136107 lr 0.00046764 rank 1
2023-02-21 09:56:58,174 DEBUG TRAIN Batch 13/5900 loss 13.717745 loss_att 18.772072 loss_ctc 22.137997 loss_rnnt 11.570464 hw_loss 0.025716 lr 0.00046757 rank 7
2023-02-21 09:56:58,174 DEBUG TRAIN Batch 13/5900 loss 18.825689 loss_att 20.559868 loss_ctc 20.156200 loss_rnnt 18.260342 hw_loss 0.077082 lr 0.00046763 rank 5
2023-02-21 09:56:58,174 DEBUG TRAIN Batch 13/5900 loss 18.185774 loss_att 21.482738 loss_ctc 20.226751 loss_rnnt 17.194172 hw_loss 0.112648 lr 0.00046755 rank 2
2023-02-21 09:56:58,175 DEBUG TRAIN Batch 13/5900 loss 11.585786 loss_att 15.538366 loss_ctc 13.873564 loss_rnnt 10.416522 hw_loss 0.138207 lr 0.00046758 rank 4
2023-02-21 09:56:58,176 DEBUG TRAIN Batch 13/5900 loss 13.310680 loss_att 13.362735 loss_ctc 16.085173 loss_rnnt 12.876918 hw_loss 0.100160 lr 0.00046759 rank 6
2023-02-21 09:56:58,179 DEBUG TRAIN Batch 13/5900 loss 11.348823 loss_att 14.268578 loss_ctc 11.792353 loss_rnnt 10.649462 hw_loss 0.105513 lr 0.00046760 rank 3
2023-02-21 09:58:17,615 DEBUG TRAIN Batch 13/6000 loss 16.484505 loss_att 20.116459 loss_ctc 19.196918 loss_rnnt 15.356262 hw_loss 0.075367 lr 0.00046737 rank 7
2023-02-21 09:58:17,616 DEBUG TRAIN Batch 13/6000 loss 13.006847 loss_att 19.618587 loss_ctc 16.933685 loss_rnnt 11.112979 hw_loss 0.089892 lr 0.00046743 rank 1
2023-02-21 09:58:17,618 DEBUG TRAIN Batch 13/6000 loss 11.595303 loss_att 16.379745 loss_ctc 15.381750 loss_rnnt 10.100512 hw_loss 0.061956 lr 0.00046734 rank 2
2023-02-21 09:58:17,619 DEBUG TRAIN Batch 13/6000 loss 11.392978 loss_att 15.007853 loss_ctc 18.250481 loss_rnnt 9.625031 hw_loss 0.244947 lr 0.00046737 rank 4
2023-02-21 09:58:17,619 DEBUG TRAIN Batch 13/6000 loss 20.098772 loss_att 22.679466 loss_ctc 23.098831 loss_rnnt 19.135918 hw_loss 0.087577 lr 0.00046744 rank 0
2023-02-21 09:58:17,620 DEBUG TRAIN Batch 13/6000 loss 19.993418 loss_att 23.851749 loss_ctc 23.490921 loss_rnnt 18.661999 hw_loss 0.175162 lr 0.00046738 rank 6
2023-02-21 09:58:17,627 DEBUG TRAIN Batch 13/6000 loss 16.274130 loss_att 20.678347 loss_ctc 24.113605 loss_rnnt 14.308818 hw_loss 0.073511 lr 0.00046742 rank 5
2023-02-21 09:58:17,671 DEBUG TRAIN Batch 13/6000 loss 14.017405 loss_att 17.179308 loss_ctc 23.881578 loss_rnnt 11.943308 hw_loss 0.237174 lr 0.00046739 rank 3
2023-02-21 09:59:36,580 DEBUG TRAIN Batch 13/6100 loss 14.752012 loss_att 17.263906 loss_ctc 20.629187 loss_rnnt 13.364061 hw_loss 0.191152 lr 0.00046718 rank 6
2023-02-21 09:59:36,581 DEBUG TRAIN Batch 13/6100 loss 14.045238 loss_att 15.992871 loss_ctc 16.372879 loss_rnnt 13.315109 hw_loss 0.056717 lr 0.00046717 rank 4
2023-02-21 09:59:36,582 DEBUG TRAIN Batch 13/6100 loss 10.857616 loss_att 15.228378 loss_ctc 12.196915 loss_rnnt 9.731564 hw_loss 0.137488 lr 0.00046723 rank 1
2023-02-21 09:59:36,582 DEBUG TRAIN Batch 13/6100 loss 18.238096 loss_att 23.378374 loss_ctc 30.597126 loss_rnnt 15.507648 hw_loss 0.102230 lr 0.00046717 rank 7
2023-02-21 09:59:36,582 DEBUG TRAIN Batch 13/6100 loss 14.651033 loss_att 18.166780 loss_ctc 17.849018 loss_rnnt 13.441130 hw_loss 0.150672 lr 0.00046719 rank 3
2023-02-21 09:59:36,586 DEBUG TRAIN Batch 13/6100 loss 20.928505 loss_att 23.164986 loss_ctc 28.368872 loss_rnnt 19.387735 hw_loss 0.190170 lr 0.00046714 rank 2
2023-02-21 09:59:36,587 DEBUG TRAIN Batch 13/6100 loss 33.162384 loss_att 42.567204 loss_ctc 30.863365 loss_rnnt 31.559113 hw_loss 0.054081 lr 0.00046722 rank 5
2023-02-21 09:59:36,591 DEBUG TRAIN Batch 13/6100 loss 11.210138 loss_att 11.169539 loss_ctc 13.513374 loss_rnnt 10.797523 hw_loss 0.213069 lr 0.00046724 rank 0
2023-02-21 10:00:55,638 DEBUG TRAIN Batch 13/6200 loss 13.828315 loss_att 19.536541 loss_ctc 18.259359 loss_rnnt 12.049918 hw_loss 0.086147 lr 0.00046696 rank 4
2023-02-21 10:00:55,644 DEBUG TRAIN Batch 13/6200 loss 14.559843 loss_att 19.970428 loss_ctc 20.491255 loss_rnnt 12.639776 hw_loss 0.088303 lr 0.00046697 rank 6
2023-02-21 10:00:55,645 DEBUG TRAIN Batch 13/6200 loss 7.236774 loss_att 9.375764 loss_ctc 11.292158 loss_rnnt 6.202395 hw_loss 0.123493 lr 0.00046696 rank 7
2023-02-21 10:00:55,646 DEBUG TRAIN Batch 13/6200 loss 14.001912 loss_att 16.889500 loss_ctc 14.928441 loss_rnnt 13.233634 hw_loss 0.126042 lr 0.00046693 rank 2
2023-02-21 10:00:55,647 DEBUG TRAIN Batch 13/6200 loss 17.632277 loss_att 16.208279 loss_ctc 25.778522 loss_rnnt 16.781548 hw_loss 0.092555 lr 0.00046703 rank 1
2023-02-21 10:00:55,647 DEBUG TRAIN Batch 13/6200 loss 16.600342 loss_att 20.490784 loss_ctc 25.005327 loss_rnnt 14.603670 hw_loss 0.183597 lr 0.00046702 rank 5
2023-02-21 10:00:55,649 DEBUG TRAIN Batch 13/6200 loss 10.222194 loss_att 11.103062 loss_ctc 11.644838 loss_rnnt 9.649147 hw_loss 0.388475 lr 0.00046699 rank 3
2023-02-21 10:00:55,652 DEBUG TRAIN Batch 13/6200 loss 21.677963 loss_att 24.904243 loss_ctc 31.761801 loss_rnnt 19.637882 hw_loss 0.094337 lr 0.00046704 rank 0
2023-02-21 10:02:13,848 DEBUG TRAIN Batch 13/6300 loss 11.931900 loss_att 12.381105 loss_ctc 12.951810 loss_rnnt 11.640179 hw_loss 0.123546 lr 0.00046676 rank 4
2023-02-21 10:02:13,851 DEBUG TRAIN Batch 13/6300 loss 3.599952 loss_att 6.635128 loss_ctc 6.909927 loss_rnnt 2.498193 hw_loss 0.100114 lr 0.00046673 rank 2
2023-02-21 10:02:13,852 DEBUG TRAIN Batch 13/6300 loss 19.843208 loss_att 20.381393 loss_ctc 26.294449 loss_rnnt 18.826786 hw_loss 0.091160 lr 0.00046682 rank 1
2023-02-21 10:02:13,854 DEBUG TRAIN Batch 13/6300 loss 12.558739 loss_att 15.141042 loss_ctc 13.470469 loss_rnnt 11.834842 hw_loss 0.161010 lr 0.00046676 rank 7
2023-02-21 10:02:13,859 DEBUG TRAIN Batch 13/6300 loss 15.165785 loss_att 13.962339 loss_ctc 19.020834 loss_rnnt 14.773772 hw_loss 0.222555 lr 0.00046683 rank 0
2023-02-21 10:02:13,860 DEBUG TRAIN Batch 13/6300 loss 17.513279 loss_att 21.573030 loss_ctc 21.558441 loss_rnnt 16.052759 hw_loss 0.204775 lr 0.00046677 rank 6
2023-02-21 10:02:13,862 DEBUG TRAIN Batch 13/6300 loss 19.842947 loss_att 22.338711 loss_ctc 23.208269 loss_rnnt 18.737045 hw_loss 0.296321 lr 0.00046678 rank 3
2023-02-21 10:02:13,864 DEBUG TRAIN Batch 13/6300 loss 11.839854 loss_att 12.614659 loss_ctc 14.270656 loss_rnnt 11.287329 hw_loss 0.137733 lr 0.00046681 rank 5
2023-02-21 10:03:35,188 DEBUG TRAIN Batch 13/6400 loss 9.232748 loss_att 11.941383 loss_ctc 11.022799 loss_rnnt 8.389445 hw_loss 0.117943 lr 0.00046653 rank 2
2023-02-21 10:03:35,189 DEBUG TRAIN Batch 13/6400 loss 10.528096 loss_att 12.560637 loss_ctc 13.137912 loss_rnnt 9.739676 hw_loss 0.063633 lr 0.00046662 rank 1
2023-02-21 10:03:35,194 DEBUG TRAIN Batch 13/6400 loss 15.524894 loss_att 15.726110 loss_ctc 19.288292 loss_rnnt 14.931177 hw_loss 0.096912 lr 0.00046657 rank 6
2023-02-21 10:03:35,198 DEBUG TRAIN Batch 13/6400 loss 7.504043 loss_att 14.359159 loss_ctc 10.108470 loss_rnnt 5.724483 hw_loss 0.114899 lr 0.00046658 rank 3
2023-02-21 10:03:35,201 DEBUG TRAIN Batch 13/6400 loss 10.868545 loss_att 16.057005 loss_ctc 14.902016 loss_rnnt 9.244366 hw_loss 0.091293 lr 0.00046663 rank 0
2023-02-21 10:03:35,204 DEBUG TRAIN Batch 13/6400 loss 13.557285 loss_att 15.122492 loss_ctc 20.512743 loss_rnnt 12.268840 hw_loss 0.090018 lr 0.00046661 rank 5
2023-02-21 10:03:35,214 DEBUG TRAIN Batch 13/6400 loss 32.938236 loss_att 35.791710 loss_ctc 40.178795 loss_rnnt 31.350744 hw_loss 0.096363 lr 0.00046655 rank 7
2023-02-21 10:03:35,215 DEBUG TRAIN Batch 13/6400 loss 7.607736 loss_att 11.476021 loss_ctc 8.643478 loss_rnnt 6.652812 hw_loss 0.080940 lr 0.00046656 rank 4
2023-02-21 10:04:53,130 DEBUG TRAIN Batch 13/6500 loss 21.217600 loss_att 20.468178 loss_ctc 23.008705 loss_rnnt 20.972332 hw_loss 0.293137 lr 0.00046641 rank 5
2023-02-21 10:04:53,130 DEBUG TRAIN Batch 13/6500 loss 9.014578 loss_att 11.831554 loss_ctc 10.803724 loss_rnnt 8.114872 hw_loss 0.183296 lr 0.00046636 rank 6
2023-02-21 10:04:53,132 DEBUG TRAIN Batch 13/6500 loss 8.902463 loss_att 14.277493 loss_ctc 10.083733 loss_rnnt 7.645044 hw_loss 0.046707 lr 0.00046635 rank 4
2023-02-21 10:04:53,132 DEBUG TRAIN Batch 13/6500 loss 10.105700 loss_att 14.253927 loss_ctc 13.673732 loss_rnnt 8.747194 hw_loss 0.099603 lr 0.00046642 rank 1
2023-02-21 10:04:53,132 DEBUG TRAIN Batch 13/6500 loss 9.955346 loss_att 12.357608 loss_ctc 13.887850 loss_rnnt 8.850520 hw_loss 0.187572 lr 0.00046643 rank 0
2023-02-21 10:04:53,154 DEBUG TRAIN Batch 13/6500 loss 7.374347 loss_att 12.252228 loss_ctc 10.555338 loss_rnnt 5.939291 hw_loss 0.066278 lr 0.00046635 rank 7
2023-02-21 10:04:53,167 DEBUG TRAIN Batch 13/6500 loss 13.255631 loss_att 14.993262 loss_ctc 18.201809 loss_rnnt 12.119828 hw_loss 0.241474 lr 0.00046632 rank 2
2023-02-21 10:04:53,179 DEBUG TRAIN Batch 13/6500 loss 16.185236 loss_att 20.110466 loss_ctc 23.703726 loss_rnnt 14.363588 hw_loss 0.064008 lr 0.00046638 rank 3
2023-02-21 10:06:14,118 DEBUG TRAIN Batch 13/6600 loss 9.616380 loss_att 12.825686 loss_ctc 11.201305 loss_rnnt 8.731426 hw_loss 0.059567 lr 0.00046615 rank 4
2023-02-21 10:06:14,119 DEBUG TRAIN Batch 13/6600 loss 15.775786 loss_att 17.823898 loss_ctc 24.115597 loss_rnnt 14.240707 hw_loss 0.025278 lr 0.00046612 rank 2
2023-02-21 10:06:14,120 DEBUG TRAIN Batch 13/6600 loss 16.555281 loss_att 20.431814 loss_ctc 21.694036 loss_rnnt 15.081810 hw_loss 0.024367 lr 0.00046615 rank 7
2023-02-21 10:06:14,123 DEBUG TRAIN Batch 13/6600 loss 19.705540 loss_att 25.398928 loss_ctc 24.702255 loss_rnnt 17.859264 hw_loss 0.077565 lr 0.00046621 rank 1
2023-02-21 10:06:14,122 DEBUG TRAIN Batch 13/6600 loss 19.844196 loss_att 23.494354 loss_ctc 28.276045 loss_rnnt 17.903332 hw_loss 0.162350 lr 0.00046622 rank 0
2023-02-21 10:06:14,123 DEBUG TRAIN Batch 13/6600 loss 23.925077 loss_att 33.785049 loss_ctc 27.928686 loss_rnnt 21.387854 hw_loss 0.058902 lr 0.00046617 rank 3
2023-02-21 10:06:14,125 DEBUG TRAIN Batch 13/6600 loss 17.409777 loss_att 20.868744 loss_ctc 23.988914 loss_rnnt 15.808692 hw_loss 0.060138 lr 0.00046616 rank 6
2023-02-21 10:06:14,174 DEBUG TRAIN Batch 13/6600 loss 10.717668 loss_att 12.453848 loss_ctc 13.322737 loss_rnnt 9.985446 hw_loss 0.070582 lr 0.00046620 rank 5
2023-02-21 10:07:35,084 DEBUG TRAIN Batch 13/6700 loss 6.504118 loss_att 8.893887 loss_ctc 7.517869 loss_rnnt 5.850627 hw_loss 0.075695 lr 0.00046596 rank 6
2023-02-21 10:07:35,084 DEBUG TRAIN Batch 13/6700 loss 12.603354 loss_att 18.567295 loss_ctc 13.620537 loss_rnnt 11.260280 hw_loss 0.027491 lr 0.00046592 rank 2
2023-02-21 10:07:35,088 DEBUG TRAIN Batch 13/6700 loss 16.452362 loss_att 20.286329 loss_ctc 21.495911 loss_rnnt 14.956614 hw_loss 0.105903 lr 0.00046595 rank 4
2023-02-21 10:07:35,090 DEBUG TRAIN Batch 13/6700 loss 22.517790 loss_att 25.876259 loss_ctc 31.975973 loss_rnnt 20.530499 hw_loss 0.102200 lr 0.00046600 rank 5
2023-02-21 10:07:35,091 DEBUG TRAIN Batch 13/6700 loss 14.129398 loss_att 18.584202 loss_ctc 17.781586 loss_rnnt 12.737117 hw_loss 0.026932 lr 0.00046597 rank 3
2023-02-21 10:07:35,093 DEBUG TRAIN Batch 13/6700 loss 8.801725 loss_att 10.561351 loss_ctc 8.365227 loss_rnnt 8.397385 hw_loss 0.207406 lr 0.00046595 rank 7
2023-02-21 10:07:35,095 DEBUG TRAIN Batch 13/6700 loss 14.326484 loss_att 15.644785 loss_ctc 19.321857 loss_rnnt 13.334084 hw_loss 0.117544 lr 0.00046601 rank 1
2023-02-21 10:07:35,131 DEBUG TRAIN Batch 13/6700 loss 4.196447 loss_att 6.224144 loss_ctc 6.467973 loss_rnnt 3.405063 hw_loss 0.155578 lr 0.00046602 rank 0
2023-02-21 10:08:55,670 DEBUG TRAIN Batch 13/6800 loss 10.353344 loss_att 13.538733 loss_ctc 14.710777 loss_rnnt 9.082335 hw_loss 0.099264 lr 0.00046581 rank 1
2023-02-21 10:08:55,676 DEBUG TRAIN Batch 13/6800 loss 15.444276 loss_att 16.573450 loss_ctc 17.302670 loss_rnnt 14.846034 hw_loss 0.233668 lr 0.00046575 rank 4
2023-02-21 10:08:55,677 DEBUG TRAIN Batch 13/6800 loss 5.519608 loss_att 9.734655 loss_ctc 6.601240 loss_rnnt 4.515682 hw_loss 0.031309 lr 0.00046576 rank 6
2023-02-21 10:08:55,678 DEBUG TRAIN Batch 13/6800 loss 10.504353 loss_att 12.556990 loss_ctc 12.589211 loss_rnnt 9.727804 hw_loss 0.165074 lr 0.00046574 rank 7
2023-02-21 10:08:55,679 DEBUG TRAIN Batch 13/6800 loss 18.255568 loss_att 17.549107 loss_ctc 20.167049 loss_rnnt 18.094357 hw_loss 0.089321 lr 0.00046572 rank 2
2023-02-21 10:08:55,680 DEBUG TRAIN Batch 13/6800 loss 14.822194 loss_att 18.629568 loss_ctc 17.867004 loss_rnnt 13.562642 hw_loss 0.172691 lr 0.00046577 rank 3
2023-02-21 10:08:55,681 DEBUG TRAIN Batch 13/6800 loss 14.089166 loss_att 16.046209 loss_ctc 15.840588 loss_rnnt 13.343606 hw_loss 0.226175 lr 0.00046582 rank 0
2023-02-21 10:08:55,683 DEBUG TRAIN Batch 13/6800 loss 8.279436 loss_att 12.459373 loss_ctc 12.097520 loss_rnnt 6.916755 hw_loss 0.033028 lr 0.00046580 rank 5
2023-02-21 10:10:15,965 DEBUG TRAIN Batch 13/6900 loss 13.743861 loss_att 17.835325 loss_ctc 15.277198 loss_rnnt 12.658648 hw_loss 0.117142 lr 0.00046554 rank 4
2023-02-21 10:10:15,966 DEBUG TRAIN Batch 13/6900 loss 10.904964 loss_att 13.118237 loss_ctc 11.423298 loss_rnnt 10.333945 hw_loss 0.111101 lr 0.00046555 rank 6
2023-02-21 10:10:15,967 DEBUG TRAIN Batch 13/6900 loss 7.084839 loss_att 10.403988 loss_ctc 7.987785 loss_rnnt 6.187953 hw_loss 0.211245 lr 0.00046551 rank 2
2023-02-21 10:10:15,970 DEBUG TRAIN Batch 13/6900 loss 6.507431 loss_att 9.538211 loss_ctc 7.140367 loss_rnnt 5.753518 hw_loss 0.118808 lr 0.00046557 rank 3
2023-02-21 10:10:15,971 DEBUG TRAIN Batch 13/6900 loss 8.427023 loss_att 9.550117 loss_ctc 11.686415 loss_rnnt 7.696469 hw_loss 0.133782 lr 0.00046562 rank 0
2023-02-21 10:10:15,974 DEBUG TRAIN Batch 13/6900 loss 4.819658 loss_att 8.669683 loss_ctc 5.374112 loss_rnnt 3.913144 hw_loss 0.117341 lr 0.00046561 rank 1
2023-02-21 10:10:15,975 DEBUG TRAIN Batch 13/6900 loss 10.025423 loss_att 10.945588 loss_ctc 17.214138 loss_rnnt 8.815619 hw_loss 0.126144 lr 0.00046554 rank 7
2023-02-21 10:10:15,978 DEBUG TRAIN Batch 13/6900 loss 9.427401 loss_att 12.835319 loss_ctc 14.238747 loss_rnnt 8.023788 hw_loss 0.150967 lr 0.00046560 rank 5
2023-02-21 10:11:34,524 DEBUG TRAIN Batch 13/7000 loss 16.475863 loss_att 18.704494 loss_ctc 22.454376 loss_rnnt 15.215423 hw_loss 0.032957 lr 0.00046534 rank 4
2023-02-21 10:11:34,525 DEBUG TRAIN Batch 13/7000 loss 16.867712 loss_att 17.337057 loss_ctc 19.857161 loss_rnnt 16.320423 hw_loss 0.102803 lr 0.00046531 rank 2
2023-02-21 10:11:34,528 DEBUG TRAIN Batch 13/7000 loss 8.746662 loss_att 12.044420 loss_ctc 12.508813 loss_rnnt 7.517238 hw_loss 0.127974 lr 0.00046535 rank 6
2023-02-21 10:11:34,528 DEBUG TRAIN Batch 13/7000 loss 11.749177 loss_att 11.014783 loss_ctc 14.519115 loss_rnnt 11.463938 hw_loss 0.117733 lr 0.00046534 rank 7
2023-02-21 10:11:34,528 DEBUG TRAIN Batch 13/7000 loss 15.384673 loss_att 18.999229 loss_ctc 22.782394 loss_rnnt 13.546957 hw_loss 0.240828 lr 0.00046541 rank 1
2023-02-21 10:11:34,533 DEBUG TRAIN Batch 13/7000 loss 18.576704 loss_att 23.696466 loss_ctc 21.152845 loss_rnnt 17.191771 hw_loss 0.032801 lr 0.00046542 rank 0
2023-02-21 10:11:34,535 DEBUG TRAIN Batch 13/7000 loss 8.774094 loss_att 13.867119 loss_ctc 11.659476 loss_rnnt 7.352515 hw_loss 0.034230 lr 0.00046540 rank 5
2023-02-21 10:11:34,537 DEBUG TRAIN Batch 13/7000 loss 15.553953 loss_att 17.391596 loss_ctc 17.073349 loss_rnnt 14.854107 hw_loss 0.243249 lr 0.00046537 rank 3
2023-02-21 10:12:56,047 DEBUG TRAIN Batch 13/7100 loss 14.037044 loss_att 16.621462 loss_ctc 15.382826 loss_rnnt 13.325988 hw_loss 0.027627 lr 0.00046514 rank 4
2023-02-21 10:12:56,051 DEBUG TRAIN Batch 13/7100 loss 13.509989 loss_att 20.040585 loss_ctc 15.298754 loss_rnnt 11.844431 hw_loss 0.226756 lr 0.00046514 rank 7
2023-02-21 10:12:56,051 DEBUG TRAIN Batch 13/7100 loss 11.688092 loss_att 13.727806 loss_ctc 13.274829 loss_rnnt 10.939433 hw_loss 0.242160 lr 0.00046516 rank 3
2023-02-21 10:12:56,052 DEBUG TRAIN Batch 13/7100 loss 17.332895 loss_att 20.971964 loss_ctc 27.463852 loss_rnnt 15.165947 hw_loss 0.165641 lr 0.00046520 rank 1
2023-02-21 10:12:56,053 DEBUG TRAIN Batch 13/7100 loss 12.951865 loss_att 14.061598 loss_ctc 19.411295 loss_rnnt 11.753469 hw_loss 0.215985 lr 0.00046511 rank 2
2023-02-21 10:12:56,056 DEBUG TRAIN Batch 13/7100 loss 15.063332 loss_att 20.184578 loss_ctc 23.174599 loss_rnnt 12.895773 hw_loss 0.115889 lr 0.00046521 rank 0
2023-02-21 10:12:56,108 DEBUG TRAIN Batch 13/7100 loss 25.904749 loss_att 28.752439 loss_ctc 31.750362 loss_rnnt 24.490194 hw_loss 0.123004 lr 0.00046519 rank 5
2023-02-21 10:12:56,128 DEBUG TRAIN Batch 13/7100 loss 12.420366 loss_att 11.584572 loss_ctc 16.567608 loss_rnnt 11.991048 hw_loss 0.081583 lr 0.00046515 rank 6
2023-02-21 10:14:16,226 DEBUG TRAIN Batch 13/7200 loss 26.539469 loss_att 33.319931 loss_ctc 36.347752 loss_rnnt 23.760763 hw_loss 0.215330 lr 0.00046491 rank 2
2023-02-21 10:14:16,228 DEBUG TRAIN Batch 13/7200 loss 10.601082 loss_att 18.915663 loss_ctc 17.020636 loss_rnnt 8.025087 hw_loss 0.107132 lr 0.00046500 rank 1
2023-02-21 10:14:16,228 DEBUG TRAIN Batch 13/7200 loss 15.231680 loss_att 19.289209 loss_ctc 19.642605 loss_rnnt 13.746580 hw_loss 0.160261 lr 0.00046494 rank 7
2023-02-21 10:14:16,231 DEBUG TRAIN Batch 13/7200 loss 17.342018 loss_att 20.248669 loss_ctc 23.460808 loss_rnnt 15.904723 hw_loss 0.075237 lr 0.00046494 rank 4
2023-02-21 10:14:16,234 DEBUG TRAIN Batch 13/7200 loss 23.275961 loss_att 25.548393 loss_ctc 28.458122 loss_rnnt 22.073471 hw_loss 0.106970 lr 0.00046495 rank 6
2023-02-21 10:14:16,235 DEBUG TRAIN Batch 13/7200 loss 8.794631 loss_att 14.479113 loss_ctc 13.542202 loss_rnnt 6.960927 hw_loss 0.119621 lr 0.00046501 rank 0
2023-02-21 10:14:16,238 DEBUG TRAIN Batch 13/7200 loss 6.688489 loss_att 10.855100 loss_ctc 10.236349 loss_rnnt 5.332518 hw_loss 0.093001 lr 0.00046499 rank 5
2023-02-21 10:14:16,284 DEBUG TRAIN Batch 13/7200 loss 14.165799 loss_att 16.556793 loss_ctc 24.411087 loss_rnnt 12.240404 hw_loss 0.152172 lr 0.00046496 rank 3
2023-02-21 10:15:35,073 DEBUG TRAIN Batch 13/7300 loss 16.535509 loss_att 17.763256 loss_ctc 14.517498 loss_rnnt 16.546108 hw_loss 0.024226 lr 0.00046480 rank 1
2023-02-21 10:15:35,074 DEBUG TRAIN Batch 13/7300 loss 4.218550 loss_att 10.641285 loss_ctc 4.877042 loss_rnnt 2.750632 hw_loss 0.179197 lr 0.00046475 rank 6
2023-02-21 10:15:35,075 DEBUG TRAIN Batch 13/7300 loss 8.840878 loss_att 13.960338 loss_ctc 10.506034 loss_rnnt 7.564160 hw_loss 0.057762 lr 0.00046474 rank 4
2023-02-21 10:15:35,075 DEBUG TRAIN Batch 13/7300 loss 11.758494 loss_att 17.216892 loss_ctc 14.250412 loss_rnnt 10.255743 hw_loss 0.147783 lr 0.00046471 rank 2
2023-02-21 10:15:35,082 DEBUG TRAIN Batch 13/7300 loss 7.194123 loss_att 10.717935 loss_ctc 8.153517 loss_rnnt 6.287883 hw_loss 0.137921 lr 0.00046474 rank 7
2023-02-21 10:15:35,086 DEBUG TRAIN Batch 13/7300 loss 9.348642 loss_att 14.254504 loss_ctc 11.587427 loss_rnnt 8.033089 hw_loss 0.067268 lr 0.00046479 rank 5
2023-02-21 10:15:35,089 DEBUG TRAIN Batch 13/7300 loss 13.532747 loss_att 18.778801 loss_ctc 19.372322 loss_rnnt 11.609481 hw_loss 0.178961 lr 0.00046476 rank 3
2023-02-21 10:15:35,128 DEBUG TRAIN Batch 13/7300 loss 16.545973 loss_att 16.920668 loss_ctc 17.289007 loss_rnnt 16.288300 hw_loss 0.156863 lr 0.00046481 rank 0
2023-02-21 10:16:54,818 DEBUG TRAIN Batch 13/7400 loss 13.203296 loss_att 16.306648 loss_ctc 17.902378 loss_rnnt 11.840296 hw_loss 0.217096 lr 0.00046460 rank 1
2023-02-21 10:16:54,818 DEBUG TRAIN Batch 13/7400 loss 2.949114 loss_att 6.732338 loss_ctc 6.326994 loss_rnnt 1.700907 hw_loss 0.077208 lr 0.00046454 rank 7
2023-02-21 10:16:54,824 DEBUG TRAIN Batch 13/7400 loss 8.297911 loss_att 14.615061 loss_ctc 11.500721 loss_rnnt 6.561744 hw_loss 0.085678 lr 0.00046451 rank 2
2023-02-21 10:16:54,827 DEBUG TRAIN Batch 13/7400 loss 21.633640 loss_att 22.609676 loss_ctc 23.788521 loss_rnnt 21.079195 hw_loss 0.134848 lr 0.00046454 rank 4
2023-02-21 10:16:54,833 DEBUG TRAIN Batch 13/7400 loss 10.098409 loss_att 12.032529 loss_ctc 13.960041 loss_rnnt 9.156469 hw_loss 0.075432 lr 0.00046455 rank 6
2023-02-21 10:16:54,837 DEBUG TRAIN Batch 13/7400 loss 11.869597 loss_att 15.836712 loss_ctc 17.315670 loss_rnnt 10.308440 hw_loss 0.077985 lr 0.00046459 rank 5
2023-02-21 10:16:54,844 DEBUG TRAIN Batch 13/7400 loss 12.017924 loss_att 13.184031 loss_ctc 12.857746 loss_rnnt 11.528154 hw_loss 0.271072 lr 0.00046461 rank 0
2023-02-21 10:16:54,874 DEBUG TRAIN Batch 13/7400 loss 11.151315 loss_att 11.710141 loss_ctc 13.071558 loss_rnnt 10.749218 hw_loss 0.064312 lr 0.00046456 rank 3
2023-02-21 10:18:15,675 DEBUG TRAIN Batch 13/7500 loss 30.792759 loss_att 28.025745 loss_ctc 33.879333 loss_rnnt 30.891510 hw_loss 0.080826 lr 0.00046441 rank 0
2023-02-21 10:18:15,677 DEBUG TRAIN Batch 13/7500 loss 10.062071 loss_att 14.067763 loss_ctc 9.107873 loss_rnnt 9.311859 hw_loss 0.143060 lr 0.00046431 rank 2
2023-02-21 10:18:15,678 DEBUG TRAIN Batch 13/7500 loss 6.523678 loss_att 8.483385 loss_ctc 6.358415 loss_rnnt 6.073878 hw_loss 0.149802 lr 0.00046434 rank 7
2023-02-21 10:18:15,680 DEBUG TRAIN Batch 13/7500 loss 22.099531 loss_att 23.890400 loss_ctc 32.112534 loss_rnnt 20.337696 hw_loss 0.128613 lr 0.00046440 rank 1
2023-02-21 10:18:15,681 DEBUG TRAIN Batch 13/7500 loss 22.899319 loss_att 23.001232 loss_ctc 26.650661 loss_rnnt 22.295242 hw_loss 0.156589 lr 0.00046434 rank 4
2023-02-21 10:18:15,683 DEBUG TRAIN Batch 13/7500 loss 8.345716 loss_att 12.041529 loss_ctc 12.080070 loss_rnnt 7.055391 hw_loss 0.099841 lr 0.00046435 rank 6
2023-02-21 10:18:15,683 DEBUG TRAIN Batch 13/7500 loss 12.937373 loss_att 15.694708 loss_ctc 17.461222 loss_rnnt 11.724338 hw_loss 0.109479 lr 0.00046436 rank 3
2023-02-21 10:18:15,693 DEBUG TRAIN Batch 13/7500 loss 8.060987 loss_att 10.467586 loss_ctc 11.525021 loss_rnnt 7.043885 hw_loss 0.138584 lr 0.00046439 rank 5
2023-02-21 10:19:34,822 DEBUG TRAIN Batch 13/7600 loss 8.556032 loss_att 11.837138 loss_ctc 11.610647 loss_rnnt 7.470010 hw_loss 0.042224 lr 0.00046414 rank 7
2023-02-21 10:19:34,823 DEBUG TRAIN Batch 13/7600 loss 14.811299 loss_att 19.012499 loss_ctc 19.206066 loss_rnnt 13.308681 hw_loss 0.143268 lr 0.00046420 rank 1
2023-02-21 10:19:34,827 DEBUG TRAIN Batch 13/7600 loss 8.040847 loss_att 16.228231 loss_ctc 11.256317 loss_rnnt 5.959455 hw_loss 0.028472 lr 0.00046421 rank 0
2023-02-21 10:19:34,829 DEBUG TRAIN Batch 13/7600 loss 9.544217 loss_att 12.451021 loss_ctc 14.296071 loss_rnnt 8.246894 hw_loss 0.154465 lr 0.00046414 rank 4
2023-02-21 10:19:34,830 DEBUG TRAIN Batch 13/7600 loss 17.448755 loss_att 22.352701 loss_ctc 25.983498 loss_rnnt 15.254712 hw_loss 0.141166 lr 0.00046411 rank 2
2023-02-21 10:19:34,833 DEBUG TRAIN Batch 13/7600 loss 12.504942 loss_att 17.786766 loss_ctc 14.407299 loss_rnnt 11.179089 hw_loss 0.029701 lr 0.00046419 rank 5
2023-02-21 10:19:34,833 DEBUG TRAIN Batch 13/7600 loss 15.171643 loss_att 21.321892 loss_ctc 19.260515 loss_rnnt 13.326079 hw_loss 0.131872 lr 0.00046415 rank 6
2023-02-21 10:19:34,837 DEBUG TRAIN Batch 13/7600 loss 7.465015 loss_att 8.430113 loss_ctc 8.224012 loss_rnnt 7.138786 hw_loss 0.060019 lr 0.00046416 rank 3
2023-02-21 10:20:53,826 DEBUG TRAIN Batch 13/7700 loss 12.174498 loss_att 17.623913 loss_ctc 17.521042 loss_rnnt 10.303988 hw_loss 0.127039 lr 0.00046394 rank 7
2023-02-21 10:20:53,828 DEBUG TRAIN Batch 13/7700 loss 23.471233 loss_att 24.880035 loss_ctc 27.469831 loss_rnnt 22.621618 hw_loss 0.065079 lr 0.00046394 rank 4
2023-02-21 10:20:53,831 DEBUG TRAIN Batch 13/7700 loss 5.347010 loss_att 7.581486 loss_ctc 10.240360 loss_rnnt 4.203004 hw_loss 0.083744 lr 0.00046400 rank 1
2023-02-21 10:20:53,830 DEBUG TRAIN Batch 13/7700 loss 17.143299 loss_att 21.438360 loss_ctc 19.936163 loss_rnnt 15.865863 hw_loss 0.086332 lr 0.00046391 rank 2
2023-02-21 10:20:53,831 DEBUG TRAIN Batch 13/7700 loss 10.560942 loss_att 24.930130 loss_ctc 8.118904 loss_rnnt 7.927877 hw_loss 0.159059 lr 0.00046401 rank 0
2023-02-21 10:20:53,832 DEBUG TRAIN Batch 13/7700 loss 12.669432 loss_att 13.553129 loss_ctc 17.006498 loss_rnnt 11.879455 hw_loss 0.065553 lr 0.00046396 rank 3
2023-02-21 10:20:53,833 DEBUG TRAIN Batch 13/7700 loss 19.714493 loss_att 20.490931 loss_ctc 20.793182 loss_rnnt 19.366150 hw_loss 0.092308 lr 0.00046399 rank 5
2023-02-21 10:20:53,878 DEBUG TRAIN Batch 13/7700 loss 13.086596 loss_att 13.485775 loss_ctc 15.781382 loss_rnnt 12.557101 hw_loss 0.169414 lr 0.00046395 rank 6
2023-02-21 10:22:14,552 DEBUG TRAIN Batch 13/7800 loss 8.334961 loss_att 12.604147 loss_ctc 14.720199 loss_rnnt 6.447314 hw_loss 0.342084 lr 0.00046374 rank 4
2023-02-21 10:22:14,553 DEBUG TRAIN Batch 13/7800 loss 19.249989 loss_att 23.067402 loss_ctc 27.114786 loss_rnnt 17.372253 hw_loss 0.123025 lr 0.00046381 rank 0
2023-02-21 10:22:14,553 DEBUG TRAIN Batch 13/7800 loss 29.069786 loss_att 34.631897 loss_ctc 39.753235 loss_rnnt 26.449364 hw_loss 0.156640 lr 0.00046371 rank 2
2023-02-21 10:22:14,556 DEBUG TRAIN Batch 13/7800 loss 7.317128 loss_att 10.897961 loss_ctc 9.362173 loss_rnnt 6.268275 hw_loss 0.112526 lr 0.00046375 rank 6
2023-02-21 10:22:14,563 DEBUG TRAIN Batch 13/7800 loss 21.454357 loss_att 29.436127 loss_ctc 25.486670 loss_rnnt 19.253447 hw_loss 0.125468 lr 0.00046374 rank 7
2023-02-21 10:22:14,564 DEBUG TRAIN Batch 13/7800 loss 21.593109 loss_att 21.967710 loss_ctc 22.526081 loss_rnnt 21.368902 hw_loss 0.046674 lr 0.00046379 rank 5
2023-02-21 10:22:14,568 DEBUG TRAIN Batch 13/7800 loss 6.820414 loss_att 8.943026 loss_ctc 8.233930 loss_rnnt 6.111181 hw_loss 0.180454 lr 0.00046376 rank 3
2023-02-21 10:22:14,576 DEBUG TRAIN Batch 13/7800 loss 28.880539 loss_att 33.377029 loss_ctc 37.527924 loss_rnnt 26.711065 hw_loss 0.219738 lr 0.00046380 rank 1
2023-02-21 10:23:33,591 DEBUG TRAIN Batch 13/7900 loss 13.930152 loss_att 18.512970 loss_ctc 20.121374 loss_rnnt 12.102687 hw_loss 0.160133 lr 0.00046354 rank 7
2023-02-21 10:23:33,598 DEBUG TRAIN Batch 13/7900 loss 9.641563 loss_att 11.728336 loss_ctc 12.024152 loss_rnnt 8.874255 hw_loss 0.060515 lr 0.00046361 rank 0
2023-02-21 10:23:33,598 DEBUG TRAIN Batch 13/7900 loss 11.168002 loss_att 12.812855 loss_ctc 11.746044 loss_rnnt 10.737120 hw_loss 0.046575 lr 0.00046354 rank 4
2023-02-21 10:23:33,599 DEBUG TRAIN Batch 13/7900 loss 3.567461 loss_att 6.294680 loss_ctc 4.234744 loss_rnnt 2.878663 hw_loss 0.101970 lr 0.00046351 rank 2
2023-02-21 10:23:33,600 DEBUG TRAIN Batch 13/7900 loss 16.187685 loss_att 17.566246 loss_ctc 17.559132 loss_rnnt 15.618675 hw_loss 0.207074 lr 0.00046360 rank 1
2023-02-21 10:23:33,601 DEBUG TRAIN Batch 13/7900 loss 10.022964 loss_att 13.561541 loss_ctc 15.454710 loss_rnnt 8.545864 hw_loss 0.084659 lr 0.00046355 rank 6
2023-02-21 10:23:33,607 DEBUG TRAIN Batch 13/7900 loss 12.461271 loss_att 17.026752 loss_ctc 17.809999 loss_rnnt 10.820911 hw_loss 0.026437 lr 0.00046356 rank 3
2023-02-21 10:23:33,644 DEBUG TRAIN Batch 13/7900 loss 15.274984 loss_att 17.164619 loss_ctc 19.216101 loss_rnnt 14.308369 hw_loss 0.118513 lr 0.00046359 rank 5
2023-02-21 10:24:52,827 DEBUG TRAIN Batch 13/8000 loss 10.206313 loss_att 12.507168 loss_ctc 14.016935 loss_rnnt 9.106421 hw_loss 0.246824 lr 0.00046335 rank 6
2023-02-21 10:24:52,827 DEBUG TRAIN Batch 13/8000 loss 9.759457 loss_att 14.832219 loss_ctc 13.460166 loss_rnnt 8.201214 hw_loss 0.094243 lr 0.00046334 rank 7
2023-02-21 10:24:52,829 DEBUG TRAIN Batch 13/8000 loss 7.636204 loss_att 10.979597 loss_ctc 9.820725 loss_rnnt 6.589862 hw_loss 0.161989 lr 0.00046341 rank 0
2023-02-21 10:24:52,830 DEBUG TRAIN Batch 13/8000 loss 12.595976 loss_att 16.168062 loss_ctc 14.047934 loss_rnnt 11.611599 hw_loss 0.143186 lr 0.00046340 rank 1
2023-02-21 10:24:52,831 DEBUG TRAIN Batch 13/8000 loss 6.294027 loss_att 10.798561 loss_ctc 8.401333 loss_rnnt 5.053354 hw_loss 0.110234 lr 0.00046331 rank 2
2023-02-21 10:24:52,833 DEBUG TRAIN Batch 13/8000 loss 20.205959 loss_att 23.765697 loss_ctc 22.589849 loss_rnnt 19.160505 hw_loss 0.029348 lr 0.00046334 rank 4
2023-02-21 10:24:52,836 DEBUG TRAIN Batch 13/8000 loss 9.354803 loss_att 14.351365 loss_ctc 11.287201 loss_rnnt 7.975291 hw_loss 0.229774 lr 0.00046336 rank 3
2023-02-21 10:24:52,885 DEBUG TRAIN Batch 13/8000 loss 12.178195 loss_att 14.829010 loss_ctc 15.139309 loss_rnnt 11.192846 hw_loss 0.113195 lr 0.00046339 rank 5
2023-02-21 10:26:11,679 DEBUG TRAIN Batch 13/8100 loss 5.728275 loss_att 8.292511 loss_ctc 6.546721 loss_rnnt 4.956440 hw_loss 0.280990 lr 0.00046314 rank 7
2023-02-21 10:26:11,679 DEBUG TRAIN Batch 13/8100 loss 18.577408 loss_att 19.175007 loss_ctc 24.364342 loss_rnnt 17.619299 hw_loss 0.125624 lr 0.00046314 rank 4
2023-02-21 10:26:11,681 DEBUG TRAIN Batch 13/8100 loss 7.196949 loss_att 13.350709 loss_ctc 7.566954 loss_rnnt 5.867272 hw_loss 0.092983 lr 0.00046311 rank 2
2023-02-21 10:26:11,680 DEBUG TRAIN Batch 13/8100 loss 17.920450 loss_att 20.761898 loss_ctc 25.169325 loss_rnnt 16.330193 hw_loss 0.103970 lr 0.00046320 rank 1
2023-02-21 10:26:11,682 DEBUG TRAIN Batch 13/8100 loss 11.727785 loss_att 14.013952 loss_ctc 13.656403 loss_rnnt 10.938956 hw_loss 0.139586 lr 0.00046319 rank 5
2023-02-21 10:26:11,685 DEBUG TRAIN Batch 13/8100 loss 10.596280 loss_att 11.745519 loss_ctc 12.091273 loss_rnnt 10.092411 hw_loss 0.140042 lr 0.00046321 rank 0
2023-02-21 10:26:11,687 DEBUG TRAIN Batch 13/8100 loss 13.566390 loss_att 15.639543 loss_ctc 16.018185 loss_rnnt 12.741392 hw_loss 0.156490 lr 0.00046315 rank 6
2023-02-21 10:26:11,687 DEBUG TRAIN Batch 13/8100 loss 19.172323 loss_att 21.775288 loss_ctc 24.487114 loss_rnnt 17.909809 hw_loss 0.062406 lr 0.00046316 rank 3
2023-02-21 10:27:30,302 DEBUG TRAIN Batch 13/8200 loss 11.701813 loss_att 13.519073 loss_ctc 15.202594 loss_rnnt 10.737633 hw_loss 0.251170 lr 0.00046294 rank 7
2023-02-21 10:27:30,303 DEBUG TRAIN Batch 13/8200 loss 17.127625 loss_att 21.155222 loss_ctc 18.732222 loss_rnnt 16.063400 hw_loss 0.083921 lr 0.00046294 rank 4
2023-02-21 10:27:30,304 DEBUG TRAIN Batch 13/8200 loss 13.001341 loss_att 16.853546 loss_ctc 14.080833 loss_rnnt 12.060534 hw_loss 0.049563 lr 0.00046291 rank 2
2023-02-21 10:27:30,306 DEBUG TRAIN Batch 13/8200 loss 24.617449 loss_att 23.051384 loss_ctc 32.113007 loss_rnnt 23.851200 hw_loss 0.150099 lr 0.00046295 rank 6
2023-02-21 10:27:30,310 DEBUG TRAIN Batch 13/8200 loss 22.419319 loss_att 28.036060 loss_ctc 34.794006 loss_rnnt 19.529415 hw_loss 0.218621 lr 0.00046301 rank 0
2023-02-21 10:27:30,312 DEBUG TRAIN Batch 13/8200 loss 11.165885 loss_att 13.316805 loss_ctc 16.611956 loss_rnnt 9.895796 hw_loss 0.213306 lr 0.00046300 rank 1
2023-02-21 10:27:30,315 DEBUG TRAIN Batch 13/8200 loss 27.049385 loss_att 27.144562 loss_ctc 36.560440 loss_rnnt 25.744617 hw_loss 0.032983 lr 0.00046296 rank 3
2023-02-21 10:27:30,361 DEBUG TRAIN Batch 13/8200 loss 15.781278 loss_att 16.831789 loss_ctc 15.162201 loss_rnnt 15.625355 hw_loss 0.053185 lr 0.00046299 rank 5
2023-02-21 10:28:47,635 DEBUG TRAIN Batch 13/8300 loss 18.373009 loss_att 22.842560 loss_ctc 23.539948 loss_rnnt 16.749470 hw_loss 0.076316 lr 0.00046281 rank 1
2023-02-21 10:28:47,640 DEBUG TRAIN Batch 13/8300 loss 17.109884 loss_att 24.421284 loss_ctc 19.233269 loss_rnnt 15.349455 hw_loss 0.028188 lr 0.00046280 rank 5
2023-02-21 10:28:47,640 DEBUG TRAIN Batch 13/8300 loss 15.629360 loss_att 19.620850 loss_ctc 17.842773 loss_rnnt 14.499146 hw_loss 0.068990 lr 0.00046275 rank 6
2023-02-21 10:28:47,640 DEBUG TRAIN Batch 13/8300 loss 13.886749 loss_att 15.684029 loss_ctc 16.838951 loss_rnnt 13.073995 hw_loss 0.111886 lr 0.00046274 rank 4
2023-02-21 10:28:47,642 DEBUG TRAIN Batch 13/8300 loss 32.044716 loss_att 39.687248 loss_ctc 36.991543 loss_rnnt 29.803946 hw_loss 0.098794 lr 0.00046272 rank 2
2023-02-21 10:28:47,644 DEBUG TRAIN Batch 13/8300 loss 18.442226 loss_att 19.516432 loss_ctc 21.024654 loss_rnnt 17.820206 hw_loss 0.117854 lr 0.00046274 rank 7
2023-02-21 10:28:47,644 DEBUG TRAIN Batch 13/8300 loss 10.636385 loss_att 16.013420 loss_ctc 13.713095 loss_rnnt 9.104359 hw_loss 0.086984 lr 0.00046282 rank 0
2023-02-21 10:28:47,644 DEBUG TRAIN Batch 13/8300 loss 10.212210 loss_att 9.784046 loss_ctc 11.603763 loss_rnnt 9.881044 hw_loss 0.433608 lr 0.00046277 rank 3
2023-02-21 10:29:47,845 DEBUG CV Batch 13/0 loss 2.484928 loss_att 2.594648 loss_ctc 3.156066 loss_rnnt 2.146322 hw_loss 0.425958 history loss 2.392894 rank 5
2023-02-21 10:29:47,847 DEBUG CV Batch 13/0 loss 2.484928 loss_att 2.594648 loss_ctc 3.156066 loss_rnnt 2.146322 hw_loss 0.425958 history loss 2.392894 rank 1
2023-02-21 10:29:47,848 DEBUG CV Batch 13/0 loss 2.484928 loss_att 2.594648 loss_ctc 3.156066 loss_rnnt 2.146322 hw_loss 0.425958 history loss 2.392894 rank 2
2023-02-21 10:29:47,849 DEBUG CV Batch 13/0 loss 2.484928 loss_att 2.594648 loss_ctc 3.156066 loss_rnnt 2.146322 hw_loss 0.425958 history loss 2.392894 rank 6
2023-02-21 10:29:47,850 DEBUG CV Batch 13/0 loss 2.484928 loss_att 2.594648 loss_ctc 3.156066 loss_rnnt 2.146322 hw_loss 0.425959 history loss 2.392894 rank 0
2023-02-21 10:29:47,871 DEBUG CV Batch 13/0 loss 2.484928 loss_att 2.594648 loss_ctc 3.156066 loss_rnnt 2.146322 hw_loss 0.425958 history loss 2.392894 rank 3
2023-02-21 10:29:47,874 DEBUG CV Batch 13/0 loss 2.484928 loss_att 2.594648 loss_ctc 3.156066 loss_rnnt 2.146322 hw_loss 0.425958 history loss 2.392894 rank 4
2023-02-21 10:29:47,876 DEBUG CV Batch 13/0 loss 2.484928 loss_att 2.594648 loss_ctc 3.156066 loss_rnnt 2.146322 hw_loss 0.425958 history loss 2.392894 rank 7
2023-02-21 10:29:58,861 DEBUG CV Batch 13/100 loss 7.837949 loss_att 8.803307 loss_ctc 10.882270 loss_rnnt 7.154501 hw_loss 0.158376 history loss 4.420305 rank 1
2023-02-21 10:29:58,903 DEBUG CV Batch 13/100 loss 7.837949 loss_att 8.803307 loss_ctc 10.882270 loss_rnnt 7.154501 hw_loss 0.158376 history loss 4.420305 rank 2
2023-02-21 10:29:58,963 DEBUG CV Batch 13/100 loss 7.837949 loss_att 8.803307 loss_ctc 10.882270 loss_rnnt 7.154501 hw_loss 0.158376 history loss 4.420305 rank 4
2023-02-21 10:29:58,998 DEBUG CV Batch 13/100 loss 7.837949 loss_att 8.803307 loss_ctc 10.882270 loss_rnnt 7.154501 hw_loss 0.158376 history loss 4.420305 rank 7
2023-02-21 10:29:59,063 DEBUG CV Batch 13/100 loss 7.837949 loss_att 8.803307 loss_ctc 10.882270 loss_rnnt 7.154501 hw_loss 0.158376 history loss 4.420305 rank 6
2023-02-21 10:29:59,070 DEBUG CV Batch 13/100 loss 7.837949 loss_att 8.803307 loss_ctc 10.882270 loss_rnnt 7.154501 hw_loss 0.158376 history loss 4.420305 rank 5
2023-02-21 10:29:59,257 DEBUG CV Batch 13/100 loss 7.837949 loss_att 8.803307 loss_ctc 10.882270 loss_rnnt 7.154501 hw_loss 0.158376 history loss 4.420305 rank 0
2023-02-21 10:29:59,715 DEBUG CV Batch 13/100 loss 7.837949 loss_att 8.803307 loss_ctc 10.882270 loss_rnnt 7.154501 hw_loss 0.158376 history loss 4.420305 rank 3
2023-02-21 10:30:12,059 DEBUG CV Batch 13/200 loss 5.947923 loss_att 18.196142 loss_ctc 7.233022 loss_rnnt 3.279296 hw_loss 0.089319 history loss 5.059812 rank 1
2023-02-21 10:30:12,081 DEBUG CV Batch 13/200 loss 5.947923 loss_att 18.196142 loss_ctc 7.233022 loss_rnnt 3.279296 hw_loss 0.089319 history loss 5.059812 rank 2
2023-02-21 10:30:12,206 DEBUG CV Batch 13/200 loss 5.947923 loss_att 18.196142 loss_ctc 7.233022 loss_rnnt 3.279296 hw_loss 0.089319 history loss 5.059812 rank 7
2023-02-21 10:30:12,209 DEBUG CV Batch 13/200 loss 5.947923 loss_att 18.196142 loss_ctc 7.233022 loss_rnnt 3.279296 hw_loss 0.089319 history loss 5.059812 rank 4
2023-02-21 10:30:12,406 DEBUG CV Batch 13/200 loss 5.947923 loss_att 18.196142 loss_ctc 7.233022 loss_rnnt 3.279296 hw_loss 0.089319 history loss 5.059812 rank 5
2023-02-21 10:30:12,443 DEBUG CV Batch 13/200 loss 5.947923 loss_att 18.196142 loss_ctc 7.233022 loss_rnnt 3.279296 hw_loss 0.089319 history loss 5.059812 rank 6
2023-02-21 10:30:12,771 DEBUG CV Batch 13/200 loss 5.947923 loss_att 18.196142 loss_ctc 7.233022 loss_rnnt 3.279296 hw_loss 0.089319 history loss 5.059812 rank 0
2023-02-21 10:30:13,459 DEBUG CV Batch 13/200 loss 5.947923 loss_att 18.196142 loss_ctc 7.233022 loss_rnnt 3.279296 hw_loss 0.089319 history loss 5.059812 rank 3
2023-02-21 10:30:24,203 DEBUG CV Batch 13/300 loss 6.720671 loss_att 7.335290 loss_ctc 9.152028 loss_rnnt 6.169508 hw_loss 0.195110 history loss 5.293578 rank 1
2023-02-21 10:30:24,232 DEBUG CV Batch 13/300 loss 6.720671 loss_att 7.335290 loss_ctc 9.152028 loss_rnnt 6.169508 hw_loss 0.195110 history loss 5.293578 rank 4
2023-02-21 10:30:24,244 DEBUG CV Batch 13/300 loss 6.720671 loss_att 7.335290 loss_ctc 9.152028 loss_rnnt 6.169508 hw_loss 0.195110 history loss 5.293578 rank 7
2023-02-21 10:30:24,354 DEBUG CV Batch 13/300 loss 6.720671 loss_att 7.335290 loss_ctc 9.152028 loss_rnnt 6.169508 hw_loss 0.195110 history loss 5.293578 rank 2
2023-02-21 10:30:24,488 DEBUG CV Batch 13/300 loss 6.720671 loss_att 7.335290 loss_ctc 9.152028 loss_rnnt 6.169508 hw_loss 0.195110 history loss 5.293578 rank 5
2023-02-21 10:30:24,893 DEBUG CV Batch 13/300 loss 6.720671 loss_att 7.335290 loss_ctc 9.152028 loss_rnnt 6.169508 hw_loss 0.195110 history loss 5.293578 rank 6
2023-02-21 10:30:25,076 DEBUG CV Batch 13/300 loss 6.720671 loss_att 7.335290 loss_ctc 9.152028 loss_rnnt 6.169508 hw_loss 0.195110 history loss 5.293578 rank 0
2023-02-21 10:30:25,687 DEBUG CV Batch 13/300 loss 6.720671 loss_att 7.335290 loss_ctc 9.152028 loss_rnnt 6.169508 hw_loss 0.195110 history loss 5.293578 rank 3
2023-02-21 10:30:36,178 DEBUG CV Batch 13/400 loss 23.843088 loss_att 108.795303 loss_ctc 11.657254 loss_rnnt 8.462614 hw_loss 0.027767 history loss 6.471384 rank 1
2023-02-21 10:30:36,202 DEBUG CV Batch 13/400 loss 23.843088 loss_att 108.795303 loss_ctc 11.657254 loss_rnnt 8.462614 hw_loss 0.027767 history loss 6.471384 rank 7
2023-02-21 10:30:36,216 DEBUG CV Batch 13/400 loss 23.843088 loss_att 108.795303 loss_ctc 11.657254 loss_rnnt 8.462614 hw_loss 0.027767 history loss 6.471384 rank 4
2023-02-21 10:30:36,327 DEBUG CV Batch 13/400 loss 23.843088 loss_att 108.795303 loss_ctc 11.657254 loss_rnnt 8.462614 hw_loss 0.027767 history loss 6.471384 rank 2
2023-02-21 10:30:36,659 DEBUG CV Batch 13/400 loss 23.843088 loss_att 108.795303 loss_ctc 11.657254 loss_rnnt 8.462614 hw_loss 0.027767 history loss 6.471384 rank 5
2023-02-21 10:30:37,611 DEBUG CV Batch 13/400 loss 23.843088 loss_att 108.795303 loss_ctc 11.657254 loss_rnnt 8.462614 hw_loss 0.027767 history loss 6.471384 rank 6
2023-02-21 10:30:37,739 DEBUG CV Batch 13/400 loss 23.843088 loss_att 108.795303 loss_ctc 11.657254 loss_rnnt 8.462614 hw_loss 0.027767 history loss 6.471384 rank 3
2023-02-21 10:30:37,738 DEBUG CV Batch 13/400 loss 23.843088 loss_att 108.795303 loss_ctc 11.657254 loss_rnnt 8.462614 hw_loss 0.027767 history loss 6.471384 rank 0
2023-02-21 10:30:46,683 DEBUG CV Batch 13/500 loss 4.226191 loss_att 5.783689 loss_ctc 6.016493 loss_rnnt 3.593477 hw_loss 0.154700 history loss 7.379181 rank 1
2023-02-21 10:30:46,693 DEBUG CV Batch 13/500 loss 4.226191 loss_att 5.783689 loss_ctc 6.016493 loss_rnnt 3.593477 hw_loss 0.154700 history loss 7.379181 rank 4
2023-02-21 10:30:46,825 DEBUG CV Batch 13/500 loss 4.226191 loss_att 5.783689 loss_ctc 6.016493 loss_rnnt 3.593477 hw_loss 0.154700 history loss 7.379181 rank 7
2023-02-21 10:30:46,901 DEBUG CV Batch 13/500 loss 4.226191 loss_att 5.783689 loss_ctc 6.016493 loss_rnnt 3.593477 hw_loss 0.154700 history loss 7.379181 rank 2
2023-02-21 10:30:47,295 DEBUG CV Batch 13/500 loss 4.226191 loss_att 5.783689 loss_ctc 6.016493 loss_rnnt 3.593477 hw_loss 0.154700 history loss 7.379181 rank 5
2023-02-21 10:30:48,310 DEBUG CV Batch 13/500 loss 4.226191 loss_att 5.783689 loss_ctc 6.016493 loss_rnnt 3.593477 hw_loss 0.154700 history loss 7.379181 rank 6
2023-02-21 10:30:48,467 DEBUG CV Batch 13/500 loss 4.226191 loss_att 5.783689 loss_ctc 6.016493 loss_rnnt 3.593477 hw_loss 0.154700 history loss 7.379181 rank 3
2023-02-21 10:30:48,573 DEBUG CV Batch 13/500 loss 4.226191 loss_att 5.783689 loss_ctc 6.016493 loss_rnnt 3.593477 hw_loss 0.154700 history loss 7.379181 rank 0
2023-02-21 10:30:58,716 DEBUG CV Batch 13/600 loss 8.235453 loss_att 8.994591 loss_ctc 11.465553 loss_rnnt 7.497099 hw_loss 0.292211 history loss 8.406782 rank 1
2023-02-21 10:30:58,751 DEBUG CV Batch 13/600 loss 8.235453 loss_att 8.994591 loss_ctc 11.465553 loss_rnnt 7.497099 hw_loss 0.292211 history loss 8.406782 rank 4
2023-02-21 10:30:58,903 DEBUG CV Batch 13/600 loss 8.235453 loss_att 8.994591 loss_ctc 11.465553 loss_rnnt 7.497099 hw_loss 0.292211 history loss 8.406782 rank 2
2023-02-21 10:30:58,975 DEBUG CV Batch 13/600 loss 8.235453 loss_att 8.994591 loss_ctc 11.465553 loss_rnnt 7.497099 hw_loss 0.292211 history loss 8.406782 rank 7
2023-02-21 10:30:59,378 DEBUG CV Batch 13/600 loss 8.235453 loss_att 8.994591 loss_ctc 11.465553 loss_rnnt 7.497099 hw_loss 0.292211 history loss 8.406782 rank 5
2023-02-21 10:31:00,402 DEBUG CV Batch 13/600 loss 8.235453 loss_att 8.994591 loss_ctc 11.465553 loss_rnnt 7.497099 hw_loss 0.292211 history loss 8.406782 rank 6
2023-02-21 10:31:00,560 DEBUG CV Batch 13/600 loss 8.235453 loss_att 8.994591 loss_ctc 11.465553 loss_rnnt 7.497099 hw_loss 0.292211 history loss 8.406782 rank 3
2023-02-21 10:31:00,875 DEBUG CV Batch 13/600 loss 8.235453 loss_att 8.994591 loss_ctc 11.465553 loss_rnnt 7.497099 hw_loss 0.292211 history loss 8.406782 rank 0
2023-02-21 10:31:09,967 DEBUG CV Batch 13/700 loss 22.435818 loss_att 90.540573 loss_ctc 19.829262 loss_rnnt 9.147598 hw_loss 0.027767 history loss 9.293001 rank 1
2023-02-21 10:31:09,969 DEBUG CV Batch 13/700 loss 22.435818 loss_att 90.540573 loss_ctc 19.829262 loss_rnnt 9.147598 hw_loss 0.027767 history loss 9.293001 rank 4
2023-02-21 10:31:10,140 DEBUG CV Batch 13/700 loss 22.435818 loss_att 90.540573 loss_ctc 19.829262 loss_rnnt 9.147598 hw_loss 0.027767 history loss 9.293001 rank 2
2023-02-21 10:31:10,303 DEBUG CV Batch 13/700 loss 22.435818 loss_att 90.540573 loss_ctc 19.829262 loss_rnnt 9.147598 hw_loss 0.027767 history loss 9.293001 rank 7
2023-02-21 10:31:10,807 DEBUG CV Batch 13/700 loss 22.435818 loss_att 90.540573 loss_ctc 19.829262 loss_rnnt 9.147598 hw_loss 0.027767 history loss 9.293001 rank 5
2023-02-21 10:31:11,834 DEBUG CV Batch 13/700 loss 22.435818 loss_att 90.540573 loss_ctc 19.829262 loss_rnnt 9.147598 hw_loss 0.027767 history loss 9.293001 rank 6
2023-02-21 10:31:12,049 DEBUG CV Batch 13/700 loss 22.435818 loss_att 90.540573 loss_ctc 19.829262 loss_rnnt 9.147598 hw_loss 0.027767 history loss 9.293001 rank 3
2023-02-21 10:31:13,807 DEBUG CV Batch 13/700 loss 22.435818 loss_att 90.540573 loss_ctc 19.829262 loss_rnnt 9.147598 hw_loss 0.027767 history loss 9.293001 rank 0
2023-02-21 10:31:21,194 DEBUG CV Batch 13/800 loss 13.810025 loss_att 12.560400 loss_ctc 18.864485 loss_rnnt 13.262856 hw_loss 0.230937 history loss 8.648074 rank 1
2023-02-21 10:31:21,361 DEBUG CV Batch 13/800 loss 13.810025 loss_att 12.560400 loss_ctc 18.864485 loss_rnnt 13.262856 hw_loss 0.230937 history loss 8.648074 rank 4
2023-02-21 10:31:21,454 DEBUG CV Batch 13/800 loss 13.810025 loss_att 12.560400 loss_ctc 18.864485 loss_rnnt 13.262856 hw_loss 0.230937 history loss 8.648074 rank 2
2023-02-21 10:31:21,502 DEBUG CV Batch 13/800 loss 13.810025 loss_att 12.560400 loss_ctc 18.864485 loss_rnnt 13.262856 hw_loss 0.230937 history loss 8.648074 rank 7
2023-02-21 10:31:22,132 DEBUG CV Batch 13/800 loss 13.810025 loss_att 12.560400 loss_ctc 18.864485 loss_rnnt 13.262856 hw_loss 0.230937 history loss 8.648074 rank 5
2023-02-21 10:31:23,073 DEBUG CV Batch 13/800 loss 13.810025 loss_att 12.560400 loss_ctc 18.864485 loss_rnnt 13.262856 hw_loss 0.230937 history loss 8.648074 rank 6
2023-02-21 10:31:23,283 DEBUG CV Batch 13/800 loss 13.810025 loss_att 12.560400 loss_ctc 18.864485 loss_rnnt 13.262856 hw_loss 0.230937 history loss 8.648074 rank 3
2023-02-21 10:31:25,456 DEBUG CV Batch 13/800 loss 13.810025 loss_att 12.560400 loss_ctc 18.864485 loss_rnnt 13.262856 hw_loss 0.230937 history loss 8.648074 rank 0
2023-02-21 10:31:34,328 DEBUG CV Batch 13/900 loss 16.876953 loss_att 27.323303 loss_ctc 23.657270 loss_rnnt 13.847195 hw_loss 0.068335 history loss 8.392746 rank 1
2023-02-21 10:31:34,471 DEBUG CV Batch 13/900 loss 16.876953 loss_att 27.323303 loss_ctc 23.657270 loss_rnnt 13.847195 hw_loss 0.068335 history loss 8.392746 rank 4
2023-02-21 10:31:34,534 DEBUG CV Batch 13/900 loss 16.876953 loss_att 27.323303 loss_ctc 23.657270 loss_rnnt 13.847195 hw_loss 0.068335 history loss 8.392746 rank 7
2023-02-21 10:31:34,609 DEBUG CV Batch 13/900 loss 16.876953 loss_att 27.323303 loss_ctc 23.657270 loss_rnnt 13.847195 hw_loss 0.068335 history loss 8.392746 rank 2
2023-02-21 10:31:35,446 DEBUG CV Batch 13/900 loss 16.876953 loss_att 27.323303 loss_ctc 23.657270 loss_rnnt 13.847195 hw_loss 0.068335 history loss 8.392746 rank 5
2023-02-21 10:31:36,451 DEBUG CV Batch 13/900 loss 16.876953 loss_att 27.323303 loss_ctc 23.657270 loss_rnnt 13.847195 hw_loss 0.068335 history loss 8.392746 rank 6
2023-02-21 10:31:36,619 DEBUG CV Batch 13/900 loss 16.876953 loss_att 27.323303 loss_ctc 23.657270 loss_rnnt 13.847195 hw_loss 0.068335 history loss 8.392746 rank 3
2023-02-21 10:31:38,745 DEBUG CV Batch 13/900 loss 16.876953 loss_att 27.323303 loss_ctc 23.657270 loss_rnnt 13.847195 hw_loss 0.068335 history loss 8.392746 rank 0
2023-02-21 10:31:46,628 DEBUG CV Batch 13/1000 loss 4.506433 loss_att 5.551370 loss_ctc 4.848213 loss_rnnt 4.144937 hw_loss 0.200508 history loss 8.122693 rank 1
2023-02-21 10:31:46,670 DEBUG CV Batch 13/1000 loss 4.506433 loss_att 5.551370 loss_ctc 4.848213 loss_rnnt 4.144937 hw_loss 0.200508 history loss 8.122693 rank 2
2023-02-21 10:31:46,686 DEBUG CV Batch 13/1000 loss 4.506433 loss_att 5.551370 loss_ctc 4.848213 loss_rnnt 4.144937 hw_loss 0.200508 history loss 8.122693 rank 4
2023-02-21 10:31:46,744 DEBUG CV Batch 13/1000 loss 4.506433 loss_att 5.551370 loss_ctc 4.848213 loss_rnnt 4.144937 hw_loss 0.200508 history loss 8.122693 rank 7
2023-02-21 10:31:47,639 DEBUG CV Batch 13/1000 loss 4.506433 loss_att 5.551370 loss_ctc 4.848213 loss_rnnt 4.144937 hw_loss 0.200508 history loss 8.122693 rank 5
2023-02-21 10:31:48,623 DEBUG CV Batch 13/1000 loss 4.506433 loss_att 5.551370 loss_ctc 4.848213 loss_rnnt 4.144937 hw_loss 0.200508 history loss 8.122693 rank 6
2023-02-21 10:31:48,830 DEBUG CV Batch 13/1000 loss 4.506433 loss_att 5.551370 loss_ctc 4.848213 loss_rnnt 4.144937 hw_loss 0.200508 history loss 8.122693 rank 3
2023-02-21 10:31:51,236 DEBUG CV Batch 13/1000 loss 4.506433 loss_att 5.551370 loss_ctc 4.848213 loss_rnnt 4.144937 hw_loss 0.200508 history loss 8.122693 rank 0
2023-02-21 10:31:58,517 DEBUG CV Batch 13/1100 loss 6.909351 loss_att 6.366639 loss_ctc 9.357158 loss_rnnt 6.379211 hw_loss 0.585578 history loss 8.116439 rank 2
2023-02-21 10:31:58,540 DEBUG CV Batch 13/1100 loss 6.909351 loss_att 6.366639 loss_ctc 9.357158 loss_rnnt 6.379211 hw_loss 0.585578 history loss 8.116439 rank 1
2023-02-21 10:31:58,570 DEBUG CV Batch 13/1100 loss 6.909351 loss_att 6.366639 loss_ctc 9.357158 loss_rnnt 6.379211 hw_loss 0.585578 history loss 8.116439 rank 4
2023-02-21 10:31:58,586 DEBUG CV Batch 13/1100 loss 6.909351 loss_att 6.366639 loss_ctc 9.357158 loss_rnnt 6.379211 hw_loss 0.585578 history loss 8.116439 rank 7
2023-02-21 10:31:59,501 DEBUG CV Batch 13/1100 loss 6.909351 loss_att 6.366639 loss_ctc 9.357158 loss_rnnt 6.379211 hw_loss 0.585578 history loss 8.116439 rank 5
2023-02-21 10:32:00,564 DEBUG CV Batch 13/1100 loss 6.909351 loss_att 6.366639 loss_ctc 9.357158 loss_rnnt 6.379211 hw_loss 0.585578 history loss 8.116439 rank 6
2023-02-21 10:32:00,805 DEBUG CV Batch 13/1100 loss 6.909351 loss_att 6.366639 loss_ctc 9.357158 loss_rnnt 6.379211 hw_loss 0.585578 history loss 8.116439 rank 3
2023-02-21 10:32:03,298 DEBUG CV Batch 13/1100 loss 6.909351 loss_att 6.366639 loss_ctc 9.357158 loss_rnnt 6.379211 hw_loss 0.585578 history loss 8.116439 rank 0
2023-02-21 10:32:09,019 DEBUG CV Batch 13/1200 loss 7.401664 loss_att 9.004617 loss_ctc 8.966770 loss_rnnt 6.789885 hw_loss 0.154700 history loss 8.497058 rank 2
2023-02-21 10:32:09,118 DEBUG CV Batch 13/1200 loss 7.401664 loss_att 9.004617 loss_ctc 8.966770 loss_rnnt 6.789885 hw_loss 0.154700 history loss 8.497058 rank 1
2023-02-21 10:32:09,126 DEBUG CV Batch 13/1200 loss 7.401664 loss_att 9.004617 loss_ctc 8.966770 loss_rnnt 6.789885 hw_loss 0.154700 history loss 8.497058 rank 4
2023-02-21 10:32:09,136 DEBUG CV Batch 13/1200 loss 7.401664 loss_att 9.004617 loss_ctc 8.966770 loss_rnnt 6.789885 hw_loss 0.154700 history loss 8.497058 rank 7
2023-02-21 10:32:10,041 DEBUG CV Batch 13/1200 loss 7.401664 loss_att 9.004617 loss_ctc 8.966770 loss_rnnt 6.789885 hw_loss 0.154700 history loss 8.497058 rank 5
2023-02-21 10:32:11,204 DEBUG CV Batch 13/1200 loss 7.401664 loss_att 9.004617 loss_ctc 8.966770 loss_rnnt 6.789885 hw_loss 0.154700 history loss 8.497058 rank 6
2023-02-21 10:32:11,448 DEBUG CV Batch 13/1200 loss 7.401664 loss_att 9.004617 loss_ctc 8.966770 loss_rnnt 6.789885 hw_loss 0.154700 history loss 8.497058 rank 3
2023-02-21 10:32:14,065 DEBUG CV Batch 13/1200 loss 7.401664 loss_att 9.004617 loss_ctc 8.966770 loss_rnnt 6.789885 hw_loss 0.154700 history loss 8.497058 rank 0
2023-02-21 10:32:20,951 DEBUG CV Batch 13/1300 loss 7.943815 loss_att 7.382671 loss_ctc 8.981840 loss_rnnt 7.773014 hw_loss 0.271176 history loss 8.833408 rank 2
2023-02-21 10:32:21,045 DEBUG CV Batch 13/1300 loss 7.943815 loss_att 7.382671 loss_ctc 8.981840 loss_rnnt 7.773014 hw_loss 0.271175 history loss 8.833408 rank 4
2023-02-21 10:32:21,101 DEBUG CV Batch 13/1300 loss 7.943815 loss_att 7.382671 loss_ctc 8.981840 loss_rnnt 7.773014 hw_loss 0.271176 history loss 8.833408 rank 7
2023-02-21 10:32:21,153 DEBUG CV Batch 13/1300 loss 7.943815 loss_att 7.382671 loss_ctc 8.981840 loss_rnnt 7.773014 hw_loss 0.271176 history loss 8.833408 rank 1
2023-02-21 10:32:22,111 DEBUG CV Batch 13/1300 loss 7.943815 loss_att 7.382671 loss_ctc 8.981840 loss_rnnt 7.773014 hw_loss 0.271176 history loss 8.833408 rank 5
2023-02-21 10:32:23,248 DEBUG CV Batch 13/1300 loss 7.943815 loss_att 7.382671 loss_ctc 8.981840 loss_rnnt 7.773014 hw_loss 0.271176 history loss 8.833408 rank 6
2023-02-21 10:32:23,504 DEBUG CV Batch 13/1300 loss 7.943815 loss_att 7.382671 loss_ctc 8.981840 loss_rnnt 7.773014 hw_loss 0.271176 history loss 8.833408 rank 3
2023-02-21 10:32:26,158 DEBUG CV Batch 13/1300 loss 7.943815 loss_att 7.382671 loss_ctc 8.981840 loss_rnnt 7.773014 hw_loss 0.271176 history loss 8.833408 rank 0
2023-02-21 10:32:32,042 DEBUG CV Batch 13/1400 loss 12.629918 loss_att 42.098625 loss_ctc 9.576937 loss_rnnt 7.128432 hw_loss 0.027767 history loss 9.254498 rank 2
2023-02-21 10:32:32,215 DEBUG CV Batch 13/1400 loss 12.629918 loss_att 42.098625 loss_ctc 9.576937 loss_rnnt 7.128432 hw_loss 0.027767 history loss 9.254498 rank 7
2023-02-21 10:32:32,291 DEBUG CV Batch 13/1400 loss 12.629918 loss_att 42.098625 loss_ctc 9.576937 loss_rnnt 7.128432 hw_loss 0.027767 history loss 9.254498 rank 4
2023-02-21 10:32:32,330 DEBUG CV Batch 13/1400 loss 12.629918 loss_att 42.098625 loss_ctc 9.576937 loss_rnnt 7.128432 hw_loss 0.027767 history loss 9.254498 rank 1
2023-02-21 10:32:33,352 DEBUG CV Batch 13/1400 loss 12.629918 loss_att 42.098625 loss_ctc 9.576937 loss_rnnt 7.128432 hw_loss 0.027767 history loss 9.254498 rank 5
2023-02-21 10:32:34,526 DEBUG CV Batch 13/1400 loss 12.629918 loss_att 42.098625 loss_ctc 9.576937 loss_rnnt 7.128432 hw_loss 0.027767 history loss 9.254498 rank 6
2023-02-21 10:32:34,796 DEBUG CV Batch 13/1400 loss 12.629918 loss_att 42.098625 loss_ctc 9.576937 loss_rnnt 7.128432 hw_loss 0.027767 history loss 9.254498 rank 3
2023-02-21 10:32:37,438 DEBUG CV Batch 13/1400 loss 12.629918 loss_att 42.098625 loss_ctc 9.576937 loss_rnnt 7.128432 hw_loss 0.027767 history loss 9.254498 rank 0
2023-02-21 10:32:43,441 DEBUG CV Batch 13/1500 loss 6.456907 loss_att 8.751141 loss_ctc 6.167927 loss_rnnt 6.000145 hw_loss 0.068335 history loss 9.030154 rank 2
2023-02-21 10:32:43,639 DEBUG CV Batch 13/1500 loss 6.456907 loss_att 8.751141 loss_ctc 6.167927 loss_rnnt 6.000145 hw_loss 0.068335 history loss 9.030154 rank 4
2023-02-21 10:32:43,655 DEBUG CV Batch 13/1500 loss 6.456907 loss_att 8.751141 loss_ctc 6.167927 loss_rnnt 6.000145 hw_loss 0.068335 history loss 9.030154 rank 7
2023-02-21 10:32:43,692 DEBUG CV Batch 13/1500 loss 6.456907 loss_att 8.751141 loss_ctc 6.167927 loss_rnnt 6.000145 hw_loss 0.068335 history loss 9.030154 rank 1
2023-02-21 10:32:44,874 DEBUG CV Batch 13/1500 loss 6.456907 loss_att 8.751141 loss_ctc 6.167927 loss_rnnt 6.000145 hw_loss 0.068335 history loss 9.030154 rank 5
2023-02-21 10:32:46,034 DEBUG CV Batch 13/1500 loss 6.456907 loss_att 8.751141 loss_ctc 6.167927 loss_rnnt 6.000145 hw_loss 0.068335 history loss 9.030154 rank 6
2023-02-21 10:32:46,245 DEBUG CV Batch 13/1500 loss 6.456907 loss_att 8.751141 loss_ctc 6.167927 loss_rnnt 6.000145 hw_loss 0.068335 history loss 9.030154 rank 3
2023-02-21 10:32:49,143 DEBUG CV Batch 13/1500 loss 6.456907 loss_att 8.751141 loss_ctc 6.167927 loss_rnnt 6.000145 hw_loss 0.068335 history loss 9.030154 rank 0
2023-02-21 10:32:56,371 DEBUG CV Batch 13/1600 loss 9.652231 loss_att 20.692940 loss_ctc 13.867154 loss_rnnt 6.831590 hw_loss 0.094704 history loss 8.922227 rank 2
2023-02-21 10:32:56,479 DEBUG CV Batch 13/1600 loss 9.652231 loss_att 20.692940 loss_ctc 13.867154 loss_rnnt 6.831590 hw_loss 0.094704 history loss 8.922227 rank 4
2023-02-21 10:32:56,610 DEBUG CV Batch 13/1600 loss 9.652231 loss_att 20.692940 loss_ctc 13.867154 loss_rnnt 6.831590 hw_loss 0.094704 history loss 8.922227 rank 7
2023-02-21 10:32:56,680 DEBUG CV Batch 13/1600 loss 9.652231 loss_att 20.692940 loss_ctc 13.867154 loss_rnnt 6.831590 hw_loss 0.094704 history loss 8.922227 rank 1
2023-02-21 10:32:57,856 DEBUG CV Batch 13/1600 loss 9.652231 loss_att 20.692940 loss_ctc 13.867154 loss_rnnt 6.831590 hw_loss 0.094704 history loss 8.922227 rank 5
2023-02-21 10:32:59,005 DEBUG CV Batch 13/1600 loss 9.652231 loss_att 20.692940 loss_ctc 13.867154 loss_rnnt 6.831590 hw_loss 0.094704 history loss 8.922227 rank 6
2023-02-21 10:32:59,287 DEBUG CV Batch 13/1600 loss 9.652231 loss_att 20.692940 loss_ctc 13.867154 loss_rnnt 6.831590 hw_loss 0.094704 history loss 8.922227 rank 3
2023-02-21 10:33:02,865 DEBUG CV Batch 13/1600 loss 9.652231 loss_att 20.692940 loss_ctc 13.867154 loss_rnnt 6.831590 hw_loss 0.094704 history loss 8.922227 rank 0
2023-02-21 10:33:08,714 DEBUG CV Batch 13/1700 loss 11.801508 loss_att 11.158253 loss_ctc 15.993319 loss_rnnt 11.240843 hw_loss 0.244516 history loss 8.798292 rank 4
2023-02-21 10:33:08,752 DEBUG CV Batch 13/1700 loss 11.801508 loss_att 11.158253 loss_ctc 15.993319 loss_rnnt 11.240843 hw_loss 0.244516 history loss 8.798292 rank 2
2023-02-21 10:33:08,953 DEBUG CV Batch 13/1700 loss 11.801508 loss_att 11.158253 loss_ctc 15.993319 loss_rnnt 11.240843 hw_loss 0.244516 history loss 8.798292 rank 1
2023-02-21 10:33:08,961 DEBUG CV Batch 13/1700 loss 11.801508 loss_att 11.158253 loss_ctc 15.993319 loss_rnnt 11.240843 hw_loss 0.244516 history loss 8.798292 rank 7
2023-02-21 10:33:10,137 DEBUG CV Batch 13/1700 loss 11.801508 loss_att 11.158253 loss_ctc 15.993319 loss_rnnt 11.240843 hw_loss 0.244516 history loss 8.798292 rank 5
2023-02-21 10:33:11,242 DEBUG CV Batch 13/1700 loss 11.801508 loss_att 11.158253 loss_ctc 15.993319 loss_rnnt 11.240843 hw_loss 0.244516 history loss 8.798292 rank 6
2023-02-21 10:33:11,562 DEBUG CV Batch 13/1700 loss 11.801508 loss_att 11.158253 loss_ctc 15.993319 loss_rnnt 11.240843 hw_loss 0.244516 history loss 8.798292 rank 3
2023-02-21 10:33:14,982 DEBUG CV Batch 13/1700 loss 11.801508 loss_att 11.158253 loss_ctc 15.993319 loss_rnnt 11.240843 hw_loss 0.244516 history loss 8.798292 rank 0
2023-02-21 10:33:17,697 INFO Epoch 13 CV info cv_loss 8.744880173502327
2023-02-21 10:33:17,698 INFO Epoch 14 TRAIN info lr 0.0004626853715430218
2023-02-21 10:33:17,701 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 10:33:17,746 INFO Epoch 13 CV info cv_loss 8.74488017287346
2023-02-21 10:33:17,747 INFO Epoch 14 TRAIN info lr 0.0004626893336197016
2023-02-21 10:33:17,750 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 10:33:17,968 INFO Epoch 13 CV info cv_loss 8.744880173493714
2023-02-21 10:33:17,969 INFO Epoch 14 TRAIN info lr 0.00046276265040254093
2023-02-21 10:33:17,973 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 10:33:18,022 INFO Epoch 13 CV info cv_loss 8.744880173493714
2023-02-21 10:33:18,023 INFO Epoch 14 TRAIN info lr 0.0004626497174325613
2023-02-21 10:33:18,026 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 10:33:19,086 INFO Epoch 13 CV info cv_loss 8.744880171874163
2023-02-21 10:33:19,087 INFO Epoch 14 TRAIN info lr 0.0004626893336197016
2023-02-21 10:33:19,090 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 10:33:20,240 INFO Epoch 13 CV info cv_loss 8.744880173605704
2023-02-21 10:33:20,241 INFO Epoch 14 TRAIN info lr 0.00046271310821744197
2023-02-21 10:33:20,244 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 10:33:20,527 INFO Epoch 13 CV info cv_loss 8.744880173631548
2023-02-21 10:33:20,527 INFO Epoch 14 TRAIN info lr 0.00046269131469621103
2023-02-21 10:33:20,531 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 10:33:24,141 INFO Epoch 13 CV info cv_loss 8.744880173752152
2023-02-21 10:33:24,142 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/13.pt
2023-02-21 10:33:24,893 INFO Epoch 14 TRAIN info lr 0.00046267942861884156
2023-02-21 10:33:24,897 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 10:34:38,707 DEBUG TRAIN Batch 14/0 loss 12.330433 loss_att 12.419395 loss_ctc 15.738499 loss_rnnt 11.656756 hw_loss 0.377766 lr 0.00046265 rank 7
2023-02-21 10:34:38,708 DEBUG TRAIN Batch 14/0 loss 10.609549 loss_att 9.931961 loss_ctc 12.746072 loss_rnnt 10.136405 hw_loss 0.607108 lr 0.00046268 rank 4
2023-02-21 10:34:38,710 DEBUG TRAIN Batch 14/0 loss 9.752630 loss_att 8.760826 loss_ctc 12.697781 loss_rnnt 9.217095 hw_loss 0.639766 lr 0.00046276 rank 1
2023-02-21 10:34:38,712 DEBUG TRAIN Batch 14/0 loss 9.456494 loss_att 8.798191 loss_ctc 11.678213 loss_rnnt 9.120317 hw_loss 0.321766 lr 0.00046271 rank 6
2023-02-21 10:34:38,713 DEBUG TRAIN Batch 14/0 loss 11.487516 loss_att 10.592525 loss_ctc 13.188760 loss_rnnt 11.216743 hw_loss 0.418009 lr 0.00046269 rank 2
2023-02-21 10:34:38,714 DEBUG TRAIN Batch 14/0 loss 11.418621 loss_att 11.508744 loss_ctc 16.072111 loss_rnnt 10.481589 hw_loss 0.559766 lr 0.00046269 rank 5
2023-02-21 10:34:38,733 DEBUG TRAIN Batch 14/0 loss 11.490289 loss_att 9.735800 loss_ctc 13.571698 loss_rnnt 11.338056 hw_loss 0.423016 lr 0.00046269 rank 3
2023-02-21 10:34:38,747 DEBUG TRAIN Batch 14/0 loss 11.033996 loss_att 9.989509 loss_ctc 12.852521 loss_rnnt 10.745851 hw_loss 0.477321 lr 0.00046268 rank 0
2023-02-21 10:35:57,016 DEBUG TRAIN Batch 14/100 loss 11.044276 loss_att 11.862766 loss_ctc 15.629053 loss_rnnt 10.253332 hw_loss 0.029891 lr 0.00046249 rank 2
2023-02-21 10:35:57,016 DEBUG TRAIN Batch 14/100 loss 12.560159 loss_att 18.977272 loss_ctc 16.684399 loss_rnnt 10.665701 hw_loss 0.114631 lr 0.00046256 rank 1
2023-02-21 10:35:57,017 DEBUG TRAIN Batch 14/100 loss 12.507140 loss_att 20.185785 loss_ctc 15.021654 loss_rnnt 10.571493 hw_loss 0.121216 lr 0.00046249 rank 4
2023-02-21 10:35:57,018 DEBUG TRAIN Batch 14/100 loss 9.206732 loss_att 16.943270 loss_ctc 15.811209 loss_rnnt 6.712726 hw_loss 0.123939 lr 0.00046245 rank 7
2023-02-21 10:35:57,019 DEBUG TRAIN Batch 14/100 loss 8.673882 loss_att 9.977095 loss_ctc 7.654165 loss_rnnt 8.476773 hw_loss 0.135800 lr 0.00046251 rank 6
2023-02-21 10:35:57,021 DEBUG TRAIN Batch 14/100 loss 18.905787 loss_att 22.773224 loss_ctc 30.349163 loss_rnnt 16.499302 hw_loss 0.201025 lr 0.00046248 rank 0
2023-02-21 10:35:57,022 DEBUG TRAIN Batch 14/100 loss 10.475677 loss_att 14.037981 loss_ctc 16.875378 loss_rnnt 8.834239 hw_loss 0.141905 lr 0.00046249 rank 3
2023-02-21 10:35:57,063 DEBUG TRAIN Batch 14/100 loss 17.477013 loss_att 18.952145 loss_ctc 18.786629 loss_rnnt 16.959843 hw_loss 0.089114 lr 0.00046249 rank 5
2023-02-21 10:37:14,529 DEBUG TRAIN Batch 14/200 loss 24.043226 loss_att 25.989252 loss_ctc 31.491726 loss_rnnt 22.616852 hw_loss 0.082569 lr 0.00046229 rank 4
2023-02-21 10:37:14,529 DEBUG TRAIN Batch 14/200 loss 10.380613 loss_att 13.433143 loss_ctc 11.855655 loss_rnnt 9.457479 hw_loss 0.217416 lr 0.00046229 rank 2
2023-02-21 10:37:14,530 DEBUG TRAIN Batch 14/200 loss 11.458415 loss_att 12.738585 loss_ctc 11.808630 loss_rnnt 11.096983 hw_loss 0.110068 lr 0.00046225 rank 7
2023-02-21 10:37:14,530 DEBUG TRAIN Batch 14/200 loss 6.185886 loss_att 9.782468 loss_ctc 9.699348 loss_rnnt 4.936262 hw_loss 0.115961 lr 0.00046236 rank 1
2023-02-21 10:37:14,535 DEBUG TRAIN Batch 14/200 loss 16.432083 loss_att 24.872211 loss_ctc 21.200542 loss_rnnt 14.051079 hw_loss 0.107222 lr 0.00046232 rank 6
2023-02-21 10:37:14,535 DEBUG TRAIN Batch 14/200 loss 18.411547 loss_att 22.154488 loss_ctc 25.012709 loss_rnnt 16.714218 hw_loss 0.128601 lr 0.00046228 rank 0
2023-02-21 10:37:14,538 DEBUG TRAIN Batch 14/200 loss 21.033838 loss_att 21.870422 loss_ctc 25.995422 loss_rnnt 20.070707 hw_loss 0.251754 lr 0.00046229 rank 3
2023-02-21 10:37:14,539 DEBUG TRAIN Batch 14/200 loss 24.030905 loss_att 26.975084 loss_ctc 30.905540 loss_rnnt 22.510506 hw_loss 0.028022 lr 0.00046229 rank 5
2023-02-21 10:38:33,053 DEBUG TRAIN Batch 14/300 loss 7.889903 loss_att 12.379242 loss_ctc 10.387004 loss_rnnt 6.627795 hw_loss 0.058675 lr 0.00046209 rank 4
2023-02-21 10:38:33,054 DEBUG TRAIN Batch 14/300 loss 31.236944 loss_att 34.287861 loss_ctc 41.725471 loss_rnnt 29.140915 hw_loss 0.163831 lr 0.00046209 rank 2
2023-02-21 10:38:33,057 DEBUG TRAIN Batch 14/300 loss 7.897129 loss_att 10.379559 loss_ctc 7.333869 loss_rnnt 7.455318 hw_loss 0.038298 lr 0.00046212 rank 6
2023-02-21 10:38:33,058 DEBUG TRAIN Batch 14/300 loss 11.297587 loss_att 15.833271 loss_ctc 14.034256 loss_rnnt 9.964993 hw_loss 0.113566 lr 0.00046208 rank 0
2023-02-21 10:38:33,063 DEBUG TRAIN Batch 14/300 loss 11.285424 loss_att 15.605801 loss_ctc 14.771742 loss_rnnt 9.858979 hw_loss 0.182864 lr 0.00046210 rank 3
2023-02-21 10:38:33,064 DEBUG TRAIN Batch 14/300 loss 17.780354 loss_att 21.405071 loss_ctc 21.561121 loss_rnnt 16.450199 hw_loss 0.189579 lr 0.00046209 rank 5
2023-02-21 10:38:33,074 DEBUG TRAIN Batch 14/300 loss 10.178500 loss_att 14.019922 loss_ctc 11.238367 loss_rnnt 9.211802 hw_loss 0.107060 lr 0.00046205 rank 7
2023-02-21 10:38:33,088 DEBUG TRAIN Batch 14/300 loss 10.620629 loss_att 18.140282 loss_ctc 14.456862 loss_rnnt 8.563042 hw_loss 0.079047 lr 0.00046217 rank 1
2023-02-21 10:39:52,887 DEBUG TRAIN Batch 14/400 loss 14.618662 loss_att 20.505520 loss_ctc 19.237930 loss_rnnt 12.759554 hw_loss 0.123438 lr 0.00046190 rank 2
2023-02-21 10:39:52,889 DEBUG TRAIN Batch 14/400 loss 6.401451 loss_att 12.449980 loss_ctc 9.803303 loss_rnnt 4.643145 hw_loss 0.178162 lr 0.00046186 rank 7
2023-02-21 10:39:52,890 DEBUG TRAIN Batch 14/400 loss 4.459384 loss_att 8.491463 loss_ctc 6.007267 loss_rnnt 3.383108 hw_loss 0.119019 lr 0.00046189 rank 0
2023-02-21 10:39:52,894 DEBUG TRAIN Batch 14/400 loss 14.333864 loss_att 18.666386 loss_ctc 18.063824 loss_rnnt 12.925589 hw_loss 0.083333 lr 0.00046197 rank 1
2023-02-21 10:39:52,894 DEBUG TRAIN Batch 14/400 loss 17.210575 loss_att 22.758818 loss_ctc 24.210464 loss_rnnt 15.128378 hw_loss 0.073558 lr 0.00046189 rank 4
2023-02-21 10:39:52,897 DEBUG TRAIN Batch 14/400 loss 8.197711 loss_att 10.049475 loss_ctc 11.084176 loss_rnnt 7.380931 hw_loss 0.115434 lr 0.00046190 rank 5
2023-02-21 10:39:52,901 DEBUG TRAIN Batch 14/400 loss 15.126810 loss_att 18.665665 loss_ctc 15.782363 loss_rnnt 14.252445 hw_loss 0.148475 lr 0.00046190 rank 3
2023-02-21 10:39:52,940 DEBUG TRAIN Batch 14/400 loss 5.168322 loss_att 8.924114 loss_ctc 7.972298 loss_rnnt 3.976720 hw_loss 0.124837 lr 0.00046192 rank 6
2023-02-21 10:41:10,567 DEBUG TRAIN Batch 14/500 loss 12.170523 loss_att 15.257386 loss_ctc 15.864054 loss_rnnt 11.001352 hw_loss 0.111238 lr 0.00046170 rank 4
2023-02-21 10:41:10,568 DEBUG TRAIN Batch 14/500 loss 15.240206 loss_att 18.181122 loss_ctc 22.224932 loss_rnnt 13.658725 hw_loss 0.116253 lr 0.00046172 rank 6
2023-02-21 10:41:10,569 DEBUG TRAIN Batch 14/500 loss 8.805235 loss_att 10.313901 loss_ctc 10.266575 loss_rnnt 8.250312 hw_loss 0.109396 lr 0.00046166 rank 7
2023-02-21 10:41:10,571 DEBUG TRAIN Batch 14/500 loss 10.601134 loss_att 11.708582 loss_ctc 11.564562 loss_rnnt 10.076866 hw_loss 0.326853 lr 0.00046170 rank 5
2023-02-21 10:41:10,575 DEBUG TRAIN Batch 14/500 loss 18.057436 loss_att 20.406515 loss_ctc 26.319502 loss_rnnt 16.418631 hw_loss 0.126340 lr 0.00046170 rank 2
2023-02-21 10:41:10,574 DEBUG TRAIN Batch 14/500 loss 22.578457 loss_att 25.615505 loss_ctc 30.962368 loss_rnnt 20.797749 hw_loss 0.103961 lr 0.00046177 rank 1
2023-02-21 10:41:10,582 DEBUG TRAIN Batch 14/500 loss 7.503219 loss_att 10.103609 loss_ctc 9.856861 loss_rnnt 6.567211 hw_loss 0.191458 lr 0.00046169 rank 0
2023-02-21 10:41:10,582 DEBUG TRAIN Batch 14/500 loss 8.007529 loss_att 10.259253 loss_ctc 10.640436 loss_rnnt 7.186305 hw_loss 0.037173 lr 0.00046170 rank 3
2023-02-21 10:42:28,777 DEBUG TRAIN Batch 14/600 loss 12.334699 loss_att 12.586687 loss_ctc 15.588283 loss_rnnt 11.806317 hw_loss 0.082822 lr 0.00046150 rank 2
2023-02-21 10:42:28,785 DEBUG TRAIN Batch 14/600 loss 11.749639 loss_att 10.991705 loss_ctc 14.690099 loss_rnnt 11.404389 hw_loss 0.196451 lr 0.00046146 rank 7
2023-02-21 10:42:28,785 DEBUG TRAIN Batch 14/600 loss 12.323718 loss_att 12.246025 loss_ctc 15.967162 loss_rnnt 11.751513 hw_loss 0.191158 lr 0.00046150 rank 4
2023-02-21 10:42:28,785 DEBUG TRAIN Batch 14/600 loss 28.187248 loss_att 26.081409 loss_ctc 34.321091 loss_rnnt 27.718117 hw_loss 0.135846 lr 0.00046153 rank 6
2023-02-21 10:42:28,787 DEBUG TRAIN Batch 14/600 loss 11.464457 loss_att 12.608046 loss_ctc 13.728283 loss_rnnt 10.709181 hw_loss 0.421341 lr 0.00046158 rank 1
2023-02-21 10:42:28,787 DEBUG TRAIN Batch 14/600 loss 7.492904 loss_att 10.629678 loss_ctc 9.344026 loss_rnnt 6.603348 hw_loss 0.028846 lr 0.00046151 rank 3
2023-02-21 10:42:28,792 DEBUG TRAIN Batch 14/600 loss 8.414153 loss_att 8.118617 loss_ctc 10.489122 loss_rnnt 8.039062 hw_loss 0.295380 lr 0.00046150 rank 5
2023-02-21 10:42:28,795 DEBUG TRAIN Batch 14/600 loss 9.486527 loss_att 9.118147 loss_ctc 10.881287 loss_rnnt 9.296099 hw_loss 0.146507 lr 0.00046149 rank 0
2023-02-21 10:43:49,283 DEBUG TRAIN Batch 14/700 loss 4.631091 loss_att 10.064241 loss_ctc 6.827045 loss_rnnt 3.164689 hw_loss 0.163083 lr 0.00046127 rank 7
2023-02-21 10:43:49,284 DEBUG TRAIN Batch 14/700 loss 7.539695 loss_att 10.486525 loss_ctc 11.707235 loss_rnnt 6.343379 hw_loss 0.096146 lr 0.00046138 rank 1
2023-02-21 10:43:49,286 DEBUG TRAIN Batch 14/700 loss 9.807170 loss_att 13.340370 loss_ctc 15.489321 loss_rnnt 8.329214 hw_loss 0.025680 lr 0.00046133 rank 6
2023-02-21 10:43:49,286 DEBUG TRAIN Batch 14/700 loss 12.582493 loss_att 12.106968 loss_ctc 15.734448 loss_rnnt 12.139284 hw_loss 0.221348 lr 0.00046131 rank 3
2023-02-21 10:43:49,288 DEBUG TRAIN Batch 14/700 loss 7.939120 loss_att 12.711850 loss_ctc 6.979143 loss_rnnt 7.073557 hw_loss 0.073150 lr 0.00046131 rank 2
2023-02-21 10:43:49,289 DEBUG TRAIN Batch 14/700 loss 4.213633 loss_att 7.565938 loss_ctc 5.249161 loss_rnnt 3.310357 hw_loss 0.177646 lr 0.00046130 rank 0
2023-02-21 10:43:49,289 DEBUG TRAIN Batch 14/700 loss 7.889874 loss_att 12.380490 loss_ctc 7.844118 loss_rnnt 6.984590 hw_loss 0.024866 lr 0.00046130 rank 4
2023-02-21 10:43:49,293 DEBUG TRAIN Batch 14/700 loss 13.634105 loss_att 16.562506 loss_ctc 16.134388 loss_rnnt 12.667267 hw_loss 0.089601 lr 0.00046131 rank 5
2023-02-21 10:45:09,116 DEBUG TRAIN Batch 14/800 loss 5.653726 loss_att 10.773069 loss_ctc 9.317508 loss_rnnt 4.079597 hw_loss 0.115795 lr 0.00046118 rank 1
2023-02-21 10:45:09,117 DEBUG TRAIN Batch 14/800 loss 5.839031 loss_att 10.674984 loss_ctc 9.068800 loss_rnnt 4.375214 hw_loss 0.123732 lr 0.00046107 rank 7
2023-02-21 10:45:09,122 DEBUG TRAIN Batch 14/800 loss 17.598124 loss_att 21.484943 loss_ctc 24.391342 loss_rnnt 15.901230 hw_loss 0.025813 lr 0.00046111 rank 2
2023-02-21 10:45:09,121 DEBUG TRAIN Batch 14/800 loss 22.225143 loss_att 22.846531 loss_ctc 28.457317 loss_rnnt 21.224874 hw_loss 0.084445 lr 0.00046111 rank 4
2023-02-21 10:45:09,127 DEBUG TRAIN Batch 14/800 loss 5.561358 loss_att 11.724840 loss_ctc 7.340948 loss_rnnt 4.032905 hw_loss 0.109647 lr 0.00046110 rank 0
2023-02-21 10:45:09,130 DEBUG TRAIN Batch 14/800 loss 16.428051 loss_att 21.405327 loss_ctc 20.490658 loss_rnnt 14.793657 hw_loss 0.182355 lr 0.00046111 rank 3
2023-02-21 10:45:09,138 DEBUG TRAIN Batch 14/800 loss 7.260653 loss_att 12.866955 loss_ctc 15.150719 loss_rnnt 5.010123 hw_loss 0.144864 lr 0.00046111 rank 5
2023-02-21 10:45:09,165 DEBUG TRAIN Batch 14/800 loss 9.588459 loss_att 9.823366 loss_ctc 9.240873 loss_rnnt 9.473773 hw_loss 0.213842 lr 0.00046113 rank 6
2023-02-21 10:46:28,250 DEBUG TRAIN Batch 14/900 loss 11.636702 loss_att 14.475143 loss_ctc 13.618013 loss_rnnt 10.764501 hw_loss 0.075634 lr 0.00046091 rank 2
2023-02-21 10:46:28,252 DEBUG TRAIN Batch 14/900 loss 11.876472 loss_att 14.368957 loss_ctc 17.828156 loss_rnnt 10.471474 hw_loss 0.211767 lr 0.00046090 rank 0
2023-02-21 10:46:28,254 DEBUG TRAIN Batch 14/900 loss 13.133983 loss_att 16.837528 loss_ctc 17.555054 loss_rnnt 11.660842 hw_loss 0.268041 lr 0.00046088 rank 7
2023-02-21 10:46:28,255 DEBUG TRAIN Batch 14/900 loss 9.335154 loss_att 13.393661 loss_ctc 12.799086 loss_rnnt 7.991241 hw_loss 0.131912 lr 0.00046091 rank 4
2023-02-21 10:46:28,254 DEBUG TRAIN Batch 14/900 loss 7.450266 loss_att 10.502988 loss_ctc 11.489342 loss_rnnt 6.234346 hw_loss 0.125312 lr 0.00046092 rank 3
2023-02-21 10:46:28,255 DEBUG TRAIN Batch 14/900 loss 5.440323 loss_att 13.288624 loss_ctc 7.982224 loss_rnnt 3.472811 hw_loss 0.110497 lr 0.00046094 rank 6
2023-02-21 10:46:28,258 DEBUG TRAIN Batch 14/900 loss 7.509533 loss_att 8.888214 loss_ctc 11.359334 loss_rnnt 6.688555 hw_loss 0.059876 lr 0.00046099 rank 1
2023-02-21 10:46:28,265 DEBUG TRAIN Batch 14/900 loss 4.438938 loss_att 8.430310 loss_ctc 4.046804 loss_rnnt 3.555217 hw_loss 0.258245 lr 0.00046091 rank 5
2023-02-21 10:47:46,255 DEBUG TRAIN Batch 14/1000 loss 5.861055 loss_att 8.605187 loss_ctc 7.175817 loss_rnnt 5.102142 hw_loss 0.065222 lr 0.00046068 rank 7
2023-02-21 10:47:46,258 DEBUG TRAIN Batch 14/1000 loss 11.983675 loss_att 15.216792 loss_ctc 13.004831 loss_rnnt 11.085888 hw_loss 0.215644 lr 0.00046072 rank 4
2023-02-21 10:47:46,258 DEBUG TRAIN Batch 14/1000 loss 11.067082 loss_att 12.938787 loss_ctc 12.200236 loss_rnnt 10.390013 hw_loss 0.284329 lr 0.00046074 rank 6
2023-02-21 10:47:46,263 DEBUG TRAIN Batch 14/1000 loss 13.246706 loss_att 17.133507 loss_ctc 14.481854 loss_rnnt 12.266876 hw_loss 0.070844 lr 0.00046071 rank 0
2023-02-21 10:47:46,265 DEBUG TRAIN Batch 14/1000 loss 11.736033 loss_att 12.982034 loss_ctc 11.224141 loss_rnnt 11.484958 hw_loss 0.131494 lr 0.00046079 rank 1
2023-02-21 10:47:46,267 DEBUG TRAIN Batch 14/1000 loss 9.267283 loss_att 13.727068 loss_ctc 11.057131 loss_rnnt 8.045711 hw_loss 0.170569 lr 0.00046072 rank 2
2023-02-21 10:47:46,268 DEBUG TRAIN Batch 14/1000 loss 9.308361 loss_att 13.147717 loss_ctc 13.500497 loss_rnnt 7.948837 hw_loss 0.061314 lr 0.00046072 rank 5
2023-02-21 10:47:46,270 DEBUG TRAIN Batch 14/1000 loss 4.620569 loss_att 7.087012 loss_ctc 3.092609 loss_rnnt 4.275621 hw_loss 0.103852 lr 0.00046072 rank 3
2023-02-21 10:49:07,029 DEBUG TRAIN Batch 14/1100 loss 11.005927 loss_att 14.257282 loss_ctc 17.919205 loss_rnnt 9.338697 hw_loss 0.178479 lr 0.00046060 rank 1
2023-02-21 10:49:07,034 DEBUG TRAIN Batch 14/1100 loss 11.830804 loss_att 15.063318 loss_ctc 14.542884 loss_rnnt 10.802174 hw_loss 0.038468 lr 0.00046052 rank 4
2023-02-21 10:49:07,034 DEBUG TRAIN Batch 14/1100 loss 16.909243 loss_att 18.428879 loss_ctc 17.824518 loss_rnnt 16.422743 hw_loss 0.113505 lr 0.00046052 rank 5
2023-02-21 10:49:07,035 DEBUG TRAIN Batch 14/1100 loss 25.280037 loss_att 27.314602 loss_ctc 30.930019 loss_rnnt 24.073696 hw_loss 0.086431 lr 0.00046052 rank 2
2023-02-21 10:49:07,036 DEBUG TRAIN Batch 14/1100 loss 11.617428 loss_att 17.935230 loss_ctc 17.953243 loss_rnnt 9.445213 hw_loss 0.119772 lr 0.00046053 rank 3
2023-02-21 10:49:07,036 DEBUG TRAIN Batch 14/1100 loss 23.486763 loss_att 27.745611 loss_ctc 38.163216 loss_rnnt 20.591171 hw_loss 0.163048 lr 0.00046048 rank 7
2023-02-21 10:49:07,037 DEBUG TRAIN Batch 14/1100 loss 4.625280 loss_att 6.986365 loss_ctc 5.148528 loss_rnnt 3.982972 hw_loss 0.188110 lr 0.00046051 rank 0
2023-02-21 10:49:07,082 DEBUG TRAIN Batch 14/1100 loss 11.575320 loss_att 12.339105 loss_ctc 12.980669 loss_rnnt 11.184533 hw_loss 0.094970 lr 0.00046055 rank 6
2023-02-21 10:50:26,148 DEBUG TRAIN Batch 14/1200 loss 11.947044 loss_att 12.629822 loss_ctc 14.660289 loss_rnnt 11.393440 hw_loss 0.103655 lr 0.00046029 rank 7
2023-02-21 10:50:26,154 DEBUG TRAIN Batch 14/1200 loss 9.015854 loss_att 10.845241 loss_ctc 13.553203 loss_rnnt 8.015061 hw_loss 0.056128 lr 0.00046032 rank 4
2023-02-21 10:50:26,156 DEBUG TRAIN Batch 14/1200 loss 13.249812 loss_att 14.690721 loss_ctc 16.057837 loss_rnnt 12.451078 hw_loss 0.255280 lr 0.00046040 rank 1
2023-02-21 10:50:26,155 DEBUG TRAIN Batch 14/1200 loss 26.287703 loss_att 28.135387 loss_ctc 31.871979 loss_rnnt 25.091234 hw_loss 0.154426 lr 0.00046033 rank 2
2023-02-21 10:50:26,158 DEBUG TRAIN Batch 14/1200 loss 17.200998 loss_att 16.830755 loss_ctc 19.548590 loss_rnnt 16.908667 hw_loss 0.100062 lr 0.00046032 rank 0
2023-02-21 10:50:26,159 DEBUG TRAIN Batch 14/1200 loss 22.398262 loss_att 22.995136 loss_ctc 25.298918 loss_rnnt 21.851480 hw_loss 0.076224 lr 0.00046033 rank 3
2023-02-21 10:50:26,160 DEBUG TRAIN Batch 14/1200 loss 12.166791 loss_att 11.945211 loss_ctc 12.520067 loss_rnnt 12.042391 hw_loss 0.228024 lr 0.00046033 rank 5
2023-02-21 10:50:26,205 DEBUG TRAIN Batch 14/1200 loss 12.771770 loss_att 15.381651 loss_ctc 17.683014 loss_rnnt 11.431751 hw_loss 0.306017 lr 0.00046035 rank 6
2023-02-21 10:51:45,088 DEBUG TRAIN Batch 14/1300 loss 24.080975 loss_att 27.605026 loss_ctc 27.487173 loss_rnnt 22.906967 hw_loss 0.028189 lr 0.00046013 rank 2
2023-02-21 10:51:45,096 DEBUG TRAIN Batch 14/1300 loss 12.260842 loss_att 16.962690 loss_ctc 19.768929 loss_rnnt 10.279440 hw_loss 0.074913 lr 0.00046016 rank 6
2023-02-21 10:51:45,098 DEBUG TRAIN Batch 14/1300 loss 10.978006 loss_att 15.935141 loss_ctc 16.189873 loss_rnnt 9.248267 hw_loss 0.081370 lr 0.00046021 rank 1
2023-02-21 10:51:45,097 DEBUG TRAIN Batch 14/1300 loss 12.963840 loss_att 18.335205 loss_ctc 21.101341 loss_rnnt 10.747446 hw_loss 0.107102 lr 0.00046013 rank 4
2023-02-21 10:51:45,098 DEBUG TRAIN Batch 14/1300 loss 4.976375 loss_att 9.283193 loss_ctc 6.323545 loss_rnnt 3.868554 hw_loss 0.125315 lr 0.00046013 rank 5
2023-02-21 10:51:45,099 DEBUG TRAIN Batch 14/1300 loss 14.843358 loss_att 17.506136 loss_ctc 18.282299 loss_rnnt 13.808228 hw_loss 0.082594 lr 0.00046009 rank 7
2023-02-21 10:51:45,099 DEBUG TRAIN Batch 14/1300 loss 11.557966 loss_att 17.020811 loss_ctc 16.982924 loss_rnnt 9.668091 hw_loss 0.138709 lr 0.00046012 rank 0
2023-02-21 10:51:45,101 DEBUG TRAIN Batch 14/1300 loss 11.744554 loss_att 12.143467 loss_ctc 16.159037 loss_rnnt 11.008476 hw_loss 0.126933 lr 0.00046014 rank 3
2023-02-21 10:53:05,719 DEBUG TRAIN Batch 14/1400 loss 16.934214 loss_att 20.326221 loss_ctc 19.591961 loss_rnnt 15.835800 hw_loss 0.123085 lr 0.00045994 rank 2
2023-02-21 10:53:05,719 DEBUG TRAIN Batch 14/1400 loss 9.518625 loss_att 14.724522 loss_ctc 13.543158 loss_rnnt 7.864786 hw_loss 0.142603 lr 0.00046001 rank 1
2023-02-21 10:53:05,723 DEBUG TRAIN Batch 14/1400 loss 18.631123 loss_att 20.836733 loss_ctc 23.892681 loss_rnnt 17.456554 hw_loss 0.059824 lr 0.00045990 rank 7
2023-02-21 10:53:05,727 DEBUG TRAIN Batch 14/1400 loss 4.076554 loss_att 8.468172 loss_ctc 3.491801 loss_rnnt 3.241111 hw_loss 0.065787 lr 0.00045996 rank 6
2023-02-21 10:53:05,728 DEBUG TRAIN Batch 14/1400 loss 19.216757 loss_att 19.819170 loss_ctc 32.045574 loss_rnnt 17.342899 hw_loss 0.080376 lr 0.00045993 rank 4
2023-02-21 10:53:05,730 DEBUG TRAIN Batch 14/1400 loss 9.038597 loss_att 13.214396 loss_ctc 14.410288 loss_rnnt 7.442750 hw_loss 0.083365 lr 0.00045993 rank 0
2023-02-21 10:53:05,730 DEBUG TRAIN Batch 14/1400 loss 5.621193 loss_att 8.919596 loss_ctc 9.996990 loss_rnnt 4.263174 hw_loss 0.215436 lr 0.00045994 rank 3
2023-02-21 10:53:05,732 DEBUG TRAIN Batch 14/1400 loss 17.970383 loss_att 20.859009 loss_ctc 23.514652 loss_rnnt 16.640291 hw_loss 0.024620 lr 0.00045994 rank 5
2023-02-21 10:54:25,680 DEBUG TRAIN Batch 14/1500 loss 9.393517 loss_att 11.834391 loss_ctc 13.721242 loss_rnnt 8.215338 hw_loss 0.211826 lr 0.00045974 rank 2
2023-02-21 10:54:25,684 DEBUG TRAIN Batch 14/1500 loss 16.660254 loss_att 24.773956 loss_ctc 18.337624 loss_rnnt 14.739421 hw_loss 0.139580 lr 0.00045973 rank 0
2023-02-21 10:54:25,685 DEBUG TRAIN Batch 14/1500 loss 12.969592 loss_att 15.458907 loss_ctc 15.799665 loss_rnnt 12.064381 hw_loss 0.056259 lr 0.00045982 rank 1
2023-02-21 10:54:25,685 DEBUG TRAIN Batch 14/1500 loss 14.480894 loss_att 18.496887 loss_ctc 15.877748 loss_rnnt 13.463799 hw_loss 0.051841 lr 0.00045971 rank 7
2023-02-21 10:54:25,688 DEBUG TRAIN Batch 14/1500 loss 9.397974 loss_att 14.040676 loss_ctc 10.414656 loss_rnnt 8.237490 hw_loss 0.180723 lr 0.00045974 rank 4
2023-02-21 10:54:25,693 DEBUG TRAIN Batch 14/1500 loss 7.171481 loss_att 11.638227 loss_ctc 8.743284 loss_rnnt 6.032393 hw_loss 0.067808 lr 0.00045975 rank 3
2023-02-21 10:54:25,694 DEBUG TRAIN Batch 14/1500 loss 8.844039 loss_att 12.446061 loss_ctc 10.196085 loss_rnnt 7.850105 hw_loss 0.174856 lr 0.00045974 rank 5
2023-02-21 10:54:25,738 DEBUG TRAIN Batch 14/1500 loss 6.423924 loss_att 11.148413 loss_ctc 13.567275 loss_rnnt 4.413834 hw_loss 0.211397 lr 0.00045977 rank 6
2023-02-21 10:55:43,397 DEBUG TRAIN Batch 14/1600 loss 23.700096 loss_att 32.046913 loss_ctc 27.518259 loss_rnnt 21.472641 hw_loss 0.091881 lr 0.00045951 rank 7
2023-02-21 10:55:43,400 DEBUG TRAIN Batch 14/1600 loss 3.445026 loss_att 5.743924 loss_ctc 4.093617 loss_rnnt 2.810798 hw_loss 0.164944 lr 0.00045957 rank 6
2023-02-21 10:55:43,400 DEBUG TRAIN Batch 14/1600 loss 13.666179 loss_att 16.715145 loss_ctc 15.690661 loss_rnnt 12.771555 hw_loss 0.027936 lr 0.00045955 rank 3
2023-02-21 10:55:43,402 DEBUG TRAIN Batch 14/1600 loss 11.824116 loss_att 15.896229 loss_ctc 14.681896 loss_rnnt 10.588939 hw_loss 0.074469 lr 0.00045955 rank 4
2023-02-21 10:55:43,402 DEBUG TRAIN Batch 14/1600 loss 9.565473 loss_att 11.941643 loss_ctc 12.806016 loss_rnnt 8.613640 hw_loss 0.083488 lr 0.00045955 rank 2
2023-02-21 10:55:43,404 DEBUG TRAIN Batch 14/1600 loss 14.449058 loss_att 15.283422 loss_ctc 19.178841 loss_rnnt 13.572809 hw_loss 0.147632 lr 0.00045962 rank 1
2023-02-21 10:55:43,407 DEBUG TRAIN Batch 14/1600 loss 22.185440 loss_att 22.847202 loss_ctc 23.523899 loss_rnnt 21.790579 hw_loss 0.157590 lr 0.00045955 rank 5
2023-02-21 10:55:43,453 DEBUG TRAIN Batch 14/1600 loss 18.728558 loss_att 23.092134 loss_ctc 26.045288 loss_rnnt 16.785053 hw_loss 0.178546 lr 0.00045954 rank 0
2023-02-21 10:57:02,052 DEBUG TRAIN Batch 14/1700 loss 12.418834 loss_att 13.943430 loss_ctc 13.905987 loss_rnnt 11.878817 hw_loss 0.069019 lr 0.00045932 rank 7
2023-02-21 10:57:02,053 DEBUG TRAIN Batch 14/1700 loss 9.800321 loss_att 14.393858 loss_ctc 14.628904 loss_rnnt 8.171585 hw_loss 0.124156 lr 0.00045938 rank 6
2023-02-21 10:57:02,054 DEBUG TRAIN Batch 14/1700 loss 20.654655 loss_att 22.552502 loss_ctc 26.351368 loss_rnnt 19.482939 hw_loss 0.061098 lr 0.00045943 rank 1
2023-02-21 10:57:02,054 DEBUG TRAIN Batch 14/1700 loss 13.008463 loss_att 16.326645 loss_ctc 17.299417 loss_rnnt 11.686831 hw_loss 0.161005 lr 0.00045936 rank 2
2023-02-21 10:57:02,056 DEBUG TRAIN Batch 14/1700 loss 12.358072 loss_att 18.227932 loss_ctc 14.252850 loss_rnnt 10.849212 hw_loss 0.154221 lr 0.00045935 rank 4
2023-02-21 10:57:02,058 DEBUG TRAIN Batch 14/1700 loss 7.802607 loss_att 11.600920 loss_ctc 11.732337 loss_rnnt 6.411240 hw_loss 0.202013 lr 0.00045935 rank 0
2023-02-21 10:57:02,062 DEBUG TRAIN Batch 14/1700 loss 12.558990 loss_att 15.298557 loss_ctc 13.426205 loss_rnnt 11.804060 hw_loss 0.171353 lr 0.00045936 rank 5
2023-02-21 10:57:02,072 DEBUG TRAIN Batch 14/1700 loss 6.752909 loss_att 11.793873 loss_ctc 9.446108 loss_rnnt 5.317647 hw_loss 0.127455 lr 0.00045936 rank 3
2023-02-21 10:58:23,969 DEBUG TRAIN Batch 14/1800 loss 17.453400 loss_att 19.284725 loss_ctc 23.115189 loss_rnnt 16.269087 hw_loss 0.118393 lr 0.00045916 rank 2
2023-02-21 10:58:23,973 DEBUG TRAIN Batch 14/1800 loss 13.666661 loss_att 14.781133 loss_ctc 20.718822 loss_rnnt 12.413411 hw_loss 0.168877 lr 0.00045919 rank 6
2023-02-21 10:58:23,974 DEBUG TRAIN Batch 14/1800 loss 12.141052 loss_att 12.936852 loss_ctc 14.379313 loss_rnnt 11.610920 hw_loss 0.136008 lr 0.00045916 rank 4
2023-02-21 10:58:23,975 DEBUG TRAIN Batch 14/1800 loss 9.385468 loss_att 15.052767 loss_ctc 12.755189 loss_rnnt 7.724718 hw_loss 0.146242 lr 0.00045912 rank 7
2023-02-21 10:58:23,976 DEBUG TRAIN Batch 14/1800 loss 11.609586 loss_att 14.190811 loss_ctc 13.655619 loss_rnnt 10.745238 hw_loss 0.141184 lr 0.00045915 rank 0
2023-02-21 10:58:23,978 DEBUG TRAIN Batch 14/1800 loss 11.402429 loss_att 15.670980 loss_ctc 13.804506 loss_rnnt 10.209522 hw_loss 0.035474 lr 0.00045916 rank 3
2023-02-21 10:58:23,979 DEBUG TRAIN Batch 14/1800 loss 11.928499 loss_att 12.298869 loss_ctc 13.269107 loss_rnnt 11.593778 hw_loss 0.153561 lr 0.00045923 rank 1
2023-02-21 10:58:24,026 DEBUG TRAIN Batch 14/1800 loss 19.712126 loss_att 19.763756 loss_ctc 23.155798 loss_rnnt 19.149895 hw_loss 0.173904 lr 0.00045916 rank 5
2023-02-21 10:59:40,644 DEBUG TRAIN Batch 14/1900 loss 10.199616 loss_att 11.300164 loss_ctc 8.619129 loss_rnnt 10.094819 hw_loss 0.178912 lr 0.00045897 rank 2
2023-02-21 10:59:40,646 DEBUG TRAIN Batch 14/1900 loss 12.671552 loss_att 12.820787 loss_ctc 15.548213 loss_rnnt 12.086498 hw_loss 0.321847 lr 0.00045893 rank 7
2023-02-21 10:59:40,649 DEBUG TRAIN Batch 14/1900 loss 10.370152 loss_att 13.637580 loss_ctc 15.945290 loss_rnnt 8.939277 hw_loss 0.063823 lr 0.00045904 rank 1
2023-02-21 10:59:40,649 DEBUG TRAIN Batch 14/1900 loss 12.566276 loss_att 16.514490 loss_ctc 16.770065 loss_rnnt 11.171278 hw_loss 0.084093 lr 0.00045896 rank 0
2023-02-21 10:59:40,650 DEBUG TRAIN Batch 14/1900 loss 13.799610 loss_att 19.490005 loss_ctc 17.771568 loss_rnnt 12.039153 hw_loss 0.173969 lr 0.00045899 rank 6
2023-02-21 10:59:40,652 DEBUG TRAIN Batch 14/1900 loss 15.588558 loss_att 18.460253 loss_ctc 19.230766 loss_rnnt 14.513678 hw_loss 0.027964 lr 0.00045896 rank 4
2023-02-21 10:59:40,652 DEBUG TRAIN Batch 14/1900 loss 15.359928 loss_att 21.032726 loss_ctc 17.724899 loss_rnnt 13.853132 hw_loss 0.106702 lr 0.00045897 rank 3
2023-02-21 10:59:40,654 DEBUG TRAIN Batch 14/1900 loss 7.553740 loss_att 10.818523 loss_ctc 9.091528 loss_rnnt 6.661731 hw_loss 0.063777 lr 0.00045897 rank 5
2023-02-21 11:00:57,914 DEBUG TRAIN Batch 14/2000 loss 13.401002 loss_att 22.382183 loss_ctc 10.996593 loss_rnnt 11.816665 hw_loss 0.203792 lr 0.00045877 rank 4
2023-02-21 11:00:57,915 DEBUG TRAIN Batch 14/2000 loss 23.461300 loss_att 23.466606 loss_ctc 24.871649 loss_rnnt 23.221115 hw_loss 0.095770 lr 0.00045874 rank 7
2023-02-21 11:00:57,915 DEBUG TRAIN Batch 14/2000 loss 15.390331 loss_att 17.207493 loss_ctc 17.956011 loss_rnnt 14.613361 hw_loss 0.133963 lr 0.00045878 rank 2
2023-02-21 11:00:57,915 DEBUG TRAIN Batch 14/2000 loss 8.040977 loss_att 10.051810 loss_ctc 9.638851 loss_rnnt 7.313858 hw_loss 0.209818 lr 0.00045880 rank 6
2023-02-21 11:00:57,917 DEBUG TRAIN Batch 14/2000 loss 15.504505 loss_att 21.035107 loss_ctc 16.986206 loss_rnnt 14.153563 hw_loss 0.088614 lr 0.00045878 rank 5
2023-02-21 11:00:57,919 DEBUG TRAIN Batch 14/2000 loss 9.357402 loss_att 14.034260 loss_ctc 11.498846 loss_rnnt 8.121168 hw_loss 0.028756 lr 0.00045885 rank 1
2023-02-21 11:00:57,920 DEBUG TRAIN Batch 14/2000 loss 14.326300 loss_att 18.003721 loss_ctc 15.526402 loss_rnnt 13.364780 hw_loss 0.123791 lr 0.00045877 rank 0
2023-02-21 11:00:57,962 DEBUG TRAIN Batch 14/2000 loss 11.823476 loss_att 11.934975 loss_ctc 13.356366 loss_rnnt 11.553717 hw_loss 0.080766 lr 0.00045878 rank 3
2023-02-21 11:02:18,352 DEBUG TRAIN Batch 14/2100 loss 17.375629 loss_att 19.797974 loss_ctc 22.278433 loss_rnnt 16.168741 hw_loss 0.128831 lr 0.00045854 rank 7
2023-02-21 11:02:18,352 DEBUG TRAIN Batch 14/2100 loss 6.887709 loss_att 9.243083 loss_ctc 10.610733 loss_rnnt 5.863365 hw_loss 0.106623 lr 0.00045858 rank 2
2023-02-21 11:02:18,354 DEBUG TRAIN Batch 14/2100 loss 14.146492 loss_att 18.037975 loss_ctc 18.166204 loss_rnnt 12.799469 hw_loss 0.061434 lr 0.00045858 rank 4
2023-02-21 11:02:18,356 DEBUG TRAIN Batch 14/2100 loss 12.501206 loss_att 15.714244 loss_ctc 14.808332 loss_rnnt 11.438807 hw_loss 0.210329 lr 0.00045865 rank 1
2023-02-21 11:02:18,357 DEBUG TRAIN Batch 14/2100 loss 5.556993 loss_att 7.522670 loss_ctc 6.036127 loss_rnnt 5.019796 hw_loss 0.150331 lr 0.00045858 rank 5
2023-02-21 11:02:18,360 DEBUG TRAIN Batch 14/2100 loss 7.972536 loss_att 15.409790 loss_ctc 9.212231 loss_rnnt 6.306613 hw_loss 0.024710 lr 0.00045857 rank 0
2023-02-21 11:02:18,360 DEBUG TRAIN Batch 14/2100 loss 16.938173 loss_att 18.944019 loss_ctc 23.632858 loss_rnnt 15.613577 hw_loss 0.057756 lr 0.00045858 rank 3
2023-02-21 11:02:18,362 DEBUG TRAIN Batch 14/2100 loss 14.297042 loss_att 21.904984 loss_ctc 17.887707 loss_rnnt 12.260689 hw_loss 0.067518 lr 0.00045861 rank 6
2023-02-21 11:03:38,846 DEBUG TRAIN Batch 14/2200 loss 8.960006 loss_att 13.367048 loss_ctc 11.809543 loss_rnnt 7.628205 hw_loss 0.132100 lr 0.00045839 rank 2
2023-02-21 11:03:38,849 DEBUG TRAIN Batch 14/2200 loss 8.578240 loss_att 13.537462 loss_ctc 12.061827 loss_rnnt 7.049474 hw_loss 0.135832 lr 0.00045839 rank 4
2023-02-21 11:03:38,852 DEBUG TRAIN Batch 14/2200 loss 17.770296 loss_att 21.122648 loss_ctc 22.100452 loss_rnnt 16.405464 hw_loss 0.219388 lr 0.00045838 rank 0
2023-02-21 11:03:38,853 DEBUG TRAIN Batch 14/2200 loss 9.558371 loss_att 12.757517 loss_ctc 11.372147 loss_rnnt 8.606319 hw_loss 0.131971 lr 0.00045835 rank 7
2023-02-21 11:03:38,854 DEBUG TRAIN Batch 14/2200 loss 9.016808 loss_att 13.169647 loss_ctc 13.150140 loss_rnnt 7.621156 hw_loss 0.026197 lr 0.00045841 rank 6
2023-02-21 11:03:38,855 DEBUG TRAIN Batch 14/2200 loss 13.282353 loss_att 17.617805 loss_ctc 17.021023 loss_rnnt 11.850689 hw_loss 0.123909 lr 0.00045846 rank 1
2023-02-21 11:03:38,859 DEBUG TRAIN Batch 14/2200 loss 11.636426 loss_att 13.844357 loss_ctc 12.434408 loss_rnnt 11.034178 hw_loss 0.101748 lr 0.00045839 rank 5
2023-02-21 11:03:38,897 DEBUG TRAIN Batch 14/2200 loss 14.241714 loss_att 17.748207 loss_ctc 23.499287 loss_rnnt 12.272293 hw_loss 0.063333 lr 0.00045839 rank 3
2023-02-21 11:04:57,812 DEBUG TRAIN Batch 14/2300 loss 9.756008 loss_att 13.573540 loss_ctc 13.206166 loss_rnnt 8.458312 hw_loss 0.139066 lr 0.00045819 rank 4
2023-02-21 11:04:57,812 DEBUG TRAIN Batch 14/2300 loss 11.810343 loss_att 13.861848 loss_ctc 15.359982 loss_rnnt 10.876554 hw_loss 0.094130 lr 0.00045816 rank 7
2023-02-21 11:04:57,815 DEBUG TRAIN Batch 14/2300 loss 7.185646 loss_att 10.632867 loss_ctc 10.261391 loss_rnnt 6.050268 hw_loss 0.067188 lr 0.00045822 rank 6
2023-02-21 11:04:57,817 DEBUG TRAIN Batch 14/2300 loss 12.617930 loss_att 13.008400 loss_ctc 14.838734 loss_rnnt 12.185749 hw_loss 0.108712 lr 0.00045820 rank 2
2023-02-21 11:04:57,820 DEBUG TRAIN Batch 14/2300 loss 14.785065 loss_att 20.730631 loss_ctc 16.837145 loss_rnnt 13.282815 hw_loss 0.074109 lr 0.00045827 rank 1
2023-02-21 11:04:57,822 DEBUG TRAIN Batch 14/2300 loss 7.106293 loss_att 9.671125 loss_ctc 11.280397 loss_rnnt 5.982236 hw_loss 0.102268 lr 0.00045820 rank 5
2023-02-21 11:04:57,827 DEBUG TRAIN Batch 14/2300 loss 8.923027 loss_att 13.567910 loss_ctc 11.045350 loss_rnnt 7.638825 hw_loss 0.135466 lr 0.00045819 rank 0
2023-02-21 11:04:57,875 DEBUG TRAIN Batch 14/2300 loss 14.234750 loss_att 16.110712 loss_ctc 19.277153 loss_rnnt 13.088835 hw_loss 0.184505 lr 0.00045820 rank 3
2023-02-21 11:06:15,835 DEBUG TRAIN Batch 14/2400 loss 8.341810 loss_att 9.544528 loss_ctc 10.380250 loss_rnnt 7.759805 hw_loss 0.130629 lr 0.00045800 rank 2
2023-02-21 11:06:15,838 DEBUG TRAIN Batch 14/2400 loss 15.670130 loss_att 18.852539 loss_ctc 22.312813 loss_rnnt 14.074369 hw_loss 0.137978 lr 0.00045800 rank 0
2023-02-21 11:06:15,840 DEBUG TRAIN Batch 14/2400 loss 11.170235 loss_att 14.575806 loss_ctc 14.747161 loss_rnnt 9.969382 hw_loss 0.080278 lr 0.00045801 rank 3
2023-02-21 11:06:15,841 DEBUG TRAIN Batch 14/2400 loss 8.515423 loss_att 11.230362 loss_ctc 10.640372 loss_rnnt 7.638842 hw_loss 0.094249 lr 0.00045797 rank 7
2023-02-21 11:06:15,841 DEBUG TRAIN Batch 14/2400 loss 17.599527 loss_att 19.044779 loss_ctc 22.119125 loss_rnnt 16.662365 hw_loss 0.085310 lr 0.00045808 rank 1
2023-02-21 11:06:15,841 DEBUG TRAIN Batch 14/2400 loss 9.261332 loss_att 11.647047 loss_ctc 13.865984 loss_rnnt 8.074132 hw_loss 0.180192 lr 0.00045800 rank 4
2023-02-21 11:06:15,849 DEBUG TRAIN Batch 14/2400 loss 13.285670 loss_att 14.454729 loss_ctc 19.844194 loss_rnnt 12.045502 hw_loss 0.247286 lr 0.00045800 rank 5
2023-02-21 11:06:15,851 DEBUG TRAIN Batch 14/2400 loss 15.255793 loss_att 14.592896 loss_ctc 19.298285 loss_rnnt 14.791521 hw_loss 0.108470 lr 0.00045803 rank 6
2023-02-21 11:07:37,948 DEBUG TRAIN Batch 14/2500 loss 10.704815 loss_att 11.327440 loss_ctc 11.596684 loss_rnnt 10.357563 hw_loss 0.194645 lr 0.00045781 rank 2
2023-02-21 11:07:37,949 DEBUG TRAIN Batch 14/2500 loss 13.515260 loss_att 13.961039 loss_ctc 16.901497 loss_rnnt 12.895314 hw_loss 0.148671 lr 0.00045788 rank 1
2023-02-21 11:07:37,953 DEBUG TRAIN Batch 14/2500 loss 11.785824 loss_att 11.578964 loss_ctc 13.507525 loss_rnnt 11.512089 hw_loss 0.160398 lr 0.00045781 rank 4
2023-02-21 11:07:37,954 DEBUG TRAIN Batch 14/2500 loss 9.941422 loss_att 11.979285 loss_ctc 12.241504 loss_rnnt 9.113573 hw_loss 0.212995 lr 0.00045780 rank 0
2023-02-21 11:07:37,955 DEBUG TRAIN Batch 14/2500 loss 10.980759 loss_att 11.466630 loss_ctc 12.936511 loss_rnnt 10.414322 hw_loss 0.390927 lr 0.00045777 rank 7
2023-02-21 11:07:37,957 DEBUG TRAIN Batch 14/2500 loss 6.526049 loss_att 12.788224 loss_ctc 7.308799 loss_rnnt 5.067691 hw_loss 0.190418 lr 0.00045781 rank 5
2023-02-21 11:07:37,961 DEBUG TRAIN Batch 14/2500 loss 11.065454 loss_att 13.211220 loss_ctc 13.825747 loss_rnnt 10.216897 hw_loss 0.096308 lr 0.00045781 rank 3
2023-02-21 11:07:37,997 DEBUG TRAIN Batch 14/2500 loss 9.943146 loss_att 14.289901 loss_ctc 13.751625 loss_rnnt 8.521025 hw_loss 0.084324 lr 0.00045784 rank 6
2023-02-21 11:08:54,958 DEBUG TRAIN Batch 14/2600 loss 13.537633 loss_att 20.842993 loss_ctc 20.872454 loss_rnnt 10.989309 hw_loss 0.204889 lr 0.00045769 rank 1
2023-02-21 11:08:54,962 DEBUG TRAIN Batch 14/2600 loss 10.583353 loss_att 16.114788 loss_ctc 14.466311 loss_rnnt 8.884569 hw_loss 0.140193 lr 0.00045758 rank 7
2023-02-21 11:08:54,965 DEBUG TRAIN Batch 14/2600 loss 9.168444 loss_att 11.685130 loss_ctc 10.093309 loss_rnnt 8.478944 hw_loss 0.117837 lr 0.00045764 rank 6
2023-02-21 11:08:54,966 DEBUG TRAIN Batch 14/2600 loss 11.134470 loss_att 16.812595 loss_ctc 19.385683 loss_rnnt 8.853144 hw_loss 0.085384 lr 0.00045762 rank 4
2023-02-21 11:08:54,967 DEBUG TRAIN Batch 14/2600 loss 14.656801 loss_att 20.343014 loss_ctc 20.270077 loss_rnnt 12.684181 hw_loss 0.163011 lr 0.00045762 rank 2
2023-02-21 11:08:54,967 DEBUG TRAIN Batch 14/2600 loss 5.571293 loss_att 8.758725 loss_ctc 8.762257 loss_rnnt 4.469048 hw_loss 0.073682 lr 0.00045761 rank 0
2023-02-21 11:08:54,968 DEBUG TRAIN Batch 14/2600 loss 17.103895 loss_att 22.872316 loss_ctc 24.518818 loss_rnnt 14.915926 hw_loss 0.085554 lr 0.00045762 rank 5
2023-02-21 11:08:54,968 DEBUG TRAIN Batch 14/2600 loss 10.588098 loss_att 13.335250 loss_ctc 14.139127 loss_rnnt 9.471992 hw_loss 0.174759 lr 0.00045762 rank 3
2023-02-21 11:10:13,640 DEBUG TRAIN Batch 14/2700 loss 11.102807 loss_att 16.882357 loss_ctc 17.071978 loss_rnnt 9.080820 hw_loss 0.131603 lr 0.00045750 rank 1
2023-02-21 11:10:13,641 DEBUG TRAIN Batch 14/2700 loss 11.115996 loss_att 14.444710 loss_ctc 18.667593 loss_rnnt 9.429729 hw_loss 0.025586 lr 0.00045739 rank 7
2023-02-21 11:10:13,643 DEBUG TRAIN Batch 14/2700 loss 10.250843 loss_att 11.610750 loss_ctc 12.413947 loss_rnnt 9.677280 hw_loss 0.024688 lr 0.00045743 rank 5
2023-02-21 11:10:13,645 DEBUG TRAIN Batch 14/2700 loss 16.178499 loss_att 17.447853 loss_ctc 21.792242 loss_rnnt 15.107562 hw_loss 0.128563 lr 0.00045745 rank 6
2023-02-21 11:10:13,649 DEBUG TRAIN Batch 14/2700 loss 17.847422 loss_att 23.063353 loss_ctc 23.352585 loss_rnnt 16.024714 hw_loss 0.085309 lr 0.00045742 rank 0
2023-02-21 11:10:13,651 DEBUG TRAIN Batch 14/2700 loss 11.170775 loss_att 11.137989 loss_ctc 13.026339 loss_rnnt 10.725386 hw_loss 0.383510 lr 0.00045743 rank 3
2023-02-21 11:10:13,670 DEBUG TRAIN Batch 14/2700 loss 11.408542 loss_att 13.889153 loss_ctc 16.164062 loss_rnnt 10.209353 hw_loss 0.129369 lr 0.00045743 rank 2
2023-02-21 11:10:13,681 DEBUG TRAIN Batch 14/2700 loss 11.823140 loss_att 15.623260 loss_ctc 17.657288 loss_rnnt 10.201669 hw_loss 0.156677 lr 0.00045743 rank 4
2023-02-21 11:11:33,960 DEBUG TRAIN Batch 14/2800 loss 9.903825 loss_att 12.979469 loss_ctc 11.729868 loss_rnnt 8.987146 hw_loss 0.108893 lr 0.00045720 rank 7
2023-02-21 11:11:33,964 DEBUG TRAIN Batch 14/2800 loss 7.967426 loss_att 12.949249 loss_ctc 9.613156 loss_rnnt 6.693596 hw_loss 0.108813 lr 0.00045723 rank 0
2023-02-21 11:11:33,965 DEBUG TRAIN Batch 14/2800 loss 7.263003 loss_att 10.584890 loss_ctc 10.821215 loss_rnnt 6.047367 hw_loss 0.144057 lr 0.00045731 rank 1
2023-02-21 11:11:33,969 DEBUG TRAIN Batch 14/2800 loss 6.392850 loss_att 9.966411 loss_ctc 11.824605 loss_rnnt 4.939467 hw_loss 0.027069 lr 0.00045726 rank 6
2023-02-21 11:11:33,970 DEBUG TRAIN Batch 14/2800 loss 14.480452 loss_att 16.286201 loss_ctc 15.215776 loss_rnnt 13.960863 hw_loss 0.113239 lr 0.00045724 rank 5
2023-02-21 11:11:33,972 DEBUG TRAIN Batch 14/2800 loss 10.732892 loss_att 14.815968 loss_ctc 16.758871 loss_rnnt 9.074390 hw_loss 0.072043 lr 0.00045724 rank 2
2023-02-21 11:11:33,972 DEBUG TRAIN Batch 14/2800 loss 7.597253 loss_att 14.124890 loss_ctc 12.200134 loss_rnnt 5.664012 hw_loss 0.026243 lr 0.00045723 rank 4
2023-02-21 11:11:34,013 DEBUG TRAIN Batch 14/2800 loss 11.808198 loss_att 14.852505 loss_ctc 11.686340 loss_rnnt 11.151978 hw_loss 0.119263 lr 0.00045724 rank 3
2023-02-21 11:12:53,923 DEBUG TRAIN Batch 14/2900 loss 14.193301 loss_att 18.993917 loss_ctc 20.318310 loss_rnnt 12.367781 hw_loss 0.091370 lr 0.00045712 rank 1
2023-02-21 11:12:53,924 DEBUG TRAIN Batch 14/2900 loss 11.590816 loss_att 16.880779 loss_ctc 15.432895 loss_rnnt 9.930390 hw_loss 0.169043 lr 0.00045707 rank 6
2023-02-21 11:12:53,928 DEBUG TRAIN Batch 14/2900 loss 6.815304 loss_att 9.793805 loss_ctc 10.416889 loss_rnnt 5.644790 hw_loss 0.177381 lr 0.00045704 rank 0
2023-02-21 11:12:53,929 DEBUG TRAIN Batch 14/2900 loss 11.708692 loss_att 17.394581 loss_ctc 16.468880 loss_rnnt 9.874136 hw_loss 0.117536 lr 0.00045705 rank 2
2023-02-21 11:12:53,930 DEBUG TRAIN Batch 14/2900 loss 13.659924 loss_att 19.003933 loss_ctc 14.745478 loss_rnnt 12.407153 hw_loss 0.073555 lr 0.00045704 rank 4
2023-02-21 11:12:53,930 DEBUG TRAIN Batch 14/2900 loss 8.408825 loss_att 12.898510 loss_ctc 8.556376 loss_rnnt 7.474232 hw_loss 0.031843 lr 0.00045705 rank 3
2023-02-21 11:12:53,933 DEBUG TRAIN Batch 14/2900 loss 7.057945 loss_att 11.467443 loss_ctc 10.071487 loss_rnnt 5.730599 hw_loss 0.081827 lr 0.00045701 rank 7
2023-02-21 11:12:53,934 DEBUG TRAIN Batch 14/2900 loss 6.817582 loss_att 10.988508 loss_ctc 10.019016 loss_rnnt 5.530675 hw_loss 0.048494 lr 0.00045705 rank 5
2023-02-21 11:14:11,507 DEBUG TRAIN Batch 14/3000 loss 11.426338 loss_att 13.028324 loss_ctc 18.408911 loss_rnnt 10.130014 hw_loss 0.084219 lr 0.00045682 rank 7
2023-02-21 11:14:11,513 DEBUG TRAIN Batch 14/3000 loss 20.699289 loss_att 24.793163 loss_ctc 27.528936 loss_rnnt 18.919147 hw_loss 0.095149 lr 0.00045686 rank 2
2023-02-21 11:14:11,514 DEBUG TRAIN Batch 14/3000 loss 12.818593 loss_att 14.866316 loss_ctc 19.410316 loss_rnnt 11.462109 hw_loss 0.127580 lr 0.00045685 rank 0
2023-02-21 11:14:11,514 DEBUG TRAIN Batch 14/3000 loss 13.744368 loss_att 15.241674 loss_ctc 20.730389 loss_rnnt 12.473040 hw_loss 0.075742 lr 0.00045685 rank 4
2023-02-21 11:14:11,516 DEBUG TRAIN Batch 14/3000 loss 12.008697 loss_att 11.554803 loss_ctc 10.121076 loss_rnnt 12.298400 hw_loss 0.098922 lr 0.00045688 rank 6
2023-02-21 11:14:11,520 DEBUG TRAIN Batch 14/3000 loss 14.157383 loss_att 15.163040 loss_ctc 18.769047 loss_rnnt 13.213348 hw_loss 0.240027 lr 0.00045686 rank 3
2023-02-21 11:14:11,519 DEBUG TRAIN Batch 14/3000 loss 17.290585 loss_att 19.868217 loss_ctc 23.003990 loss_rnnt 15.961598 hw_loss 0.096884 lr 0.00045693 rank 1
2023-02-21 11:14:11,563 DEBUG TRAIN Batch 14/3000 loss 8.035190 loss_att 10.325970 loss_ctc 12.818915 loss_rnnt 6.855165 hw_loss 0.157573 lr 0.00045686 rank 5
2023-02-21 11:15:31,107 DEBUG TRAIN Batch 14/3100 loss 9.684919 loss_att 11.114676 loss_ctc 11.800670 loss_rnnt 9.023877 hw_loss 0.174358 lr 0.00045667 rank 2
2023-02-21 11:15:31,108 DEBUG TRAIN Batch 14/3100 loss 19.306856 loss_att 20.068104 loss_ctc 21.467758 loss_rnnt 18.744724 hw_loss 0.228307 lr 0.00045663 rank 7
2023-02-21 11:15:31,110 DEBUG TRAIN Batch 14/3100 loss 14.116830 loss_att 17.134501 loss_ctc 16.675484 loss_rnnt 13.046190 hw_loss 0.236159 lr 0.00045674 rank 1
2023-02-21 11:15:31,113 DEBUG TRAIN Batch 14/3100 loss 15.258815 loss_att 15.088448 loss_ctc 17.862373 loss_rnnt 14.761295 hw_loss 0.345845 lr 0.00045666 rank 0
2023-02-21 11:15:31,114 DEBUG TRAIN Batch 14/3100 loss 11.322089 loss_att 12.784768 loss_ctc 14.180833 loss_rnnt 10.555448 hw_loss 0.174262 lr 0.00045667 rank 3
2023-02-21 11:15:31,114 DEBUG TRAIN Batch 14/3100 loss 11.067247 loss_att 11.559144 loss_ctc 16.149595 loss_rnnt 10.225142 hw_loss 0.123902 lr 0.00045666 rank 4
2023-02-21 11:15:31,119 DEBUG TRAIN Batch 14/3100 loss 12.380825 loss_att 13.037639 loss_ctc 14.966477 loss_rnnt 11.789170 hw_loss 0.216635 lr 0.00045667 rank 5
2023-02-21 11:15:31,156 DEBUG TRAIN Batch 14/3100 loss 15.636212 loss_att 16.143084 loss_ctc 20.281713 loss_rnnt 14.814348 hw_loss 0.189541 lr 0.00045669 rank 6
2023-02-21 11:16:52,623 DEBUG TRAIN Batch 14/3200 loss 6.427316 loss_att 12.184355 loss_ctc 9.175230 loss_rnnt 4.813354 hw_loss 0.180311 lr 0.00045648 rank 3
2023-02-21 11:16:52,624 DEBUG TRAIN Batch 14/3200 loss 5.189213 loss_att 7.186849 loss_ctc 7.311157 loss_rnnt 4.461275 hw_loss 0.085283 lr 0.00045647 rank 4
2023-02-21 11:16:52,626 DEBUG TRAIN Batch 14/3200 loss 14.110764 loss_att 20.963413 loss_ctc 20.965382 loss_rnnt 11.754505 hw_loss 0.134585 lr 0.00045650 rank 6
2023-02-21 11:16:52,625 DEBUG TRAIN Batch 14/3200 loss 23.029755 loss_att 28.392725 loss_ctc 30.444103 loss_rnnt 20.934500 hw_loss 0.063902 lr 0.00045647 rank 0
2023-02-21 11:16:52,627 DEBUG TRAIN Batch 14/3200 loss 10.455081 loss_att 10.501288 loss_ctc 14.105319 loss_rnnt 9.850317 hw_loss 0.204045 lr 0.00045655 rank 1
2023-02-21 11:16:52,634 DEBUG TRAIN Batch 14/3200 loss 4.782963 loss_att 9.470849 loss_ctc 6.476045 loss_rnnt 3.595600 hw_loss 0.045078 lr 0.00045648 rank 2
2023-02-21 11:16:52,641 DEBUG TRAIN Batch 14/3200 loss 18.833797 loss_att 25.199308 loss_ctc 27.309515 loss_rnnt 16.388916 hw_loss 0.078160 lr 0.00045644 rank 7
2023-02-21 11:16:52,673 DEBUG TRAIN Batch 14/3200 loss 17.451952 loss_att 25.753927 loss_ctc 16.038996 loss_rnnt 15.966322 hw_loss 0.025555 lr 0.00045648 rank 5
2023-02-21 11:18:11,913 DEBUG TRAIN Batch 14/3300 loss 6.326580 loss_att 9.792188 loss_ctc 11.468313 loss_rnnt 4.936009 hw_loss 0.022284 lr 0.00045631 rank 6
2023-02-21 11:18:11,913 DEBUG TRAIN Batch 14/3300 loss 5.198339 loss_att 8.355730 loss_ctc 5.751304 loss_rnnt 4.370767 hw_loss 0.229435 lr 0.00045636 rank 1
2023-02-21 11:18:11,915 DEBUG TRAIN Batch 14/3300 loss 4.798337 loss_att 8.504756 loss_ctc 9.718582 loss_rnnt 3.354333 hw_loss 0.087538 lr 0.00045628 rank 4
2023-02-21 11:18:11,916 DEBUG TRAIN Batch 14/3300 loss 7.569524 loss_att 8.218388 loss_ctc 10.973923 loss_rnnt 6.974127 hw_loss 0.021945 lr 0.00045629 rank 2
2023-02-21 11:18:11,917 DEBUG TRAIN Batch 14/3300 loss 9.005242 loss_att 11.165130 loss_ctc 9.549995 loss_rnnt 8.476492 hw_loss 0.045261 lr 0.00045625 rank 7
2023-02-21 11:18:11,923 DEBUG TRAIN Batch 14/3300 loss 9.912279 loss_att 10.535275 loss_ctc 13.488338 loss_rnnt 9.259331 hw_loss 0.096637 lr 0.00045629 rank 3
2023-02-21 11:18:11,928 DEBUG TRAIN Batch 14/3300 loss 8.703625 loss_att 15.634180 loss_ctc 15.314380 loss_rnnt 6.359300 hw_loss 0.143961 lr 0.00045629 rank 5
2023-02-21 11:18:11,963 DEBUG TRAIN Batch 14/3300 loss 15.526736 loss_att 20.490524 loss_ctc 22.270248 loss_rnnt 13.587459 hw_loss 0.088846 lr 0.00045628 rank 0
2023-02-21 11:19:29,866 DEBUG TRAIN Batch 14/3400 loss 10.640178 loss_att 14.790332 loss_ctc 13.288163 loss_rnnt 9.427180 hw_loss 0.056067 lr 0.00045612 rank 6
2023-02-21 11:19:29,866 DEBUG TRAIN Batch 14/3400 loss 12.736797 loss_att 17.079567 loss_ctc 20.187752 loss_rnnt 10.826735 hw_loss 0.090090 lr 0.00045610 rank 3
2023-02-21 11:19:29,868 DEBUG TRAIN Batch 14/3400 loss 12.366565 loss_att 14.346613 loss_ctc 13.639857 loss_rnnt 11.777174 hw_loss 0.044268 lr 0.00045617 rank 1
2023-02-21 11:19:29,869 DEBUG TRAIN Batch 14/3400 loss 5.521877 loss_att 9.957723 loss_ctc 9.717152 loss_rnnt 4.024698 hw_loss 0.094952 lr 0.00045609 rank 0
2023-02-21 11:19:29,869 DEBUG TRAIN Batch 14/3400 loss 5.962955 loss_att 9.006669 loss_ctc 6.692245 loss_rnnt 5.186491 hw_loss 0.132157 lr 0.00045606 rank 7
2023-02-21 11:19:29,871 DEBUG TRAIN Batch 14/3400 loss 23.241404 loss_att 26.921471 loss_ctc 31.562147 loss_rnnt 21.310310 hw_loss 0.160589 lr 0.00045609 rank 4
2023-02-21 11:19:29,871 DEBUG TRAIN Batch 14/3400 loss 26.677588 loss_att 27.364552 loss_ctc 27.272743 loss_rnnt 26.409592 hw_loss 0.096092 lr 0.00045610 rank 2
2023-02-21 11:19:29,875 DEBUG TRAIN Batch 14/3400 loss 8.912182 loss_att 12.559615 loss_ctc 13.783534 loss_rnnt 7.445309 hw_loss 0.164762 lr 0.00045610 rank 5
2023-02-21 11:20:49,301 DEBUG TRAIN Batch 14/3500 loss 20.276884 loss_att 22.036659 loss_ctc 27.353722 loss_rnnt 18.930450 hw_loss 0.095437 lr 0.00045587 rank 7
2023-02-21 11:20:49,305 DEBUG TRAIN Batch 14/3500 loss 7.749424 loss_att 11.062002 loss_ctc 9.173781 loss_rnnt 6.834597 hw_loss 0.116994 lr 0.00045590 rank 0
2023-02-21 11:20:49,307 DEBUG TRAIN Batch 14/3500 loss 11.606710 loss_att 14.835238 loss_ctc 14.030218 loss_rnnt 10.581376 hw_loss 0.105927 lr 0.00045598 rank 1
2023-02-21 11:20:49,308 DEBUG TRAIN Batch 14/3500 loss 16.939548 loss_att 15.723099 loss_ctc 17.907116 loss_rnnt 17.026047 hw_loss 0.052093 lr 0.00045591 rank 3
2023-02-21 11:20:49,310 DEBUG TRAIN Batch 14/3500 loss 18.237761 loss_att 25.165752 loss_ctc 23.719580 loss_rnnt 16.085209 hw_loss 0.067582 lr 0.00045590 rank 4
2023-02-21 11:20:49,312 DEBUG TRAIN Batch 14/3500 loss 4.828511 loss_att 6.903667 loss_ctc 5.969369 loss_rnnt 4.154830 hw_loss 0.199753 lr 0.00045593 rank 6
2023-02-21 11:20:49,314 DEBUG TRAIN Batch 14/3500 loss 11.875183 loss_att 14.512726 loss_ctc 15.930296 loss_rnnt 10.656403 hw_loss 0.282355 lr 0.00045591 rank 2
2023-02-21 11:20:49,315 DEBUG TRAIN Batch 14/3500 loss 9.392546 loss_att 11.215715 loss_ctc 10.620040 loss_rnnt 8.817324 hw_loss 0.087980 lr 0.00045591 rank 5
2023-02-21 11:22:09,687 DEBUG TRAIN Batch 14/3600 loss 15.867378 loss_att 17.785469 loss_ctc 17.838739 loss_rnnt 15.132012 hw_loss 0.166687 lr 0.00045568 rank 7
2023-02-21 11:22:09,691 DEBUG TRAIN Batch 14/3600 loss 13.026927 loss_att 16.415108 loss_ctc 14.986919 loss_rnnt 12.023174 hw_loss 0.121472 lr 0.00045572 rank 2
2023-02-21 11:22:09,693 DEBUG TRAIN Batch 14/3600 loss 7.922791 loss_att 10.299128 loss_ctc 10.517646 loss_rnnt 7.026148 hw_loss 0.141368 lr 0.00045574 rank 6
2023-02-21 11:22:09,697 DEBUG TRAIN Batch 14/3600 loss 9.257570 loss_att 11.561594 loss_ctc 11.663861 loss_rnnt 8.403959 hw_loss 0.134937 lr 0.00045571 rank 0
2023-02-21 11:22:09,700 DEBUG TRAIN Batch 14/3600 loss 9.607432 loss_att 14.196903 loss_ctc 13.123220 loss_rnnt 8.202880 hw_loss 0.033539 lr 0.00045572 rank 5
2023-02-21 11:22:09,700 DEBUG TRAIN Batch 14/3600 loss 8.740448 loss_att 12.137912 loss_ctc 11.528402 loss_rnnt 7.626378 hw_loss 0.117844 lr 0.00045571 rank 4
2023-02-21 11:22:09,700 DEBUG TRAIN Batch 14/3600 loss 13.270885 loss_att 17.308937 loss_ctc 16.215952 loss_rnnt 11.984478 hw_loss 0.161474 lr 0.00045572 rank 3
2023-02-21 11:22:09,708 DEBUG TRAIN Batch 14/3600 loss 23.314533 loss_att 27.639492 loss_ctc 33.253212 loss_rnnt 21.040047 hw_loss 0.158134 lr 0.00045579 rank 1
2023-02-21 11:23:28,820 DEBUG TRAIN Batch 14/3700 loss 11.854574 loss_att 13.812318 loss_ctc 14.155070 loss_rnnt 11.113718 hw_loss 0.079826 lr 0.00045553 rank 2
2023-02-21 11:23:28,823 DEBUG TRAIN Batch 14/3700 loss 15.180078 loss_att 17.232185 loss_ctc 22.132086 loss_rnnt 13.722986 hw_loss 0.224502 lr 0.00045549 rank 7
2023-02-21 11:23:28,828 DEBUG TRAIN Batch 14/3700 loss 15.261619 loss_att 19.690857 loss_ctc 20.938116 loss_rnnt 13.557111 hw_loss 0.115862 lr 0.00045560 rank 1
2023-02-21 11:23:28,827 DEBUG TRAIN Batch 14/3700 loss 9.626158 loss_att 11.971679 loss_ctc 10.251983 loss_rnnt 9.014557 hw_loss 0.110724 lr 0.00045552 rank 4
2023-02-21 11:23:28,828 DEBUG TRAIN Batch 14/3700 loss 15.290956 loss_att 18.214027 loss_ctc 26.319738 loss_rnnt 13.072429 hw_loss 0.306391 lr 0.00045552 rank 0
2023-02-21 11:23:28,829 DEBUG TRAIN Batch 14/3700 loss 8.555246 loss_att 14.226522 loss_ctc 14.765419 loss_rnnt 6.557878 hw_loss 0.065795 lr 0.00045553 rank 3
2023-02-21 11:23:28,834 DEBUG TRAIN Batch 14/3700 loss 11.915208 loss_att 13.811556 loss_ctc 14.340364 loss_rnnt 11.147361 hw_loss 0.122293 lr 0.00045553 rank 5
2023-02-21 11:23:28,834 DEBUG TRAIN Batch 14/3700 loss 12.719246 loss_att 14.936147 loss_ctc 16.125637 loss_rnnt 11.781019 hw_loss 0.076238 lr 0.00045555 rank 6
2023-02-21 11:24:46,715 DEBUG TRAIN Batch 14/3800 loss 14.753879 loss_att 15.906527 loss_ctc 17.965366 loss_rnnt 13.962669 hw_loss 0.248401 lr 0.00045530 rank 7
2023-02-21 11:24:46,715 DEBUG TRAIN Batch 14/3800 loss 13.656982 loss_att 14.137730 loss_ctc 15.264931 loss_rnnt 13.303068 hw_loss 0.081321 lr 0.00045536 rank 6
2023-02-21 11:24:46,716 DEBUG TRAIN Batch 14/3800 loss 4.664957 loss_att 6.362389 loss_ctc 5.928966 loss_rnnt 4.122299 hw_loss 0.064945 lr 0.00045533 rank 4
2023-02-21 11:24:46,718 DEBUG TRAIN Batch 14/3800 loss 22.826519 loss_att 21.751554 loss_ctc 34.605663 loss_rnnt 21.226318 hw_loss 0.458703 lr 0.00045534 rank 2
2023-02-21 11:24:46,719 DEBUG TRAIN Batch 14/3800 loss 21.066162 loss_att 23.583836 loss_ctc 26.256292 loss_rnnt 19.822002 hw_loss 0.091145 lr 0.00045533 rank 0
2023-02-21 11:24:46,720 DEBUG TRAIN Batch 14/3800 loss 19.783922 loss_att 20.799505 loss_ctc 23.397341 loss_rnnt 18.985106 hw_loss 0.213580 lr 0.00045541 rank 1
2023-02-21 11:24:46,724 DEBUG TRAIN Batch 14/3800 loss 11.068977 loss_att 13.746199 loss_ctc 13.937346 loss_rnnt 10.096771 hw_loss 0.101836 lr 0.00045534 rank 3
2023-02-21 11:24:46,767 DEBUG TRAIN Batch 14/3800 loss 11.981585 loss_att 14.905861 loss_ctc 16.169861 loss_rnnt 10.718548 hw_loss 0.224522 lr 0.00045534 rank 5
2023-02-21 11:26:07,583 DEBUG TRAIN Batch 14/3900 loss 11.284440 loss_att 17.193258 loss_ctc 16.428886 loss_rnnt 9.381811 hw_loss 0.065511 lr 0.00045511 rank 7
2023-02-21 11:26:07,585 DEBUG TRAIN Batch 14/3900 loss 20.975958 loss_att 28.301085 loss_ctc 31.691891 loss_rnnt 18.045668 hw_loss 0.068388 lr 0.00045515 rank 4
2023-02-21 11:26:07,588 DEBUG TRAIN Batch 14/3900 loss 21.689812 loss_att 21.009327 loss_ctc 26.538197 loss_rnnt 21.166122 hw_loss 0.025001 lr 0.00045517 rank 6
2023-02-21 11:26:07,588 DEBUG TRAIN Batch 14/3900 loss 12.951899 loss_att 12.594307 loss_ctc 16.641851 loss_rnnt 12.518016 hw_loss 0.025140 lr 0.00045514 rank 0
2023-02-21 11:26:07,588 DEBUG TRAIN Batch 14/3900 loss 8.370798 loss_att 10.778486 loss_ctc 11.748497 loss_rnnt 7.425731 hw_loss 0.024695 lr 0.00045515 rank 5
2023-02-21 11:26:07,590 DEBUG TRAIN Batch 14/3900 loss 12.247313 loss_att 16.613560 loss_ctc 18.187527 loss_rnnt 10.515985 hw_loss 0.123842 lr 0.00045522 rank 1
2023-02-21 11:26:07,607 DEBUG TRAIN Batch 14/3900 loss 6.292393 loss_att 12.045506 loss_ctc 7.913925 loss_rnnt 4.768819 hw_loss 0.293902 lr 0.00045515 rank 2
2023-02-21 11:26:07,639 DEBUG TRAIN Batch 14/3900 loss 8.823264 loss_att 10.882236 loss_ctc 11.892578 loss_rnnt 7.926628 hw_loss 0.141749 lr 0.00045515 rank 3
2023-02-21 11:27:26,963 DEBUG TRAIN Batch 14/4000 loss 10.557144 loss_att 16.920059 loss_ctc 14.200752 loss_rnnt 8.754355 hw_loss 0.083234 lr 0.00045498 rank 6
2023-02-21 11:27:26,964 DEBUG TRAIN Batch 14/4000 loss 11.449661 loss_att 16.502058 loss_ctc 17.961813 loss_rnnt 9.532356 hw_loss 0.072259 lr 0.00045492 rank 7
2023-02-21 11:27:26,966 DEBUG TRAIN Batch 14/4000 loss 5.882621 loss_att 8.717363 loss_ctc 6.291839 loss_rnnt 5.093709 hw_loss 0.313877 lr 0.00045496 rank 2
2023-02-21 11:27:26,967 DEBUG TRAIN Batch 14/4000 loss 9.165221 loss_att 13.711895 loss_ctc 11.402115 loss_rnnt 7.923005 hw_loss 0.064929 lr 0.00045496 rank 3
2023-02-21 11:27:26,970 DEBUG TRAIN Batch 14/4000 loss 11.267160 loss_att 13.536190 loss_ctc 17.966362 loss_rnnt 9.896343 hw_loss 0.044593 lr 0.00045496 rank 4
2023-02-21 11:27:26,970 DEBUG TRAIN Batch 14/4000 loss 7.500269 loss_att 10.276670 loss_ctc 8.171543 loss_rnnt 6.842428 hw_loss 0.024483 lr 0.00045503 rank 1
2023-02-21 11:27:26,972 DEBUG TRAIN Batch 14/4000 loss 8.422509 loss_att 10.774400 loss_ctc 11.085385 loss_rnnt 7.534721 hw_loss 0.116924 lr 0.00045496 rank 5
2023-02-21 11:27:27,020 DEBUG TRAIN Batch 14/4000 loss 7.767100 loss_att 11.020284 loss_ctc 7.575790 loss_rnnt 7.073235 hw_loss 0.128881 lr 0.00045495 rank 0
2023-02-21 11:28:45,483 DEBUG TRAIN Batch 14/4100 loss 11.425442 loss_att 12.714809 loss_ctc 13.822710 loss_rnnt 10.746572 hw_loss 0.190052 lr 0.00045474 rank 7
2023-02-21 11:28:45,486 DEBUG TRAIN Batch 14/4100 loss 14.834520 loss_att 20.557652 loss_ctc 18.414967 loss_rnnt 13.116470 hw_loss 0.180059 lr 0.00045477 rank 2
2023-02-21 11:28:45,488 DEBUG TRAIN Batch 14/4100 loss 10.931419 loss_att 14.554865 loss_ctc 17.220583 loss_rnnt 9.312474 hw_loss 0.104437 lr 0.00045477 rank 4
2023-02-21 11:28:45,488 DEBUG TRAIN Batch 14/4100 loss 9.174454 loss_att 14.438482 loss_ctc 14.141555 loss_rnnt 7.351015 hw_loss 0.203161 lr 0.00045484 rank 1
2023-02-21 11:28:45,491 DEBUG TRAIN Batch 14/4100 loss 3.221852 loss_att 6.201895 loss_ctc 5.189114 loss_rnnt 2.281253 hw_loss 0.154291 lr 0.00045480 rank 6
2023-02-21 11:28:45,493 DEBUG TRAIN Batch 14/4100 loss 20.886177 loss_att 24.375614 loss_ctc 30.315275 loss_rnnt 18.839146 hw_loss 0.172370 lr 0.00045477 rank 5
2023-02-21 11:28:45,494 DEBUG TRAIN Batch 14/4100 loss 11.855512 loss_att 14.599020 loss_ctc 18.506107 loss_rnnt 10.353747 hw_loss 0.124342 lr 0.00045476 rank 0
2023-02-21 11:28:45,536 DEBUG TRAIN Batch 14/4100 loss 6.590518 loss_att 11.730351 loss_ctc 8.399199 loss_rnnt 5.283566 hw_loss 0.070925 lr 0.00045477 rank 3
2023-02-21 11:30:04,294 DEBUG TRAIN Batch 14/4200 loss 19.973120 loss_att 22.189175 loss_ctc 25.041700 loss_rnnt 18.785410 hw_loss 0.128791 lr 0.00045458 rank 4
2023-02-21 11:30:04,296 DEBUG TRAIN Batch 14/4200 loss 8.448437 loss_att 12.537146 loss_ctc 11.264932 loss_rnnt 7.239460 hw_loss 0.029439 lr 0.00045458 rank 0
2023-02-21 11:30:04,295 DEBUG TRAIN Batch 14/4200 loss 13.803572 loss_att 18.449759 loss_ctc 23.788025 loss_rnnt 11.412313 hw_loss 0.245177 lr 0.00045465 rank 1
2023-02-21 11:30:04,297 DEBUG TRAIN Batch 14/4200 loss 5.494365 loss_att 8.515779 loss_ctc 11.456353 loss_rnnt 4.051448 hw_loss 0.081941 lr 0.00045459 rank 3
2023-02-21 11:30:04,298 DEBUG TRAIN Batch 14/4200 loss 18.099279 loss_att 18.524174 loss_ctc 21.488800 loss_rnnt 17.472855 hw_loss 0.167835 lr 0.00045455 rank 7
2023-02-21 11:30:04,298 DEBUG TRAIN Batch 14/4200 loss 15.600430 loss_att 17.668198 loss_ctc 25.047462 loss_rnnt 13.881895 hw_loss 0.085085 lr 0.00045461 rank 6
2023-02-21 11:30:04,300 DEBUG TRAIN Batch 14/4200 loss 8.054257 loss_att 11.387659 loss_ctc 11.187206 loss_rnnt 6.860248 hw_loss 0.205506 lr 0.00045458 rank 2
2023-02-21 11:30:04,306 DEBUG TRAIN Batch 14/4200 loss 18.422426 loss_att 20.719927 loss_ctc 21.745911 loss_rnnt 17.448191 hw_loss 0.134256 lr 0.00045458 rank 5
2023-02-21 11:31:24,054 DEBUG TRAIN Batch 14/4300 loss 4.346702 loss_att 8.284513 loss_ctc 7.292711 loss_rnnt 3.068994 hw_loss 0.182520 lr 0.00045436 rank 7
2023-02-21 11:31:24,056 DEBUG TRAIN Batch 14/4300 loss 13.382078 loss_att 14.816010 loss_ctc 15.596143 loss_rnnt 12.698837 hw_loss 0.189835 lr 0.00045439 rank 4
2023-02-21 11:31:24,056 DEBUG TRAIN Batch 14/4300 loss 9.730697 loss_att 10.804504 loss_ctc 9.920953 loss_rnnt 9.367184 hw_loss 0.231346 lr 0.00045440 rank 2
2023-02-21 11:31:24,060 DEBUG TRAIN Batch 14/4300 loss 10.291335 loss_att 12.121153 loss_ctc 14.355682 loss_rnnt 9.353897 hw_loss 0.055428 lr 0.00045440 rank 5
2023-02-21 11:31:24,061 DEBUG TRAIN Batch 14/4300 loss 5.514914 loss_att 9.992949 loss_ctc 10.062078 loss_rnnt 3.954007 hw_loss 0.110646 lr 0.00045442 rank 6
2023-02-21 11:31:24,064 DEBUG TRAIN Batch 14/4300 loss 14.245553 loss_att 15.302534 loss_ctc 15.668018 loss_rnnt 13.717142 hw_loss 0.238784 lr 0.00045439 rank 0
2023-02-21 11:31:24,064 DEBUG TRAIN Batch 14/4300 loss 11.900003 loss_att 14.282242 loss_ctc 15.381772 loss_rnnt 10.892820 hw_loss 0.124689 lr 0.00045447 rank 1
2023-02-21 11:31:24,068 DEBUG TRAIN Batch 14/4300 loss 9.261456 loss_att 13.998717 loss_ctc 14.709116 loss_rnnt 7.565473 hw_loss 0.041582 lr 0.00045440 rank 3
2023-02-21 11:32:42,233 DEBUG TRAIN Batch 14/4400 loss 17.704803 loss_att 18.203096 loss_ctc 23.933414 loss_rnnt 16.737257 hw_loss 0.070136 lr 0.00045423 rank 6
2023-02-21 11:32:42,233 DEBUG TRAIN Batch 14/4400 loss 6.287664 loss_att 10.531371 loss_ctc 7.769584 loss_rnnt 5.154751 hw_loss 0.162342 lr 0.00045421 rank 2
2023-02-21 11:32:42,235 DEBUG TRAIN Batch 14/4400 loss 7.121383 loss_att 8.822486 loss_ctc 7.692471 loss_rnnt 6.635820 hw_loss 0.129745 lr 0.00045428 rank 1
2023-02-21 11:32:42,239 DEBUG TRAIN Batch 14/4400 loss 16.418629 loss_att 18.857437 loss_ctc 20.805725 loss_rnnt 15.234450 hw_loss 0.209007 lr 0.00045421 rank 4
2023-02-21 11:32:42,241 DEBUG TRAIN Batch 14/4400 loss 9.638524 loss_att 12.859950 loss_ctc 10.793136 loss_rnnt 8.776979 hw_loss 0.118709 lr 0.00045421 rank 3
2023-02-21 11:32:42,243 DEBUG TRAIN Batch 14/4400 loss 10.206547 loss_att 12.527575 loss_ctc 12.917433 loss_rnnt 9.299467 hw_loss 0.152666 lr 0.00045417 rank 7
2023-02-21 11:32:42,245 DEBUG TRAIN Batch 14/4400 loss 3.185978 loss_att 5.890737 loss_ctc 4.694112 loss_rnnt 2.428273 hw_loss 0.029380 lr 0.00045421 rank 5
2023-02-21 11:32:42,287 DEBUG TRAIN Batch 14/4400 loss 16.419088 loss_att 17.889561 loss_ctc 21.535372 loss_rnnt 15.407855 hw_loss 0.065565 lr 0.00045420 rank 0
2023-02-21 11:33:59,476 DEBUG TRAIN Batch 14/4500 loss 7.353087 loss_att 9.399331 loss_ctc 10.497475 loss_rnnt 6.495968 hw_loss 0.053659 lr 0.00045398 rank 7
2023-02-21 11:33:59,478 DEBUG TRAIN Batch 14/4500 loss 5.672890 loss_att 12.473683 loss_ctc 7.453512 loss_rnnt 4.041785 hw_loss 0.062868 lr 0.00045402 rank 2
2023-02-21 11:33:59,481 DEBUG TRAIN Batch 14/4500 loss 16.124302 loss_att 21.211977 loss_ctc 19.955322 loss_rnnt 14.560996 hw_loss 0.065563 lr 0.00045409 rank 1
2023-02-21 11:33:59,481 DEBUG TRAIN Batch 14/4500 loss 15.391641 loss_att 21.161144 loss_ctc 20.020439 loss_rnnt 13.562962 hw_loss 0.108011 lr 0.00045402 rank 4
2023-02-21 11:33:59,483 DEBUG TRAIN Batch 14/4500 loss 13.966226 loss_att 14.015831 loss_ctc 14.887484 loss_rnnt 13.662682 hw_loss 0.320228 lr 0.00045402 rank 3
2023-02-21 11:33:59,485 DEBUG TRAIN Batch 14/4500 loss 14.963564 loss_att 15.473040 loss_ctc 16.985172 loss_rnnt 14.530731 hw_loss 0.115106 lr 0.00045401 rank 0
2023-02-21 11:33:59,487 DEBUG TRAIN Batch 14/4500 loss 16.450968 loss_att 21.613102 loss_ctc 20.823925 loss_rnnt 14.777092 hw_loss 0.109478 lr 0.00045402 rank 5
2023-02-21 11:33:59,524 DEBUG TRAIN Batch 14/4500 loss 12.101685 loss_att 14.905544 loss_ctc 13.991932 loss_rnnt 11.201659 hw_loss 0.163541 lr 0.00045404 rank 6
2023-02-21 11:35:20,115 DEBUG TRAIN Batch 14/4600 loss 7.623257 loss_att 10.279329 loss_ctc 12.645888 loss_rnnt 6.408208 hw_loss 0.026531 lr 0.00045384 rank 2
2023-02-21 11:35:20,118 DEBUG TRAIN Batch 14/4600 loss 13.919786 loss_att 13.014139 loss_ctc 12.094933 loss_rnnt 14.267184 hw_loss 0.144460 lr 0.00045380 rank 7
2023-02-21 11:35:20,118 DEBUG TRAIN Batch 14/4600 loss 7.645756 loss_att 9.540154 loss_ctc 9.018415 loss_rnnt 7.040422 hw_loss 0.081438 lr 0.00045390 rank 1
2023-02-21 11:35:20,122 DEBUG TRAIN Batch 14/4600 loss 9.236561 loss_att 11.758070 loss_ctc 13.195501 loss_rnnt 8.155083 hw_loss 0.092471 lr 0.00045383 rank 0
2023-02-21 11:35:20,123 DEBUG TRAIN Batch 14/4600 loss 6.170790 loss_att 13.327376 loss_ctc 9.833479 loss_rnnt 4.205410 hw_loss 0.085693 lr 0.00045383 rank 4
2023-02-21 11:35:20,123 DEBUG TRAIN Batch 14/4600 loss 20.487659 loss_att 23.525635 loss_ctc 26.694578 loss_rnnt 18.932970 hw_loss 0.224069 lr 0.00045384 rank 3
2023-02-21 11:35:20,131 DEBUG TRAIN Batch 14/4600 loss 15.993574 loss_att 19.043196 loss_ctc 21.733496 loss_rnnt 14.538872 hw_loss 0.148979 lr 0.00045386 rank 6
2023-02-21 11:35:20,168 DEBUG TRAIN Batch 14/4600 loss 12.142911 loss_att 18.098875 loss_ctc 21.105461 loss_rnnt 9.662258 hw_loss 0.177101 lr 0.00045384 rank 5
2023-02-21 11:36:39,121 DEBUG TRAIN Batch 14/4700 loss 14.684023 loss_att 16.569841 loss_ctc 19.082991 loss_rnnt 13.661299 hw_loss 0.110684 lr 0.00045361 rank 7
2023-02-21 11:36:39,121 DEBUG TRAIN Batch 14/4700 loss 3.250180 loss_att 5.345263 loss_ctc 4.454268 loss_rnnt 2.570393 hw_loss 0.187924 lr 0.00045364 rank 4
2023-02-21 11:36:39,122 DEBUG TRAIN Batch 14/4700 loss 21.264711 loss_att 21.186522 loss_ctc 27.728966 loss_rnnt 20.274513 hw_loss 0.269881 lr 0.00045367 rank 6
2023-02-21 11:36:39,122 DEBUG TRAIN Batch 14/4700 loss 10.411277 loss_att 13.362124 loss_ctc 16.631264 loss_rnnt 8.944053 hw_loss 0.089481 lr 0.00045365 rank 2
2023-02-21 11:36:39,124 DEBUG TRAIN Batch 14/4700 loss 19.550032 loss_att 19.353180 loss_ctc 26.940273 loss_rnnt 18.554474 hw_loss 0.092931 lr 0.00045365 rank 3
2023-02-21 11:36:39,125 DEBUG TRAIN Batch 14/4700 loss 20.283659 loss_att 22.531549 loss_ctc 25.259178 loss_rnnt 19.105370 hw_loss 0.122455 lr 0.00045372 rank 1
2023-02-21 11:36:39,127 DEBUG TRAIN Batch 14/4700 loss 13.397534 loss_att 16.184097 loss_ctc 18.177021 loss_rnnt 12.082933 hw_loss 0.225040 lr 0.00045364 rank 0
2023-02-21 11:36:39,128 DEBUG TRAIN Batch 14/4700 loss 16.790688 loss_att 19.536274 loss_ctc 18.690928 loss_rnnt 15.849636 hw_loss 0.259812 lr 0.00045365 rank 5
2023-02-21 11:37:56,887 DEBUG TRAIN Batch 14/4800 loss 10.428588 loss_att 12.115936 loss_ctc 13.731032 loss_rnnt 9.607161 hw_loss 0.081809 lr 0.00045342 rank 7
2023-02-21 11:37:56,888 DEBUG TRAIN Batch 14/4800 loss 9.406928 loss_att 11.503364 loss_ctc 14.601112 loss_rnnt 8.220867 hw_loss 0.139156 lr 0.00045346 rank 4
2023-02-21 11:37:56,891 DEBUG TRAIN Batch 14/4800 loss 18.733992 loss_att 20.923286 loss_ctc 24.732084 loss_rnnt 17.451740 hw_loss 0.083715 lr 0.00045353 rank 1
2023-02-21 11:37:56,892 DEBUG TRAIN Batch 14/4800 loss 28.624117 loss_att 29.919498 loss_ctc 35.599457 loss_rnnt 27.359865 hw_loss 0.140868 lr 0.00045346 rank 2
2023-02-21 11:37:56,898 DEBUG TRAIN Batch 14/4800 loss 25.934393 loss_att 31.475887 loss_ctc 36.790810 loss_rnnt 23.330420 hw_loss 0.090290 lr 0.00045348 rank 6
2023-02-21 11:37:56,898 DEBUG TRAIN Batch 14/4800 loss 16.090761 loss_att 19.508183 loss_ctc 22.812809 loss_rnnt 14.349832 hw_loss 0.302197 lr 0.00045345 rank 0
2023-02-21 11:37:56,899 DEBUG TRAIN Batch 14/4800 loss 9.062234 loss_att 12.994413 loss_ctc 10.973412 loss_rnnt 7.989433 hw_loss 0.059141 lr 0.00045346 rank 3
2023-02-21 11:37:56,901 DEBUG TRAIN Batch 14/4800 loss 11.225871 loss_att 13.826488 loss_ctc 13.950912 loss_rnnt 10.293180 hw_loss 0.092303 lr 0.00045346 rank 5
2023-02-21 11:39:15,407 DEBUG TRAIN Batch 14/4900 loss 13.147176 loss_att 16.613447 loss_ctc 20.002201 loss_rnnt 11.502985 hw_loss 0.069249 lr 0.00045324 rank 7
2023-02-21 11:39:15,407 DEBUG TRAIN Batch 14/4900 loss 12.925342 loss_att 14.887163 loss_ctc 18.165491 loss_rnnt 11.802363 hw_loss 0.059864 lr 0.00045327 rank 4
2023-02-21 11:39:15,407 DEBUG TRAIN Batch 14/4900 loss 7.675406 loss_att 10.497431 loss_ctc 13.654590 loss_rnnt 6.211317 hw_loss 0.192114 lr 0.00045328 rank 3
2023-02-21 11:39:15,412 DEBUG TRAIN Batch 14/4900 loss 5.905404 loss_att 9.971462 loss_ctc 7.120469 loss_rnnt 4.839514 hw_loss 0.170005 lr 0.00045328 rank 2
2023-02-21 11:39:15,413 DEBUG TRAIN Batch 14/4900 loss 8.330016 loss_att 11.262836 loss_ctc 11.411879 loss_rnnt 7.281739 hw_loss 0.095245 lr 0.00045327 rank 0
2023-02-21 11:39:15,415 DEBUG TRAIN Batch 14/4900 loss 10.815280 loss_att 11.764144 loss_ctc 16.618389 loss_rnnt 9.817463 hw_loss 0.064305 lr 0.00045334 rank 1
2023-02-21 11:39:15,448 DEBUG TRAIN Batch 14/4900 loss 13.325259 loss_att 15.102157 loss_ctc 16.393936 loss_rnnt 12.507603 hw_loss 0.099600 lr 0.00045328 rank 5
2023-02-21 11:39:15,455 DEBUG TRAIN Batch 14/4900 loss 18.143795 loss_att 21.518036 loss_ctc 23.202862 loss_rnnt 16.751184 hw_loss 0.081040 lr 0.00045330 rank 6
2023-02-21 11:40:36,624 DEBUG TRAIN Batch 14/5000 loss 6.608552 loss_att 9.738846 loss_ctc 11.350246 loss_rnnt 5.262743 hw_loss 0.164108 lr 0.00045311 rank 6
2023-02-21 11:40:36,627 DEBUG TRAIN Batch 14/5000 loss 11.575455 loss_att 16.406078 loss_ctc 15.906596 loss_rnnt 9.870805 hw_loss 0.301950 lr 0.00045316 rank 1
2023-02-21 11:40:36,627 DEBUG TRAIN Batch 14/5000 loss 6.954934 loss_att 8.238890 loss_ctc 8.855494 loss_rnnt 6.295646 hw_loss 0.279543 lr 0.00045305 rank 7
2023-02-21 11:40:36,629 DEBUG TRAIN Batch 14/5000 loss 14.124103 loss_att 16.959158 loss_ctc 17.198730 loss_rnnt 13.066599 hw_loss 0.151014 lr 0.00045308 rank 0
2023-02-21 11:40:36,630 DEBUG TRAIN Batch 14/5000 loss 14.452595 loss_att 14.989254 loss_ctc 18.163071 loss_rnnt 13.793950 hw_loss 0.106092 lr 0.00045309 rank 4
2023-02-21 11:40:36,632 DEBUG TRAIN Batch 14/5000 loss 11.327372 loss_att 14.765461 loss_ctc 16.774891 loss_rnnt 9.823676 hw_loss 0.168267 lr 0.00045309 rank 2
2023-02-21 11:40:36,638 DEBUG TRAIN Batch 14/5000 loss 8.671205 loss_att 12.445566 loss_ctc 10.820462 loss_rnnt 7.603779 hw_loss 0.048724 lr 0.00045309 rank 3
2023-02-21 11:40:36,648 DEBUG TRAIN Batch 14/5000 loss 11.354405 loss_att 13.711267 loss_ctc 12.129728 loss_rnnt 10.730047 hw_loss 0.093018 lr 0.00045309 rank 5
2023-02-21 11:41:53,862 DEBUG TRAIN Batch 14/5100 loss 23.926575 loss_att 25.912151 loss_ctc 30.478331 loss_rnnt 22.641590 hw_loss 0.026810 lr 0.00045291 rank 3
2023-02-21 11:41:53,863 DEBUG TRAIN Batch 14/5100 loss 20.407179 loss_att 26.659227 loss_ctc 29.597855 loss_rnnt 17.908676 hw_loss 0.042506 lr 0.00045290 rank 4
2023-02-21 11:41:53,862 DEBUG TRAIN Batch 14/5100 loss 12.131118 loss_att 15.104907 loss_ctc 18.426620 loss_rnnt 10.568012 hw_loss 0.241777 lr 0.00045290 rank 2
2023-02-21 11:41:53,867 DEBUG TRAIN Batch 14/5100 loss 8.928715 loss_att 13.899556 loss_ctc 11.544136 loss_rnnt 7.496940 hw_loss 0.166656 lr 0.00045287 rank 7
2023-02-21 11:41:53,869 DEBUG TRAIN Batch 14/5100 loss 18.743687 loss_att 26.026264 loss_ctc 24.773657 loss_rnnt 16.459784 hw_loss 0.043860 lr 0.00045290 rank 5
2023-02-21 11:41:53,872 DEBUG TRAIN Batch 14/5100 loss 13.735171 loss_att 18.434101 loss_ctc 17.152761 loss_rnnt 12.261359 hw_loss 0.146902 lr 0.00045289 rank 0
2023-02-21 11:41:53,874 DEBUG TRAIN Batch 14/5100 loss 14.547418 loss_att 14.624718 loss_ctc 16.552282 loss_rnnt 14.197899 hw_loss 0.125145 lr 0.00045297 rank 1
2023-02-21 11:41:53,912 DEBUG TRAIN Batch 14/5100 loss 4.909861 loss_att 8.953753 loss_ctc 7.864580 loss_rnnt 3.645681 hw_loss 0.115197 lr 0.00045293 rank 6
2023-02-21 11:43:13,090 DEBUG TRAIN Batch 14/5200 loss 18.529913 loss_att 24.769541 loss_ctc 25.700615 loss_rnnt 16.291925 hw_loss 0.063690 lr 0.00045279 rank 1
2023-02-21 11:43:13,091 DEBUG TRAIN Batch 14/5200 loss 13.721317 loss_att 14.520540 loss_ctc 15.005774 loss_rnnt 13.274299 hw_loss 0.217339 lr 0.00045268 rank 7
2023-02-21 11:43:13,091 DEBUG TRAIN Batch 14/5200 loss 3.208715 loss_att 6.336429 loss_ctc 6.928928 loss_rnnt 2.012292 hw_loss 0.140348 lr 0.00045271 rank 4
2023-02-21 11:43:13,093 DEBUG TRAIN Batch 14/5200 loss 4.033455 loss_att 7.308559 loss_ctc 4.110766 loss_rnnt 3.297604 hw_loss 0.132230 lr 0.00045274 rank 6
2023-02-21 11:43:13,094 DEBUG TRAIN Batch 14/5200 loss 23.508820 loss_att 28.729364 loss_ctc 28.930035 loss_rnnt 21.727646 hw_loss 0.026694 lr 0.00045272 rank 3
2023-02-21 11:43:13,094 DEBUG TRAIN Batch 14/5200 loss 6.012402 loss_att 9.606706 loss_ctc 5.079076 loss_rnnt 5.383771 hw_loss 0.064149 lr 0.00045272 rank 2
2023-02-21 11:43:13,098 DEBUG TRAIN Batch 14/5200 loss 12.230960 loss_att 15.770036 loss_ctc 15.945731 loss_rnnt 10.960121 hw_loss 0.126978 lr 0.00045272 rank 5
2023-02-21 11:43:13,136 DEBUG TRAIN Batch 14/5200 loss 11.729501 loss_att 11.216558 loss_ctc 11.020110 loss_rnnt 11.817894 hw_loss 0.203965 lr 0.00045271 rank 0
2023-02-21 11:44:34,066 DEBUG TRAIN Batch 14/5300 loss 13.063004 loss_att 15.859071 loss_ctc 15.904882 loss_rnnt 12.019197 hw_loss 0.198139 lr 0.00045260 rank 1
2023-02-21 11:44:34,067 DEBUG TRAIN Batch 14/5300 loss 17.403801 loss_att 20.475245 loss_ctc 23.608503 loss_rnnt 15.884451 hw_loss 0.145816 lr 0.00045250 rank 7
2023-02-21 11:44:34,070 DEBUG TRAIN Batch 14/5300 loss 14.295846 loss_att 19.191963 loss_ctc 18.160667 loss_rnnt 12.768087 hw_loss 0.062300 lr 0.00045253 rank 2
2023-02-21 11:44:34,073 DEBUG TRAIN Batch 14/5300 loss 26.134869 loss_att 24.954075 loss_ctc 29.581303 loss_rnnt 25.776241 hw_loss 0.253622 lr 0.00045252 rank 0
2023-02-21 11:44:34,076 DEBUG TRAIN Batch 14/5300 loss 8.710163 loss_att 10.747780 loss_ctc 11.665892 loss_rnnt 7.841711 hw_loss 0.125310 lr 0.00045253 rank 3
2023-02-21 11:44:34,076 DEBUG TRAIN Batch 14/5300 loss 6.835921 loss_att 9.071915 loss_ctc 8.365108 loss_rnnt 6.097935 hw_loss 0.162930 lr 0.00045253 rank 4
2023-02-21 11:44:34,081 DEBUG TRAIN Batch 14/5300 loss 14.234306 loss_att 19.537369 loss_ctc 19.093498 loss_rnnt 12.494513 hw_loss 0.058670 lr 0.00045253 rank 5
2023-02-21 11:44:34,120 DEBUG TRAIN Batch 14/5300 loss 8.758281 loss_att 11.764926 loss_ctc 13.941391 loss_rnnt 7.419385 hw_loss 0.087160 lr 0.00045255 rank 6
2023-02-21 11:45:53,678 DEBUG TRAIN Batch 14/5400 loss 27.817389 loss_att 26.216345 loss_ctc 29.618612 loss_rnnt 27.836327 hw_loss 0.114578 lr 0.00045234 rank 4
2023-02-21 11:45:53,679 DEBUG TRAIN Batch 14/5400 loss 11.346301 loss_att 12.626987 loss_ctc 15.991285 loss_rnnt 10.412954 hw_loss 0.108522 lr 0.00045231 rank 7
2023-02-21 11:45:53,683 DEBUG TRAIN Batch 14/5400 loss 9.339561 loss_att 11.769844 loss_ctc 11.676726 loss_rnnt 8.456671 hw_loss 0.159771 lr 0.00045235 rank 2
2023-02-21 11:45:53,686 DEBUG TRAIN Batch 14/5400 loss 7.451654 loss_att 10.276421 loss_ctc 8.174231 loss_rnnt 6.667558 hw_loss 0.230249 lr 0.00045235 rank 3
2023-02-21 11:45:53,689 DEBUG TRAIN Batch 14/5400 loss 12.477173 loss_att 15.592024 loss_ctc 16.448265 loss_rnnt 11.286829 hw_loss 0.071050 lr 0.00045242 rank 1
2023-02-21 11:45:53,692 DEBUG TRAIN Batch 14/5400 loss 5.383659 loss_att 9.217198 loss_ctc 8.804180 loss_rnnt 4.117192 hw_loss 0.081918 lr 0.00045235 rank 5
2023-02-21 11:45:53,692 DEBUG TRAIN Batch 14/5400 loss 19.497484 loss_att 23.728941 loss_ctc 27.680996 loss_rnnt 17.448059 hw_loss 0.210001 lr 0.00045234 rank 0
2023-02-21 11:45:53,739 DEBUG TRAIN Batch 14/5400 loss 8.450993 loss_att 12.428774 loss_ctc 10.050916 loss_rnnt 7.373362 hw_loss 0.128909 lr 0.00045237 rank 6
2023-02-21 11:47:12,844 DEBUG TRAIN Batch 14/5500 loss 10.032372 loss_att 12.191225 loss_ctc 13.322592 loss_rnnt 9.107845 hw_loss 0.101362 lr 0.00045218 rank 6
2023-02-21 11:47:12,844 DEBUG TRAIN Batch 14/5500 loss 4.724305 loss_att 7.527346 loss_ctc 5.753267 loss_rnnt 3.930596 hw_loss 0.179825 lr 0.00045216 rank 4
2023-02-21 11:47:12,846 DEBUG TRAIN Batch 14/5500 loss 15.964476 loss_att 19.829193 loss_ctc 18.010960 loss_rnnt 14.867575 hw_loss 0.095801 lr 0.00045223 rank 1
2023-02-21 11:47:12,847 DEBUG TRAIN Batch 14/5500 loss 15.108741 loss_att 17.714893 loss_ctc 22.249369 loss_rnnt 13.549557 hw_loss 0.161007 lr 0.00045216 rank 2
2023-02-21 11:47:12,848 DEBUG TRAIN Batch 14/5500 loss 10.281667 loss_att 17.935848 loss_ctc 11.099310 loss_rnnt 8.585467 hw_loss 0.105645 lr 0.00045213 rank 7
2023-02-21 11:47:12,854 DEBUG TRAIN Batch 14/5500 loss 22.342283 loss_att 24.412220 loss_ctc 26.983921 loss_rnnt 21.220110 hw_loss 0.167435 lr 0.00045215 rank 0
2023-02-21 11:47:12,855 DEBUG TRAIN Batch 14/5500 loss 15.777280 loss_att 20.249083 loss_ctc 20.743279 loss_rnnt 14.115827 hw_loss 0.196797 lr 0.00045216 rank 3
2023-02-21 11:47:12,857 DEBUG TRAIN Batch 14/5500 loss 13.793947 loss_att 12.581730 loss_ctc 14.949224 loss_rnnt 13.797752 hw_loss 0.158628 lr 0.00045216 rank 5
2023-02-21 11:48:30,957 DEBUG TRAIN Batch 14/5600 loss 7.981721 loss_att 9.194567 loss_ctc 11.477106 loss_rnnt 7.207735 hw_loss 0.122561 lr 0.00045194 rank 7
2023-02-21 11:48:30,956 DEBUG TRAIN Batch 14/5600 loss 15.270767 loss_att 18.551182 loss_ctc 19.292812 loss_rnnt 14.010534 hw_loss 0.127270 lr 0.00045198 rank 2
2023-02-21 11:48:30,958 DEBUG TRAIN Batch 14/5600 loss 10.295787 loss_att 16.514158 loss_ctc 11.722475 loss_rnnt 8.715033 hw_loss 0.275352 lr 0.00045205 rank 1
2023-02-21 11:48:30,966 DEBUG TRAIN Batch 14/5600 loss 16.065006 loss_att 19.830843 loss_ctc 22.225216 loss_rnnt 14.410764 hw_loss 0.149459 lr 0.00045197 rank 0
2023-02-21 11:48:30,967 DEBUG TRAIN Batch 14/5600 loss 8.812443 loss_att 10.261138 loss_ctc 11.756036 loss_rnnt 8.062143 hw_loss 0.127654 lr 0.00045197 rank 4
2023-02-21 11:48:30,969 DEBUG TRAIN Batch 14/5600 loss 9.802462 loss_att 11.775620 loss_ctc 15.597111 loss_rnnt 8.617510 hw_loss 0.033189 lr 0.00045198 rank 3
2023-02-21 11:48:30,985 DEBUG TRAIN Batch 14/5600 loss 14.172282 loss_att 15.519437 loss_ctc 21.733040 loss_rnnt 12.818476 hw_loss 0.143016 lr 0.00045200 rank 6
2023-02-21 11:48:30,999 DEBUG TRAIN Batch 14/5600 loss 10.715226 loss_att 14.595001 loss_ctc 16.748318 loss_rnnt 9.106668 hw_loss 0.052858 lr 0.00045198 rank 5
2023-02-21 11:49:52,399 DEBUG TRAIN Batch 14/5700 loss 10.992292 loss_att 13.468204 loss_ctc 15.231720 loss_rnnt 9.835882 hw_loss 0.179946 lr 0.00045181 rank 6
2023-02-21 11:49:52,400 DEBUG TRAIN Batch 14/5700 loss 9.059409 loss_att 10.771873 loss_ctc 12.346236 loss_rnnt 8.203307 hw_loss 0.141312 lr 0.00045179 rank 2
2023-02-21 11:49:52,405 DEBUG TRAIN Batch 14/5700 loss 12.151323 loss_att 12.391717 loss_ctc 15.278955 loss_rnnt 11.533177 hw_loss 0.286969 lr 0.00045178 rank 0
2023-02-21 11:49:52,406 DEBUG TRAIN Batch 14/5700 loss 16.300665 loss_att 18.931606 loss_ctc 17.657461 loss_rnnt 15.473705 hw_loss 0.224750 lr 0.00045176 rank 7
2023-02-21 11:49:52,405 DEBUG TRAIN Batch 14/5700 loss 14.541131 loss_att 14.630091 loss_ctc 20.865170 loss_rnnt 13.619012 hw_loss 0.114600 lr 0.00045186 rank 1
2023-02-21 11:49:52,406 DEBUG TRAIN Batch 14/5700 loss 8.549075 loss_att 9.714551 loss_ctc 10.990483 loss_rnnt 7.807072 hw_loss 0.343849 lr 0.00045179 rank 4
2023-02-21 11:49:52,409 DEBUG TRAIN Batch 14/5700 loss 10.161081 loss_att 13.259413 loss_ctc 14.209773 loss_rnnt 8.947060 hw_loss 0.102242 lr 0.00045179 rank 5
2023-02-21 11:49:52,412 DEBUG TRAIN Batch 14/5700 loss 15.076089 loss_att 18.132528 loss_ctc 24.524485 loss_rnnt 13.160330 hw_loss 0.083786 lr 0.00045179 rank 3
2023-02-21 11:51:11,190 DEBUG TRAIN Batch 14/5800 loss 20.316822 loss_att 23.842781 loss_ctc 26.079170 loss_rnnt 18.769051 hw_loss 0.139244 lr 0.00045157 rank 7
2023-02-21 11:51:11,191 DEBUG TRAIN Batch 14/5800 loss 5.834983 loss_att 9.878153 loss_ctc 10.395142 loss_rnnt 4.333971 hw_loss 0.158172 lr 0.00045163 rank 6
2023-02-21 11:51:11,193 DEBUG TRAIN Batch 14/5800 loss 2.400606 loss_att 6.358407 loss_ctc 6.967720 loss_rnnt 0.914604 hw_loss 0.160299 lr 0.00045160 rank 4
2023-02-21 11:51:11,195 DEBUG TRAIN Batch 14/5800 loss 7.618883 loss_att 10.571578 loss_ctc 12.867256 loss_rnnt 6.288948 hw_loss 0.074276 lr 0.00045161 rank 5
2023-02-21 11:51:11,195 DEBUG TRAIN Batch 14/5800 loss 9.310759 loss_att 12.519888 loss_ctc 11.842392 loss_rnnt 8.297970 hw_loss 0.062647 lr 0.00045168 rank 1
2023-02-21 11:51:11,196 DEBUG TRAIN Batch 14/5800 loss 13.164534 loss_att 13.452514 loss_ctc 16.212559 loss_rnnt 12.530127 hw_loss 0.319515 lr 0.00045161 rank 3
2023-02-21 11:51:11,198 DEBUG TRAIN Batch 14/5800 loss 6.784632 loss_att 10.517378 loss_ctc 8.631871 loss_rnnt 5.766422 hw_loss 0.047554 lr 0.00045161 rank 2
2023-02-21 11:51:11,241 DEBUG TRAIN Batch 14/5800 loss 7.427453 loss_att 10.539838 loss_ctc 9.764093 loss_rnnt 6.410644 hw_loss 0.155213 lr 0.00045160 rank 0
2023-02-21 11:52:30,034 DEBUG TRAIN Batch 14/5900 loss 18.556721 loss_att 21.401766 loss_ctc 26.058468 loss_rnnt 16.973715 hw_loss 0.025811 lr 0.00045142 rank 4
2023-02-21 11:52:30,038 DEBUG TRAIN Batch 14/5900 loss 9.191084 loss_att 12.189739 loss_ctc 9.819292 loss_rnnt 8.403271 hw_loss 0.195600 lr 0.00045142 rank 2
2023-02-21 11:52:30,040 DEBUG TRAIN Batch 14/5900 loss 6.879287 loss_att 12.358714 loss_ctc 11.066889 loss_rnnt 5.148480 hw_loss 0.143575 lr 0.00045139 rank 7
2023-02-21 11:52:30,040 DEBUG TRAIN Batch 14/5900 loss 12.832474 loss_att 17.335991 loss_ctc 15.362131 loss_rnnt 11.566454 hw_loss 0.052552 lr 0.00045142 rank 5
2023-02-21 11:52:30,040 DEBUG TRAIN Batch 14/5900 loss 17.926487 loss_att 15.936125 loss_ctc 22.645554 loss_rnnt 17.641006 hw_loss 0.101896 lr 0.00045142 rank 0
2023-02-21 11:52:30,043 DEBUG TRAIN Batch 14/5900 loss 6.702540 loss_att 9.004395 loss_ctc 9.669552 loss_rnnt 5.796058 hw_loss 0.094706 lr 0.00045149 rank 1
2023-02-21 11:52:30,045 DEBUG TRAIN Batch 14/5900 loss 6.724761 loss_att 13.113417 loss_ctc 5.825336 loss_rnnt 5.506889 hw_loss 0.112621 lr 0.00045143 rank 3
2023-02-21 11:52:30,090 DEBUG TRAIN Batch 14/5900 loss 3.564940 loss_att 9.002723 loss_ctc 7.135413 loss_rnnt 1.959209 hw_loss 0.078959 lr 0.00045145 rank 6
2023-02-21 11:53:50,887 DEBUG TRAIN Batch 14/6000 loss 15.437453 loss_att 17.235603 loss_ctc 19.158909 loss_rnnt 14.520119 hw_loss 0.115332 lr 0.00045120 rank 7
2023-02-21 11:53:50,888 DEBUG TRAIN Batch 14/6000 loss 10.362592 loss_att 15.061543 loss_ctc 14.449272 loss_rnnt 8.863592 hw_loss 0.026848 lr 0.00045124 rank 4
2023-02-21 11:53:50,888 DEBUG TRAIN Batch 14/6000 loss 16.103064 loss_att 19.599997 loss_ctc 17.949755 loss_rnnt 15.079098 hw_loss 0.146912 lr 0.00045124 rank 2
2023-02-21 11:53:50,889 DEBUG TRAIN Batch 14/6000 loss 11.453284 loss_att 15.827000 loss_ctc 15.298807 loss_rnnt 10.030767 hw_loss 0.065695 lr 0.00045126 rank 6
2023-02-21 11:53:50,891 DEBUG TRAIN Batch 14/6000 loss 6.348420 loss_att 11.085363 loss_ctc 12.678408 loss_rnnt 4.419169 hw_loss 0.258494 lr 0.00045131 rank 1
2023-02-21 11:53:50,892 DEBUG TRAIN Batch 14/6000 loss 3.365586 loss_att 7.074409 loss_ctc 4.879369 loss_rnnt 2.407307 hw_loss 0.027517 lr 0.00045124 rank 3
2023-02-21 11:53:50,892 DEBUG TRAIN Batch 14/6000 loss 19.348850 loss_att 20.692707 loss_ctc 29.616600 loss_rnnt 17.635612 hw_loss 0.141434 lr 0.00045124 rank 5
2023-02-21 11:53:50,898 DEBUG TRAIN Batch 14/6000 loss 6.449933 loss_att 9.285320 loss_ctc 11.783802 loss_rnnt 5.131335 hw_loss 0.075633 lr 0.00045123 rank 0
2023-02-21 11:55:10,397 DEBUG TRAIN Batch 14/6100 loss 22.785448 loss_att 27.656174 loss_ctc 31.875206 loss_rnnt 20.567667 hw_loss 0.059377 lr 0.00045105 rank 4
2023-02-21 11:55:10,397 DEBUG TRAIN Batch 14/6100 loss 13.492875 loss_att 21.580841 loss_ctc 24.687054 loss_rnnt 10.352007 hw_loss 0.057595 lr 0.00045108 rank 6
2023-02-21 11:55:10,398 DEBUG TRAIN Batch 14/6100 loss 21.016918 loss_att 25.187370 loss_ctc 32.487675 loss_rnnt 18.535225 hw_loss 0.221567 lr 0.00045102 rank 7
2023-02-21 11:55:10,400 DEBUG TRAIN Batch 14/6100 loss 12.292245 loss_att 15.434948 loss_ctc 14.940279 loss_rnnt 11.276904 hw_loss 0.063241 lr 0.00045106 rank 5
2023-02-21 11:55:10,402 DEBUG TRAIN Batch 14/6100 loss 6.285228 loss_att 11.028167 loss_ctc 11.237928 loss_rnnt 4.647608 hw_loss 0.053760 lr 0.00045106 rank 2
2023-02-21 11:55:10,404 DEBUG TRAIN Batch 14/6100 loss 11.478130 loss_att 14.481224 loss_ctc 14.399044 loss_rnnt 10.472029 hw_loss 0.030049 lr 0.00045105 rank 0
2023-02-21 11:55:10,404 DEBUG TRAIN Batch 14/6100 loss 10.521635 loss_att 11.552616 loss_ctc 11.763138 loss_rnnt 10.074638 hw_loss 0.141124 lr 0.00045112 rank 1
2023-02-21 11:55:10,450 DEBUG TRAIN Batch 14/6100 loss 19.565985 loss_att 23.015705 loss_ctc 29.574375 loss_rnnt 17.457661 hw_loss 0.157367 lr 0.00045106 rank 3
2023-02-21 11:56:29,059 DEBUG TRAIN Batch 14/6200 loss 17.400852 loss_att 20.604855 loss_ctc 20.381721 loss_rnnt 16.181833 hw_loss 0.338942 lr 0.00045087 rank 2
2023-02-21 11:56:29,061 DEBUG TRAIN Batch 14/6200 loss 11.447831 loss_att 13.649596 loss_ctc 14.825389 loss_rnnt 10.507755 hw_loss 0.092591 lr 0.00045090 rank 6
2023-02-21 11:56:29,063 DEBUG TRAIN Batch 14/6200 loss 13.367547 loss_att 14.816582 loss_ctc 15.867417 loss_rnnt 12.724367 hw_loss 0.037606 lr 0.00045087 rank 4
2023-02-21 11:56:29,065 DEBUG TRAIN Batch 14/6200 loss 18.902588 loss_att 22.449303 loss_ctc 29.473621 loss_rnnt 16.673283 hw_loss 0.207171 lr 0.00045094 rank 1
2023-02-21 11:56:29,065 DEBUG TRAIN Batch 14/6200 loss 11.931102 loss_att 15.574759 loss_ctc 14.019387 loss_rnnt 10.817127 hw_loss 0.200260 lr 0.00045086 rank 0
2023-02-21 11:56:29,067 DEBUG TRAIN Batch 14/6200 loss 8.743841 loss_att 11.143147 loss_ctc 12.720294 loss_rnnt 7.650050 hw_loss 0.157006 lr 0.00045084 rank 7
2023-02-21 11:56:29,066 DEBUG TRAIN Batch 14/6200 loss 13.393254 loss_att 17.323723 loss_ctc 20.326902 loss_rnnt 11.601550 hw_loss 0.152109 lr 0.00045088 rank 3
2023-02-21 11:56:29,071 DEBUG TRAIN Batch 14/6200 loss 4.982078 loss_att 5.977887 loss_ctc 5.654573 loss_rnnt 4.631650 hw_loss 0.115500 lr 0.00045087 rank 5
2023-02-21 11:57:47,403 DEBUG TRAIN Batch 14/6300 loss 9.713490 loss_att 13.446422 loss_ctc 14.686275 loss_rnnt 8.284156 hw_loss 0.036957 lr 0.00045065 rank 7
2023-02-21 11:57:47,409 DEBUG TRAIN Batch 14/6300 loss 18.462437 loss_att 20.088749 loss_ctc 22.767740 loss_rnnt 17.415886 hw_loss 0.276087 lr 0.00045069 rank 4
2023-02-21 11:57:47,411 DEBUG TRAIN Batch 14/6300 loss 10.965429 loss_att 11.414256 loss_ctc 13.870222 loss_rnnt 10.400005 hw_loss 0.165662 lr 0.00045076 rank 1
2023-02-21 11:57:47,413 DEBUG TRAIN Batch 14/6300 loss 9.071233 loss_att 9.705159 loss_ctc 11.993028 loss_rnnt 8.456260 hw_loss 0.184903 lr 0.00045069 rank 2
2023-02-21 11:57:47,414 DEBUG TRAIN Batch 14/6300 loss 14.205162 loss_att 15.512230 loss_ctc 16.172966 loss_rnnt 13.583501 hw_loss 0.183514 lr 0.00045068 rank 0
2023-02-21 11:57:47,418 DEBUG TRAIN Batch 14/6300 loss 15.331108 loss_att 18.154404 loss_ctc 19.937559 loss_rnnt 14.092951 hw_loss 0.111194 lr 0.00045071 rank 6
2023-02-21 11:57:47,420 DEBUG TRAIN Batch 14/6300 loss 20.002626 loss_att 21.342682 loss_ctc 23.953941 loss_rnnt 19.089870 hw_loss 0.221065 lr 0.00045069 rank 3
2023-02-21 11:57:47,420 DEBUG TRAIN Batch 14/6300 loss 12.911206 loss_att 14.093716 loss_ctc 15.798137 loss_rnnt 12.223208 hw_loss 0.124822 lr 0.00045069 rank 5
2023-02-21 11:59:09,459 DEBUG TRAIN Batch 14/6400 loss 21.062042 loss_att 25.642153 loss_ctc 28.675644 loss_rnnt 19.090820 hw_loss 0.075098 lr 0.00045050 rank 4
2023-02-21 11:59:09,462 DEBUG TRAIN Batch 14/6400 loss 13.764647 loss_att 16.054937 loss_ctc 12.952793 loss_rnnt 13.373560 hw_loss 0.077392 lr 0.00045057 rank 1
2023-02-21 11:59:09,463 DEBUG TRAIN Batch 14/6400 loss 12.137787 loss_att 16.261517 loss_ctc 18.859966 loss_rnnt 10.307824 hw_loss 0.204235 lr 0.00045047 rank 7
2023-02-21 11:59:09,464 DEBUG TRAIN Batch 14/6400 loss 10.671719 loss_att 14.694792 loss_ctc 19.545010 loss_rnnt 8.658648 hw_loss 0.047532 lr 0.00045053 rank 6
2023-02-21 11:59:09,464 DEBUG TRAIN Batch 14/6400 loss 13.512898 loss_att 17.395281 loss_ctc 22.184307 loss_rnnt 11.447795 hw_loss 0.248324 lr 0.00045051 rank 2
2023-02-21 11:59:09,467 DEBUG TRAIN Batch 14/6400 loss 15.402983 loss_att 17.787107 loss_ctc 17.453823 loss_rnnt 14.570586 hw_loss 0.153988 lr 0.00045051 rank 5
2023-02-21 11:59:09,468 DEBUG TRAIN Batch 14/6400 loss 15.211412 loss_att 20.747166 loss_ctc 16.348890 loss_rnnt 13.937548 hw_loss 0.028219 lr 0.00045050 rank 0
2023-02-21 11:59:09,473 DEBUG TRAIN Batch 14/6400 loss 16.243984 loss_att 19.665672 loss_ctc 21.978880 loss_rnnt 14.713007 hw_loss 0.153724 lr 0.00045051 rank 3
2023-02-21 12:00:27,223 DEBUG TRAIN Batch 14/6500 loss 8.689362 loss_att 10.350323 loss_ctc 10.024082 loss_rnnt 8.162965 hw_loss 0.030453 lr 0.00045032 rank 2
2023-02-21 12:00:27,226 DEBUG TRAIN Batch 14/6500 loss 15.012624 loss_att 19.165977 loss_ctc 20.680975 loss_rnnt 13.373348 hw_loss 0.099047 lr 0.00045029 rank 7
2023-02-21 12:00:27,229 DEBUG TRAIN Batch 14/6500 loss 5.811780 loss_att 9.665684 loss_ctc 6.531419 loss_rnnt 4.918291 hw_loss 0.050169 lr 0.00045035 rank 6
2023-02-21 12:00:27,229 DEBUG TRAIN Batch 14/6500 loss 7.270904 loss_att 9.707140 loss_ctc 10.273916 loss_rnnt 6.367225 hw_loss 0.030059 lr 0.00045032 rank 4
2023-02-21 12:00:27,231 DEBUG TRAIN Batch 14/6500 loss 9.665849 loss_att 13.830515 loss_ctc 12.804828 loss_rnnt 8.337562 hw_loss 0.144045 lr 0.00045032 rank 5
2023-02-21 12:00:27,231 DEBUG TRAIN Batch 14/6500 loss 14.896411 loss_att 19.485893 loss_ctc 20.862522 loss_rnnt 13.128119 hw_loss 0.102966 lr 0.00045039 rank 1
2023-02-21 12:00:27,233 DEBUG TRAIN Batch 14/6500 loss 7.907902 loss_att 10.047732 loss_ctc 9.103107 loss_rnnt 7.231727 hw_loss 0.166590 lr 0.00045032 rank 0
2023-02-21 12:00:27,236 DEBUG TRAIN Batch 14/6500 loss 12.090293 loss_att 12.893894 loss_ctc 12.815094 loss_rnnt 11.731810 hw_loss 0.189607 lr 0.00045033 rank 3
2023-02-21 12:01:45,208 DEBUG TRAIN Batch 14/6600 loss 12.784325 loss_att 15.276311 loss_ctc 16.471947 loss_rnnt 11.748416 hw_loss 0.085928 lr 0.00045016 rank 6
2023-02-21 12:01:45,209 DEBUG TRAIN Batch 14/6600 loss 8.906359 loss_att 11.138371 loss_ctc 10.242888 loss_rnnt 8.219357 hw_loss 0.116993 lr 0.00045011 rank 7
2023-02-21 12:01:45,212 DEBUG TRAIN Batch 14/6600 loss 17.734877 loss_att 19.457371 loss_ctc 23.935516 loss_rnnt 16.549444 hw_loss 0.026593 lr 0.00045021 rank 1
2023-02-21 12:01:45,213 DEBUG TRAIN Batch 14/6600 loss 11.121432 loss_att 12.597216 loss_ctc 16.611876 loss_rnnt 10.043049 hw_loss 0.095940 lr 0.00045014 rank 4
2023-02-21 12:01:45,214 DEBUG TRAIN Batch 14/6600 loss 8.314626 loss_att 12.309202 loss_ctc 8.981807 loss_rnnt 7.400765 hw_loss 0.048725 lr 0.00045014 rank 2
2023-02-21 12:01:45,219 DEBUG TRAIN Batch 14/6600 loss 19.262142 loss_att 24.812143 loss_ctc 22.655037 loss_rnnt 17.669891 hw_loss 0.055997 lr 0.00045013 rank 0
2023-02-21 12:01:45,222 DEBUG TRAIN Batch 14/6600 loss 15.937633 loss_att 20.439922 loss_ctc 18.707825 loss_rnnt 14.578843 hw_loss 0.166821 lr 0.00045014 rank 5
2023-02-21 12:01:45,259 DEBUG TRAIN Batch 14/6600 loss 9.640489 loss_att 14.259806 loss_ctc 11.591842 loss_rnnt 8.348871 hw_loss 0.201701 lr 0.00045014 rank 3
2023-02-21 12:03:04,593 DEBUG TRAIN Batch 14/6700 loss 5.747433 loss_att 9.319663 loss_ctc 8.027670 loss_rnnt 4.685705 hw_loss 0.081097 lr 0.00044992 rank 7
2023-02-21 12:03:04,594 DEBUG TRAIN Batch 14/6700 loss 11.060836 loss_att 14.041496 loss_ctc 15.882822 loss_rnnt 9.696791 hw_loss 0.234339 lr 0.00044996 rank 2
2023-02-21 12:03:04,595 DEBUG TRAIN Batch 14/6700 loss 10.677360 loss_att 18.501074 loss_ctc 12.945790 loss_rnnt 8.774117 hw_loss 0.067579 lr 0.00045003 rank 1
2023-02-21 12:03:04,599 DEBUG TRAIN Batch 14/6700 loss 12.289682 loss_att 17.723953 loss_ctc 17.408611 loss_rnnt 10.424320 hw_loss 0.179970 lr 0.00044996 rank 3
2023-02-21 12:03:04,599 DEBUG TRAIN Batch 14/6700 loss 22.638264 loss_att 24.465595 loss_ctc 29.269438 loss_rnnt 21.343328 hw_loss 0.084958 lr 0.00044995 rank 0
2023-02-21 12:03:04,599 DEBUG TRAIN Batch 14/6700 loss 18.686523 loss_att 24.388725 loss_ctc 22.656097 loss_rnnt 16.976797 hw_loss 0.075015 lr 0.00044998 rank 6
2023-02-21 12:03:04,600 DEBUG TRAIN Batch 14/6700 loss 13.817555 loss_att 15.989368 loss_ctc 17.498791 loss_rnnt 12.842596 hw_loss 0.093311 lr 0.00044996 rank 4
2023-02-21 12:03:04,616 DEBUG TRAIN Batch 14/6700 loss 6.417597 loss_att 9.917892 loss_ctc 8.526124 loss_rnnt 5.357941 hw_loss 0.147113 lr 0.00044996 rank 5
2023-02-21 12:04:24,601 DEBUG TRAIN Batch 14/6800 loss 11.535780 loss_att 13.709409 loss_ctc 12.425720 loss_rnnt 10.939406 hw_loss 0.080605 lr 0.00044974 rank 7
2023-02-21 12:04:24,607 DEBUG TRAIN Batch 14/6800 loss 6.438546 loss_att 12.131202 loss_ctc 9.131119 loss_rnnt 4.889417 hw_loss 0.096729 lr 0.00044977 rank 4
2023-02-21 12:04:24,610 DEBUG TRAIN Batch 14/6800 loss 3.640555 loss_att 6.657732 loss_ctc 4.132763 loss_rnnt 2.865799 hw_loss 0.198176 lr 0.00044984 rank 1
2023-02-21 12:04:24,612 DEBUG TRAIN Batch 14/6800 loss 7.740800 loss_att 9.465487 loss_ctc 7.749660 loss_rnnt 7.328661 hw_loss 0.123787 lr 0.00044978 rank 2
2023-02-21 12:04:24,613 DEBUG TRAIN Batch 14/6800 loss 18.944931 loss_att 22.170330 loss_ctc 32.596916 loss_rnnt 16.405283 hw_loss 0.139319 lr 0.00044980 rank 6
2023-02-21 12:04:24,615 DEBUG TRAIN Batch 14/6800 loss 11.001548 loss_att 12.885475 loss_ctc 13.059791 loss_rnnt 10.252027 hw_loss 0.184319 lr 0.00044978 rank 5
2023-02-21 12:04:24,614 DEBUG TRAIN Batch 14/6800 loss 18.461107 loss_att 21.323315 loss_ctc 22.761299 loss_rnnt 17.206512 hw_loss 0.203991 lr 0.00044978 rank 3
2023-02-21 12:04:24,615 DEBUG TRAIN Batch 14/6800 loss 13.701183 loss_att 15.303408 loss_ctc 15.921771 loss_rnnt 13.027897 hw_loss 0.106431 lr 0.00044977 rank 0
2023-02-21 12:05:43,814 DEBUG TRAIN Batch 14/6900 loss 13.281003 loss_att 15.770461 loss_ctc 17.557415 loss_rnnt 12.169561 hw_loss 0.081305 lr 0.00044959 rank 4
2023-02-21 12:05:43,816 DEBUG TRAIN Batch 14/6900 loss 14.744934 loss_att 16.985020 loss_ctc 15.359065 loss_rnnt 14.146808 hw_loss 0.127922 lr 0.00044966 rank 1
2023-02-21 12:05:43,820 DEBUG TRAIN Batch 14/6900 loss 8.210430 loss_att 10.937202 loss_ctc 12.031727 loss_rnnt 7.137300 hw_loss 0.034256 lr 0.00044960 rank 3
2023-02-21 12:05:43,822 DEBUG TRAIN Batch 14/6900 loss 16.764063 loss_att 19.164907 loss_ctc 21.346985 loss_rnnt 15.588306 hw_loss 0.158498 lr 0.00044956 rank 7
2023-02-21 12:05:43,823 DEBUG TRAIN Batch 14/6900 loss 15.337366 loss_att 17.644024 loss_ctc 23.779104 loss_rnnt 13.705792 hw_loss 0.083768 lr 0.00044960 rank 2
2023-02-21 12:05:43,825 DEBUG TRAIN Batch 14/6900 loss 10.822007 loss_att 13.962647 loss_ctc 14.289927 loss_rnnt 9.625574 hw_loss 0.198592 lr 0.00044962 rank 6
2023-02-21 12:05:43,826 DEBUG TRAIN Batch 14/6900 loss 18.352837 loss_att 21.786530 loss_ctc 25.667114 loss_rnnt 16.649532 hw_loss 0.077492 lr 0.00044959 rank 0
2023-02-21 12:05:43,871 DEBUG TRAIN Batch 14/6900 loss 6.593871 loss_att 6.935342 loss_ctc 9.033468 loss_rnnt 6.081650 hw_loss 0.222461 lr 0.00044960 rank 5
2023-02-21 12:07:02,557 DEBUG TRAIN Batch 14/7000 loss 11.095741 loss_att 11.737263 loss_ctc 16.020493 loss_rnnt 10.216519 hw_loss 0.176783 lr 0.00044941 rank 4
2023-02-21 12:07:02,560 DEBUG TRAIN Batch 14/7000 loss 23.102167 loss_att 22.407679 loss_ctc 27.238461 loss_rnnt 22.534897 hw_loss 0.289990 lr 0.00044941 rank 2
2023-02-21 12:07:02,562 DEBUG TRAIN Batch 14/7000 loss 2.679336 loss_att 6.325563 loss_ctc 3.182483 loss_rnnt 1.709205 hw_loss 0.325873 lr 0.00044938 rank 7
2023-02-21 12:07:02,563 DEBUG TRAIN Batch 14/7000 loss 13.888749 loss_att 18.950899 loss_ctc 15.787596 loss_rnnt 12.507743 hw_loss 0.216369 lr 0.00044940 rank 0
2023-02-21 12:07:02,564 DEBUG TRAIN Batch 14/7000 loss 10.216627 loss_att 10.407893 loss_ctc 11.942313 loss_rnnt 9.738442 hw_loss 0.393450 lr 0.00044948 rank 1
2023-02-21 12:07:02,567 DEBUG TRAIN Batch 14/7000 loss 20.037861 loss_att 20.159731 loss_ctc 21.581799 loss_rnnt 19.693493 hw_loss 0.214003 lr 0.00044942 rank 3
2023-02-21 12:07:02,573 DEBUG TRAIN Batch 14/7000 loss 3.739727 loss_att 9.637339 loss_ctc 5.164384 loss_rnnt 2.319713 hw_loss 0.094760 lr 0.00044941 rank 5
2023-02-21 12:07:02,615 DEBUG TRAIN Batch 14/7000 loss 14.906626 loss_att 20.226059 loss_ctc 22.418516 loss_rnnt 12.733965 hw_loss 0.200978 lr 0.00044944 rank 6
2023-02-21 12:08:23,112 DEBUG TRAIN Batch 14/7100 loss 7.150228 loss_att 13.142605 loss_ctc 11.392207 loss_rnnt 5.320143 hw_loss 0.123772 lr 0.00044920 rank 7
2023-02-21 12:08:23,113 DEBUG TRAIN Batch 14/7100 loss 9.810595 loss_att 13.260600 loss_ctc 15.219807 loss_rnnt 8.330920 hw_loss 0.128333 lr 0.00044923 rank 2
2023-02-21 12:08:23,113 DEBUG TRAIN Batch 14/7100 loss 6.314069 loss_att 10.651063 loss_ctc 9.115958 loss_rnnt 5.038681 hw_loss 0.064506 lr 0.00044930 rank 1
2023-02-21 12:08:23,115 DEBUG TRAIN Batch 14/7100 loss 8.606194 loss_att 10.960825 loss_ctc 9.838205 loss_rnnt 7.925397 hw_loss 0.085507 lr 0.00044925 rank 6
2023-02-21 12:08:23,116 DEBUG TRAIN Batch 14/7100 loss 20.664156 loss_att 20.009335 loss_ctc 24.681545 loss_rnnt 20.172590 hw_loss 0.162896 lr 0.00044923 rank 3
2023-02-21 12:08:23,120 DEBUG TRAIN Batch 14/7100 loss 10.895565 loss_att 14.235133 loss_ctc 12.385622 loss_rnnt 9.994307 hw_loss 0.065007 lr 0.00044923 rank 5
2023-02-21 12:08:23,136 DEBUG TRAIN Batch 14/7100 loss 5.944051 loss_att 7.787982 loss_ctc 6.248020 loss_rnnt 5.462204 hw_loss 0.135998 lr 0.00044923 rank 4
2023-02-21 12:08:23,163 DEBUG TRAIN Batch 14/7100 loss 10.576780 loss_att 16.071943 loss_ctc 12.652735 loss_rnnt 9.120880 hw_loss 0.150139 lr 0.00044922 rank 0
2023-02-21 12:09:41,984 DEBUG TRAIN Batch 14/7200 loss 9.852220 loss_att 14.869695 loss_ctc 11.462001 loss_rnnt 8.586851 hw_loss 0.088567 lr 0.00044905 rank 4
2023-02-21 12:09:41,986 DEBUG TRAIN Batch 14/7200 loss 22.628263 loss_att 23.808109 loss_ctc 28.627640 loss_rnnt 21.445400 hw_loss 0.275584 lr 0.00044907 rank 6
2023-02-21 12:09:41,991 DEBUG TRAIN Batch 14/7200 loss 13.194088 loss_att 16.649408 loss_ctc 16.401440 loss_rnnt 11.996382 hw_loss 0.148116 lr 0.00044902 rank 7
2023-02-21 12:09:41,992 DEBUG TRAIN Batch 14/7200 loss 14.201697 loss_att 21.303846 loss_ctc 23.767103 loss_rnnt 11.461607 hw_loss 0.083011 lr 0.00044904 rank 0
2023-02-21 12:09:41,993 DEBUG TRAIN Batch 14/7200 loss 10.772470 loss_att 13.800102 loss_ctc 12.228251 loss_rnnt 9.926659 hw_loss 0.086589 lr 0.00044905 rank 2
2023-02-21 12:09:41,995 DEBUG TRAIN Batch 14/7200 loss 13.919004 loss_att 14.976706 loss_ctc 18.567196 loss_rnnt 13.018115 hw_loss 0.130482 lr 0.00044912 rank 1
2023-02-21 12:09:41,997 DEBUG TRAIN Batch 14/7200 loss 3.483826 loss_att 9.097389 loss_ctc 3.607050 loss_rnnt 2.236876 hw_loss 0.202140 lr 0.00044905 rank 3
2023-02-21 12:09:42,045 DEBUG TRAIN Batch 14/7200 loss 11.735540 loss_att 14.361371 loss_ctc 16.284283 loss_rnnt 10.499126 hw_loss 0.196405 lr 0.00044905 rank 5
2023-02-21 12:10:59,890 DEBUG TRAIN Batch 14/7300 loss 10.175879 loss_att 11.119937 loss_ctc 11.900444 loss_rnnt 9.661785 hw_loss 0.178765 lr 0.00044883 rank 7
2023-02-21 12:10:59,890 DEBUG TRAIN Batch 14/7300 loss 2.936041 loss_att 6.524714 loss_ctc 6.597427 loss_rnnt 1.639234 hw_loss 0.170414 lr 0.00044887 rank 4
2023-02-21 12:10:59,894 DEBUG TRAIN Batch 14/7300 loss 22.377508 loss_att 22.586231 loss_ctc 30.585773 loss_rnnt 21.140997 hw_loss 0.188122 lr 0.00044894 rank 1
2023-02-21 12:10:59,895 DEBUG TRAIN Batch 14/7300 loss 12.025131 loss_att 14.697087 loss_ctc 18.355919 loss_rnnt 10.574409 hw_loss 0.135421 lr 0.00044887 rank 2
2023-02-21 12:10:59,900 DEBUG TRAIN Batch 14/7300 loss 8.003892 loss_att 10.835394 loss_ctc 8.499938 loss_rnnt 7.355923 hw_loss 0.029118 lr 0.00044886 rank 0
2023-02-21 12:10:59,901 DEBUG TRAIN Batch 14/7300 loss 13.380136 loss_att 14.559376 loss_ctc 13.825774 loss_rnnt 12.994165 hw_loss 0.170068 lr 0.00044889 rank 6
2023-02-21 12:10:59,925 DEBUG TRAIN Batch 14/7300 loss 20.363302 loss_att 17.833197 loss_ctc 19.770250 loss_rnnt 20.919964 hw_loss 0.053310 lr 0.00044887 rank 3
2023-02-21 12:10:59,936 DEBUG TRAIN Batch 14/7300 loss 7.242406 loss_att 11.659327 loss_ctc 9.071625 loss_rnnt 6.055345 hw_loss 0.112089 lr 0.00044887 rank 5
2023-02-21 12:12:18,648 DEBUG TRAIN Batch 14/7400 loss 9.978363 loss_att 11.433438 loss_ctc 14.710286 loss_rnnt 9.017389 hw_loss 0.073192 lr 0.00044868 rank 0
2023-02-21 12:12:18,649 DEBUG TRAIN Batch 14/7400 loss 13.454738 loss_att 15.921507 loss_ctc 14.172932 loss_rnnt 12.831244 hw_loss 0.064467 lr 0.00044871 rank 6
2023-02-21 12:12:18,650 DEBUG TRAIN Batch 14/7400 loss 12.905052 loss_att 14.875164 loss_ctc 15.708638 loss_rnnt 12.120611 hw_loss 0.031140 lr 0.00044876 rank 1
2023-02-21 12:12:18,652 DEBUG TRAIN Batch 14/7400 loss 11.104537 loss_att 13.437531 loss_ctc 11.623387 loss_rnnt 10.528822 hw_loss 0.074881 lr 0.00044869 rank 3
2023-02-21 12:12:18,653 DEBUG TRAIN Batch 14/7400 loss 14.923345 loss_att 18.006721 loss_ctc 18.194496 loss_rnnt 13.828966 hw_loss 0.077903 lr 0.00044865 rank 7
2023-02-21 12:12:18,653 DEBUG TRAIN Batch 14/7400 loss 8.533240 loss_att 14.192131 loss_ctc 11.508603 loss_rnnt 6.927719 hw_loss 0.144430 lr 0.00044869 rank 4
2023-02-21 12:12:18,657 DEBUG TRAIN Batch 14/7400 loss 13.176795 loss_att 13.763803 loss_ctc 15.528834 loss_rnnt 12.729327 hw_loss 0.030865 lr 0.00044869 rank 2
2023-02-21 12:12:18,700 DEBUG TRAIN Batch 14/7400 loss 13.368312 loss_att 16.803839 loss_ctc 21.106493 loss_rnnt 11.603160 hw_loss 0.086790 lr 0.00044869 rank 5
2023-02-21 12:13:39,328 DEBUG TRAIN Batch 14/7500 loss 9.278060 loss_att 12.960133 loss_ctc 13.591805 loss_rnnt 7.878107 hw_loss 0.165700 lr 0.00044851 rank 4
2023-02-21 12:13:39,337 DEBUG TRAIN Batch 14/7500 loss 14.243353 loss_att 18.364588 loss_ctc 19.798792 loss_rnnt 12.658986 hw_loss 0.036364 lr 0.00044858 rank 1
2023-02-21 12:13:39,339 DEBUG TRAIN Batch 14/7500 loss 9.442928 loss_att 11.746010 loss_ctc 11.701641 loss_rnnt 8.509251 hw_loss 0.322311 lr 0.00044850 rank 0
2023-02-21 12:13:39,340 DEBUG TRAIN Batch 14/7500 loss 3.608467 loss_att 6.929472 loss_ctc 5.150478 loss_rnnt 2.710605 hw_loss 0.052611 lr 0.00044851 rank 2
2023-02-21 12:13:39,345 DEBUG TRAIN Batch 14/7500 loss 13.261382 loss_att 14.332344 loss_ctc 16.542698 loss_rnnt 12.504545 hw_loss 0.197129 lr 0.00044847 rank 7
2023-02-21 12:13:39,345 DEBUG TRAIN Batch 14/7500 loss 16.572996 loss_att 20.085716 loss_ctc 23.592876 loss_rnnt 14.777374 hw_loss 0.294554 lr 0.00044851 rank 3
2023-02-21 12:13:39,352 DEBUG TRAIN Batch 14/7500 loss 15.337894 loss_att 17.399273 loss_ctc 19.476238 loss_rnnt 14.325175 hw_loss 0.091246 lr 0.00044851 rank 5
2023-02-21 12:13:39,395 DEBUG TRAIN Batch 14/7500 loss 10.288638 loss_att 13.091955 loss_ctc 14.615491 loss_rnnt 9.027033 hw_loss 0.232551 lr 0.00044853 rank 6
2023-02-21 12:14:58,090 DEBUG TRAIN Batch 14/7600 loss 10.690061 loss_att 12.232857 loss_ctc 13.153750 loss_rnnt 9.998128 hw_loss 0.102902 lr 0.00044833 rank 2
2023-02-21 12:14:58,092 DEBUG TRAIN Batch 14/7600 loss 8.793100 loss_att 9.749205 loss_ctc 9.548769 loss_rnnt 8.393957 hw_loss 0.200937 lr 0.00044840 rank 1
2023-02-21 12:14:58,093 DEBUG TRAIN Batch 14/7600 loss 12.496264 loss_att 13.690252 loss_ctc 15.392931 loss_rnnt 11.750272 hw_loss 0.226822 lr 0.00044833 rank 4
2023-02-21 12:14:58,094 DEBUG TRAIN Batch 14/7600 loss 9.524659 loss_att 12.380537 loss_ctc 11.679630 loss_rnnt 8.596593 hw_loss 0.130427 lr 0.00044829 rank 7
2023-02-21 12:14:58,098 DEBUG TRAIN Batch 14/7600 loss 16.555405 loss_att 19.771029 loss_ctc 19.889692 loss_rnnt 15.404062 hw_loss 0.119335 lr 0.00044832 rank 0
2023-02-21 12:14:58,100 DEBUG TRAIN Batch 14/7600 loss 10.285128 loss_att 10.104671 loss_ctc 14.641727 loss_rnnt 9.691774 hw_loss 0.091056 lr 0.00044835 rank 6
2023-02-21 12:14:58,102 DEBUG TRAIN Batch 14/7600 loss 14.084227 loss_att 17.381992 loss_ctc 15.852357 loss_rnnt 13.065303 hw_loss 0.231788 lr 0.00044833 rank 3
2023-02-21 12:14:58,147 DEBUG TRAIN Batch 14/7600 loss 5.394187 loss_att 9.104692 loss_ctc 9.920028 loss_rnnt 4.010246 hw_loss 0.071991 lr 0.00044833 rank 5
2023-02-21 12:16:15,978 DEBUG TRAIN Batch 14/7700 loss 14.726381 loss_att 16.200497 loss_ctc 14.590560 loss_rnnt 14.367895 hw_loss 0.153324 lr 0.00044817 rank 6
2023-02-21 12:16:15,980 DEBUG TRAIN Batch 14/7700 loss 20.298937 loss_att 25.325960 loss_ctc 27.764122 loss_rnnt 18.226387 hw_loss 0.134596 lr 0.00044811 rank 7
2023-02-21 12:16:15,985 DEBUG TRAIN Batch 14/7700 loss 13.262996 loss_att 17.045452 loss_ctc 18.678967 loss_rnnt 11.736958 hw_loss 0.088907 lr 0.00044822 rank 1
2023-02-21 12:16:15,985 DEBUG TRAIN Batch 14/7700 loss 10.681570 loss_att 10.915321 loss_ctc 14.204027 loss_rnnt 10.053356 hw_loss 0.209629 lr 0.00044814 rank 4
2023-02-21 12:16:15,986 DEBUG TRAIN Batch 14/7700 loss 13.940682 loss_att 16.184757 loss_ctc 18.961344 loss_rnnt 12.713688 hw_loss 0.203921 lr 0.00044815 rank 2
2023-02-21 12:16:15,988 DEBUG TRAIN Batch 14/7700 loss 16.049757 loss_att 16.416269 loss_ctc 19.277397 loss_rnnt 15.405676 hw_loss 0.263298 lr 0.00044815 rank 5
2023-02-21 12:16:15,989 DEBUG TRAIN Batch 14/7700 loss 10.118863 loss_att 12.939215 loss_ctc 9.542490 loss_rnnt 9.495895 hw_loss 0.254524 lr 0.00044814 rank 0
2023-02-21 12:16:16,034 DEBUG TRAIN Batch 14/7700 loss 11.856188 loss_att 15.250113 loss_ctc 18.007238 loss_rnnt 10.281764 hw_loss 0.141559 lr 0.00044815 rank 3
2023-02-21 12:17:35,767 DEBUG TRAIN Batch 14/7800 loss 6.645851 loss_att 8.444698 loss_ctc 7.684976 loss_rnnt 6.062778 hw_loss 0.158912 lr 0.00044797 rank 2
2023-02-21 12:17:35,767 DEBUG TRAIN Batch 14/7800 loss 6.685058 loss_att 9.032488 loss_ctc 6.879696 loss_rnnt 6.051582 hw_loss 0.258821 lr 0.00044793 rank 7
2023-02-21 12:17:35,769 DEBUG TRAIN Batch 14/7800 loss 8.847536 loss_att 10.023879 loss_ctc 14.653744 loss_rnnt 7.763106 hw_loss 0.140626 lr 0.00044799 rank 6
2023-02-21 12:17:35,771 DEBUG TRAIN Batch 14/7800 loss 10.304656 loss_att 14.691319 loss_ctc 16.602840 loss_rnnt 8.553886 hw_loss 0.063148 lr 0.00044804 rank 1
2023-02-21 12:17:35,771 DEBUG TRAIN Batch 14/7800 loss 7.595264 loss_att 10.278796 loss_ctc 15.672457 loss_rnnt 5.892889 hw_loss 0.166332 lr 0.00044796 rank 0
2023-02-21 12:17:35,772 DEBUG TRAIN Batch 14/7800 loss 12.875018 loss_att 17.946747 loss_ctc 20.911253 loss_rnnt 10.719421 hw_loss 0.130787 lr 0.00044797 rank 4
2023-02-21 12:17:35,778 DEBUG TRAIN Batch 14/7800 loss 8.485373 loss_att 10.788260 loss_ctc 10.048134 loss_rnnt 7.746815 hw_loss 0.130525 lr 0.00044797 rank 5
2023-02-21 12:17:35,786 DEBUG TRAIN Batch 14/7800 loss 15.960065 loss_att 15.692695 loss_ctc 19.950779 loss_rnnt 15.392634 hw_loss 0.166520 lr 0.00044797 rank 3
2023-02-21 12:18:54,937 DEBUG TRAIN Batch 14/7900 loss 23.233650 loss_att 26.356186 loss_ctc 29.388786 loss_rnnt 21.741360 hw_loss 0.088311 lr 0.00044778 rank 0
2023-02-21 12:18:54,938 DEBUG TRAIN Batch 14/7900 loss 20.955299 loss_att 25.726479 loss_ctc 29.074352 loss_rnnt 18.905613 hw_loss 0.024204 lr 0.00044779 rank 2
2023-02-21 12:18:54,938 DEBUG TRAIN Batch 14/7900 loss 9.691710 loss_att 13.554644 loss_ctc 13.781927 loss_rnnt 8.318872 hw_loss 0.102913 lr 0.00044775 rank 7
2023-02-21 12:18:54,939 DEBUG TRAIN Batch 14/7900 loss 11.669415 loss_att 14.698004 loss_ctc 16.006582 loss_rnnt 10.388829 hw_loss 0.181084 lr 0.00044781 rank 6
2023-02-21 12:18:54,941 DEBUG TRAIN Batch 14/7900 loss 10.355147 loss_att 13.524042 loss_ctc 11.476581 loss_rnnt 9.533156 hw_loss 0.072539 lr 0.00044779 rank 4
2023-02-21 12:18:54,942 DEBUG TRAIN Batch 14/7900 loss 4.837927 loss_att 9.780869 loss_ctc 9.860130 loss_rnnt 3.108261 hw_loss 0.133971 lr 0.00044786 rank 1
2023-02-21 12:18:54,943 DEBUG TRAIN Batch 14/7900 loss 4.586899 loss_att 8.180074 loss_ctc 5.892293 loss_rnnt 3.549650 hw_loss 0.271052 lr 0.00044779 rank 3
2023-02-21 12:18:54,943 DEBUG TRAIN Batch 14/7900 loss 10.367532 loss_att 15.567742 loss_ctc 11.917639 loss_rnnt 9.009052 hw_loss 0.209544 lr 0.00044779 rank 5
2023-02-21 12:20:12,278 DEBUG TRAIN Batch 14/8000 loss 13.548668 loss_att 17.515635 loss_ctc 18.203062 loss_rnnt 12.104452 hw_loss 0.056696 lr 0.00044761 rank 2
2023-02-21 12:20:12,279 DEBUG TRAIN Batch 14/8000 loss 10.362988 loss_att 15.047746 loss_ctc 10.733641 loss_rnnt 9.259962 hw_loss 0.218727 lr 0.00044761 rank 4
2023-02-21 12:20:12,280 DEBUG TRAIN Batch 14/8000 loss 14.318863 loss_att 16.360165 loss_ctc 18.617210 loss_rnnt 13.235276 hw_loss 0.191651 lr 0.00044757 rank 7
2023-02-21 12:20:12,283 DEBUG TRAIN Batch 14/8000 loss 15.724598 loss_att 16.942129 loss_ctc 18.516726 loss_rnnt 15.002105 hw_loss 0.200071 lr 0.00044768 rank 1
2023-02-21 12:20:12,283 DEBUG TRAIN Batch 14/8000 loss 6.108522 loss_att 8.742153 loss_ctc 8.092254 loss_rnnt 5.248540 hw_loss 0.128920 lr 0.00044760 rank 0
2023-02-21 12:20:12,288 DEBUG TRAIN Batch 14/8000 loss 25.184551 loss_att 27.660721 loss_ctc 35.725952 loss_rnnt 23.231789 hw_loss 0.097512 lr 0.00044761 rank 5
2023-02-21 12:20:12,290 DEBUG TRAIN Batch 14/8000 loss 19.204432 loss_att 20.407808 loss_ctc 21.134354 loss_rnnt 18.591232 hw_loss 0.216003 lr 0.00044763 rank 6
2023-02-21 12:20:12,294 DEBUG TRAIN Batch 14/8000 loss 13.739082 loss_att 16.582125 loss_ctc 16.002401 loss_rnnt 12.829327 hw_loss 0.073821 lr 0.00044761 rank 3
2023-02-21 12:21:30,712 DEBUG TRAIN Batch 14/8100 loss 5.191226 loss_att 7.819743 loss_ctc 5.722138 loss_rnnt 4.550187 hw_loss 0.083524 lr 0.00044743 rank 2
2023-02-21 12:21:30,712 DEBUG TRAIN Batch 14/8100 loss 7.588154 loss_att 10.878351 loss_ctc 10.211685 loss_rnnt 6.509039 hw_loss 0.133635 lr 0.00044742 rank 0
2023-02-21 12:21:30,714 DEBUG TRAIN Batch 14/8100 loss 14.851383 loss_att 17.295452 loss_ctc 19.242111 loss_rnnt 13.673481 hw_loss 0.194360 lr 0.00044745 rank 6
2023-02-21 12:21:30,718 DEBUG TRAIN Batch 14/8100 loss 19.679264 loss_att 20.714731 loss_ctc 26.606291 loss_rnnt 18.452065 hw_loss 0.180941 lr 0.00044739 rank 7
2023-02-21 12:21:30,719 DEBUG TRAIN Batch 14/8100 loss 10.775837 loss_att 13.892714 loss_ctc 16.716688 loss_rnnt 9.280603 hw_loss 0.149522 lr 0.00044750 rank 1
2023-02-21 12:21:30,720 DEBUG TRAIN Batch 14/8100 loss 16.080347 loss_att 16.490845 loss_ctc 18.372774 loss_rnnt 15.616431 hw_loss 0.142796 lr 0.00044743 rank 5
2023-02-21 12:21:30,728 DEBUG TRAIN Batch 14/8100 loss 11.904264 loss_att 16.798923 loss_ctc 15.583314 loss_rnnt 10.348825 hw_loss 0.161190 lr 0.00044743 rank 4
2023-02-21 12:21:30,746 DEBUG TRAIN Batch 14/8100 loss 8.353722 loss_att 11.600685 loss_ctc 11.627338 loss_rnnt 7.186633 hw_loss 0.152276 lr 0.00044743 rank 3
2023-02-21 12:22:48,977 DEBUG TRAIN Batch 14/8200 loss 10.701449 loss_att 18.640186 loss_ctc 13.181549 loss_rnnt 8.738001 hw_loss 0.084415 lr 0.00044722 rank 7
2023-02-21 12:22:48,984 DEBUG TRAIN Batch 14/8200 loss 10.402402 loss_att 12.018934 loss_ctc 12.878578 loss_rnnt 9.688056 hw_loss 0.114155 lr 0.00044725 rank 2
2023-02-21 12:22:48,985 DEBUG TRAIN Batch 14/8200 loss 14.039742 loss_att 16.424156 loss_ctc 14.831462 loss_rnnt 13.381751 hw_loss 0.141649 lr 0.00044725 rank 4
2023-02-21 12:22:48,985 DEBUG TRAIN Batch 14/8200 loss 8.750746 loss_att 10.000598 loss_ctc 12.437033 loss_rnnt 7.956472 hw_loss 0.098996 lr 0.00044724 rank 0
2023-02-21 12:22:48,988 DEBUG TRAIN Batch 14/8200 loss 5.472960 loss_att 8.281466 loss_ctc 7.915639 loss_rnnt 4.497626 hw_loss 0.164892 lr 0.00044725 rank 3
2023-02-21 12:22:48,988 DEBUG TRAIN Batch 14/8200 loss 12.531480 loss_att 14.144783 loss_ctc 16.264458 loss_rnnt 11.694096 hw_loss 0.031863 lr 0.00044725 rank 5
2023-02-21 12:22:48,989 DEBUG TRAIN Batch 14/8200 loss 2.514459 loss_att 4.208418 loss_ctc 2.620717 loss_rnnt 2.099869 hw_loss 0.115557 lr 0.00044732 rank 1
2023-02-21 12:22:49,033 DEBUG TRAIN Batch 14/8200 loss 12.704979 loss_att 16.308893 loss_ctc 16.720825 loss_rnnt 11.355591 hw_loss 0.174674 lr 0.00044727 rank 6
2023-02-21 12:24:06,350 DEBUG TRAIN Batch 14/8300 loss 7.225107 loss_att 9.912142 loss_ctc 8.931919 loss_rnnt 6.425937 hw_loss 0.064104 lr 0.00044704 rank 7
2023-02-21 12:24:06,350 DEBUG TRAIN Batch 14/8300 loss 6.754473 loss_att 15.538868 loss_ctc 15.045275 loss_rnnt 3.842907 hw_loss 0.092335 lr 0.00044709 rank 6
2023-02-21 12:24:06,351 DEBUG TRAIN Batch 14/8300 loss 12.622718 loss_att 16.639227 loss_ctc 15.415146 loss_rnnt 11.386303 hw_loss 0.113980 lr 0.00044707 rank 4
2023-02-21 12:24:06,352 DEBUG TRAIN Batch 14/8300 loss 10.141189 loss_att 15.425026 loss_ctc 17.424147 loss_rnnt 8.078474 hw_loss 0.065410 lr 0.00044714 rank 1
2023-02-21 12:24:06,352 DEBUG TRAIN Batch 14/8300 loss 11.458138 loss_att 14.609701 loss_ctc 15.787658 loss_rnnt 10.099839 hw_loss 0.282595 lr 0.00044707 rank 2
2023-02-21 12:24:06,354 DEBUG TRAIN Batch 14/8300 loss 14.429248 loss_att 24.277107 loss_ctc 22.166365 loss_rnnt 11.332525 hw_loss 0.179128 lr 0.00044706 rank 0
2023-02-21 12:24:06,363 DEBUG TRAIN Batch 14/8300 loss 20.330797 loss_att 29.295910 loss_ctc 24.855412 loss_rnnt 17.859520 hw_loss 0.140572 lr 0.00044707 rank 5
2023-02-21 12:24:06,403 DEBUG TRAIN Batch 14/8300 loss 3.756054 loss_att 8.772705 loss_ctc 5.711784 loss_rnnt 2.385985 hw_loss 0.198703 lr 0.00044707 rank 3
2023-02-21 12:25:07,768 DEBUG CV Batch 14/0 loss 3.081857 loss_att 2.628860 loss_ctc 3.574210 loss_rnnt 2.760293 hw_loss 0.649718 history loss 2.967714 rank 0
2023-02-21 12:25:07,777 DEBUG CV Batch 14/0 loss 3.081857 loss_att 2.628860 loss_ctc 3.574210 loss_rnnt 2.760293 hw_loss 0.649718 history loss 2.967714 rank 3
2023-02-21 12:25:07,780 DEBUG CV Batch 14/0 loss 3.081857 loss_att 2.628860 loss_ctc 3.574210 loss_rnnt 2.760293 hw_loss 0.649718 history loss 2.967714 rank 4
2023-02-21 12:25:07,780 DEBUG CV Batch 14/0 loss 3.081857 loss_att 2.628860 loss_ctc 3.574210 loss_rnnt 2.760293 hw_loss 0.649718 history loss 2.967714 rank 7
2023-02-21 12:25:07,786 DEBUG CV Batch 14/0 loss 3.081857 loss_att 2.628860 loss_ctc 3.574210 loss_rnnt 2.760293 hw_loss 0.649718 history loss 2.967714 rank 2
2023-02-21 12:25:07,798 DEBUG CV Batch 14/0 loss 3.081857 loss_att 2.628860 loss_ctc 3.574210 loss_rnnt 2.760293 hw_loss 0.649718 history loss 2.967714 rank 1
2023-02-21 12:25:07,803 DEBUG CV Batch 14/0 loss 3.081857 loss_att 2.628860 loss_ctc 3.574210 loss_rnnt 2.760293 hw_loss 0.649718 history loss 2.967714 rank 6
2023-02-21 12:25:07,820 DEBUG CV Batch 14/0 loss 3.081857 loss_att 2.628860 loss_ctc 3.574210 loss_rnnt 2.760293 hw_loss 0.649718 history loss 2.967714 rank 5
2023-02-21 12:25:18,857 DEBUG CV Batch 14/100 loss 7.752304 loss_att 9.188816 loss_ctc 11.731426 loss_rnnt 6.849270 hw_loss 0.159715 history loss 4.154266 rank 7
2023-02-21 12:25:18,915 DEBUG CV Batch 14/100 loss 7.752304 loss_att 9.188816 loss_ctc 11.731426 loss_rnnt 6.849270 hw_loss 0.159715 history loss 4.154266 rank 4
2023-02-21 12:25:19,011 DEBUG CV Batch 14/100 loss 7.752304 loss_att 9.188816 loss_ctc 11.731426 loss_rnnt 6.849270 hw_loss 0.159715 history loss 4.154266 rank 0
2023-02-21 12:25:19,011 DEBUG CV Batch 14/100 loss 7.752304 loss_att 9.188816 loss_ctc 11.731426 loss_rnnt 6.849270 hw_loss 0.159715 history loss 4.154266 rank 6
2023-02-21 12:25:19,019 DEBUG CV Batch 14/100 loss 7.752304 loss_att 9.188816 loss_ctc 11.731426 loss_rnnt 6.849270 hw_loss 0.159715 history loss 4.154266 rank 2
2023-02-21 12:25:19,051 DEBUG CV Batch 14/100 loss 7.752304 loss_att 9.188816 loss_ctc 11.731426 loss_rnnt 6.849270 hw_loss 0.159715 history loss 4.154266 rank 1
2023-02-21 12:25:19,190 DEBUG CV Batch 14/100 loss 7.752304 loss_att 9.188816 loss_ctc 11.731426 loss_rnnt 6.849270 hw_loss 0.159715 history loss 4.154266 rank 5
2023-02-21 12:25:19,570 DEBUG CV Batch 14/100 loss 7.752304 loss_att 9.188816 loss_ctc 11.731426 loss_rnnt 6.849270 hw_loss 0.159715 history loss 4.154266 rank 3
2023-02-21 12:25:32,070 DEBUG CV Batch 14/200 loss 11.112932 loss_att 24.316113 loss_ctc 12.038492 loss_rnnt 8.290494 hw_loss 0.109488 history loss 4.740283 rank 7
2023-02-21 12:25:32,158 DEBUG CV Batch 14/200 loss 11.112932 loss_att 24.316113 loss_ctc 12.038492 loss_rnnt 8.290494 hw_loss 0.109488 history loss 4.740283 rank 4
2023-02-21 12:25:32,302 DEBUG CV Batch 14/200 loss 11.112932 loss_att 24.316113 loss_ctc 12.038492 loss_rnnt 8.290494 hw_loss 0.109488 history loss 4.740283 rank 2
2023-02-21 12:25:32,409 DEBUG CV Batch 14/200 loss 11.112932 loss_att 24.316113 loss_ctc 12.038492 loss_rnnt 8.290494 hw_loss 0.109488 history loss 4.740283 rank 1
2023-02-21 12:25:32,414 DEBUG CV Batch 14/200 loss 11.112932 loss_att 24.316113 loss_ctc 12.038492 loss_rnnt 8.290494 hw_loss 0.109488 history loss 4.740283 rank 0
2023-02-21 12:25:32,435 DEBUG CV Batch 14/200 loss 11.112932 loss_att 24.316113 loss_ctc 12.038492 loss_rnnt 8.290494 hw_loss 0.109488 history loss 4.740283 rank 6
2023-02-21 12:25:32,643 DEBUG CV Batch 14/200 loss 11.112932 loss_att 24.316113 loss_ctc 12.038492 loss_rnnt 8.290494 hw_loss 0.109488 history loss 4.740283 rank 5
2023-02-21 12:25:33,297 DEBUG CV Batch 14/200 loss 11.112932 loss_att 24.316113 loss_ctc 12.038492 loss_rnnt 8.290494 hw_loss 0.109488 history loss 4.740283 rank 3
2023-02-21 12:25:44,138 DEBUG CV Batch 14/300 loss 5.065870 loss_att 5.678318 loss_ctc 6.971756 loss_rnnt 4.622288 hw_loss 0.125576 history loss 4.895451 rank 7
2023-02-21 12:25:44,297 DEBUG CV Batch 14/300 loss 5.065870 loss_att 5.678318 loss_ctc 6.971756 loss_rnnt 4.622288 hw_loss 0.125576 history loss 4.895451 rank 4
2023-02-21 12:25:44,395 DEBUG CV Batch 14/300 loss 5.065870 loss_att 5.678318 loss_ctc 6.971756 loss_rnnt 4.622288 hw_loss 0.125576 history loss 4.895451 rank 2
2023-02-21 12:25:44,464 DEBUG CV Batch 14/300 loss 5.065870 loss_att 5.678318 loss_ctc 6.971756 loss_rnnt 4.622288 hw_loss 0.125576 history loss 4.895451 rank 1
2023-02-21 12:25:44,582 DEBUG CV Batch 14/300 loss 5.065870 loss_att 5.678318 loss_ctc 6.971756 loss_rnnt 4.622288 hw_loss 0.125576 history loss 4.895451 rank 0
2023-02-21 12:25:44,654 DEBUG CV Batch 14/300 loss 5.065870 loss_att 5.678318 loss_ctc 6.971756 loss_rnnt 4.622288 hw_loss 0.125576 history loss 4.895451 rank 6
2023-02-21 12:25:44,849 DEBUG CV Batch 14/300 loss 5.065870 loss_att 5.678318 loss_ctc 6.971756 loss_rnnt 4.622288 hw_loss 0.125576 history loss 4.895451 rank 5
2023-02-21 12:25:45,574 DEBUG CV Batch 14/300 loss 5.065870 loss_att 5.678318 loss_ctc 6.971756 loss_rnnt 4.622288 hw_loss 0.125576 history loss 4.895451 rank 3
2023-02-21 12:25:56,020 DEBUG CV Batch 14/400 loss 20.883486 loss_att 109.682671 loss_ctc 6.001380 loss_rnnt 5.094826 hw_loss 0.024566 history loss 5.989518 rank 7
2023-02-21 12:25:56,292 DEBUG CV Batch 14/400 loss 20.883486 loss_att 109.682671 loss_ctc 6.001380 loss_rnnt 5.094826 hw_loss 0.024566 history loss 5.989518 rank 4
2023-02-21 12:25:56,372 DEBUG CV Batch 14/400 loss 20.883486 loss_att 109.682671 loss_ctc 6.001380 loss_rnnt 5.094826 hw_loss 0.024566 history loss 5.989518 rank 2
2023-02-21 12:25:56,373 DEBUG CV Batch 14/400 loss 20.883486 loss_att 109.682671 loss_ctc 6.001380 loss_rnnt 5.094826 hw_loss 0.024566 history loss 5.989518 rank 1
2023-02-21 12:25:56,638 DEBUG CV Batch 14/400 loss 20.883486 loss_att 109.682671 loss_ctc 6.001380 loss_rnnt 5.094826 hw_loss 0.024566 history loss 5.989518 rank 0
2023-02-21 12:25:56,726 DEBUG CV Batch 14/400 loss 20.883486 loss_att 109.682671 loss_ctc 6.001380 loss_rnnt 5.094826 hw_loss 0.024566 history loss 5.989518 rank 6
2023-02-21 12:25:56,913 DEBUG CV Batch 14/400 loss 20.883486 loss_att 109.682671 loss_ctc 6.001380 loss_rnnt 5.094826 hw_loss 0.024566 history loss 5.989518 rank 5
2023-02-21 12:25:57,676 DEBUG CV Batch 14/400 loss 20.883486 loss_att 109.682671 loss_ctc 6.001380 loss_rnnt 5.094826 hw_loss 0.024566 history loss 5.989518 rank 3
2023-02-21 12:26:06,781 DEBUG CV Batch 14/500 loss 6.896085 loss_att 6.891687 loss_ctc 8.315195 loss_rnnt 6.694647 hw_loss 0.024566 history loss 6.985038 rank 7
2023-02-21 12:26:06,833 DEBUG CV Batch 14/500 loss 6.896085 loss_att 6.891687 loss_ctc 8.315195 loss_rnnt 6.694647 hw_loss 0.024566 history loss 6.985038 rank 4
2023-02-21 12:26:06,896 DEBUG CV Batch 14/500 loss 6.896085 loss_att 6.891687 loss_ctc 8.315195 loss_rnnt 6.694647 hw_loss 0.024566 history loss 6.985038 rank 2
2023-02-21 12:26:07,066 DEBUG CV Batch 14/500 loss 6.896085 loss_att 6.891687 loss_ctc 8.315195 loss_rnnt 6.694647 hw_loss 0.024566 history loss 6.985038 rank 1
2023-02-21 12:26:07,191 DEBUG CV Batch 14/500 loss 6.896085 loss_att 6.891687 loss_ctc 8.315195 loss_rnnt 6.694647 hw_loss 0.024566 history loss 6.985038 rank 0
2023-02-21 12:26:07,466 DEBUG CV Batch 14/500 loss 6.896085 loss_att 6.891687 loss_ctc 8.315195 loss_rnnt 6.694647 hw_loss 0.024566 history loss 6.985038 rank 5
2023-02-21 12:26:07,610 DEBUG CV Batch 14/500 loss 6.896085 loss_att 6.891687 loss_ctc 8.315195 loss_rnnt 6.694647 hw_loss 0.024566 history loss 6.985038 rank 6
2023-02-21 12:26:08,277 DEBUG CV Batch 14/500 loss 6.896085 loss_att 6.891687 loss_ctc 8.315195 loss_rnnt 6.694647 hw_loss 0.024566 history loss 6.985038 rank 3
2023-02-21 12:26:18,812 DEBUG CV Batch 14/600 loss 8.408662 loss_att 8.737859 loss_ctc 11.426878 loss_rnnt 7.766758 hw_loss 0.325565 history loss 7.996625 rank 4
2023-02-21 12:26:18,895 DEBUG CV Batch 14/600 loss 8.408662 loss_att 8.737859 loss_ctc 11.426878 loss_rnnt 7.766758 hw_loss 0.325565 history loss 7.996625 rank 2
2023-02-21 12:26:18,929 DEBUG CV Batch 14/600 loss 8.408662 loss_att 8.737859 loss_ctc 11.426878 loss_rnnt 7.766758 hw_loss 0.325565 history loss 7.996625 rank 7
2023-02-21 12:26:19,094 DEBUG CV Batch 14/600 loss 8.408662 loss_att 8.737859 loss_ctc 11.426878 loss_rnnt 7.766758 hw_loss 0.325565 history loss 7.996625 rank 1
2023-02-21 12:26:19,339 DEBUG CV Batch 14/600 loss 8.408662 loss_att 8.737859 loss_ctc 11.426878 loss_rnnt 7.766758 hw_loss 0.325565 history loss 7.996625 rank 0
2023-02-21 12:26:19,465 DEBUG CV Batch 14/600 loss 8.408662 loss_att 8.737859 loss_ctc 11.426878 loss_rnnt 7.766758 hw_loss 0.325565 history loss 7.996625 rank 5
2023-02-21 12:26:19,675 DEBUG CV Batch 14/600 loss 8.408662 loss_att 8.737859 loss_ctc 11.426878 loss_rnnt 7.766758 hw_loss 0.325565 history loss 7.996625 rank 6
2023-02-21 12:26:20,477 DEBUG CV Batch 14/600 loss 8.408662 loss_att 8.737859 loss_ctc 11.426878 loss_rnnt 7.766758 hw_loss 0.325565 history loss 7.996625 rank 3
2023-02-21 12:26:30,090 DEBUG CV Batch 14/700 loss 20.709846 loss_att 56.423378 loss_ctc 16.567587 loss_rnnt 14.022503 hw_loss 0.181761 history loss 8.800525 rank 4
2023-02-21 12:26:30,206 DEBUG CV Batch 14/700 loss 20.709846 loss_att 56.423378 loss_ctc 16.567587 loss_rnnt 14.022503 hw_loss 0.181761 history loss 8.800525 rank 2
2023-02-21 12:26:30,407 DEBUG CV Batch 14/700 loss 20.709846 loss_att 56.423378 loss_ctc 16.567587 loss_rnnt 14.022503 hw_loss 0.181761 history loss 8.800525 rank 7
2023-02-21 12:26:30,457 DEBUG CV Batch 14/700 loss 20.709846 loss_att 56.423378 loss_ctc 16.567587 loss_rnnt 14.022503 hw_loss 0.181761 history loss 8.800525 rank 1
2023-02-21 12:26:30,792 DEBUG CV Batch 14/700 loss 20.709846 loss_att 56.423378 loss_ctc 16.567587 loss_rnnt 14.022503 hw_loss 0.181761 history loss 8.800525 rank 0
2023-02-21 12:26:30,840 DEBUG CV Batch 14/700 loss 20.709846 loss_att 56.423378 loss_ctc 16.567587 loss_rnnt 14.022503 hw_loss 0.181761 history loss 8.800525 rank 5
2023-02-21 12:26:31,057 DEBUG CV Batch 14/700 loss 20.709846 loss_att 56.423378 loss_ctc 16.567587 loss_rnnt 14.022503 hw_loss 0.181761 history loss 8.800525 rank 6
2023-02-21 12:26:31,910 DEBUG CV Batch 14/700 loss 20.709846 loss_att 56.423378 loss_ctc 16.567587 loss_rnnt 14.022503 hw_loss 0.181761 history loss 8.800525 rank 3
2023-02-21 12:26:41,271 DEBUG CV Batch 14/800 loss 11.775548 loss_att 12.318338 loss_ctc 17.161772 loss_rnnt 10.895681 hw_loss 0.099649 history loss 8.172997 rank 4
2023-02-21 12:26:41,362 DEBUG CV Batch 14/800 loss 11.775548 loss_att 12.318338 loss_ctc 17.161772 loss_rnnt 10.895681 hw_loss 0.099649 history loss 8.172997 rank 2
2023-02-21 12:26:41,535 DEBUG CV Batch 14/800 loss 11.775548 loss_att 12.318338 loss_ctc 17.161772 loss_rnnt 10.895681 hw_loss 0.099649 history loss 8.172997 rank 7
2023-02-21 12:26:41,554 DEBUG CV Batch 14/800 loss 11.775548 loss_att 12.318338 loss_ctc 17.161772 loss_rnnt 10.895681 hw_loss 0.099649 history loss 8.172997 rank 1
2023-02-21 12:26:42,044 DEBUG CV Batch 14/800 loss 11.775548 loss_att 12.318338 loss_ctc 17.161772 loss_rnnt 10.895681 hw_loss 0.099649 history loss 8.172997 rank 0
2023-02-21 12:26:42,071 DEBUG CV Batch 14/800 loss 11.775548 loss_att 12.318338 loss_ctc 17.161772 loss_rnnt 10.895681 hw_loss 0.099649 history loss 8.172997 rank 5
2023-02-21 12:26:42,301 DEBUG CV Batch 14/800 loss 11.775548 loss_att 12.318338 loss_ctc 17.161772 loss_rnnt 10.895681 hw_loss 0.099649 history loss 8.172997 rank 6
2023-02-21 12:26:44,431 DEBUG CV Batch 14/800 loss 11.775548 loss_att 12.318338 loss_ctc 17.161772 loss_rnnt 10.895681 hw_loss 0.099649 history loss 8.172997 rank 3
2023-02-21 12:26:54,420 DEBUG CV Batch 14/900 loss 18.832607 loss_att 22.257305 loss_ctc 22.403128 loss_rnnt 17.636108 hw_loss 0.066544 history loss 7.922835 rank 4
2023-02-21 12:26:54,637 DEBUG CV Batch 14/900 loss 18.832607 loss_att 22.257305 loss_ctc 22.403128 loss_rnnt 17.636108 hw_loss 0.066544 history loss 7.922835 rank 2
2023-02-21 12:26:54,637 DEBUG CV Batch 14/900 loss 18.832607 loss_att 22.257305 loss_ctc 22.403128 loss_rnnt 17.636108 hw_loss 0.066544 history loss 7.922835 rank 7
2023-02-21 12:26:54,665 DEBUG CV Batch 14/900 loss 18.832607 loss_att 22.257305 loss_ctc 22.403128 loss_rnnt 17.636108 hw_loss 0.066544 history loss 7.922835 rank 1
2023-02-21 12:26:55,251 DEBUG CV Batch 14/900 loss 18.832607 loss_att 22.257305 loss_ctc 22.403128 loss_rnnt 17.636108 hw_loss 0.066544 history loss 7.922835 rank 5
2023-02-21 12:26:55,294 DEBUG CV Batch 14/900 loss 18.832607 loss_att 22.257305 loss_ctc 22.403128 loss_rnnt 17.636108 hw_loss 0.066544 history loss 7.922835 rank 0
2023-02-21 12:26:55,572 DEBUG CV Batch 14/900 loss 18.832607 loss_att 22.257305 loss_ctc 22.403128 loss_rnnt 17.636108 hw_loss 0.066544 history loss 7.922835 rank 6
2023-02-21 12:26:58,330 DEBUG CV Batch 14/900 loss 18.832607 loss_att 22.257305 loss_ctc 22.403128 loss_rnnt 17.636108 hw_loss 0.066544 history loss 7.922835 rank 3
2023-02-21 12:27:06,323 DEBUG CV Batch 14/1000 loss 4.675689 loss_att 6.069039 loss_ctc 5.671027 loss_rnnt 4.116153 hw_loss 0.277789 history loss 7.650643 rank 4
2023-02-21 12:27:06,560 DEBUG CV Batch 14/1000 loss 4.675689 loss_att 6.069039 loss_ctc 5.671027 loss_rnnt 4.116153 hw_loss 0.277789 history loss 7.650643 rank 7
2023-02-21 12:27:06,669 DEBUG CV Batch 14/1000 loss 4.675689 loss_att 6.069039 loss_ctc 5.671027 loss_rnnt 4.116153 hw_loss 0.277789 history loss 7.650643 rank 2
2023-02-21 12:27:06,706 DEBUG CV Batch 14/1000 loss 4.675689 loss_att 6.069039 loss_ctc 5.671027 loss_rnnt 4.116153 hw_loss 0.277789 history loss 7.650643 rank 1
2023-02-21 12:27:07,405 DEBUG CV Batch 14/1000 loss 4.675689 loss_att 6.069039 loss_ctc 5.671027 loss_rnnt 4.116153 hw_loss 0.277789 history loss 7.650643 rank 5
2023-02-21 12:27:07,499 DEBUG CV Batch 14/1000 loss 4.675689 loss_att 6.069039 loss_ctc 5.671027 loss_rnnt 4.116153 hw_loss 0.277789 history loss 7.650643 rank 0
2023-02-21 12:27:07,862 DEBUG CV Batch 14/1000 loss 4.675689 loss_att 6.069039 loss_ctc 5.671027 loss_rnnt 4.116153 hw_loss 0.277789 history loss 7.650643 rank 6
2023-02-21 12:27:10,635 DEBUG CV Batch 14/1000 loss 4.675689 loss_att 6.069039 loss_ctc 5.671027 loss_rnnt 4.116153 hw_loss 0.277789 history loss 7.650643 rank 3
2023-02-21 12:27:18,139 DEBUG CV Batch 14/1100 loss 7.325581 loss_att 6.062080 loss_ctc 9.354623 loss_rnnt 7.096254 hw_loss 0.396539 history loss 7.617547 rank 4
2023-02-21 12:27:18,484 DEBUG CV Batch 14/1100 loss 7.325581 loss_att 6.062080 loss_ctc 9.354623 loss_rnnt 7.096254 hw_loss 0.396539 history loss 7.617547 rank 7
2023-02-21 12:27:18,511 DEBUG CV Batch 14/1100 loss 7.325581 loss_att 6.062080 loss_ctc 9.354623 loss_rnnt 7.096254 hw_loss 0.396539 history loss 7.617547 rank 2
2023-02-21 12:27:18,531 DEBUG CV Batch 14/1100 loss 7.325581 loss_att 6.062080 loss_ctc 9.354623 loss_rnnt 7.096254 hw_loss 0.396539 history loss 7.617547 rank 1
2023-02-21 12:27:19,325 DEBUG CV Batch 14/1100 loss 7.325581 loss_att 6.062080 loss_ctc 9.354623 loss_rnnt 7.096254 hw_loss 0.396539 history loss 7.617547 rank 5
2023-02-21 12:27:19,467 DEBUG CV Batch 14/1100 loss 7.325581 loss_att 6.062080 loss_ctc 9.354623 loss_rnnt 7.096254 hw_loss 0.396539 history loss 7.617547 rank 0
2023-02-21 12:27:20,467 DEBUG CV Batch 14/1100 loss 7.325581 loss_att 6.062080 loss_ctc 9.354623 loss_rnnt 7.096254 hw_loss 0.396539 history loss 7.617547 rank 6
2023-02-21 12:27:22,567 DEBUG CV Batch 14/1100 loss 7.325581 loss_att 6.062080 loss_ctc 9.354623 loss_rnnt 7.096254 hw_loss 0.396539 history loss 7.617547 rank 3
2023-02-21 12:27:28,607 DEBUG CV Batch 14/1200 loss 9.043032 loss_att 10.445539 loss_ctc 10.003589 loss_rnnt 8.568816 hw_loss 0.123075 history loss 8.036638 rank 4
2023-02-21 12:27:28,995 DEBUG CV Batch 14/1200 loss 9.043032 loss_att 10.445539 loss_ctc 10.003589 loss_rnnt 8.568816 hw_loss 0.123075 history loss 8.036638 rank 7
2023-02-21 12:27:29,017 DEBUG CV Batch 14/1200 loss 9.043032 loss_att 10.445539 loss_ctc 10.003589 loss_rnnt 8.568816 hw_loss 0.123075 history loss 8.036638 rank 1
2023-02-21 12:27:29,116 DEBUG CV Batch 14/1200 loss 9.043032 loss_att 10.445539 loss_ctc 10.003589 loss_rnnt 8.568816 hw_loss 0.123075 history loss 8.036638 rank 2
2023-02-21 12:27:29,985 DEBUG CV Batch 14/1200 loss 9.043032 loss_att 10.445539 loss_ctc 10.003589 loss_rnnt 8.568816 hw_loss 0.123075 history loss 8.036638 rank 5
2023-02-21 12:27:30,448 DEBUG CV Batch 14/1200 loss 9.043032 loss_att 10.445539 loss_ctc 10.003589 loss_rnnt 8.568816 hw_loss 0.123075 history loss 8.036638 rank 0
2023-02-21 12:27:31,152 DEBUG CV Batch 14/1200 loss 9.043032 loss_att 10.445539 loss_ctc 10.003589 loss_rnnt 8.568816 hw_loss 0.123075 history loss 8.036638 rank 6
2023-02-21 12:27:33,305 DEBUG CV Batch 14/1200 loss 9.043032 loss_att 10.445539 loss_ctc 10.003589 loss_rnnt 8.568816 hw_loss 0.123075 history loss 8.036638 rank 3
2023-02-21 12:27:40,568 DEBUG CV Batch 14/1300 loss 6.833733 loss_att 6.703725 loss_ctc 9.354080 loss_rnnt 6.353868 hw_loss 0.318413 history loss 8.365263 rank 4
2023-02-21 12:27:40,875 DEBUG CV Batch 14/1300 loss 6.833733 loss_att 6.703725 loss_ctc 9.354080 loss_rnnt 6.353868 hw_loss 0.318413 history loss 8.365263 rank 1
2023-02-21 12:27:40,878 DEBUG CV Batch 14/1300 loss 6.833733 loss_att 6.703725 loss_ctc 9.354080 loss_rnnt 6.353868 hw_loss 0.318413 history loss 8.365263 rank 7
2023-02-21 12:27:41,090 DEBUG CV Batch 14/1300 loss 6.833733 loss_att 6.703725 loss_ctc 9.354080 loss_rnnt 6.353868 hw_loss 0.318413 history loss 8.365263 rank 2
2023-02-21 12:27:42,100 DEBUG CV Batch 14/1300 loss 6.833733 loss_att 6.703725 loss_ctc 9.354080 loss_rnnt 6.353868 hw_loss 0.318413 history loss 8.365263 rank 5
2023-02-21 12:27:43,168 DEBUG CV Batch 14/1300 loss 6.833733 loss_att 6.703725 loss_ctc 9.354080 loss_rnnt 6.353868 hw_loss 0.318413 history loss 8.365263 rank 6
2023-02-21 12:27:43,498 DEBUG CV Batch 14/1300 loss 6.833733 loss_att 6.703725 loss_ctc 9.354080 loss_rnnt 6.353868 hw_loss 0.318413 history loss 8.365263 rank 0
2023-02-21 12:27:45,409 DEBUG CV Batch 14/1300 loss 6.833733 loss_att 6.703725 loss_ctc 9.354080 loss_rnnt 6.353868 hw_loss 0.318413 history loss 8.365263 rank 3
2023-02-21 12:27:51,698 DEBUG CV Batch 14/1400 loss 10.992152 loss_att 32.978886 loss_ctc 8.954889 loss_rnnt 6.804692 hw_loss 0.115778 history loss 8.753282 rank 4
2023-02-21 12:27:51,945 DEBUG CV Batch 14/1400 loss 10.992152 loss_att 32.978886 loss_ctc 8.954889 loss_rnnt 6.804692 hw_loss 0.115778 history loss 8.753282 rank 7
2023-02-21 12:27:52,041 DEBUG CV Batch 14/1400 loss 10.992152 loss_att 32.978886 loss_ctc 8.954889 loss_rnnt 6.804692 hw_loss 0.115778 history loss 8.753282 rank 1
2023-02-21 12:27:52,263 DEBUG CV Batch 14/1400 loss 10.992152 loss_att 32.978886 loss_ctc 8.954889 loss_rnnt 6.804692 hw_loss 0.115778 history loss 8.753282 rank 2
2023-02-21 12:27:53,370 DEBUG CV Batch 14/1400 loss 10.992152 loss_att 32.978886 loss_ctc 8.954889 loss_rnnt 6.804692 hw_loss 0.115778 history loss 8.753282 rank 5
2023-02-21 12:27:54,386 DEBUG CV Batch 14/1400 loss 10.992152 loss_att 32.978886 loss_ctc 8.954889 loss_rnnt 6.804692 hw_loss 0.115778 history loss 8.753282 rank 6
2023-02-21 12:27:54,758 DEBUG CV Batch 14/1400 loss 10.992152 loss_att 32.978886 loss_ctc 8.954889 loss_rnnt 6.804692 hw_loss 0.115778 history loss 8.753282 rank 0
2023-02-21 12:27:56,722 DEBUG CV Batch 14/1400 loss 10.992152 loss_att 32.978886 loss_ctc 8.954889 loss_rnnt 6.804692 hw_loss 0.115778 history loss 8.753282 rank 3
2023-02-21 12:28:03,114 DEBUG CV Batch 14/1500 loss 8.096477 loss_att 8.272636 loss_ctc 6.793448 loss_rnnt 8.206954 hw_loss 0.052552 history loss 8.546562 rank 4
2023-02-21 12:28:03,308 DEBUG CV Batch 14/1500 loss 8.096477 loss_att 8.272636 loss_ctc 6.793448 loss_rnnt 8.206954 hw_loss 0.052552 history loss 8.546562 rank 7
2023-02-21 12:28:03,504 DEBUG CV Batch 14/1500 loss 8.096477 loss_att 8.272636 loss_ctc 6.793448 loss_rnnt 8.206954 hw_loss 0.052552 history loss 8.546562 rank 1
2023-02-21 12:28:03,562 DEBUG CV Batch 14/1500 loss 8.096477 loss_att 8.272636 loss_ctc 6.793448 loss_rnnt 8.206954 hw_loss 0.052552 history loss 8.546562 rank 2
2023-02-21 12:28:05,799 DEBUG CV Batch 14/1500 loss 8.096477 loss_att 8.272636 loss_ctc 6.793448 loss_rnnt 8.206954 hw_loss 0.052552 history loss 8.546562 rank 6
2023-02-21 12:28:05,888 DEBUG CV Batch 14/1500 loss 8.096477 loss_att 8.272636 loss_ctc 6.793448 loss_rnnt 8.206954 hw_loss 0.052552 history loss 8.546562 rank 5
2023-02-21 12:28:06,205 DEBUG CV Batch 14/1500 loss 8.096477 loss_att 8.272636 loss_ctc 6.793448 loss_rnnt 8.206954 hw_loss 0.052552 history loss 8.546562 rank 0
2023-02-21 12:28:08,169 DEBUG CV Batch 14/1500 loss 8.096477 loss_att 8.272636 loss_ctc 6.793448 loss_rnnt 8.206954 hw_loss 0.052552 history loss 8.546562 rank 3
2023-02-21 12:28:16,046 DEBUG CV Batch 14/1600 loss 8.642354 loss_att 13.841754 loss_ctc 12.214291 loss_rnnt 7.113113 hw_loss 0.024566 history loss 8.448103 rank 4
2023-02-21 12:28:16,326 DEBUG CV Batch 14/1600 loss 8.642354 loss_att 13.841754 loss_ctc 12.214291 loss_rnnt 7.113113 hw_loss 0.024566 history loss 8.448103 rank 7
2023-02-21 12:28:16,353 DEBUG CV Batch 14/1600 loss 8.642354 loss_att 13.841754 loss_ctc 12.214291 loss_rnnt 7.113113 hw_loss 0.024566 history loss 8.448103 rank 1
2023-02-21 12:28:16,481 DEBUG CV Batch 14/1600 loss 8.642354 loss_att 13.841754 loss_ctc 12.214291 loss_rnnt 7.113113 hw_loss 0.024566 history loss 8.448103 rank 2
2023-02-21 12:28:18,777 DEBUG CV Batch 14/1600 loss 8.642354 loss_att 13.841754 loss_ctc 12.214291 loss_rnnt 7.113113 hw_loss 0.024566 history loss 8.448103 rank 6
2023-02-21 12:28:19,169 DEBUG CV Batch 14/1600 loss 8.642354 loss_att 13.841754 loss_ctc 12.214291 loss_rnnt 7.113113 hw_loss 0.024566 history loss 8.448103 rank 0
2023-02-21 12:28:19,676 DEBUG CV Batch 14/1600 loss 8.642354 loss_att 13.841754 loss_ctc 12.214291 loss_rnnt 7.113113 hw_loss 0.024566 history loss 8.448103 rank 5
2023-02-21 12:28:21,173 DEBUG CV Batch 14/1600 loss 8.642354 loss_att 13.841754 loss_ctc 12.214291 loss_rnnt 7.113113 hw_loss 0.024566 history loss 8.448103 rank 3
2023-02-21 12:28:28,303 DEBUG CV Batch 14/1700 loss 11.828387 loss_att 11.202387 loss_ctc 15.463207 loss_rnnt 11.399551 hw_loss 0.130111 history loss 8.328522 rank 4
2023-02-21 12:28:28,459 DEBUG CV Batch 14/1700 loss 11.828387 loss_att 11.202387 loss_ctc 15.463207 loss_rnnt 11.399551 hw_loss 0.130111 history loss 8.328522 rank 1
2023-02-21 12:28:28,582 DEBUG CV Batch 14/1700 loss 11.828387 loss_att 11.202387 loss_ctc 15.463207 loss_rnnt 11.399551 hw_loss 0.130111 history loss 8.328522 rank 7
2023-02-21 12:28:28,710 DEBUG CV Batch 14/1700 loss 11.828387 loss_att 11.202387 loss_ctc 15.463207 loss_rnnt 11.399551 hw_loss 0.130111 history loss 8.328522 rank 2
2023-02-21 12:28:31,153 DEBUG CV Batch 14/1700 loss 11.828387 loss_att 11.202387 loss_ctc 15.463207 loss_rnnt 11.399551 hw_loss 0.130111 history loss 8.328522 rank 6
2023-02-21 12:28:31,413 DEBUG CV Batch 14/1700 loss 11.828387 loss_att 11.202387 loss_ctc 15.463207 loss_rnnt 11.399551 hw_loss 0.130111 history loss 8.328522 rank 0
2023-02-21 12:28:32,165 DEBUG CV Batch 14/1700 loss 11.828387 loss_att 11.202387 loss_ctc 15.463207 loss_rnnt 11.399551 hw_loss 0.130111 history loss 8.328522 rank 5
2023-02-21 12:28:33,327 DEBUG CV Batch 14/1700 loss 11.828387 loss_att 11.202387 loss_ctc 15.463207 loss_rnnt 11.399551 hw_loss 0.130111 history loss 8.328522 rank 3
2023-02-21 12:28:37,243 INFO Epoch 14 CV info cv_loss 8.2834330782809
2023-02-21 12:28:37,244 INFO Epoch 15 TRAIN info lr 0.0004470258840397933
2023-02-21 12:28:37,247 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 12:28:37,428 INFO Epoch 14 CV info cv_loss 8.283433078190447
2023-02-21 12:28:37,429 INFO Epoch 15 TRAIN info lr 0.00044706519448586894
2023-02-21 12:28:37,434 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 12:28:37,563 INFO Epoch 14 CV info cv_loss 8.28343307908206
2023-02-21 12:28:37,565 INFO Epoch 15 TRAIN info lr 0.0004469312241674465
2023-02-21 12:28:37,569 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 12:28:37,775 INFO Epoch 14 CV info cv_loss 8.283433078401504
2023-02-21 12:28:37,776 INFO Epoch 15 TRAIN info lr 0.0004470115919029179
2023-02-21 12:28:37,781 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 12:28:40,136 INFO Epoch 14 CV info cv_loss 8.283433078879616
2023-02-21 12:28:40,137 INFO Epoch 15 TRAIN info lr 0.00044702052432781745
2023-02-21 12:28:40,140 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 12:28:40,455 INFO Epoch 14 CV info cv_loss 8.283433078591026
2023-02-21 12:28:40,456 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/14.pt
2023-02-21 12:28:41,139 INFO Epoch 14 CV info cv_loss 8.283433078483343
2023-02-21 12:28:41,141 INFO Epoch 15 TRAIN info lr 0.0004469455085967575
2023-02-21 12:28:41,144 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 12:28:42,444 INFO Epoch 14 CV info cv_loss 8.283433078591026
2023-02-21 12:28:42,445 INFO Epoch 15 TRAIN info lr 0.00044705268549177074
2023-02-21 12:28:42,449 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 12:28:43,762 INFO Epoch 15 TRAIN info lr 0.00044699372865940134
2023-02-21 12:28:43,767 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 12:29:56,871 DEBUG TRAIN Batch 15/0 loss 12.162313 loss_att 11.451536 loss_ctc 14.457822 loss_rnnt 11.823277 hw_loss 0.328355 lr 0.00044693 rank 7
2023-02-21 12:29:56,873 DEBUG TRAIN Batch 15/0 loss 13.219018 loss_att 11.388081 loss_ctc 16.549730 loss_rnnt 12.836130 hw_loss 0.571837 lr 0.00044706 rank 1
2023-02-21 12:29:56,873 DEBUG TRAIN Batch 15/0 loss 10.532589 loss_att 9.849402 loss_ctc 12.079592 loss_rnnt 10.238246 hw_loss 0.421338 lr 0.00044702 rank 4
2023-02-21 12:29:56,874 DEBUG TRAIN Batch 15/0 loss 16.227119 loss_att 14.012815 loss_ctc 17.790272 loss_rnnt 16.303593 hw_loss 0.296190 lr 0.00044702 rank 6
2023-02-21 12:29:56,881 DEBUG TRAIN Batch 15/0 loss 9.145741 loss_att 8.145904 loss_ctc 10.110231 loss_rnnt 9.018580 hw_loss 0.372244 lr 0.00044701 rank 2
2023-02-21 12:29:56,883 DEBUG TRAIN Batch 15/0 loss 10.282669 loss_att 10.548442 loss_ctc 12.864994 loss_rnnt 9.643421 hw_loss 0.453343 lr 0.00044694 rank 5
2023-02-21 12:29:56,886 DEBUG TRAIN Batch 15/0 loss 11.736568 loss_att 10.195735 loss_ctc 13.400496 loss_rnnt 11.578461 hw_loss 0.458284 lr 0.00044705 rank 3
2023-02-21 12:29:56,893 DEBUG TRAIN Batch 15/0 loss 11.192072 loss_att 10.439982 loss_ctc 12.721794 loss_rnnt 10.918038 hw_loss 0.413416 lr 0.00044699 rank 0
2023-02-21 12:31:13,370 DEBUG TRAIN Batch 15/100 loss 22.703243 loss_att 29.842192 loss_ctc 32.295601 loss_rnnt 19.983627 hw_loss 0.024082 lr 0.00044675 rank 7
2023-02-21 12:31:13,373 DEBUG TRAIN Batch 15/100 loss 10.483373 loss_att 12.139708 loss_ctc 14.801163 loss_rnnt 9.542135 hw_loss 0.064246 lr 0.00044681 rank 0
2023-02-21 12:31:13,374 DEBUG TRAIN Batch 15/100 loss 7.872842 loss_att 11.853434 loss_ctc 11.653960 loss_rnnt 6.559602 hw_loss 0.024323 lr 0.00044688 rank 1
2023-02-21 12:31:13,376 DEBUG TRAIN Batch 15/100 loss 9.952461 loss_att 13.386868 loss_ctc 14.409561 loss_rnnt 8.616566 hw_loss 0.102628 lr 0.00044685 rank 4
2023-02-21 12:31:13,380 DEBUG TRAIN Batch 15/100 loss 14.671896 loss_att 18.319782 loss_ctc 16.932096 loss_rnnt 13.579210 hw_loss 0.115776 lr 0.00044687 rank 3
2023-02-21 12:31:13,381 DEBUG TRAIN Batch 15/100 loss 17.125198 loss_att 20.004805 loss_ctc 24.927166 loss_rnnt 15.419654 hw_loss 0.167552 lr 0.00044683 rank 2
2023-02-21 12:31:13,383 DEBUG TRAIN Batch 15/100 loss 5.956786 loss_att 10.929138 loss_ctc 6.663862 loss_rnnt 4.841347 hw_loss 0.050046 lr 0.00044684 rank 6
2023-02-21 12:31:13,385 DEBUG TRAIN Batch 15/100 loss 17.192822 loss_att 23.786034 loss_ctc 22.054256 loss_rnnt 15.213112 hw_loss 0.024142 lr 0.00044677 rank 5
2023-02-21 12:32:30,787 DEBUG TRAIN Batch 15/200 loss 11.971914 loss_att 16.032623 loss_ctc 16.270901 loss_rnnt 10.521182 hw_loss 0.122609 lr 0.00044665 rank 2
2023-02-21 12:32:30,788 DEBUG TRAIN Batch 15/200 loss 3.863665 loss_att 7.776654 loss_ctc 7.171020 loss_rnnt 2.592057 hw_loss 0.090055 lr 0.00044666 rank 6
2023-02-21 12:32:30,788 DEBUG TRAIN Batch 15/200 loss 14.912611 loss_att 14.558662 loss_ctc 22.702635 loss_rnnt 13.824457 hw_loss 0.225511 lr 0.00044657 rank 7
2023-02-21 12:32:30,792 DEBUG TRAIN Batch 15/200 loss 16.730530 loss_att 18.128716 loss_ctc 26.065437 loss_rnnt 15.146208 hw_loss 0.112556 lr 0.00044669 rank 3
2023-02-21 12:32:30,795 DEBUG TRAIN Batch 15/200 loss 8.050751 loss_att 10.295462 loss_ctc 9.662716 loss_rnnt 7.278204 hw_loss 0.203767 lr 0.00044667 rank 4
2023-02-21 12:32:30,796 DEBUG TRAIN Batch 15/200 loss 11.521372 loss_att 12.717748 loss_ctc 20.962965 loss_rnnt 9.976445 hw_loss 0.087699 lr 0.00044659 rank 5
2023-02-21 12:32:30,798 DEBUG TRAIN Batch 15/200 loss 8.298915 loss_att 10.065475 loss_ctc 9.050346 loss_rnnt 7.833408 hw_loss 0.022508 lr 0.00044671 rank 1
2023-02-21 12:32:30,799 DEBUG TRAIN Batch 15/200 loss 20.031582 loss_att 22.911106 loss_ctc 27.601002 loss_rnnt 18.395611 hw_loss 0.095267 lr 0.00044664 rank 0
2023-02-21 12:33:49,851 DEBUG TRAIN Batch 15/300 loss 7.949284 loss_att 10.965478 loss_ctc 10.759687 loss_rnnt 6.841116 hw_loss 0.244141 lr 0.00044639 rank 7
2023-02-21 12:33:49,850 DEBUG TRAIN Batch 15/300 loss 11.681793 loss_att 14.713177 loss_ctc 14.478265 loss_rnnt 10.620834 hw_loss 0.153409 lr 0.00044649 rank 4
2023-02-21 12:33:49,856 DEBUG TRAIN Batch 15/300 loss 6.381042 loss_att 8.648228 loss_ctc 10.349144 loss_rnnt 5.195101 hw_loss 0.381418 lr 0.00044646 rank 0
2023-02-21 12:33:49,859 DEBUG TRAIN Batch 15/300 loss 21.328335 loss_att 25.840651 loss_ctc 29.030916 loss_rnnt 19.278252 hw_loss 0.226147 lr 0.00044648 rank 6
2023-02-21 12:33:49,859 DEBUG TRAIN Batch 15/300 loss 10.363034 loss_att 11.049277 loss_ctc 11.717724 loss_rnnt 9.968927 hw_loss 0.142936 lr 0.00044653 rank 1
2023-02-21 12:33:49,859 DEBUG TRAIN Batch 15/300 loss 3.619558 loss_att 8.238553 loss_ctc 6.856745 loss_rnnt 2.185829 hw_loss 0.146820 lr 0.00044647 rank 2
2023-02-21 12:33:49,861 DEBUG TRAIN Batch 15/300 loss 13.698021 loss_att 18.124392 loss_ctc 19.072964 loss_rnnt 12.009839 hw_loss 0.161714 lr 0.00044641 rank 5
2023-02-21 12:33:49,865 DEBUG TRAIN Batch 15/300 loss 6.310671 loss_att 10.168918 loss_ctc 9.372291 loss_rnnt 5.082551 hw_loss 0.090477 lr 0.00044652 rank 3
2023-02-21 12:35:10,514 DEBUG TRAIN Batch 15/400 loss 10.241254 loss_att 13.467169 loss_ctc 14.411346 loss_rnnt 8.982132 hw_loss 0.108613 lr 0.00044631 rank 4
2023-02-21 12:35:10,519 DEBUG TRAIN Batch 15/400 loss 7.285954 loss_att 9.832867 loss_ctc 10.093325 loss_rnnt 6.309240 hw_loss 0.174402 lr 0.00044628 rank 0
2023-02-21 12:35:10,520 DEBUG TRAIN Batch 15/400 loss 14.585649 loss_att 17.415501 loss_ctc 15.383131 loss_rnnt 13.876071 hw_loss 0.069894 lr 0.00044635 rank 1
2023-02-21 12:35:10,522 DEBUG TRAIN Batch 15/400 loss 16.990953 loss_att 20.410429 loss_ctc 20.620136 loss_rnnt 15.735367 hw_loss 0.164630 lr 0.00044634 rank 3
2023-02-21 12:35:10,523 DEBUG TRAIN Batch 15/400 loss 12.342607 loss_att 17.401447 loss_ctc 16.481956 loss_rnnt 10.668888 hw_loss 0.206322 lr 0.00044623 rank 5
2023-02-21 12:35:10,524 DEBUG TRAIN Batch 15/400 loss 23.162796 loss_att 26.652885 loss_ctc 31.280872 loss_rnnt 21.320395 hw_loss 0.116202 lr 0.00044631 rank 6
2023-02-21 12:35:10,538 DEBUG TRAIN Batch 15/400 loss 5.405925 loss_att 7.493934 loss_ctc 5.642123 loss_rnnt 4.883651 hw_loss 0.137211 lr 0.00044630 rank 2
2023-02-21 12:35:10,544 DEBUG TRAIN Batch 15/400 loss 12.732335 loss_att 13.116373 loss_ctc 14.136887 loss_rnnt 12.366759 hw_loss 0.190304 lr 0.00044622 rank 7
2023-02-21 12:36:28,868 DEBUG TRAIN Batch 15/500 loss 13.116309 loss_att 14.116855 loss_ctc 14.912960 loss_rnnt 12.598844 hw_loss 0.145881 lr 0.00044612 rank 2
2023-02-21 12:36:28,872 DEBUG TRAIN Batch 15/500 loss 11.170667 loss_att 11.624589 loss_ctc 14.183588 loss_rnnt 10.638458 hw_loss 0.074439 lr 0.00044604 rank 7
2023-02-21 12:36:28,875 DEBUG TRAIN Batch 15/500 loss 15.196905 loss_att 15.989330 loss_ctc 17.268997 loss_rnnt 14.695557 hw_loss 0.124846 lr 0.00044613 rank 4
2023-02-21 12:36:28,878 DEBUG TRAIN Batch 15/500 loss 9.288599 loss_att 12.845626 loss_ctc 10.505960 loss_rnnt 8.205278 hw_loss 0.392999 lr 0.00044610 rank 0
2023-02-21 12:36:28,878 DEBUG TRAIN Batch 15/500 loss 10.615906 loss_att 11.476498 loss_ctc 17.415476 loss_rnnt 9.502811 hw_loss 0.064438 lr 0.00044617 rank 1
2023-02-21 12:36:28,878 DEBUG TRAIN Batch 15/500 loss 10.749764 loss_att 13.976390 loss_ctc 12.155249 loss_rnnt 9.820202 hw_loss 0.181575 lr 0.00044613 rank 6
2023-02-21 12:36:28,883 DEBUG TRAIN Batch 15/500 loss 9.480142 loss_att 11.532043 loss_ctc 10.637558 loss_rnnt 8.809216 hw_loss 0.199167 lr 0.00044605 rank 5
2023-02-21 12:36:28,924 DEBUG TRAIN Batch 15/500 loss 10.681955 loss_att 13.775340 loss_ctc 14.611450 loss_rnnt 9.479095 hw_loss 0.112969 lr 0.00044616 rank 3
2023-02-21 12:37:48,607 DEBUG TRAIN Batch 15/600 loss 7.585743 loss_att 8.749713 loss_ctc 8.867614 loss_rnnt 7.119636 hw_loss 0.116997 lr 0.00044598 rank 3
2023-02-21 12:37:48,608 DEBUG TRAIN Batch 15/600 loss 18.198317 loss_att 19.607948 loss_ctc 23.366484 loss_rnnt 17.137405 hw_loss 0.168550 lr 0.00044596 rank 4
2023-02-21 12:37:48,608 DEBUG TRAIN Batch 15/600 loss 11.566618 loss_att 12.977783 loss_ctc 15.708719 loss_rnnt 10.659988 hw_loss 0.135216 lr 0.00044594 rank 2
2023-02-21 12:37:48,609 DEBUG TRAIN Batch 15/600 loss 10.500566 loss_att 11.318310 loss_ctc 12.233043 loss_rnnt 9.977679 hw_loss 0.240641 lr 0.00044600 rank 1
2023-02-21 12:37:48,612 DEBUG TRAIN Batch 15/600 loss 10.174288 loss_att 11.487942 loss_ctc 11.964520 loss_rnnt 9.611450 hw_loss 0.115141 lr 0.00044595 rank 6
2023-02-21 12:37:48,614 DEBUG TRAIN Batch 15/600 loss 9.634278 loss_att 9.175038 loss_ctc 11.636003 loss_rnnt 9.420848 hw_loss 0.071967 lr 0.00044586 rank 7
2023-02-21 12:37:48,618 DEBUG TRAIN Batch 15/600 loss 11.561182 loss_att 14.092014 loss_ctc 17.047169 loss_rnnt 10.269157 hw_loss 0.101988 lr 0.00044588 rank 5
2023-02-21 12:37:48,658 DEBUG TRAIN Batch 15/600 loss 11.550482 loss_att 11.724414 loss_ctc 14.701291 loss_rnnt 10.871037 hw_loss 0.421032 lr 0.00044592 rank 0
2023-02-21 12:39:09,523 DEBUG TRAIN Batch 15/700 loss 5.115316 loss_att 8.239777 loss_ctc 7.118644 loss_rnnt 4.165569 hw_loss 0.108271 lr 0.00044568 rank 7
2023-02-21 12:39:09,525 DEBUG TRAIN Batch 15/700 loss 9.376736 loss_att 14.837013 loss_ctc 14.438385 loss_rnnt 7.529505 hw_loss 0.150542 lr 0.00044576 rank 2
2023-02-21 12:39:09,529 DEBUG TRAIN Batch 15/700 loss 8.425453 loss_att 10.099157 loss_ctc 9.459606 loss_rnnt 7.917478 hw_loss 0.066275 lr 0.00044582 rank 1
2023-02-21 12:39:09,531 DEBUG TRAIN Batch 15/700 loss 9.688699 loss_att 12.636652 loss_ctc 10.480645 loss_rnnt 8.958113 hw_loss 0.066377 lr 0.00044575 rank 0
2023-02-21 12:39:09,533 DEBUG TRAIN Batch 15/700 loss 3.784565 loss_att 5.208217 loss_ctc 4.362445 loss_rnnt 3.343095 hw_loss 0.149418 lr 0.00044577 rank 6
2023-02-21 12:39:09,537 DEBUG TRAIN Batch 15/700 loss 16.818865 loss_att 19.299477 loss_ctc 21.888321 loss_rnnt 15.574271 hw_loss 0.136016 lr 0.00044581 rank 3
2023-02-21 12:39:09,538 DEBUG TRAIN Batch 15/700 loss 13.495209 loss_att 19.988022 loss_ctc 18.747072 loss_rnnt 11.423827 hw_loss 0.136070 lr 0.00044570 rank 5
2023-02-21 12:39:09,548 DEBUG TRAIN Batch 15/700 loss 21.386499 loss_att 28.664806 loss_ctc 29.602453 loss_rnnt 18.739365 hw_loss 0.180027 lr 0.00044578 rank 4
2023-02-21 12:40:27,671 DEBUG TRAIN Batch 15/800 loss 13.230494 loss_att 12.900082 loss_ctc 14.416338 loss_rnnt 13.074862 hw_loss 0.119252 lr 0.00044560 rank 4
2023-02-21 12:40:27,673 DEBUG TRAIN Batch 15/800 loss 12.298382 loss_att 13.909554 loss_ctc 12.832549 loss_rnnt 11.844810 hw_loss 0.112714 lr 0.00044551 rank 7
2023-02-21 12:40:27,676 DEBUG TRAIN Batch 15/800 loss 6.000541 loss_att 7.524694 loss_ctc 10.952632 loss_rnnt 5.020816 hw_loss 0.027404 lr 0.00044559 rank 2
2023-02-21 12:40:27,677 DEBUG TRAIN Batch 15/800 loss 10.183659 loss_att 12.682203 loss_ctc 14.763232 loss_rnnt 8.997095 hw_loss 0.142959 lr 0.00044564 rank 1
2023-02-21 12:40:27,680 DEBUG TRAIN Batch 15/800 loss 12.590639 loss_att 16.363503 loss_ctc 18.668751 loss_rnnt 10.988531 hw_loss 0.069605 lr 0.00044563 rank 3
2023-02-21 12:40:27,680 DEBUG TRAIN Batch 15/800 loss 13.236249 loss_att 13.093211 loss_ctc 14.833618 loss_rnnt 13.013560 hw_loss 0.071839 lr 0.00044557 rank 0
2023-02-21 12:40:27,680 DEBUG TRAIN Batch 15/800 loss 8.616633 loss_att 11.584679 loss_ctc 8.135162 loss_rnnt 8.049837 hw_loss 0.070093 lr 0.00044552 rank 5
2023-02-21 12:40:27,725 DEBUG TRAIN Batch 15/800 loss 15.078571 loss_att 17.639856 loss_ctc 14.828657 loss_rnnt 14.584919 hw_loss 0.027595 lr 0.00044560 rank 6
2023-02-21 12:41:45,198 DEBUG TRAIN Batch 15/900 loss 13.551759 loss_att 17.606075 loss_ctc 19.633890 loss_rnnt 11.855635 hw_loss 0.139333 lr 0.00044546 rank 1
2023-02-21 12:41:45,201 DEBUG TRAIN Batch 15/900 loss 14.982685 loss_att 17.985970 loss_ctc 19.955263 loss_rnnt 13.628985 hw_loss 0.168811 lr 0.00044533 rank 7
2023-02-21 12:41:45,202 DEBUG TRAIN Batch 15/900 loss 6.802037 loss_att 9.260918 loss_ctc 9.511306 loss_rnnt 5.892847 hw_loss 0.105333 lr 0.00044541 rank 2
2023-02-21 12:41:45,204 DEBUG TRAIN Batch 15/900 loss 12.240038 loss_att 16.818886 loss_ctc 15.436207 loss_rnnt 10.845686 hw_loss 0.098301 lr 0.00044542 rank 4
2023-02-21 12:41:45,205 DEBUG TRAIN Batch 15/900 loss 7.921349 loss_att 12.577215 loss_ctc 13.001989 loss_rnnt 6.281190 hw_loss 0.059186 lr 0.00044539 rank 0
2023-02-21 12:41:45,205 DEBUG TRAIN Batch 15/900 loss 7.747776 loss_att 9.955276 loss_ctc 11.112222 loss_rnnt 6.756430 hw_loss 0.189850 lr 0.00044545 rank 3
2023-02-21 12:41:45,208 DEBUG TRAIN Batch 15/900 loss 10.507718 loss_att 16.249155 loss_ctc 13.235365 loss_rnnt 8.954535 hw_loss 0.077269 lr 0.00044535 rank 5
2023-02-21 12:41:45,248 DEBUG TRAIN Batch 15/900 loss 7.210952 loss_att 10.815271 loss_ctc 11.275192 loss_rnnt 5.906943 hw_loss 0.077338 lr 0.00044542 rank 6
2023-02-21 12:43:03,058 DEBUG TRAIN Batch 15/1000 loss 9.942984 loss_att 15.570055 loss_ctc 12.280741 loss_rnnt 8.441381 hw_loss 0.120915 lr 0.00044515 rank 7
2023-02-21 12:43:03,064 DEBUG TRAIN Batch 15/1000 loss 15.927544 loss_att 21.063313 loss_ctc 23.471165 loss_rnnt 13.840210 hw_loss 0.101931 lr 0.00044527 rank 3
2023-02-21 12:43:03,065 DEBUG TRAIN Batch 15/1000 loss 13.880166 loss_att 17.313705 loss_ctc 21.788149 loss_rnnt 12.113214 hw_loss 0.048458 lr 0.00044524 rank 6
2023-02-21 12:43:03,066 DEBUG TRAIN Batch 15/1000 loss 10.216401 loss_att 11.658010 loss_ctc 11.504862 loss_rnnt 9.664927 hw_loss 0.171295 lr 0.00044522 rank 0
2023-02-21 12:43:03,071 DEBUG TRAIN Batch 15/1000 loss 14.826063 loss_att 18.463005 loss_ctc 17.795750 loss_rnnt 13.676374 hw_loss 0.049392 lr 0.00044517 rank 5
2023-02-21 12:43:03,075 DEBUG TRAIN Batch 15/1000 loss 4.519929 loss_att 7.403328 loss_ctc 6.470977 loss_rnnt 3.609890 hw_loss 0.137287 lr 0.00044529 rank 1
2023-02-21 12:43:03,085 DEBUG TRAIN Batch 15/1000 loss 10.055233 loss_att 10.943783 loss_ctc 14.477017 loss_rnnt 9.150566 hw_loss 0.257600 lr 0.00044525 rank 4
2023-02-21 12:43:03,094 DEBUG TRAIN Batch 15/1000 loss 12.046372 loss_att 15.475372 loss_ctc 15.271980 loss_rnnt 10.866776 hw_loss 0.119464 lr 0.00044523 rank 2
2023-02-21 12:44:23,956 DEBUG TRAIN Batch 15/1100 loss 6.173610 loss_att 8.453795 loss_ctc 7.788305 loss_rnnt 5.386635 hw_loss 0.216835 lr 0.00044498 rank 7
2023-02-21 12:44:23,959 DEBUG TRAIN Batch 15/1100 loss 5.021413 loss_att 8.004997 loss_ctc 8.300615 loss_rnnt 3.932800 hw_loss 0.102506 lr 0.00044507 rank 4
2023-02-21 12:44:23,963 DEBUG TRAIN Batch 15/1100 loss 11.325055 loss_att 14.341374 loss_ctc 14.801310 loss_rnnt 10.201116 hw_loss 0.107203 lr 0.00044506 rank 2
2023-02-21 12:44:23,967 DEBUG TRAIN Batch 15/1100 loss 4.543125 loss_att 7.474222 loss_ctc 6.242537 loss_rnnt 3.697369 hw_loss 0.061779 lr 0.00044499 rank 5
2023-02-21 12:44:23,966 DEBUG TRAIN Batch 15/1100 loss 13.994954 loss_att 17.274797 loss_ctc 21.851587 loss_rnnt 12.207208 hw_loss 0.157923 lr 0.00044511 rank 1
2023-02-21 12:44:23,970 DEBUG TRAIN Batch 15/1100 loss 18.361425 loss_att 23.159327 loss_ctc 26.051117 loss_rnnt 16.340092 hw_loss 0.068365 lr 0.00044507 rank 6
2023-02-21 12:44:23,971 DEBUG TRAIN Batch 15/1100 loss 7.498592 loss_att 13.358735 loss_ctc 11.482899 loss_rnnt 5.714190 hw_loss 0.152123 lr 0.00044510 rank 3
2023-02-21 12:44:24,017 DEBUG TRAIN Batch 15/1100 loss 12.950481 loss_att 13.674763 loss_ctc 13.882957 loss_rnnt 12.599089 hw_loss 0.154137 lr 0.00044504 rank 0
2023-02-21 12:45:42,483 DEBUG TRAIN Batch 15/1200 loss 7.579954 loss_att 10.123721 loss_ctc 9.241129 loss_rnnt 6.826504 hw_loss 0.043514 lr 0.00044480 rank 7
2023-02-21 12:45:42,489 DEBUG TRAIN Batch 15/1200 loss 21.077909 loss_att 19.589615 loss_ctc 23.816160 loss_rnnt 20.902908 hw_loss 0.201671 lr 0.00044493 rank 1
2023-02-21 12:45:42,490 DEBUG TRAIN Batch 15/1200 loss 6.784790 loss_att 8.851059 loss_ctc 9.913065 loss_rnnt 5.869915 hw_loss 0.158471 lr 0.00044490 rank 4
2023-02-21 12:45:42,492 DEBUG TRAIN Batch 15/1200 loss 18.017000 loss_att 19.936882 loss_ctc 26.341610 loss_rnnt 16.438129 hw_loss 0.159273 lr 0.00044488 rank 2
2023-02-21 12:45:42,493 DEBUG TRAIN Batch 15/1200 loss 11.377261 loss_att 10.666220 loss_ctc 15.494639 loss_rnnt 10.861523 hw_loss 0.204306 lr 0.00044489 rank 6
2023-02-21 12:45:42,497 DEBUG TRAIN Batch 15/1200 loss 9.984036 loss_att 14.284897 loss_ctc 13.399573 loss_rnnt 8.652321 hw_loss 0.030259 lr 0.00044482 rank 5
2023-02-21 12:45:42,498 DEBUG TRAIN Batch 15/1200 loss 16.702934 loss_att 18.487989 loss_ctc 22.473675 loss_rnnt 15.459875 hw_loss 0.218653 lr 0.00044486 rank 0
2023-02-21 12:45:42,500 DEBUG TRAIN Batch 15/1200 loss 14.616269 loss_att 17.536690 loss_ctc 19.600235 loss_rnnt 13.159686 hw_loss 0.389942 lr 0.00044492 rank 3
2023-02-21 12:47:00,322 DEBUG TRAIN Batch 15/1300 loss 13.641777 loss_att 15.847095 loss_ctc 19.009655 loss_rnnt 12.382458 hw_loss 0.192257 lr 0.00044471 rank 2
2023-02-21 12:47:00,329 DEBUG TRAIN Batch 15/1300 loss 14.656425 loss_att 16.535227 loss_ctc 16.774862 loss_rnnt 13.934605 hw_loss 0.119249 lr 0.00044471 rank 6
2023-02-21 12:47:00,329 DEBUG TRAIN Batch 15/1300 loss 4.983637 loss_att 8.363881 loss_ctc 7.814237 loss_rnnt 3.829407 hw_loss 0.188941 lr 0.00044476 rank 1
2023-02-21 12:47:00,331 DEBUG TRAIN Batch 15/1300 loss 13.527493 loss_att 12.693347 loss_ctc 17.189600 loss_rnnt 13.062224 hw_loss 0.269655 lr 0.00044472 rank 4
2023-02-21 12:47:00,332 DEBUG TRAIN Batch 15/1300 loss 12.849590 loss_att 13.216713 loss_ctc 11.063212 loss_rnnt 12.882979 hw_loss 0.246321 lr 0.00044463 rank 7
2023-02-21 12:47:00,333 DEBUG TRAIN Batch 15/1300 loss 3.920015 loss_att 5.814799 loss_ctc 6.656842 loss_rnnt 3.120099 hw_loss 0.105091 lr 0.00044464 rank 5
2023-02-21 12:47:00,339 DEBUG TRAIN Batch 15/1300 loss 10.542726 loss_att 14.015287 loss_ctc 12.412003 loss_rnnt 9.574480 hw_loss 0.045932 lr 0.00044469 rank 0
2023-02-21 12:47:00,340 DEBUG TRAIN Batch 15/1300 loss 9.107311 loss_att 11.826313 loss_ctc 9.937724 loss_rnnt 8.394299 hw_loss 0.109671 lr 0.00044475 rank 3
2023-02-21 12:48:19,854 DEBUG TRAIN Batch 15/1400 loss 4.723997 loss_att 6.770836 loss_ctc 4.607013 loss_rnnt 4.187396 hw_loss 0.267809 lr 0.00044445 rank 7
2023-02-21 12:48:19,855 DEBUG TRAIN Batch 15/1400 loss 14.219677 loss_att 19.811693 loss_ctc 18.291285 loss_rnnt 12.524467 hw_loss 0.063610 lr 0.00044453 rank 2
2023-02-21 12:48:19,857 DEBUG TRAIN Batch 15/1400 loss 10.014173 loss_att 14.526367 loss_ctc 12.742310 loss_rnnt 8.672628 hw_loss 0.141290 lr 0.00044458 rank 1
2023-02-21 12:48:19,857 DEBUG TRAIN Batch 15/1400 loss 11.456150 loss_att 17.585825 loss_ctc 16.325043 loss_rnnt 9.512678 hw_loss 0.128160 lr 0.00044457 rank 3
2023-02-21 12:48:19,859 DEBUG TRAIN Batch 15/1400 loss 10.318755 loss_att 11.219858 loss_ctc 15.217834 loss_rnnt 9.441698 hw_loss 0.081798 lr 0.00044454 rank 4
2023-02-21 12:48:19,860 DEBUG TRAIN Batch 15/1400 loss 9.620282 loss_att 11.562037 loss_ctc 15.705328 loss_rnnt 8.292511 hw_loss 0.240151 lr 0.00044454 rank 6
2023-02-21 12:48:19,865 DEBUG TRAIN Batch 15/1400 loss 8.605479 loss_att 11.667785 loss_ctc 9.360038 loss_rnnt 7.837224 hw_loss 0.103475 lr 0.00044446 rank 5
2023-02-21 12:48:19,865 DEBUG TRAIN Batch 15/1400 loss 10.988428 loss_att 18.211489 loss_ctc 16.100630 loss_rnnt 8.820797 hw_loss 0.077609 lr 0.00044451 rank 0
2023-02-21 12:49:39,200 DEBUG TRAIN Batch 15/1500 loss 8.988547 loss_att 12.084639 loss_ctc 10.809614 loss_rnnt 7.977291 hw_loss 0.279806 lr 0.00044437 rank 4
2023-02-21 12:49:39,204 DEBUG TRAIN Batch 15/1500 loss 20.259508 loss_att 30.426277 loss_ctc 32.709839 loss_rnnt 16.500933 hw_loss 0.122209 lr 0.00044428 rank 7
2023-02-21 12:49:39,206 DEBUG TRAIN Batch 15/1500 loss 9.149467 loss_att 11.858498 loss_ctc 11.940081 loss_rnnt 8.136886 hw_loss 0.185049 lr 0.00044434 rank 0
2023-02-21 12:49:39,207 DEBUG TRAIN Batch 15/1500 loss 11.782846 loss_att 17.479969 loss_ctc 15.406966 loss_rnnt 10.047265 hw_loss 0.211764 lr 0.00044441 rank 1
2023-02-21 12:49:39,207 DEBUG TRAIN Batch 15/1500 loss 4.567605 loss_att 7.837608 loss_ctc 5.235602 loss_rnnt 3.745191 hw_loss 0.148774 lr 0.00044435 rank 2
2023-02-21 12:49:39,208 DEBUG TRAIN Batch 15/1500 loss 13.742005 loss_att 14.547777 loss_ctc 20.567150 loss_rnnt 12.655920 hw_loss 0.027960 lr 0.00044436 rank 6
2023-02-21 12:49:39,210 DEBUG TRAIN Batch 15/1500 loss 10.535349 loss_att 10.883932 loss_ctc 14.244432 loss_rnnt 9.922264 hw_loss 0.091542 lr 0.00044429 rank 5
2023-02-21 12:49:39,212 DEBUG TRAIN Batch 15/1500 loss 9.287648 loss_att 12.924334 loss_ctc 12.172522 loss_rnnt 8.150058 hw_loss 0.048006 lr 0.00044439 rank 3
2023-02-21 12:50:58,423 DEBUG TRAIN Batch 15/1600 loss 14.077771 loss_att 19.315044 loss_ctc 18.956907 loss_rnnt 12.273718 hw_loss 0.198840 lr 0.00044419 rank 4
2023-02-21 12:50:58,425 DEBUG TRAIN Batch 15/1600 loss 7.053403 loss_att 9.883040 loss_ctc 8.180776 loss_rnnt 6.305525 hw_loss 0.059314 lr 0.00044410 rank 7
2023-02-21 12:50:58,427 DEBUG TRAIN Batch 15/1600 loss 16.246935 loss_att 18.224478 loss_ctc 21.998287 loss_rnnt 14.994764 hw_loss 0.168404 lr 0.00044418 rank 2
2023-02-21 12:50:58,429 DEBUG TRAIN Batch 15/1600 loss 8.548297 loss_att 10.637562 loss_ctc 9.517730 loss_rnnt 7.895872 hw_loss 0.197465 lr 0.00044419 rank 6
2023-02-21 12:50:58,429 DEBUG TRAIN Batch 15/1600 loss 3.806276 loss_att 6.642602 loss_ctc 3.584601 loss_rnnt 3.244631 hw_loss 0.044880 lr 0.00044423 rank 1
2023-02-21 12:50:58,431 DEBUG TRAIN Batch 15/1600 loss 10.385103 loss_att 14.254229 loss_ctc 16.275749 loss_rnnt 8.655546 hw_loss 0.319338 lr 0.00044422 rank 3
2023-02-21 12:50:58,432 DEBUG TRAIN Batch 15/1600 loss 10.170730 loss_att 15.469858 loss_ctc 13.898166 loss_rnnt 8.544188 hw_loss 0.130733 lr 0.00044411 rank 5
2023-02-21 12:50:58,433 DEBUG TRAIN Batch 15/1600 loss 6.178888 loss_att 9.078716 loss_ctc 7.223572 loss_rnnt 5.343558 hw_loss 0.217636 lr 0.00044416 rank 0
2023-02-21 12:52:18,304 DEBUG TRAIN Batch 15/1700 loss 23.552998 loss_att 28.692347 loss_ctc 33.479042 loss_rnnt 21.161798 hw_loss 0.074728 lr 0.00044402 rank 4
2023-02-21 12:52:18,305 DEBUG TRAIN Batch 15/1700 loss 12.809923 loss_att 15.628709 loss_ctc 15.810555 loss_rnnt 11.822603 hw_loss 0.044021 lr 0.00044392 rank 7
2023-02-21 12:52:18,308 DEBUG TRAIN Batch 15/1700 loss 11.189852 loss_att 13.045469 loss_ctc 14.120738 loss_rnnt 10.411682 hw_loss 0.030489 lr 0.00044406 rank 1
2023-02-21 12:52:18,313 DEBUG TRAIN Batch 15/1700 loss 5.627921 loss_att 11.278801 loss_ctc 7.073247 loss_rnnt 4.288871 hw_loss 0.030305 lr 0.00044400 rank 2
2023-02-21 12:52:18,314 DEBUG TRAIN Batch 15/1700 loss 7.730622 loss_att 10.242535 loss_ctc 7.898193 loss_rnnt 7.189636 hw_loss 0.030489 lr 0.00044401 rank 6
2023-02-21 12:52:18,316 DEBUG TRAIN Batch 15/1700 loss 11.278991 loss_att 13.114955 loss_ctc 15.150757 loss_rnnt 10.351995 hw_loss 0.081689 lr 0.00044399 rank 0
2023-02-21 12:52:18,345 DEBUG TRAIN Batch 15/1700 loss 12.467546 loss_att 16.405199 loss_ctc 18.695541 loss_rnnt 10.787748 hw_loss 0.116000 lr 0.00044404 rank 3
2023-02-21 12:52:18,354 DEBUG TRAIN Batch 15/1700 loss 5.820490 loss_att 8.100878 loss_ctc 6.947653 loss_rnnt 5.143353 hw_loss 0.132695 lr 0.00044394 rank 5
2023-02-21 12:53:38,927 DEBUG TRAIN Batch 15/1800 loss 6.986398 loss_att 9.866648 loss_ctc 8.122802 loss_rnnt 6.175265 hw_loss 0.156680 lr 0.00044384 rank 4
2023-02-21 12:53:38,928 DEBUG TRAIN Batch 15/1800 loss 11.790576 loss_att 15.399210 loss_ctc 16.646271 loss_rnnt 10.349892 hw_loss 0.134123 lr 0.00044383 rank 2
2023-02-21 12:53:38,930 DEBUG TRAIN Batch 15/1800 loss 13.669788 loss_att 18.214266 loss_ctc 15.733716 loss_rnnt 12.430842 hw_loss 0.102863 lr 0.00044375 rank 7
2023-02-21 12:53:38,931 DEBUG TRAIN Batch 15/1800 loss 10.668691 loss_att 11.424670 loss_ctc 14.095873 loss_rnnt 9.989027 hw_loss 0.134082 lr 0.00044387 rank 3
2023-02-21 12:53:38,932 DEBUG TRAIN Batch 15/1800 loss 7.192921 loss_att 11.418243 loss_ctc 10.314783 loss_rnnt 5.826779 hw_loss 0.196554 lr 0.00044388 rank 1
2023-02-21 12:53:38,932 DEBUG TRAIN Batch 15/1800 loss 12.372721 loss_att 11.949811 loss_ctc 17.714893 loss_rnnt 11.697062 hw_loss 0.089908 lr 0.00044381 rank 0
2023-02-21 12:53:38,934 DEBUG TRAIN Batch 15/1800 loss 4.383146 loss_att 5.519488 loss_ctc 5.932530 loss_rnnt 3.878201 hw_loss 0.133297 lr 0.00044376 rank 5
2023-02-21 12:53:38,979 DEBUG TRAIN Batch 15/1800 loss 11.772771 loss_att 12.582078 loss_ctc 18.010609 loss_rnnt 10.764405 hw_loss 0.027735 lr 0.00044384 rank 6
2023-02-21 12:54:57,278 DEBUG TRAIN Batch 15/1900 loss 12.952688 loss_att 12.343264 loss_ctc 16.005964 loss_rnnt 12.516172 hw_loss 0.283682 lr 0.00044369 rank 3
2023-02-21 12:54:57,280 DEBUG TRAIN Batch 15/1900 loss 12.120094 loss_att 13.996872 loss_ctc 16.819569 loss_rnnt 11.004524 hw_loss 0.213032 lr 0.00044364 rank 0
2023-02-21 12:54:57,281 DEBUG TRAIN Batch 15/1900 loss 7.324056 loss_att 9.979681 loss_ctc 12.817596 loss_rnnt 5.946783 hw_loss 0.213142 lr 0.00044371 rank 1
2023-02-21 12:54:57,281 DEBUG TRAIN Batch 15/1900 loss 12.383963 loss_att 10.908707 loss_ctc 15.238838 loss_rnnt 12.068759 hw_loss 0.430511 lr 0.00044367 rank 4
2023-02-21 12:54:57,282 DEBUG TRAIN Batch 15/1900 loss 15.134522 loss_att 14.101869 loss_ctc 16.998983 loss_rnnt 14.923320 hw_loss 0.317134 lr 0.00044358 rank 7
2023-02-21 12:54:57,283 DEBUG TRAIN Batch 15/1900 loss 5.953412 loss_att 8.761442 loss_ctc 5.896984 loss_rnnt 5.293533 hw_loss 0.198368 lr 0.00044366 rank 6
2023-02-21 12:54:57,283 DEBUG TRAIN Batch 15/1900 loss 10.134656 loss_att 16.229986 loss_ctc 14.742704 loss_rnnt 8.244152 hw_loss 0.106933 lr 0.00044365 rank 2
2023-02-21 12:54:57,290 DEBUG TRAIN Batch 15/1900 loss 12.507154 loss_att 12.681682 loss_ctc 16.481979 loss_rnnt 11.893630 hw_loss 0.091205 lr 0.00044359 rank 5
2023-02-21 12:56:15,763 DEBUG TRAIN Batch 15/2000 loss 22.093363 loss_att 21.795275 loss_ctc 23.538738 loss_rnnt 21.914547 hw_loss 0.085722 lr 0.00044353 rank 1
2023-02-21 12:56:15,767 DEBUG TRAIN Batch 15/2000 loss 9.112692 loss_att 11.659466 loss_ctc 10.484896 loss_rnnt 8.322479 hw_loss 0.183557 lr 0.00044340 rank 7
2023-02-21 12:56:15,768 DEBUG TRAIN Batch 15/2000 loss 10.786337 loss_att 14.608353 loss_ctc 18.167271 loss_rnnt 8.991920 hw_loss 0.086042 lr 0.00044349 rank 4
2023-02-21 12:56:15,772 DEBUG TRAIN Batch 15/2000 loss 10.102116 loss_att 11.746133 loss_ctc 14.658035 loss_rnnt 9.116505 hw_loss 0.092535 lr 0.00044341 rank 5
2023-02-21 12:56:15,773 DEBUG TRAIN Batch 15/2000 loss 6.914026 loss_att 8.208595 loss_ctc 7.356122 loss_rnnt 6.490093 hw_loss 0.198885 lr 0.00044349 rank 6
2023-02-21 12:56:15,774 DEBUG TRAIN Batch 15/2000 loss 7.835823 loss_att 12.667316 loss_ctc 13.761063 loss_rnnt 6.012596 hw_loss 0.125431 lr 0.00044348 rank 2
2023-02-21 12:56:15,777 DEBUG TRAIN Batch 15/2000 loss 16.752506 loss_att 14.369060 loss_ctc 17.209137 loss_rnnt 17.122042 hw_loss 0.086756 lr 0.00044352 rank 3
2023-02-21 12:56:15,776 DEBUG TRAIN Batch 15/2000 loss 15.609395 loss_att 21.908930 loss_ctc 28.519979 loss_rnnt 12.569885 hw_loss 0.109108 lr 0.00044346 rank 0
2023-02-21 12:57:35,853 DEBUG TRAIN Batch 15/2100 loss 8.288570 loss_att 11.952513 loss_ctc 14.003788 loss_rnnt 6.759341 hw_loss 0.064524 lr 0.00044330 rank 2
2023-02-21 12:57:35,855 DEBUG TRAIN Batch 15/2100 loss 11.440624 loss_att 14.406910 loss_ctc 14.862025 loss_rnnt 10.229128 hw_loss 0.303847 lr 0.00044332 rank 4
2023-02-21 12:57:35,856 DEBUG TRAIN Batch 15/2100 loss 2.955103 loss_att 8.098948 loss_ctc 3.470834 loss_rnnt 1.734054 hw_loss 0.231593 lr 0.00044336 rank 1
2023-02-21 12:57:35,860 DEBUG TRAIN Batch 15/2100 loss 4.502318 loss_att 8.818975 loss_ctc 8.988031 loss_rnnt 2.971392 hw_loss 0.130311 lr 0.00044331 rank 6
2023-02-21 12:57:35,860 DEBUG TRAIN Batch 15/2100 loss 13.691330 loss_att 16.198357 loss_ctc 19.632507 loss_rnnt 12.361137 hw_loss 0.068681 lr 0.00044323 rank 7
2023-02-21 12:57:35,860 DEBUG TRAIN Batch 15/2100 loss 6.161128 loss_att 8.586113 loss_ctc 6.734028 loss_rnnt 5.543791 hw_loss 0.104912 lr 0.00044334 rank 3
2023-02-21 12:57:35,863 DEBUG TRAIN Batch 15/2100 loss 17.038692 loss_att 20.462934 loss_ctc 23.367186 loss_rnnt 15.410437 hw_loss 0.186768 lr 0.00044329 rank 0
2023-02-21 12:57:35,863 DEBUG TRAIN Batch 15/2100 loss 6.751614 loss_att 8.849319 loss_ctc 7.560086 loss_rnnt 6.159327 hw_loss 0.121780 lr 0.00044324 rank 5
2023-02-21 12:58:55,279 DEBUG TRAIN Batch 15/2200 loss 6.409276 loss_att 9.149532 loss_ctc 10.157353 loss_rnnt 5.331019 hw_loss 0.057114 lr 0.00044314 rank 6
2023-02-21 12:58:55,280 DEBUG TRAIN Batch 15/2200 loss 11.278804 loss_att 14.066864 loss_ctc 11.563355 loss_rnnt 10.658295 hw_loss 0.046795 lr 0.00044305 rank 7
2023-02-21 12:58:55,284 DEBUG TRAIN Batch 15/2200 loss 18.453114 loss_att 21.489925 loss_ctc 22.342663 loss_rnnt 17.199032 hw_loss 0.240211 lr 0.00044311 rank 0
2023-02-21 12:58:55,285 DEBUG TRAIN Batch 15/2200 loss 10.205045 loss_att 13.938464 loss_ctc 11.515093 loss_rnnt 9.179106 hw_loss 0.196089 lr 0.00044314 rank 4
2023-02-21 12:58:55,288 DEBUG TRAIN Batch 15/2200 loss 15.525034 loss_att 17.191164 loss_ctc 22.315590 loss_rnnt 14.123508 hw_loss 0.305423 lr 0.00044313 rank 2
2023-02-21 12:58:55,289 DEBUG TRAIN Batch 15/2200 loss 6.878413 loss_att 9.563063 loss_ctc 10.722832 loss_rnnt 5.763348 hw_loss 0.122896 lr 0.00044318 rank 1
2023-02-21 12:58:55,291 DEBUG TRAIN Batch 15/2200 loss 10.757267 loss_att 13.393837 loss_ctc 14.590347 loss_rnnt 9.652987 hw_loss 0.123545 lr 0.00044307 rank 5
2023-02-21 12:58:55,295 DEBUG TRAIN Batch 15/2200 loss 11.808578 loss_att 15.121420 loss_ctc 16.873419 loss_rnnt 10.427134 hw_loss 0.081683 lr 0.00044317 rank 3
2023-02-21 13:00:14,496 DEBUG TRAIN Batch 15/2300 loss 13.658394 loss_att 18.819477 loss_ctc 15.734224 loss_rnnt 12.304182 hw_loss 0.084784 lr 0.00044297 rank 4
2023-02-21 13:00:14,501 DEBUG TRAIN Batch 15/2300 loss 14.663495 loss_att 17.692680 loss_ctc 24.038271 loss_rnnt 12.774553 hw_loss 0.062124 lr 0.00044301 rank 1
2023-02-21 13:00:14,503 DEBUG TRAIN Batch 15/2300 loss 5.510650 loss_att 8.632981 loss_ctc 7.366529 loss_rnnt 4.592807 hw_loss 0.086112 lr 0.00044288 rank 7
2023-02-21 13:00:14,504 DEBUG TRAIN Batch 15/2300 loss 14.088935 loss_att 17.224182 loss_ctc 19.013027 loss_rnnt 12.669483 hw_loss 0.254731 lr 0.00044296 rank 2
2023-02-21 13:00:14,506 DEBUG TRAIN Batch 15/2300 loss 4.678014 loss_att 6.203072 loss_ctc 7.031291 loss_rnnt 3.959470 hw_loss 0.187054 lr 0.00044294 rank 0
2023-02-21 13:00:14,509 DEBUG TRAIN Batch 15/2300 loss 6.965199 loss_att 13.254586 loss_ctc 12.627117 loss_rnnt 4.897791 hw_loss 0.102390 lr 0.00044289 rank 5
2023-02-21 13:00:14,510 DEBUG TRAIN Batch 15/2300 loss 11.173835 loss_att 12.515844 loss_ctc 15.911284 loss_rnnt 10.209296 hw_loss 0.120894 lr 0.00044297 rank 6
2023-02-21 13:00:14,513 DEBUG TRAIN Batch 15/2300 loss 16.657845 loss_att 18.073301 loss_ctc 19.111202 loss_rnnt 16.007515 hw_loss 0.075231 lr 0.00044300 rank 3
2023-02-21 13:01:33,849 DEBUG TRAIN Batch 15/2400 loss 15.262636 loss_att 19.364292 loss_ctc 19.918018 loss_rnnt 13.754656 hw_loss 0.125494 lr 0.00044271 rank 7
2023-02-21 13:01:33,851 DEBUG TRAIN Batch 15/2400 loss 13.643022 loss_att 16.182570 loss_ctc 16.577034 loss_rnnt 12.646332 hw_loss 0.182959 lr 0.00044278 rank 2
2023-02-21 13:01:33,852 DEBUG TRAIN Batch 15/2400 loss 10.963513 loss_att 12.798884 loss_ctc 13.654902 loss_rnnt 10.212486 hw_loss 0.047067 lr 0.00044280 rank 4
2023-02-21 13:01:33,854 DEBUG TRAIN Batch 15/2400 loss 17.310223 loss_att 17.178200 loss_ctc 20.574694 loss_rnnt 16.800606 hw_loss 0.188921 lr 0.00044277 rank 0
2023-02-21 13:01:33,854 DEBUG TRAIN Batch 15/2400 loss 10.955620 loss_att 14.283158 loss_ctc 17.150660 loss_rnnt 9.438399 hw_loss 0.048201 lr 0.00044279 rank 6
2023-02-21 13:01:33,855 DEBUG TRAIN Batch 15/2400 loss 5.521059 loss_att 9.417776 loss_ctc 7.196310 loss_rnnt 4.500892 hw_loss 0.032733 lr 0.00044284 rank 1
2023-02-21 13:01:33,858 DEBUG TRAIN Batch 15/2400 loss 12.584816 loss_att 11.769152 loss_ctc 13.815291 loss_rnnt 12.521078 hw_loss 0.117765 lr 0.00044272 rank 5
2023-02-21 13:01:33,859 DEBUG TRAIN Batch 15/2400 loss 7.289105 loss_att 11.801275 loss_ctc 10.534878 loss_rnnt 5.927905 hw_loss 0.048745 lr 0.00044282 rank 3
2023-02-21 13:02:55,878 DEBUG TRAIN Batch 15/2500 loss 6.830668 loss_att 9.813093 loss_ctc 10.788798 loss_rnnt 5.634927 hw_loss 0.134073 lr 0.00044261 rank 2
2023-02-21 13:02:55,883 DEBUG TRAIN Batch 15/2500 loss 21.970154 loss_att 22.672152 loss_ctc 28.496738 loss_rnnt 20.815098 hw_loss 0.270835 lr 0.00044262 rank 4
2023-02-21 13:02:55,884 DEBUG TRAIN Batch 15/2500 loss 13.901665 loss_att 15.161957 loss_ctc 17.820644 loss_rnnt 13.078743 hw_loss 0.090624 lr 0.00044262 rank 6
2023-02-21 13:02:55,886 DEBUG TRAIN Batch 15/2500 loss 4.874618 loss_att 7.168063 loss_ctc 6.534965 loss_rnnt 4.114762 hw_loss 0.149602 lr 0.00044265 rank 3
2023-02-21 13:02:55,886 DEBUG TRAIN Batch 15/2500 loss 12.351850 loss_att 13.795201 loss_ctc 15.546046 loss_rnnt 11.533849 hw_loss 0.193945 lr 0.00044266 rank 1
2023-02-21 13:02:55,891 DEBUG TRAIN Batch 15/2500 loss 13.526310 loss_att 13.683863 loss_ctc 15.918598 loss_rnnt 12.982949 hw_loss 0.361647 lr 0.00044253 rank 7
2023-02-21 13:02:55,892 DEBUG TRAIN Batch 15/2500 loss 8.367207 loss_att 12.502996 loss_ctc 11.827618 loss_rnnt 7.024125 hw_loss 0.102252 lr 0.00044255 rank 5
2023-02-21 13:02:55,936 DEBUG TRAIN Batch 15/2500 loss 5.897927 loss_att 7.311847 loss_ctc 7.996467 loss_rnnt 5.194637 hw_loss 0.263814 lr 0.00044259 rank 0
2023-02-21 13:04:14,775 DEBUG TRAIN Batch 15/2600 loss 11.069769 loss_att 16.188099 loss_ctc 13.650589 loss_rnnt 9.617866 hw_loss 0.157740 lr 0.00044236 rank 7
2023-02-21 13:04:14,776 DEBUG TRAIN Batch 15/2600 loss 8.288266 loss_att 10.735676 loss_ctc 10.916743 loss_rnnt 7.387464 hw_loss 0.114106 lr 0.00044244 rank 6
2023-02-21 13:04:14,777 DEBUG TRAIN Batch 15/2600 loss 12.719386 loss_att 13.483183 loss_ctc 15.524773 loss_rnnt 12.036491 hw_loss 0.292655 lr 0.00044244 rank 2
2023-02-21 13:04:14,777 DEBUG TRAIN Batch 15/2600 loss 9.350631 loss_att 12.124506 loss_ctc 11.151398 loss_rnnt 8.501321 hw_loss 0.102059 lr 0.00044245 rank 4
2023-02-21 13:04:14,782 DEBUG TRAIN Batch 15/2600 loss 11.179275 loss_att 16.533989 loss_ctc 14.783290 loss_rnnt 9.571477 hw_loss 0.105597 lr 0.00044242 rank 0
2023-02-21 13:04:14,785 DEBUG TRAIN Batch 15/2600 loss 10.687517 loss_att 14.121428 loss_ctc 13.517779 loss_rnnt 9.533444 hw_loss 0.168604 lr 0.00044249 rank 1
2023-02-21 13:04:14,786 DEBUG TRAIN Batch 15/2600 loss 11.639562 loss_att 14.691474 loss_ctc 16.146942 loss_rnnt 10.412997 hw_loss 0.028495 lr 0.00044237 rank 5
2023-02-21 13:04:14,786 DEBUG TRAIN Batch 15/2600 loss 12.529413 loss_att 16.707302 loss_ctc 22.723904 loss_rnnt 10.253107 hw_loss 0.152740 lr 0.00044248 rank 3
2023-02-21 13:05:32,082 DEBUG TRAIN Batch 15/2700 loss 6.389531 loss_att 10.387764 loss_ctc 13.372250 loss_rnnt 4.600646 hw_loss 0.109142 lr 0.00044228 rank 4
2023-02-21 13:05:32,083 DEBUG TRAIN Batch 15/2700 loss 5.643901 loss_att 8.645700 loss_ctc 8.242042 loss_rnnt 4.647437 hw_loss 0.093160 lr 0.00044226 rank 2
2023-02-21 13:05:32,083 DEBUG TRAIN Batch 15/2700 loss 27.742933 loss_att 29.797657 loss_ctc 40.155266 loss_rnnt 25.642086 hw_loss 0.065489 lr 0.00044219 rank 7
2023-02-21 13:05:32,085 DEBUG TRAIN Batch 15/2700 loss 8.574610 loss_att 9.860345 loss_ctc 11.566858 loss_rnnt 7.901146 hw_loss 0.032532 lr 0.00044230 rank 3
2023-02-21 13:05:32,088 DEBUG TRAIN Batch 15/2700 loss 10.066413 loss_att 14.114851 loss_ctc 12.742094 loss_rnnt 8.771248 hw_loss 0.241348 lr 0.00044220 rank 5
2023-02-21 13:05:32,090 DEBUG TRAIN Batch 15/2700 loss 6.318068 loss_att 9.473154 loss_ctc 7.185499 loss_rnnt 5.464157 hw_loss 0.201069 lr 0.00044232 rank 1
2023-02-21 13:05:32,092 DEBUG TRAIN Batch 15/2700 loss 4.560570 loss_att 9.728045 loss_ctc 7.482450 loss_rnnt 3.079080 hw_loss 0.109521 lr 0.00044225 rank 0
2023-02-21 13:05:32,134 DEBUG TRAIN Batch 15/2700 loss 10.871525 loss_att 13.132843 loss_ctc 13.929138 loss_rnnt 9.945379 hw_loss 0.124127 lr 0.00044227 rank 6
2023-02-21 13:06:52,847 DEBUG TRAIN Batch 15/2800 loss 7.283908 loss_att 12.801075 loss_ctc 10.652865 loss_rnnt 5.698236 hw_loss 0.061958 lr 0.00044210 rank 4
2023-02-21 13:06:52,848 DEBUG TRAIN Batch 15/2800 loss 14.439077 loss_att 16.310328 loss_ctc 19.531429 loss_rnnt 13.324193 hw_loss 0.115600 lr 0.00044201 rank 7
2023-02-21 13:06:52,851 DEBUG TRAIN Batch 15/2800 loss 5.858917 loss_att 10.405554 loss_ctc 6.752353 loss_rnnt 4.775925 hw_loss 0.102264 lr 0.00044209 rank 2
2023-02-21 13:06:52,852 DEBUG TRAIN Batch 15/2800 loss 9.530291 loss_att 12.799677 loss_ctc 13.595239 loss_rnnt 8.297462 hw_loss 0.069297 lr 0.00044203 rank 5
2023-02-21 13:06:52,853 DEBUG TRAIN Batch 15/2800 loss 9.331535 loss_att 11.229616 loss_ctc 9.546981 loss_rnnt 8.864367 hw_loss 0.110298 lr 0.00044207 rank 0
2023-02-21 13:06:52,858 DEBUG TRAIN Batch 15/2800 loss 19.289667 loss_att 21.792080 loss_ctc 26.429840 loss_rnnt 17.814922 hw_loss 0.041699 lr 0.00044214 rank 1
2023-02-21 13:06:52,861 DEBUG TRAIN Batch 15/2800 loss 11.789478 loss_att 17.063728 loss_ctc 15.193991 loss_rnnt 10.222025 hw_loss 0.110002 lr 0.00044213 rank 3
2023-02-21 13:06:52,894 DEBUG TRAIN Batch 15/2800 loss 13.746380 loss_att 16.483097 loss_ctc 20.032568 loss_rnnt 12.284712 hw_loss 0.142813 lr 0.00044210 rank 6
2023-02-21 13:08:11,887 DEBUG TRAIN Batch 15/2900 loss 11.075801 loss_att 14.534315 loss_ctc 15.041588 loss_rnnt 9.840096 hw_loss 0.028555 lr 0.00044192 rank 2
2023-02-21 13:08:11,887 DEBUG TRAIN Batch 15/2900 loss 16.022881 loss_att 17.365364 loss_ctc 17.799366 loss_rnnt 15.423567 hw_loss 0.176160 lr 0.00044184 rank 7
2023-02-21 13:08:11,892 DEBUG TRAIN Batch 15/2900 loss 22.266499 loss_att 24.810202 loss_ctc 31.385410 loss_rnnt 20.519642 hw_loss 0.041738 lr 0.00044190 rank 0
2023-02-21 13:08:11,892 DEBUG TRAIN Batch 15/2900 loss 8.835891 loss_att 13.069135 loss_ctc 11.356651 loss_rnnt 7.629803 hw_loss 0.043757 lr 0.00044193 rank 6
2023-02-21 13:08:11,892 DEBUG TRAIN Batch 15/2900 loss 7.928330 loss_att 10.324944 loss_ctc 10.276155 loss_rnnt 7.077783 hw_loss 0.109091 lr 0.00044197 rank 1
2023-02-21 13:08:11,895 DEBUG TRAIN Batch 15/2900 loss 13.975214 loss_att 20.598381 loss_ctc 18.598713 loss_rnnt 11.977401 hw_loss 0.106339 lr 0.00044196 rank 3
2023-02-21 13:08:11,896 DEBUG TRAIN Batch 15/2900 loss 20.033693 loss_att 19.694786 loss_ctc 22.976107 loss_rnnt 19.590416 hw_loss 0.222631 lr 0.00044193 rank 4
2023-02-21 13:08:11,902 DEBUG TRAIN Batch 15/2900 loss 8.998593 loss_att 12.871038 loss_ctc 14.265005 loss_rnnt 7.454218 hw_loss 0.126934 lr 0.00044185 rank 5
2023-02-21 13:09:29,814 DEBUG TRAIN Batch 15/3000 loss 21.715178 loss_att 22.964066 loss_ctc 25.901142 loss_rnnt 20.778534 hw_loss 0.241382 lr 0.00044176 rank 4
2023-02-21 13:09:29,815 DEBUG TRAIN Batch 15/3000 loss 12.735074 loss_att 18.838535 loss_ctc 20.035173 loss_rnnt 10.486053 hw_loss 0.103089 lr 0.00044180 rank 1
2023-02-21 13:09:29,817 DEBUG TRAIN Batch 15/3000 loss 12.269663 loss_att 14.329393 loss_ctc 15.548004 loss_rnnt 11.367989 hw_loss 0.098656 lr 0.00044167 rank 7
2023-02-21 13:09:29,819 DEBUG TRAIN Batch 15/3000 loss 13.531789 loss_att 16.697514 loss_ctc 20.205919 loss_rnnt 11.919718 hw_loss 0.166953 lr 0.00044175 rank 6
2023-02-21 13:09:29,818 DEBUG TRAIN Batch 15/3000 loss 8.204795 loss_att 10.246409 loss_ctc 10.510082 loss_rnnt 7.448154 hw_loss 0.076774 lr 0.00044175 rank 2
2023-02-21 13:09:29,824 DEBUG TRAIN Batch 15/3000 loss 7.114988 loss_att 12.148452 loss_ctc 8.638462 loss_rnnt 5.828588 hw_loss 0.143582 lr 0.00044173 rank 0
2023-02-21 13:09:29,824 DEBUG TRAIN Batch 15/3000 loss 6.145617 loss_att 7.480520 loss_ctc 7.970784 loss_rnnt 5.561760 hw_loss 0.137850 lr 0.00044178 rank 3
2023-02-21 13:09:29,874 DEBUG TRAIN Batch 15/3000 loss 13.719728 loss_att 16.482807 loss_ctc 20.711264 loss_rnnt 12.163146 hw_loss 0.134554 lr 0.00044168 rank 5
2023-02-21 13:10:50,011 DEBUG TRAIN Batch 15/3100 loss 11.224334 loss_att 12.318377 loss_ctc 14.817988 loss_rnnt 10.407940 hw_loss 0.222058 lr 0.00044159 rank 4
2023-02-21 13:10:50,012 DEBUG TRAIN Batch 15/3100 loss 12.132803 loss_att 16.333157 loss_ctc 17.659262 loss_rnnt 10.504587 hw_loss 0.096160 lr 0.00044162 rank 1
2023-02-21 13:10:50,014 DEBUG TRAIN Batch 15/3100 loss 12.877568 loss_att 15.189795 loss_ctc 14.911887 loss_rnnt 12.126020 hw_loss 0.033488 lr 0.00044157 rank 2
2023-02-21 13:10:50,018 DEBUG TRAIN Batch 15/3100 loss 19.331730 loss_att 20.448971 loss_ctc 24.018639 loss_rnnt 18.406641 hw_loss 0.143848 lr 0.00044151 rank 5
2023-02-21 13:10:50,018 DEBUG TRAIN Batch 15/3100 loss 19.762293 loss_att 19.112064 loss_ctc 24.708488 loss_rnnt 19.174892 hw_loss 0.108661 lr 0.00044158 rank 6
2023-02-21 13:10:50,020 DEBUG TRAIN Batch 15/3100 loss 8.734259 loss_att 11.504920 loss_ctc 10.847102 loss_rnnt 7.753021 hw_loss 0.272613 lr 0.00044150 rank 7
2023-02-21 13:10:50,022 DEBUG TRAIN Batch 15/3100 loss 15.338899 loss_att 17.032335 loss_ctc 17.447592 loss_rnnt 14.613042 hw_loss 0.198766 lr 0.00044161 rank 3
2023-02-21 13:10:50,024 DEBUG TRAIN Batch 15/3100 loss 6.947364 loss_att 10.728916 loss_ctc 10.578990 loss_rnnt 5.632044 hw_loss 0.140237 lr 0.00044156 rank 0
2023-02-21 13:12:10,821 DEBUG TRAIN Batch 15/3200 loss 6.547722 loss_att 9.174616 loss_ctc 4.992698 loss_rnnt 6.178489 hw_loss 0.095982 lr 0.00044132 rank 7
2023-02-21 13:12:10,822 DEBUG TRAIN Batch 15/3200 loss 14.581252 loss_att 14.360169 loss_ctc 17.892488 loss_rnnt 14.058244 hw_loss 0.235736 lr 0.00044145 rank 1
2023-02-21 13:12:10,826 DEBUG TRAIN Batch 15/3200 loss 16.105940 loss_att 19.010521 loss_ctc 23.984243 loss_rnnt 14.369932 hw_loss 0.196223 lr 0.00044141 rank 4
2023-02-21 13:12:10,830 DEBUG TRAIN Batch 15/3200 loss 11.793183 loss_att 11.992861 loss_ctc 16.728638 loss_rnnt 10.924376 hw_loss 0.320269 lr 0.00044140 rank 2
2023-02-21 13:12:10,831 DEBUG TRAIN Batch 15/3200 loss 10.103407 loss_att 12.928686 loss_ctc 12.905917 loss_rnnt 9.102915 hw_loss 0.115814 lr 0.00044141 rank 6
2023-02-21 13:12:10,838 DEBUG TRAIN Batch 15/3200 loss 28.714069 loss_att 25.958342 loss_ctc 38.347210 loss_rnnt 27.857534 hw_loss 0.231119 lr 0.00044134 rank 5
2023-02-21 13:12:10,838 DEBUG TRAIN Batch 15/3200 loss 11.618392 loss_att 12.364445 loss_ctc 14.726883 loss_rnnt 10.889623 hw_loss 0.309549 lr 0.00044144 rank 3
2023-02-21 13:12:10,855 DEBUG TRAIN Batch 15/3200 loss 9.783413 loss_att 11.870033 loss_ctc 15.797470 loss_rnnt 8.491627 hw_loss 0.136102 lr 0.00044138 rank 0
2023-02-21 13:13:29,529 DEBUG TRAIN Batch 15/3300 loss 8.921358 loss_att 13.161591 loss_ctc 12.093458 loss_rnnt 7.614618 hw_loss 0.067024 lr 0.00044128 rank 1
2023-02-21 13:13:29,537 DEBUG TRAIN Batch 15/3300 loss 17.499964 loss_att 24.409412 loss_ctc 24.990513 loss_rnnt 15.048653 hw_loss 0.132528 lr 0.00044124 rank 4
2023-02-21 13:13:29,538 DEBUG TRAIN Batch 15/3300 loss 15.829412 loss_att 21.001820 loss_ctc 19.982403 loss_rnnt 14.205852 hw_loss 0.066274 lr 0.00044123 rank 2
2023-02-21 13:13:29,538 DEBUG TRAIN Batch 15/3300 loss 10.508443 loss_att 11.437490 loss_ctc 15.465528 loss_rnnt 9.623866 hw_loss 0.070918 lr 0.00044121 rank 0
2023-02-21 13:13:29,538 DEBUG TRAIN Batch 15/3300 loss 5.703324 loss_att 10.681362 loss_ctc 6.311119 loss_rnnt 4.589335 hw_loss 0.070016 lr 0.00044124 rank 6
2023-02-21 13:13:29,540 DEBUG TRAIN Batch 15/3300 loss 12.426242 loss_att 13.031314 loss_ctc 13.255047 loss_rnnt 12.138721 hw_loss 0.105002 lr 0.00044115 rank 7
2023-02-21 13:13:29,541 DEBUG TRAIN Batch 15/3300 loss 17.856092 loss_att 19.647869 loss_ctc 22.089016 loss_rnnt 16.873308 hw_loss 0.112571 lr 0.00044117 rank 5
2023-02-21 13:13:29,588 DEBUG TRAIN Batch 15/3300 loss 8.809544 loss_att 10.382903 loss_ctc 10.525295 loss_rnnt 8.218769 hw_loss 0.088754 lr 0.00044127 rank 3
2023-02-21 13:14:48,675 DEBUG TRAIN Batch 15/3400 loss 9.322869 loss_att 16.059582 loss_ctc 12.989291 loss_rnnt 7.452970 hw_loss 0.063187 lr 0.00044111 rank 1
2023-02-21 13:14:48,676 DEBUG TRAIN Batch 15/3400 loss 16.014645 loss_att 20.437023 loss_ctc 19.649178 loss_rnnt 14.602294 hw_loss 0.081129 lr 0.00044107 rank 4
2023-02-21 13:14:48,681 DEBUG TRAIN Batch 15/3400 loss 8.544892 loss_att 11.939153 loss_ctc 11.998523 loss_rnnt 7.326775 hw_loss 0.147714 lr 0.00044098 rank 7
2023-02-21 13:14:48,684 DEBUG TRAIN Batch 15/3400 loss 12.734368 loss_att 16.733446 loss_ctc 16.630272 loss_rnnt 11.384632 hw_loss 0.057128 lr 0.00044107 rank 6
2023-02-21 13:14:48,684 DEBUG TRAIN Batch 15/3400 loss 3.167207 loss_att 6.768880 loss_ctc 4.264025 loss_rnnt 2.287483 hw_loss 0.024650 lr 0.00044106 rank 2
2023-02-21 13:14:48,685 DEBUG TRAIN Batch 15/3400 loss 7.453335 loss_att 12.583523 loss_ctc 12.584099 loss_rnnt 5.682328 hw_loss 0.114125 lr 0.00044110 rank 3
2023-02-21 13:14:48,687 DEBUG TRAIN Batch 15/3400 loss 13.556400 loss_att 14.487272 loss_ctc 16.582338 loss_rnnt 12.926773 hw_loss 0.074989 lr 0.00044099 rank 5
2023-02-21 13:14:48,733 DEBUG TRAIN Batch 15/3400 loss 14.727044 loss_att 18.004137 loss_ctc 20.895798 loss_rnnt 13.205634 hw_loss 0.081546 lr 0.00044104 rank 0
2023-02-21 13:16:07,636 DEBUG TRAIN Batch 15/3500 loss 7.507137 loss_att 11.093758 loss_ctc 12.108928 loss_rnnt 6.137528 hw_loss 0.072587 lr 0.00044089 rank 6
2023-02-21 13:16:07,637 DEBUG TRAIN Batch 15/3500 loss 5.787809 loss_att 10.597327 loss_ctc 9.185737 loss_rnnt 4.343138 hw_loss 0.055708 lr 0.00044094 rank 1
2023-02-21 13:16:07,637 DEBUG TRAIN Batch 15/3500 loss 10.857645 loss_att 13.363625 loss_ctc 15.624611 loss_rnnt 9.665248 hw_loss 0.104261 lr 0.00044087 rank 0
2023-02-21 13:16:07,639 DEBUG TRAIN Batch 15/3500 loss 15.997207 loss_att 15.037443 loss_ctc 15.751936 loss_rnnt 16.177822 hw_loss 0.082577 lr 0.00044092 rank 3
2023-02-21 13:16:07,639 DEBUG TRAIN Batch 15/3500 loss 10.860596 loss_att 14.475986 loss_ctc 15.202665 loss_rnnt 9.471674 hw_loss 0.162940 lr 0.00044081 rank 7
2023-02-21 13:16:07,639 DEBUG TRAIN Batch 15/3500 loss 5.262996 loss_att 8.349109 loss_ctc 6.684746 loss_rnnt 4.381593 hw_loss 0.139899 lr 0.00044082 rank 5
2023-02-21 13:16:07,640 DEBUG TRAIN Batch 15/3500 loss 16.959120 loss_att 16.555830 loss_ctc 22.242626 loss_rnnt 16.211323 hw_loss 0.232471 lr 0.00044089 rank 2
2023-02-21 13:16:07,641 DEBUG TRAIN Batch 15/3500 loss 7.502012 loss_att 12.132168 loss_ctc 10.930574 loss_rnnt 6.105987 hw_loss 0.024099 lr 0.00044090 rank 4
2023-02-21 13:17:27,604 DEBUG TRAIN Batch 15/3600 loss 17.850576 loss_att 18.411659 loss_ctc 22.535089 loss_rnnt 17.054871 hw_loss 0.110411 lr 0.00044072 rank 6
2023-02-21 13:17:27,605 DEBUG TRAIN Batch 15/3600 loss 15.694743 loss_att 18.639601 loss_ctc 21.571882 loss_rnnt 14.273325 hw_loss 0.091550 lr 0.00044071 rank 2
2023-02-21 13:17:27,606 DEBUG TRAIN Batch 15/3600 loss 9.281622 loss_att 12.885197 loss_ctc 14.121187 loss_rnnt 7.870677 hw_loss 0.084291 lr 0.00044077 rank 1
2023-02-21 13:17:27,608 DEBUG TRAIN Batch 15/3600 loss 11.672779 loss_att 14.207914 loss_ctc 16.492357 loss_rnnt 10.504160 hw_loss 0.035590 lr 0.00044073 rank 4
2023-02-21 13:17:27,608 DEBUG TRAIN Batch 15/3600 loss 14.863721 loss_att 16.565186 loss_ctc 19.551195 loss_rnnt 13.824041 hw_loss 0.139483 lr 0.00044070 rank 0
2023-02-21 13:17:27,609 DEBUG TRAIN Batch 15/3600 loss 11.406275 loss_att 12.536228 loss_ctc 15.257226 loss_rnnt 10.639153 hw_loss 0.051885 lr 0.00044064 rank 7
2023-02-21 13:17:27,614 DEBUG TRAIN Batch 15/3600 loss 13.308085 loss_att 14.752995 loss_ctc 15.080154 loss_rnnt 12.677649 hw_loss 0.197213 lr 0.00044075 rank 3
2023-02-21 13:17:27,615 DEBUG TRAIN Batch 15/3600 loss 8.214837 loss_att 12.514171 loss_ctc 9.898196 loss_rnnt 7.090030 hw_loss 0.075924 lr 0.00044065 rank 5
2023-02-21 13:18:46,671 DEBUG TRAIN Batch 15/3700 loss 10.941224 loss_att 11.623960 loss_ctc 11.582735 loss_rnnt 10.599959 hw_loss 0.223468 lr 0.00044059 rank 1
2023-02-21 13:18:46,672 DEBUG TRAIN Batch 15/3700 loss 12.364628 loss_att 16.746708 loss_ctc 21.419907 loss_rnnt 10.198668 hw_loss 0.154076 lr 0.00044047 rank 7
2023-02-21 13:18:46,676 DEBUG TRAIN Batch 15/3700 loss 7.189707 loss_att 7.088607 loss_ctc 5.039216 loss_rnnt 7.374026 hw_loss 0.229938 lr 0.00044056 rank 4
2023-02-21 13:18:46,677 DEBUG TRAIN Batch 15/3700 loss 11.141336 loss_att 13.183344 loss_ctc 16.177336 loss_rnnt 10.022792 hw_loss 0.072517 lr 0.00044055 rank 6
2023-02-21 13:18:46,680 DEBUG TRAIN Batch 15/3700 loss 4.459854 loss_att 7.510458 loss_ctc 6.797726 loss_rnnt 3.438729 hw_loss 0.186163 lr 0.00044054 rank 2
2023-02-21 13:18:46,682 DEBUG TRAIN Batch 15/3700 loss 8.806558 loss_att 13.734314 loss_ctc 14.692027 loss_rnnt 6.982555 hw_loss 0.100730 lr 0.00044048 rank 5
2023-02-21 13:18:46,682 DEBUG TRAIN Batch 15/3700 loss 16.452644 loss_att 19.281532 loss_ctc 22.653881 loss_rnnt 15.040373 hw_loss 0.036871 lr 0.00044053 rank 0
2023-02-21 13:18:46,685 DEBUG TRAIN Batch 15/3700 loss 7.793601 loss_att 10.161574 loss_ctc 11.300878 loss_rnnt 6.773792 hw_loss 0.147330 lr 0.00044058 rank 3
2023-02-21 13:20:04,387 DEBUG TRAIN Batch 15/3800 loss 6.762100 loss_att 9.212380 loss_ctc 10.373261 loss_rnnt 5.675736 hw_loss 0.215284 lr 0.00044036 rank 0
2023-02-21 13:20:04,386 DEBUG TRAIN Batch 15/3800 loss 20.687307 loss_att 24.343069 loss_ctc 29.770411 loss_rnnt 18.681501 hw_loss 0.119201 lr 0.00044037 rank 2
2023-02-21 13:20:04,388 DEBUG TRAIN Batch 15/3800 loss 16.313217 loss_att 17.415598 loss_ctc 19.802916 loss_rnnt 15.525503 hw_loss 0.191151 lr 0.00044039 rank 4
2023-02-21 13:20:04,390 DEBUG TRAIN Batch 15/3800 loss 13.707557 loss_att 14.767903 loss_ctc 22.411036 loss_rnnt 12.120692 hw_loss 0.401871 lr 0.00044042 rank 1
2023-02-21 13:20:04,391 DEBUG TRAIN Batch 15/3800 loss 12.193384 loss_att 10.737814 loss_ctc 14.137975 loss_rnnt 12.082064 hw_loss 0.268416 lr 0.00044038 rank 6
2023-02-21 13:20:04,391 DEBUG TRAIN Batch 15/3800 loss 7.471153 loss_att 10.088502 loss_ctc 11.709461 loss_rnnt 6.245401 hw_loss 0.257201 lr 0.00044041 rank 3
2023-02-21 13:20:04,393 DEBUG TRAIN Batch 15/3800 loss 10.971301 loss_att 9.796416 loss_ctc 13.403033 loss_rnnt 10.768028 hw_loss 0.213785 lr 0.00044030 rank 7
2023-02-21 13:20:04,395 DEBUG TRAIN Batch 15/3800 loss 7.554290 loss_att 10.330145 loss_ctc 12.533590 loss_rnnt 6.227632 hw_loss 0.201713 lr 0.00044031 rank 5
2023-02-21 13:21:25,104 DEBUG TRAIN Batch 15/3900 loss 5.127206 loss_att 8.619203 loss_ctc 9.336720 loss_rnnt 3.830090 hw_loss 0.070216 lr 0.00044014 rank 5
2023-02-21 13:21:25,104 DEBUG TRAIN Batch 15/3900 loss 11.309890 loss_att 13.124588 loss_ctc 13.735751 loss_rnnt 10.585348 hw_loss 0.071540 lr 0.00044022 rank 4
2023-02-21 13:21:25,105 DEBUG TRAIN Batch 15/3900 loss 3.369878 loss_att 7.640711 loss_ctc 5.621538 loss_rnnt 2.112019 hw_loss 0.194006 lr 0.00044024 rank 3
2023-02-21 13:21:25,106 DEBUG TRAIN Batch 15/3900 loss 12.009772 loss_att 16.637548 loss_ctc 19.264969 loss_rnnt 10.103592 hw_loss 0.024873 lr 0.00044018 rank 0
2023-02-21 13:21:25,109 DEBUG TRAIN Batch 15/3900 loss 13.249790 loss_att 16.793213 loss_ctc 14.576639 loss_rnnt 12.305480 hw_loss 0.110086 lr 0.00044025 rank 1
2023-02-21 13:21:25,132 DEBUG TRAIN Batch 15/3900 loss 7.364471 loss_att 11.373774 loss_ctc 8.572384 loss_rnnt 6.321660 hw_loss 0.149804 lr 0.00044012 rank 7
2023-02-21 13:21:25,144 DEBUG TRAIN Batch 15/3900 loss 8.965978 loss_att 8.035999 loss_ctc 10.045187 loss_rnnt 8.776110 hw_loss 0.434941 lr 0.00044020 rank 2
2023-02-21 13:21:25,146 DEBUG TRAIN Batch 15/3900 loss 9.720716 loss_att 11.241870 loss_ctc 10.771664 loss_rnnt 9.250793 hw_loss 0.047937 lr 0.00044021 rank 6
2023-02-21 13:22:42,640 DEBUG TRAIN Batch 15/4000 loss 4.236776 loss_att 6.636788 loss_ctc 7.415689 loss_rnnt 3.294766 hw_loss 0.071536 lr 0.00044008 rank 1
2023-02-21 13:22:42,641 DEBUG TRAIN Batch 15/4000 loss 10.212540 loss_att 12.250092 loss_ctc 12.632879 loss_rnnt 9.402588 hw_loss 0.149493 lr 0.00043995 rank 7
2023-02-21 13:22:42,645 DEBUG TRAIN Batch 15/4000 loss 19.239895 loss_att 24.449749 loss_ctc 21.484228 loss_rnnt 17.839294 hw_loss 0.111346 lr 0.00044001 rank 0
2023-02-21 13:22:42,645 DEBUG TRAIN Batch 15/4000 loss 10.069473 loss_att 11.591348 loss_ctc 11.710396 loss_rnnt 9.475686 hw_loss 0.132417 lr 0.00044003 rank 2
2023-02-21 13:22:42,645 DEBUG TRAIN Batch 15/4000 loss 32.675106 loss_att 34.847427 loss_ctc 41.572781 loss_rnnt 31.013227 hw_loss 0.076981 lr 0.00043997 rank 5
2023-02-21 13:22:42,645 DEBUG TRAIN Batch 15/4000 loss 5.928667 loss_att 8.877499 loss_ctc 10.579323 loss_rnnt 4.675394 hw_loss 0.081409 lr 0.00044004 rank 4
2023-02-21 13:22:42,650 DEBUG TRAIN Batch 15/4000 loss 9.475608 loss_att 12.530613 loss_ctc 10.839561 loss_rnnt 8.648393 hw_loss 0.064412 lr 0.00044007 rank 3
2023-02-21 13:22:42,685 DEBUG TRAIN Batch 15/4000 loss 9.060195 loss_att 12.110321 loss_ctc 9.194654 loss_rnnt 8.356154 hw_loss 0.142665 lr 0.00044004 rank 6
2023-02-21 13:24:02,260 DEBUG TRAIN Batch 15/4100 loss 14.269381 loss_att 18.705780 loss_ctc 17.213419 loss_rnnt 12.902547 hw_loss 0.163151 lr 0.00043991 rank 1
2023-02-21 13:24:02,262 DEBUG TRAIN Batch 15/4100 loss 13.609714 loss_att 15.240351 loss_ctc 16.704626 loss_rnnt 12.765522 hw_loss 0.197643 lr 0.00043986 rank 2
2023-02-21 13:24:02,265 DEBUG TRAIN Batch 15/4100 loss 10.068352 loss_att 14.570799 loss_ctc 14.992640 loss_rnnt 8.459745 hw_loss 0.096645 lr 0.00043978 rank 7
2023-02-21 13:24:02,265 DEBUG TRAIN Batch 15/4100 loss 4.136943 loss_att 7.996432 loss_ctc 6.356738 loss_rnnt 3.016903 hw_loss 0.097819 lr 0.00043980 rank 5
2023-02-21 13:24:02,266 DEBUG TRAIN Batch 15/4100 loss 16.342686 loss_att 20.541208 loss_ctc 22.807547 loss_rnnt 14.554934 hw_loss 0.161370 lr 0.00043984 rank 0
2023-02-21 13:24:02,265 DEBUG TRAIN Batch 15/4100 loss 15.442770 loss_att 19.119221 loss_ctc 21.704746 loss_rnnt 13.793530 hw_loss 0.148158 lr 0.00043987 rank 4
2023-02-21 13:24:02,270 DEBUG TRAIN Batch 15/4100 loss 13.121593 loss_att 13.901653 loss_ctc 13.688437 loss_rnnt 12.830721 hw_loss 0.111150 lr 0.00043990 rank 3
2023-02-21 13:24:02,320 DEBUG TRAIN Batch 15/4100 loss 6.598488 loss_att 8.830555 loss_ctc 8.156846 loss_rnnt 5.852974 hw_loss 0.171223 lr 0.00043987 rank 6
2023-02-21 13:25:20,914 DEBUG TRAIN Batch 15/4200 loss 13.004678 loss_att 16.913097 loss_ctc 21.677876 loss_rnnt 10.945492 hw_loss 0.227018 lr 0.00043961 rank 7
2023-02-21 13:25:20,914 DEBUG TRAIN Batch 15/4200 loss 3.807712 loss_att 6.822459 loss_ctc 4.947485 loss_rnnt 3.018394 hw_loss 0.064499 lr 0.00043974 rank 1
2023-02-21 13:25:20,918 DEBUG TRAIN Batch 15/4200 loss 14.566878 loss_att 17.173740 loss_ctc 18.717981 loss_rnnt 13.388047 hw_loss 0.194957 lr 0.00043970 rank 4
2023-02-21 13:25:20,919 DEBUG TRAIN Batch 15/4200 loss 4.454996 loss_att 6.818342 loss_ctc 5.652536 loss_rnnt 3.777510 hw_loss 0.084645 lr 0.00043969 rank 2
2023-02-21 13:25:20,921 DEBUG TRAIN Batch 15/4200 loss 12.948442 loss_att 19.920885 loss_ctc 19.080860 loss_rnnt 10.650085 hw_loss 0.161648 lr 0.00043970 rank 6
2023-02-21 13:25:20,925 DEBUG TRAIN Batch 15/4200 loss 13.258152 loss_att 17.363247 loss_ctc 17.774288 loss_rnnt 11.734505 hw_loss 0.188393 lr 0.00043967 rank 0
2023-02-21 13:25:20,927 DEBUG TRAIN Batch 15/4200 loss 12.558649 loss_att 16.841255 loss_ctc 18.541298 loss_rnnt 10.863726 hw_loss 0.076342 lr 0.00043973 rank 3
2023-02-21 13:25:20,927 DEBUG TRAIN Batch 15/4200 loss 13.882770 loss_att 16.360601 loss_ctc 19.300659 loss_rnnt 12.625462 hw_loss 0.073793 lr 0.00043963 rank 5
2023-02-21 13:26:41,051 DEBUG TRAIN Batch 15/4300 loss 10.845519 loss_att 12.960894 loss_ctc 13.750360 loss_rnnt 9.934937 hw_loss 0.187866 lr 0.00043957 rank 1
2023-02-21 13:26:41,052 DEBUG TRAIN Batch 15/4300 loss 8.027521 loss_att 9.292579 loss_ctc 8.294003 loss_rnnt 7.616038 hw_loss 0.230513 lr 0.00043953 rank 4
2023-02-21 13:26:41,055 DEBUG TRAIN Batch 15/4300 loss 20.939787 loss_att 22.569040 loss_ctc 25.778025 loss_rnnt 19.823593 hw_loss 0.272331 lr 0.00043953 rank 6
2023-02-21 13:26:41,055 DEBUG TRAIN Batch 15/4300 loss 10.044419 loss_att 13.105607 loss_ctc 14.000658 loss_rnnt 8.838298 hw_loss 0.124474 lr 0.00043952 rank 2
2023-02-21 13:26:41,056 DEBUG TRAIN Batch 15/4300 loss 8.772448 loss_att 11.379492 loss_ctc 12.751564 loss_rnnt 7.701231 hw_loss 0.036110 lr 0.00043950 rank 0
2023-02-21 13:26:41,056 DEBUG TRAIN Batch 15/4300 loss 4.379143 loss_att 7.092440 loss_ctc 5.388845 loss_rnnt 3.640351 hw_loss 0.115322 lr 0.00043944 rank 7
2023-02-21 13:26:41,062 DEBUG TRAIN Batch 15/4300 loss 7.112141 loss_att 11.548602 loss_ctc 11.493769 loss_rnnt 5.567199 hw_loss 0.137687 lr 0.00043946 rank 5
2023-02-21 13:26:41,108 DEBUG TRAIN Batch 15/4300 loss 11.523838 loss_att 16.227352 loss_ctc 17.903049 loss_rnnt 9.636067 hw_loss 0.180948 lr 0.00043956 rank 3
2023-02-21 13:27:59,440 DEBUG TRAIN Batch 15/4400 loss 21.259966 loss_att 23.612579 loss_ctc 29.560078 loss_rnnt 19.627037 hw_loss 0.104487 lr 0.00043936 rank 4
2023-02-21 13:27:59,441 DEBUG TRAIN Batch 15/4400 loss 16.460297 loss_att 17.457977 loss_ctc 22.797302 loss_rnnt 15.302275 hw_loss 0.212913 lr 0.00043927 rank 7
2023-02-21 13:27:59,447 DEBUG TRAIN Batch 15/4400 loss 10.341016 loss_att 14.118374 loss_ctc 15.211794 loss_rnnt 8.895844 hw_loss 0.075496 lr 0.00043935 rank 2
2023-02-21 13:27:59,451 DEBUG TRAIN Batch 15/4400 loss 10.542112 loss_att 15.056366 loss_ctc 13.121184 loss_rnnt 9.183852 hw_loss 0.209125 lr 0.00043939 rank 3
2023-02-21 13:27:59,450 DEBUG TRAIN Batch 15/4400 loss 12.892691 loss_att 12.363551 loss_ctc 16.807886 loss_rnnt 12.266422 hw_loss 0.393883 lr 0.00043929 rank 5
2023-02-21 13:27:59,452 DEBUG TRAIN Batch 15/4400 loss 14.645808 loss_att 16.918512 loss_ctc 18.404919 loss_rnnt 13.639148 hw_loss 0.095445 lr 0.00043933 rank 0
2023-02-21 13:27:59,452 DEBUG TRAIN Batch 15/4400 loss 9.647731 loss_att 9.210645 loss_ctc 12.281600 loss_rnnt 9.295506 hw_loss 0.165863 lr 0.00043940 rank 1
2023-02-21 13:27:59,495 DEBUG TRAIN Batch 15/4400 loss 14.317474 loss_att 16.519913 loss_ctc 21.283550 loss_rnnt 12.837811 hw_loss 0.206935 lr 0.00043936 rank 6
2023-02-21 13:29:18,324 DEBUG TRAIN Batch 15/4500 loss 18.440453 loss_att 21.249235 loss_ctc 26.529034 loss_rnnt 16.776070 hw_loss 0.045279 lr 0.00043923 rank 1
2023-02-21 13:29:18,325 DEBUG TRAIN Batch 15/4500 loss 15.196896 loss_att 21.962948 loss_ctc 20.082298 loss_rnnt 13.121628 hw_loss 0.132506 lr 0.00043911 rank 7
2023-02-21 13:29:18,326 DEBUG TRAIN Batch 15/4500 loss 9.384631 loss_att 12.328613 loss_ctc 10.627389 loss_rnnt 8.597663 hw_loss 0.060882 lr 0.00043918 rank 2
2023-02-21 13:29:18,327 DEBUG TRAIN Batch 15/4500 loss 11.043842 loss_att 14.041685 loss_ctc 13.192221 loss_rnnt 10.059864 hw_loss 0.183674 lr 0.00043919 rank 6
2023-02-21 13:29:18,330 DEBUG TRAIN Batch 15/4500 loss 10.432003 loss_att 16.104237 loss_ctc 11.213310 loss_rnnt 9.180472 hw_loss 0.024203 lr 0.00043920 rank 4
2023-02-21 13:29:18,332 DEBUG TRAIN Batch 15/4500 loss 3.476497 loss_att 6.758133 loss_ctc 4.845995 loss_rnnt 2.601276 hw_loss 0.068052 lr 0.00043912 rank 5
2023-02-21 13:29:18,335 DEBUG TRAIN Batch 15/4500 loss 9.013124 loss_att 7.769130 loss_ctc 11.503464 loss_rnnt 8.757367 hw_loss 0.323456 lr 0.00043922 rank 3
2023-02-21 13:29:18,373 DEBUG TRAIN Batch 15/4500 loss 19.734411 loss_att 22.649513 loss_ctc 19.582411 loss_rnnt 19.082575 hw_loss 0.167031 lr 0.00043916 rank 0
2023-02-21 13:30:39,038 DEBUG TRAIN Batch 15/4600 loss 6.684962 loss_att 8.386742 loss_ctc 10.630009 loss_rnnt 5.752952 hw_loss 0.123092 lr 0.00043903 rank 4
2023-02-21 13:30:39,040 DEBUG TRAIN Batch 15/4600 loss 9.083386 loss_att 12.146425 loss_ctc 12.384995 loss_rnnt 8.016933 hw_loss 0.025557 lr 0.00043900 rank 0
2023-02-21 13:30:39,040 DEBUG TRAIN Batch 15/4600 loss 11.674547 loss_att 15.828312 loss_ctc 16.089552 loss_rnnt 10.178289 hw_loss 0.144071 lr 0.00043901 rank 2
2023-02-21 13:30:39,041 DEBUG TRAIN Batch 15/4600 loss 15.996119 loss_att 20.661358 loss_ctc 18.892443 loss_rnnt 14.577422 hw_loss 0.186509 lr 0.00043906 rank 1
2023-02-21 13:30:39,043 DEBUG TRAIN Batch 15/4600 loss 17.967394 loss_att 21.265606 loss_ctc 26.308838 loss_rnnt 16.144316 hw_loss 0.096081 lr 0.00043894 rank 7
2023-02-21 13:30:39,045 DEBUG TRAIN Batch 15/4600 loss 11.425186 loss_att 18.391527 loss_ctc 16.627411 loss_rnnt 9.263340 hw_loss 0.140529 lr 0.00043895 rank 5
2023-02-21 13:30:39,048 DEBUG TRAIN Batch 15/4600 loss 5.086000 loss_att 9.345022 loss_ctc 5.391747 loss_rnnt 4.129046 hw_loss 0.120718 lr 0.00043902 rank 6
2023-02-21 13:30:39,049 DEBUG TRAIN Batch 15/4600 loss 18.201826 loss_att 26.163067 loss_ctc 26.423191 loss_rnnt 15.337492 hw_loss 0.329824 lr 0.00043905 rank 3
2023-02-21 13:31:58,278 DEBUG TRAIN Batch 15/4700 loss 7.140275 loss_att 10.343788 loss_ctc 11.673681 loss_rnnt 5.881006 hw_loss 0.026463 lr 0.00043886 rank 4
2023-02-21 13:31:58,284 DEBUG TRAIN Batch 15/4700 loss 11.713901 loss_att 14.820684 loss_ctc 18.515163 loss_rnnt 10.085299 hw_loss 0.188271 lr 0.00043877 rank 7
2023-02-21 13:31:58,285 DEBUG TRAIN Batch 15/4700 loss 15.681354 loss_att 17.923504 loss_ctc 23.686840 loss_rnnt 14.141269 hw_loss 0.045482 lr 0.00043884 rank 2
2023-02-21 13:31:58,286 DEBUG TRAIN Batch 15/4700 loss 19.586512 loss_att 21.980232 loss_ctc 24.570358 loss_rnnt 18.339293 hw_loss 0.194929 lr 0.00043889 rank 1
2023-02-21 13:31:58,291 DEBUG TRAIN Batch 15/4700 loss 9.560643 loss_att 13.596720 loss_ctc 13.138098 loss_rnnt 8.240734 hw_loss 0.066936 lr 0.00043878 rank 5
2023-02-21 13:31:58,293 DEBUG TRAIN Batch 15/4700 loss 12.707044 loss_att 16.030542 loss_ctc 15.880549 loss_rnnt 11.582151 hw_loss 0.069487 lr 0.00043885 rank 6
2023-02-21 13:31:58,293 DEBUG TRAIN Batch 15/4700 loss 8.803246 loss_att 16.210148 loss_ctc 12.433089 loss_rnnt 6.766416 hw_loss 0.134007 lr 0.00043888 rank 3
2023-02-21 13:31:58,294 DEBUG TRAIN Batch 15/4700 loss 13.807337 loss_att 17.518337 loss_ctc 17.960220 loss_rnnt 12.423774 hw_loss 0.164334 lr 0.00043883 rank 0
2023-02-21 13:33:17,484 DEBUG TRAIN Batch 15/4800 loss 10.408195 loss_att 14.029276 loss_ctc 12.136414 loss_rnnt 9.367317 hw_loss 0.161684 lr 0.00043872 rank 1
2023-02-21 13:33:17,487 DEBUG TRAIN Batch 15/4800 loss 17.147890 loss_att 17.771313 loss_ctc 17.056499 loss_rnnt 16.973066 hw_loss 0.116859 lr 0.00043867 rank 2
2023-02-21 13:33:17,488 DEBUG TRAIN Batch 15/4800 loss 8.907171 loss_att 14.225796 loss_ctc 10.578102 loss_rnnt 7.515112 hw_loss 0.197894 lr 0.00043860 rank 7
2023-02-21 13:33:17,489 DEBUG TRAIN Batch 15/4800 loss 11.239169 loss_att 13.401902 loss_ctc 19.674549 loss_rnnt 9.648634 hw_loss 0.062384 lr 0.00043861 rank 5
2023-02-21 13:33:17,489 DEBUG TRAIN Batch 15/4800 loss 12.124231 loss_att 12.938591 loss_ctc 15.563198 loss_rnnt 11.433402 hw_loss 0.130175 lr 0.00043868 rank 6
2023-02-21 13:33:17,490 DEBUG TRAIN Batch 15/4800 loss 8.484941 loss_att 12.939580 loss_ctc 11.512801 loss_rnnt 7.109995 hw_loss 0.150571 lr 0.00043869 rank 4
2023-02-21 13:33:17,491 DEBUG TRAIN Batch 15/4800 loss 4.707793 loss_att 7.708431 loss_ctc 5.892058 loss_rnnt 3.910347 hw_loss 0.073906 lr 0.00043866 rank 0
2023-02-21 13:33:17,493 DEBUG TRAIN Batch 15/4800 loss 3.907574 loss_att 8.512346 loss_ctc 6.677360 loss_rnnt 2.584475 hw_loss 0.061575 lr 0.00043871 rank 3
2023-02-21 13:34:36,204 DEBUG TRAIN Batch 15/4900 loss 11.520858 loss_att 13.303782 loss_ctc 14.846989 loss_rnnt 10.692176 hw_loss 0.053649 lr 0.00043856 rank 1
2023-02-21 13:34:36,205 DEBUG TRAIN Batch 15/4900 loss 12.675327 loss_att 14.923648 loss_ctc 14.328470 loss_rnnt 11.937837 hw_loss 0.126387 lr 0.00043843 rank 7
2023-02-21 13:34:36,211 DEBUG TRAIN Batch 15/4900 loss 13.867396 loss_att 16.574337 loss_ctc 15.278227 loss_rnnt 13.101477 hw_loss 0.068291 lr 0.00043852 rank 4
2023-02-21 13:34:36,212 DEBUG TRAIN Batch 15/4900 loss 10.446579 loss_att 13.515843 loss_ctc 13.493835 loss_rnnt 9.383981 hw_loss 0.079584 lr 0.00043851 rank 6
2023-02-21 13:34:36,214 DEBUG TRAIN Batch 15/4900 loss 17.203693 loss_att 22.185154 loss_ctc 23.336693 loss_rnnt 15.346498 hw_loss 0.080939 lr 0.00043851 rank 2
2023-02-21 13:34:36,215 DEBUG TRAIN Batch 15/4900 loss 17.968355 loss_att 21.689528 loss_ctc 20.108677 loss_rnnt 16.861658 hw_loss 0.144533 lr 0.00043849 rank 0
2023-02-21 13:34:36,215 DEBUG TRAIN Batch 15/4900 loss 13.880305 loss_att 16.666555 loss_ctc 20.405651 loss_rnnt 12.385576 hw_loss 0.126435 lr 0.00043844 rank 5
2023-02-21 13:34:36,217 DEBUG TRAIN Batch 15/4900 loss 20.901350 loss_att 21.771015 loss_ctc 28.995708 loss_rnnt 19.563950 hw_loss 0.157916 lr 0.00043854 rank 3
2023-02-21 13:35:57,390 DEBUG TRAIN Batch 15/5000 loss 11.591745 loss_att 13.095860 loss_ctc 15.756335 loss_rnnt 10.664864 hw_loss 0.132710 lr 0.00043839 rank 1
2023-02-21 13:35:57,390 DEBUG TRAIN Batch 15/5000 loss 5.921077 loss_att 7.826610 loss_ctc 7.881944 loss_rnnt 5.221805 hw_loss 0.106345 lr 0.00043835 rank 4
2023-02-21 13:35:57,394 DEBUG TRAIN Batch 15/5000 loss 6.725089 loss_att 7.770890 loss_ctc 10.561779 loss_rnnt 5.917398 hw_loss 0.163072 lr 0.00043827 rank 5
2023-02-21 13:35:57,396 DEBUG TRAIN Batch 15/5000 loss 15.405721 loss_att 15.891335 loss_ctc 18.211613 loss_rnnt 14.864939 hw_loss 0.130387 lr 0.00043835 rank 6
2023-02-21 13:35:57,396 DEBUG TRAIN Batch 15/5000 loss 9.635252 loss_att 12.756679 loss_ctc 13.597324 loss_rnnt 8.341858 hw_loss 0.264060 lr 0.00043832 rank 0
2023-02-21 13:35:57,397 DEBUG TRAIN Batch 15/5000 loss 9.295132 loss_att 14.567366 loss_ctc 11.892458 loss_rnnt 7.832289 hw_loss 0.116411 lr 0.00043838 rank 3
2023-02-21 13:35:57,411 DEBUG TRAIN Batch 15/5000 loss 14.636164 loss_att 19.021374 loss_ctc 20.368174 loss_rnnt 12.929892 hw_loss 0.121804 lr 0.00043826 rank 7
2023-02-21 13:35:57,421 DEBUG TRAIN Batch 15/5000 loss 9.882292 loss_att 12.325321 loss_ctc 14.057884 loss_rnnt 8.772721 hw_loss 0.120412 lr 0.00043834 rank 2
2023-02-21 13:37:16,059 DEBUG TRAIN Batch 15/5100 loss 20.290051 loss_att 23.561821 loss_ctc 27.403601 loss_rnnt 18.596413 hw_loss 0.170268 lr 0.00043809 rank 7
2023-02-21 13:37:16,066 DEBUG TRAIN Batch 15/5100 loss 13.627651 loss_att 13.505134 loss_ctc 17.344299 loss_rnnt 12.955342 hw_loss 0.377358 lr 0.00043821 rank 3
2023-02-21 13:37:16,066 DEBUG TRAIN Batch 15/5100 loss 8.550811 loss_att 10.778575 loss_ctc 12.962211 loss_rnnt 7.469911 hw_loss 0.088425 lr 0.00043817 rank 2
2023-02-21 13:37:16,069 DEBUG TRAIN Batch 15/5100 loss 9.074259 loss_att 11.577593 loss_ctc 12.120132 loss_rnnt 8.089205 hw_loss 0.146756 lr 0.00043815 rank 0
2023-02-21 13:37:16,069 DEBUG TRAIN Batch 15/5100 loss 10.138839 loss_att 16.295731 loss_ctc 15.651560 loss_rnnt 8.157677 hw_loss 0.027664 lr 0.00043822 rank 1
2023-02-21 13:37:16,070 DEBUG TRAIN Batch 15/5100 loss 11.898470 loss_att 11.775230 loss_ctc 14.124824 loss_rnnt 11.549097 hw_loss 0.144698 lr 0.00043818 rank 4
2023-02-21 13:37:16,071 DEBUG TRAIN Batch 15/5100 loss 10.257049 loss_att 11.603134 loss_ctc 14.404380 loss_rnnt 9.351660 hw_loss 0.155990 lr 0.00043811 rank 5
2023-02-21 13:37:16,071 DEBUG TRAIN Batch 15/5100 loss 12.692008 loss_att 12.790753 loss_ctc 16.354145 loss_rnnt 12.047561 hw_loss 0.255776 lr 0.00043818 rank 6
2023-02-21 13:38:34,109 DEBUG TRAIN Batch 15/5200 loss 18.612036 loss_att 23.538397 loss_ctc 26.609459 loss_rnnt 16.438549 hw_loss 0.228549 lr 0.00043801 rank 4
2023-02-21 13:38:34,109 DEBUG TRAIN Batch 15/5200 loss 13.582771 loss_att 15.542157 loss_ctc 19.490507 loss_rnnt 12.313435 hw_loss 0.168303 lr 0.00043805 rank 1
2023-02-21 13:38:34,110 DEBUG TRAIN Batch 15/5200 loss 5.373271 loss_att 9.025999 loss_ctc 10.788321 loss_rnnt 3.870202 hw_loss 0.094719 lr 0.00043792 rank 7
2023-02-21 13:38:34,113 DEBUG TRAIN Batch 15/5200 loss 13.057795 loss_att 14.483698 loss_ctc 15.818253 loss_rnnt 12.390418 hw_loss 0.026503 lr 0.00043794 rank 5
2023-02-21 13:38:34,113 DEBUG TRAIN Batch 15/5200 loss 20.058020 loss_att 21.314280 loss_ctc 33.736546 loss_rnnt 17.968607 hw_loss 0.026919 lr 0.00043804 rank 3
2023-02-21 13:38:34,115 DEBUG TRAIN Batch 15/5200 loss 12.624489 loss_att 14.567429 loss_ctc 17.061434 loss_rnnt 11.597457 hw_loss 0.087849 lr 0.00043801 rank 6
2023-02-21 13:38:34,115 DEBUG TRAIN Batch 15/5200 loss 6.812929 loss_att 7.721067 loss_ctc 8.596644 loss_rnnt 6.187909 hw_loss 0.385430 lr 0.00043800 rank 2
2023-02-21 13:38:34,116 DEBUG TRAIN Batch 15/5200 loss 8.814211 loss_att 12.715043 loss_ctc 13.766131 loss_rnnt 7.338821 hw_loss 0.065564 lr 0.00043798 rank 0
2023-02-21 13:39:54,621 DEBUG TRAIN Batch 15/5300 loss 10.636483 loss_att 14.219206 loss_ctc 17.146446 loss_rnnt 9.037905 hw_loss 0.026322 lr 0.00043776 rank 7
2023-02-21 13:39:54,624 DEBUG TRAIN Batch 15/5300 loss 6.800331 loss_att 10.218181 loss_ctc 12.474344 loss_rnnt 5.225549 hw_loss 0.252517 lr 0.00043788 rank 1
2023-02-21 13:39:54,624 DEBUG TRAIN Batch 15/5300 loss 2.633101 loss_att 6.228306 loss_ctc 2.928781 loss_rnnt 1.826671 hw_loss 0.089934 lr 0.00043783 rank 2
2023-02-21 13:39:54,626 DEBUG TRAIN Batch 15/5300 loss 10.360159 loss_att 15.521605 loss_ctc 13.662313 loss_rnnt 8.797164 hw_loss 0.169534 lr 0.00043784 rank 6
2023-02-21 13:39:54,626 DEBUG TRAIN Batch 15/5300 loss 6.572310 loss_att 9.512777 loss_ctc 7.041786 loss_rnnt 5.872977 hw_loss 0.091205 lr 0.00043787 rank 3
2023-02-21 13:39:54,627 DEBUG TRAIN Batch 15/5300 loss 11.736621 loss_att 13.724590 loss_ctc 19.026979 loss_rnnt 10.282286 hw_loss 0.158799 lr 0.00043785 rank 4
2023-02-21 13:39:54,630 DEBUG TRAIN Batch 15/5300 loss 12.227343 loss_att 14.794184 loss_ctc 17.570862 loss_rnnt 10.941428 hw_loss 0.112646 lr 0.00043782 rank 0
2023-02-21 13:39:54,656 DEBUG TRAIN Batch 15/5300 loss 13.719637 loss_att 14.426989 loss_ctc 17.191299 loss_rnnt 13.045221 hw_loss 0.131356 lr 0.00043777 rank 5
2023-02-21 13:41:14,986 DEBUG TRAIN Batch 15/5400 loss 12.320526 loss_att 13.634439 loss_ctc 16.787983 loss_rnnt 11.420784 hw_loss 0.077434 lr 0.00043768 rank 4
2023-02-21 13:41:14,986 DEBUG TRAIN Batch 15/5400 loss 16.313169 loss_att 17.806334 loss_ctc 22.323030 loss_rnnt 15.174608 hw_loss 0.072400 lr 0.00043771 rank 1
2023-02-21 13:41:14,988 DEBUG TRAIN Batch 15/5400 loss 5.640415 loss_att 8.342984 loss_ctc 7.791927 loss_rnnt 4.797637 hw_loss 0.028868 lr 0.00043766 rank 2
2023-02-21 13:41:14,988 DEBUG TRAIN Batch 15/5400 loss 3.656516 loss_att 6.986894 loss_ctc 4.569202 loss_rnnt 2.764100 hw_loss 0.196218 lr 0.00043760 rank 5
2023-02-21 13:41:14,988 DEBUG TRAIN Batch 15/5400 loss 8.851495 loss_att 13.432615 loss_ctc 13.808751 loss_rnnt 7.139162 hw_loss 0.253389 lr 0.00043759 rank 7
2023-02-21 13:41:14,988 DEBUG TRAIN Batch 15/5400 loss 21.041403 loss_att 22.549437 loss_ctc 25.027060 loss_rnnt 20.192780 hw_loss 0.029237 lr 0.00043765 rank 0
2023-02-21 13:41:14,991 DEBUG TRAIN Batch 15/5400 loss 10.984628 loss_att 14.089507 loss_ctc 13.068038 loss_rnnt 10.010719 hw_loss 0.140897 lr 0.00043770 rank 3
2023-02-21 13:41:15,038 DEBUG TRAIN Batch 15/5400 loss 13.267393 loss_att 15.417015 loss_ctc 14.660379 loss_rnnt 12.533738 hw_loss 0.221247 lr 0.00043767 rank 6
2023-02-21 13:42:33,389 DEBUG TRAIN Batch 15/5500 loss 5.634086 loss_att 9.446372 loss_ctc 9.785091 loss_rnnt 4.230847 hw_loss 0.163713 lr 0.00043750 rank 2
2023-02-21 13:42:33,395 DEBUG TRAIN Batch 15/5500 loss 23.836142 loss_att 23.894402 loss_ctc 30.168169 loss_rnnt 22.906052 hw_loss 0.139063 lr 0.00043742 rank 7
2023-02-21 13:42:33,397 DEBUG TRAIN Batch 15/5500 loss 25.260031 loss_att 29.627403 loss_ctc 30.334721 loss_rnnt 23.621548 hw_loss 0.165718 lr 0.00043755 rank 1
2023-02-21 13:42:33,397 DEBUG TRAIN Batch 15/5500 loss 19.675686 loss_att 21.953739 loss_ctc 24.184484 loss_rnnt 18.594334 hw_loss 0.046066 lr 0.00043751 rank 4
2023-02-21 13:42:33,399 DEBUG TRAIN Batch 15/5500 loss 11.565659 loss_att 16.851816 loss_ctc 20.181505 loss_rnnt 9.305010 hw_loss 0.102443 lr 0.00043748 rank 0
2023-02-21 13:42:33,399 DEBUG TRAIN Batch 15/5500 loss 9.297585 loss_att 13.206930 loss_ctc 14.471977 loss_rnnt 7.786470 hw_loss 0.073735 lr 0.00043751 rank 6
2023-02-21 13:42:33,400 DEBUG TRAIN Batch 15/5500 loss 15.222506 loss_att 18.550325 loss_ctc 17.258347 loss_rnnt 14.251072 hw_loss 0.064544 lr 0.00043744 rank 5
2023-02-21 13:42:33,401 DEBUG TRAIN Batch 15/5500 loss 14.890217 loss_att 16.054077 loss_ctc 18.922050 loss_rnnt 14.040888 hw_loss 0.148084 lr 0.00043754 rank 3
2023-02-21 13:43:53,165 DEBUG TRAIN Batch 15/5600 loss 8.262957 loss_att 9.237070 loss_ctc 10.285918 loss_rnnt 7.750575 hw_loss 0.089684 lr 0.00043725 rank 7
2023-02-21 13:43:53,167 DEBUG TRAIN Batch 15/5600 loss 10.511672 loss_att 11.924203 loss_ctc 13.385027 loss_rnnt 9.797960 hw_loss 0.090173 lr 0.00043734 rank 4
2023-02-21 13:43:53,170 DEBUG TRAIN Batch 15/5600 loss 12.377839 loss_att 14.290402 loss_ctc 14.846034 loss_rnnt 11.617779 hw_loss 0.090854 lr 0.00043738 rank 1
2023-02-21 13:43:53,170 DEBUG TRAIN Batch 15/5600 loss 4.365664 loss_att 6.934192 loss_ctc 4.718503 loss_rnnt 3.752984 hw_loss 0.097369 lr 0.00043733 rank 2
2023-02-21 13:43:53,172 DEBUG TRAIN Batch 15/5600 loss 4.641971 loss_att 6.912972 loss_ctc 7.319070 loss_rnnt 3.788890 hw_loss 0.078627 lr 0.00043734 rank 6
2023-02-21 13:43:53,177 DEBUG TRAIN Batch 15/5600 loss 10.771127 loss_att 12.973429 loss_ctc 13.643775 loss_rnnt 9.873584 hw_loss 0.138866 lr 0.00043727 rank 5
2023-02-21 13:43:53,178 DEBUG TRAIN Batch 15/5600 loss 7.425504 loss_att 9.320798 loss_ctc 11.779266 loss_rnnt 6.346446 hw_loss 0.224059 lr 0.00043737 rank 3
2023-02-21 13:43:53,223 DEBUG TRAIN Batch 15/5600 loss 7.728453 loss_att 12.262237 loss_ctc 9.991444 loss_rnnt 6.491577 hw_loss 0.053226 lr 0.00043731 rank 0
2023-02-21 13:45:14,907 DEBUG TRAIN Batch 15/5700 loss 6.224761 loss_att 11.383854 loss_ctc 7.235813 loss_rnnt 5.030690 hw_loss 0.051460 lr 0.00043721 rank 1
2023-02-21 13:45:14,907 DEBUG TRAIN Batch 15/5700 loss 13.249662 loss_att 12.697796 loss_ctc 17.154215 loss_rnnt 12.723492 hw_loss 0.217381 lr 0.00043717 rank 6
2023-02-21 13:45:14,910 DEBUG TRAIN Batch 15/5700 loss 10.808392 loss_att 14.746200 loss_ctc 16.612326 loss_rnnt 9.128746 hw_loss 0.221672 lr 0.00043715 rank 0
2023-02-21 13:45:14,913 DEBUG TRAIN Batch 15/5700 loss 7.091734 loss_att 9.441612 loss_ctc 10.692350 loss_rnnt 6.125925 hw_loss 0.029535 lr 0.00043709 rank 7
2023-02-21 13:45:14,913 DEBUG TRAIN Batch 15/5700 loss 8.310785 loss_att 12.181093 loss_ctc 11.571836 loss_rnnt 6.979992 hw_loss 0.228610 lr 0.00043718 rank 4
2023-02-21 13:45:14,915 DEBUG TRAIN Batch 15/5700 loss 9.320805 loss_att 13.481858 loss_ctc 14.537976 loss_rnnt 7.744371 hw_loss 0.091124 lr 0.00043716 rank 2
2023-02-21 13:45:14,915 DEBUG TRAIN Batch 15/5700 loss 9.682856 loss_att 10.984446 loss_ctc 13.023714 loss_rnnt 8.844823 hw_loss 0.247999 lr 0.00043720 rank 3
2023-02-21 13:45:14,967 DEBUG TRAIN Batch 15/5700 loss 16.750584 loss_att 18.753654 loss_ctc 19.985945 loss_rnnt 15.853340 hw_loss 0.122339 lr 0.00043710 rank 5
2023-02-21 13:46:31,673 DEBUG TRAIN Batch 15/5800 loss 6.978198 loss_att 7.377623 loss_ctc 9.084064 loss_rnnt 6.541978 hw_loss 0.141661 lr 0.00043700 rank 2
2023-02-21 13:46:31,676 DEBUG TRAIN Batch 15/5800 loss 5.600787 loss_att 8.460197 loss_ctc 8.129127 loss_rnnt 4.644756 hw_loss 0.088194 lr 0.00043701 rank 4
2023-02-21 13:46:31,677 DEBUG TRAIN Batch 15/5800 loss 4.630887 loss_att 6.924977 loss_ctc 5.548099 loss_rnnt 4.022675 hw_loss 0.050812 lr 0.00043705 rank 1
2023-02-21 13:46:31,681 DEBUG TRAIN Batch 15/5800 loss 13.847565 loss_att 14.172042 loss_ctc 17.482698 loss_rnnt 13.238783 hw_loss 0.111002 lr 0.00043692 rank 7
2023-02-21 13:46:31,684 DEBUG TRAIN Batch 15/5800 loss 3.960766 loss_att 9.544126 loss_ctc 4.323247 loss_rnnt 2.705691 hw_loss 0.168884 lr 0.00043698 rank 0
2023-02-21 13:46:31,685 DEBUG TRAIN Batch 15/5800 loss 5.901203 loss_att 8.933457 loss_ctc 8.881134 loss_rnnt 4.883886 hw_loss 0.025391 lr 0.00043700 rank 6
2023-02-21 13:46:31,688 DEBUG TRAIN Batch 15/5800 loss 14.502032 loss_att 16.253376 loss_ctc 20.420139 loss_rnnt 13.303465 hw_loss 0.111034 lr 0.00043703 rank 3
2023-02-21 13:46:32,022 DEBUG TRAIN Batch 15/5800 loss 9.846091 loss_att 11.144362 loss_ctc 15.255898 loss_rnnt 8.826604 hw_loss 0.072234 lr 0.00043693 rank 5
2023-02-21 13:47:50,133 DEBUG TRAIN Batch 15/5900 loss 11.734876 loss_att 17.174515 loss_ctc 13.802382 loss_rnnt 10.266954 hw_loss 0.195610 lr 0.00043684 rank 4
2023-02-21 13:47:50,135 DEBUG TRAIN Batch 15/5900 loss 4.952314 loss_att 7.601299 loss_ctc 10.852564 loss_rnnt 3.586986 hw_loss 0.091557 lr 0.00043675 rank 7
2023-02-21 13:47:50,135 DEBUG TRAIN Batch 15/5900 loss 9.044729 loss_att 10.422690 loss_ctc 9.626623 loss_rnnt 8.657873 hw_loss 0.063147 lr 0.00043681 rank 0
2023-02-21 13:47:50,137 DEBUG TRAIN Batch 15/5900 loss 12.351233 loss_att 16.594767 loss_ctc 12.309299 loss_rnnt 11.453586 hw_loss 0.102246 lr 0.00043688 rank 1
2023-02-21 13:47:50,139 DEBUG TRAIN Batch 15/5900 loss 12.678432 loss_att 18.147114 loss_ctc 18.809807 loss_rnnt 10.732456 hw_loss 0.065105 lr 0.00043683 rank 2
2023-02-21 13:47:50,144 DEBUG TRAIN Batch 15/5900 loss 9.557864 loss_att 10.970326 loss_ctc 15.354921 loss_rnnt 8.467693 hw_loss 0.065133 lr 0.00043677 rank 5
2023-02-21 13:47:50,145 DEBUG TRAIN Batch 15/5900 loss 7.882268 loss_att 10.533631 loss_ctc 9.878326 loss_rnnt 7.038072 hw_loss 0.089591 lr 0.00043687 rank 3
2023-02-21 13:47:50,189 DEBUG TRAIN Batch 15/5900 loss 11.159050 loss_att 14.842676 loss_ctc 15.446284 loss_rnnt 9.765444 hw_loss 0.159843 lr 0.00043684 rank 6
2023-02-21 13:49:10,726 DEBUG TRAIN Batch 15/6000 loss 18.118061 loss_att 25.350897 loss_ctc 21.472271 loss_rnnt 16.186310 hw_loss 0.071169 lr 0.00043666 rank 2
2023-02-21 13:49:10,728 DEBUG TRAIN Batch 15/6000 loss 11.269324 loss_att 16.507679 loss_ctc 15.538532 loss_rnnt 9.535248 hw_loss 0.219710 lr 0.00043668 rank 4
2023-02-21 13:49:10,731 DEBUG TRAIN Batch 15/6000 loss 12.868096 loss_att 17.254421 loss_ctc 17.865963 loss_rnnt 11.228062 hw_loss 0.180727 lr 0.00043667 rank 6
2023-02-21 13:49:10,731 DEBUG TRAIN Batch 15/6000 loss 5.507997 loss_att 8.746548 loss_ctc 5.396320 loss_rnnt 4.859592 hw_loss 0.029224 lr 0.00043659 rank 7
2023-02-21 13:49:10,733 DEBUG TRAIN Batch 15/6000 loss 10.319931 loss_att 13.750998 loss_ctc 17.465500 loss_rnnt 8.648710 hw_loss 0.060495 lr 0.00043670 rank 3
2023-02-21 13:49:10,738 DEBUG TRAIN Batch 15/6000 loss 8.313408 loss_att 10.366043 loss_ctc 11.372265 loss_rnnt 7.464013 hw_loss 0.058163 lr 0.00043660 rank 5
2023-02-21 13:49:10,739 DEBUG TRAIN Batch 15/6000 loss 25.220402 loss_att 26.318497 loss_ctc 33.937935 loss_rnnt 23.720402 hw_loss 0.221327 lr 0.00043665 rank 0
2023-02-21 13:49:10,753 DEBUG TRAIN Batch 15/6000 loss 15.275211 loss_att 17.083351 loss_ctc 21.166729 loss_rnnt 14.053839 hw_loss 0.139142 lr 0.00043671 rank 1
2023-02-21 13:50:30,612 DEBUG TRAIN Batch 15/6100 loss 12.542818 loss_att 13.709143 loss_ctc 17.044029 loss_rnnt 11.685327 hw_loss 0.045121 lr 0.00043655 rank 1
2023-02-21 13:50:30,614 DEBUG TRAIN Batch 15/6100 loss 8.735147 loss_att 13.000608 loss_ctc 15.748615 loss_rnnt 6.899491 hw_loss 0.088938 lr 0.00043650 rank 6
2023-02-21 13:50:30,615 DEBUG TRAIN Batch 15/6100 loss 16.053854 loss_att 22.270802 loss_ctc 25.307835 loss_rnnt 13.554676 hw_loss 0.041108 lr 0.00043651 rank 4
2023-02-21 13:50:30,617 DEBUG TRAIN Batch 15/6100 loss 8.002321 loss_att 13.957964 loss_ctc 11.375144 loss_rnnt 6.290147 hw_loss 0.133753 lr 0.00043648 rank 0
2023-02-21 13:50:30,617 DEBUG TRAIN Batch 15/6100 loss 8.583043 loss_att 14.154002 loss_ctc 14.358690 loss_rnnt 6.613487 hw_loss 0.159896 lr 0.00043650 rank 2
2023-02-21 13:50:30,618 DEBUG TRAIN Batch 15/6100 loss 4.557818 loss_att 7.465256 loss_ctc 6.277584 loss_rnnt 3.647298 hw_loss 0.186997 lr 0.00043642 rank 7
2023-02-21 13:50:30,622 DEBUG TRAIN Batch 15/6100 loss 11.287721 loss_att 14.487787 loss_ctc 16.883472 loss_rnnt 9.878827 hw_loss 0.042713 lr 0.00043643 rank 5
2023-02-21 13:50:30,670 DEBUG TRAIN Batch 15/6100 loss 17.934870 loss_att 21.793703 loss_ctc 22.076914 loss_rnnt 16.549240 hw_loss 0.115481 lr 0.00043653 rank 3
2023-02-21 13:51:48,618 DEBUG TRAIN Batch 15/6200 loss 13.753536 loss_att 17.979057 loss_ctc 17.379589 loss_rnnt 12.385983 hw_loss 0.073080 lr 0.00043625 rank 7
2023-02-21 13:51:48,618 DEBUG TRAIN Batch 15/6200 loss 18.958189 loss_att 22.778854 loss_ctc 23.504738 loss_rnnt 17.504168 hw_loss 0.156903 lr 0.00043637 rank 3
2023-02-21 13:51:48,619 DEBUG TRAIN Batch 15/6200 loss 13.796319 loss_att 13.811004 loss_ctc 17.025846 loss_rnnt 13.291293 hw_loss 0.134036 lr 0.00043638 rank 1
2023-02-21 13:51:48,620 DEBUG TRAIN Batch 15/6200 loss 10.664862 loss_att 12.029681 loss_ctc 13.805412 loss_rnnt 9.888031 hw_loss 0.159615 lr 0.00043634 rank 6
2023-02-21 13:51:48,621 DEBUG TRAIN Batch 15/6200 loss 16.559490 loss_att 18.809948 loss_ctc 21.587584 loss_rnnt 15.360943 hw_loss 0.146331 lr 0.00043634 rank 4
2023-02-21 13:51:48,622 DEBUG TRAIN Batch 15/6200 loss 9.865228 loss_att 13.222435 loss_ctc 13.386284 loss_rnnt 8.684158 hw_loss 0.075287 lr 0.00043633 rank 2
2023-02-21 13:51:48,625 DEBUG TRAIN Batch 15/6200 loss 22.141611 loss_att 24.733072 loss_ctc 23.562424 loss_rnnt 21.402363 hw_loss 0.059087 lr 0.00043631 rank 0
2023-02-21 13:51:48,628 DEBUG TRAIN Batch 15/6200 loss 14.875683 loss_att 17.942158 loss_ctc 21.167414 loss_rnnt 13.390841 hw_loss 0.061217 lr 0.00043627 rank 5
2023-02-21 13:53:06,411 DEBUG TRAIN Batch 15/6300 loss 13.654699 loss_att 15.459415 loss_ctc 22.395275 loss_rnnt 12.060425 hw_loss 0.127350 lr 0.00043617 rank 6
2023-02-21 13:53:06,413 DEBUG TRAIN Batch 15/6300 loss 11.533237 loss_att 14.384680 loss_ctc 18.114038 loss_rnnt 10.040777 hw_loss 0.083871 lr 0.00043621 rank 1
2023-02-21 13:53:06,414 DEBUG TRAIN Batch 15/6300 loss 8.150221 loss_att 8.813234 loss_ctc 11.997595 loss_rnnt 7.367061 hw_loss 0.257952 lr 0.00043616 rank 2
2023-02-21 13:53:06,416 DEBUG TRAIN Batch 15/6300 loss 7.299718 loss_att 10.406479 loss_ctc 15.464419 loss_rnnt 5.505557 hw_loss 0.157842 lr 0.00043618 rank 4
2023-02-21 13:53:06,418 DEBUG TRAIN Batch 15/6300 loss 8.552444 loss_att 9.829392 loss_ctc 13.261000 loss_rnnt 7.597734 hw_loss 0.134087 lr 0.00043615 rank 0
2023-02-21 13:53:06,419 DEBUG TRAIN Batch 15/6300 loss 18.713997 loss_att 18.408741 loss_ctc 19.512926 loss_rnnt 18.560608 hw_loss 0.202340 lr 0.00043620 rank 3
2023-02-21 13:53:06,419 DEBUG TRAIN Batch 15/6300 loss 10.300593 loss_att 11.978985 loss_ctc 15.230490 loss_rnnt 9.223263 hw_loss 0.158121 lr 0.00043610 rank 5
2023-02-21 13:53:06,420 DEBUG TRAIN Batch 15/6300 loss 15.964367 loss_att 17.374113 loss_ctc 19.588194 loss_rnnt 15.092907 hw_loss 0.199375 lr 0.00043609 rank 7
2023-02-21 13:54:28,603 DEBUG TRAIN Batch 15/6400 loss 6.832993 loss_att 5.797338 loss_ctc 8.254822 loss_rnnt 6.778584 hw_loss 0.134927 lr 0.00043592 rank 7
2023-02-21 13:54:28,604 DEBUG TRAIN Batch 15/6400 loss 9.503989 loss_att 12.131485 loss_ctc 15.280796 loss_rnnt 8.149195 hw_loss 0.110727 lr 0.00043600 rank 2
2023-02-21 13:54:28,605 DEBUG TRAIN Batch 15/6400 loss 9.691907 loss_att 11.139118 loss_ctc 11.373804 loss_rnnt 9.111944 hw_loss 0.124252 lr 0.00043598 rank 0
2023-02-21 13:54:28,605 DEBUG TRAIN Batch 15/6400 loss 3.374422 loss_att 6.766649 loss_ctc 5.283087 loss_rnnt 2.396267 hw_loss 0.084790 lr 0.00043601 rank 4
2023-02-21 13:54:28,607 DEBUG TRAIN Batch 15/6400 loss 19.000610 loss_att 22.102791 loss_ctc 20.204929 loss_rnnt 18.162508 hw_loss 0.107045 lr 0.00043605 rank 1
2023-02-21 13:54:28,610 DEBUG TRAIN Batch 15/6400 loss 24.584711 loss_att 27.794394 loss_ctc 32.617867 loss_rnnt 22.857101 hw_loss 0.027346 lr 0.00043594 rank 5
2023-02-21 13:54:28,615 DEBUG TRAIN Batch 15/6400 loss 25.358721 loss_att 31.053198 loss_ctc 36.360802 loss_rnnt 22.658737 hw_loss 0.176522 lr 0.00043604 rank 3
2023-02-21 13:54:28,652 DEBUG TRAIN Batch 15/6400 loss 10.361052 loss_att 15.980111 loss_ctc 16.359770 loss_rnnt 8.411970 hw_loss 0.047700 lr 0.00043601 rank 6
2023-02-21 13:55:47,317 DEBUG TRAIN Batch 15/6500 loss 20.740025 loss_att 24.336370 loss_ctc 24.896421 loss_rnnt 19.434181 hw_loss 0.060729 lr 0.00043588 rank 1
2023-02-21 13:55:47,322 DEBUG TRAIN Batch 15/6500 loss 9.774534 loss_att 12.588734 loss_ctc 9.766380 loss_rnnt 9.167397 hw_loss 0.085096 lr 0.00043584 rank 6
2023-02-21 13:55:47,323 DEBUG TRAIN Batch 15/6500 loss 10.297684 loss_att 13.958853 loss_ctc 11.267656 loss_rnnt 9.423151 hw_loss 0.024317 lr 0.00043583 rank 2
2023-02-21 13:55:47,326 DEBUG TRAIN Batch 15/6500 loss 17.756704 loss_att 23.714890 loss_ctc 27.215200 loss_rnnt 15.234236 hw_loss 0.130685 lr 0.00043587 rank 3
2023-02-21 13:55:47,327 DEBUG TRAIN Batch 15/6500 loss 15.529332 loss_att 18.669432 loss_ctc 21.556652 loss_rnnt 13.978300 hw_loss 0.223819 lr 0.00043585 rank 4
2023-02-21 13:55:47,327 DEBUG TRAIN Batch 15/6500 loss 11.957678 loss_att 18.154835 loss_ctc 15.340904 loss_rnnt 10.232209 hw_loss 0.065514 lr 0.00043577 rank 5
2023-02-21 13:55:47,329 DEBUG TRAIN Batch 15/6500 loss 17.869823 loss_att 19.316269 loss_ctc 20.205383 loss_rnnt 17.232229 hw_loss 0.069185 lr 0.00043576 rank 7
2023-02-21 13:55:47,333 DEBUG TRAIN Batch 15/6500 loss 14.787235 loss_att 18.441725 loss_ctc 17.234070 loss_rnnt 13.681956 hw_loss 0.090255 lr 0.00043582 rank 0
2023-02-21 13:57:05,106 DEBUG TRAIN Batch 15/6600 loss 15.356849 loss_att 17.848747 loss_ctc 17.632114 loss_rnnt 14.460333 hw_loss 0.177691 lr 0.00043565 rank 0
2023-02-21 13:57:05,106 DEBUG TRAIN Batch 15/6600 loss 12.352913 loss_att 16.653534 loss_ctc 15.422043 loss_rnnt 11.003515 hw_loss 0.150109 lr 0.00043572 rank 1
2023-02-21 13:57:05,107 DEBUG TRAIN Batch 15/6600 loss 10.054383 loss_att 14.334723 loss_ctc 14.980490 loss_rnnt 8.455642 hw_loss 0.160988 lr 0.00043559 rank 7
2023-02-21 13:57:05,108 DEBUG TRAIN Batch 15/6600 loss 7.509966 loss_att 10.298998 loss_ctc 16.390509 loss_rnnt 5.754519 hw_loss 0.025440 lr 0.00043567 rank 2
2023-02-21 13:57:05,109 DEBUG TRAIN Batch 15/6600 loss 19.264597 loss_att 19.842386 loss_ctc 25.449865 loss_rnnt 18.268166 hw_loss 0.105318 lr 0.00043568 rank 4
2023-02-21 13:57:05,111 DEBUG TRAIN Batch 15/6600 loss 12.656596 loss_att 15.489009 loss_ctc 20.864708 loss_rnnt 10.955481 hw_loss 0.075409 lr 0.00043567 rank 6
2023-02-21 13:57:05,110 DEBUG TRAIN Batch 15/6600 loss 11.738797 loss_att 13.196254 loss_ctc 15.396856 loss_rnnt 10.900828 hw_loss 0.110134 lr 0.00043561 rank 5
2023-02-21 13:57:05,118 DEBUG TRAIN Batch 15/6600 loss 12.654514 loss_att 18.683722 loss_ctc 15.017899 loss_rnnt 11.087378 hw_loss 0.086584 lr 0.00043570 rank 3
2023-02-21 13:58:24,001 DEBUG TRAIN Batch 15/6700 loss 14.663575 loss_att 16.253630 loss_ctc 20.096283 loss_rnnt 13.567080 hw_loss 0.101479 lr 0.00043543 rank 7
2023-02-21 13:58:24,005 DEBUG TRAIN Batch 15/6700 loss 8.721092 loss_att 11.196785 loss_ctc 10.789797 loss_rnnt 7.855992 hw_loss 0.176502 lr 0.00043551 rank 4
2023-02-21 13:58:24,008 DEBUG TRAIN Batch 15/6700 loss 14.641410 loss_att 20.975056 loss_ctc 15.935709 loss_rnnt 13.110724 hw_loss 0.171343 lr 0.00043550 rank 2
2023-02-21 13:58:24,009 DEBUG TRAIN Batch 15/6700 loss 13.914353 loss_att 17.692741 loss_ctc 17.807615 loss_rnnt 12.626442 hw_loss 0.024622 lr 0.00043548 rank 0
2023-02-21 13:58:24,010 DEBUG TRAIN Batch 15/6700 loss 10.375839 loss_att 12.218109 loss_ctc 13.172239 loss_rnnt 9.611814 hw_loss 0.042597 lr 0.00043555 rank 1
2023-02-21 13:58:24,013 DEBUG TRAIN Batch 15/6700 loss 12.367331 loss_att 16.119137 loss_ctc 15.202564 loss_rnnt 11.177389 hw_loss 0.115405 lr 0.00043551 rank 6
2023-02-21 13:58:24,015 DEBUG TRAIN Batch 15/6700 loss 17.270445 loss_att 16.726906 loss_ctc 26.068123 loss_rnnt 16.073954 hw_loss 0.247825 lr 0.00043554 rank 3
2023-02-21 13:58:24,018 DEBUG TRAIN Batch 15/6700 loss 10.511774 loss_att 13.414327 loss_ctc 15.157142 loss_rnnt 9.298903 hw_loss 0.024336 lr 0.00043544 rank 5
2023-02-21 13:59:43,640 DEBUG TRAIN Batch 15/6800 loss 13.847701 loss_att 14.186308 loss_ctc 16.855822 loss_rnnt 13.320777 hw_loss 0.108974 lr 0.00043534 rank 2
2023-02-21 13:59:43,644 DEBUG TRAIN Batch 15/6800 loss 10.695259 loss_att 13.944313 loss_ctc 13.588406 loss_rnnt 9.596682 hw_loss 0.118151 lr 0.00043537 rank 3
2023-02-21 13:59:43,644 DEBUG TRAIN Batch 15/6800 loss 12.682781 loss_att 14.057415 loss_ctc 18.577873 loss_rnnt 11.560785 hw_loss 0.114481 lr 0.00043535 rank 4
2023-02-21 13:59:43,645 DEBUG TRAIN Batch 15/6800 loss 4.206050 loss_att 6.692054 loss_ctc 5.531432 loss_rnnt 3.470893 hw_loss 0.114823 lr 0.00043539 rank 1
2023-02-21 13:59:43,646 DEBUG TRAIN Batch 15/6800 loss 6.701546 loss_att 10.734173 loss_ctc 8.471601 loss_rnnt 5.591068 hw_loss 0.127399 lr 0.00043527 rank 5
2023-02-21 13:59:43,646 DEBUG TRAIN Batch 15/6800 loss 16.404133 loss_att 19.446510 loss_ctc 18.856510 loss_rnnt 15.359156 hw_loss 0.205346 lr 0.00043526 rank 7
2023-02-21 13:59:43,648 DEBUG TRAIN Batch 15/6800 loss 12.685199 loss_att 15.600686 loss_ctc 19.587791 loss_rnnt 11.081193 hw_loss 0.188554 lr 0.00043534 rank 6
2023-02-21 13:59:43,695 DEBUG TRAIN Batch 15/6800 loss 23.259617 loss_att 25.293247 loss_ctc 29.918148 loss_rnnt 21.932240 hw_loss 0.061585 lr 0.00043532 rank 0
2023-02-21 14:01:00,569 DEBUG TRAIN Batch 15/6900 loss 12.275484 loss_att 12.632360 loss_ctc 15.411746 loss_rnnt 11.660191 hw_loss 0.235780 lr 0.00043518 rank 6
2023-02-21 14:01:00,570 DEBUG TRAIN Batch 15/6900 loss 15.203543 loss_att 15.117851 loss_ctc 18.371880 loss_rnnt 14.751592 hw_loss 0.087456 lr 0.00043522 rank 1
2023-02-21 14:01:00,575 DEBUG TRAIN Batch 15/6900 loss 11.060439 loss_att 12.227199 loss_ctc 12.673913 loss_rnnt 10.395192 hw_loss 0.406435 lr 0.00043510 rank 7
2023-02-21 14:01:00,576 DEBUG TRAIN Batch 15/6900 loss 8.907009 loss_att 13.063276 loss_ctc 16.120560 loss_rnnt 7.095850 hw_loss 0.033938 lr 0.00043517 rank 2
2023-02-21 14:01:00,576 DEBUG TRAIN Batch 15/6900 loss 8.740610 loss_att 11.777596 loss_ctc 12.524917 loss_rnnt 7.520457 hw_loss 0.202838 lr 0.00043518 rank 4
2023-02-21 14:01:00,577 DEBUG TRAIN Batch 15/6900 loss 10.541284 loss_att 12.072541 loss_ctc 12.359325 loss_rnnt 9.940557 hw_loss 0.097628 lr 0.00043515 rank 0
2023-02-21 14:01:00,579 DEBUG TRAIN Batch 15/6900 loss 8.095702 loss_att 10.199569 loss_ctc 8.601666 loss_rnnt 7.520650 hw_loss 0.162781 lr 0.00043521 rank 3
2023-02-21 14:01:00,580 DEBUG TRAIN Batch 15/6900 loss 4.318419 loss_att 7.625041 loss_ctc 6.363307 loss_rnnt 3.278413 hw_loss 0.198805 lr 0.00043511 rank 5
2023-02-21 14:02:19,141 DEBUG TRAIN Batch 15/7000 loss 13.075218 loss_att 15.776507 loss_ctc 15.031013 loss_rnnt 12.258273 hw_loss 0.029840 lr 0.00043501 rank 6
2023-02-21 14:02:19,142 DEBUG TRAIN Batch 15/7000 loss 11.340863 loss_att 14.056738 loss_ctc 17.905857 loss_rnnt 9.883484 hw_loss 0.072884 lr 0.00043502 rank 4
2023-02-21 14:02:19,143 DEBUG TRAIN Batch 15/7000 loss 11.540794 loss_att 12.135225 loss_ctc 14.933846 loss_rnnt 10.895292 hw_loss 0.139143 lr 0.00043493 rank 7
2023-02-21 14:02:19,144 DEBUG TRAIN Batch 15/7000 loss 9.188094 loss_att 11.479641 loss_ctc 12.583690 loss_rnnt 8.176284 hw_loss 0.188916 lr 0.00043501 rank 2
2023-02-21 14:02:19,145 DEBUG TRAIN Batch 15/7000 loss 8.348128 loss_att 7.874888 loss_ctc 9.363852 loss_rnnt 8.148322 hw_loss 0.298172 lr 0.00043495 rank 5
2023-02-21 14:02:19,146 DEBUG TRAIN Batch 15/7000 loss 16.244499 loss_att 18.952629 loss_ctc 23.913603 loss_rnnt 14.583824 hw_loss 0.180943 lr 0.00043506 rank 1
2023-02-21 14:02:19,149 DEBUG TRAIN Batch 15/7000 loss 26.747227 loss_att 34.155270 loss_ctc 40.480957 loss_rnnt 23.398708 hw_loss 0.067024 lr 0.00043504 rank 3
2023-02-21 14:02:19,197 DEBUG TRAIN Batch 15/7000 loss 11.312840 loss_att 12.748376 loss_ctc 13.458916 loss_rnnt 10.668120 hw_loss 0.134003 lr 0.00043499 rank 0
2023-02-21 14:03:38,742 DEBUG TRAIN Batch 15/7100 loss 11.132143 loss_att 14.432876 loss_ctc 16.642250 loss_rnnt 9.703016 hw_loss 0.064312 lr 0.00043485 rank 4
2023-02-21 14:03:38,742 DEBUG TRAIN Batch 15/7100 loss 15.210504 loss_att 18.779644 loss_ctc 21.980068 loss_rnnt 13.562536 hw_loss 0.059121 lr 0.00043477 rank 7
2023-02-21 14:03:38,743 DEBUG TRAIN Batch 15/7100 loss 6.248582 loss_att 13.674012 loss_ctc 9.223577 loss_rnnt 4.300206 hw_loss 0.124920 lr 0.00043485 rank 6
2023-02-21 14:03:38,745 DEBUG TRAIN Batch 15/7100 loss 7.723560 loss_att 12.555897 loss_ctc 10.720458 loss_rnnt 6.319469 hw_loss 0.071320 lr 0.00043489 rank 1
2023-02-21 14:03:38,747 DEBUG TRAIN Batch 15/7100 loss 11.887279 loss_att 10.600045 loss_ctc 13.940304 loss_rnnt 11.770246 hw_loss 0.188892 lr 0.00043483 rank 0
2023-02-21 14:03:38,746 DEBUG TRAIN Batch 15/7100 loss 13.879452 loss_att 17.223717 loss_ctc 16.334286 loss_rnnt 12.823191 hw_loss 0.112682 lr 0.00043488 rank 3
2023-02-21 14:03:38,750 DEBUG TRAIN Batch 15/7100 loss 11.789621 loss_att 12.669096 loss_ctc 15.486171 loss_rnnt 10.999618 hw_loss 0.227315 lr 0.00043484 rank 2
2023-02-21 14:03:38,752 DEBUG TRAIN Batch 15/7100 loss 6.117652 loss_att 10.309498 loss_ctc 7.387629 loss_rnnt 5.002351 hw_loss 0.201753 lr 0.00043478 rank 5
2023-02-21 14:04:56,416 DEBUG TRAIN Batch 15/7200 loss 10.595950 loss_att 13.448163 loss_ctc 13.595035 loss_rnnt 9.548476 hw_loss 0.144662 lr 0.00043460 rank 7
2023-02-21 14:04:56,420 DEBUG TRAIN Batch 15/7200 loss 7.294931 loss_att 10.239777 loss_ctc 9.374683 loss_rnnt 6.281474 hw_loss 0.275978 lr 0.00043473 rank 1
2023-02-21 14:04:56,419 DEBUG TRAIN Batch 15/7200 loss 4.754178 loss_att 7.592668 loss_ctc 5.338185 loss_rnnt 4.020105 hw_loss 0.165951 lr 0.00043468 rank 2
2023-02-21 14:04:56,421 DEBUG TRAIN Batch 15/7200 loss 7.464208 loss_att 11.593113 loss_ctc 11.527119 loss_rnnt 6.065210 hw_loss 0.059054 lr 0.00043469 rank 4
2023-02-21 14:04:56,422 DEBUG TRAIN Batch 15/7200 loss 11.075730 loss_att 18.291725 loss_ctc 14.637476 loss_rnnt 9.100796 hw_loss 0.106567 lr 0.00043472 rank 3
2023-02-21 14:04:56,423 DEBUG TRAIN Batch 15/7200 loss 21.195272 loss_att 20.947800 loss_ctc 23.948612 loss_rnnt 20.805593 hw_loss 0.135114 lr 0.00043469 rank 6
2023-02-21 14:04:56,426 DEBUG TRAIN Batch 15/7200 loss 7.879490 loss_att 13.136381 loss_ctc 12.934683 loss_rnnt 6.078045 hw_loss 0.142577 lr 0.00043466 rank 0
2023-02-21 14:04:56,472 DEBUG TRAIN Batch 15/7200 loss 16.614227 loss_att 22.439232 loss_ctc 22.807087 loss_rnnt 14.589872 hw_loss 0.063077 lr 0.00043462 rank 5
2023-02-21 14:06:15,034 DEBUG TRAIN Batch 15/7300 loss 15.543074 loss_att 19.485748 loss_ctc 19.198288 loss_rnnt 14.221320 hw_loss 0.085982 lr 0.00043451 rank 2
2023-02-21 14:06:15,034 DEBUG TRAIN Batch 15/7300 loss 3.007733 loss_att 5.062925 loss_ctc 5.630828 loss_rnnt 2.234555 hw_loss 0.023236 lr 0.00043453 rank 4
2023-02-21 14:06:15,037 DEBUG TRAIN Batch 15/7300 loss 16.714191 loss_att 20.967777 loss_ctc 24.438786 loss_rnnt 14.804914 hw_loss 0.053655 lr 0.00043452 rank 6
2023-02-21 14:06:15,037 DEBUG TRAIN Batch 15/7300 loss 18.161007 loss_att 23.333965 loss_ctc 23.382030 loss_rnnt 16.393871 hw_loss 0.068265 lr 0.00043444 rank 7
2023-02-21 14:06:15,038 DEBUG TRAIN Batch 15/7300 loss 19.479488 loss_att 22.378168 loss_ctc 21.597401 loss_rnnt 18.556595 hw_loss 0.113940 lr 0.00043456 rank 1
2023-02-21 14:06:15,042 DEBUG TRAIN Batch 15/7300 loss 10.039002 loss_att 11.960157 loss_ctc 12.714699 loss_rnnt 9.242551 hw_loss 0.103988 lr 0.00043450 rank 0
2023-02-21 14:06:15,045 DEBUG TRAIN Batch 15/7300 loss 8.016187 loss_att 11.431913 loss_ctc 10.981623 loss_rnnt 6.885468 hw_loss 0.097843 lr 0.00043455 rank 3
2023-02-21 14:06:15,045 DEBUG TRAIN Batch 15/7300 loss 11.025505 loss_att 12.485613 loss_ctc 11.592991 loss_rnnt 10.601532 hw_loss 0.105538 lr 0.00043445 rank 5
2023-02-21 14:07:34,594 DEBUG TRAIN Batch 15/7400 loss 11.224667 loss_att 16.636852 loss_ctc 15.114805 loss_rnnt 9.535524 hw_loss 0.165035 lr 0.00043436 rank 6
2023-02-21 14:07:34,596 DEBUG TRAIN Batch 15/7400 loss 6.960517 loss_att 9.865180 loss_ctc 10.028243 loss_rnnt 5.906282 hw_loss 0.120510 lr 0.00043440 rank 1
2023-02-21 14:07:34,598 DEBUG TRAIN Batch 15/7400 loss 20.629921 loss_att 19.642990 loss_ctc 23.737516 loss_rnnt 20.192720 hw_loss 0.412949 lr 0.00043428 rank 7
2023-02-21 14:07:34,599 DEBUG TRAIN Batch 15/7400 loss 6.609331 loss_att 11.271721 loss_ctc 9.033762 loss_rnnt 5.285793 hw_loss 0.127128 lr 0.00043433 rank 0
2023-02-21 14:07:34,601 DEBUG TRAIN Batch 15/7400 loss 9.801900 loss_att 13.083437 loss_ctc 9.780994 loss_rnnt 9.121672 hw_loss 0.050077 lr 0.00043439 rank 3
2023-02-21 14:07:34,600 DEBUG TRAIN Batch 15/7400 loss 13.176787 loss_att 15.134207 loss_ctc 23.185249 loss_rnnt 11.381122 hw_loss 0.130724 lr 0.00043436 rank 4
2023-02-21 14:07:34,601 DEBUG TRAIN Batch 15/7400 loss 14.833454 loss_att 21.610979 loss_ctc 17.690815 loss_rnnt 13.059372 hw_loss 0.070493 lr 0.00043435 rank 2
2023-02-21 14:07:34,607 DEBUG TRAIN Batch 15/7400 loss 2.896514 loss_att 5.322384 loss_ctc 3.301630 loss_rnnt 2.294650 hw_loss 0.117516 lr 0.00043429 rank 5
2023-02-21 14:08:55,024 DEBUG TRAIN Batch 15/7500 loss 8.511336 loss_att 13.231986 loss_ctc 13.704680 loss_rnnt 6.787347 hw_loss 0.163901 lr 0.00043420 rank 4
2023-02-21 14:08:55,032 DEBUG TRAIN Batch 15/7500 loss 12.784511 loss_att 15.573221 loss_ctc 17.084328 loss_rnnt 11.591497 hw_loss 0.116179 lr 0.00043417 rank 0
2023-02-21 14:08:55,033 DEBUG TRAIN Batch 15/7500 loss 9.387183 loss_att 10.124756 loss_ctc 12.474051 loss_rnnt 8.709310 hw_loss 0.222704 lr 0.00043419 rank 6
2023-02-21 14:08:55,033 DEBUG TRAIN Batch 15/7500 loss 20.199888 loss_att 22.787052 loss_ctc 25.581573 loss_rnnt 18.874222 hw_loss 0.170018 lr 0.00043419 rank 2
2023-02-21 14:08:55,035 DEBUG TRAIN Batch 15/7500 loss 6.691471 loss_att 9.400250 loss_ctc 9.310924 loss_rnnt 5.713852 hw_loss 0.162379 lr 0.00043423 rank 1
2023-02-21 14:08:55,036 DEBUG TRAIN Batch 15/7500 loss 12.864134 loss_att 16.077131 loss_ctc 16.073318 loss_rnnt 11.701772 hw_loss 0.172257 lr 0.00043412 rank 5
2023-02-21 14:08:55,036 DEBUG TRAIN Batch 15/7500 loss 11.795487 loss_att 12.461451 loss_ctc 16.419418 loss_rnnt 11.011993 hw_loss 0.063331 lr 0.00043422 rank 3
2023-02-21 14:08:55,038 DEBUG TRAIN Batch 15/7500 loss 14.782196 loss_att 13.371857 loss_ctc 15.792372 loss_rnnt 14.817467 hw_loss 0.210201 lr 0.00043411 rank 7
2023-02-21 14:10:13,920 DEBUG TRAIN Batch 15/7600 loss 9.645975 loss_att 9.665002 loss_ctc 12.001746 loss_rnnt 9.313272 hw_loss 0.027740 lr 0.00043403 rank 4
2023-02-21 14:10:13,924 DEBUG TRAIN Batch 15/7600 loss 14.900150 loss_att 22.080414 loss_ctc 27.697708 loss_rnnt 11.733114 hw_loss 0.046206 lr 0.00043403 rank 6
2023-02-21 14:10:13,925 DEBUG TRAIN Batch 15/7600 loss 8.439900 loss_att 8.573847 loss_ctc 10.991448 loss_rnnt 7.952803 hw_loss 0.225192 lr 0.00043406 rank 3
2023-02-21 14:10:13,927 DEBUG TRAIN Batch 15/7600 loss 8.224055 loss_att 8.250732 loss_ctc 10.832099 loss_rnnt 7.736032 hw_loss 0.253028 lr 0.00043395 rank 7
2023-02-21 14:10:13,931 DEBUG TRAIN Batch 15/7600 loss 12.945693 loss_att 13.060480 loss_ctc 14.896292 loss_rnnt 12.647240 hw_loss 0.028906 lr 0.00043407 rank 1
2023-02-21 14:10:13,931 DEBUG TRAIN Batch 15/7600 loss 17.578434 loss_att 17.627354 loss_ctc 18.834774 loss_rnnt 17.331425 hw_loss 0.130713 lr 0.00043402 rank 2
2023-02-21 14:10:13,932 DEBUG TRAIN Batch 15/7600 loss 9.459016 loss_att 11.579905 loss_ctc 14.188437 loss_rnnt 8.359379 hw_loss 0.084131 lr 0.00043401 rank 0
2023-02-21 14:10:13,934 DEBUG TRAIN Batch 15/7600 loss 5.189711 loss_att 6.960742 loss_ctc 8.798913 loss_rnnt 4.307625 hw_loss 0.087474 lr 0.00043396 rank 5
2023-02-21 14:11:32,498 DEBUG TRAIN Batch 15/7700 loss 7.910968 loss_att 11.405084 loss_ctc 6.761288 loss_rnnt 7.351472 hw_loss 0.026182 lr 0.00043387 rank 4
2023-02-21 14:11:32,499 DEBUG TRAIN Batch 15/7700 loss 8.463717 loss_att 8.283776 loss_ctc 10.213004 loss_rnnt 8.129321 hw_loss 0.257151 lr 0.00043386 rank 2
2023-02-21 14:11:32,505 DEBUG TRAIN Batch 15/7700 loss 18.567142 loss_att 25.066681 loss_ctc 27.345770 loss_rnnt 16.007782 hw_loss 0.166818 lr 0.00043387 rank 6
2023-02-21 14:11:32,507 DEBUG TRAIN Batch 15/7700 loss 16.881264 loss_att 23.366964 loss_ctc 25.914658 loss_rnnt 14.321135 hw_loss 0.109759 lr 0.00043391 rank 1
2023-02-21 14:11:32,509 DEBUG TRAIN Batch 15/7700 loss 11.468786 loss_att 15.627058 loss_ctc 15.709497 loss_rnnt 10.037270 hw_loss 0.064563 lr 0.00043380 rank 5
2023-02-21 14:11:32,509 DEBUG TRAIN Batch 15/7700 loss 13.317398 loss_att 17.232067 loss_ctc 19.492315 loss_rnnt 11.630893 hw_loss 0.150469 lr 0.00043378 rank 7
2023-02-21 14:11:32,512 DEBUG TRAIN Batch 15/7700 loss 6.601912 loss_att 10.633734 loss_ctc 11.243719 loss_rnnt 5.127869 hw_loss 0.091447 lr 0.00043390 rank 3
2023-02-21 14:11:32,555 DEBUG TRAIN Batch 15/7700 loss 7.585639 loss_att 8.191873 loss_ctc 10.165403 loss_rnnt 6.990015 hw_loss 0.244514 lr 0.00043384 rank 0
2023-02-21 14:12:52,473 DEBUG TRAIN Batch 15/7800 loss 8.871881 loss_att 11.498596 loss_ctc 11.926552 loss_rnnt 7.903225 hw_loss 0.067544 lr 0.00043362 rank 7
2023-02-21 14:12:52,477 DEBUG TRAIN Batch 15/7800 loss 2.152614 loss_att 4.965537 loss_ctc 3.382104 loss_rnnt 1.273660 hw_loss 0.285819 lr 0.00043371 rank 4
2023-02-21 14:12:52,479 DEBUG TRAIN Batch 15/7800 loss 6.947588 loss_att 10.664712 loss_ctc 7.937393 loss_rnnt 6.025283 hw_loss 0.087948 lr 0.00043370 rank 2
2023-02-21 14:12:52,480 DEBUG TRAIN Batch 15/7800 loss 4.391575 loss_att 10.662033 loss_ctc 8.457501 loss_rnnt 2.580937 hw_loss 0.027043 lr 0.00043370 rank 6
2023-02-21 14:12:52,481 DEBUG TRAIN Batch 15/7800 loss 6.721860 loss_att 10.936543 loss_ctc 9.963774 loss_rnnt 5.367263 hw_loss 0.148886 lr 0.00043373 rank 3
2023-02-21 14:12:52,484 DEBUG TRAIN Batch 15/7800 loss 7.661593 loss_att 10.267308 loss_ctc 11.092110 loss_rnnt 6.567662 hw_loss 0.216347 lr 0.00043374 rank 1
2023-02-21 14:12:52,485 DEBUG TRAIN Batch 15/7800 loss 4.277908 loss_att 9.401281 loss_ctc 6.924198 loss_rnnt 2.849103 hw_loss 0.096173 lr 0.00043363 rank 5
2023-02-21 14:12:52,519 DEBUG TRAIN Batch 15/7800 loss 10.930930 loss_att 13.866299 loss_ctc 17.289595 loss_rnnt 9.364305 hw_loss 0.246991 lr 0.00043368 rank 0
2023-02-21 14:14:11,270 DEBUG TRAIN Batch 15/7900 loss 2.562778 loss_att 6.685630 loss_ctc 3.046279 loss_rnnt 1.614021 hw_loss 0.111974 lr 0.00043358 rank 1
2023-02-21 14:14:11,273 DEBUG TRAIN Batch 15/7900 loss 7.717841 loss_att 10.511652 loss_ctc 11.245540 loss_rnnt 6.554090 hw_loss 0.252427 lr 0.00043353 rank 2
2023-02-21 14:14:11,274 DEBUG TRAIN Batch 15/7900 loss 11.698354 loss_att 12.489905 loss_ctc 14.953540 loss_rnnt 11.024239 hw_loss 0.153338 lr 0.00043355 rank 4
2023-02-21 14:14:11,274 DEBUG TRAIN Batch 15/7900 loss 28.968475 loss_att 30.571136 loss_ctc 40.607224 loss_rnnt 26.977354 hw_loss 0.222671 lr 0.00043346 rank 7
2023-02-21 14:14:11,275 DEBUG TRAIN Batch 15/7900 loss 10.012001 loss_att 12.174267 loss_ctc 11.189615 loss_rnnt 9.301087 hw_loss 0.227710 lr 0.00043357 rank 3
2023-02-21 14:14:11,275 DEBUG TRAIN Batch 15/7900 loss 8.692467 loss_att 11.272510 loss_ctc 11.780876 loss_rnnt 7.663340 hw_loss 0.189995 lr 0.00043354 rank 6
2023-02-21 14:14:11,277 DEBUG TRAIN Batch 15/7900 loss 11.298605 loss_att 14.323820 loss_ctc 14.832733 loss_rnnt 10.182579 hw_loss 0.074561 lr 0.00043352 rank 0
2023-02-21 14:14:11,283 DEBUG TRAIN Batch 15/7900 loss 14.254642 loss_att 17.253391 loss_ctc 19.158779 loss_rnnt 12.969515 hw_loss 0.059047 lr 0.00043347 rank 5
2023-02-21 14:15:29,377 DEBUG TRAIN Batch 15/8000 loss 5.853752 loss_att 7.498286 loss_ctc 7.383388 loss_rnnt 5.185156 hw_loss 0.254509 lr 0.00043335 rank 0
2023-02-21 14:15:29,379 DEBUG TRAIN Batch 15/8000 loss 11.551094 loss_att 16.141407 loss_ctc 16.846581 loss_rnnt 9.860036 hw_loss 0.125494 lr 0.00043341 rank 3
2023-02-21 14:15:29,381 DEBUG TRAIN Batch 15/8000 loss 9.360858 loss_att 12.708979 loss_ctc 13.696657 loss_rnnt 8.056452 hw_loss 0.106265 lr 0.00043338 rank 4
2023-02-21 14:15:29,383 DEBUG TRAIN Batch 15/8000 loss 16.591909 loss_att 19.022949 loss_ctc 23.903687 loss_rnnt 15.075068 hw_loss 0.104494 lr 0.00043338 rank 6
2023-02-21 14:15:29,383 DEBUG TRAIN Batch 15/8000 loss 10.547981 loss_att 13.209303 loss_ctc 15.390820 loss_rnnt 9.354105 hw_loss 0.029812 lr 0.00043331 rank 5
2023-02-21 14:15:29,384 DEBUG TRAIN Batch 15/8000 loss 7.436843 loss_att 10.646395 loss_ctc 13.145721 loss_rnnt 5.981292 hw_loss 0.098358 lr 0.00043330 rank 7
2023-02-21 14:15:29,397 DEBUG TRAIN Batch 15/8000 loss 9.257899 loss_att 10.927624 loss_ctc 11.687152 loss_rnnt 8.531353 hw_loss 0.128814 lr 0.00043342 rank 1
2023-02-21 14:15:29,406 DEBUG TRAIN Batch 15/8000 loss 5.061298 loss_att 10.125482 loss_ctc 5.999517 loss_rnnt 3.850189 hw_loss 0.137205 lr 0.00043337 rank 2
2023-02-21 14:16:46,611 DEBUG TRAIN Batch 15/8100 loss 5.371952 loss_att 8.901862 loss_ctc 7.942203 loss_rnnt 4.248842 hw_loss 0.139554 lr 0.00043326 rank 1
2023-02-21 14:16:46,614 DEBUG TRAIN Batch 15/8100 loss 25.176226 loss_att 23.122602 loss_ctc 34.451824 loss_rnnt 24.268021 hw_loss 0.154088 lr 0.00043313 rank 7
2023-02-21 14:16:46,613 DEBUG TRAIN Batch 15/8100 loss 4.591381 loss_att 8.647593 loss_ctc 7.445055 loss_rnnt 3.337802 hw_loss 0.115961 lr 0.00043321 rank 2
2023-02-21 14:16:46,616 DEBUG TRAIN Batch 15/8100 loss 22.879059 loss_att 25.720264 loss_ctc 31.793713 loss_rnnt 21.071041 hw_loss 0.095915 lr 0.00043321 rank 6
2023-02-21 14:16:46,618 DEBUG TRAIN Batch 15/8100 loss 11.225697 loss_att 17.217974 loss_ctc 14.256027 loss_rnnt 9.561966 hw_loss 0.114807 lr 0.00043322 rank 4
2023-02-21 14:16:46,619 DEBUG TRAIN Batch 15/8100 loss 10.034108 loss_att 14.072260 loss_ctc 13.880890 loss_rnnt 8.664433 hw_loss 0.092139 lr 0.00043319 rank 0
2023-02-21 14:16:46,622 DEBUG TRAIN Batch 15/8100 loss 9.207300 loss_att 13.876574 loss_ctc 13.939916 loss_rnnt 7.551395 hw_loss 0.170690 lr 0.00043324 rank 3
2023-02-21 14:16:46,627 DEBUG TRAIN Batch 15/8100 loss 4.502406 loss_att 6.408723 loss_ctc 8.807730 loss_rnnt 3.482676 hw_loss 0.120793 lr 0.00043315 rank 5
2023-02-21 14:18:05,391 DEBUG TRAIN Batch 15/8200 loss 10.748250 loss_att 11.433050 loss_ctc 14.649493 loss_rnnt 9.940691 hw_loss 0.282063 lr 0.00043309 rank 1
2023-02-21 14:18:05,391 DEBUG TRAIN Batch 15/8200 loss 15.094053 loss_att 15.178972 loss_ctc 20.227295 loss_rnnt 14.277106 hw_loss 0.216620 lr 0.00043297 rank 7
2023-02-21 14:18:05,396 DEBUG TRAIN Batch 15/8200 loss 13.159117 loss_att 15.678572 loss_ctc 18.673834 loss_rnnt 11.849248 hw_loss 0.132530 lr 0.00043306 rank 4
2023-02-21 14:18:05,396 DEBUG TRAIN Batch 15/8200 loss 16.817875 loss_att 16.558945 loss_ctc 23.991663 loss_rnnt 15.865501 hw_loss 0.089351 lr 0.00043303 rank 0
2023-02-21 14:18:05,398 DEBUG TRAIN Batch 15/8200 loss 7.914751 loss_att 9.061046 loss_ctc 9.142395 loss_rnnt 7.447512 hw_loss 0.139302 lr 0.00043305 rank 6
2023-02-21 14:18:05,399 DEBUG TRAIN Batch 15/8200 loss 11.931052 loss_att 11.777892 loss_ctc 19.435352 loss_rnnt 10.875129 hw_loss 0.161218 lr 0.00043308 rank 3
2023-02-21 14:18:05,399 DEBUG TRAIN Batch 15/8200 loss 8.586422 loss_att 11.850950 loss_ctc 12.159956 loss_rnnt 7.429534 hw_loss 0.051584 lr 0.00043304 rank 2
2023-02-21 14:18:05,406 DEBUG TRAIN Batch 15/8200 loss 6.401287 loss_att 7.967407 loss_ctc 6.702882 loss_rnnt 5.944262 hw_loss 0.194228 lr 0.00043298 rank 5
2023-02-21 14:19:22,505 DEBUG TRAIN Batch 15/8300 loss 9.235098 loss_att 9.731409 loss_ctc 10.977808 loss_rnnt 8.770416 hw_loss 0.249483 lr 0.00043289 rank 4
2023-02-21 14:19:22,512 DEBUG TRAIN Batch 15/8300 loss 10.155858 loss_att 13.004448 loss_ctc 11.255649 loss_rnnt 9.310890 hw_loss 0.241147 lr 0.00043287 rank 0
2023-02-21 14:19:22,512 DEBUG TRAIN Batch 15/8300 loss 9.266663 loss_att 12.465874 loss_ctc 13.273163 loss_rnnt 7.983214 hw_loss 0.205135 lr 0.00043288 rank 2
2023-02-21 14:19:22,512 DEBUG TRAIN Batch 15/8300 loss 13.119610 loss_att 14.759156 loss_ctc 16.627119 loss_rnnt 12.192241 hw_loss 0.247110 lr 0.00043281 rank 7
2023-02-21 14:19:22,514 DEBUG TRAIN Batch 15/8300 loss 12.752095 loss_att 15.771584 loss_ctc 20.769087 loss_rnnt 11.034307 hw_loss 0.084295 lr 0.00043293 rank 1
2023-02-21 14:19:22,518 DEBUG TRAIN Batch 15/8300 loss 11.343476 loss_att 13.939702 loss_ctc 13.827713 loss_rnnt 10.446545 hw_loss 0.087101 lr 0.00043292 rank 3
2023-02-21 14:19:22,519 DEBUG TRAIN Batch 15/8300 loss 14.816211 loss_att 16.154245 loss_ctc 16.866989 loss_rnnt 14.214903 hw_loss 0.112996 lr 0.00043282 rank 5
2023-02-21 14:19:22,563 DEBUG TRAIN Batch 15/8300 loss 13.451247 loss_att 14.590199 loss_ctc 17.330997 loss_rnnt 12.662086 hw_loss 0.082633 lr 0.00043289 rank 6
2023-02-21 14:20:12,285 DEBUG CV Batch 15/0 loss 2.214472 loss_att 2.257293 loss_ctc 2.600846 loss_rnnt 1.756773 hw_loss 0.745534 history loss 2.132454 rank 2
2023-02-21 14:20:12,290 DEBUG CV Batch 15/0 loss 2.214472 loss_att 2.257293 loss_ctc 2.600846 loss_rnnt 1.756773 hw_loss 0.745533 history loss 2.132454 rank 4
2023-02-21 14:20:12,290 DEBUG CV Batch 15/0 loss 2.214472 loss_att 2.257293 loss_ctc 2.600846 loss_rnnt 1.756773 hw_loss 0.745534 history loss 2.132454 rank 7
2023-02-21 14:20:12,291 DEBUG CV Batch 15/0 loss 2.214472 loss_att 2.257293 loss_ctc 2.600846 loss_rnnt 1.756773 hw_loss 0.745534 history loss 2.132454 rank 1
2023-02-21 14:20:12,296 DEBUG CV Batch 15/0 loss 2.214472 loss_att 2.257293 loss_ctc 2.600846 loss_rnnt 1.756773 hw_loss 0.745533 history loss 2.132454 rank 0
2023-02-21 14:20:12,299 DEBUG CV Batch 15/0 loss 2.214472 loss_att 2.257293 loss_ctc 2.600846 loss_rnnt 1.756773 hw_loss 0.745533 history loss 2.132454 rank 6
2023-02-21 14:20:12,299 DEBUG CV Batch 15/0 loss 2.214472 loss_att 2.257293 loss_ctc 2.600846 loss_rnnt 1.756773 hw_loss 0.745534 history loss 2.132454 rank 5
2023-02-21 14:20:12,327 DEBUG CV Batch 15/0 loss 2.214472 loss_att 2.257293 loss_ctc 2.600846 loss_rnnt 1.756773 hw_loss 0.745533 history loss 2.132454 rank 3
2023-02-21 14:20:23,346 DEBUG CV Batch 15/100 loss 5.826434 loss_att 7.742623 loss_ctc 8.701803 loss_rnnt 4.960368 hw_loss 0.186462 history loss 3.812982 rank 7
2023-02-21 14:20:23,435 DEBUG CV Batch 15/100 loss 5.826434 loss_att 7.742623 loss_ctc 8.701803 loss_rnnt 4.960368 hw_loss 0.186462 history loss 3.812982 rank 2
2023-02-21 14:20:23,447 DEBUG CV Batch 15/100 loss 5.826434 loss_att 7.742623 loss_ctc 8.701803 loss_rnnt 4.960368 hw_loss 0.186462 history loss 3.812982 rank 1
2023-02-21 14:20:23,516 DEBUG CV Batch 15/100 loss 5.826434 loss_att 7.742623 loss_ctc 8.701803 loss_rnnt 4.960368 hw_loss 0.186462 history loss 3.812982 rank 5
2023-02-21 14:20:23,520 DEBUG CV Batch 15/100 loss 5.826434 loss_att 7.742623 loss_ctc 8.701803 loss_rnnt 4.960368 hw_loss 0.186462 history loss 3.812982 rank 4
2023-02-21 14:20:23,557 DEBUG CV Batch 15/100 loss 5.826434 loss_att 7.742623 loss_ctc 8.701803 loss_rnnt 4.960368 hw_loss 0.186462 history loss 3.812982 rank 6
2023-02-21 14:20:23,574 DEBUG CV Batch 15/100 loss 5.826434 loss_att 7.742623 loss_ctc 8.701803 loss_rnnt 4.960368 hw_loss 0.186462 history loss 3.812982 rank 0
2023-02-21 14:20:23,632 DEBUG CV Batch 15/100 loss 5.826434 loss_att 7.742623 loss_ctc 8.701803 loss_rnnt 4.960368 hw_loss 0.186462 history loss 3.812982 rank 3
2023-02-21 14:20:36,568 DEBUG CV Batch 15/200 loss 7.691805 loss_att 20.833729 loss_ctc 7.255192 loss_rnnt 5.073668 hw_loss 0.089937 history loss 4.367825 rank 7
2023-02-21 14:20:36,731 DEBUG CV Batch 15/200 loss 7.691805 loss_att 20.833729 loss_ctc 7.255192 loss_rnnt 5.073668 hw_loss 0.089937 history loss 4.367825 rank 1
2023-02-21 14:20:36,786 DEBUG CV Batch 15/200 loss 7.691805 loss_att 20.833729 loss_ctc 7.255192 loss_rnnt 5.073668 hw_loss 0.089937 history loss 4.367825 rank 2
2023-02-21 14:20:36,821 DEBUG CV Batch 15/200 loss 7.691805 loss_att 20.833729 loss_ctc 7.255192 loss_rnnt 5.073668 hw_loss 0.089937 history loss 4.367825 rank 4
2023-02-21 14:20:36,880 DEBUG CV Batch 15/200 loss 7.691805 loss_att 20.833729 loss_ctc 7.255192 loss_rnnt 5.073668 hw_loss 0.089937 history loss 4.367825 rank 5
2023-02-21 14:20:37,030 DEBUG CV Batch 15/200 loss 7.691805 loss_att 20.833729 loss_ctc 7.255192 loss_rnnt 5.073668 hw_loss 0.089937 history loss 4.367825 rank 0
2023-02-21 14:20:37,037 DEBUG CV Batch 15/200 loss 7.691805 loss_att 20.833729 loss_ctc 7.255192 loss_rnnt 5.073668 hw_loss 0.089937 history loss 4.367825 rank 3
2023-02-21 14:20:38,071 DEBUG CV Batch 15/200 loss 7.691805 loss_att 20.833729 loss_ctc 7.255192 loss_rnnt 5.073668 hw_loss 0.089937 history loss 4.367825 rank 6
2023-02-21 14:20:48,685 DEBUG CV Batch 15/300 loss 5.278677 loss_att 5.821625 loss_ctc 8.675066 loss_rnnt 4.635701 hw_loss 0.152877 history loss 4.517887 rank 7
2023-02-21 14:20:48,750 DEBUG CV Batch 15/300 loss 5.278677 loss_att 5.821625 loss_ctc 8.675066 loss_rnnt 4.635701 hw_loss 0.152877 history loss 4.517887 rank 1
2023-02-21 14:20:48,877 DEBUG CV Batch 15/300 loss 5.278677 loss_att 5.821625 loss_ctc 8.675066 loss_rnnt 4.635701 hw_loss 0.152877 history loss 4.517887 rank 2
2023-02-21 14:20:48,946 DEBUG CV Batch 15/300 loss 5.278677 loss_att 5.821625 loss_ctc 8.675066 loss_rnnt 4.635701 hw_loss 0.152877 history loss 4.517887 rank 4
2023-02-21 14:20:49,022 DEBUG CV Batch 15/300 loss 5.278677 loss_att 5.821625 loss_ctc 8.675066 loss_rnnt 4.635701 hw_loss 0.152877 history loss 4.517887 rank 5
2023-02-21 14:20:49,168 DEBUG CV Batch 15/300 loss 5.278677 loss_att 5.821625 loss_ctc 8.675066 loss_rnnt 4.635701 hw_loss 0.152877 history loss 4.517887 rank 3
2023-02-21 14:20:49,169 DEBUG CV Batch 15/300 loss 5.278677 loss_att 5.821625 loss_ctc 8.675066 loss_rnnt 4.635701 hw_loss 0.152877 history loss 4.517887 rank 0
2023-02-21 14:20:51,414 DEBUG CV Batch 15/300 loss 5.278677 loss_att 5.821625 loss_ctc 8.675066 loss_rnnt 4.635701 hw_loss 0.152877 history loss 4.517887 rank 6
2023-02-21 14:21:00,580 DEBUG CV Batch 15/400 loss 26.171257 loss_att 100.032417 loss_ctc 15.880158 loss_rnnt 12.755511 hw_loss 0.029364 history loss 5.600781 rank 7
2023-02-21 14:21:00,637 DEBUG CV Batch 15/400 loss 26.171257 loss_att 100.032417 loss_ctc 15.880158 loss_rnnt 12.755511 hw_loss 0.029364 history loss 5.600780 rank 1
2023-02-21 14:21:00,881 DEBUG CV Batch 15/400 loss 26.171257 loss_att 100.032417 loss_ctc 15.880158 loss_rnnt 12.755511 hw_loss 0.029364 history loss 5.600781 rank 2
2023-02-21 14:21:01,014 DEBUG CV Batch 15/400 loss 26.171257 loss_att 100.032417 loss_ctc 15.880158 loss_rnnt 12.755511 hw_loss 0.029364 history loss 5.600780 rank 5
2023-02-21 14:21:01,014 DEBUG CV Batch 15/400 loss 26.171257 loss_att 100.032417 loss_ctc 15.880158 loss_rnnt 12.755511 hw_loss 0.029364 history loss 5.600781 rank 4
2023-02-21 14:21:01,136 DEBUG CV Batch 15/400 loss 26.171257 loss_att 100.032417 loss_ctc 15.880158 loss_rnnt 12.755511 hw_loss 0.029364 history loss 5.600780 rank 3
2023-02-21 14:21:01,216 DEBUG CV Batch 15/400 loss 26.171257 loss_att 100.032417 loss_ctc 15.880158 loss_rnnt 12.755511 hw_loss 0.029364 history loss 5.600780 rank 0
2023-02-21 14:21:03,555 DEBUG CV Batch 15/400 loss 26.171257 loss_att 100.032417 loss_ctc 15.880158 loss_rnnt 12.755511 hw_loss 0.029364 history loss 5.600781 rank 6
2023-02-21 14:21:10,972 DEBUG CV Batch 15/500 loss 6.531324 loss_att 6.014564 loss_ctc 8.182460 loss_rnnt 6.365553 hw_loss 0.091822 history loss 6.490620 rank 7
2023-02-21 14:21:11,176 DEBUG CV Batch 15/500 loss 6.531324 loss_att 6.014564 loss_ctc 8.182460 loss_rnnt 6.365553 hw_loss 0.091822 history loss 6.490620 rank 1
2023-02-21 14:21:11,463 DEBUG CV Batch 15/500 loss 6.531324 loss_att 6.014564 loss_ctc 8.182460 loss_rnnt 6.365553 hw_loss 0.091822 history loss 6.490620 rank 4
2023-02-21 14:21:11,525 DEBUG CV Batch 15/500 loss 6.531324 loss_att 6.014564 loss_ctc 8.182460 loss_rnnt 6.365553 hw_loss 0.091822 history loss 6.490620 rank 2
2023-02-21 14:21:11,672 DEBUG CV Batch 15/500 loss 6.531324 loss_att 6.014564 loss_ctc 8.182460 loss_rnnt 6.365553 hw_loss 0.091822 history loss 6.490620 rank 5
2023-02-21 14:21:11,748 DEBUG CV Batch 15/500 loss 6.531324 loss_att 6.014564 loss_ctc 8.182460 loss_rnnt 6.365553 hw_loss 0.091822 history loss 6.490620 rank 3
2023-02-21 14:21:11,941 DEBUG CV Batch 15/500 loss 6.531324 loss_att 6.014564 loss_ctc 8.182460 loss_rnnt 6.365553 hw_loss 0.091822 history loss 6.490620 rank 0
2023-02-21 14:21:14,335 DEBUG CV Batch 15/500 loss 6.531324 loss_att 6.014564 loss_ctc 8.182460 loss_rnnt 6.365553 hw_loss 0.091822 history loss 6.490620 rank 6
2023-02-21 14:21:22,941 DEBUG CV Batch 15/600 loss 7.860294 loss_att 8.083706 loss_ctc 11.257940 loss_rnnt 7.208135 hw_loss 0.289607 history loss 7.541840 rank 7
2023-02-21 14:21:23,128 DEBUG CV Batch 15/600 loss 7.860294 loss_att 8.083706 loss_ctc 11.257940 loss_rnnt 7.208135 hw_loss 0.289607 history loss 7.541840 rank 1
2023-02-21 14:21:23,425 DEBUG CV Batch 15/600 loss 7.860294 loss_att 8.083706 loss_ctc 11.257940 loss_rnnt 7.208135 hw_loss 0.289607 history loss 7.541840 rank 4
2023-02-21 14:21:23,522 DEBUG CV Batch 15/600 loss 7.860294 loss_att 8.083706 loss_ctc 11.257940 loss_rnnt 7.208135 hw_loss 0.289607 history loss 7.541840 rank 2
2023-02-21 14:21:23,820 DEBUG CV Batch 15/600 loss 7.860294 loss_att 8.083706 loss_ctc 11.257940 loss_rnnt 7.208135 hw_loss 0.289607 history loss 7.541840 rank 3
2023-02-21 14:21:24,112 DEBUG CV Batch 15/600 loss 7.860294 loss_att 8.083706 loss_ctc 11.257940 loss_rnnt 7.208135 hw_loss 0.289607 history loss 7.541840 rank 0
2023-02-21 14:21:24,485 DEBUG CV Batch 15/600 loss 7.860294 loss_att 8.083706 loss_ctc 11.257940 loss_rnnt 7.208135 hw_loss 0.289607 history loss 7.541840 rank 5
2023-02-21 14:21:26,413 DEBUG CV Batch 15/600 loss 7.860294 loss_att 8.083706 loss_ctc 11.257940 loss_rnnt 7.208135 hw_loss 0.289607 history loss 7.541840 rank 6
2023-02-21 14:21:34,343 DEBUG CV Batch 15/700 loss 16.722776 loss_att 52.501297 loss_ctc 19.207422 loss_rnnt 9.180258 hw_loss 0.104114 history loss 8.312070 rank 7
2023-02-21 14:21:34,441 DEBUG CV Batch 15/700 loss 16.722776 loss_att 52.501297 loss_ctc 19.207422 loss_rnnt 9.180258 hw_loss 0.104114 history loss 8.312070 rank 1
2023-02-21 14:21:34,751 DEBUG CV Batch 15/700 loss 16.722776 loss_att 52.501297 loss_ctc 19.207422 loss_rnnt 9.180258 hw_loss 0.104114 history loss 8.312070 rank 4
2023-02-21 14:21:34,919 DEBUG CV Batch 15/700 loss 16.722776 loss_att 52.501297 loss_ctc 19.207422 loss_rnnt 9.180258 hw_loss 0.104114 history loss 8.312070 rank 2
2023-02-21 14:21:35,549 DEBUG CV Batch 15/700 loss 16.722776 loss_att 52.501297 loss_ctc 19.207422 loss_rnnt 9.180258 hw_loss 0.104114 history loss 8.312070 rank 0
2023-02-21 14:21:35,774 DEBUG CV Batch 15/700 loss 16.722776 loss_att 52.501297 loss_ctc 19.207422 loss_rnnt 9.180258 hw_loss 0.104114 history loss 8.312070 rank 3
2023-02-21 14:21:35,888 DEBUG CV Batch 15/700 loss 16.722776 loss_att 52.501297 loss_ctc 19.207422 loss_rnnt 9.180258 hw_loss 0.104114 history loss 8.312070 rank 5
2023-02-21 14:21:37,829 DEBUG CV Batch 15/700 loss 16.722776 loss_att 52.501297 loss_ctc 19.207422 loss_rnnt 9.180258 hw_loss 0.104114 history loss 8.312070 rank 6
2023-02-21 14:21:45,426 DEBUG CV Batch 15/800 loss 9.288126 loss_att 10.592941 loss_ctc 15.707430 loss_rnnt 8.109894 hw_loss 0.115053 history loss 7.710053 rank 7
2023-02-21 14:21:45,641 DEBUG CV Batch 15/800 loss 9.288126 loss_att 10.592941 loss_ctc 15.707430 loss_rnnt 8.109894 hw_loss 0.115053 history loss 7.710053 rank 1
2023-02-21 14:21:45,951 DEBUG CV Batch 15/800 loss 9.288126 loss_att 10.592941 loss_ctc 15.707430 loss_rnnt 8.109894 hw_loss 0.115053 history loss 7.710053 rank 4
2023-02-21 14:21:46,031 DEBUG CV Batch 15/800 loss 9.288126 loss_att 10.592941 loss_ctc 15.707430 loss_rnnt 8.109894 hw_loss 0.115053 history loss 7.710053 rank 2
2023-02-21 14:21:46,869 DEBUG CV Batch 15/800 loss 9.288126 loss_att 10.592941 loss_ctc 15.707430 loss_rnnt 8.109894 hw_loss 0.115053 history loss 7.710053 rank 0
2023-02-21 14:21:47,142 DEBUG CV Batch 15/800 loss 9.288126 loss_att 10.592941 loss_ctc 15.707430 loss_rnnt 8.109894 hw_loss 0.115053 history loss 7.710053 rank 5
2023-02-21 14:21:47,230 DEBUG CV Batch 15/800 loss 9.288126 loss_att 10.592941 loss_ctc 15.707430 loss_rnnt 8.109894 hw_loss 0.115053 history loss 7.710053 rank 3
2023-02-21 14:21:49,174 DEBUG CV Batch 15/800 loss 9.288126 loss_att 10.592941 loss_ctc 15.707430 loss_rnnt 8.109894 hw_loss 0.115053 history loss 7.710053 rank 6
2023-02-21 14:21:58,705 DEBUG CV Batch 15/900 loss 12.934738 loss_att 19.944347 loss_ctc 24.360157 loss_rnnt 9.993766 hw_loss 0.029363 history loss 7.462001 rank 7
2023-02-21 14:21:58,766 DEBUG CV Batch 15/900 loss 12.934738 loss_att 19.944347 loss_ctc 24.360157 loss_rnnt 9.993766 hw_loss 0.029363 history loss 7.462001 rank 1
2023-02-21 14:21:59,200 DEBUG CV Batch 15/900 loss 12.934738 loss_att 19.944347 loss_ctc 24.360157 loss_rnnt 9.993766 hw_loss 0.029363 history loss 7.462001 rank 4
2023-02-21 14:21:59,232 DEBUG CV Batch 15/900 loss 12.934738 loss_att 19.944347 loss_ctc 24.360157 loss_rnnt 9.993766 hw_loss 0.029363 history loss 7.462001 rank 2
2023-02-21 14:22:00,091 DEBUG CV Batch 15/900 loss 12.934738 loss_att 19.944347 loss_ctc 24.360157 loss_rnnt 9.993766 hw_loss 0.029363 history loss 7.462001 rank 0
2023-02-21 14:22:00,482 DEBUG CV Batch 15/900 loss 12.934738 loss_att 19.944347 loss_ctc 24.360157 loss_rnnt 9.993766 hw_loss 0.029363 history loss 7.462001 rank 5
2023-02-21 14:22:00,604 DEBUG CV Batch 15/900 loss 12.934738 loss_att 19.944347 loss_ctc 24.360157 loss_rnnt 9.993766 hw_loss 0.029363 history loss 7.462001 rank 3
2023-02-21 14:22:02,501 DEBUG CV Batch 15/900 loss 12.934738 loss_att 19.944347 loss_ctc 24.360157 loss_rnnt 9.993766 hw_loss 0.029363 history loss 7.462001 rank 6
2023-02-21 14:22:10,776 DEBUG CV Batch 15/1000 loss 4.283153 loss_att 5.076925 loss_ctc 4.662634 loss_rnnt 4.005252 hw_loss 0.128529 history loss 7.190120 rank 7
2023-02-21 14:22:10,832 DEBUG CV Batch 15/1000 loss 4.283153 loss_att 5.076925 loss_ctc 4.662634 loss_rnnt 4.005252 hw_loss 0.128529 history loss 7.190120 rank 1
2023-02-21 14:22:11,208 DEBUG CV Batch 15/1000 loss 4.283153 loss_att 5.076925 loss_ctc 4.662634 loss_rnnt 4.005252 hw_loss 0.128529 history loss 7.190120 rank 4
2023-02-21 14:22:11,244 DEBUG CV Batch 15/1000 loss 4.283153 loss_att 5.076925 loss_ctc 4.662634 loss_rnnt 4.005252 hw_loss 0.128529 history loss 7.190120 rank 2
2023-02-21 14:22:12,222 DEBUG CV Batch 15/1000 loss 4.283153 loss_att 5.076925 loss_ctc 4.662634 loss_rnnt 4.005252 hw_loss 0.128529 history loss 7.190120 rank 0
2023-02-21 14:22:12,730 DEBUG CV Batch 15/1000 loss 4.283153 loss_att 5.076925 loss_ctc 4.662634 loss_rnnt 4.005252 hw_loss 0.128529 history loss 7.190120 rank 3
2023-02-21 14:22:13,954 DEBUG CV Batch 15/1000 loss 4.283153 loss_att 5.076925 loss_ctc 4.662634 loss_rnnt 4.005252 hw_loss 0.128529 history loss 7.190120 rank 5
2023-02-21 14:22:14,720 DEBUG CV Batch 15/1000 loss 4.283153 loss_att 5.076925 loss_ctc 4.662634 loss_rnnt 4.005252 hw_loss 0.128529 history loss 7.190120 rank 6
2023-02-21 14:22:22,675 DEBUG CV Batch 15/1100 loss 7.745312 loss_att 6.887560 loss_ctc 9.475584 loss_rnnt 7.468810 hw_loss 0.407530 history loss 7.175963 rank 7
2023-02-21 14:22:22,719 DEBUG CV Batch 15/1100 loss 7.745312 loss_att 6.887560 loss_ctc 9.475584 loss_rnnt 7.468810 hw_loss 0.407530 history loss 7.175963 rank 1
2023-02-21 14:22:23,036 DEBUG CV Batch 15/1100 loss 7.745312 loss_att 6.887560 loss_ctc 9.475584 loss_rnnt 7.468810 hw_loss 0.407530 history loss 7.175963 rank 2
2023-02-21 14:22:23,047 DEBUG CV Batch 15/1100 loss 7.745312 loss_att 6.887560 loss_ctc 9.475584 loss_rnnt 7.468810 hw_loss 0.407530 history loss 7.175963 rank 4
2023-02-21 14:22:24,080 DEBUG CV Batch 15/1100 loss 7.745312 loss_att 6.887560 loss_ctc 9.475584 loss_rnnt 7.468810 hw_loss 0.407530 history loss 7.175963 rank 0
2023-02-21 14:22:24,643 DEBUG CV Batch 15/1100 loss 7.745312 loss_att 6.887560 loss_ctc 9.475584 loss_rnnt 7.468810 hw_loss 0.407530 history loss 7.175963 rank 3
2023-02-21 14:22:26,378 DEBUG CV Batch 15/1100 loss 7.745312 loss_att 6.887560 loss_ctc 9.475584 loss_rnnt 7.468810 hw_loss 0.407530 history loss 7.175963 rank 5
2023-02-21 14:22:26,646 DEBUG CV Batch 15/1100 loss 7.745312 loss_att 6.887560 loss_ctc 9.475584 loss_rnnt 7.468810 hw_loss 0.407530 history loss 7.175963 rank 6
2023-02-21 14:22:33,205 DEBUG CV Batch 15/1200 loss 10.976748 loss_att 10.812564 loss_ctc 12.456214 loss_rnnt 10.763351 hw_loss 0.091822 history loss 7.551896 rank 7
2023-02-21 14:22:33,255 DEBUG CV Batch 15/1200 loss 10.976748 loss_att 10.812564 loss_ctc 12.456214 loss_rnnt 10.763351 hw_loss 0.091822 history loss 7.551896 rank 1
2023-02-21 14:22:33,457 DEBUG CV Batch 15/1200 loss 10.976748 loss_att 10.812564 loss_ctc 12.456214 loss_rnnt 10.763351 hw_loss 0.091822 history loss 7.551896 rank 4
2023-02-21 14:22:33,518 DEBUG CV Batch 15/1200 loss 10.976748 loss_att 10.812564 loss_ctc 12.456214 loss_rnnt 10.763351 hw_loss 0.091822 history loss 7.551896 rank 2
2023-02-21 14:22:34,725 DEBUG CV Batch 15/1200 loss 10.976748 loss_att 10.812564 loss_ctc 12.456214 loss_rnnt 10.763351 hw_loss 0.091822 history loss 7.551896 rank 0
2023-02-21 14:22:35,388 DEBUG CV Batch 15/1200 loss 10.976748 loss_att 10.812564 loss_ctc 12.456214 loss_rnnt 10.763351 hw_loss 0.091822 history loss 7.551896 rank 3
2023-02-21 14:22:37,027 DEBUG CV Batch 15/1200 loss 10.976748 loss_att 10.812564 loss_ctc 12.456214 loss_rnnt 10.763351 hw_loss 0.091822 history loss 7.551896 rank 5
2023-02-21 14:22:37,402 DEBUG CV Batch 15/1200 loss 10.976748 loss_att 10.812564 loss_ctc 12.456214 loss_rnnt 10.763351 hw_loss 0.091822 history loss 7.551896 rank 6
2023-02-21 14:22:45,024 DEBUG CV Batch 15/1300 loss 6.025117 loss_att 5.722679 loss_ctc 7.968710 loss_rnnt 5.704335 hw_loss 0.228982 history loss 7.894092 rank 7
2023-02-21 14:22:45,228 DEBUG CV Batch 15/1300 loss 6.025117 loss_att 5.722679 loss_ctc 7.968710 loss_rnnt 5.704335 hw_loss 0.228982 history loss 7.894092 rank 1
2023-02-21 14:22:45,311 DEBUG CV Batch 15/1300 loss 6.025117 loss_att 5.722679 loss_ctc 7.968710 loss_rnnt 5.704335 hw_loss 0.228982 history loss 7.894092 rank 4
2023-02-21 14:22:45,405 DEBUG CV Batch 15/1300 loss 6.025117 loss_att 5.722679 loss_ctc 7.968710 loss_rnnt 5.704335 hw_loss 0.228982 history loss 7.894092 rank 2
2023-02-21 14:22:46,819 DEBUG CV Batch 15/1300 loss 6.025117 loss_att 5.722679 loss_ctc 7.968710 loss_rnnt 5.704335 hw_loss 0.228982 history loss 7.894092 rank 0
2023-02-21 14:22:47,395 DEBUG CV Batch 15/1300 loss 6.025117 loss_att 5.722679 loss_ctc 7.968710 loss_rnnt 5.704335 hw_loss 0.228982 history loss 7.894092 rank 3
2023-02-21 14:22:49,125 DEBUG CV Batch 15/1300 loss 6.025117 loss_att 5.722679 loss_ctc 7.968710 loss_rnnt 5.704335 hw_loss 0.228982 history loss 7.894092 rank 5
2023-02-21 14:22:49,531 DEBUG CV Batch 15/1300 loss 6.025117 loss_att 5.722679 loss_ctc 7.968710 loss_rnnt 5.704335 hw_loss 0.228982 history loss 7.894092 rank 6
2023-02-21 14:22:56,083 DEBUG CV Batch 15/1400 loss 9.074224 loss_att 39.364067 loss_ctc 4.447592 loss_rnnt 3.582780 hw_loss 0.094424 history loss 8.269405 rank 7
2023-02-21 14:22:56,396 DEBUG CV Batch 15/1400 loss 9.074224 loss_att 39.364067 loss_ctc 4.447592 loss_rnnt 3.582780 hw_loss 0.094424 history loss 8.269405 rank 1
2023-02-21 14:22:56,473 DEBUG CV Batch 15/1400 loss 9.074224 loss_att 39.364067 loss_ctc 4.447592 loss_rnnt 3.582780 hw_loss 0.094424 history loss 8.269405 rank 4
2023-02-21 14:22:56,566 DEBUG CV Batch 15/1400 loss 9.074224 loss_att 39.364067 loss_ctc 4.447592 loss_rnnt 3.582780 hw_loss 0.094424 history loss 8.269405 rank 2
2023-02-21 14:22:58,150 DEBUG CV Batch 15/1400 loss 9.074224 loss_att 39.364067 loss_ctc 4.447592 loss_rnnt 3.582780 hw_loss 0.094424 history loss 8.269405 rank 0
2023-02-21 14:22:58,650 DEBUG CV Batch 15/1400 loss 9.074224 loss_att 39.364067 loss_ctc 4.447592 loss_rnnt 3.582780 hw_loss 0.094424 history loss 8.269405 rank 3
2023-02-21 14:23:00,410 DEBUG CV Batch 15/1400 loss 9.074224 loss_att 39.364067 loss_ctc 4.447592 loss_rnnt 3.582780 hw_loss 0.094424 history loss 8.269405 rank 5
2023-02-21 14:23:00,833 DEBUG CV Batch 15/1400 loss 9.074224 loss_att 39.364067 loss_ctc 4.447592 loss_rnnt 3.582780 hw_loss 0.094424 history loss 8.269405 rank 6
2023-02-21 14:23:07,359 DEBUG CV Batch 15/1500 loss 7.569204 loss_att 8.596118 loss_ctc 6.974790 loss_rnnt 7.377732 hw_loss 0.122519 history loss 8.076594 rank 7
2023-02-21 14:23:07,788 DEBUG CV Batch 15/1500 loss 7.569204 loss_att 8.596118 loss_ctc 6.974790 loss_rnnt 7.377732 hw_loss 0.122519 history loss 8.076594 rank 4
2023-02-21 14:23:07,791 DEBUG CV Batch 15/1500 loss 7.569204 loss_att 8.596118 loss_ctc 6.974790 loss_rnnt 7.377732 hw_loss 0.122519 history loss 8.076594 rank 1
2023-02-21 14:23:07,801 DEBUG CV Batch 15/1500 loss 7.569204 loss_att 8.596118 loss_ctc 6.974790 loss_rnnt 7.377732 hw_loss 0.122519 history loss 8.076594 rank 2
2023-02-21 14:23:09,803 DEBUG CV Batch 15/1500 loss 7.569204 loss_att 8.596118 loss_ctc 6.974790 loss_rnnt 7.377732 hw_loss 0.122519 history loss 8.076594 rank 0
2023-02-21 14:23:10,238 DEBUG CV Batch 15/1500 loss 7.569204 loss_att 8.596118 loss_ctc 6.974790 loss_rnnt 7.377732 hw_loss 0.122519 history loss 8.076594 rank 3
2023-02-21 14:23:12,239 DEBUG CV Batch 15/1500 loss 7.569204 loss_att 8.596118 loss_ctc 6.974790 loss_rnnt 7.377732 hw_loss 0.122519 history loss 8.076594 rank 5
2023-02-21 14:23:12,396 DEBUG CV Batch 15/1500 loss 7.569204 loss_att 8.596118 loss_ctc 6.974790 loss_rnnt 7.377732 hw_loss 0.122519 history loss 8.076594 rank 6
2023-02-21 14:23:20,597 DEBUG CV Batch 15/1600 loss 8.838761 loss_att 16.553907 loss_ctc 12.107234 loss_rnnt 6.809142 hw_loss 0.095238 history loss 7.982514 rank 4
2023-02-21 14:23:20,677 DEBUG CV Batch 15/1600 loss 8.838761 loss_att 16.553907 loss_ctc 12.107234 loss_rnnt 6.809142 hw_loss 0.095238 history loss 7.982514 rank 2
2023-02-21 14:23:20,686 DEBUG CV Batch 15/1600 loss 8.838761 loss_att 16.553907 loss_ctc 12.107234 loss_rnnt 6.809142 hw_loss 0.095238 history loss 7.982514 rank 1
2023-02-21 14:23:20,817 DEBUG CV Batch 15/1600 loss 8.838761 loss_att 16.553907 loss_ctc 12.107234 loss_rnnt 6.809142 hw_loss 0.095238 history loss 7.982514 rank 7
2023-02-21 14:23:23,237 DEBUG CV Batch 15/1600 loss 8.838761 loss_att 16.553907 loss_ctc 12.107234 loss_rnnt 6.809142 hw_loss 0.095238 history loss 7.982514 rank 3
2023-02-21 14:23:23,416 DEBUG CV Batch 15/1600 loss 8.838761 loss_att 16.553907 loss_ctc 12.107234 loss_rnnt 6.809142 hw_loss 0.095238 history loss 7.982514 rank 0
2023-02-21 14:23:25,335 DEBUG CV Batch 15/1600 loss 8.838761 loss_att 16.553907 loss_ctc 12.107234 loss_rnnt 6.809142 hw_loss 0.095238 history loss 7.982514 rank 5
2023-02-21 14:23:25,427 DEBUG CV Batch 15/1600 loss 8.838761 loss_att 16.553907 loss_ctc 12.107234 loss_rnnt 6.809142 hw_loss 0.095238 history loss 7.982514 rank 6
2023-02-21 14:23:32,734 DEBUG CV Batch 15/1700 loss 10.632743 loss_att 9.550467 loss_ctc 14.291066 loss_rnnt 10.285533 hw_loss 0.142290 history loss 7.864711 rank 4
2023-02-21 14:23:32,783 DEBUG CV Batch 15/1700 loss 10.632743 loss_att 9.550467 loss_ctc 14.291066 loss_rnnt 10.285533 hw_loss 0.142290 history loss 7.864711 rank 2
2023-02-21 14:23:32,914 DEBUG CV Batch 15/1700 loss 10.632743 loss_att 9.550467 loss_ctc 14.291066 loss_rnnt 10.285533 hw_loss 0.142290 history loss 7.864711 rank 1
2023-02-21 14:23:33,088 DEBUG CV Batch 15/1700 loss 10.632743 loss_att 9.550467 loss_ctc 14.291066 loss_rnnt 10.285533 hw_loss 0.142290 history loss 7.864711 rank 7
2023-02-21 14:23:35,466 DEBUG CV Batch 15/1700 loss 10.632743 loss_att 9.550467 loss_ctc 14.291066 loss_rnnt 10.285533 hw_loss 0.142290 history loss 7.864711 rank 3
2023-02-21 14:23:35,848 DEBUG CV Batch 15/1700 loss 10.632743 loss_att 9.550467 loss_ctc 14.291066 loss_rnnt 10.285533 hw_loss 0.142290 history loss 7.864711 rank 0
2023-02-21 14:23:37,702 DEBUG CV Batch 15/1700 loss 10.632743 loss_att 9.550467 loss_ctc 14.291066 loss_rnnt 10.285533 hw_loss 0.142290 history loss 7.864711 rank 6
2023-02-21 14:23:38,947 DEBUG CV Batch 15/1700 loss 10.632743 loss_att 9.550467 loss_ctc 14.291066 loss_rnnt 10.285533 hw_loss 0.142290 history loss 7.864711 rank 5
2023-02-21 14:23:41,677 INFO Epoch 15 CV info cv_loss 7.820544944260141
2023-02-21 14:23:41,678 INFO Epoch 16 TRAIN info lr 0.000432836356999696
2023-02-21 14:23:41,681 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 14:23:41,733 INFO Epoch 15 CV info cv_loss 7.820544944889009
2023-02-21 14:23:41,734 INFO Epoch 16 TRAIN info lr 0.00043284122252541465
2023-02-21 14:23:41,736 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 14:23:41,894 INFO Epoch 15 CV info cv_loss 7.8205449429679454
2023-02-21 14:23:41,895 INFO Epoch 16 TRAIN info lr 0.0004328428444037836
2023-02-21 14:23:41,900 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 14:23:42,053 INFO Epoch 15 CV info cv_loss 7.820544943691575
2023-02-21 14:23:42,054 INFO Epoch 16 TRAIN info lr 0.00043274880558465253
2023-02-21 14:23:42,059 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 14:23:44,509 INFO Epoch 15 CV info cv_loss 7.820544942330463
2023-02-21 14:23:44,511 INFO Epoch 16 TRAIN info lr 0.00043284771014828337
2023-02-21 14:23:44,515 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 14:23:44,864 INFO Epoch 15 CV info cv_loss 7.820544942201243
2023-02-21 14:23:44,865 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/15.pt
2023-02-21 14:23:45,677 INFO Epoch 16 TRAIN info lr 0.0004328314916380524
2023-02-21 14:23:45,681 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 14:23:46,762 INFO Epoch 15 CV info cv_loss 7.820544944337672
2023-02-21 14:23:46,763 INFO Epoch 16 TRAIN info lr 0.00043281203183204316
2023-02-21 14:23:46,767 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 14:23:49,044 INFO Epoch 15 CV info cv_loss 7.820544943273766
2023-02-21 14:23:49,045 INFO Epoch 16 TRAIN info lr 0.00043276177281233045
2023-02-21 14:23:49,049 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 14:25:02,106 DEBUG TRAIN Batch 16/0 loss 9.092338 loss_att 7.988259 loss_ctc 10.654851 loss_rnnt 8.842611 hw_loss 0.491638 lr 0.00043283 rank 4
2023-02-21 14:25:02,110 DEBUG TRAIN Batch 16/0 loss 6.942781 loss_att 7.356298 loss_ctc 8.698874 loss_rnnt 6.511654 hw_loss 0.214273 lr 0.00043284 rank 1
2023-02-21 14:25:02,111 DEBUG TRAIN Batch 16/0 loss 9.086370 loss_att 9.100763 loss_ctc 12.293144 loss_rnnt 8.502484 hw_loss 0.287693 lr 0.00043281 rank 6
2023-02-21 14:25:02,112 DEBUG TRAIN Batch 16/0 loss 7.728820 loss_att 8.260923 loss_ctc 11.166242 loss_rnnt 7.000787 hw_loss 0.306168 lr 0.00043275 rank 7
2023-02-21 14:25:02,115 DEBUG TRAIN Batch 16/0 loss 10.156857 loss_att 9.081219 loss_ctc 12.648339 loss_rnnt 9.891864 hw_loss 0.277360 lr 0.00043283 rank 0
2023-02-21 14:25:02,116 DEBUG TRAIN Batch 16/0 loss 13.010752 loss_att 12.272757 loss_ctc 16.364834 loss_rnnt 12.521984 hw_loss 0.354668 lr 0.00043284 rank 2
2023-02-21 14:25:02,123 DEBUG TRAIN Batch 16/0 loss 6.006991 loss_att 6.107316 loss_ctc 7.821352 loss_rnnt 5.537592 hw_loss 0.388910 lr 0.00043285 rank 3
2023-02-21 14:25:02,195 DEBUG TRAIN Batch 16/0 loss 9.458316 loss_att 9.718422 loss_ctc 11.120178 loss_rnnt 9.012906 hw_loss 0.322137 lr 0.00043276 rank 5
2023-02-21 14:26:20,160 DEBUG TRAIN Batch 16/100 loss 3.380685 loss_att 7.760928 loss_ctc 6.446868 loss_rnnt 2.058315 hw_loss 0.070309 lr 0.00043268 rank 2
2023-02-21 14:26:20,166 DEBUG TRAIN Batch 16/100 loss 10.506731 loss_att 13.874638 loss_ctc 14.342571 loss_rnnt 9.286217 hw_loss 0.066537 lr 0.00043259 rank 7
2023-02-21 14:26:20,166 DEBUG TRAIN Batch 16/100 loss 9.127635 loss_att 12.155790 loss_ctc 10.130061 loss_rnnt 8.361212 hw_loss 0.050878 lr 0.00043268 rank 3
2023-02-21 14:26:20,167 DEBUG TRAIN Batch 16/100 loss 7.355701 loss_att 9.713267 loss_ctc 8.967419 loss_rnnt 6.654430 hw_loss 0.027865 lr 0.00043268 rank 1
2023-02-21 14:26:20,170 DEBUG TRAIN Batch 16/100 loss 9.982498 loss_att 13.284115 loss_ctc 12.047035 loss_rnnt 8.954367 hw_loss 0.173504 lr 0.00043265 rank 6
2023-02-21 14:26:20,170 DEBUG TRAIN Batch 16/100 loss 19.029673 loss_att 25.210270 loss_ctc 29.689720 loss_rnnt 16.288982 hw_loss 0.156058 lr 0.00043267 rank 4
2023-02-21 14:26:20,170 DEBUG TRAIN Batch 16/100 loss 22.329546 loss_att 27.691399 loss_ctc 28.133512 loss_rnnt 20.410007 hw_loss 0.137450 lr 0.00043267 rank 0
2023-02-21 14:26:20,170 DEBUG TRAIN Batch 16/100 loss 7.621687 loss_att 10.429602 loss_ctc 13.188574 loss_rnnt 6.281658 hw_loss 0.067865 lr 0.00043260 rank 5
2023-02-21 14:27:38,271 DEBUG TRAIN Batch 16/200 loss 5.630382 loss_att 9.124838 loss_ctc 5.756433 loss_rnnt 4.840498 hw_loss 0.139099 lr 0.00043252 rank 2
2023-02-21 14:27:38,273 DEBUG TRAIN Batch 16/200 loss 12.573065 loss_att 12.893831 loss_ctc 14.990166 loss_rnnt 12.117987 hw_loss 0.128711 lr 0.00043242 rank 7
2023-02-21 14:27:38,273 DEBUG TRAIN Batch 16/200 loss 10.252050 loss_att 12.665895 loss_ctc 13.045149 loss_rnnt 9.356147 hw_loss 0.076354 lr 0.00043251 rank 4
2023-02-21 14:27:38,274 DEBUG TRAIN Batch 16/200 loss 8.252893 loss_att 13.199594 loss_ctc 12.291659 loss_rnnt 6.656387 hw_loss 0.128747 lr 0.00043251 rank 0
2023-02-21 14:27:38,275 DEBUG TRAIN Batch 16/200 loss 3.836321 loss_att 8.269188 loss_ctc 6.730697 loss_rnnt 2.502366 hw_loss 0.115245 lr 0.00043249 rank 6
2023-02-21 14:27:38,276 DEBUG TRAIN Batch 16/200 loss 10.786648 loss_att 14.150208 loss_ctc 14.038589 loss_rnnt 9.627428 hw_loss 0.099217 lr 0.00043252 rank 3
2023-02-21 14:27:38,276 DEBUG TRAIN Batch 16/200 loss 9.254451 loss_att 14.002712 loss_ctc 12.378604 loss_rnnt 7.859787 hw_loss 0.053360 lr 0.00043252 rank 1
2023-02-21 14:27:38,280 DEBUG TRAIN Batch 16/200 loss 6.497160 loss_att 10.168904 loss_ctc 11.651805 loss_rnnt 4.989425 hw_loss 0.161437 lr 0.00043244 rank 5
2023-02-21 14:28:58,137 DEBUG TRAIN Batch 16/300 loss 7.758633 loss_att 11.174733 loss_ctc 12.312288 loss_rnnt 6.318322 hw_loss 0.281130 lr 0.00043235 rank 4
2023-02-21 14:28:58,140 DEBUG TRAIN Batch 16/300 loss 9.570004 loss_att 15.185706 loss_ctc 15.708040 loss_rnnt 7.590314 hw_loss 0.071519 lr 0.00043227 rank 5
2023-02-21 14:28:58,140 DEBUG TRAIN Batch 16/300 loss 7.449256 loss_att 10.217900 loss_ctc 11.538847 loss_rnnt 6.245861 hw_loss 0.195727 lr 0.00043232 rank 6
2023-02-21 14:28:58,142 DEBUG TRAIN Batch 16/300 loss 15.449519 loss_att 18.066313 loss_ctc 16.019772 loss_rnnt 14.776299 hw_loss 0.138428 lr 0.00043226 rank 7
2023-02-21 14:28:58,142 DEBUG TRAIN Batch 16/300 loss 13.646923 loss_att 15.728192 loss_ctc 17.285049 loss_rnnt 12.675084 hw_loss 0.132189 lr 0.00043236 rank 3
2023-02-21 14:28:58,144 DEBUG TRAIN Batch 16/300 loss 10.476033 loss_att 11.809769 loss_ctc 11.566504 loss_rnnt 10.007938 hw_loss 0.104910 lr 0.00043236 rank 1
2023-02-21 14:28:58,146 DEBUG TRAIN Batch 16/300 loss 7.182479 loss_att 10.956455 loss_ctc 8.881525 loss_rnnt 6.155956 hw_loss 0.084728 lr 0.00043235 rank 2
2023-02-21 14:28:58,189 DEBUG TRAIN Batch 16/300 loss 8.514662 loss_att 13.784057 loss_ctc 14.682858 loss_rnnt 6.609105 hw_loss 0.054846 lr 0.00043234 rank 0
2023-02-21 14:30:16,673 DEBUG TRAIN Batch 16/400 loss 10.387018 loss_att 13.743924 loss_ctc 11.629345 loss_rnnt 9.532646 hw_loss 0.032524 lr 0.00043219 rank 1
2023-02-21 14:30:16,674 DEBUG TRAIN Batch 16/400 loss 12.575331 loss_att 15.695555 loss_ctc 18.033024 loss_rnnt 11.191044 hw_loss 0.061031 lr 0.00043219 rank 4
2023-02-21 14:30:16,680 DEBUG TRAIN Batch 16/400 loss 12.283313 loss_att 13.773540 loss_ctc 15.001637 loss_rnnt 11.539207 hw_loss 0.156779 lr 0.00043220 rank 3
2023-02-21 14:30:16,681 DEBUG TRAIN Batch 16/400 loss 9.970344 loss_att 11.525259 loss_ctc 13.419115 loss_rnnt 9.135689 hw_loss 0.119692 lr 0.00043218 rank 0
2023-02-21 14:30:16,682 DEBUG TRAIN Batch 16/400 loss 21.239435 loss_att 23.601357 loss_ctc 30.806971 loss_rnnt 19.449389 hw_loss 0.078730 lr 0.00043211 rank 5
2023-02-21 14:30:16,711 DEBUG TRAIN Batch 16/400 loss 4.819098 loss_att 8.096089 loss_ctc 7.863305 loss_rnnt 3.648150 hw_loss 0.205602 lr 0.00043219 rank 2
2023-02-21 14:30:16,711 DEBUG TRAIN Batch 16/400 loss 11.209961 loss_att 16.582062 loss_ctc 17.943382 loss_rnnt 9.162680 hw_loss 0.140761 lr 0.00043210 rank 7
2023-02-21 14:30:16,721 DEBUG TRAIN Batch 16/400 loss 11.156265 loss_att 13.836344 loss_ctc 14.891150 loss_rnnt 10.059964 hw_loss 0.116814 lr 0.00043216 rank 6
2023-02-21 14:31:34,975 DEBUG TRAIN Batch 16/500 loss 6.547949 loss_att 11.956128 loss_ctc 9.302161 loss_rnnt 5.014553 hw_loss 0.158496 lr 0.00043203 rank 4
2023-02-21 14:31:34,977 DEBUG TRAIN Batch 16/500 loss 5.457066 loss_att 7.989340 loss_ctc 7.649787 loss_rnnt 4.586612 hw_loss 0.134318 lr 0.00043203 rank 1
2023-02-21 14:31:34,977 DEBUG TRAIN Batch 16/500 loss 15.423145 loss_att 17.823174 loss_ctc 19.492994 loss_rnnt 14.353783 hw_loss 0.087581 lr 0.00043200 rank 6
2023-02-21 14:31:34,981 DEBUG TRAIN Batch 16/500 loss 16.928585 loss_att 20.032963 loss_ctc 20.536880 loss_rnnt 15.782503 hw_loss 0.082689 lr 0.00043203 rank 2
2023-02-21 14:31:34,984 DEBUG TRAIN Batch 16/500 loss 14.183111 loss_att 18.991638 loss_ctc 19.895145 loss_rnnt 12.361886 hw_loss 0.183591 lr 0.00043202 rank 0
2023-02-21 14:31:34,985 DEBUG TRAIN Batch 16/500 loss 6.239553 loss_att 9.460323 loss_ctc 13.307761 loss_rnnt 4.623694 hw_loss 0.054896 lr 0.00043194 rank 7
2023-02-21 14:31:34,991 DEBUG TRAIN Batch 16/500 loss 12.285110 loss_att 17.851040 loss_ctc 15.757309 loss_rnnt 10.604393 hw_loss 0.196067 lr 0.00043204 rank 3
2023-02-21 14:31:35,028 DEBUG TRAIN Batch 16/500 loss 9.025068 loss_att 12.491488 loss_ctc 14.950519 loss_rnnt 7.475255 hw_loss 0.124629 lr 0.00043195 rank 5
2023-02-21 14:32:55,233 DEBUG TRAIN Batch 16/600 loss 10.218670 loss_att 9.934324 loss_ctc 13.695128 loss_rnnt 9.653699 hw_loss 0.296836 lr 0.00043187 rank 2
2023-02-21 14:32:55,233 DEBUG TRAIN Batch 16/600 loss 10.088828 loss_att 10.096765 loss_ctc 12.699138 loss_rnnt 9.589488 hw_loss 0.280710 lr 0.00043187 rank 1
2023-02-21 14:32:55,235 DEBUG TRAIN Batch 16/600 loss 12.096988 loss_att 12.303389 loss_ctc 19.196302 loss_rnnt 11.062692 hw_loss 0.087076 lr 0.00043186 rank 4
2023-02-21 14:32:55,238 DEBUG TRAIN Batch 16/600 loss 13.602803 loss_att 15.042694 loss_ctc 15.789584 loss_rnnt 12.946301 hw_loss 0.144289 lr 0.00043188 rank 3
2023-02-21 14:32:55,240 DEBUG TRAIN Batch 16/600 loss 9.081766 loss_att 11.698612 loss_ctc 10.199739 loss_rnnt 8.325564 hw_loss 0.157067 lr 0.00043186 rank 0
2023-02-21 14:32:55,240 DEBUG TRAIN Batch 16/600 loss 8.734525 loss_att 9.048245 loss_ctc 10.656405 loss_rnnt 8.235544 hw_loss 0.337475 lr 0.00043178 rank 7
2023-02-21 14:32:55,244 DEBUG TRAIN Batch 16/600 loss 14.237644 loss_att 13.409058 loss_ctc 15.728616 loss_rnnt 14.099260 hw_loss 0.197445 lr 0.00043179 rank 5
2023-02-21 14:32:55,293 DEBUG TRAIN Batch 16/600 loss 12.864658 loss_att 12.301536 loss_ctc 14.577852 loss_rnnt 12.554568 hw_loss 0.364292 lr 0.00043184 rank 6
2023-02-21 14:34:16,943 DEBUG TRAIN Batch 16/700 loss 13.522188 loss_att 13.680467 loss_ctc 17.631222 loss_rnnt 12.929115 hw_loss 0.025396 lr 0.00043162 rank 7
2023-02-21 14:34:16,947 DEBUG TRAIN Batch 16/700 loss 16.960024 loss_att 21.399475 loss_ctc 23.587299 loss_rnnt 15.107962 hw_loss 0.151003 lr 0.00043170 rank 4
2023-02-21 14:34:16,948 DEBUG TRAIN Batch 16/700 loss 14.202870 loss_att 16.226849 loss_ctc 18.786560 loss_rnnt 13.093807 hw_loss 0.174578 lr 0.00043171 rank 2
2023-02-21 14:34:16,950 DEBUG TRAIN Batch 16/700 loss 7.807322 loss_att 9.720312 loss_ctc 9.579203 loss_rnnt 7.102922 hw_loss 0.160408 lr 0.00043168 rank 6
2023-02-21 14:34:16,952 DEBUG TRAIN Batch 16/700 loss 12.206536 loss_att 19.002716 loss_ctc 13.519855 loss_rnnt 10.648148 hw_loss 0.045081 lr 0.00043171 rank 1
2023-02-21 14:34:16,954 DEBUG TRAIN Batch 16/700 loss 16.567150 loss_att 18.870472 loss_ctc 18.100956 loss_rnnt 15.888057 hw_loss 0.026104 lr 0.00043172 rank 3
2023-02-21 14:34:16,965 DEBUG TRAIN Batch 16/700 loss 11.616335 loss_att 15.029535 loss_ctc 19.474718 loss_rnnt 9.872366 hw_loss 0.025395 lr 0.00043163 rank 5
2023-02-21 14:34:16,993 DEBUG TRAIN Batch 16/700 loss 9.800343 loss_att 14.796972 loss_ctc 14.071349 loss_rnnt 8.216851 hw_loss 0.027557 lr 0.00043170 rank 0
2023-02-21 14:35:35,498 DEBUG TRAIN Batch 16/800 loss 12.147514 loss_att 19.673969 loss_ctc 16.054705 loss_rnnt 10.045708 hw_loss 0.141668 lr 0.00043146 rank 7
2023-02-21 14:35:35,498 DEBUG TRAIN Batch 16/800 loss 6.960777 loss_att 11.395061 loss_ctc 9.230593 loss_rnnt 5.757866 hw_loss 0.025149 lr 0.00043147 rank 5
2023-02-21 14:35:35,501 DEBUG TRAIN Batch 16/800 loss 8.194328 loss_att 10.841202 loss_ctc 13.295864 loss_rnnt 6.919172 hw_loss 0.122958 lr 0.00043155 rank 2
2023-02-21 14:35:35,501 DEBUG TRAIN Batch 16/800 loss 5.586015 loss_att 8.402807 loss_ctc 7.174453 loss_rnnt 4.796794 hw_loss 0.026383 lr 0.00043152 rank 6
2023-02-21 14:35:35,501 DEBUG TRAIN Batch 16/800 loss 4.312430 loss_att 8.280529 loss_ctc 5.880866 loss_rnnt 3.209427 hw_loss 0.187986 lr 0.00043155 rank 1
2023-02-21 14:35:35,502 DEBUG TRAIN Batch 16/800 loss 16.408068 loss_att 19.865543 loss_ctc 24.791290 loss_rnnt 14.480154 hw_loss 0.222480 lr 0.00043154 rank 4
2023-02-21 14:35:35,503 DEBUG TRAIN Batch 16/800 loss 7.071793 loss_att 8.193722 loss_ctc 6.667512 loss_rnnt 6.849965 hw_loss 0.096272 lr 0.00043154 rank 0
2023-02-21 14:35:35,552 DEBUG TRAIN Batch 16/800 loss 11.714377 loss_att 12.210376 loss_ctc 14.516886 loss_rnnt 11.162964 hw_loss 0.147274 lr 0.00043155 rank 3
2023-02-21 14:36:55,579 DEBUG TRAIN Batch 16/900 loss 12.021984 loss_att 15.040848 loss_ctc 15.648075 loss_rnnt 10.901369 hw_loss 0.062553 lr 0.00043138 rank 4
2023-02-21 14:36:55,580 DEBUG TRAIN Batch 16/900 loss 11.078168 loss_att 12.062315 loss_ctc 15.718485 loss_rnnt 10.207943 hw_loss 0.102536 lr 0.00043139 rank 2
2023-02-21 14:36:55,581 DEBUG TRAIN Batch 16/900 loss 11.507657 loss_att 13.112236 loss_ctc 17.601105 loss_rnnt 10.281670 hw_loss 0.173648 lr 0.00043131 rank 5
2023-02-21 14:36:55,583 DEBUG TRAIN Batch 16/900 loss 5.767748 loss_att 7.587384 loss_ctc 6.468430 loss_rnnt 5.272051 hw_loss 0.071897 lr 0.00043139 rank 1
2023-02-21 14:36:55,584 DEBUG TRAIN Batch 16/900 loss 7.108365 loss_att 9.964777 loss_ctc 11.173025 loss_rnnt 5.886536 hw_loss 0.203610 lr 0.00043130 rank 7
2023-02-21 14:36:55,585 DEBUG TRAIN Batch 16/900 loss 11.680943 loss_att 12.014137 loss_ctc 16.765079 loss_rnnt 10.886801 hw_loss 0.093037 lr 0.00043136 rank 6
2023-02-21 14:36:55,585 DEBUG TRAIN Batch 16/900 loss 12.187075 loss_att 15.118843 loss_ctc 17.706228 loss_rnnt 10.820688 hw_loss 0.082769 lr 0.00043138 rank 0
2023-02-21 14:36:55,590 DEBUG TRAIN Batch 16/900 loss 6.426978 loss_att 10.082427 loss_ctc 9.382586 loss_rnnt 5.225619 hw_loss 0.142854 lr 0.00043139 rank 3
2023-02-21 14:38:14,972 DEBUG TRAIN Batch 16/1000 loss 12.147769 loss_att 17.521538 loss_ctc 16.812445 loss_rnnt 10.394329 hw_loss 0.106365 lr 0.00043122 rank 4
2023-02-21 14:38:14,975 DEBUG TRAIN Batch 16/1000 loss 6.640916 loss_att 10.029400 loss_ctc 9.560785 loss_rnnt 5.370113 hw_loss 0.382108 lr 0.00043123 rank 1
2023-02-21 14:38:14,975 DEBUG TRAIN Batch 16/1000 loss 6.574275 loss_att 8.755222 loss_ctc 8.798288 loss_rnnt 5.815564 hw_loss 0.048723 lr 0.00043122 rank 0
2023-02-21 14:38:14,978 DEBUG TRAIN Batch 16/1000 loss 11.207041 loss_att 14.056930 loss_ctc 13.668880 loss_rnnt 10.266300 hw_loss 0.079720 lr 0.00043123 rank 2
2023-02-21 14:38:14,979 DEBUG TRAIN Batch 16/1000 loss 7.052097 loss_att 9.398292 loss_ctc 9.714680 loss_rnnt 6.188793 hw_loss 0.073225 lr 0.00043114 rank 7
2023-02-21 14:38:14,981 DEBUG TRAIN Batch 16/1000 loss 11.288238 loss_att 15.651344 loss_ctc 17.506361 loss_rnnt 9.532495 hw_loss 0.101320 lr 0.00043115 rank 5
2023-02-21 14:38:14,985 DEBUG TRAIN Batch 16/1000 loss 11.968032 loss_att 18.024401 loss_ctc 14.463154 loss_rnnt 10.335031 hw_loss 0.166958 lr 0.00043120 rank 6
2023-02-21 14:38:14,985 DEBUG TRAIN Batch 16/1000 loss 12.992017 loss_att 16.816643 loss_ctc 15.895589 loss_rnnt 11.785343 hw_loss 0.102386 lr 0.00043123 rank 3
2023-02-21 14:39:36,461 DEBUG TRAIN Batch 16/1100 loss 13.042052 loss_att 16.705574 loss_ctc 20.167002 loss_rnnt 11.303334 hw_loss 0.105034 lr 0.00043107 rank 1
2023-02-21 14:39:36,466 DEBUG TRAIN Batch 16/1100 loss 15.708035 loss_att 18.889330 loss_ctc 22.200928 loss_rnnt 14.127653 hw_loss 0.147008 lr 0.00043106 rank 4
2023-02-21 14:39:36,467 DEBUG TRAIN Batch 16/1100 loss 12.535836 loss_att 16.435770 loss_ctc 17.857922 loss_rnnt 11.007142 hw_loss 0.073305 lr 0.00043098 rank 7
2023-02-21 14:39:36,468 DEBUG TRAIN Batch 16/1100 loss 4.980837 loss_att 8.289526 loss_ctc 6.596643 loss_rnnt 4.070626 hw_loss 0.061935 lr 0.00043104 rank 6
2023-02-21 14:39:36,470 DEBUG TRAIN Batch 16/1100 loss 10.226881 loss_att 16.157835 loss_ctc 14.982790 loss_rnnt 8.364908 hw_loss 0.078114 lr 0.00043107 rank 2
2023-02-21 14:39:36,473 DEBUG TRAIN Batch 16/1100 loss 9.420722 loss_att 11.507481 loss_ctc 13.549544 loss_rnnt 8.364340 hw_loss 0.165977 lr 0.00043107 rank 3
2023-02-21 14:39:36,475 DEBUG TRAIN Batch 16/1100 loss 18.064594 loss_att 22.107227 loss_ctc 24.793194 loss_rnnt 16.270300 hw_loss 0.166162 lr 0.00043099 rank 5
2023-02-21 14:39:36,520 DEBUG TRAIN Batch 16/1100 loss 13.637462 loss_att 15.276889 loss_ctc 17.531059 loss_rnnt 12.755369 hw_loss 0.065738 lr 0.00043106 rank 0
2023-02-21 14:40:56,062 DEBUG TRAIN Batch 16/1200 loss 10.903437 loss_att 12.091160 loss_ctc 14.504031 loss_rnnt 10.107866 hw_loss 0.146150 lr 0.00043082 rank 7
2023-02-21 14:40:56,066 DEBUG TRAIN Batch 16/1200 loss 6.735642 loss_att 9.443312 loss_ctc 7.991251 loss_rnnt 5.958343 hw_loss 0.128158 lr 0.00043091 rank 2
2023-02-21 14:40:56,067 DEBUG TRAIN Batch 16/1200 loss 4.199551 loss_att 7.113717 loss_ctc 6.960362 loss_rnnt 3.183750 hw_loss 0.121610 lr 0.00043090 rank 4
2023-02-21 14:40:56,068 DEBUG TRAIN Batch 16/1200 loss 8.957624 loss_att 9.429152 loss_ctc 10.909588 loss_rnnt 8.543624 hw_loss 0.111438 lr 0.00043091 rank 1
2023-02-21 14:40:56,069 DEBUG TRAIN Batch 16/1200 loss 7.076263 loss_att 8.489302 loss_ctc 10.750883 loss_rnnt 6.210922 hw_loss 0.173969 lr 0.00043088 rank 6
2023-02-21 14:40:56,073 DEBUG TRAIN Batch 16/1200 loss 7.337202 loss_att 8.526281 loss_ctc 8.258193 loss_rnnt 6.873053 hw_loss 0.194126 lr 0.00043091 rank 3
2023-02-21 14:40:56,075 DEBUG TRAIN Batch 16/1200 loss 5.318747 loss_att 7.047874 loss_ctc 9.088786 loss_rnnt 4.403690 hw_loss 0.124797 lr 0.00043083 rank 5
2023-02-21 14:40:56,077 DEBUG TRAIN Batch 16/1200 loss 12.527694 loss_att 16.550858 loss_ctc 18.014746 loss_rnnt 10.949737 hw_loss 0.078220 lr 0.00043090 rank 0
2023-02-21 14:42:14,734 DEBUG TRAIN Batch 16/1300 loss 10.753181 loss_att 10.177787 loss_ctc 11.616791 loss_rnnt 10.574368 hw_loss 0.335142 lr 0.00043074 rank 4
2023-02-21 14:42:14,734 DEBUG TRAIN Batch 16/1300 loss 22.956642 loss_att 28.664680 loss_ctc 27.726151 loss_rnnt 21.072004 hw_loss 0.200808 lr 0.00043075 rank 2
2023-02-21 14:42:14,735 DEBUG TRAIN Batch 16/1300 loss 9.999542 loss_att 12.059282 loss_ctc 13.789982 loss_rnnt 9.027312 hw_loss 0.102919 lr 0.00043075 rank 1
2023-02-21 14:42:14,736 DEBUG TRAIN Batch 16/1300 loss 13.883925 loss_att 18.581081 loss_ctc 17.404655 loss_rnnt 12.377533 hw_loss 0.182869 lr 0.00043066 rank 7
2023-02-21 14:42:14,739 DEBUG TRAIN Batch 16/1300 loss 16.261360 loss_att 16.456970 loss_ctc 19.813507 loss_rnnt 15.672359 hw_loss 0.142985 lr 0.00043074 rank 0
2023-02-21 14:42:14,741 DEBUG TRAIN Batch 16/1300 loss 11.915973 loss_att 11.692913 loss_ctc 14.734578 loss_rnnt 11.346512 hw_loss 0.446738 lr 0.00043075 rank 3
2023-02-21 14:42:14,742 DEBUG TRAIN Batch 16/1300 loss 6.204338 loss_att 9.147488 loss_ctc 9.647499 loss_rnnt 5.081677 hw_loss 0.140516 lr 0.00043067 rank 5
2023-02-21 14:42:14,786 DEBUG TRAIN Batch 16/1300 loss 16.575798 loss_att 22.188719 loss_ctc 21.648209 loss_rnnt 14.714041 hw_loss 0.117851 lr 0.00043072 rank 6
2023-02-21 14:43:35,152 DEBUG TRAIN Batch 16/1400 loss 17.843624 loss_att 20.320995 loss_ctc 25.584732 loss_rnnt 16.291220 hw_loss 0.046468 lr 0.00043050 rank 7
2023-02-21 14:43:35,154 DEBUG TRAIN Batch 16/1400 loss 11.948285 loss_att 12.715137 loss_ctc 13.526027 loss_rnnt 11.570028 hw_loss 0.027228 lr 0.00043056 rank 6
2023-02-21 14:43:35,156 DEBUG TRAIN Batch 16/1400 loss 6.618662 loss_att 11.266447 loss_ctc 8.674397 loss_rnnt 5.400976 hw_loss 0.026308 lr 0.00043059 rank 1
2023-02-21 14:43:35,156 DEBUG TRAIN Batch 16/1400 loss 8.262652 loss_att 12.140382 loss_ctc 14.236593 loss_rnnt 6.654832 hw_loss 0.067028 lr 0.00043058 rank 4
2023-02-21 14:43:35,159 DEBUG TRAIN Batch 16/1400 loss 12.584775 loss_att 16.835745 loss_ctc 12.075924 loss_rnnt 11.788222 hw_loss 0.026636 lr 0.00043059 rank 3
2023-02-21 14:43:35,162 DEBUG TRAIN Batch 16/1400 loss 13.300257 loss_att 14.954159 loss_ctc 18.562458 loss_rnnt 12.206035 hw_loss 0.115903 lr 0.00043059 rank 2
2023-02-21 14:43:35,164 DEBUG TRAIN Batch 16/1400 loss 8.230781 loss_att 11.076782 loss_ctc 10.674454 loss_rnnt 7.309855 hw_loss 0.048565 lr 0.00043058 rank 0
2023-02-21 14:43:35,167 DEBUG TRAIN Batch 16/1400 loss 7.671196 loss_att 9.986094 loss_ctc 9.455786 loss_rnnt 6.922981 hw_loss 0.088668 lr 0.00043051 rank 5
2023-02-21 14:44:53,564 DEBUG TRAIN Batch 16/1500 loss 3.939842 loss_att 5.807138 loss_ctc 3.552631 loss_rnnt 3.546789 hw_loss 0.133541 lr 0.00043042 rank 4
2023-02-21 14:44:53,566 DEBUG TRAIN Batch 16/1500 loss 9.318995 loss_att 12.866108 loss_ctc 13.770762 loss_rnnt 8.000177 hw_loss 0.029676 lr 0.00043043 rank 2
2023-02-21 14:44:53,571 DEBUG TRAIN Batch 16/1500 loss 21.743078 loss_att 27.482498 loss_ctc 30.256041 loss_rnnt 19.444540 hw_loss 0.029234 lr 0.00043040 rank 6
2023-02-21 14:44:53,573 DEBUG TRAIN Batch 16/1500 loss 16.253962 loss_att 18.909807 loss_ctc 24.234749 loss_rnnt 14.595825 hw_loss 0.117864 lr 0.00043042 rank 0
2023-02-21 14:44:53,574 DEBUG TRAIN Batch 16/1500 loss 11.321037 loss_att 15.327467 loss_ctc 12.017645 loss_rnnt 10.347578 hw_loss 0.148673 lr 0.00043043 rank 1
2023-02-21 14:44:53,574 DEBUG TRAIN Batch 16/1500 loss 8.704779 loss_att 16.352688 loss_ctc 12.799772 loss_rnnt 6.581943 hw_loss 0.088604 lr 0.00043043 rank 3
2023-02-21 14:44:53,576 DEBUG TRAIN Batch 16/1500 loss 8.092389 loss_att 10.727240 loss_ctc 13.002848 loss_rnnt 6.846640 hw_loss 0.120097 lr 0.00043034 rank 7
2023-02-21 14:44:53,577 DEBUG TRAIN Batch 16/1500 loss 8.741832 loss_att 13.519758 loss_ctc 12.608706 loss_rnnt 7.227842 hw_loss 0.080291 lr 0.00043035 rank 5
2023-02-21 14:46:11,780 DEBUG TRAIN Batch 16/1600 loss 10.223160 loss_att 14.450503 loss_ctc 15.080664 loss_rnnt 8.640722 hw_loss 0.167438 lr 0.00043027 rank 1
2023-02-21 14:46:11,781 DEBUG TRAIN Batch 16/1600 loss 10.583857 loss_att 12.994658 loss_ctc 16.373383 loss_rnnt 9.280277 hw_loss 0.092779 lr 0.00043024 rank 6
2023-02-21 14:46:11,781 DEBUG TRAIN Batch 16/1600 loss 7.897409 loss_att 11.249316 loss_ctc 9.238544 loss_rnnt 6.937830 hw_loss 0.206961 lr 0.00043027 rank 2
2023-02-21 14:46:11,783 DEBUG TRAIN Batch 16/1600 loss 8.409561 loss_att 11.047948 loss_ctc 11.718524 loss_rnnt 7.399392 hw_loss 0.077429 lr 0.00043018 rank 7
2023-02-21 14:46:11,786 DEBUG TRAIN Batch 16/1600 loss 18.056986 loss_att 23.170343 loss_ctc 27.124294 loss_rnnt 15.773765 hw_loss 0.096703 lr 0.00043026 rank 4
2023-02-21 14:46:11,788 DEBUG TRAIN Batch 16/1600 loss 12.313106 loss_att 14.689367 loss_ctc 14.251978 loss_rnnt 11.559772 hw_loss 0.036683 lr 0.00043026 rank 0
2023-02-21 14:46:11,791 DEBUG TRAIN Batch 16/1600 loss 11.895149 loss_att 14.675670 loss_ctc 14.862467 loss_rnnt 10.849077 hw_loss 0.176859 lr 0.00043019 rank 5
2023-02-21 14:46:11,832 DEBUG TRAIN Batch 16/1600 loss 5.690078 loss_att 8.242134 loss_ctc 7.932978 loss_rnnt 4.839401 hw_loss 0.077273 lr 0.00043027 rank 3
2023-02-21 14:47:30,993 DEBUG TRAIN Batch 16/1700 loss 11.974608 loss_att 14.910456 loss_ctc 22.972149 loss_rnnt 9.873786 hw_loss 0.088713 lr 0.00043011 rank 1
2023-02-21 14:47:30,994 DEBUG TRAIN Batch 16/1700 loss 6.557346 loss_att 8.925447 loss_ctc 6.552855 loss_rnnt 6.027797 hw_loss 0.105991 lr 0.00043010 rank 4
2023-02-21 14:47:30,995 DEBUG TRAIN Batch 16/1700 loss 10.681911 loss_att 10.953239 loss_ctc 14.587030 loss_rnnt 10.060614 hw_loss 0.086901 lr 0.00043011 rank 2
2023-02-21 14:47:31,005 DEBUG TRAIN Batch 16/1700 loss 8.165652 loss_att 11.660512 loss_ctc 10.135777 loss_rnnt 7.151310 hw_loss 0.098786 lr 0.00043011 rank 3
2023-02-21 14:47:31,009 DEBUG TRAIN Batch 16/1700 loss 12.836338 loss_att 14.648257 loss_ctc 18.794376 loss_rnnt 11.590737 hw_loss 0.166522 lr 0.00043003 rank 5
2023-02-21 14:47:31,033 DEBUG TRAIN Batch 16/1700 loss 8.837801 loss_att 12.508135 loss_ctc 10.417012 loss_rnnt 7.874039 hw_loss 0.035875 lr 0.00043002 rank 7
2023-02-21 14:47:31,048 DEBUG TRAIN Batch 16/1700 loss 10.599657 loss_att 12.969763 loss_ctc 13.485005 loss_rnnt 9.670682 hw_loss 0.131703 lr 0.00043008 rank 6
2023-02-21 14:47:31,048 DEBUG TRAIN Batch 16/1700 loss 14.710798 loss_att 18.588890 loss_ctc 19.223021 loss_rnnt 13.282625 hw_loss 0.095482 lr 0.00043010 rank 0
2023-02-21 14:48:51,746 DEBUG TRAIN Batch 16/1800 loss 16.281179 loss_att 18.213636 loss_ctc 25.034794 loss_rnnt 14.673859 hw_loss 0.100648 lr 0.00042994 rank 0
2023-02-21 14:48:51,746 DEBUG TRAIN Batch 16/1800 loss 11.624296 loss_att 14.330582 loss_ctc 17.104691 loss_rnnt 10.282598 hw_loss 0.130727 lr 0.00042994 rank 4
2023-02-21 14:48:51,748 DEBUG TRAIN Batch 16/1800 loss 13.667478 loss_att 16.262945 loss_ctc 18.049847 loss_rnnt 12.500885 hw_loss 0.118468 lr 0.00042986 rank 7
2023-02-21 14:48:51,748 DEBUG TRAIN Batch 16/1800 loss 11.972861 loss_att 15.216749 loss_ctc 17.881033 loss_rnnt 10.491822 hw_loss 0.083446 lr 0.00042995 rank 2
2023-02-21 14:48:51,750 DEBUG TRAIN Batch 16/1800 loss 26.640003 loss_att 28.441132 loss_ctc 35.890823 loss_rnnt 25.002150 hw_loss 0.082846 lr 0.00042987 rank 5
2023-02-21 14:48:51,751 DEBUG TRAIN Batch 16/1800 loss 17.343811 loss_att 17.852407 loss_ctc 22.606573 loss_rnnt 16.424971 hw_loss 0.216412 lr 0.00042995 rank 1
2023-02-21 14:48:51,753 DEBUG TRAIN Batch 16/1800 loss 18.945169 loss_att 20.663763 loss_ctc 25.848480 loss_rnnt 17.647223 hw_loss 0.063347 lr 0.00042996 rank 3
2023-02-21 14:48:51,795 DEBUG TRAIN Batch 16/1800 loss 12.196000 loss_att 14.341841 loss_ctc 17.513233 loss_rnnt 10.941036 hw_loss 0.219057 lr 0.00042992 rank 6
2023-02-21 14:50:11,911 DEBUG TRAIN Batch 16/1900 loss 9.693657 loss_att 13.812363 loss_ctc 15.467261 loss_rnnt 8.055899 hw_loss 0.082878 lr 0.00042976 rank 6
2023-02-21 14:50:11,911 DEBUG TRAIN Batch 16/1900 loss 6.803341 loss_att 7.690059 loss_ctc 8.393310 loss_rnnt 6.306775 hw_loss 0.201050 lr 0.00042979 rank 4
2023-02-21 14:50:11,913 DEBUG TRAIN Batch 16/1900 loss 19.099701 loss_att 18.121212 loss_ctc 19.677238 loss_rnnt 19.162855 hw_loss 0.104138 lr 0.00042978 rank 0
2023-02-21 14:50:11,920 DEBUG TRAIN Batch 16/1900 loss 10.517925 loss_att 11.251579 loss_ctc 12.896600 loss_rnnt 10.010459 hw_loss 0.081711 lr 0.00042970 rank 7
2023-02-21 14:50:11,920 DEBUG TRAIN Batch 16/1900 loss 7.822136 loss_att 8.562189 loss_ctc 9.127223 loss_rnnt 7.447874 hw_loss 0.097952 lr 0.00042980 rank 3
2023-02-21 14:50:11,922 DEBUG TRAIN Batch 16/1900 loss 16.080915 loss_att 19.323311 loss_ctc 18.584774 loss_rnnt 15.082606 hw_loss 0.029969 lr 0.00042971 rank 5
2023-02-21 14:50:11,941 DEBUG TRAIN Batch 16/1900 loss 16.538103 loss_att 14.725439 loss_ctc 21.641201 loss_rnnt 16.064686 hw_loss 0.291631 lr 0.00042979 rank 2
2023-02-21 14:50:11,953 DEBUG TRAIN Batch 16/1900 loss 12.998348 loss_att 13.458923 loss_ctc 16.917522 loss_rnnt 12.367453 hw_loss 0.030420 lr 0.00042979 rank 1
2023-02-21 14:51:29,458 DEBUG TRAIN Batch 16/2000 loss 14.248530 loss_att 17.437393 loss_ctc 18.703709 loss_rnnt 12.892283 hw_loss 0.233344 lr 0.00042963 rank 1
2023-02-21 14:51:29,459 DEBUG TRAIN Batch 16/2000 loss 16.873384 loss_att 16.941904 loss_ctc 17.686077 loss_rnnt 16.717627 hw_loss 0.063177 lr 0.00042963 rank 2
2023-02-21 14:51:29,459 DEBUG TRAIN Batch 16/2000 loss 9.828849 loss_att 11.794991 loss_ctc 11.721479 loss_rnnt 9.020984 hw_loss 0.304286 lr 0.00042963 rank 4
2023-02-21 14:51:29,461 DEBUG TRAIN Batch 16/2000 loss 15.209078 loss_att 20.441341 loss_ctc 22.155975 loss_rnnt 13.119308 hw_loss 0.219495 lr 0.00042954 rank 7
2023-02-21 14:51:29,463 DEBUG TRAIN Batch 16/2000 loss 14.121780 loss_att 15.338894 loss_ctc 17.695135 loss_rnnt 13.328390 hw_loss 0.137853 lr 0.00042962 rank 0
2023-02-21 14:51:29,466 DEBUG TRAIN Batch 16/2000 loss 10.095677 loss_att 12.634589 loss_ctc 12.857471 loss_rnnt 9.168713 hw_loss 0.095521 lr 0.00042955 rank 5
2023-02-21 14:51:29,468 DEBUG TRAIN Batch 16/2000 loss 13.462215 loss_att 15.542647 loss_ctc 24.865986 loss_rnnt 11.454948 hw_loss 0.132520 lr 0.00042964 rank 3
2023-02-21 14:51:29,513 DEBUG TRAIN Batch 16/2000 loss 4.344229 loss_att 7.982437 loss_ctc 5.594570 loss_rnnt 3.417466 hw_loss 0.060769 lr 0.00042960 rank 6
2023-02-21 14:52:49,760 DEBUG TRAIN Batch 16/2100 loss 5.168987 loss_att 10.046346 loss_ctc 9.807076 loss_rnnt 3.528665 hw_loss 0.087072 lr 0.00042944 rank 6
2023-02-21 14:52:49,763 DEBUG TRAIN Batch 16/2100 loss 8.954006 loss_att 12.671139 loss_ctc 10.567715 loss_rnnt 7.970254 hw_loss 0.047184 lr 0.00042947 rank 4
2023-02-21 14:52:49,764 DEBUG TRAIN Batch 16/2100 loss 7.014474 loss_att 9.413011 loss_ctc 9.134443 loss_rnnt 6.195114 hw_loss 0.106858 lr 0.00042947 rank 1
2023-02-21 14:52:49,766 DEBUG TRAIN Batch 16/2100 loss 7.832702 loss_att 11.637392 loss_ctc 11.762904 loss_rnnt 6.494797 hw_loss 0.099263 lr 0.00042946 rank 0
2023-02-21 14:52:49,768 DEBUG TRAIN Batch 16/2100 loss 13.172333 loss_att 14.969416 loss_ctc 15.226635 loss_rnnt 12.525200 hw_loss 0.025893 lr 0.00042940 rank 5
2023-02-21 14:52:49,767 DEBUG TRAIN Batch 16/2100 loss 10.092217 loss_att 14.494955 loss_ctc 11.429087 loss_rnnt 8.977678 hw_loss 0.104516 lr 0.00042947 rank 2
2023-02-21 14:52:49,767 DEBUG TRAIN Batch 16/2100 loss 8.401022 loss_att 10.186418 loss_ctc 9.132613 loss_rnnt 7.910268 hw_loss 0.067741 lr 0.00042938 rank 7
2023-02-21 14:52:49,772 DEBUG TRAIN Batch 16/2100 loss 8.046122 loss_att 9.514906 loss_ctc 9.367586 loss_rnnt 7.439658 hw_loss 0.255960 lr 0.00042948 rank 3
2023-02-21 14:54:09,966 DEBUG TRAIN Batch 16/2200 loss 13.960521 loss_att 14.035969 loss_ctc 19.926424 loss_rnnt 12.979725 hw_loss 0.319224 lr 0.00042929 rank 6
2023-02-21 14:54:09,967 DEBUG TRAIN Batch 16/2200 loss 7.545046 loss_att 10.675714 loss_ctc 12.624625 loss_rnnt 6.226521 hw_loss 0.028337 lr 0.00042932 rank 2
2023-02-21 14:54:09,971 DEBUG TRAIN Batch 16/2200 loss 15.631639 loss_att 14.697495 loss_ctc 17.653820 loss_rnnt 15.486079 hw_loss 0.117683 lr 0.00042931 rank 0
2023-02-21 14:54:09,972 DEBUG TRAIN Batch 16/2200 loss 12.677090 loss_att 16.335501 loss_ctc 16.836348 loss_rnnt 11.309017 hw_loss 0.153417 lr 0.00042931 rank 4
2023-02-21 14:54:09,973 DEBUG TRAIN Batch 16/2200 loss 14.924495 loss_att 16.887897 loss_ctc 16.200197 loss_rnnt 14.306821 hw_loss 0.102935 lr 0.00042922 rank 7
2023-02-21 14:54:09,974 DEBUG TRAIN Batch 16/2200 loss 16.553047 loss_att 21.583988 loss_ctc 24.758867 loss_rnnt 14.399612 hw_loss 0.099629 lr 0.00042932 rank 3
2023-02-21 14:54:09,977 DEBUG TRAIN Batch 16/2200 loss 9.455128 loss_att 14.198339 loss_ctc 14.816124 loss_rnnt 7.741749 hw_loss 0.093630 lr 0.00042932 rank 1
2023-02-21 14:54:09,978 DEBUG TRAIN Batch 16/2200 loss 3.436860 loss_att 6.522727 loss_ctc 5.642715 loss_rnnt 2.415007 hw_loss 0.207310 lr 0.00042924 rank 5
2023-02-21 14:55:26,946 DEBUG TRAIN Batch 16/2300 loss 15.018535 loss_att 18.357010 loss_ctc 20.669922 loss_rnnt 13.579522 hw_loss 0.033373 lr 0.00042915 rank 4
2023-02-21 14:55:26,948 DEBUG TRAIN Batch 16/2300 loss 9.666121 loss_att 11.704161 loss_ctc 13.066896 loss_rnnt 8.786140 hw_loss 0.035502 lr 0.00042916 rank 2
2023-02-21 14:55:26,950 DEBUG TRAIN Batch 16/2300 loss 10.490479 loss_att 15.592849 loss_ctc 13.879715 loss_rnnt 8.958670 hw_loss 0.111447 lr 0.00042907 rank 7
2023-02-21 14:55:26,951 DEBUG TRAIN Batch 16/2300 loss 12.170920 loss_att 14.362328 loss_ctc 14.958717 loss_rnnt 11.341727 hw_loss 0.036010 lr 0.00042915 rank 0
2023-02-21 14:55:26,951 DEBUG TRAIN Batch 16/2300 loss 18.439419 loss_att 18.713188 loss_ctc 22.566862 loss_rnnt 17.731773 hw_loss 0.192309 lr 0.00042913 rank 6
2023-02-21 14:55:26,952 DEBUG TRAIN Batch 16/2300 loss 11.436150 loss_att 13.195333 loss_ctc 16.643101 loss_rnnt 10.329636 hw_loss 0.113279 lr 0.00042916 rank 1
2023-02-21 14:55:26,952 DEBUG TRAIN Batch 16/2300 loss 11.031623 loss_att 15.199020 loss_ctc 14.134410 loss_rnnt 9.726677 hw_loss 0.108304 lr 0.00042916 rank 3
2023-02-21 14:55:26,958 DEBUG TRAIN Batch 16/2300 loss 9.271424 loss_att 10.966430 loss_ctc 9.706984 loss_rnnt 8.782732 hw_loss 0.171781 lr 0.00042908 rank 5
2023-02-21 14:56:46,225 DEBUG TRAIN Batch 16/2400 loss 17.447346 loss_att 19.761318 loss_ctc 24.906403 loss_rnnt 15.917805 hw_loss 0.135384 lr 0.00042891 rank 7
2023-02-21 14:56:46,227 DEBUG TRAIN Batch 16/2400 loss 14.087351 loss_att 15.701115 loss_ctc 18.345022 loss_rnnt 13.105350 hw_loss 0.171674 lr 0.00042897 rank 6
2023-02-21 14:56:46,229 DEBUG TRAIN Batch 16/2400 loss 9.299263 loss_att 11.669522 loss_ctc 11.389191 loss_rnnt 8.468017 hw_loss 0.147259 lr 0.00042900 rank 1
2023-02-21 14:56:46,232 DEBUG TRAIN Batch 16/2400 loss 11.000244 loss_att 13.487959 loss_ctc 19.728058 loss_rnnt 9.212416 hw_loss 0.237332 lr 0.00042899 rank 4
2023-02-21 14:56:46,234 DEBUG TRAIN Batch 16/2400 loss 17.908131 loss_att 20.375214 loss_ctc 26.554701 loss_rnnt 16.182394 hw_loss 0.148961 lr 0.00042892 rank 5
2023-02-21 14:56:46,234 DEBUG TRAIN Batch 16/2400 loss 7.535705 loss_att 10.605919 loss_ctc 8.616901 loss_rnnt 6.691132 hw_loss 0.161945 lr 0.00042900 rank 2
2023-02-21 14:56:46,246 DEBUG TRAIN Batch 16/2400 loss 10.582455 loss_att 12.551519 loss_ctc 16.246025 loss_rnnt 9.405432 hw_loss 0.052626 lr 0.00042899 rank 0
2023-02-21 14:56:46,281 DEBUG TRAIN Batch 16/2400 loss 12.152974 loss_att 15.143852 loss_ctc 15.267618 loss_rnnt 11.071612 hw_loss 0.127313 lr 0.00042901 rank 3
2023-02-21 14:58:08,000 DEBUG TRAIN Batch 16/2500 loss 13.146871 loss_att 12.769911 loss_ctc 20.819342 loss_rnnt 12.123249 hw_loss 0.142533 lr 0.00042884 rank 2
2023-02-21 14:58:08,004 DEBUG TRAIN Batch 16/2500 loss 8.708315 loss_att 12.520262 loss_ctc 11.627258 loss_rnnt 7.498127 hw_loss 0.109887 lr 0.00042875 rank 7
2023-02-21 14:58:08,006 DEBUG TRAIN Batch 16/2500 loss 8.880378 loss_att 8.020689 loss_ctc 10.549688 loss_rnnt 8.642327 hw_loss 0.351401 lr 0.00042881 rank 6
2023-02-21 14:58:08,007 DEBUG TRAIN Batch 16/2500 loss 10.411759 loss_att 12.972283 loss_ctc 13.951919 loss_rnnt 9.349663 hw_loss 0.146195 lr 0.00042884 rank 4
2023-02-21 14:58:08,008 DEBUG TRAIN Batch 16/2500 loss 10.048910 loss_att 10.437670 loss_ctc 11.621548 loss_rnnt 9.664824 hw_loss 0.181215 lr 0.00042885 rank 3
2023-02-21 14:58:08,009 DEBUG TRAIN Batch 16/2500 loss 14.625854 loss_att 13.126113 loss_ctc 16.774654 loss_rnnt 14.443110 hw_loss 0.367846 lr 0.00042884 rank 1
2023-02-21 14:58:08,015 DEBUG TRAIN Batch 16/2500 loss 7.661957 loss_att 13.326204 loss_ctc 12.004092 loss_rnnt 5.893138 hw_loss 0.106911 lr 0.00042876 rank 5
2023-02-21 14:58:08,055 DEBUG TRAIN Batch 16/2500 loss 8.352116 loss_att 8.833723 loss_ctc 9.065646 loss_rnnt 8.108187 hw_loss 0.098382 lr 0.00042883 rank 0
2023-02-21 14:59:27,085 DEBUG TRAIN Batch 16/2600 loss 8.871017 loss_att 11.379471 loss_ctc 8.625463 loss_rnnt 8.389838 hw_loss 0.022927 lr 0.00042859 rank 7
2023-02-21 14:59:27,089 DEBUG TRAIN Batch 16/2600 loss 6.798845 loss_att 11.418581 loss_ctc 11.447849 loss_rnnt 5.185115 hw_loss 0.131092 lr 0.00042868 rank 4
2023-02-21 14:59:27,089 DEBUG TRAIN Batch 16/2600 loss 7.222021 loss_att 12.592145 loss_ctc 8.411210 loss_rnnt 5.947457 hw_loss 0.078710 lr 0.00042866 rank 6
2023-02-21 14:59:27,092 DEBUG TRAIN Batch 16/2600 loss 7.055529 loss_att 11.690180 loss_ctc 12.815433 loss_rnnt 5.262352 hw_loss 0.184236 lr 0.00042869 rank 3
2023-02-21 14:59:27,093 DEBUG TRAIN Batch 16/2600 loss 8.098660 loss_att 15.649687 loss_ctc 14.058723 loss_rnnt 5.742056 hw_loss 0.096981 lr 0.00042868 rank 2
2023-02-21 14:59:27,095 DEBUG TRAIN Batch 16/2600 loss 10.704432 loss_att 12.927361 loss_ctc 11.292709 loss_rnnt 10.020885 hw_loss 0.300983 lr 0.00042869 rank 1
2023-02-21 14:59:27,098 DEBUG TRAIN Batch 16/2600 loss 5.980841 loss_att 10.020670 loss_ctc 8.051329 loss_rnnt 4.802997 hw_loss 0.175902 lr 0.00042867 rank 0
2023-02-21 14:59:27,143 DEBUG TRAIN Batch 16/2600 loss 8.913876 loss_att 14.987633 loss_ctc 13.539101 loss_rnnt 7.070199 hw_loss 0.022927 lr 0.00042861 rank 5
2023-02-21 15:00:45,094 DEBUG TRAIN Batch 16/2700 loss 9.237254 loss_att 12.549664 loss_ctc 15.333643 loss_rnnt 7.659925 hw_loss 0.191239 lr 0.00042850 rank 6
2023-02-21 15:00:45,100 DEBUG TRAIN Batch 16/2700 loss 21.379087 loss_att 19.235910 loss_ctc 24.248556 loss_rnnt 21.328474 hw_loss 0.181221 lr 0.00042853 rank 2
2023-02-21 15:00:45,102 DEBUG TRAIN Batch 16/2700 loss 7.680404 loss_att 13.270308 loss_ctc 13.939980 loss_rnnt 5.665854 hw_loss 0.116171 lr 0.00042852 rank 0
2023-02-21 15:00:45,103 DEBUG TRAIN Batch 16/2700 loss 4.313550 loss_att 7.437643 loss_ctc 8.357521 loss_rnnt 3.085904 hw_loss 0.119308 lr 0.00042853 rank 1
2023-02-21 15:00:45,103 DEBUG TRAIN Batch 16/2700 loss 11.241919 loss_att 14.702255 loss_ctc 13.486845 loss_rnnt 10.154831 hw_loss 0.179432 lr 0.00042844 rank 7
2023-02-21 15:00:45,106 DEBUG TRAIN Batch 16/2700 loss 6.926346 loss_att 7.932798 loss_ctc 7.602269 loss_rnnt 6.593215 hw_loss 0.078222 lr 0.00042852 rank 4
2023-02-21 15:00:45,109 DEBUG TRAIN Batch 16/2700 loss 8.677493 loss_att 12.358320 loss_ctc 13.187884 loss_rnnt 7.324909 hw_loss 0.028188 lr 0.00042853 rank 3
2023-02-21 15:00:45,148 DEBUG TRAIN Batch 16/2700 loss 3.645268 loss_att 6.800313 loss_ctc 5.735466 loss_rnnt 2.700732 hw_loss 0.065315 lr 0.00042845 rank 5
2023-02-21 15:02:05,583 DEBUG TRAIN Batch 16/2800 loss 10.531135 loss_att 14.223577 loss_ctc 11.175535 loss_rnnt 9.654087 hw_loss 0.098697 lr 0.00042828 rank 7
2023-02-21 15:02:05,592 DEBUG TRAIN Batch 16/2800 loss 3.945297 loss_att 5.879399 loss_ctc 4.130856 loss_rnnt 3.488106 hw_loss 0.085556 lr 0.00042836 rank 4
2023-02-21 15:02:05,592 DEBUG TRAIN Batch 16/2800 loss 15.052835 loss_att 21.245867 loss_ctc 17.752733 loss_rnnt 13.413036 hw_loss 0.077260 lr 0.00042837 rank 2
2023-02-21 15:02:05,593 DEBUG TRAIN Batch 16/2800 loss 8.690750 loss_att 12.500462 loss_ctc 10.054871 loss_rnnt 7.701156 hw_loss 0.085817 lr 0.00042837 rank 3
2023-02-21 15:02:05,594 DEBUG TRAIN Batch 16/2800 loss 8.417563 loss_att 10.463675 loss_ctc 12.057695 loss_rnnt 7.421045 hw_loss 0.191148 lr 0.00042837 rank 1
2023-02-21 15:02:05,597 DEBUG TRAIN Batch 16/2800 loss 20.091824 loss_att 24.342867 loss_ctc 26.788179 loss_rnnt 18.281061 hw_loss 0.126949 lr 0.00042834 rank 6
2023-02-21 15:02:05,598 DEBUG TRAIN Batch 16/2800 loss 5.147644 loss_att 7.609426 loss_ctc 6.995960 loss_rnnt 4.340076 hw_loss 0.128941 lr 0.00042829 rank 5
2023-02-21 15:02:05,603 DEBUG TRAIN Batch 16/2800 loss 13.131394 loss_att 16.412981 loss_ctc 18.423340 loss_rnnt 11.700893 hw_loss 0.128608 lr 0.00042836 rank 0
2023-02-21 15:03:24,824 DEBUG TRAIN Batch 16/2900 loss 6.275307 loss_att 10.509099 loss_ctc 8.441621 loss_rnnt 5.090759 hw_loss 0.091777 lr 0.00042812 rank 7
2023-02-21 15:03:24,826 DEBUG TRAIN Batch 16/2900 loss 7.480863 loss_att 11.678596 loss_ctc 9.445409 loss_rnnt 6.309812 hw_loss 0.130433 lr 0.00042821 rank 2
2023-02-21 15:03:24,826 DEBUG TRAIN Batch 16/2900 loss 9.555362 loss_att 12.418489 loss_ctc 14.010387 loss_rnnt 8.209709 hw_loss 0.335670 lr 0.00042821 rank 4
2023-02-21 15:03:24,827 DEBUG TRAIN Batch 16/2900 loss 17.119974 loss_att 22.561054 loss_ctc 21.649746 loss_rnnt 15.367776 hw_loss 0.112521 lr 0.00042818 rank 6
2023-02-21 15:03:24,828 DEBUG TRAIN Batch 16/2900 loss 9.706740 loss_att 11.246197 loss_ctc 11.779293 loss_rnnt 8.991110 hw_loss 0.246372 lr 0.00042821 rank 1
2023-02-21 15:03:24,829 DEBUG TRAIN Batch 16/2900 loss 11.205407 loss_att 13.821024 loss_ctc 14.769768 loss_rnnt 10.188953 hw_loss 0.033906 lr 0.00042820 rank 0
2023-02-21 15:03:24,833 DEBUG TRAIN Batch 16/2900 loss 5.524596 loss_att 10.416084 loss_ctc 8.954361 loss_rnnt 4.022975 hw_loss 0.123790 lr 0.00042822 rank 3
2023-02-21 15:03:24,834 DEBUG TRAIN Batch 16/2900 loss 6.629598 loss_att 11.615031 loss_ctc 9.088134 loss_rnnt 5.156608 hw_loss 0.277685 lr 0.00042813 rank 5
2023-02-21 15:04:44,082 DEBUG TRAIN Batch 16/3000 loss 9.171583 loss_att 12.734731 loss_ctc 15.416584 loss_rnnt 7.581848 hw_loss 0.083323 lr 0.00042806 rank 1
2023-02-21 15:04:44,082 DEBUG TRAIN Batch 16/3000 loss 9.528599 loss_att 12.270473 loss_ctc 11.386749 loss_rnnt 8.687341 hw_loss 0.084618 lr 0.00042806 rank 3
2023-02-21 15:04:44,083 DEBUG TRAIN Batch 16/3000 loss 16.887980 loss_att 19.771717 loss_ctc 21.600731 loss_rnnt 15.651043 hw_loss 0.059668 lr 0.00042805 rank 4
2023-02-21 15:04:44,084 DEBUG TRAIN Batch 16/3000 loss 20.017284 loss_att 22.297733 loss_ctc 26.854418 loss_rnnt 18.615791 hw_loss 0.063349 lr 0.00042803 rank 6
2023-02-21 15:04:44,085 DEBUG TRAIN Batch 16/3000 loss 10.641643 loss_att 15.543557 loss_ctc 15.837059 loss_rnnt 8.906748 hw_loss 0.115855 lr 0.00042797 rank 7
2023-02-21 15:04:44,085 DEBUG TRAIN Batch 16/3000 loss 5.243077 loss_att 9.622122 loss_ctc 8.117146 loss_rnnt 3.836251 hw_loss 0.277140 lr 0.00042805 rank 2
2023-02-21 15:04:44,091 DEBUG TRAIN Batch 16/3000 loss 11.990265 loss_att 15.674072 loss_ctc 15.152135 loss_rnnt 10.735927 hw_loss 0.179991 lr 0.00042798 rank 5
2023-02-21 15:04:44,130 DEBUG TRAIN Batch 16/3000 loss 12.751267 loss_att 14.894910 loss_ctc 17.308048 loss_rnnt 11.661631 hw_loss 0.100007 lr 0.00042805 rank 0
2023-02-21 15:06:02,387 DEBUG TRAIN Batch 16/3100 loss 14.923697 loss_att 16.997013 loss_ctc 22.905354 loss_rnnt 13.333309 hw_loss 0.209069 lr 0.00042789 rank 4
2023-02-21 15:06:02,388 DEBUG TRAIN Batch 16/3100 loss 11.954385 loss_att 12.966849 loss_ctc 11.993441 loss_rnnt 11.684729 hw_loss 0.116169 lr 0.00042781 rank 7
2023-02-21 15:06:02,391 DEBUG TRAIN Batch 16/3100 loss 8.794164 loss_att 10.010701 loss_ctc 11.382274 loss_rnnt 8.133764 hw_loss 0.135018 lr 0.00042787 rank 6
2023-02-21 15:06:02,394 DEBUG TRAIN Batch 16/3100 loss 10.404370 loss_att 10.939928 loss_ctc 14.956580 loss_rnnt 9.631151 hw_loss 0.110897 lr 0.00042790 rank 2
2023-02-21 15:06:02,395 DEBUG TRAIN Batch 16/3100 loss 8.020127 loss_att 11.752042 loss_ctc 9.673438 loss_rnnt 6.952274 hw_loss 0.189429 lr 0.00042790 rank 1
2023-02-21 15:06:02,396 DEBUG TRAIN Batch 16/3100 loss 10.930295 loss_att 14.274668 loss_ctc 13.863035 loss_rnnt 9.804804 hw_loss 0.122972 lr 0.00042789 rank 0
2023-02-21 15:06:02,400 DEBUG TRAIN Batch 16/3100 loss 4.715363 loss_att 6.557306 loss_ctc 6.104254 loss_rnnt 4.107212 hw_loss 0.102331 lr 0.00042782 rank 5
2023-02-21 15:06:02,403 DEBUG TRAIN Batch 16/3100 loss 11.920212 loss_att 13.703778 loss_ctc 12.622398 loss_rnnt 11.393229 hw_loss 0.143708 lr 0.00042790 rank 3
2023-02-21 15:07:24,097 DEBUG TRAIN Batch 16/3200 loss 11.605694 loss_att 11.846083 loss_ctc 14.319748 loss_rnnt 11.097377 hw_loss 0.184436 lr 0.00042774 rank 4
2023-02-21 15:07:24,098 DEBUG TRAIN Batch 16/3200 loss 13.369475 loss_att 17.124729 loss_ctc 17.478296 loss_rnnt 12.022814 hw_loss 0.089565 lr 0.00042774 rank 1
2023-02-21 15:07:24,100 DEBUG TRAIN Batch 16/3200 loss 8.567239 loss_att 7.955158 loss_ctc 11.023484 loss_rnnt 8.261116 hw_loss 0.189450 lr 0.00042773 rank 0
2023-02-21 15:07:24,101 DEBUG TRAIN Batch 16/3200 loss 5.814813 loss_att 6.963880 loss_ctc 6.941771 loss_rnnt 5.316418 hw_loss 0.221849 lr 0.00042765 rank 7
2023-02-21 15:07:24,102 DEBUG TRAIN Batch 16/3200 loss 6.654188 loss_att 8.696016 loss_ctc 9.950519 loss_rnnt 5.729133 hw_loss 0.144711 lr 0.00042774 rank 2
2023-02-21 15:07:24,105 DEBUG TRAIN Batch 16/3200 loss 13.531446 loss_att 17.020805 loss_ctc 20.198017 loss_rnnt 11.817652 hw_loss 0.238210 lr 0.00042775 rank 3
2023-02-21 15:07:24,130 DEBUG TRAIN Batch 16/3200 loss 3.074272 loss_att 6.704895 loss_ctc 5.127164 loss_rnnt 1.865933 hw_loss 0.390931 lr 0.00042766 rank 5
2023-02-21 15:07:24,141 DEBUG TRAIN Batch 16/3200 loss 8.006327 loss_att 12.162040 loss_ctc 11.446369 loss_rnnt 6.631968 hw_loss 0.158518 lr 0.00042771 rank 6
2023-02-21 15:08:41,893 DEBUG TRAIN Batch 16/3300 loss 5.981541 loss_att 6.700094 loss_ctc 6.401050 loss_rnnt 5.695617 hw_loss 0.161774 lr 0.00042758 rank 2
2023-02-21 15:08:41,897 DEBUG TRAIN Batch 16/3300 loss 10.206826 loss_att 13.056002 loss_ctc 14.363863 loss_rnnt 9.003710 hw_loss 0.148141 lr 0.00042756 rank 6
2023-02-21 15:08:41,897 DEBUG TRAIN Batch 16/3300 loss 9.429256 loss_att 11.750216 loss_ctc 10.588606 loss_rnnt 8.678217 hw_loss 0.248001 lr 0.00042758 rank 4
2023-02-21 15:08:41,898 DEBUG TRAIN Batch 16/3300 loss 16.711119 loss_att 18.001263 loss_ctc 23.150198 loss_rnnt 15.571773 hw_loss 0.042698 lr 0.00042750 rank 7
2023-02-21 15:08:41,899 DEBUG TRAIN Batch 16/3300 loss 5.052071 loss_att 9.113386 loss_ctc 8.485832 loss_rnnt 3.721069 hw_loss 0.114194 lr 0.00042759 rank 1
2023-02-21 15:08:41,901 DEBUG TRAIN Batch 16/3300 loss 4.396073 loss_att 8.719886 loss_ctc 7.954589 loss_rnnt 2.989885 hw_loss 0.125544 lr 0.00042758 rank 0
2023-02-21 15:08:41,907 DEBUG TRAIN Batch 16/3300 loss 17.321333 loss_att 26.387348 loss_ctc 25.414236 loss_rnnt 14.406123 hw_loss 0.043032 lr 0.00042751 rank 5
2023-02-21 15:08:41,951 DEBUG TRAIN Batch 16/3300 loss 28.530399 loss_att 34.480293 loss_ctc 38.319195 loss_rnnt 25.986351 hw_loss 0.091681 lr 0.00042759 rank 3
2023-02-21 15:09:59,977 DEBUG TRAIN Batch 16/3400 loss 10.762200 loss_att 13.645124 loss_ctc 14.911369 loss_rnnt 9.532687 hw_loss 0.186951 lr 0.00042740 rank 6
2023-02-21 15:09:59,982 DEBUG TRAIN Batch 16/3400 loss 12.020077 loss_att 18.187843 loss_ctc 16.156448 loss_rnnt 10.220327 hw_loss 0.027523 lr 0.00042734 rank 7
2023-02-21 15:09:59,984 DEBUG TRAIN Batch 16/3400 loss 9.889767 loss_att 11.892114 loss_ctc 16.057171 loss_rnnt 8.550282 hw_loss 0.218805 lr 0.00042743 rank 2
2023-02-21 15:09:59,986 DEBUG TRAIN Batch 16/3400 loss 3.435461 loss_att 8.539749 loss_ctc 7.516178 loss_rnnt 1.817487 hw_loss 0.099413 lr 0.00042742 rank 0
2023-02-21 15:09:59,986 DEBUG TRAIN Batch 16/3400 loss 11.849979 loss_att 15.600612 loss_ctc 18.063496 loss_rnnt 10.229729 hw_loss 0.078104 lr 0.00042743 rank 1
2023-02-21 15:09:59,986 DEBUG TRAIN Batch 16/3400 loss 8.562426 loss_att 10.080006 loss_ctc 13.473541 loss_rnnt 7.554711 hw_loss 0.092592 lr 0.00042742 rank 4
2023-02-21 15:09:59,989 DEBUG TRAIN Batch 16/3400 loss 9.830910 loss_att 11.325471 loss_ctc 14.332479 loss_rnnt 8.879237 hw_loss 0.098534 lr 0.00042735 rank 5
2023-02-21 15:10:00,037 DEBUG TRAIN Batch 16/3400 loss 8.808189 loss_att 9.852794 loss_ctc 12.071864 loss_rnnt 7.999792 hw_loss 0.308098 lr 0.00042743 rank 3
2023-02-21 15:11:19,829 DEBUG TRAIN Batch 16/3500 loss 8.281252 loss_att 12.034400 loss_ctc 11.797585 loss_rnnt 7.007300 hw_loss 0.102145 lr 0.00042718 rank 7
2023-02-21 15:11:19,834 DEBUG TRAIN Batch 16/3500 loss 12.961192 loss_att 18.650322 loss_ctc 18.567968 loss_rnnt 11.024925 hw_loss 0.095382 lr 0.00042727 rank 4
2023-02-21 15:11:19,838 DEBUG TRAIN Batch 16/3500 loss 15.954987 loss_att 17.061344 loss_ctc 16.902550 loss_rnnt 15.514643 hw_loss 0.173869 lr 0.00042727 rank 1
2023-02-21 15:11:19,840 DEBUG TRAIN Batch 16/3500 loss 14.603569 loss_att 14.448187 loss_ctc 16.268097 loss_rnnt 14.338503 hw_loss 0.139132 lr 0.00042727 rank 2
2023-02-21 15:11:19,842 DEBUG TRAIN Batch 16/3500 loss 6.608378 loss_att 9.923942 loss_ctc 9.040295 loss_rnnt 5.581989 hw_loss 0.073165 lr 0.00042720 rank 5
2023-02-21 15:11:19,856 DEBUG TRAIN Batch 16/3500 loss 7.742413 loss_att 10.824646 loss_ctc 12.872935 loss_rnnt 6.395336 hw_loss 0.087302 lr 0.00042726 rank 0
2023-02-21 15:11:19,866 DEBUG TRAIN Batch 16/3500 loss 5.973470 loss_att 8.458391 loss_ctc 8.327916 loss_rnnt 5.122028 hw_loss 0.075995 lr 0.00042728 rank 3
2023-02-21 15:11:19,887 DEBUG TRAIN Batch 16/3500 loss 17.643351 loss_att 22.146078 loss_ctc 25.009439 loss_rnnt 15.714815 hw_loss 0.085959 lr 0.00042724 rank 6
2023-02-21 15:12:39,570 DEBUG TRAIN Batch 16/3600 loss 6.695789 loss_att 8.582828 loss_ctc 7.741713 loss_rnnt 6.160388 hw_loss 0.034757 lr 0.00042711 rank 4
2023-02-21 15:12:39,580 DEBUG TRAIN Batch 16/3600 loss 23.190750 loss_att 24.695223 loss_ctc 26.986526 loss_rnnt 22.349831 hw_loss 0.063605 lr 0.00042703 rank 7
2023-02-21 15:12:39,581 DEBUG TRAIN Batch 16/3600 loss 5.889442 loss_att 7.582079 loss_ctc 7.310431 loss_rnnt 5.280534 hw_loss 0.151716 lr 0.00042712 rank 1
2023-02-21 15:12:39,581 DEBUG TRAIN Batch 16/3600 loss 8.764866 loss_att 12.247377 loss_ctc 13.202160 loss_rnnt 7.428003 hw_loss 0.091353 lr 0.00042709 rank 6
2023-02-21 15:12:39,581 DEBUG TRAIN Batch 16/3600 loss 13.789997 loss_att 16.030209 loss_ctc 21.266346 loss_rnnt 12.268710 hw_loss 0.143247 lr 0.00042712 rank 2
2023-02-21 15:12:39,585 DEBUG TRAIN Batch 16/3600 loss 13.571722 loss_att 19.473312 loss_ctc 18.206217 loss_rnnt 11.754425 hw_loss 0.035713 lr 0.00042712 rank 3
2023-02-21 15:12:39,585 DEBUG TRAIN Batch 16/3600 loss 7.061296 loss_att 11.074895 loss_ctc 11.801498 loss_rnnt 5.560264 hw_loss 0.124284 lr 0.00042704 rank 5
2023-02-21 15:12:39,586 DEBUG TRAIN Batch 16/3600 loss 10.199548 loss_att 13.388063 loss_ctc 13.531899 loss_rnnt 9.035346 hw_loss 0.154097 lr 0.00042711 rank 0
2023-02-21 15:13:59,389 DEBUG TRAIN Batch 16/3700 loss 8.754683 loss_att 12.337551 loss_ctc 17.075733 loss_rnnt 6.798651 hw_loss 0.243720 lr 0.00042696 rank 2
2023-02-21 15:13:59,391 DEBUG TRAIN Batch 16/3700 loss 17.248594 loss_att 17.570175 loss_ctc 21.500816 loss_rnnt 16.555277 hw_loss 0.116321 lr 0.00042687 rank 7
2023-02-21 15:13:59,394 DEBUG TRAIN Batch 16/3700 loss 11.385549 loss_att 14.402681 loss_ctc 14.715989 loss_rnnt 10.217763 hw_loss 0.225562 lr 0.00042695 rank 0
2023-02-21 15:13:59,395 DEBUG TRAIN Batch 16/3700 loss 7.148438 loss_att 9.450096 loss_ctc 7.913707 loss_rnnt 6.491343 hw_loss 0.177613 lr 0.00042696 rank 1
2023-02-21 15:13:59,396 DEBUG TRAIN Batch 16/3700 loss 8.995459 loss_att 10.586600 loss_ctc 14.685098 loss_rnnt 7.835032 hw_loss 0.156713 lr 0.00042696 rank 4
2023-02-21 15:13:59,402 DEBUG TRAIN Batch 16/3700 loss 8.532534 loss_att 11.931561 loss_ctc 10.348653 loss_rnnt 7.518766 hw_loss 0.172150 lr 0.00042688 rank 5
2023-02-21 15:13:59,403 DEBUG TRAIN Batch 16/3700 loss 13.239616 loss_att 12.969460 loss_ctc 19.977427 loss_rnnt 12.328605 hw_loss 0.125003 lr 0.00042697 rank 3
2023-02-21 15:13:59,444 DEBUG TRAIN Batch 16/3700 loss 10.380210 loss_att 12.506066 loss_ctc 13.330554 loss_rnnt 9.403884 hw_loss 0.295831 lr 0.00042693 rank 6
2023-02-21 15:15:18,654 DEBUG TRAIN Batch 16/3800 loss 10.741158 loss_att 11.197380 loss_ctc 12.734100 loss_rnnt 10.289586 hw_loss 0.177375 lr 0.00042681 rank 2
2023-02-21 15:15:18,654 DEBUG TRAIN Batch 16/3800 loss 6.325862 loss_att 7.571480 loss_ctc 7.276340 loss_rnnt 5.863978 hw_loss 0.161307 lr 0.00042680 rank 4
2023-02-21 15:15:18,654 DEBUG TRAIN Batch 16/3800 loss 14.219563 loss_att 16.804338 loss_ctc 16.721907 loss_rnnt 13.213709 hw_loss 0.291100 lr 0.00042672 rank 7
2023-02-21 15:15:18,658 DEBUG TRAIN Batch 16/3800 loss 7.590187 loss_att 8.706140 loss_ctc 9.293559 loss_rnnt 7.023502 hw_loss 0.218207 lr 0.00042678 rank 6
2023-02-21 15:15:18,659 DEBUG TRAIN Batch 16/3800 loss 10.182667 loss_att 11.143566 loss_ctc 12.608710 loss_rnnt 9.587611 hw_loss 0.148883 lr 0.00042681 rank 1
2023-02-21 15:15:18,664 DEBUG TRAIN Batch 16/3800 loss 14.957489 loss_att 15.884958 loss_ctc 19.462013 loss_rnnt 14.109869 hw_loss 0.115356 lr 0.00042681 rank 3
2023-02-21 15:15:18,664 DEBUG TRAIN Batch 16/3800 loss 13.117133 loss_att 19.681391 loss_ctc 20.197899 loss_rnnt 10.799623 hw_loss 0.113541 lr 0.00042680 rank 0
2023-02-21 15:15:18,668 DEBUG TRAIN Batch 16/3800 loss 5.850248 loss_att 9.581458 loss_ctc 9.208490 loss_rnnt 4.571614 hw_loss 0.158675 lr 0.00042673 rank 5
2023-02-21 15:16:39,836 DEBUG TRAIN Batch 16/3900 loss 6.965339 loss_att 10.484354 loss_ctc 9.413802 loss_rnnt 5.898101 hw_loss 0.069324 lr 0.00042656 rank 7
2023-02-21 15:16:39,844 DEBUG TRAIN Batch 16/3900 loss 9.064033 loss_att 10.358893 loss_ctc 15.750917 loss_rnnt 7.807336 hw_loss 0.199010 lr 0.00042665 rank 2
2023-02-21 15:16:39,845 DEBUG TRAIN Batch 16/3900 loss 11.828089 loss_att 13.459131 loss_ctc 15.639633 loss_rnnt 10.957026 hw_loss 0.068718 lr 0.00042665 rank 1
2023-02-21 15:16:39,847 DEBUG TRAIN Batch 16/3900 loss 3.764732 loss_att 8.537797 loss_ctc 7.905488 loss_rnnt 2.207393 hw_loss 0.094922 lr 0.00042665 rank 4
2023-02-21 15:16:39,846 DEBUG TRAIN Batch 16/3900 loss 8.806372 loss_att 15.054772 loss_ctc 13.751350 loss_rnnt 6.811611 hw_loss 0.160782 lr 0.00042662 rank 6
2023-02-21 15:16:39,851 DEBUG TRAIN Batch 16/3900 loss 11.187609 loss_att 12.048450 loss_ctc 12.313856 loss_rnnt 10.695115 hw_loss 0.319048 lr 0.00042664 rank 0
2023-02-21 15:16:39,865 DEBUG TRAIN Batch 16/3900 loss 17.410423 loss_att 16.530697 loss_ctc 30.237837 loss_rnnt 15.799502 hw_loss 0.143518 lr 0.00042666 rank 3
2023-02-21 15:16:39,897 DEBUG TRAIN Batch 16/3900 loss 7.390623 loss_att 15.436265 loss_ctc 12.875427 loss_rnnt 4.943247 hw_loss 0.200511 lr 0.00042657 rank 5
2023-02-21 15:17:59,238 DEBUG TRAIN Batch 16/4000 loss 9.010444 loss_att 11.047914 loss_ctc 11.469385 loss_rnnt 8.194267 hw_loss 0.151545 lr 0.00042649 rank 4
2023-02-21 15:17:59,239 DEBUG TRAIN Batch 16/4000 loss 13.056899 loss_att 22.338306 loss_ctc 15.751919 loss_rnnt 10.773585 hw_loss 0.126932 lr 0.00042641 rank 7
2023-02-21 15:17:59,243 DEBUG TRAIN Batch 16/4000 loss 12.614462 loss_att 14.354680 loss_ctc 17.255974 loss_rnnt 11.631960 hw_loss 0.029229 lr 0.00042650 rank 3
2023-02-21 15:17:59,245 DEBUG TRAIN Batch 16/4000 loss 17.006186 loss_att 21.972969 loss_ctc 22.256666 loss_rnnt 15.259898 hw_loss 0.099123 lr 0.00042650 rank 1
2023-02-21 15:17:59,247 DEBUG TRAIN Batch 16/4000 loss 8.044754 loss_att 12.293590 loss_ctc 12.128273 loss_rnnt 6.607559 hw_loss 0.080546 lr 0.00042647 rank 6
2023-02-21 15:17:59,248 DEBUG TRAIN Batch 16/4000 loss 8.007233 loss_att 10.732858 loss_ctc 13.528532 loss_rnnt 6.619541 hw_loss 0.199488 lr 0.00042649 rank 2
2023-02-21 15:17:59,248 DEBUG TRAIN Batch 16/4000 loss 8.545581 loss_att 10.078367 loss_ctc 8.796366 loss_rnnt 8.133114 hw_loss 0.135884 lr 0.00042642 rank 5
2023-02-21 15:17:59,251 DEBUG TRAIN Batch 16/4000 loss 6.768423 loss_att 9.832678 loss_ctc 8.512579 loss_rnnt 5.866195 hw_loss 0.106544 lr 0.00042649 rank 0
2023-02-21 15:19:18,072 DEBUG TRAIN Batch 16/4100 loss 15.574041 loss_att 18.675358 loss_ctc 21.707047 loss_rnnt 14.100993 hw_loss 0.065719 lr 0.00042625 rank 7
2023-02-21 15:19:18,073 DEBUG TRAIN Batch 16/4100 loss 12.131495 loss_att 16.792728 loss_ctc 17.038115 loss_rnnt 10.495990 hw_loss 0.091954 lr 0.00042633 rank 4
2023-02-21 15:19:18,077 DEBUG TRAIN Batch 16/4100 loss 13.144580 loss_att 15.653708 loss_ctc 15.276813 loss_rnnt 12.333945 hw_loss 0.045959 lr 0.00042631 rank 6
2023-02-21 15:19:18,078 DEBUG TRAIN Batch 16/4100 loss 21.289095 loss_att 24.708979 loss_ctc 31.696438 loss_rnnt 19.132109 hw_loss 0.160062 lr 0.00042634 rank 2
2023-02-21 15:19:18,079 DEBUG TRAIN Batch 16/4100 loss 6.179597 loss_att 10.344439 loss_ctc 9.130829 loss_rnnt 4.937672 hw_loss 0.028985 lr 0.00042626 rank 5
2023-02-21 15:19:18,079 DEBUG TRAIN Batch 16/4100 loss 6.077101 loss_att 9.573694 loss_ctc 6.864923 loss_rnnt 5.224196 hw_loss 0.091017 lr 0.00042634 rank 1
2023-02-21 15:19:18,081 DEBUG TRAIN Batch 16/4100 loss 11.303873 loss_att 13.772586 loss_ctc 14.964046 loss_rnnt 10.237310 hw_loss 0.158998 lr 0.00042633 rank 0
2023-02-21 15:19:18,082 DEBUG TRAIN Batch 16/4100 loss 4.317222 loss_att 8.732179 loss_ctc 8.944029 loss_rnnt 2.766096 hw_loss 0.096051 lr 0.00042635 rank 3
2023-02-21 15:20:37,084 DEBUG TRAIN Batch 16/4200 loss 14.258389 loss_att 15.947268 loss_ctc 16.495541 loss_rnnt 13.558082 hw_loss 0.120462 lr 0.00042618 rank 2
2023-02-21 15:20:37,086 DEBUG TRAIN Batch 16/4200 loss 13.604517 loss_att 17.969248 loss_ctc 18.552229 loss_rnnt 12.038923 hw_loss 0.061789 lr 0.00042618 rank 0
2023-02-21 15:20:37,087 DEBUG TRAIN Batch 16/4200 loss 7.825878 loss_att 9.559313 loss_ctc 9.150578 loss_rnnt 7.242683 hw_loss 0.112278 lr 0.00042619 rank 1
2023-02-21 15:20:37,088 DEBUG TRAIN Batch 16/4200 loss 18.185549 loss_att 19.966116 loss_ctc 23.262878 loss_rnnt 17.057358 hw_loss 0.178313 lr 0.00042610 rank 7
2023-02-21 15:20:37,089 DEBUG TRAIN Batch 16/4200 loss 8.580624 loss_att 11.814577 loss_ctc 11.727369 loss_rnnt 7.498262 hw_loss 0.030008 lr 0.00042619 rank 3
2023-02-21 15:20:37,089 DEBUG TRAIN Batch 16/4200 loss 9.247127 loss_att 11.506579 loss_ctc 10.766378 loss_rnnt 8.520793 hw_loss 0.134768 lr 0.00042618 rank 4
2023-02-21 15:20:37,096 DEBUG TRAIN Batch 16/4200 loss 11.945338 loss_att 16.729584 loss_ctc 12.735201 loss_rnnt 10.825396 hw_loss 0.108333 lr 0.00042616 rank 6
2023-02-21 15:20:37,106 DEBUG TRAIN Batch 16/4200 loss 17.623117 loss_att 20.372177 loss_ctc 26.536768 loss_rnnt 15.843657 hw_loss 0.077181 lr 0.00042611 rank 5
2023-02-21 15:21:57,147 DEBUG TRAIN Batch 16/4300 loss 11.645513 loss_att 14.330367 loss_ctc 15.200952 loss_rnnt 10.565204 hw_loss 0.129900 lr 0.00042603 rank 1
2023-02-21 15:21:57,149 DEBUG TRAIN Batch 16/4300 loss 10.831198 loss_att 12.649391 loss_ctc 14.568952 loss_rnnt 9.876891 hw_loss 0.173064 lr 0.00042600 rank 6
2023-02-21 15:21:57,149 DEBUG TRAIN Batch 16/4300 loss 6.606077 loss_att 8.796326 loss_ctc 8.707923 loss_rnnt 5.776143 hw_loss 0.209321 lr 0.00042603 rank 4
2023-02-21 15:21:57,152 DEBUG TRAIN Batch 16/4300 loss 11.396210 loss_att 16.612118 loss_ctc 20.933662 loss_rnnt 9.013929 hw_loss 0.126447 lr 0.00042594 rank 7
2023-02-21 15:21:57,152 DEBUG TRAIN Batch 16/4300 loss 9.830014 loss_att 13.611939 loss_ctc 12.472029 loss_rnnt 8.655993 hw_loss 0.122564 lr 0.00042603 rank 2
2023-02-21 15:21:57,153 DEBUG TRAIN Batch 16/4300 loss 11.241576 loss_att 14.605852 loss_ctc 13.567963 loss_rnnt 10.224171 hw_loss 0.064435 lr 0.00042602 rank 0
2023-02-21 15:21:57,159 DEBUG TRAIN Batch 16/4300 loss 8.656259 loss_att 9.965074 loss_ctc 13.654942 loss_rnnt 7.589571 hw_loss 0.259562 lr 0.00042604 rank 3
2023-02-21 15:21:57,200 DEBUG TRAIN Batch 16/4300 loss 21.915070 loss_att 21.251678 loss_ctc 27.970955 loss_rnnt 21.221420 hw_loss 0.035390 lr 0.00042595 rank 5
2023-02-21 15:23:16,375 DEBUG TRAIN Batch 16/4400 loss 10.173244 loss_att 11.226824 loss_ctc 17.349674 loss_rnnt 8.894365 hw_loss 0.208694 lr 0.00042588 rank 2
2023-02-21 15:23:16,378 DEBUG TRAIN Batch 16/4400 loss 14.056536 loss_att 14.260899 loss_ctc 18.544140 loss_rnnt 13.252394 hw_loss 0.309229 lr 0.00042585 rank 6
2023-02-21 15:23:16,378 DEBUG TRAIN Batch 16/4400 loss 15.792373 loss_att 19.678453 loss_ctc 21.676687 loss_rnnt 14.187282 hw_loss 0.081186 lr 0.00042588 rank 1
2023-02-21 15:23:16,378 DEBUG TRAIN Batch 16/4400 loss 3.390577 loss_att 7.245102 loss_ctc 7.409905 loss_rnnt 2.038528 hw_loss 0.084813 lr 0.00042587 rank 0
2023-02-21 15:23:16,386 DEBUG TRAIN Batch 16/4400 loss 12.419127 loss_att 11.782219 loss_ctc 13.308404 loss_rnnt 12.320139 hw_loss 0.202123 lr 0.00042588 rank 3
2023-02-21 15:23:16,387 DEBUG TRAIN Batch 16/4400 loss 12.928396 loss_att 14.431696 loss_ctc 18.061306 loss_rnnt 11.846540 hw_loss 0.181518 lr 0.00042580 rank 5
2023-02-21 15:23:16,390 DEBUG TRAIN Batch 16/4400 loss 21.361303 loss_att 21.922798 loss_ctc 28.412832 loss_rnnt 20.275461 hw_loss 0.062507 lr 0.00042579 rank 7
2023-02-21 15:23:16,389 DEBUG TRAIN Batch 16/4400 loss 11.228229 loss_att 13.504976 loss_ctc 13.015288 loss_rnnt 10.494679 hw_loss 0.074859 lr 0.00042587 rank 4
2023-02-21 15:24:35,256 DEBUG TRAIN Batch 16/4500 loss 16.763863 loss_att 16.838394 loss_ctc 20.589420 loss_rnnt 16.224953 hw_loss 0.026115 lr 0.00042563 rank 7
2023-02-21 15:24:35,257 DEBUG TRAIN Batch 16/4500 loss 14.913832 loss_att 36.429340 loss_ctc 16.421103 loss_rnnt 10.395625 hw_loss 0.026504 lr 0.00042572 rank 1
2023-02-21 15:24:35,258 DEBUG TRAIN Batch 16/4500 loss 2.802377 loss_att 5.888440 loss_ctc 4.980455 loss_rnnt 1.831122 hw_loss 0.119311 lr 0.00042572 rank 2
2023-02-21 15:24:35,258 DEBUG TRAIN Batch 16/4500 loss 13.805058 loss_att 12.407579 loss_ctc 16.801249 loss_rnnt 13.548133 hw_loss 0.256739 lr 0.00042572 rank 4
2023-02-21 15:24:35,261 DEBUG TRAIN Batch 16/4500 loss 12.156380 loss_att 13.622657 loss_ctc 13.119189 loss_rnnt 11.627296 hw_loss 0.201473 lr 0.00042571 rank 0
2023-02-21 15:24:35,261 DEBUG TRAIN Batch 16/4500 loss 10.314456 loss_att 17.254736 loss_ctc 21.395916 loss_rnnt 7.434225 hw_loss 0.027463 lr 0.00042569 rank 6
2023-02-21 15:24:35,263 DEBUG TRAIN Batch 16/4500 loss 11.703400 loss_att 14.721744 loss_ctc 17.396870 loss_rnnt 10.275432 hw_loss 0.122195 lr 0.00042573 rank 3
2023-02-21 15:24:35,267 DEBUG TRAIN Batch 16/4500 loss 4.543174 loss_att 6.488249 loss_ctc 5.182300 loss_rnnt 3.953054 hw_loss 0.217290 lr 0.00042565 rank 5
2023-02-21 15:25:56,612 DEBUG TRAIN Batch 16/4600 loss 14.635320 loss_att 18.951864 loss_ctc 22.145987 loss_rnnt 12.754565 hw_loss 0.030042 lr 0.00042548 rank 7
2023-02-21 15:25:56,613 DEBUG TRAIN Batch 16/4600 loss 20.641590 loss_att 24.170259 loss_ctc 28.486404 loss_rnnt 18.821312 hw_loss 0.128567 lr 0.00042556 rank 0
2023-02-21 15:25:56,616 DEBUG TRAIN Batch 16/4600 loss 18.062201 loss_att 22.816013 loss_ctc 25.447386 loss_rnnt 16.100658 hw_loss 0.048912 lr 0.00042554 rank 6
2023-02-21 15:25:56,617 DEBUG TRAIN Batch 16/4600 loss 9.395382 loss_att 12.173681 loss_ctc 10.512501 loss_rnnt 8.627026 hw_loss 0.119525 lr 0.00042556 rank 4
2023-02-21 15:25:56,618 DEBUG TRAIN Batch 16/4600 loss 9.830488 loss_att 11.561064 loss_ctc 13.228027 loss_rnnt 8.893926 hw_loss 0.257705 lr 0.00042557 rank 1
2023-02-21 15:25:56,618 DEBUG TRAIN Batch 16/4600 loss 14.671343 loss_att 14.159246 loss_ctc 15.415655 loss_rnnt 14.616846 hw_loss 0.108137 lr 0.00042557 rank 2
2023-02-21 15:25:56,631 DEBUG TRAIN Batch 16/4600 loss 16.590818 loss_att 17.916080 loss_ctc 22.903503 loss_rnnt 15.414851 hw_loss 0.129796 lr 0.00042557 rank 3
2023-02-21 15:25:56,666 DEBUG TRAIN Batch 16/4600 loss 14.219412 loss_att 24.246902 loss_ctc 15.363976 loss_rnnt 12.003918 hw_loss 0.107600 lr 0.00042549 rank 5
2023-02-21 15:27:16,302 DEBUG TRAIN Batch 16/4700 loss 7.706273 loss_att 13.089121 loss_ctc 13.197066 loss_rnnt 5.798169 hw_loss 0.186426 lr 0.00042541 rank 2
2023-02-21 15:27:16,303 DEBUG TRAIN Batch 16/4700 loss 8.184005 loss_att 12.188084 loss_ctc 10.070549 loss_rnnt 7.053196 hw_loss 0.147101 lr 0.00042541 rank 1
2023-02-21 15:27:16,303 DEBUG TRAIN Batch 16/4700 loss 7.886418 loss_att 12.166465 loss_ctc 8.770238 loss_rnnt 6.784647 hw_loss 0.239850 lr 0.00042532 rank 7
2023-02-21 15:27:16,306 DEBUG TRAIN Batch 16/4700 loss 10.314579 loss_att 14.477837 loss_ctc 14.414050 loss_rnnt 8.834135 hw_loss 0.189743 lr 0.00042540 rank 0
2023-02-21 15:27:16,306 DEBUG TRAIN Batch 16/4700 loss 4.871970 loss_att 8.273308 loss_ctc 7.736351 loss_rnnt 3.795897 hw_loss 0.026041 lr 0.00042541 rank 4
2023-02-21 15:27:16,308 DEBUG TRAIN Batch 16/4700 loss 15.437676 loss_att 15.125236 loss_ctc 19.490452 loss_rnnt 14.934361 hw_loss 0.047688 lr 0.00042534 rank 5
2023-02-21 15:27:16,308 DEBUG TRAIN Batch 16/4700 loss 11.788485 loss_att 15.062836 loss_ctc 16.128342 loss_rnnt 10.532938 hw_loss 0.041304 lr 0.00042542 rank 3
2023-02-21 15:27:16,358 DEBUG TRAIN Batch 16/4700 loss 16.343630 loss_att 20.604534 loss_ctc 22.950380 loss_rnnt 14.506826 hw_loss 0.194479 lr 0.00042538 rank 6
2023-02-21 15:28:35,121 DEBUG TRAIN Batch 16/4800 loss 11.896239 loss_att 13.238176 loss_ctc 15.550313 loss_rnnt 11.021351 hw_loss 0.223670 lr 0.00042517 rank 7
2023-02-21 15:28:35,121 DEBUG TRAIN Batch 16/4800 loss 8.513593 loss_att 13.411861 loss_ctc 15.387247 loss_rnnt 6.492511 hw_loss 0.234263 lr 0.00042525 rank 4
2023-02-21 15:28:35,128 DEBUG TRAIN Batch 16/4800 loss 6.121147 loss_att 11.770785 loss_ctc 8.609324 loss_rnnt 4.558594 hw_loss 0.189129 lr 0.00042525 rank 0
2023-02-21 15:28:35,130 DEBUG TRAIN Batch 16/4800 loss 12.638197 loss_att 15.929749 loss_ctc 16.101269 loss_rnnt 11.457014 hw_loss 0.114620 lr 0.00042526 rank 1
2023-02-21 15:28:35,131 DEBUG TRAIN Batch 16/4800 loss 8.342585 loss_att 11.701867 loss_ctc 11.171162 loss_rnnt 7.220585 hw_loss 0.136873 lr 0.00042518 rank 5
2023-02-21 15:28:35,133 DEBUG TRAIN Batch 16/4800 loss 13.192535 loss_att 15.960211 loss_ctc 16.540716 loss_rnnt 12.136733 hw_loss 0.104707 lr 0.00042526 rank 2
2023-02-21 15:28:35,133 DEBUG TRAIN Batch 16/4800 loss 6.508934 loss_att 10.316065 loss_ctc 10.219146 loss_rnnt 5.195081 hw_loss 0.108248 lr 0.00042526 rank 3
2023-02-21 15:28:35,168 DEBUG TRAIN Batch 16/4800 loss 20.394024 loss_att 21.027580 loss_ctc 24.634544 loss_rnnt 19.608692 hw_loss 0.174785 lr 0.00042523 rank 6
2023-02-21 15:29:54,781 DEBUG TRAIN Batch 16/4900 loss 6.266512 loss_att 8.690574 loss_ctc 10.318703 loss_rnnt 5.205704 hw_loss 0.066944 lr 0.00042502 rank 7
2023-02-21 15:29:54,785 DEBUG TRAIN Batch 16/4900 loss 6.094371 loss_att 8.675568 loss_ctc 9.016476 loss_rnnt 5.116271 hw_loss 0.135463 lr 0.00042511 rank 1
2023-02-21 15:29:54,786 DEBUG TRAIN Batch 16/4900 loss 11.438497 loss_att 14.757736 loss_ctc 15.177191 loss_rnnt 10.229215 hw_loss 0.088014 lr 0.00042510 rank 2
2023-02-21 15:29:54,789 DEBUG TRAIN Batch 16/4900 loss 5.408199 loss_att 8.584869 loss_ctc 7.998653 loss_rnnt 4.339744 hw_loss 0.164490 lr 0.00042510 rank 4
2023-02-21 15:29:54,793 DEBUG TRAIN Batch 16/4900 loss 6.484883 loss_att 8.773053 loss_ctc 9.672361 loss_rnnt 5.522244 hw_loss 0.150015 lr 0.00042511 rank 3
2023-02-21 15:29:54,794 DEBUG TRAIN Batch 16/4900 loss 10.514696 loss_att 20.815409 loss_ctc 9.493352 loss_rnnt 8.491343 hw_loss 0.186358 lr 0.00042510 rank 0
2023-02-21 15:29:54,843 DEBUG TRAIN Batch 16/4900 loss 5.724532 loss_att 8.319782 loss_ctc 7.001179 loss_rnnt 4.968157 hw_loss 0.125823 lr 0.00042503 rank 5
2023-02-21 15:29:55,114 DEBUG TRAIN Batch 16/4900 loss 13.734315 loss_att 20.945312 loss_ctc 23.657986 loss_rnnt 10.905880 hw_loss 0.118273 lr 0.00042508 rank 6
2023-02-21 15:31:15,950 DEBUG TRAIN Batch 16/5000 loss 11.508949 loss_att 13.120030 loss_ctc 15.415894 loss_rnnt 10.597595 hw_loss 0.127898 lr 0.00042486 rank 7
2023-02-21 15:31:15,951 DEBUG TRAIN Batch 16/5000 loss 7.596520 loss_att 11.688448 loss_ctc 11.547775 loss_rnnt 6.201993 hw_loss 0.092451 lr 0.00042495 rank 1
2023-02-21 15:31:15,952 DEBUG TRAIN Batch 16/5000 loss 20.018774 loss_att 20.157558 loss_ctc 28.021349 loss_rnnt 18.862373 hw_loss 0.115563 lr 0.00042495 rank 4
2023-02-21 15:31:15,953 DEBUG TRAIN Batch 16/5000 loss 11.974113 loss_att 11.135225 loss_ctc 13.451671 loss_rnnt 11.883688 hw_loss 0.114742 lr 0.00042495 rank 2
2023-02-21 15:31:15,955 DEBUG TRAIN Batch 16/5000 loss 20.537254 loss_att 22.107817 loss_ctc 26.973694 loss_rnnt 19.265219 hw_loss 0.186996 lr 0.00042492 rank 6
2023-02-21 15:31:15,958 DEBUG TRAIN Batch 16/5000 loss 11.475134 loss_att 11.392811 loss_ctc 15.267797 loss_rnnt 10.883807 hw_loss 0.191446 lr 0.00042496 rank 3
2023-02-21 15:31:15,959 DEBUG TRAIN Batch 16/5000 loss 6.382083 loss_att 8.169210 loss_ctc 8.493424 loss_rnnt 5.714417 hw_loss 0.053865 lr 0.00042494 rank 0
2023-02-21 15:31:15,964 DEBUG TRAIN Batch 16/5000 loss 7.192808 loss_att 9.660993 loss_ctc 11.515005 loss_rnnt 6.083617 hw_loss 0.073615 lr 0.00042488 rank 5
2023-02-21 15:32:35,665 DEBUG TRAIN Batch 16/5100 loss 10.035458 loss_att 10.585817 loss_ctc 13.405242 loss_rnnt 9.322120 hw_loss 0.288681 lr 0.00042480 rank 1
2023-02-21 15:32:35,666 DEBUG TRAIN Batch 16/5100 loss 6.560779 loss_att 10.339120 loss_ctc 6.152359 loss_rnnt 5.778039 hw_loss 0.152863 lr 0.00042480 rank 2
2023-02-21 15:32:35,667 DEBUG TRAIN Batch 16/5100 loss 7.803948 loss_att 7.754038 loss_ctc 11.793337 loss_rnnt 7.198998 hw_loss 0.155648 lr 0.00042479 rank 4
2023-02-21 15:32:35,669 DEBUG TRAIN Batch 16/5100 loss 29.325369 loss_att 29.004015 loss_ctc 31.455734 loss_rnnt 29.067001 hw_loss 0.072358 lr 0.00042471 rank 7
2023-02-21 15:32:35,670 DEBUG TRAIN Batch 16/5100 loss 19.308304 loss_att 18.762388 loss_ctc 21.465559 loss_rnnt 18.995041 hw_loss 0.252770 lr 0.00042479 rank 0
2023-02-21 15:32:35,671 DEBUG TRAIN Batch 16/5100 loss 8.421421 loss_att 9.603614 loss_ctc 10.524288 loss_rnnt 7.814584 hw_loss 0.168781 lr 0.00042477 rank 6
2023-02-21 15:32:35,675 DEBUG TRAIN Batch 16/5100 loss 5.626316 loss_att 9.909767 loss_ctc 8.728095 loss_rnnt 4.255675 hw_loss 0.188212 lr 0.00042480 rank 3
2023-02-21 15:32:35,719 DEBUG TRAIN Batch 16/5100 loss 12.685915 loss_att 12.946011 loss_ctc 12.908892 loss_rnnt 12.478815 hw_loss 0.235033 lr 0.00042472 rank 5
2023-02-21 15:33:54,263 DEBUG TRAIN Batch 16/5200 loss 8.400043 loss_att 11.587191 loss_ctc 12.346342 loss_rnnt 7.101639 hw_loss 0.252749 lr 0.00042465 rank 1
2023-02-21 15:33:54,264 DEBUG TRAIN Batch 16/5200 loss 7.110360 loss_att 11.088614 loss_ctc 8.522854 loss_rnnt 6.082691 hw_loss 0.081911 lr 0.00042456 rank 7
2023-02-21 15:33:54,264 DEBUG TRAIN Batch 16/5200 loss 8.632991 loss_att 13.057775 loss_ctc 11.825012 loss_rnnt 7.198261 hw_loss 0.232821 lr 0.00042464 rank 4
2023-02-21 15:33:54,265 DEBUG TRAIN Batch 16/5200 loss 7.872483 loss_att 9.741413 loss_ctc 10.161671 loss_rnnt 7.098301 hw_loss 0.178444 lr 0.00042462 rank 6
2023-02-21 15:33:54,265 DEBUG TRAIN Batch 16/5200 loss 4.771834 loss_att 7.276189 loss_ctc 5.659301 loss_rnnt 4.049438 hw_loss 0.193492 lr 0.00042464 rank 2
2023-02-21 15:33:54,267 DEBUG TRAIN Batch 16/5200 loss 9.509266 loss_att 9.609571 loss_ctc 11.796857 loss_rnnt 9.086400 hw_loss 0.183362 lr 0.00042464 rank 0
2023-02-21 15:33:54,269 DEBUG TRAIN Batch 16/5200 loss 9.182684 loss_att 12.440958 loss_ctc 11.169765 loss_rnnt 8.222638 hw_loss 0.081460 lr 0.00042465 rank 3
2023-02-21 15:33:54,270 DEBUG TRAIN Batch 16/5200 loss 6.598843 loss_att 11.108085 loss_ctc 12.112118 loss_rnnt 4.947020 hw_loss 0.027885 lr 0.00042457 rank 5
2023-02-21 15:35:15,097 DEBUG TRAIN Batch 16/5300 loss 6.678787 loss_att 10.227146 loss_ctc 9.170752 loss_rnnt 5.621535 hw_loss 0.028722 lr 0.00042446 rank 6
2023-02-21 15:35:15,105 DEBUG TRAIN Batch 16/5300 loss 12.539168 loss_att 15.977869 loss_ctc 18.430950 loss_rnnt 11.051258 hw_loss 0.027373 lr 0.00042449 rank 4
2023-02-21 15:35:15,105 DEBUG TRAIN Batch 16/5300 loss 16.551311 loss_att 21.302750 loss_ctc 27.855560 loss_rnnt 14.039931 hw_loss 0.100986 lr 0.00042449 rank 1
2023-02-21 15:35:15,110 DEBUG TRAIN Batch 16/5300 loss 22.600056 loss_att 22.768799 loss_ctc 29.610683 loss_rnnt 21.572943 hw_loss 0.109900 lr 0.00042450 rank 3
2023-02-21 15:35:15,110 DEBUG TRAIN Batch 16/5300 loss 16.714703 loss_att 17.265968 loss_ctc 22.839630 loss_rnnt 15.737122 hw_loss 0.095007 lr 0.00042440 rank 7
2023-02-21 15:35:15,111 DEBUG TRAIN Batch 16/5300 loss 13.737467 loss_att 18.299999 loss_ctc 24.364174 loss_rnnt 11.378109 hw_loss 0.056168 lr 0.00042442 rank 5
2023-02-21 15:35:15,111 DEBUG TRAIN Batch 16/5300 loss 15.112643 loss_att 18.653502 loss_ctc 18.934401 loss_rnnt 13.824454 hw_loss 0.132093 lr 0.00042449 rank 2
2023-02-21 15:35:15,113 DEBUG TRAIN Batch 16/5300 loss 1.843238 loss_att 4.905071 loss_ctc 1.530282 loss_rnnt 1.196896 hw_loss 0.141943 lr 0.00042448 rank 0
2023-02-21 15:36:35,180 DEBUG TRAIN Batch 16/5400 loss 11.178472 loss_att 13.830906 loss_ctc 10.454008 loss_rnnt 10.642652 hw_loss 0.191115 lr 0.00042434 rank 1
2023-02-21 15:36:35,186 DEBUG TRAIN Batch 16/5400 loss 10.899337 loss_att 12.993399 loss_ctc 14.288553 loss_rnnt 9.950068 hw_loss 0.147301 lr 0.00042425 rank 7
2023-02-21 15:36:35,188 DEBUG TRAIN Batch 16/5400 loss 8.504577 loss_att 9.621655 loss_ctc 9.089355 loss_rnnt 8.089039 hw_loss 0.214034 lr 0.00042426 rank 5
2023-02-21 15:36:35,188 DEBUG TRAIN Batch 16/5400 loss 16.746855 loss_att 19.645689 loss_ctc 21.568653 loss_rnnt 15.484020 hw_loss 0.075302 lr 0.00042434 rank 2
2023-02-21 15:36:35,190 DEBUG TRAIN Batch 16/5400 loss 4.079791 loss_att 7.678939 loss_ctc 8.765335 loss_rnnt 2.649536 hw_loss 0.160661 lr 0.00042431 rank 6
2023-02-21 15:36:35,190 DEBUG TRAIN Batch 16/5400 loss 7.768816 loss_att 11.455592 loss_ctc 11.491098 loss_rnnt 6.487491 hw_loss 0.089374 lr 0.00042434 rank 3
2023-02-21 15:36:35,191 DEBUG TRAIN Batch 16/5400 loss 17.531391 loss_att 21.804916 loss_ctc 21.698170 loss_rnnt 16.002375 hw_loss 0.222633 lr 0.00042433 rank 4
2023-02-21 15:36:35,235 DEBUG TRAIN Batch 16/5400 loss 11.534682 loss_att 12.677056 loss_ctc 11.879440 loss_rnnt 11.189742 hw_loss 0.132183 lr 0.00042433 rank 0
2023-02-21 15:37:54,655 DEBUG TRAIN Batch 16/5500 loss 12.325929 loss_att 15.338211 loss_ctc 18.262684 loss_rnnt 10.812998 hw_loss 0.222950 lr 0.00042419 rank 2
2023-02-21 15:37:54,657 DEBUG TRAIN Batch 16/5500 loss 8.450210 loss_att 10.461605 loss_ctc 10.637885 loss_rnnt 7.671656 hw_loss 0.158597 lr 0.00042410 rank 7
2023-02-21 15:37:54,658 DEBUG TRAIN Batch 16/5500 loss 4.672398 loss_att 8.873997 loss_ctc 8.948402 loss_rnnt 3.193962 hw_loss 0.127469 lr 0.00042418 rank 4
2023-02-21 15:37:54,658 DEBUG TRAIN Batch 16/5500 loss 9.857557 loss_att 14.502527 loss_ctc 16.023205 loss_rnnt 8.074001 hw_loss 0.060893 lr 0.00042416 rank 6
2023-02-21 15:37:54,659 DEBUG TRAIN Batch 16/5500 loss 10.155292 loss_att 11.365529 loss_ctc 11.779040 loss_rnnt 9.634729 hw_loss 0.116279 lr 0.00042418 rank 0
2023-02-21 15:37:54,659 DEBUG TRAIN Batch 16/5500 loss 11.730438 loss_att 15.037460 loss_ctc 11.903204 loss_rnnt 10.979494 hw_loss 0.124696 lr 0.00042419 rank 1
2023-02-21 15:37:54,661 DEBUG TRAIN Batch 16/5500 loss 7.999089 loss_att 9.940429 loss_ctc 10.216407 loss_rnnt 7.279423 hw_loss 0.067042 lr 0.00042411 rank 5
2023-02-21 15:37:54,666 DEBUG TRAIN Batch 16/5500 loss 10.571480 loss_att 13.978790 loss_ctc 15.946481 loss_rnnt 9.126432 hw_loss 0.087971 lr 0.00042419 rank 3
2023-02-21 15:39:12,882 DEBUG TRAIN Batch 16/5600 loss 8.597708 loss_att 12.217661 loss_ctc 12.742527 loss_rnnt 7.204597 hw_loss 0.218397 lr 0.00042403 rank 4
2023-02-21 15:39:12,883 DEBUG TRAIN Batch 16/5600 loss 10.924685 loss_att 14.921354 loss_ctc 15.985173 loss_rnnt 9.355234 hw_loss 0.178849 lr 0.00042395 rank 7
2023-02-21 15:39:12,886 DEBUG TRAIN Batch 16/5600 loss 12.220161 loss_att 13.029325 loss_ctc 13.237172 loss_rnnt 11.862582 hw_loss 0.112771 lr 0.00042403 rank 2
2023-02-21 15:39:12,888 DEBUG TRAIN Batch 16/5600 loss 18.661030 loss_att 21.760263 loss_ctc 30.014002 loss_rnnt 16.502003 hw_loss 0.047720 lr 0.00042403 rank 1
2023-02-21 15:39:12,888 DEBUG TRAIN Batch 16/5600 loss 13.119375 loss_att 17.308901 loss_ctc 16.279240 loss_rnnt 11.833717 hw_loss 0.049570 lr 0.00042401 rank 6
2023-02-21 15:39:12,893 DEBUG TRAIN Batch 16/5600 loss 9.122985 loss_att 9.765596 loss_ctc 13.061915 loss_rnnt 8.410887 hw_loss 0.109472 lr 0.00042404 rank 3
2023-02-21 15:39:12,896 DEBUG TRAIN Batch 16/5600 loss 12.282706 loss_att 15.415456 loss_ctc 14.153111 loss_rnnt 11.352236 hw_loss 0.102248 lr 0.00042396 rank 5
2023-02-21 15:39:12,935 DEBUG TRAIN Batch 16/5600 loss 9.213199 loss_att 12.002140 loss_ctc 14.804527 loss_rnnt 7.832390 hw_loss 0.145332 lr 0.00042402 rank 0
2023-02-21 15:40:34,925 DEBUG TRAIN Batch 16/5700 loss 6.304701 loss_att 9.240481 loss_ctc 10.773902 loss_rnnt 5.049993 hw_loss 0.134361 lr 0.00042388 rank 4
2023-02-21 15:40:34,927 DEBUG TRAIN Batch 16/5700 loss 4.874605 loss_att 6.880128 loss_ctc 7.975792 loss_rnnt 3.999863 hw_loss 0.112773 lr 0.00042388 rank 1
2023-02-21 15:40:34,928 DEBUG TRAIN Batch 16/5700 loss 13.759722 loss_att 13.446838 loss_ctc 17.339695 loss_rnnt 13.289207 hw_loss 0.104553 lr 0.00042379 rank 7
2023-02-21 15:40:34,929 DEBUG TRAIN Batch 16/5700 loss 3.228250 loss_att 7.104380 loss_ctc 3.729122 loss_rnnt 2.336744 hw_loss 0.092807 lr 0.00042388 rank 2
2023-02-21 15:40:34,931 DEBUG TRAIN Batch 16/5700 loss 12.268524 loss_att 14.634291 loss_ctc 16.002567 loss_rnnt 11.278233 hw_loss 0.036123 lr 0.00042389 rank 3
2023-02-21 15:40:34,935 DEBUG TRAIN Batch 16/5700 loss 10.637603 loss_att 12.465199 loss_ctc 12.651966 loss_rnnt 9.908396 hw_loss 0.178326 lr 0.00042385 rank 6
2023-02-21 15:40:34,935 DEBUG TRAIN Batch 16/5700 loss 10.763845 loss_att 12.143023 loss_ctc 13.281497 loss_rnnt 10.028334 hw_loss 0.232482 lr 0.00042381 rank 5
2023-02-21 15:40:34,977 DEBUG TRAIN Batch 16/5700 loss 10.858280 loss_att 12.927640 loss_ctc 15.679214 loss_rnnt 9.730347 hw_loss 0.133630 lr 0.00042387 rank 0
2023-02-21 15:41:52,211 DEBUG TRAIN Batch 16/5800 loss 12.780767 loss_att 13.249720 loss_ctc 14.124861 loss_rnnt 12.480093 hw_loss 0.051885 lr 0.00042373 rank 2
2023-02-21 15:41:52,212 DEBUG TRAIN Batch 16/5800 loss 10.938325 loss_att 15.225988 loss_ctc 18.068764 loss_rnnt 9.035525 hw_loss 0.177268 lr 0.00042372 rank 4
2023-02-21 15:41:52,215 DEBUG TRAIN Batch 16/5800 loss 13.385605 loss_att 18.402699 loss_ctc 18.201359 loss_rnnt 11.725985 hw_loss 0.026439 lr 0.00042365 rank 5
2023-02-21 15:41:52,216 DEBUG TRAIN Batch 16/5800 loss 7.986057 loss_att 9.763971 loss_ctc 10.283335 loss_rnnt 7.292338 hw_loss 0.059684 lr 0.00042372 rank 0
2023-02-21 15:41:52,216 DEBUG TRAIN Batch 16/5800 loss 5.128897 loss_att 7.066219 loss_ctc 5.694936 loss_rnnt 4.579010 hw_loss 0.163030 lr 0.00042364 rank 7
2023-02-21 15:41:52,218 DEBUG TRAIN Batch 16/5800 loss 10.951131 loss_att 10.660500 loss_ctc 13.562258 loss_rnnt 10.532855 hw_loss 0.240472 lr 0.00042373 rank 1
2023-02-21 15:41:52,241 DEBUG TRAIN Batch 16/5800 loss 10.030446 loss_att 12.893837 loss_ctc 12.288239 loss_rnnt 9.018632 hw_loss 0.258932 lr 0.00042373 rank 3
2023-02-21 15:41:52,246 DEBUG TRAIN Batch 16/5800 loss 6.477893 loss_att 10.392550 loss_ctc 9.485550 loss_rnnt 5.279140 hw_loss 0.027753 lr 0.00042370 rank 6
2023-02-21 15:43:11,574 DEBUG TRAIN Batch 16/5900 loss 13.121119 loss_att 15.056444 loss_ctc 15.549349 loss_rnnt 12.344645 hw_loss 0.123087 lr 0.00042357 rank 4
2023-02-21 15:43:11,575 DEBUG TRAIN Batch 16/5900 loss 6.330691 loss_att 11.568219 loss_ctc 11.669697 loss_rnnt 4.558265 hw_loss 0.024473 lr 0.00042349 rank 7
2023-02-21 15:43:11,579 DEBUG TRAIN Batch 16/5900 loss 8.619395 loss_att 12.233852 loss_ctc 13.095271 loss_rnnt 7.230984 hw_loss 0.128881 lr 0.00042358 rank 1
2023-02-21 15:43:11,579 DEBUG TRAIN Batch 16/5900 loss 2.623011 loss_att 5.187843 loss_ctc 3.996997 loss_rnnt 1.912928 hw_loss 0.026096 lr 0.00042358 rank 2
2023-02-21 15:43:11,581 DEBUG TRAIN Batch 16/5900 loss 7.249182 loss_att 13.218821 loss_ctc 13.275305 loss_rnnt 5.188172 hw_loss 0.119248 lr 0.00042355 rank 6
2023-02-21 15:43:11,584 DEBUG TRAIN Batch 16/5900 loss 5.118733 loss_att 9.354848 loss_ctc 7.484800 loss_rnnt 3.848375 hw_loss 0.201861 lr 0.00042357 rank 0
2023-02-21 15:43:11,586 DEBUG TRAIN Batch 16/5900 loss 2.047898 loss_att 4.901767 loss_ctc 2.399569 loss_rnnt 1.383073 hw_loss 0.088428 lr 0.00042358 rank 3
2023-02-21 15:43:11,586 DEBUG TRAIN Batch 16/5900 loss 14.997952 loss_att 19.484011 loss_ctc 18.253399 loss_rnnt 13.592756 hw_loss 0.138606 lr 0.00042350 rank 5
2023-02-21 15:44:31,265 DEBUG TRAIN Batch 16/6000 loss 14.504424 loss_att 18.533489 loss_ctc 19.307726 loss_rnnt 13.043785 hw_loss 0.026973 lr 0.00042342 rank 4
2023-02-21 15:44:31,268 DEBUG TRAIN Batch 16/6000 loss 6.851750 loss_att 8.661304 loss_ctc 7.880548 loss_rnnt 6.306808 hw_loss 0.085985 lr 0.00042334 rank 7
2023-02-21 15:44:31,269 DEBUG TRAIN Batch 16/6000 loss 7.852655 loss_att 8.460916 loss_ctc 7.996159 loss_rnnt 7.616329 hw_loss 0.179138 lr 0.00042342 rank 2
2023-02-21 15:44:31,269 DEBUG TRAIN Batch 16/6000 loss 8.852153 loss_att 10.396204 loss_ctc 10.721217 loss_rnnt 8.195950 hw_loss 0.184096 lr 0.00042342 rank 0
2023-02-21 15:44:31,270 DEBUG TRAIN Batch 16/6000 loss 6.027037 loss_att 7.603745 loss_ctc 9.190902 loss_rnnt 5.275242 hw_loss 0.027383 lr 0.00042343 rank 1
2023-02-21 15:44:31,273 DEBUG TRAIN Batch 16/6000 loss 15.636572 loss_att 17.846331 loss_ctc 16.365334 loss_rnnt 14.983831 hw_loss 0.213039 lr 0.00042343 rank 3
2023-02-21 15:44:31,273 DEBUG TRAIN Batch 16/6000 loss 9.729587 loss_att 13.446157 loss_ctc 14.528864 loss_rnnt 8.295760 hw_loss 0.094890 lr 0.00042335 rank 5
2023-02-21 15:44:31,322 DEBUG TRAIN Batch 16/6000 loss 8.475141 loss_att 11.010887 loss_ctc 11.700641 loss_rnnt 7.443931 hw_loss 0.176239 lr 0.00042340 rank 6
2023-02-21 15:45:51,508 DEBUG TRAIN Batch 16/6100 loss 11.600629 loss_att 15.926973 loss_ctc 19.426918 loss_rnnt 9.605109 hw_loss 0.162647 lr 0.00042319 rank 7
2023-02-21 15:45:51,512 DEBUG TRAIN Batch 16/6100 loss 7.752014 loss_att 12.273091 loss_ctc 9.715414 loss_rnnt 6.568051 hw_loss 0.033674 lr 0.00042325 rank 6
2023-02-21 15:45:51,515 DEBUG TRAIN Batch 16/6100 loss 3.621946 loss_att 7.721022 loss_ctc 5.299483 loss_rnnt 2.532706 hw_loss 0.085787 lr 0.00042327 rank 1
2023-02-21 15:45:51,519 DEBUG TRAIN Batch 16/6100 loss 14.773340 loss_att 19.725582 loss_ctc 20.354282 loss_rnnt 12.966487 hw_loss 0.135524 lr 0.00042327 rank 2
2023-02-21 15:45:51,520 DEBUG TRAIN Batch 16/6100 loss 11.735435 loss_att 14.173960 loss_ctc 15.650504 loss_rnnt 10.596924 hw_loss 0.241494 lr 0.00042328 rank 3
2023-02-21 15:45:51,522 DEBUG TRAIN Batch 16/6100 loss 12.196325 loss_att 18.007135 loss_ctc 18.132191 loss_rnnt 10.142599 hw_loss 0.187715 lr 0.00042320 rank 5
2023-02-21 15:45:51,524 DEBUG TRAIN Batch 16/6100 loss 11.630141 loss_att 12.370445 loss_ctc 13.413692 loss_rnnt 11.158688 hw_loss 0.160473 lr 0.00042327 rank 4
2023-02-21 15:45:51,554 DEBUG TRAIN Batch 16/6100 loss 7.804594 loss_att 10.291352 loss_ctc 9.090550 loss_rnnt 6.963270 hw_loss 0.323458 lr 0.00042326 rank 0
2023-02-21 15:47:10,574 DEBUG TRAIN Batch 16/6200 loss 17.617697 loss_att 20.228376 loss_ctc 24.243576 loss_rnnt 16.156162 hw_loss 0.104906 lr 0.00042304 rank 7
2023-02-21 15:47:10,576 DEBUG TRAIN Batch 16/6200 loss 17.423023 loss_att 23.013357 loss_ctc 24.526175 loss_rnnt 15.315941 hw_loss 0.078617 lr 0.00042309 rank 6
2023-02-21 15:47:10,581 DEBUG TRAIN Batch 16/6200 loss 8.319348 loss_att 10.785748 loss_ctc 14.237904 loss_rnnt 6.971379 hw_loss 0.122903 lr 0.00042311 rank 0
2023-02-21 15:47:10,581 DEBUG TRAIN Batch 16/6200 loss 11.114688 loss_att 13.607609 loss_ctc 16.863213 loss_rnnt 9.832075 hw_loss 0.032922 lr 0.00042312 rank 4
2023-02-21 15:47:10,582 DEBUG TRAIN Batch 16/6200 loss 7.522579 loss_att 11.796766 loss_ctc 11.748926 loss_rnnt 6.054782 hw_loss 0.092714 lr 0.00042312 rank 1
2023-02-21 15:47:10,586 DEBUG TRAIN Batch 16/6200 loss 4.572382 loss_att 5.653809 loss_ctc 5.367186 loss_rnnt 4.175945 hw_loss 0.139083 lr 0.00042305 rank 5
2023-02-21 15:47:10,587 DEBUG TRAIN Batch 16/6200 loss 9.586031 loss_att 11.065087 loss_ctc 13.390689 loss_rnnt 8.663775 hw_loss 0.223419 lr 0.00042312 rank 2
2023-02-21 15:47:10,590 DEBUG TRAIN Batch 16/6200 loss 8.447780 loss_att 10.610678 loss_ctc 11.470441 loss_rnnt 7.512443 hw_loss 0.187004 lr 0.00042313 rank 3
2023-02-21 15:48:30,440 DEBUG TRAIN Batch 16/6300 loss 10.752410 loss_att 13.780270 loss_ctc 15.122604 loss_rnnt 9.516065 hw_loss 0.090151 lr 0.00042297 rank 4
2023-02-21 15:48:30,443 DEBUG TRAIN Batch 16/6300 loss 13.296569 loss_att 13.881217 loss_ctc 16.926022 loss_rnnt 12.635836 hw_loss 0.112267 lr 0.00042294 rank 6
2023-02-21 15:48:30,445 DEBUG TRAIN Batch 16/6300 loss 8.363192 loss_att 9.394849 loss_ctc 11.988655 loss_rnnt 7.562170 hw_loss 0.208677 lr 0.00042297 rank 2
2023-02-21 15:48:30,445 DEBUG TRAIN Batch 16/6300 loss 7.486825 loss_att 11.428870 loss_ctc 9.832554 loss_rnnt 6.360113 hw_loss 0.047888 lr 0.00042290 rank 5
2023-02-21 15:48:30,446 DEBUG TRAIN Batch 16/6300 loss 16.116261 loss_att 22.607225 loss_ctc 22.593475 loss_rnnt 13.885515 hw_loss 0.129229 lr 0.00042298 rank 3
2023-02-21 15:48:30,447 DEBUG TRAIN Batch 16/6300 loss 10.484281 loss_att 16.084976 loss_ctc 12.320660 loss_rnnt 9.018891 hw_loss 0.188248 lr 0.00042297 rank 1
2023-02-21 15:48:30,452 DEBUG TRAIN Batch 16/6300 loss 9.826072 loss_att 11.771442 loss_ctc 13.565886 loss_rnnt 8.901270 hw_loss 0.069536 lr 0.00042288 rank 7
2023-02-21 15:48:30,494 DEBUG TRAIN Batch 16/6300 loss 16.074511 loss_att 18.472622 loss_ctc 18.291744 loss_rnnt 15.279522 hw_loss 0.037003 lr 0.00042296 rank 0
2023-02-21 15:49:51,329 DEBUG TRAIN Batch 16/6400 loss 17.801662 loss_att 20.964994 loss_ctc 29.493050 loss_rnnt 15.576328 hw_loss 0.063407 lr 0.00042282 rank 2
2023-02-21 15:49:51,329 DEBUG TRAIN Batch 16/6400 loss 6.845249 loss_att 7.604743 loss_ctc 8.822224 loss_rnnt 6.290312 hw_loss 0.261453 lr 0.00042273 rank 7
2023-02-21 15:49:51,335 DEBUG TRAIN Batch 16/6400 loss 11.573293 loss_att 11.516687 loss_ctc 15.115940 loss_rnnt 10.984932 hw_loss 0.238742 lr 0.00042281 rank 4
2023-02-21 15:49:51,339 DEBUG TRAIN Batch 16/6400 loss 9.530071 loss_att 10.357517 loss_ctc 10.692348 loss_rnnt 9.151566 hw_loss 0.108836 lr 0.00042282 rank 1
2023-02-21 15:49:51,340 DEBUG TRAIN Batch 16/6400 loss 18.268517 loss_att 20.193863 loss_ctc 23.994869 loss_rnnt 17.018280 hw_loss 0.190600 lr 0.00042274 rank 5
2023-02-21 15:49:51,342 DEBUG TRAIN Batch 16/6400 loss 21.497416 loss_att 24.387241 loss_ctc 26.333042 loss_rnnt 20.239437 hw_loss 0.066120 lr 0.00042282 rank 3
2023-02-21 15:49:51,343 DEBUG TRAIN Batch 16/6400 loss 12.360499 loss_att 13.928688 loss_ctc 17.670965 loss_rnnt 11.284125 hw_loss 0.102516 lr 0.00042281 rank 0
2023-02-21 15:49:51,348 DEBUG TRAIN Batch 16/6400 loss 14.593349 loss_att 18.213926 loss_ctc 20.102427 loss_rnnt 13.069499 hw_loss 0.122235 lr 0.00042279 rank 6
2023-02-21 15:51:09,273 DEBUG TRAIN Batch 16/6500 loss 10.385647 loss_att 10.496982 loss_ctc 13.919907 loss_rnnt 9.768085 hw_loss 0.232616 lr 0.00042266 rank 0
2023-02-21 15:51:09,275 DEBUG TRAIN Batch 16/6500 loss 7.274290 loss_att 10.711822 loss_ctc 10.559020 loss_rnnt 6.111726 hw_loss 0.069549 lr 0.00042258 rank 7
2023-02-21 15:51:09,276 DEBUG TRAIN Batch 16/6500 loss 15.365334 loss_att 20.813545 loss_ctc 20.879463 loss_rnnt 13.527893 hw_loss 0.023590 lr 0.00042267 rank 1
2023-02-21 15:51:09,281 DEBUG TRAIN Batch 16/6500 loss 13.862924 loss_att 16.394032 loss_ctc 14.962816 loss_rnnt 13.175696 hw_loss 0.064413 lr 0.00042267 rank 2
2023-02-21 15:51:09,282 DEBUG TRAIN Batch 16/6500 loss 14.259463 loss_att 16.215029 loss_ctc 18.666540 loss_rnnt 13.127521 hw_loss 0.287288 lr 0.00042264 rank 6
2023-02-21 15:51:09,282 DEBUG TRAIN Batch 16/6500 loss 16.544876 loss_att 17.311960 loss_ctc 19.102705 loss_rnnt 15.995466 hw_loss 0.103032 lr 0.00042266 rank 4
2023-02-21 15:51:09,285 DEBUG TRAIN Batch 16/6500 loss 6.428998 loss_att 8.666340 loss_ctc 11.603805 loss_rnnt 5.200963 hw_loss 0.169860 lr 0.00042267 rank 3
2023-02-21 15:51:09,287 DEBUG TRAIN Batch 16/6500 loss 24.627247 loss_att 27.262243 loss_ctc 39.510551 loss_rnnt 22.103401 hw_loss 0.023261 lr 0.00042259 rank 5
2023-02-21 15:52:28,475 DEBUG TRAIN Batch 16/6600 loss 16.625759 loss_att 16.920849 loss_ctc 24.080536 loss_rnnt 15.539829 hw_loss 0.061765 lr 0.00042252 rank 2
2023-02-21 15:52:28,481 DEBUG TRAIN Batch 16/6600 loss 9.102651 loss_att 10.843435 loss_ctc 12.115124 loss_rnnt 8.279432 hw_loss 0.137622 lr 0.00042251 rank 0
2023-02-21 15:52:28,481 DEBUG TRAIN Batch 16/6600 loss 15.206703 loss_att 19.634315 loss_ctc 17.807901 loss_rnnt 13.861637 hw_loss 0.211344 lr 0.00042244 rank 5
2023-02-21 15:52:28,482 DEBUG TRAIN Batch 16/6600 loss 11.598083 loss_att 18.244780 loss_ctc 13.141907 loss_rnnt 9.991951 hw_loss 0.133027 lr 0.00042243 rank 7
2023-02-21 15:52:28,482 DEBUG TRAIN Batch 16/6600 loss 7.452284 loss_att 10.631387 loss_ctc 10.735317 loss_rnnt 6.324460 hw_loss 0.101747 lr 0.00042252 rank 1
2023-02-21 15:52:28,485 DEBUG TRAIN Batch 16/6600 loss 7.697894 loss_att 13.569719 loss_ctc 17.413544 loss_rnnt 5.181451 hw_loss 0.087484 lr 0.00042251 rank 4
2023-02-21 15:52:28,488 DEBUG TRAIN Batch 16/6600 loss 13.220612 loss_att 18.009504 loss_ctc 18.906841 loss_rnnt 11.455107 hw_loss 0.092931 lr 0.00042252 rank 3
2023-02-21 15:52:28,533 DEBUG TRAIN Batch 16/6600 loss 18.253794 loss_att 22.249790 loss_ctc 34.747112 loss_rnnt 15.094101 hw_loss 0.302598 lr 0.00042249 rank 6
2023-02-21 15:53:49,161 DEBUG TRAIN Batch 16/6700 loss 13.377300 loss_att 15.641705 loss_ctc 17.054787 loss_rnnt 12.334867 hw_loss 0.186039 lr 0.00042234 rank 6
2023-02-21 15:53:49,161 DEBUG TRAIN Batch 16/6700 loss 6.113585 loss_att 10.595933 loss_ctc 11.673117 loss_rnnt 4.388384 hw_loss 0.163989 lr 0.00042237 rank 2
2023-02-21 15:53:49,163 DEBUG TRAIN Batch 16/6700 loss 19.150011 loss_att 25.046284 loss_ctc 23.050205 loss_rnnt 17.394600 hw_loss 0.105241 lr 0.00042237 rank 3
2023-02-21 15:53:49,163 DEBUG TRAIN Batch 16/6700 loss 21.505758 loss_att 27.146883 loss_ctc 28.627840 loss_rnnt 19.404413 hw_loss 0.044081 lr 0.00042229 rank 5
2023-02-21 15:53:49,164 DEBUG TRAIN Batch 16/6700 loss 6.084923 loss_att 9.187496 loss_ctc 9.043009 loss_rnnt 4.939027 hw_loss 0.245569 lr 0.00042228 rank 7
2023-02-21 15:53:49,166 DEBUG TRAIN Batch 16/6700 loss 12.417330 loss_att 16.669056 loss_ctc 19.964504 loss_rnnt 10.507709 hw_loss 0.099350 lr 0.00042237 rank 1
2023-02-21 15:53:49,167 DEBUG TRAIN Batch 16/6700 loss 5.313941 loss_att 7.731668 loss_ctc 8.729426 loss_rnnt 4.294462 hw_loss 0.151006 lr 0.00042236 rank 0
2023-02-21 15:53:49,168 DEBUG TRAIN Batch 16/6700 loss 9.567855 loss_att 15.193127 loss_ctc 13.302584 loss_rnnt 7.844796 hw_loss 0.187576 lr 0.00042236 rank 4
2023-02-21 15:55:09,653 DEBUG TRAIN Batch 16/6800 loss 9.589628 loss_att 11.833705 loss_ctc 11.899337 loss_rnnt 8.710628 hw_loss 0.229170 lr 0.00042221 rank 4
2023-02-21 15:55:09,653 DEBUG TRAIN Batch 16/6800 loss 13.126966 loss_att 15.750381 loss_ctc 19.540749 loss_rnnt 11.644670 hw_loss 0.192079 lr 0.00042222 rank 1
2023-02-21 15:55:09,655 DEBUG TRAIN Batch 16/6800 loss 14.939157 loss_att 16.849424 loss_ctc 21.707716 loss_rnnt 13.536058 hw_loss 0.222320 lr 0.00042222 rank 2
2023-02-21 15:55:09,657 DEBUG TRAIN Batch 16/6800 loss 12.102782 loss_att 15.032475 loss_ctc 19.012791 loss_rnnt 10.531126 hw_loss 0.120719 lr 0.00042213 rank 7
2023-02-21 15:55:09,659 DEBUG TRAIN Batch 16/6800 loss 14.432570 loss_att 19.185665 loss_ctc 19.030186 loss_rnnt 12.828284 hw_loss 0.076223 lr 0.00042221 rank 0
2023-02-21 15:55:09,660 DEBUG TRAIN Batch 16/6800 loss 7.619215 loss_att 7.758549 loss_ctc 11.487990 loss_rnnt 6.984084 hw_loss 0.171429 lr 0.00042222 rank 3
2023-02-21 15:55:09,664 DEBUG TRAIN Batch 16/6800 loss 11.695849 loss_att 14.894803 loss_ctc 14.450302 loss_rnnt 10.623411 hw_loss 0.122601 lr 0.00042219 rank 6
2023-02-21 15:55:09,668 DEBUG TRAIN Batch 16/6800 loss 7.912725 loss_att 12.162653 loss_ctc 11.776546 loss_rnnt 6.418935 hw_loss 0.241178 lr 0.00042214 rank 5
2023-02-21 15:56:28,913 DEBUG TRAIN Batch 16/6900 loss 7.466884 loss_att 9.416988 loss_ctc 9.856760 loss_rnnt 6.640269 hw_loss 0.221145 lr 0.00042198 rank 7
2023-02-21 15:56:28,917 DEBUG TRAIN Batch 16/6900 loss 7.462203 loss_att 10.064751 loss_ctc 10.255546 loss_rnnt 6.501690 hw_loss 0.126669 lr 0.00042206 rank 0
2023-02-21 15:56:28,918 DEBUG TRAIN Batch 16/6900 loss 12.840872 loss_att 16.933537 loss_ctc 21.041533 loss_rnnt 10.877014 hw_loss 0.097318 lr 0.00042206 rank 4
2023-02-21 15:56:28,923 DEBUG TRAIN Batch 16/6900 loss 8.665140 loss_att 11.651132 loss_ctc 11.923131 loss_rnnt 7.551369 hw_loss 0.154076 lr 0.00042199 rank 5
2023-02-21 15:56:28,926 DEBUG TRAIN Batch 16/6900 loss 20.457449 loss_att 25.007278 loss_ctc 30.283585 loss_rnnt 18.134056 hw_loss 0.193640 lr 0.00042204 rank 6
2023-02-21 15:56:28,926 DEBUG TRAIN Batch 16/6900 loss 8.197144 loss_att 11.232896 loss_ctc 12.541170 loss_rnnt 6.872118 hw_loss 0.260009 lr 0.00042206 rank 2
2023-02-21 15:56:28,930 DEBUG TRAIN Batch 16/6900 loss 9.054449 loss_att 11.848215 loss_ctc 9.902321 loss_rnnt 8.346848 hw_loss 0.067122 lr 0.00042207 rank 1
2023-02-21 15:56:28,973 DEBUG TRAIN Batch 16/6900 loss 5.730792 loss_att 5.078810 loss_ctc 6.753982 loss_rnnt 5.506518 hw_loss 0.409208 lr 0.00042207 rank 3
2023-02-21 15:57:48,478 DEBUG TRAIN Batch 16/7000 loss 8.697354 loss_att 12.860877 loss_ctc 12.985594 loss_rnnt 7.276783 hw_loss 0.030192 lr 0.00042191 rank 2
2023-02-21 15:57:48,479 DEBUG TRAIN Batch 16/7000 loss 8.185566 loss_att 10.219272 loss_ctc 13.984133 loss_rnnt 6.950016 hw_loss 0.104374 lr 0.00042183 rank 7
2023-02-21 15:57:48,481 DEBUG TRAIN Batch 16/7000 loss 9.049328 loss_att 11.446356 loss_ctc 11.373455 loss_rnnt 8.107880 hw_loss 0.285299 lr 0.00042192 rank 3
2023-02-21 15:57:48,481 DEBUG TRAIN Batch 16/7000 loss 14.213255 loss_att 15.848282 loss_ctc 15.647682 loss_rnnt 13.638415 hw_loss 0.106085 lr 0.00042191 rank 0
2023-02-21 15:57:48,483 DEBUG TRAIN Batch 16/7000 loss 9.252788 loss_att 9.534566 loss_ctc 12.063808 loss_rnnt 8.588278 hw_loss 0.437534 lr 0.00042184 rank 5
2023-02-21 15:57:48,483 DEBUG TRAIN Batch 16/7000 loss 12.380257 loss_att 14.045255 loss_ctc 17.899702 loss_rnnt 11.165825 hw_loss 0.272824 lr 0.00042191 rank 4
2023-02-21 15:57:48,484 DEBUG TRAIN Batch 16/7000 loss 12.631195 loss_att 17.693495 loss_ctc 17.547783 loss_rnnt 10.892230 hw_loss 0.133053 lr 0.00042192 rank 1
2023-02-21 15:57:48,527 DEBUG TRAIN Batch 16/7000 loss 13.398440 loss_att 14.292806 loss_ctc 17.073750 loss_rnnt 12.651435 hw_loss 0.146423 lr 0.00042189 rank 6
2023-02-21 15:59:09,981 DEBUG TRAIN Batch 16/7100 loss 10.672871 loss_att 15.701145 loss_ctc 12.588789 loss_rnnt 9.366457 hw_loss 0.084942 lr 0.00042176 rank 4
2023-02-21 15:59:09,983 DEBUG TRAIN Batch 16/7100 loss 9.828469 loss_att 13.895449 loss_ctc 17.601635 loss_rnnt 7.945512 hw_loss 0.062136 lr 0.00042169 rank 5
2023-02-21 15:59:09,984 DEBUG TRAIN Batch 16/7100 loss 9.419999 loss_att 9.902989 loss_ctc 11.500507 loss_rnnt 8.893579 hw_loss 0.285788 lr 0.00042177 rank 1
2023-02-21 15:59:09,985 DEBUG TRAIN Batch 16/7100 loss 8.902264 loss_att 9.921937 loss_ctc 10.722734 loss_rnnt 8.285013 hw_loss 0.319849 lr 0.00042176 rank 0
2023-02-21 15:59:09,986 DEBUG TRAIN Batch 16/7100 loss 15.003128 loss_att 20.134089 loss_ctc 19.448452 loss_rnnt 13.349394 hw_loss 0.065313 lr 0.00042168 rank 7
2023-02-21 15:59:09,987 DEBUG TRAIN Batch 16/7100 loss 4.187863 loss_att 4.871039 loss_ctc 3.187803 loss_rnnt 4.094403 hw_loss 0.169061 lr 0.00042174 rank 6
2023-02-21 15:59:09,987 DEBUG TRAIN Batch 16/7100 loss 18.234175 loss_att 21.003338 loss_ctc 26.148865 loss_rnnt 16.573864 hw_loss 0.095971 lr 0.00042177 rank 3
2023-02-21 15:59:09,988 DEBUG TRAIN Batch 16/7100 loss 14.214026 loss_att 15.996805 loss_ctc 17.920143 loss_rnnt 13.308441 hw_loss 0.102901 lr 0.00042176 rank 2
2023-02-21 16:00:28,955 DEBUG TRAIN Batch 16/7200 loss 10.299257 loss_att 10.727605 loss_ctc 12.065981 loss_rnnt 9.952395 hw_loss 0.048055 lr 0.00042161 rank 2
2023-02-21 16:00:28,957 DEBUG TRAIN Batch 16/7200 loss 13.891497 loss_att 15.513601 loss_ctc 19.619827 loss_rnnt 12.769717 hw_loss 0.062967 lr 0.00042153 rank 7
2023-02-21 16:00:28,959 DEBUG TRAIN Batch 16/7200 loss 17.179789 loss_att 22.019522 loss_ctc 22.930214 loss_rnnt 15.359243 hw_loss 0.161016 lr 0.00042161 rank 0
2023-02-21 16:00:28,959 DEBUG TRAIN Batch 16/7200 loss 7.742434 loss_att 10.283305 loss_ctc 8.874787 loss_rnnt 7.070219 hw_loss 0.024488 lr 0.00042159 rank 6
2023-02-21 16:00:28,959 DEBUG TRAIN Batch 16/7200 loss 6.382234 loss_att 14.093479 loss_ctc 11.265762 loss_rnnt 4.120902 hw_loss 0.127397 lr 0.00042161 rank 4
2023-02-21 16:00:28,963 DEBUG TRAIN Batch 16/7200 loss 6.336914 loss_att 8.884100 loss_ctc 8.010791 loss_rnnt 5.591679 hw_loss 0.023652 lr 0.00042162 rank 1
2023-02-21 16:00:28,966 DEBUG TRAIN Batch 16/7200 loss 1.683424 loss_att 4.606939 loss_ctc 2.715935 loss_rnnt 0.849933 hw_loss 0.208348 lr 0.00042154 rank 5
2023-02-21 16:00:29,012 DEBUG TRAIN Batch 16/7200 loss 12.617695 loss_att 15.151386 loss_ctc 16.920507 loss_rnnt 11.455593 hw_loss 0.153102 lr 0.00042162 rank 3
2023-02-21 16:01:45,932 DEBUG TRAIN Batch 16/7300 loss 9.093111 loss_att 12.508956 loss_ctc 11.127235 loss_rnnt 8.101795 hw_loss 0.069245 lr 0.00042146 rank 0
2023-02-21 16:01:45,933 DEBUG TRAIN Batch 16/7300 loss 9.335124 loss_att 13.774727 loss_ctc 12.570254 loss_rnnt 8.001225 hw_loss 0.027428 lr 0.00042138 rank 7
2023-02-21 16:01:45,940 DEBUG TRAIN Batch 16/7300 loss 9.353455 loss_att 14.279896 loss_ctc 14.401041 loss_rnnt 7.560327 hw_loss 0.252803 lr 0.00042146 rank 2
2023-02-21 16:01:45,940 DEBUG TRAIN Batch 16/7300 loss 19.608759 loss_att 20.699287 loss_ctc 22.256969 loss_rnnt 18.993538 hw_loss 0.082543 lr 0.00042139 rank 5
2023-02-21 16:01:45,940 DEBUG TRAIN Batch 16/7300 loss 18.284157 loss_att 21.604345 loss_ctc 23.538750 loss_rnnt 16.813902 hw_loss 0.198014 lr 0.00042146 rank 4
2023-02-21 16:01:45,942 DEBUG TRAIN Batch 16/7300 loss 6.031374 loss_att 8.842950 loss_ctc 7.330110 loss_rnnt 5.280528 hw_loss 0.028809 lr 0.00042144 rank 6
2023-02-21 16:01:45,944 DEBUG TRAIN Batch 16/7300 loss 8.661036 loss_att 13.217001 loss_ctc 14.743635 loss_rnnt 6.833362 hw_loss 0.197752 lr 0.00042147 rank 1
2023-02-21 16:01:45,947 DEBUG TRAIN Batch 16/7300 loss 7.454478 loss_att 8.813910 loss_ctc 7.800313 loss_rnnt 7.069742 hw_loss 0.125134 lr 0.00042147 rank 3
2023-02-21 16:03:05,032 DEBUG TRAIN Batch 16/7400 loss 13.569094 loss_att 13.480037 loss_ctc 17.491941 loss_rnnt 13.020967 hw_loss 0.080418 lr 0.00042129 rank 6
2023-02-21 16:03:05,032 DEBUG TRAIN Batch 16/7400 loss 7.069023 loss_att 12.844908 loss_ctc 7.881306 loss_rnnt 5.713872 hw_loss 0.171879 lr 0.00042131 rank 4
2023-02-21 16:03:05,033 DEBUG TRAIN Batch 16/7400 loss 5.390997 loss_att 7.668569 loss_ctc 6.634926 loss_rnnt 4.646147 hw_loss 0.231522 lr 0.00042131 rank 0
2023-02-21 16:03:05,033 DEBUG TRAIN Batch 16/7400 loss 10.297461 loss_att 14.414129 loss_ctc 14.484441 loss_rnnt 8.850641 hw_loss 0.122290 lr 0.00042132 rank 3
2023-02-21 16:03:05,034 DEBUG TRAIN Batch 16/7400 loss 7.417128 loss_att 9.809307 loss_ctc 8.207889 loss_rnnt 6.729216 hw_loss 0.195078 lr 0.00042132 rank 2
2023-02-21 16:03:05,038 DEBUG TRAIN Batch 16/7400 loss 11.394566 loss_att 13.949777 loss_ctc 13.039451 loss_rnnt 10.647221 hw_loss 0.031848 lr 0.00042123 rank 7
2023-02-21 16:03:05,038 DEBUG TRAIN Batch 16/7400 loss 12.498341 loss_att 18.565538 loss_ctc 15.972157 loss_rnnt 10.758168 hw_loss 0.119169 lr 0.00042132 rank 1
2023-02-21 16:03:05,042 DEBUG TRAIN Batch 16/7400 loss 8.472042 loss_att 12.290867 loss_ctc 12.417195 loss_rnnt 7.110998 hw_loss 0.133609 lr 0.00042124 rank 5
2023-02-21 16:04:25,512 DEBUG TRAIN Batch 16/7500 loss 12.014909 loss_att 11.889478 loss_ctc 13.844048 loss_rnnt 11.704476 hw_loss 0.171811 lr 0.00042108 rank 7
2023-02-21 16:04:25,512 DEBUG TRAIN Batch 16/7500 loss 22.958685 loss_att 26.945950 loss_ctc 31.720459 loss_rnnt 20.883547 hw_loss 0.205214 lr 0.00042117 rank 1
2023-02-21 16:04:25,513 DEBUG TRAIN Batch 16/7500 loss 25.935184 loss_att 27.772326 loss_ctc 32.719975 loss_rnnt 24.524693 hw_loss 0.259543 lr 0.00042114 rank 6
2023-02-21 16:04:25,512 DEBUG TRAIN Batch 16/7500 loss 7.161980 loss_att 9.491633 loss_ctc 9.346931 loss_rnnt 6.313090 hw_loss 0.171811 lr 0.00042116 rank 4
2023-02-21 16:04:25,514 DEBUG TRAIN Batch 16/7500 loss 8.721066 loss_att 11.553384 loss_ctc 11.916106 loss_rnnt 7.675621 hw_loss 0.099332 lr 0.00042117 rank 2
2023-02-21 16:04:25,516 DEBUG TRAIN Batch 16/7500 loss 10.463182 loss_att 11.252755 loss_ctc 13.887800 loss_rnnt 9.733580 hw_loss 0.215762 lr 0.00042117 rank 3
2023-02-21 16:04:25,517 DEBUG TRAIN Batch 16/7500 loss 5.350243 loss_att 7.672542 loss_ctc 6.557018 loss_rnnt 4.654821 hw_loss 0.131360 lr 0.00042116 rank 0
2023-02-21 16:04:25,518 DEBUG TRAIN Batch 16/7500 loss 10.978995 loss_att 12.237169 loss_ctc 12.900080 loss_rnnt 10.409208 hw_loss 0.116265 lr 0.00042109 rank 5
2023-02-21 16:05:43,789 DEBUG TRAIN Batch 16/7600 loss 15.176010 loss_att 12.751253 loss_ctc 17.956961 loss_rnnt 15.236437 hw_loss 0.100743 lr 0.00042093 rank 7
2023-02-21 16:05:43,791 DEBUG TRAIN Batch 16/7600 loss 7.574923 loss_att 9.829361 loss_ctc 8.411065 loss_rnnt 6.940841 hw_loss 0.134454 lr 0.00042099 rank 6
2023-02-21 16:05:43,792 DEBUG TRAIN Batch 16/7600 loss 14.031227 loss_att 15.886223 loss_ctc 17.437449 loss_rnnt 13.163042 hw_loss 0.080666 lr 0.00042101 rank 0
2023-02-21 16:05:43,793 DEBUG TRAIN Batch 16/7600 loss 8.566677 loss_att 11.258082 loss_ctc 15.418512 loss_rnnt 7.053665 hw_loss 0.114662 lr 0.00042102 rank 2
2023-02-21 16:05:43,793 DEBUG TRAIN Batch 16/7600 loss 4.849378 loss_att 6.664059 loss_ctc 7.100971 loss_rnnt 4.133212 hw_loss 0.099407 lr 0.00042102 rank 1
2023-02-21 16:05:43,798 DEBUG TRAIN Batch 16/7600 loss 5.266182 loss_att 7.271506 loss_ctc 6.225825 loss_rnnt 4.669469 hw_loss 0.126931 lr 0.00042101 rank 4
2023-02-21 16:05:43,801 DEBUG TRAIN Batch 16/7600 loss 7.855648 loss_att 9.857334 loss_ctc 13.576872 loss_rnnt 6.629706 hw_loss 0.117704 lr 0.00042102 rank 3
2023-02-21 16:05:43,846 DEBUG TRAIN Batch 16/7600 loss 11.462208 loss_att 16.825136 loss_ctc 16.591019 loss_rnnt 9.666663 hw_loss 0.073348 lr 0.00042094 rank 5
2023-02-21 16:07:01,967 DEBUG TRAIN Batch 16/7700 loss 11.165878 loss_att 14.224421 loss_ctc 15.339025 loss_rnnt 9.894483 hw_loss 0.193626 lr 0.00042086 rank 4
2023-02-21 16:07:01,969 DEBUG TRAIN Batch 16/7700 loss 14.931836 loss_att 18.929863 loss_ctc 17.767723 loss_rnnt 13.740832 hw_loss 0.024901 lr 0.00042087 rank 2
2023-02-21 16:07:01,972 DEBUG TRAIN Batch 16/7700 loss 14.024868 loss_att 15.149252 loss_ctc 17.732779 loss_rnnt 13.202868 hw_loss 0.192627 lr 0.00042087 rank 1
2023-02-21 16:07:01,974 DEBUG TRAIN Batch 16/7700 loss 6.726285 loss_att 11.572207 loss_ctc 7.662517 loss_rnnt 5.569373 hw_loss 0.117932 lr 0.00042078 rank 7
2023-02-21 16:07:01,975 DEBUG TRAIN Batch 16/7700 loss 12.021786 loss_att 15.382203 loss_ctc 17.656525 loss_rnnt 10.524676 hw_loss 0.138239 lr 0.00042084 rank 6
2023-02-21 16:07:01,979 DEBUG TRAIN Batch 16/7700 loss 11.687278 loss_att 10.832380 loss_ctc 13.593530 loss_rnnt 11.365207 hw_loss 0.447908 lr 0.00042079 rank 5
2023-02-21 16:07:01,980 DEBUG TRAIN Batch 16/7700 loss 9.439505 loss_att 14.884745 loss_ctc 12.899849 loss_rnnt 7.844810 hw_loss 0.083004 lr 0.00042087 rank 3
2023-02-21 16:07:02,021 DEBUG TRAIN Batch 16/7700 loss 5.358049 loss_att 7.451693 loss_ctc 5.696908 loss_rnnt 4.794847 hw_loss 0.186174 lr 0.00042086 rank 0
2023-02-21 16:08:22,669 DEBUG TRAIN Batch 16/7800 loss 9.817027 loss_att 11.414018 loss_ctc 9.148359 loss_rnnt 9.539310 hw_loss 0.089013 lr 0.00042072 rank 2
2023-02-21 16:08:22,672 DEBUG TRAIN Batch 16/7800 loss 7.672130 loss_att 13.005835 loss_ctc 11.369808 loss_rnnt 6.050596 hw_loss 0.115816 lr 0.00042069 rank 6
2023-02-21 16:08:22,675 DEBUG TRAIN Batch 16/7800 loss 15.014444 loss_att 14.522434 loss_ctc 19.414955 loss_rnnt 14.401950 hw_loss 0.232803 lr 0.00042071 rank 4
2023-02-21 16:08:22,675 DEBUG TRAIN Batch 16/7800 loss 2.992877 loss_att 5.561609 loss_ctc 3.467657 loss_rnnt 2.319280 hw_loss 0.181025 lr 0.00042072 rank 1
2023-02-21 16:08:22,675 DEBUG TRAIN Batch 16/7800 loss 12.164816 loss_att 15.079070 loss_ctc 17.972321 loss_rnnt 10.711393 hw_loss 0.180446 lr 0.00042065 rank 5
2023-02-21 16:08:22,676 DEBUG TRAIN Batch 16/7800 loss 7.563808 loss_att 11.480615 loss_ctc 10.816047 loss_rnnt 6.310878 hw_loss 0.067384 lr 0.00042063 rank 7
2023-02-21 16:08:22,680 DEBUG TRAIN Batch 16/7800 loss 11.324926 loss_att 13.940313 loss_ctc 17.385687 loss_rnnt 9.963471 hw_loss 0.056768 lr 0.00042072 rank 3
2023-02-21 16:08:22,722 DEBUG TRAIN Batch 16/7800 loss 10.149356 loss_att 13.908503 loss_ctc 11.883110 loss_rnnt 9.063515 hw_loss 0.192833 lr 0.00042071 rank 0
2023-02-21 16:09:41,656 DEBUG TRAIN Batch 16/7900 loss 18.656998 loss_att 22.817289 loss_ctc 28.062832 loss_rnnt 16.527966 hw_loss 0.080366 lr 0.00042048 rank 7
2023-02-21 16:09:41,655 DEBUG TRAIN Batch 16/7900 loss 7.440551 loss_att 9.909876 loss_ctc 9.605327 loss_rnnt 6.635278 hw_loss 0.042696 lr 0.00042056 rank 4
2023-02-21 16:09:41,656 DEBUG TRAIN Batch 16/7900 loss 24.406221 loss_att 26.025558 loss_ctc 32.004147 loss_rnnt 22.889622 hw_loss 0.336890 lr 0.00042057 rank 1
2023-02-21 16:09:41,658 DEBUG TRAIN Batch 16/7900 loss 8.134973 loss_att 10.656845 loss_ctc 12.020804 loss_rnnt 6.970535 hw_loss 0.266159 lr 0.00042054 rank 6
2023-02-21 16:09:41,659 DEBUG TRAIN Batch 16/7900 loss 8.764926 loss_att 10.576267 loss_ctc 8.324782 loss_rnnt 8.430143 hw_loss 0.058500 lr 0.00042058 rank 3
2023-02-21 16:09:41,659 DEBUG TRAIN Batch 16/7900 loss 3.230721 loss_att 8.475262 loss_ctc 5.002672 loss_rnnt 1.890996 hw_loss 0.102294 lr 0.00042056 rank 0
2023-02-21 16:09:41,663 DEBUG TRAIN Batch 16/7900 loss 13.401990 loss_att 18.489363 loss_ctc 17.543200 loss_rnnt 11.727345 hw_loss 0.196893 lr 0.00042057 rank 2
2023-02-21 16:09:41,669 DEBUG TRAIN Batch 16/7900 loss 11.065513 loss_att 14.018232 loss_ctc 17.543447 loss_rnnt 9.597265 hw_loss 0.026209 lr 0.00042050 rank 5
2023-02-21 16:10:58,993 DEBUG TRAIN Batch 16/8000 loss 6.241041 loss_att 9.004527 loss_ctc 8.005089 loss_rnnt 5.397439 hw_loss 0.104433 lr 0.00042034 rank 7
2023-02-21 16:10:58,994 DEBUG TRAIN Batch 16/8000 loss 9.308700 loss_att 12.993166 loss_ctc 11.651340 loss_rnnt 8.211945 hw_loss 0.089080 lr 0.00042043 rank 3
2023-02-21 16:10:58,994 DEBUG TRAIN Batch 16/8000 loss 18.007576 loss_att 22.085604 loss_ctc 24.014635 loss_rnnt 16.303677 hw_loss 0.163784 lr 0.00042042 rank 2
2023-02-21 16:10:58,997 DEBUG TRAIN Batch 16/8000 loss 8.240544 loss_att 10.057945 loss_ctc 9.546576 loss_rnnt 7.643301 hw_loss 0.111800 lr 0.00042042 rank 4
2023-02-21 16:10:58,998 DEBUG TRAIN Batch 16/8000 loss 17.145597 loss_att 21.041021 loss_ctc 19.429020 loss_rnnt 15.993032 hw_loss 0.129423 lr 0.00042041 rank 0
2023-02-21 16:10:58,998 DEBUG TRAIN Batch 16/8000 loss 3.072782 loss_att 6.493861 loss_ctc 7.695103 loss_rnnt 1.748676 hw_loss 0.044215 lr 0.00042039 rank 6
2023-02-21 16:10:59,002 DEBUG TRAIN Batch 16/8000 loss 11.812490 loss_att 12.267461 loss_ctc 23.806477 loss_rnnt 10.004285 hw_loss 0.221271 lr 0.00042042 rank 1
2023-02-21 16:10:59,008 DEBUG TRAIN Batch 16/8000 loss 6.860656 loss_att 10.463855 loss_ctc 10.780486 loss_rnnt 5.571642 hw_loss 0.085744 lr 0.00042035 rank 5
2023-02-21 16:12:18,150 DEBUG TRAIN Batch 16/8100 loss 8.685039 loss_att 9.692486 loss_ctc 12.894358 loss_rnnt 7.806749 hw_loss 0.216669 lr 0.00042019 rank 7
2023-02-21 16:12:18,155 DEBUG TRAIN Batch 16/8100 loss 7.262569 loss_att 9.589752 loss_ctc 11.949956 loss_rnnt 6.127322 hw_loss 0.084047 lr 0.00042026 rank 0
2023-02-21 16:12:18,155 DEBUG TRAIN Batch 16/8100 loss 17.768862 loss_att 18.955431 loss_ctc 24.237408 loss_rnnt 16.473919 hw_loss 0.365914 lr 0.00042027 rank 4
2023-02-21 16:12:18,157 DEBUG TRAIN Batch 16/8100 loss 4.791866 loss_att 6.495399 loss_ctc 6.395132 loss_rnnt 4.136963 hw_loss 0.188301 lr 0.00042027 rank 2
2023-02-21 16:12:18,158 DEBUG TRAIN Batch 16/8100 loss 7.820334 loss_att 9.676278 loss_ctc 9.960766 loss_rnnt 7.089571 hw_loss 0.139094 lr 0.00042025 rank 6
2023-02-21 16:12:18,161 DEBUG TRAIN Batch 16/8100 loss 8.928754 loss_att 10.845110 loss_ctc 12.738659 loss_rnnt 7.983515 hw_loss 0.101213 lr 0.00042020 rank 5
2023-02-21 16:12:18,164 DEBUG TRAIN Batch 16/8100 loss 13.357743 loss_att 14.598562 loss_ctc 16.535805 loss_rnnt 12.588454 hw_loss 0.182594 lr 0.00042028 rank 3
2023-02-21 16:12:18,236 DEBUG TRAIN Batch 16/8100 loss 12.205297 loss_att 14.288410 loss_ctc 13.367340 loss_rnnt 11.554548 hw_loss 0.148474 lr 0.00042027 rank 1
2023-02-21 16:13:37,998 DEBUG TRAIN Batch 16/8200 loss 7.705593 loss_att 13.310092 loss_ctc 11.593468 loss_rnnt 6.009132 hw_loss 0.107208 lr 0.00042013 rank 3
2023-02-21 16:13:37,999 DEBUG TRAIN Batch 16/8200 loss 5.646198 loss_att 6.858427 loss_ctc 6.148021 loss_rnnt 5.201785 hw_loss 0.253232 lr 0.00042012 rank 4
2023-02-21 16:13:38,000 DEBUG TRAIN Batch 16/8200 loss 11.099703 loss_att 11.390273 loss_ctc 13.801935 loss_rnnt 10.476859 hw_loss 0.383310 lr 0.00042012 rank 2
2023-02-21 16:13:38,000 DEBUG TRAIN Batch 16/8200 loss 12.376570 loss_att 14.688526 loss_ctc 15.410309 loss_rnnt 11.405324 hw_loss 0.195664 lr 0.00042004 rank 7
2023-02-21 16:13:38,002 DEBUG TRAIN Batch 16/8200 loss 12.323008 loss_att 14.603839 loss_ctc 16.441290 loss_rnnt 11.284412 hw_loss 0.062484 lr 0.00042013 rank 1
2023-02-21 16:13:38,006 DEBUG TRAIN Batch 16/8200 loss 10.733254 loss_att 12.102427 loss_ctc 13.996379 loss_rnnt 9.969917 hw_loss 0.102037 lr 0.00042010 rank 6
2023-02-21 16:13:38,007 DEBUG TRAIN Batch 16/8200 loss 7.716052 loss_att 11.263414 loss_ctc 12.807034 loss_rnnt 6.235188 hw_loss 0.173612 lr 0.00042011 rank 0
2023-02-21 16:13:38,012 DEBUG TRAIN Batch 16/8200 loss 9.508434 loss_att 13.065380 loss_ctc 14.427603 loss_rnnt 8.000787 hw_loss 0.263189 lr 0.00042005 rank 5
2023-02-21 16:14:56,378 DEBUG TRAIN Batch 16/8300 loss 12.894197 loss_att 14.471842 loss_ctc 17.218031 loss_rnnt 11.891034 hw_loss 0.208355 lr 0.00041997 rank 4
2023-02-21 16:14:56,379 DEBUG TRAIN Batch 16/8300 loss 2.703775 loss_att 5.474800 loss_ctc 3.798304 loss_rnnt 1.952875 hw_loss 0.095172 lr 0.00041998 rank 2
2023-02-21 16:14:56,381 DEBUG TRAIN Batch 16/8300 loss 15.440825 loss_att 17.564631 loss_ctc 22.434315 loss_rnnt 14.061721 hw_loss 0.041019 lr 0.00041989 rank 7
2023-02-21 16:14:56,382 DEBUG TRAIN Batch 16/8300 loss 16.310314 loss_att 18.009123 loss_ctc 21.477467 loss_rnnt 15.252813 hw_loss 0.053975 lr 0.00041998 rank 1
2023-02-21 16:14:56,383 DEBUG TRAIN Batch 16/8300 loss 18.555790 loss_att 21.217537 loss_ctc 23.724157 loss_rnnt 17.264317 hw_loss 0.131264 lr 0.00041995 rank 6
2023-02-21 16:14:56,388 DEBUG TRAIN Batch 16/8300 loss 19.473742 loss_att 24.168814 loss_ctc 27.303957 loss_rnnt 17.393539 hw_loss 0.182170 lr 0.00041990 rank 5
2023-02-21 16:14:56,389 DEBUG TRAIN Batch 16/8300 loss 11.563462 loss_att 12.381363 loss_ctc 17.259987 loss_rnnt 10.564029 hw_loss 0.143095 lr 0.00041998 rank 3
2023-02-21 16:14:56,428 DEBUG TRAIN Batch 16/8300 loss 17.292238 loss_att 19.007847 loss_ctc 21.823133 loss_rnnt 16.257086 hw_loss 0.164833 lr 0.00041997 rank 0
2023-02-21 16:15:52,244 DEBUG CV Batch 16/0 loss 1.776512 loss_att 1.818816 loss_ctc 1.831408 loss_rnnt 1.435459 hw_loss 0.609887 history loss 1.710715 rank 2
2023-02-21 16:15:52,247 DEBUG CV Batch 16/0 loss 1.776512 loss_att 1.818816 loss_ctc 1.831408 loss_rnnt 1.435459 hw_loss 0.609887 history loss 1.710715 rank 5
2023-02-21 16:15:52,248 DEBUG CV Batch 16/0 loss 1.776512 loss_att 1.818816 loss_ctc 1.831408 loss_rnnt 1.435459 hw_loss 0.609887 history loss 1.710715 rank 4
2023-02-21 16:15:52,250 DEBUG CV Batch 16/0 loss 1.776512 loss_att 1.818816 loss_ctc 1.831408 loss_rnnt 1.435459 hw_loss 0.609887 history loss 1.710715 rank 1
2023-02-21 16:15:52,251 DEBUG CV Batch 16/0 loss 1.776512 loss_att 1.818816 loss_ctc 1.831408 loss_rnnt 1.435459 hw_loss 0.609887 history loss 1.710715 rank 0
2023-02-21 16:15:52,266 DEBUG CV Batch 16/0 loss 1.776512 loss_att 1.818816 loss_ctc 1.831408 loss_rnnt 1.435459 hw_loss 0.609887 history loss 1.710715 rank 7
2023-02-21 16:15:52,284 DEBUG CV Batch 16/0 loss 1.776512 loss_att 1.818816 loss_ctc 1.831408 loss_rnnt 1.435459 hw_loss 0.609887 history loss 1.710715 rank 3
2023-02-21 16:15:52,294 DEBUG CV Batch 16/0 loss 1.776512 loss_att 1.818816 loss_ctc 1.831408 loss_rnnt 1.435459 hw_loss 0.609887 history loss 1.710715 rank 6
2023-02-21 16:16:03,376 DEBUG CV Batch 16/100 loss 6.428105 loss_att 8.000051 loss_ctc 9.131812 loss_rnnt 5.643737 hw_loss 0.205282 history loss 3.729836 rank 7
2023-02-21 16:16:03,381 DEBUG CV Batch 16/100 loss 6.428105 loss_att 8.000051 loss_ctc 9.131812 loss_rnnt 5.643737 hw_loss 0.205282 history loss 3.729836 rank 1
2023-02-21 16:16:03,446 DEBUG CV Batch 16/100 loss 6.428105 loss_att 8.000051 loss_ctc 9.131812 loss_rnnt 5.643737 hw_loss 0.205282 history loss 3.729836 rank 4
2023-02-21 16:16:03,456 DEBUG CV Batch 16/100 loss 6.428105 loss_att 8.000051 loss_ctc 9.131812 loss_rnnt 5.643737 hw_loss 0.205282 history loss 3.729836 rank 2
2023-02-21 16:16:03,514 DEBUG CV Batch 16/100 loss 6.428105 loss_att 8.000051 loss_ctc 9.131812 loss_rnnt 5.643737 hw_loss 0.205282 history loss 3.729836 rank 3
2023-02-21 16:16:03,588 DEBUG CV Batch 16/100 loss 6.428105 loss_att 8.000051 loss_ctc 9.131812 loss_rnnt 5.643737 hw_loss 0.205282 history loss 3.729836 rank 6
2023-02-21 16:16:03,655 DEBUG CV Batch 16/100 loss 6.428105 loss_att 8.000051 loss_ctc 9.131812 loss_rnnt 5.643737 hw_loss 0.205282 history loss 3.729836 rank 0
2023-02-21 16:16:04,106 DEBUG CV Batch 16/100 loss 6.428105 loss_att 8.000051 loss_ctc 9.131812 loss_rnnt 5.643737 hw_loss 0.205282 history loss 3.729836 rank 5
2023-02-21 16:16:16,620 DEBUG CV Batch 16/200 loss 4.962498 loss_att 13.700869 loss_ctc 4.750795 loss_rnnt 3.218830 hw_loss 0.045415 history loss 4.333278 rank 7
2023-02-21 16:16:16,679 DEBUG CV Batch 16/200 loss 4.962498 loss_att 13.700869 loss_ctc 4.750795 loss_rnnt 3.218830 hw_loss 0.045415 history loss 4.333278 rank 2
2023-02-21 16:16:16,695 DEBUG CV Batch 16/200 loss 4.962498 loss_att 13.700869 loss_ctc 4.750795 loss_rnnt 3.218830 hw_loss 0.045415 history loss 4.333278 rank 4
2023-02-21 16:16:16,726 DEBUG CV Batch 16/200 loss 4.962498 loss_att 13.700869 loss_ctc 4.750795 loss_rnnt 3.218830 hw_loss 0.045415 history loss 4.333278 rank 1
2023-02-21 16:16:16,901 DEBUG CV Batch 16/200 loss 4.962498 loss_att 13.700869 loss_ctc 4.750795 loss_rnnt 3.218830 hw_loss 0.045415 history loss 4.333278 rank 3
2023-02-21 16:16:17,003 DEBUG CV Batch 16/200 loss 4.962498 loss_att 13.700869 loss_ctc 4.750795 loss_rnnt 3.218830 hw_loss 0.045415 history loss 4.333278 rank 6
2023-02-21 16:16:17,072 DEBUG CV Batch 16/200 loss 4.962498 loss_att 13.700869 loss_ctc 4.750795 loss_rnnt 3.218830 hw_loss 0.045415 history loss 4.333278 rank 0
2023-02-21 16:16:17,848 DEBUG CV Batch 16/200 loss 4.962498 loss_att 13.700869 loss_ctc 4.750795 loss_rnnt 3.218830 hw_loss 0.045415 history loss 4.333278 rank 5
2023-02-21 16:16:28,615 DEBUG CV Batch 16/300 loss 5.882499 loss_att 5.700546 loss_ctc 6.949172 loss_rnnt 5.647739 hw_loss 0.241738 history loss 4.446941 rank 7
2023-02-21 16:16:28,743 DEBUG CV Batch 16/300 loss 5.882499 loss_att 5.700546 loss_ctc 6.949172 loss_rnnt 5.647739 hw_loss 0.241738 history loss 4.446941 rank 4
2023-02-21 16:16:28,753 DEBUG CV Batch 16/300 loss 5.882499 loss_att 5.700546 loss_ctc 6.949172 loss_rnnt 5.647739 hw_loss 0.241738 history loss 4.446941 rank 1
2023-02-21 16:16:28,840 DEBUG CV Batch 16/300 loss 5.882499 loss_att 5.700546 loss_ctc 6.949172 loss_rnnt 5.647739 hw_loss 0.241738 history loss 4.446941 rank 2
2023-02-21 16:16:29,106 DEBUG CV Batch 16/300 loss 5.882499 loss_att 5.700546 loss_ctc 6.949172 loss_rnnt 5.647739 hw_loss 0.241738 history loss 4.446941 rank 3
2023-02-21 16:16:29,133 DEBUG CV Batch 16/300 loss 5.882499 loss_att 5.700546 loss_ctc 6.949172 loss_rnnt 5.647739 hw_loss 0.241738 history loss 4.446941 rank 6
2023-02-21 16:16:29,285 DEBUG CV Batch 16/300 loss 5.882499 loss_att 5.700546 loss_ctc 6.949172 loss_rnnt 5.647739 hw_loss 0.241738 history loss 4.446941 rank 0
2023-02-21 16:16:30,031 DEBUG CV Batch 16/300 loss 5.882499 loss_att 5.700546 loss_ctc 6.949172 loss_rnnt 5.647739 hw_loss 0.241738 history loss 4.446941 rank 5
2023-02-21 16:16:40,548 DEBUG CV Batch 16/400 loss 20.528278 loss_att 104.732185 loss_ctc 8.630560 loss_rnnt 5.242847 hw_loss 0.058146 history loss 5.481997 rank 7
2023-02-21 16:16:40,646 DEBUG CV Batch 16/400 loss 20.528278 loss_att 104.732185 loss_ctc 8.630560 loss_rnnt 5.242847 hw_loss 0.058146 history loss 5.481997 rank 4
2023-02-21 16:16:40,665 DEBUG CV Batch 16/400 loss 20.528278 loss_att 104.732185 loss_ctc 8.630560 loss_rnnt 5.242847 hw_loss 0.058146 history loss 5.481997 rank 1
2023-02-21 16:16:40,812 DEBUG CV Batch 16/400 loss 20.528278 loss_att 104.732185 loss_ctc 8.630560 loss_rnnt 5.242847 hw_loss 0.058146 history loss 5.481997 rank 2
2023-02-21 16:16:41,112 DEBUG CV Batch 16/400 loss 20.528278 loss_att 104.732185 loss_ctc 8.630560 loss_rnnt 5.242847 hw_loss 0.058146 history loss 5.481997 rank 3
2023-02-21 16:16:41,173 DEBUG CV Batch 16/400 loss 20.528278 loss_att 104.732185 loss_ctc 8.630560 loss_rnnt 5.242847 hw_loss 0.058146 history loss 5.481997 rank 6
2023-02-21 16:16:41,897 DEBUG CV Batch 16/400 loss 20.528278 loss_att 104.732185 loss_ctc 8.630560 loss_rnnt 5.242847 hw_loss 0.058146 history loss 5.481997 rank 0
2023-02-21 16:16:42,024 DEBUG CV Batch 16/400 loss 20.528278 loss_att 104.732185 loss_ctc 8.630560 loss_rnnt 5.242847 hw_loss 0.058146 history loss 5.481997 rank 5
2023-02-21 16:16:51,004 DEBUG CV Batch 16/500 loss 6.004070 loss_att 6.247935 loss_ctc 7.978255 loss_rnnt 5.644016 hw_loss 0.090108 history loss 6.348008 rank 7
2023-02-21 16:16:51,150 DEBUG CV Batch 16/500 loss 6.004070 loss_att 6.247935 loss_ctc 7.978255 loss_rnnt 5.644016 hw_loss 0.090108 history loss 6.348008 rank 4
2023-02-21 16:16:51,337 DEBUG CV Batch 16/500 loss 6.004070 loss_att 6.247935 loss_ctc 7.978255 loss_rnnt 5.644016 hw_loss 0.090108 history loss 6.348008 rank 2
2023-02-21 16:16:51,432 DEBUG CV Batch 16/500 loss 6.004070 loss_att 6.247935 loss_ctc 7.978255 loss_rnnt 5.644016 hw_loss 0.090108 history loss 6.348008 rank 1
2023-02-21 16:16:51,760 DEBUG CV Batch 16/500 loss 6.004070 loss_att 6.247935 loss_ctc 7.978255 loss_rnnt 5.644016 hw_loss 0.090108 history loss 6.348008 rank 6
2023-02-21 16:16:52,064 DEBUG CV Batch 16/500 loss 6.004070 loss_att 6.247935 loss_ctc 7.978255 loss_rnnt 5.644016 hw_loss 0.090108 history loss 6.348008 rank 3
2023-02-21 16:16:52,636 DEBUG CV Batch 16/500 loss 6.004070 loss_att 6.247935 loss_ctc 7.978255 loss_rnnt 5.644016 hw_loss 0.090108 history loss 6.348008 rank 0
2023-02-21 16:16:52,936 DEBUG CV Batch 16/500 loss 6.004070 loss_att 6.247935 loss_ctc 7.978255 loss_rnnt 5.644016 hw_loss 0.090108 history loss 6.348008 rank 5
2023-02-21 16:17:02,914 DEBUG CV Batch 16/600 loss 6.673821 loss_att 7.090230 loss_ctc 9.287864 loss_rnnt 6.089739 hw_loss 0.285488 history loss 7.411730 rank 7
2023-02-21 16:17:03,044 DEBUG CV Batch 16/600 loss 6.673821 loss_att 7.090230 loss_ctc 9.287864 loss_rnnt 6.089739 hw_loss 0.285488 history loss 7.411730 rank 4
2023-02-21 16:17:03,270 DEBUG CV Batch 16/600 loss 6.673821 loss_att 7.090230 loss_ctc 9.287864 loss_rnnt 6.089739 hw_loss 0.285488 history loss 7.411730 rank 2
2023-02-21 16:17:03,358 DEBUG CV Batch 16/600 loss 6.673821 loss_att 7.090230 loss_ctc 9.287864 loss_rnnt 6.089739 hw_loss 0.285488 history loss 7.411730 rank 1
2023-02-21 16:17:03,869 DEBUG CV Batch 16/600 loss 6.673821 loss_att 7.090230 loss_ctc 9.287864 loss_rnnt 6.089739 hw_loss 0.285488 history loss 7.411730 rank 6
2023-02-21 16:17:04,445 DEBUG CV Batch 16/600 loss 6.673821 loss_att 7.090230 loss_ctc 9.287864 loss_rnnt 6.089739 hw_loss 0.285488 history loss 7.411730 rank 3
2023-02-21 16:17:04,772 DEBUG CV Batch 16/600 loss 6.673821 loss_att 7.090230 loss_ctc 9.287864 loss_rnnt 6.089739 hw_loss 0.285488 history loss 7.411730 rank 0
2023-02-21 16:17:05,105 DEBUG CV Batch 16/600 loss 6.673821 loss_att 7.090230 loss_ctc 9.287864 loss_rnnt 6.089739 hw_loss 0.285488 history loss 7.411730 rank 5
2023-02-21 16:17:14,186 DEBUG CV Batch 16/700 loss 18.619390 loss_att 56.711060 loss_ctc 20.434662 loss_rnnt 10.746185 hw_loss 0.024064 history loss 8.110983 rank 7
2023-02-21 16:17:14,343 DEBUG CV Batch 16/700 loss 18.619390 loss_att 56.711060 loss_ctc 20.434662 loss_rnnt 10.746185 hw_loss 0.024064 history loss 8.110983 rank 4
2023-02-21 16:17:14,615 DEBUG CV Batch 16/700 loss 18.619390 loss_att 56.711060 loss_ctc 20.434662 loss_rnnt 10.746185 hw_loss 0.024064 history loss 8.110983 rank 2
2023-02-21 16:17:14,703 DEBUG CV Batch 16/700 loss 18.619390 loss_att 56.711060 loss_ctc 20.434662 loss_rnnt 10.746185 hw_loss 0.024064 history loss 8.110983 rank 1
2023-02-21 16:17:15,258 DEBUG CV Batch 16/700 loss 18.619390 loss_att 56.711060 loss_ctc 20.434662 loss_rnnt 10.746185 hw_loss 0.024064 history loss 8.110983 rank 6
2023-02-21 16:17:16,190 DEBUG CV Batch 16/700 loss 18.619390 loss_att 56.711060 loss_ctc 20.434662 loss_rnnt 10.746185 hw_loss 0.024064 history loss 8.110983 rank 0
2023-02-21 16:17:16,423 DEBUG CV Batch 16/700 loss 18.619390 loss_att 56.711060 loss_ctc 20.434662 loss_rnnt 10.746185 hw_loss 0.024064 history loss 8.110983 rank 3
2023-02-21 16:17:16,475 DEBUG CV Batch 16/700 loss 18.619390 loss_att 56.711060 loss_ctc 20.434662 loss_rnnt 10.746185 hw_loss 0.024064 history loss 8.110983 rank 5
2023-02-21 16:17:25,379 DEBUG CV Batch 16/800 loss 10.622452 loss_att 11.409158 loss_ctc 16.543983 loss_rnnt 9.630524 hw_loss 0.084470 history loss 7.528932 rank 7
2023-02-21 16:17:25,458 DEBUG CV Batch 16/800 loss 10.622452 loss_att 11.409158 loss_ctc 16.543983 loss_rnnt 9.630524 hw_loss 0.084470 history loss 7.528932 rank 4
2023-02-21 16:17:25,779 DEBUG CV Batch 16/800 loss 10.622452 loss_att 11.409158 loss_ctc 16.543983 loss_rnnt 9.630524 hw_loss 0.084470 history loss 7.528932 rank 2
2023-02-21 16:17:25,888 DEBUG CV Batch 16/800 loss 10.622452 loss_att 11.409158 loss_ctc 16.543983 loss_rnnt 9.630524 hw_loss 0.084470 history loss 7.528932 rank 1
2023-02-21 16:17:26,571 DEBUG CV Batch 16/800 loss 10.622452 loss_att 11.409158 loss_ctc 16.543983 loss_rnnt 9.630524 hw_loss 0.084470 history loss 7.528932 rank 6
2023-02-21 16:17:27,405 DEBUG CV Batch 16/800 loss 10.622452 loss_att 11.409158 loss_ctc 16.543983 loss_rnnt 9.630524 hw_loss 0.084470 history loss 7.528932 rank 0
2023-02-21 16:17:27,753 DEBUG CV Batch 16/800 loss 10.622452 loss_att 11.409158 loss_ctc 16.543983 loss_rnnt 9.630524 hw_loss 0.084470 history loss 7.528932 rank 5
2023-02-21 16:17:28,584 DEBUG CV Batch 16/800 loss 10.622452 loss_att 11.409158 loss_ctc 16.543983 loss_rnnt 9.630524 hw_loss 0.084470 history loss 7.528932 rank 3
2023-02-21 16:17:38,624 DEBUG CV Batch 16/900 loss 10.980684 loss_att 18.091055 loss_ctc 16.171368 loss_rnnt 8.819912 hw_loss 0.087388 history loss 7.303044 rank 7
2023-02-21 16:17:38,635 DEBUG CV Batch 16/900 loss 10.980684 loss_att 18.091055 loss_ctc 16.171368 loss_rnnt 8.819912 hw_loss 0.087388 history loss 7.303044 rank 4
2023-02-21 16:17:38,918 DEBUG CV Batch 16/900 loss 10.980684 loss_att 18.091055 loss_ctc 16.171368 loss_rnnt 8.819912 hw_loss 0.087388 history loss 7.303044 rank 2
2023-02-21 16:17:39,019 DEBUG CV Batch 16/900 loss 10.980684 loss_att 18.091055 loss_ctc 16.171368 loss_rnnt 8.819912 hw_loss 0.087388 history loss 7.303044 rank 1
2023-02-21 16:17:39,824 DEBUG CV Batch 16/900 loss 10.980684 loss_att 18.091055 loss_ctc 16.171368 loss_rnnt 8.819912 hw_loss 0.087388 history loss 7.303044 rank 6
2023-02-21 16:17:40,700 DEBUG CV Batch 16/900 loss 10.980684 loss_att 18.091055 loss_ctc 16.171368 loss_rnnt 8.819912 hw_loss 0.087388 history loss 7.303044 rank 0
2023-02-21 16:17:41,590 DEBUG CV Batch 16/900 loss 10.980684 loss_att 18.091055 loss_ctc 16.171368 loss_rnnt 8.819912 hw_loss 0.087388 history loss 7.303044 rank 5
2023-02-21 16:17:41,964 DEBUG CV Batch 16/900 loss 10.980684 loss_att 18.091055 loss_ctc 16.171368 loss_rnnt 8.819912 hw_loss 0.087388 history loss 7.303044 rank 3
2023-02-21 16:17:50,714 DEBUG CV Batch 16/1000 loss 4.277581 loss_att 5.288498 loss_ctc 5.559444 loss_rnnt 3.779800 hw_loss 0.233780 history loss 7.030984 rank 4
2023-02-21 16:17:50,912 DEBUG CV Batch 16/1000 loss 4.277581 loss_att 5.288498 loss_ctc 5.559444 loss_rnnt 3.779800 hw_loss 0.233780 history loss 7.030984 rank 2
2023-02-21 16:17:51,007 DEBUG CV Batch 16/1000 loss 4.277581 loss_att 5.288498 loss_ctc 5.559444 loss_rnnt 3.779800 hw_loss 0.233780 history loss 7.030984 rank 7
2023-02-21 16:17:51,039 DEBUG CV Batch 16/1000 loss 4.277581 loss_att 5.288498 loss_ctc 5.559444 loss_rnnt 3.779800 hw_loss 0.233780 history loss 7.030984 rank 1
2023-02-21 16:17:52,008 DEBUG CV Batch 16/1000 loss 4.277581 loss_att 5.288498 loss_ctc 5.559444 loss_rnnt 3.779800 hw_loss 0.233780 history loss 7.030984 rank 6
2023-02-21 16:17:52,843 DEBUG CV Batch 16/1000 loss 4.277581 loss_att 5.288498 loss_ctc 5.559444 loss_rnnt 3.779800 hw_loss 0.233780 history loss 7.030984 rank 0
2023-02-21 16:17:53,918 DEBUG CV Batch 16/1000 loss 4.277581 loss_att 5.288498 loss_ctc 5.559444 loss_rnnt 3.779800 hw_loss 0.233780 history loss 7.030984 rank 5
2023-02-21 16:17:54,098 DEBUG CV Batch 16/1000 loss 4.277581 loss_att 5.288498 loss_ctc 5.559444 loss_rnnt 3.779800 hw_loss 0.233780 history loss 7.030984 rank 3
2023-02-21 16:18:02,509 DEBUG CV Batch 16/1100 loss 7.494137 loss_att 6.793096 loss_ctc 9.405880 loss_rnnt 7.153344 hw_loss 0.423940 history loss 7.007241 rank 4
2023-02-21 16:18:02,748 DEBUG CV Batch 16/1100 loss 7.494137 loss_att 6.793096 loss_ctc 9.405880 loss_rnnt 7.153344 hw_loss 0.423940 history loss 7.007241 rank 2
2023-02-21 16:18:02,835 DEBUG CV Batch 16/1100 loss 7.494137 loss_att 6.793096 loss_ctc 9.405880 loss_rnnt 7.153344 hw_loss 0.423940 history loss 7.007241 rank 7
2023-02-21 16:18:02,836 DEBUG CV Batch 16/1100 loss 7.494137 loss_att 6.793096 loss_ctc 9.405880 loss_rnnt 7.153344 hw_loss 0.423940 history loss 7.007241 rank 1
2023-02-21 16:18:03,927 DEBUG CV Batch 16/1100 loss 7.494137 loss_att 6.793096 loss_ctc 9.405880 loss_rnnt 7.153344 hw_loss 0.423940 history loss 7.007241 rank 6
2023-02-21 16:18:04,744 DEBUG CV Batch 16/1100 loss 7.494137 loss_att 6.793096 loss_ctc 9.405880 loss_rnnt 7.153344 hw_loss 0.423940 history loss 7.007241 rank 0
2023-02-21 16:18:05,941 DEBUG CV Batch 16/1100 loss 7.494137 loss_att 6.793096 loss_ctc 9.405880 loss_rnnt 7.153344 hw_loss 0.423940 history loss 7.007241 rank 5
2023-02-21 16:18:06,026 DEBUG CV Batch 16/1100 loss 7.494137 loss_att 6.793096 loss_ctc 9.405880 loss_rnnt 7.153344 hw_loss 0.423940 history loss 7.007241 rank 3
2023-02-21 16:18:12,981 DEBUG CV Batch 16/1200 loss 6.537171 loss_att 8.489702 loss_ctc 7.931663 loss_rnnt 5.895063 hw_loss 0.123130 history loss 7.378813 rank 4
2023-02-21 16:18:13,205 DEBUG CV Batch 16/1200 loss 6.537171 loss_att 8.489702 loss_ctc 7.931663 loss_rnnt 5.895063 hw_loss 0.123130 history loss 7.378813 rank 2
2023-02-21 16:18:13,266 DEBUG CV Batch 16/1200 loss 6.537171 loss_att 8.489702 loss_ctc 7.931663 loss_rnnt 5.895063 hw_loss 0.123130 history loss 7.378813 rank 7
2023-02-21 16:18:13,332 DEBUG CV Batch 16/1200 loss 6.537171 loss_att 8.489702 loss_ctc 7.931663 loss_rnnt 5.895063 hw_loss 0.123130 history loss 7.378813 rank 1
2023-02-21 16:18:14,571 DEBUG CV Batch 16/1200 loss 6.537171 loss_att 8.489702 loss_ctc 7.931663 loss_rnnt 5.895063 hw_loss 0.123130 history loss 7.378813 rank 6
2023-02-21 16:18:15,451 DEBUG CV Batch 16/1200 loss 6.537171 loss_att 8.489702 loss_ctc 7.931663 loss_rnnt 5.895063 hw_loss 0.123130 history loss 7.378813 rank 0
2023-02-21 16:18:16,671 DEBUG CV Batch 16/1200 loss 6.537171 loss_att 8.489702 loss_ctc 7.931663 loss_rnnt 5.895063 hw_loss 0.123130 history loss 7.378813 rank 3
2023-02-21 16:18:16,827 DEBUG CV Batch 16/1200 loss 6.537171 loss_att 8.489702 loss_ctc 7.931663 loss_rnnt 5.895063 hw_loss 0.123130 history loss 7.378813 rank 5
2023-02-21 16:18:24,839 DEBUG CV Batch 16/1300 loss 5.812066 loss_att 5.499660 loss_ctc 7.798365 loss_rnnt 5.450525 hw_loss 0.298466 history loss 7.740303 rank 4
2023-02-21 16:18:25,118 DEBUG CV Batch 16/1300 loss 5.812066 loss_att 5.499660 loss_ctc 7.798365 loss_rnnt 5.450525 hw_loss 0.298466 history loss 7.740303 rank 7
2023-02-21 16:18:25,155 DEBUG CV Batch 16/1300 loss 5.812066 loss_att 5.499660 loss_ctc 7.798365 loss_rnnt 5.450525 hw_loss 0.298466 history loss 7.740303 rank 2
2023-02-21 16:18:25,212 DEBUG CV Batch 16/1300 loss 5.812066 loss_att 5.499660 loss_ctc 7.798365 loss_rnnt 5.450525 hw_loss 0.298466 history loss 7.740303 rank 1
2023-02-21 16:18:27,478 DEBUG CV Batch 16/1300 loss 5.812066 loss_att 5.499660 loss_ctc 7.798365 loss_rnnt 5.450525 hw_loss 0.298466 history loss 7.740303 rank 0
2023-02-21 16:18:27,524 DEBUG CV Batch 16/1300 loss 5.812066 loss_att 5.499660 loss_ctc 7.798365 loss_rnnt 5.450525 hw_loss 0.298466 history loss 7.740303 rank 6
2023-02-21 16:18:28,764 DEBUG CV Batch 16/1300 loss 5.812066 loss_att 5.499660 loss_ctc 7.798365 loss_rnnt 5.450525 hw_loss 0.298466 history loss 7.740303 rank 3
2023-02-21 16:18:28,970 DEBUG CV Batch 16/1300 loss 5.812066 loss_att 5.499660 loss_ctc 7.798365 loss_rnnt 5.450525 hw_loss 0.298466 history loss 7.740303 rank 5
2023-02-21 16:18:35,947 DEBUG CV Batch 16/1400 loss 10.159598 loss_att 32.550171 loss_ctc 8.276771 loss_rnnt 5.895232 hw_loss 0.069928 history loss 8.073798 rank 4
2023-02-21 16:18:36,204 DEBUG CV Batch 16/1400 loss 10.159598 loss_att 32.550171 loss_ctc 8.276771 loss_rnnt 5.895232 hw_loss 0.069928 history loss 8.073798 rank 7
2023-02-21 16:18:36,284 DEBUG CV Batch 16/1400 loss 10.159598 loss_att 32.550171 loss_ctc 8.276771 loss_rnnt 5.895232 hw_loss 0.069928 history loss 8.073798 rank 1
2023-02-21 16:18:36,364 DEBUG CV Batch 16/1400 loss 10.159598 loss_att 32.550171 loss_ctc 8.276771 loss_rnnt 5.895232 hw_loss 0.069928 history loss 8.073798 rank 2
2023-02-21 16:18:38,716 DEBUG CV Batch 16/1400 loss 10.159598 loss_att 32.550171 loss_ctc 8.276771 loss_rnnt 5.895232 hw_loss 0.069928 history loss 8.073798 rank 0
2023-02-21 16:18:39,412 DEBUG CV Batch 16/1400 loss 10.159598 loss_att 32.550171 loss_ctc 8.276771 loss_rnnt 5.895232 hw_loss 0.069928 history loss 8.073798 rank 6
2023-02-21 16:18:40,006 DEBUG CV Batch 16/1400 loss 10.159598 loss_att 32.550171 loss_ctc 8.276771 loss_rnnt 5.895232 hw_loss 0.069928 history loss 8.073798 rank 3
2023-02-21 16:18:40,273 DEBUG CV Batch 16/1400 loss 10.159598 loss_att 32.550171 loss_ctc 8.276771 loss_rnnt 5.895232 hw_loss 0.069928 history loss 8.073798 rank 5
2023-02-21 16:18:47,361 DEBUG CV Batch 16/1500 loss 7.940711 loss_att 8.277027 loss_ctc 7.383933 loss_rnnt 7.912336 hw_loss 0.066280 history loss 7.894431 rank 4
2023-02-21 16:18:47,451 DEBUG CV Batch 16/1500 loss 7.940711 loss_att 8.277027 loss_ctc 7.383933 loss_rnnt 7.912336 hw_loss 0.066280 history loss 7.894431 rank 7
2023-02-21 16:18:47,587 DEBUG CV Batch 16/1500 loss 7.940711 loss_att 8.277027 loss_ctc 7.383933 loss_rnnt 7.912336 hw_loss 0.066280 history loss 7.894431 rank 1
2023-02-21 16:18:47,717 DEBUG CV Batch 16/1500 loss 7.940711 loss_att 8.277027 loss_ctc 7.383933 loss_rnnt 7.912336 hw_loss 0.066280 history loss 7.894431 rank 2
2023-02-21 16:18:50,243 DEBUG CV Batch 16/1500 loss 7.940711 loss_att 8.277027 loss_ctc 7.383933 loss_rnnt 7.912336 hw_loss 0.066280 history loss 7.894431 rank 0
2023-02-21 16:18:51,417 DEBUG CV Batch 16/1500 loss 7.940711 loss_att 8.277027 loss_ctc 7.383933 loss_rnnt 7.912336 hw_loss 0.066280 history loss 7.894431 rank 3
2023-02-21 16:18:51,643 DEBUG CV Batch 16/1500 loss 7.940711 loss_att 8.277027 loss_ctc 7.383933 loss_rnnt 7.912336 hw_loss 0.066280 history loss 7.894431 rank 5
2023-02-21 16:18:51,854 DEBUG CV Batch 16/1500 loss 7.940711 loss_att 8.277027 loss_ctc 7.383933 loss_rnnt 7.912336 hw_loss 0.066280 history loss 7.894431 rank 6
2023-02-21 16:19:00,220 DEBUG CV Batch 16/1600 loss 7.659327 loss_att 12.835093 loss_ctc 11.466936 loss_rnnt 6.103658 hw_loss 0.024064 history loss 7.816076 rank 7
2023-02-21 16:19:00,240 DEBUG CV Batch 16/1600 loss 7.659327 loss_att 12.835093 loss_ctc 11.466936 loss_rnnt 6.103658 hw_loss 0.024064 history loss 7.816076 rank 4
2023-02-21 16:19:00,571 DEBUG CV Batch 16/1600 loss 7.659327 loss_att 12.835093 loss_ctc 11.466936 loss_rnnt 6.103658 hw_loss 0.024064 history loss 7.816076 rank 2
2023-02-21 16:19:00,659 DEBUG CV Batch 16/1600 loss 7.659327 loss_att 12.835093 loss_ctc 11.466936 loss_rnnt 6.103658 hw_loss 0.024064 history loss 7.816076 rank 1
2023-02-21 16:19:03,195 DEBUG CV Batch 16/1600 loss 7.659327 loss_att 12.835093 loss_ctc 11.466936 loss_rnnt 6.103658 hw_loss 0.024064 history loss 7.816076 rank 0
2023-02-21 16:19:04,378 DEBUG CV Batch 16/1600 loss 7.659327 loss_att 12.835093 loss_ctc 11.466936 loss_rnnt 6.103658 hw_loss 0.024064 history loss 7.816076 rank 3
2023-02-21 16:19:04,711 DEBUG CV Batch 16/1600 loss 7.659327 loss_att 12.835093 loss_ctc 11.466936 loss_rnnt 6.103658 hw_loss 0.024064 history loss 7.816076 rank 5
2023-02-21 16:19:04,913 DEBUG CV Batch 16/1600 loss 7.659327 loss_att 12.835093 loss_ctc 11.466936 loss_rnnt 6.103658 hw_loss 0.024064 history loss 7.816076 rank 6
2023-02-21 16:19:12,438 DEBUG CV Batch 16/1700 loss 8.084470 loss_att 7.582454 loss_ctc 12.334314 loss_rnnt 7.527555 hw_loss 0.170010 history loss 7.701961 rank 4
2023-02-21 16:19:12,441 DEBUG CV Batch 16/1700 loss 8.084470 loss_att 7.582454 loss_ctc 12.334314 loss_rnnt 7.527555 hw_loss 0.170010 history loss 7.701961 rank 7
2023-02-21 16:19:12,747 DEBUG CV Batch 16/1700 loss 8.084470 loss_att 7.582454 loss_ctc 12.334314 loss_rnnt 7.527555 hw_loss 0.170010 history loss 7.701961 rank 2
2023-02-21 16:19:12,843 DEBUG CV Batch 16/1700 loss 8.084470 loss_att 7.582454 loss_ctc 12.334314 loss_rnnt 7.527555 hw_loss 0.170010 history loss 7.701961 rank 1
2023-02-21 16:19:15,963 DEBUG CV Batch 16/1700 loss 8.084470 loss_att 7.582454 loss_ctc 12.334314 loss_rnnt 7.527555 hw_loss 0.170010 history loss 7.701961 rank 0
2023-02-21 16:19:16,507 DEBUG CV Batch 16/1700 loss 8.084470 loss_att 7.582454 loss_ctc 12.334314 loss_rnnt 7.527555 hw_loss 0.170010 history loss 7.701961 rank 3
2023-02-21 16:19:17,130 DEBUG CV Batch 16/1700 loss 8.084470 loss_att 7.582454 loss_ctc 12.334314 loss_rnnt 7.527555 hw_loss 0.170010 history loss 7.701961 rank 6
2023-02-21 16:19:17,359 DEBUG CV Batch 16/1700 loss 8.084470 loss_att 7.582454 loss_ctc 12.334314 loss_rnnt 7.527555 hw_loss 0.170010 history loss 7.701961 rank 5
2023-02-21 16:19:21,380 INFO Epoch 16 CV info cv_loss 7.662865935509166
2023-02-21 16:19:21,381 INFO Epoch 17 TRAIN info lr 0.0004199205350381198
2023-02-21 16:19:21,384 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 16:19:21,429 INFO Epoch 16 CV info cv_loss 7.662865936043273
2023-02-21 16:19:21,430 INFO Epoch 17 TRAIN info lr 0.0004198494689658057
2023-02-21 16:19:21,435 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 16:19:21,742 INFO Epoch 16 CV info cv_loss 7.662865935526395
2023-02-21 16:19:21,743 INFO Epoch 17 TRAIN info lr 0.00041989536167720315
2023-02-21 16:19:21,746 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 16:19:21,763 INFO Epoch 16 CV info cv_loss 7.662865937111488
2023-02-21 16:19:21,764 INFO Epoch 17 TRAIN info lr 0.0004199560816096214
2023-02-21 16:19:21,768 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 16:19:25,442 INFO Epoch 16 CV info cv_loss 7.662865934354805
2023-02-21 16:19:25,443 INFO Epoch 17 TRAIN info lr 0.0004198864780424315
2023-02-21 16:19:25,446 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 16:19:25,884 INFO Epoch 16 CV info cv_loss 7.662865934492639
2023-02-21 16:19:25,885 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/16.pt
2023-02-21 16:19:26,124 INFO Epoch 16 CV info cv_loss 7.662865935043976
2023-02-21 16:19:26,125 INFO Epoch 17 TRAIN info lr 0.0004198968423378188
2023-02-21 16:19:26,128 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 16:19:26,465 INFO Epoch 16 CV info cv_loss 7.662865934647702
2023-02-21 16:19:26,466 INFO Epoch 17 TRAIN info lr 0.000419859830520819
2023-02-21 16:19:26,471 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 16:19:27,545 INFO Epoch 17 TRAIN info lr 0.0004199309018557182
2023-02-21 16:19:27,551 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 16:20:41,685 DEBUG TRAIN Batch 17/0 loss 6.322552 loss_att 5.992439 loss_ctc 7.422758 loss_rnnt 6.027818 hw_loss 0.401366 lr 0.00041985 rank 7
2023-02-21 16:20:41,685 DEBUG TRAIN Batch 17/0 loss 7.695129 loss_att 7.227384 loss_ctc 10.745933 loss_rnnt 7.229758 hw_loss 0.285273 lr 0.00041992 rank 4
2023-02-21 16:20:41,688 DEBUG TRAIN Batch 17/0 loss 11.262156 loss_att 10.258986 loss_ctc 12.829608 loss_rnnt 11.000095 hw_loss 0.475688 lr 0.00041995 rank 1
2023-02-21 16:20:41,688 DEBUG TRAIN Batch 17/0 loss 11.330378 loss_att 10.164921 loss_ctc 13.341099 loss_rnnt 11.100431 hw_loss 0.365514 lr 0.00041989 rank 2
2023-02-21 16:20:41,694 DEBUG TRAIN Batch 17/0 loss 9.867771 loss_att 9.992361 loss_ctc 11.662143 loss_rnnt 9.444005 hw_loss 0.299248 lr 0.00041990 rank 6
2023-02-21 16:20:41,698 DEBUG TRAIN Batch 17/0 loss 11.585116 loss_att 10.948487 loss_ctc 13.414550 loss_rnnt 11.324971 hw_loss 0.269149 lr 0.00041988 rank 3
2023-02-21 16:20:41,707 DEBUG TRAIN Batch 17/0 loss 9.015608 loss_att 9.160431 loss_ctc 11.250741 loss_rnnt 8.499673 hw_loss 0.354284 lr 0.00041986 rank 5
2023-02-21 16:20:41,791 DEBUG TRAIN Batch 17/0 loss 8.879219 loss_att 8.446574 loss_ctc 11.221169 loss_rnnt 8.392988 hw_loss 0.488436 lr 0.00041993 rank 0
2023-02-21 16:21:57,972 DEBUG TRAIN Batch 17/100 loss 7.060491 loss_att 11.246251 loss_ctc 9.476854 loss_rnnt 5.784393 hw_loss 0.218931 lr 0.00041977 rank 4
2023-02-21 16:21:57,973 DEBUG TRAIN Batch 17/100 loss 9.904763 loss_att 10.819984 loss_ctc 12.727982 loss_rnnt 9.331959 hw_loss 0.024994 lr 0.00041975 rank 2
2023-02-21 16:21:57,974 DEBUG TRAIN Batch 17/100 loss 13.857029 loss_att 16.248886 loss_ctc 20.217487 loss_rnnt 12.446895 hw_loss 0.156943 lr 0.00041970 rank 7
2023-02-21 16:21:57,977 DEBUG TRAIN Batch 17/100 loss 11.144747 loss_att 14.464184 loss_ctc 13.689205 loss_rnnt 10.105413 hw_loss 0.067845 lr 0.00041978 rank 0
2023-02-21 16:21:57,977 DEBUG TRAIN Batch 17/100 loss 12.475128 loss_att 15.021629 loss_ctc 19.882101 loss_rnnt 10.899549 hw_loss 0.147531 lr 0.00041971 rank 5
2023-02-21 16:21:57,978 DEBUG TRAIN Batch 17/100 loss 8.049851 loss_att 9.069145 loss_ctc 9.638213 loss_rnnt 7.600337 hw_loss 0.063516 lr 0.00041975 rank 6
2023-02-21 16:21:57,978 DEBUG TRAIN Batch 17/100 loss 8.891113 loss_att 10.657474 loss_ctc 13.293209 loss_rnnt 7.937826 hw_loss 0.024505 lr 0.00041981 rank 1
2023-02-21 16:21:57,981 DEBUG TRAIN Batch 17/100 loss 15.201723 loss_att 20.400236 loss_ctc 27.648815 loss_rnnt 12.450638 hw_loss 0.097069 lr 0.00041974 rank 3
2023-02-21 16:23:15,398 DEBUG TRAIN Batch 17/200 loss 15.392248 loss_att 16.945358 loss_ctc 17.814013 loss_rnnt 14.701159 hw_loss 0.107933 lr 0.00041966 rank 1
2023-02-21 16:23:15,404 DEBUG TRAIN Batch 17/200 loss 12.046126 loss_att 15.959982 loss_ctc 13.498657 loss_rnnt 11.002582 hw_loss 0.125819 lr 0.00041962 rank 4
2023-02-21 16:23:15,407 DEBUG TRAIN Batch 17/200 loss 6.187812 loss_att 9.604855 loss_ctc 8.487270 loss_rnnt 5.133885 hw_loss 0.119858 lr 0.00041960 rank 2
2023-02-21 16:23:15,407 DEBUG TRAIN Batch 17/200 loss 6.823399 loss_att 9.267200 loss_ctc 11.126518 loss_rnnt 5.743596 hw_loss 0.032424 lr 0.00041955 rank 7
2023-02-21 16:23:15,408 DEBUG TRAIN Batch 17/200 loss 5.971398 loss_att 9.092451 loss_ctc 6.038818 loss_rnnt 5.287340 hw_loss 0.095358 lr 0.00041959 rank 3
2023-02-21 16:23:15,409 DEBUG TRAIN Batch 17/200 loss 6.417398 loss_att 8.337888 loss_ctc 8.302238 loss_rnnt 5.733516 hw_loss 0.090888 lr 0.00041963 rank 0
2023-02-21 16:23:15,409 DEBUG TRAIN Batch 17/200 loss 15.574179 loss_att 17.146870 loss_ctc 18.122522 loss_rnnt 14.844234 hw_loss 0.141803 lr 0.00041956 rank 5
2023-02-21 16:23:15,454 DEBUG TRAIN Batch 17/200 loss 6.419081 loss_att 11.154676 loss_ctc 8.871347 loss_rnnt 5.076244 hw_loss 0.128906 lr 0.00041960 rank 6
2023-02-21 16:24:36,948 DEBUG TRAIN Batch 17/300 loss 17.552612 loss_att 21.680756 loss_ctc 22.212944 loss_rnnt 16.012697 hw_loss 0.174202 lr 0.00041945 rank 6
2023-02-21 16:24:36,948 DEBUG TRAIN Batch 17/300 loss 10.124858 loss_att 12.810980 loss_ctc 15.364450 loss_rnnt 8.840411 hw_loss 0.091144 lr 0.00041948 rank 4
2023-02-21 16:24:36,951 DEBUG TRAIN Batch 17/300 loss 7.976890 loss_att 10.262878 loss_ctc 9.120356 loss_rnnt 7.307322 hw_loss 0.112327 lr 0.00041951 rank 1
2023-02-21 16:24:36,951 DEBUG TRAIN Batch 17/300 loss 15.836162 loss_att 20.476767 loss_ctc 24.231342 loss_rnnt 13.710727 hw_loss 0.146170 lr 0.00041944 rank 3
2023-02-21 16:24:36,953 DEBUG TRAIN Batch 17/300 loss 9.283462 loss_att 13.197028 loss_ctc 15.185557 loss_rnnt 7.659129 hw_loss 0.102513 lr 0.00041945 rank 2
2023-02-21 16:24:36,954 DEBUG TRAIN Batch 17/300 loss 7.556102 loss_att 10.921512 loss_ctc 9.943538 loss_rnnt 6.411499 hw_loss 0.287243 lr 0.00041949 rank 0
2023-02-21 16:24:36,956 DEBUG TRAIN Batch 17/300 loss 13.341530 loss_att 14.735777 loss_ctc 17.940496 loss_rnnt 12.334455 hw_loss 0.215682 lr 0.00041941 rank 5
2023-02-21 16:24:36,956 DEBUG TRAIN Batch 17/300 loss 7.872613 loss_att 10.003046 loss_ctc 8.635620 loss_rnnt 7.238352 hw_loss 0.199577 lr 0.00041940 rank 7
2023-02-21 16:25:56,940 DEBUG TRAIN Batch 17/400 loss 7.145414 loss_att 8.078506 loss_ctc 6.792384 loss_rnnt 6.880375 hw_loss 0.235296 lr 0.00041936 rank 1
2023-02-21 16:25:56,942 DEBUG TRAIN Batch 17/400 loss 17.109150 loss_att 19.155439 loss_ctc 22.805161 loss_rnnt 15.858250 hw_loss 0.154077 lr 0.00041933 rank 4
2023-02-21 16:25:56,946 DEBUG TRAIN Batch 17/400 loss 10.556151 loss_att 12.858620 loss_ctc 13.280910 loss_rnnt 9.657122 hw_loss 0.141066 lr 0.00041929 rank 3
2023-02-21 16:25:56,946 DEBUG TRAIN Batch 17/400 loss 9.687245 loss_att 12.210499 loss_ctc 10.904707 loss_rnnt 8.987453 hw_loss 0.061525 lr 0.00041926 rank 7
2023-02-21 16:25:56,947 DEBUG TRAIN Batch 17/400 loss 10.703745 loss_att 12.210927 loss_ctc 14.086835 loss_rnnt 9.856829 hw_loss 0.177001 lr 0.00041930 rank 2
2023-02-21 16:25:56,949 DEBUG TRAIN Batch 17/400 loss 9.540824 loss_att 13.911230 loss_ctc 12.159927 loss_rnnt 8.274363 hw_loss 0.080937 lr 0.00041930 rank 6
2023-02-21 16:25:56,951 DEBUG TRAIN Batch 17/400 loss 14.869328 loss_att 17.439432 loss_ctc 22.933075 loss_rnnt 13.230158 hw_loss 0.093717 lr 0.00041927 rank 5
2023-02-21 16:25:56,997 DEBUG TRAIN Batch 17/400 loss 13.813806 loss_att 13.240795 loss_ctc 18.476734 loss_rnnt 13.272772 hw_loss 0.063586 lr 0.00041934 rank 0
2023-02-21 16:27:15,296 DEBUG TRAIN Batch 17/500 loss 12.448040 loss_att 13.467063 loss_ctc 15.041392 loss_rnnt 11.774760 hw_loss 0.231928 lr 0.00041911 rank 7
2023-02-21 16:27:15,298 DEBUG TRAIN Batch 17/500 loss 12.598972 loss_att 15.310930 loss_ctc 19.307198 loss_rnnt 11.139488 hw_loss 0.042493 lr 0.00041916 rank 2
2023-02-21 16:27:15,299 DEBUG TRAIN Batch 17/500 loss 6.026577 loss_att 7.584427 loss_ctc 10.660484 loss_rnnt 5.035217 hw_loss 0.116128 lr 0.00041915 rank 3
2023-02-21 16:27:15,300 DEBUG TRAIN Batch 17/500 loss 8.504275 loss_att 11.436486 loss_ctc 11.093892 loss_rnnt 7.525650 hw_loss 0.087939 lr 0.00041922 rank 1
2023-02-21 16:27:15,301 DEBUG TRAIN Batch 17/500 loss 9.382286 loss_att 12.235075 loss_ctc 12.721022 loss_rnnt 8.306237 hw_loss 0.113111 lr 0.00041918 rank 4
2023-02-21 16:27:15,303 DEBUG TRAIN Batch 17/500 loss 8.810037 loss_att 10.489906 loss_ctc 10.138852 loss_rnnt 8.205936 hw_loss 0.170530 lr 0.00041919 rank 0
2023-02-21 16:27:15,303 DEBUG TRAIN Batch 17/500 loss 21.045744 loss_att 23.266453 loss_ctc 26.321228 loss_rnnt 19.859715 hw_loss 0.072167 lr 0.00041916 rank 6
2023-02-21 16:27:15,305 DEBUG TRAIN Batch 17/500 loss 11.358049 loss_att 13.156031 loss_ctc 18.416708 loss_rnnt 10.002336 hw_loss 0.103053 lr 0.00041912 rank 5
2023-02-21 16:28:33,639 DEBUG TRAIN Batch 17/600 loss 8.301390 loss_att 9.669466 loss_ctc 11.730128 loss_rnnt 7.518476 hw_loss 0.097748 lr 0.00041903 rank 4
2023-02-21 16:28:33,645 DEBUG TRAIN Batch 17/600 loss 18.859148 loss_att 22.392792 loss_ctc 29.409491 loss_rnnt 16.659224 hw_loss 0.162157 lr 0.00041901 rank 2
2023-02-21 16:28:33,649 DEBUG TRAIN Batch 17/600 loss 12.240878 loss_att 12.600468 loss_ctc 16.872383 loss_rnnt 11.482150 hw_loss 0.129894 lr 0.00041904 rank 0
2023-02-21 16:28:33,648 DEBUG TRAIN Batch 17/600 loss 11.338066 loss_att 12.658140 loss_ctc 16.318647 loss_rnnt 10.311352 hw_loss 0.184918 lr 0.00041896 rank 7
2023-02-21 16:28:33,649 DEBUG TRAIN Batch 17/600 loss 6.421349 loss_att 8.221514 loss_ctc 8.110941 loss_rnnt 5.772980 hw_loss 0.118232 lr 0.00041907 rank 1
2023-02-21 16:28:33,651 DEBUG TRAIN Batch 17/600 loss 11.291645 loss_att 11.033752 loss_ctc 14.307913 loss_rnnt 10.898735 hw_loss 0.079350 lr 0.00041901 rank 6
2023-02-21 16:28:33,651 DEBUG TRAIN Batch 17/600 loss 9.884033 loss_att 11.917995 loss_ctc 14.741539 loss_rnnt 8.768145 hw_loss 0.115179 lr 0.00041897 rank 5
2023-02-21 16:28:33,653 DEBUG TRAIN Batch 17/600 loss 10.105491 loss_att 8.953019 loss_ctc 10.996645 loss_rnnt 9.977710 hw_loss 0.448977 lr 0.00041900 rank 3
2023-02-21 16:29:53,644 DEBUG TRAIN Batch 17/700 loss 8.709817 loss_att 14.239344 loss_ctc 10.279006 loss_rnnt 7.334621 hw_loss 0.112622 lr 0.00041882 rank 7
2023-02-21 16:29:53,647 DEBUG TRAIN Batch 17/700 loss 12.451596 loss_att 17.927744 loss_ctc 18.443460 loss_rnnt 10.480854 hw_loss 0.143622 lr 0.00041886 rank 6
2023-02-21 16:29:53,650 DEBUG TRAIN Batch 17/700 loss 8.778444 loss_att 10.841773 loss_ctc 8.832259 loss_rnnt 8.252119 hw_loss 0.199657 lr 0.00041892 rank 1
2023-02-21 16:29:53,654 DEBUG TRAIN Batch 17/700 loss 3.235007 loss_att 6.816561 loss_ctc 7.024590 loss_rnnt 1.949819 hw_loss 0.119249 lr 0.00041883 rank 5
2023-02-21 16:29:53,654 DEBUG TRAIN Batch 17/700 loss 11.268719 loss_att 13.244356 loss_ctc 13.312670 loss_rnnt 10.587378 hw_loss 0.025664 lr 0.00041890 rank 0
2023-02-21 16:29:53,657 DEBUG TRAIN Batch 17/700 loss 22.655149 loss_att 26.034546 loss_ctc 32.527008 loss_rnnt 20.649265 hw_loss 0.025794 lr 0.00041885 rank 3
2023-02-21 16:29:53,667 DEBUG TRAIN Batch 17/700 loss 8.728355 loss_att 12.887691 loss_ctc 16.315601 loss_rnnt 6.870960 hw_loss 0.026053 lr 0.00041886 rank 2
2023-02-21 16:29:53,676 DEBUG TRAIN Batch 17/700 loss 6.532374 loss_att 10.646043 loss_ctc 8.371080 loss_rnnt 5.433144 hw_loss 0.058754 lr 0.00041889 rank 4
2023-02-21 16:31:11,884 DEBUG TRAIN Batch 17/800 loss 3.840852 loss_att 7.124095 loss_ctc 6.325205 loss_rnnt 2.839233 hw_loss 0.025731 lr 0.00041867 rank 7
2023-02-21 16:31:11,888 DEBUG TRAIN Batch 17/800 loss 8.565906 loss_att 14.001114 loss_ctc 12.632514 loss_rnnt 6.923453 hw_loss 0.024743 lr 0.00041877 rank 1
2023-02-21 16:31:11,892 DEBUG TRAIN Batch 17/800 loss 11.283835 loss_att 15.449438 loss_ctc 14.120173 loss_rnnt 10.019808 hw_loss 0.098866 lr 0.00041874 rank 4
2023-02-21 16:31:11,894 DEBUG TRAIN Batch 17/800 loss 9.166125 loss_att 11.347331 loss_ctc 10.678229 loss_rnnt 8.484695 hw_loss 0.081704 lr 0.00041871 rank 2
2023-02-21 16:31:11,895 DEBUG TRAIN Batch 17/800 loss 8.272873 loss_att 13.420796 loss_ctc 9.421466 loss_rnnt 7.031671 hw_loss 0.109633 lr 0.00041872 rank 6
2023-02-21 16:31:11,895 DEBUG TRAIN Batch 17/800 loss 10.789236 loss_att 14.705189 loss_ctc 16.523697 loss_rnnt 9.208309 hw_loss 0.062142 lr 0.00041875 rank 0
2023-02-21 16:31:11,898 DEBUG TRAIN Batch 17/800 loss 11.077456 loss_att 17.270048 loss_ctc 18.875278 loss_rnnt 8.761657 hw_loss 0.070445 lr 0.00041871 rank 3
2023-02-21 16:31:11,902 DEBUG TRAIN Batch 17/800 loss 7.608850 loss_att 10.377694 loss_ctc 8.088589 loss_rnnt 6.965410 hw_loss 0.048201 lr 0.00041868 rank 5
2023-02-21 16:32:31,856 DEBUG TRAIN Batch 17/900 loss 18.694464 loss_att 22.369627 loss_ctc 23.774849 loss_rnnt 17.223038 hw_loss 0.110639 lr 0.00041863 rank 1
2023-02-21 16:32:31,859 DEBUG TRAIN Batch 17/900 loss 5.644908 loss_att 7.680755 loss_ctc 6.308403 loss_rnnt 5.012323 hw_loss 0.256782 lr 0.00041859 rank 4
2023-02-21 16:32:31,861 DEBUG TRAIN Batch 17/900 loss 10.666945 loss_att 14.345766 loss_ctc 17.378654 loss_rnnt 8.970436 hw_loss 0.123470 lr 0.00041860 rank 0
2023-02-21 16:32:31,862 DEBUG TRAIN Batch 17/900 loss 9.400759 loss_att 13.162173 loss_ctc 11.563097 loss_rnnt 8.323840 hw_loss 0.068110 lr 0.00041852 rank 7
2023-02-21 16:32:31,863 DEBUG TRAIN Batch 17/900 loss 15.170675 loss_att 17.678848 loss_ctc 21.348843 loss_rnnt 13.729896 hw_loss 0.216355 lr 0.00041857 rank 2
2023-02-21 16:32:31,864 DEBUG TRAIN Batch 17/900 loss 4.258677 loss_att 6.301973 loss_ctc 4.764828 loss_rnnt 3.645509 hw_loss 0.256915 lr 0.00041853 rank 5
2023-02-21 16:32:31,865 DEBUG TRAIN Batch 17/900 loss 12.766681 loss_att 13.038612 loss_ctc 15.895812 loss_rnnt 12.179053 hw_loss 0.217544 lr 0.00041857 rank 6
2023-02-21 16:32:31,866 DEBUG TRAIN Batch 17/900 loss 13.649856 loss_att 15.843524 loss_ctc 14.793455 loss_rnnt 12.969818 hw_loss 0.166546 lr 0.00041856 rank 3
2023-02-21 16:33:50,261 DEBUG TRAIN Batch 17/1000 loss 12.080256 loss_att 13.586061 loss_ctc 17.136642 loss_rnnt 11.019032 hw_loss 0.161022 lr 0.00041838 rank 7
2023-02-21 16:33:50,263 DEBUG TRAIN Batch 17/1000 loss 8.850379 loss_att 12.525803 loss_ctc 10.210168 loss_rnnt 7.826293 hw_loss 0.201928 lr 0.00041845 rank 4
2023-02-21 16:33:50,264 DEBUG TRAIN Batch 17/1000 loss 9.707698 loss_att 13.981421 loss_ctc 17.989883 loss_rnnt 7.691489 hw_loss 0.107199 lr 0.00041848 rank 1
2023-02-21 16:33:50,265 DEBUG TRAIN Batch 17/1000 loss 13.123087 loss_att 13.919495 loss_ctc 18.025887 loss_rnnt 12.232141 hw_loss 0.146173 lr 0.00041846 rank 0
2023-02-21 16:33:50,268 DEBUG TRAIN Batch 17/1000 loss 7.384688 loss_att 11.312015 loss_ctc 8.109280 loss_rnnt 6.413671 hw_loss 0.166760 lr 0.00041842 rank 6
2023-02-21 16:33:50,268 DEBUG TRAIN Batch 17/1000 loss 15.894677 loss_att 21.299751 loss_ctc 23.689240 loss_rnnt 13.743465 hw_loss 0.057978 lr 0.00041842 rank 2
2023-02-21 16:33:50,271 DEBUG TRAIN Batch 17/1000 loss 6.989262 loss_att 11.255070 loss_ctc 12.139509 loss_rnnt 5.412510 hw_loss 0.069169 lr 0.00041841 rank 3
2023-02-21 16:33:50,276 DEBUG TRAIN Batch 17/1000 loss 5.426255 loss_att 8.164889 loss_ctc 8.866032 loss_rnnt 4.394169 hw_loss 0.048229 lr 0.00041839 rank 5
2023-02-21 16:35:10,769 DEBUG TRAIN Batch 17/1100 loss 8.861341 loss_att 11.859650 loss_ctc 12.263050 loss_rnnt 7.760839 hw_loss 0.088648 lr 0.00041823 rank 7
2023-02-21 16:35:10,771 DEBUG TRAIN Batch 17/1100 loss 10.566930 loss_att 14.706101 loss_ctc 17.222736 loss_rnnt 8.783011 hw_loss 0.128706 lr 0.00041830 rank 4
2023-02-21 16:35:10,774 DEBUG TRAIN Batch 17/1100 loss 7.281383 loss_att 9.665560 loss_ctc 9.426399 loss_rnnt 6.377845 hw_loss 0.263814 lr 0.00041824 rank 5
2023-02-21 16:35:10,775 DEBUG TRAIN Batch 17/1100 loss 9.332394 loss_att 11.399907 loss_ctc 17.144009 loss_rnnt 7.767392 hw_loss 0.206156 lr 0.00041828 rank 6
2023-02-21 16:35:10,776 DEBUG TRAIN Batch 17/1100 loss 25.641809 loss_att 27.475393 loss_ctc 33.988609 loss_rnnt 24.099621 hw_loss 0.117309 lr 0.00041831 rank 0
2023-02-21 16:35:10,776 DEBUG TRAIN Batch 17/1100 loss 9.745800 loss_att 13.685072 loss_ctc 12.759786 loss_rnnt 8.485312 hw_loss 0.132691 lr 0.00041833 rank 1
2023-02-21 16:35:10,777 DEBUG TRAIN Batch 17/1100 loss 9.500956 loss_att 14.465303 loss_ctc 13.034612 loss_rnnt 7.992851 hw_loss 0.082650 lr 0.00041827 rank 2
2023-02-21 16:35:10,780 DEBUG TRAIN Batch 17/1100 loss 10.151527 loss_att 12.466316 loss_ctc 14.117007 loss_rnnt 9.102235 hw_loss 0.108009 lr 0.00041827 rank 3
2023-02-21 16:36:29,297 DEBUG TRAIN Batch 17/1200 loss 9.899341 loss_att 10.617617 loss_ctc 16.501577 loss_rnnt 8.806149 hw_loss 0.129822 lr 0.00041808 rank 7
2023-02-21 16:36:29,300 DEBUG TRAIN Batch 17/1200 loss 5.269414 loss_att 10.026369 loss_ctc 10.199301 loss_rnnt 3.635047 hw_loss 0.048109 lr 0.00041813 rank 2
2023-02-21 16:36:29,300 DEBUG TRAIN Batch 17/1200 loss 17.198383 loss_att 18.303696 loss_ctc 19.269474 loss_rnnt 16.685150 hw_loss 0.030043 lr 0.00041819 rank 1
2023-02-21 16:36:29,305 DEBUG TRAIN Batch 17/1200 loss 10.658637 loss_att 12.733816 loss_ctc 13.335721 loss_rnnt 9.771806 hw_loss 0.215344 lr 0.00041815 rank 4
2023-02-21 16:36:29,306 DEBUG TRAIN Batch 17/1200 loss 10.338688 loss_att 10.746122 loss_ctc 13.424644 loss_rnnt 9.756155 hw_loss 0.167974 lr 0.00041813 rank 6
2023-02-21 16:36:29,306 DEBUG TRAIN Batch 17/1200 loss 15.902077 loss_att 14.809561 loss_ctc 17.350367 loss_rnnt 15.642103 hw_loss 0.535074 lr 0.00041812 rank 3
2023-02-21 16:36:29,307 DEBUG TRAIN Batch 17/1200 loss 8.904926 loss_att 9.219878 loss_ctc 9.849358 loss_rnnt 8.613056 hw_loss 0.193043 lr 0.00041809 rank 5
2023-02-21 16:36:29,344 DEBUG TRAIN Batch 17/1200 loss 9.335760 loss_att 9.766308 loss_ctc 10.247762 loss_rnnt 9.073207 hw_loss 0.102829 lr 0.00041816 rank 0
2023-02-21 16:37:48,672 DEBUG TRAIN Batch 17/1300 loss 6.994230 loss_att 9.717702 loss_ctc 8.518425 loss_rnnt 6.146545 hw_loss 0.187057 lr 0.00041804 rank 1
2023-02-21 16:37:48,676 DEBUG TRAIN Batch 17/1300 loss 13.568244 loss_att 19.648735 loss_ctc 21.490978 loss_rnnt 11.280317 hw_loss 0.028996 lr 0.00041802 rank 0
2023-02-21 16:37:48,678 DEBUG TRAIN Batch 17/1300 loss 8.702104 loss_att 8.139815 loss_ctc 11.183980 loss_rnnt 8.392855 hw_loss 0.170231 lr 0.00041801 rank 4
2023-02-21 16:37:48,678 DEBUG TRAIN Batch 17/1300 loss 7.604363 loss_att 9.447328 loss_ctc 11.198364 loss_rnnt 6.678490 hw_loss 0.146401 lr 0.00041794 rank 7
2023-02-21 16:37:48,679 DEBUG TRAIN Batch 17/1300 loss 9.626691 loss_att 9.620290 loss_ctc 13.006619 loss_rnnt 9.052669 hw_loss 0.233709 lr 0.00041798 rank 2
2023-02-21 16:37:48,687 DEBUG TRAIN Batch 17/1300 loss 12.200915 loss_att 14.324956 loss_ctc 13.299544 loss_rnnt 11.556300 hw_loss 0.137481 lr 0.00041797 rank 3
2023-02-21 16:37:48,691 DEBUG TRAIN Batch 17/1300 loss 4.333871 loss_att 8.467278 loss_ctc 6.074625 loss_rnnt 3.224650 hw_loss 0.094573 lr 0.00041795 rank 5
2023-02-21 16:37:48,726 DEBUG TRAIN Batch 17/1300 loss 6.610209 loss_att 9.919520 loss_ctc 8.570742 loss_rnnt 5.618324 hw_loss 0.128658 lr 0.00041798 rank 6
2023-02-21 16:39:08,654 DEBUG TRAIN Batch 17/1400 loss 10.029575 loss_att 12.157615 loss_ctc 16.174465 loss_rnnt 8.720268 hw_loss 0.120712 lr 0.00041784 rank 6
2023-02-21 16:39:08,658 DEBUG TRAIN Batch 17/1400 loss 9.193713 loss_att 11.102264 loss_ctc 7.865762 loss_rnnt 8.953560 hw_loss 0.066568 lr 0.00041779 rank 7
2023-02-21 16:39:08,658 DEBUG TRAIN Batch 17/1400 loss 8.297261 loss_att 13.863657 loss_ctc 9.273161 loss_rnnt 6.979879 hw_loss 0.138720 lr 0.00041790 rank 1
2023-02-21 16:39:08,660 DEBUG TRAIN Batch 17/1400 loss 21.506472 loss_att 23.300768 loss_ctc 29.214842 loss_rnnt 20.036308 hw_loss 0.156605 lr 0.00041787 rank 0
2023-02-21 16:39:08,661 DEBUG TRAIN Batch 17/1400 loss 4.253955 loss_att 6.944316 loss_ctc 5.433395 loss_rnnt 3.477679 hw_loss 0.151772 lr 0.00041780 rank 5
2023-02-21 16:39:08,661 DEBUG TRAIN Batch 17/1400 loss 8.926558 loss_att 11.321500 loss_ctc 13.179893 loss_rnnt 7.853335 hw_loss 0.050856 lr 0.00041784 rank 2
2023-02-21 16:39:08,663 DEBUG TRAIN Batch 17/1400 loss 10.984763 loss_att 15.369459 loss_ctc 14.011024 loss_rnnt 9.650680 hw_loss 0.100581 lr 0.00041786 rank 4
2023-02-21 16:39:08,664 DEBUG TRAIN Batch 17/1400 loss 9.626670 loss_att 15.571516 loss_ctc 13.634529 loss_rnnt 7.839695 hw_loss 0.119294 lr 0.00041783 rank 3
2023-02-21 16:40:28,107 DEBUG TRAIN Batch 17/1500 loss 10.712325 loss_att 14.021921 loss_ctc 19.148777 loss_rnnt 8.783700 hw_loss 0.265959 lr 0.00041769 rank 6
2023-02-21 16:40:28,107 DEBUG TRAIN Batch 17/1500 loss 10.059077 loss_att 12.669566 loss_ctc 11.942057 loss_rnnt 9.185896 hw_loss 0.187538 lr 0.00041772 rank 4
2023-02-21 16:40:28,108 DEBUG TRAIN Batch 17/1500 loss 16.546530 loss_att 19.399019 loss_ctc 19.298065 loss_rnnt 15.568092 hw_loss 0.077003 lr 0.00041765 rank 7
2023-02-21 16:40:28,108 DEBUG TRAIN Batch 17/1500 loss 24.115303 loss_att 24.948563 loss_ctc 34.281883 loss_rnnt 22.449905 hw_loss 0.268504 lr 0.00041775 rank 1
2023-02-21 16:40:28,110 DEBUG TRAIN Batch 17/1500 loss 9.906930 loss_att 17.047598 loss_ctc 14.197872 loss_rnnt 7.882129 hw_loss 0.046017 lr 0.00041766 rank 5
2023-02-21 16:40:28,111 DEBUG TRAIN Batch 17/1500 loss 6.206176 loss_att 9.746711 loss_ctc 7.134597 loss_rnnt 5.360088 hw_loss 0.026610 lr 0.00041769 rank 2
2023-02-21 16:40:28,111 DEBUG TRAIN Batch 17/1500 loss 6.547260 loss_att 10.099244 loss_ctc 10.123706 loss_rnnt 5.345933 hw_loss 0.026381 lr 0.00041768 rank 3
2023-02-21 16:40:28,158 DEBUG TRAIN Batch 17/1500 loss 11.948456 loss_att 14.313781 loss_ctc 13.665309 loss_rnnt 11.164092 hw_loss 0.154474 lr 0.00041773 rank 0
2023-02-21 16:41:46,074 DEBUG TRAIN Batch 17/1600 loss 14.575671 loss_att 17.191498 loss_ctc 20.364935 loss_rnnt 13.175158 hw_loss 0.197711 lr 0.00041750 rank 7
2023-02-21 16:41:46,077 DEBUG TRAIN Batch 17/1600 loss 13.229951 loss_att 17.140057 loss_ctc 15.736849 loss_rnnt 12.072348 hw_loss 0.077492 lr 0.00041757 rank 4
2023-02-21 16:41:46,077 DEBUG TRAIN Batch 17/1600 loss 9.376185 loss_att 13.457355 loss_ctc 12.221867 loss_rnnt 8.095753 hw_loss 0.158952 lr 0.00041760 rank 1
2023-02-21 16:41:46,077 DEBUG TRAIN Batch 17/1600 loss 22.610310 loss_att 20.860762 loss_ctc 28.322697 loss_rnnt 22.149044 hw_loss 0.092857 lr 0.00041755 rank 6
2023-02-21 16:41:46,078 DEBUG TRAIN Batch 17/1600 loss 4.605658 loss_att 8.749707 loss_ctc 7.168551 loss_rnnt 3.355802 hw_loss 0.148739 lr 0.00041751 rank 5
2023-02-21 16:41:46,078 DEBUG TRAIN Batch 17/1600 loss 11.220772 loss_att 13.333426 loss_ctc 13.633249 loss_rnnt 10.391876 hw_loss 0.158815 lr 0.00041754 rank 2
2023-02-21 16:41:46,083 DEBUG TRAIN Batch 17/1600 loss 13.208343 loss_att 20.932127 loss_ctc 21.767834 loss_rnnt 10.469543 hw_loss 0.098957 lr 0.00041754 rank 3
2023-02-21 16:41:46,121 DEBUG TRAIN Batch 17/1600 loss 4.206609 loss_att 9.111337 loss_ctc 6.909685 loss_rnnt 2.759767 hw_loss 0.197788 lr 0.00041758 rank 0
2023-02-21 16:43:04,398 DEBUG TRAIN Batch 17/1700 loss 14.369559 loss_att 15.818512 loss_ctc 17.259844 loss_rnnt 13.576928 hw_loss 0.220254 lr 0.00041746 rank 1
2023-02-21 16:43:04,399 DEBUG TRAIN Batch 17/1700 loss 10.204884 loss_att 14.029518 loss_ctc 12.184004 loss_rnnt 9.088215 hw_loss 0.164738 lr 0.00041742 rank 4
2023-02-21 16:43:04,400 DEBUG TRAIN Batch 17/1700 loss 11.700334 loss_att 13.599834 loss_ctc 18.278637 loss_rnnt 10.394501 hw_loss 0.091547 lr 0.00041740 rank 2
2023-02-21 16:43:04,402 DEBUG TRAIN Batch 17/1700 loss 6.342752 loss_att 9.569253 loss_ctc 10.501556 loss_rnnt 4.998888 hw_loss 0.270104 lr 0.00041736 rank 5
2023-02-21 16:43:04,407 DEBUG TRAIN Batch 17/1700 loss 7.629442 loss_att 11.313621 loss_ctc 12.242960 loss_rnnt 6.213778 hw_loss 0.119424 lr 0.00041735 rank 7
2023-02-21 16:43:04,427 DEBUG TRAIN Batch 17/1700 loss 10.730587 loss_att 12.596758 loss_ctc 14.600127 loss_rnnt 9.765139 hw_loss 0.143015 lr 0.00041743 rank 0
2023-02-21 16:43:04,436 DEBUG TRAIN Batch 17/1700 loss 7.606940 loss_att 9.760152 loss_ctc 7.546468 loss_rnnt 7.045362 hw_loss 0.260622 lr 0.00041739 rank 3
2023-02-21 16:43:04,447 DEBUG TRAIN Batch 17/1700 loss 8.228727 loss_att 11.515606 loss_ctc 11.605921 loss_rnnt 7.006886 hw_loss 0.214075 lr 0.00041740 rank 6
2023-02-21 16:44:25,225 DEBUG TRAIN Batch 17/1800 loss 4.911829 loss_att 10.746942 loss_ctc 7.478348 loss_rnnt 3.312863 hw_loss 0.168265 lr 0.00041729 rank 0
2023-02-21 16:44:25,225 DEBUG TRAIN Batch 17/1800 loss 5.945374 loss_att 7.219702 loss_ctc 9.256732 loss_rnnt 5.160734 hw_loss 0.165489 lr 0.00041728 rank 4
2023-02-21 16:44:25,227 DEBUG TRAIN Batch 17/1800 loss 9.476818 loss_att 11.500878 loss_ctc 15.638624 loss_rnnt 8.189574 hw_loss 0.114109 lr 0.00041725 rank 2
2023-02-21 16:44:25,230 DEBUG TRAIN Batch 17/1800 loss 8.686054 loss_att 11.500824 loss_ctc 12.449246 loss_rnnt 7.589985 hw_loss 0.058794 lr 0.00041731 rank 1
2023-02-21 16:44:25,231 DEBUG TRAIN Batch 17/1800 loss 14.988063 loss_att 19.513186 loss_ctc 22.652695 loss_rnnt 12.896852 hw_loss 0.307939 lr 0.00041726 rank 6
2023-02-21 16:44:25,235 DEBUG TRAIN Batch 17/1800 loss 10.129923 loss_att 12.241409 loss_ctc 14.578359 loss_rnnt 9.059817 hw_loss 0.102531 lr 0.00041721 rank 7
2023-02-21 16:44:25,237 DEBUG TRAIN Batch 17/1800 loss 7.013906 loss_att 9.206388 loss_ctc 10.784492 loss_rnnt 5.999799 hw_loss 0.136625 lr 0.00041725 rank 3
2023-02-21 16:44:25,278 DEBUG TRAIN Batch 17/1800 loss 10.050378 loss_att 12.734442 loss_ctc 16.210720 loss_rnnt 8.620016 hw_loss 0.135317 lr 0.00041722 rank 5
2023-02-21 16:45:44,879 DEBUG TRAIN Batch 17/1900 loss 6.823797 loss_att 7.995700 loss_ctc 9.031560 loss_rnnt 6.077992 hw_loss 0.406980 lr 0.00041706 rank 7
2023-02-21 16:45:44,884 DEBUG TRAIN Batch 17/1900 loss 13.409657 loss_att 14.439470 loss_ctc 17.723164 loss_rnnt 12.533094 hw_loss 0.179002 lr 0.00041711 rank 2
2023-02-21 16:45:44,889 DEBUG TRAIN Batch 17/1900 loss 13.328285 loss_att 16.754841 loss_ctc 16.731064 loss_rnnt 12.109177 hw_loss 0.150174 lr 0.00041711 rank 6
2023-02-21 16:45:44,892 DEBUG TRAIN Batch 17/1900 loss 14.677864 loss_att 16.122417 loss_ctc 19.957832 loss_rnnt 13.559605 hw_loss 0.235039 lr 0.00041713 rank 4
2023-02-21 16:45:44,891 DEBUG TRAIN Batch 17/1900 loss 12.518645 loss_att 12.478687 loss_ctc 16.364466 loss_rnnt 11.827989 hw_loss 0.348508 lr 0.00041714 rank 0
2023-02-21 16:45:44,892 DEBUG TRAIN Batch 17/1900 loss 10.189584 loss_att 10.973855 loss_ctc 16.301933 loss_rnnt 9.189954 hw_loss 0.052116 lr 0.00041710 rank 3
2023-02-21 16:45:44,893 DEBUG TRAIN Batch 17/1900 loss 6.702050 loss_att 11.537084 loss_ctc 11.084591 loss_rnnt 5.057067 hw_loss 0.175570 lr 0.00041717 rank 1
2023-02-21 16:45:44,935 DEBUG TRAIN Batch 17/1900 loss 10.630335 loss_att 11.488281 loss_ctc 13.349926 loss_rnnt 10.027122 hw_loss 0.129396 lr 0.00041707 rank 5
2023-02-21 16:47:03,781 DEBUG TRAIN Batch 17/2000 loss 12.026109 loss_att 10.488893 loss_ctc 12.140716 loss_rnnt 12.303070 hw_loss 0.028502 lr 0.00041696 rank 2
2023-02-21 16:47:03,783 DEBUG TRAIN Batch 17/2000 loss 7.902964 loss_att 11.861587 loss_ctc 10.009922 loss_rnnt 6.760337 hw_loss 0.131202 lr 0.00041692 rank 7
2023-02-21 16:47:03,785 DEBUG TRAIN Batch 17/2000 loss 4.406733 loss_att 8.030719 loss_ctc 5.733297 loss_rnnt 3.421537 hw_loss 0.156607 lr 0.00041697 rank 6
2023-02-21 16:47:03,786 DEBUG TRAIN Batch 17/2000 loss 8.521511 loss_att 11.653142 loss_ctc 13.579261 loss_rnnt 7.115509 hw_loss 0.197453 lr 0.00041695 rank 3
2023-02-21 16:47:03,788 DEBUG TRAIN Batch 17/2000 loss 6.271791 loss_att 7.446740 loss_ctc 7.484289 loss_rnnt 5.829376 hw_loss 0.085799 lr 0.00041693 rank 5
2023-02-21 16:47:03,789 DEBUG TRAIN Batch 17/2000 loss 1.748319 loss_att 4.469708 loss_ctc 3.626465 loss_rnnt 0.900726 hw_loss 0.099180 lr 0.00041702 rank 1
2023-02-21 16:47:03,790 DEBUG TRAIN Batch 17/2000 loss 8.253508 loss_att 13.478102 loss_ctc 13.842799 loss_rnnt 6.420390 hw_loss 0.080550 lr 0.00041699 rank 4
2023-02-21 16:47:03,791 DEBUG TRAIN Batch 17/2000 loss 11.255926 loss_att 15.050713 loss_ctc 15.612085 loss_rnnt 9.855985 hw_loss 0.112806 lr 0.00041700 rank 0
2023-02-21 16:48:25,019 DEBUG TRAIN Batch 17/2100 loss 10.342366 loss_att 14.499649 loss_ctc 16.169899 loss_rnnt 8.719175 hw_loss 0.027618 lr 0.00041685 rank 0
2023-02-21 16:48:25,020 DEBUG TRAIN Batch 17/2100 loss 11.477503 loss_att 13.082428 loss_ctc 14.850394 loss_rnnt 10.592371 hw_loss 0.214554 lr 0.00041682 rank 2
2023-02-21 16:48:25,021 DEBUG TRAIN Batch 17/2100 loss 8.838015 loss_att 11.275143 loss_ctc 9.609812 loss_rnnt 8.200793 hw_loss 0.087917 lr 0.00041684 rank 4
2023-02-21 16:48:25,025 DEBUG TRAIN Batch 17/2100 loss 8.745897 loss_att 13.860525 loss_ctc 11.779327 loss_rnnt 7.303363 hw_loss 0.028409 lr 0.00041677 rank 7
2023-02-21 16:48:25,025 DEBUG TRAIN Batch 17/2100 loss 3.842102 loss_att 5.499409 loss_ctc 7.405663 loss_rnnt 2.955924 hw_loss 0.149203 lr 0.00041681 rank 3
2023-02-21 16:48:25,026 DEBUG TRAIN Batch 17/2100 loss 5.332509 loss_att 8.052127 loss_ctc 6.216924 loss_rnnt 4.607011 hw_loss 0.119347 lr 0.00041678 rank 5
2023-02-21 16:48:25,026 DEBUG TRAIN Batch 17/2100 loss 12.085135 loss_att 12.969394 loss_ctc 18.239716 loss_rnnt 11.000927 hw_loss 0.162647 lr 0.00041688 rank 1
2023-02-21 16:48:25,028 DEBUG TRAIN Batch 17/2100 loss 10.217507 loss_att 11.634203 loss_ctc 15.027538 loss_rnnt 9.211552 hw_loss 0.152397 lr 0.00041682 rank 6
2023-02-21 16:49:44,502 DEBUG TRAIN Batch 17/2200 loss 11.733788 loss_att 14.226129 loss_ctc 16.590816 loss_rnnt 10.525229 hw_loss 0.117161 lr 0.00041673 rank 1
2023-02-21 16:49:44,503 DEBUG TRAIN Batch 17/2200 loss 7.260739 loss_att 10.293451 loss_ctc 8.866825 loss_rnnt 6.383238 hw_loss 0.106524 lr 0.00041667 rank 2
2023-02-21 16:49:44,504 DEBUG TRAIN Batch 17/2200 loss 7.542315 loss_att 9.506306 loss_ctc 12.579764 loss_rnnt 6.321820 hw_loss 0.292569 lr 0.00041670 rank 4
2023-02-21 16:49:44,506 DEBUG TRAIN Batch 17/2200 loss 15.739697 loss_att 18.469133 loss_ctc 20.603657 loss_rnnt 14.489920 hw_loss 0.103805 lr 0.00041671 rank 0
2023-02-21 16:49:44,507 DEBUG TRAIN Batch 17/2200 loss 12.331923 loss_att 14.942017 loss_ctc 18.032753 loss_rnnt 10.960683 hw_loss 0.167084 lr 0.00041667 rank 3
2023-02-21 16:49:44,509 DEBUG TRAIN Batch 17/2200 loss 12.578831 loss_att 13.538329 loss_ctc 12.658978 loss_rnnt 12.303085 hw_loss 0.137174 lr 0.00041668 rank 6
2023-02-21 16:49:44,510 DEBUG TRAIN Batch 17/2200 loss 11.798311 loss_att 14.713284 loss_ctc 18.212408 loss_rnnt 10.294824 hw_loss 0.122400 lr 0.00041663 rank 7
2023-02-21 16:49:44,513 DEBUG TRAIN Batch 17/2200 loss 7.037063 loss_att 8.346485 loss_ctc 6.599965 loss_rnnt 6.766572 hw_loss 0.125410 lr 0.00041664 rank 5
2023-02-21 16:51:01,602 DEBUG TRAIN Batch 17/2300 loss 6.183304 loss_att 8.349724 loss_ctc 10.906415 loss_rnnt 5.061714 hw_loss 0.109798 lr 0.00041653 rank 6
2023-02-21 16:51:01,609 DEBUG TRAIN Batch 17/2300 loss 7.712306 loss_att 10.425292 loss_ctc 10.526241 loss_rnnt 6.643554 hw_loss 0.283057 lr 0.00041648 rank 7
2023-02-21 16:51:01,608 DEBUG TRAIN Batch 17/2300 loss 8.515952 loss_att 11.079014 loss_ctc 12.154507 loss_rnnt 7.427824 hw_loss 0.169452 lr 0.00041653 rank 2
2023-02-21 16:51:01,609 DEBUG TRAIN Batch 17/2300 loss 5.482845 loss_att 9.144198 loss_ctc 7.217308 loss_rnnt 4.480074 hw_loss 0.073572 lr 0.00041659 rank 1
2023-02-21 16:51:01,614 DEBUG TRAIN Batch 17/2300 loss 12.303134 loss_att 14.068601 loss_ctc 15.012518 loss_rnnt 11.562717 hw_loss 0.048882 lr 0.00041655 rank 4
2023-02-21 16:51:01,614 DEBUG TRAIN Batch 17/2300 loss 10.470444 loss_att 15.817682 loss_ctc 17.714867 loss_rnnt 8.391809 hw_loss 0.081119 lr 0.00041649 rank 5
2023-02-21 16:51:01,614 DEBUG TRAIN Batch 17/2300 loss 7.991885 loss_att 12.093634 loss_ctc 12.671622 loss_rnnt 6.448523 hw_loss 0.185714 lr 0.00041652 rank 3
2023-02-21 16:51:01,657 DEBUG TRAIN Batch 17/2300 loss 5.789032 loss_att 8.633884 loss_ctc 7.516379 loss_rnnt 4.939224 hw_loss 0.094732 lr 0.00041656 rank 0
2023-02-21 16:52:20,684 DEBUG TRAIN Batch 17/2400 loss 10.169922 loss_att 14.033920 loss_ctc 11.980005 loss_rnnt 9.088066 hw_loss 0.126958 lr 0.00041634 rank 7
2023-02-21 16:52:20,685 DEBUG TRAIN Batch 17/2400 loss 8.406219 loss_att 9.599295 loss_ctc 9.809273 loss_rnnt 7.917971 hw_loss 0.117300 lr 0.00041639 rank 6
2023-02-21 16:52:20,688 DEBUG TRAIN Batch 17/2400 loss 12.669775 loss_att 18.174055 loss_ctc 16.472506 loss_rnnt 11.038191 hw_loss 0.044434 lr 0.00041644 rank 1
2023-02-21 16:52:20,688 DEBUG TRAIN Batch 17/2400 loss 8.850157 loss_att 12.175348 loss_ctc 11.655380 loss_rnnt 7.742400 hw_loss 0.128790 lr 0.00041638 rank 2
2023-02-21 16:52:20,690 DEBUG TRAIN Batch 17/2400 loss 5.314132 loss_att 7.255712 loss_ctc 6.970839 loss_rnnt 4.688713 hw_loss 0.030391 lr 0.00041641 rank 4
2023-02-21 16:52:20,690 DEBUG TRAIN Batch 17/2400 loss 4.567540 loss_att 9.803315 loss_ctc 7.216346 loss_rnnt 3.125747 hw_loss 0.077745 lr 0.00041642 rank 0
2023-02-21 16:52:20,693 DEBUG TRAIN Batch 17/2400 loss 7.624520 loss_att 9.406673 loss_ctc 10.841876 loss_rnnt 6.780863 hw_loss 0.109211 lr 0.00041635 rank 5
2023-02-21 16:52:20,708 DEBUG TRAIN Batch 17/2400 loss 9.656777 loss_att 11.683001 loss_ctc 11.337722 loss_rnnt 8.994617 hw_loss 0.061478 lr 0.00041638 rank 3
2023-02-21 16:53:41,579 DEBUG TRAIN Batch 17/2500 loss 10.695201 loss_att 14.080725 loss_ctc 18.477535 loss_rnnt 8.892047 hw_loss 0.165761 lr 0.00041624 rank 2
2023-02-21 16:53:41,583 DEBUG TRAIN Batch 17/2500 loss 12.478251 loss_att 14.794088 loss_ctc 20.407494 loss_rnnt 10.872149 hw_loss 0.160690 lr 0.00041630 rank 1
2023-02-21 16:53:41,585 DEBUG TRAIN Batch 17/2500 loss 11.980716 loss_att 16.658800 loss_ctc 15.967555 loss_rnnt 10.425503 hw_loss 0.165034 lr 0.00041624 rank 6
2023-02-21 16:53:41,586 DEBUG TRAIN Batch 17/2500 loss 8.626618 loss_att 10.258331 loss_ctc 12.420731 loss_rnnt 7.721078 hw_loss 0.137468 lr 0.00041627 rank 4
2023-02-21 16:53:41,587 DEBUG TRAIN Batch 17/2500 loss 8.947934 loss_att 11.256205 loss_ctc 13.052966 loss_rnnt 7.882966 hw_loss 0.104955 lr 0.00041620 rank 7
2023-02-21 16:53:41,591 DEBUG TRAIN Batch 17/2500 loss 7.855802 loss_att 9.570313 loss_ctc 10.644313 loss_rnnt 7.053660 hw_loss 0.163946 lr 0.00041628 rank 0
2023-02-21 16:53:41,594 DEBUG TRAIN Batch 17/2500 loss 7.133239 loss_att 7.104130 loss_ctc 8.676816 loss_rnnt 6.789777 hw_loss 0.269014 lr 0.00041623 rank 3
2023-02-21 16:53:41,597 DEBUG TRAIN Batch 17/2500 loss 8.472622 loss_att 11.525716 loss_ctc 10.978454 loss_rnnt 7.493387 hw_loss 0.064697 lr 0.00041621 rank 5
2023-02-21 16:55:00,113 DEBUG TRAIN Batch 17/2600 loss 8.689384 loss_att 12.593285 loss_ctc 15.247561 loss_rnnt 6.901940 hw_loss 0.247951 lr 0.00041616 rank 1
2023-02-21 16:55:00,114 DEBUG TRAIN Batch 17/2600 loss 2.959064 loss_att 6.042749 loss_ctc 3.659190 loss_rnnt 2.183790 hw_loss 0.122224 lr 0.00041610 rank 2
2023-02-21 16:55:00,116 DEBUG TRAIN Batch 17/2600 loss 13.388895 loss_att 14.532056 loss_ctc 14.199211 loss_rnnt 13.005881 hw_loss 0.086886 lr 0.00041612 rank 4
2023-02-21 16:55:00,118 DEBUG TRAIN Batch 17/2600 loss 4.570861 loss_att 8.250936 loss_ctc 8.570827 loss_rnnt 3.194893 hw_loss 0.199921 lr 0.00041613 rank 0
2023-02-21 16:55:00,119 DEBUG TRAIN Batch 17/2600 loss 15.902962 loss_att 24.275999 loss_ctc 21.263725 loss_rnnt 13.479677 hw_loss 0.063578 lr 0.00041610 rank 6
2023-02-21 16:55:00,120 DEBUG TRAIN Batch 17/2600 loss 10.540402 loss_att 15.248238 loss_ctc 16.030022 loss_rnnt 8.637346 hw_loss 0.430386 lr 0.00041605 rank 7
2023-02-21 16:55:00,121 DEBUG TRAIN Batch 17/2600 loss 10.609514 loss_att 15.192579 loss_ctc 14.491310 loss_rnnt 9.152573 hw_loss 0.042668 lr 0.00041606 rank 5
2023-02-21 16:55:00,125 DEBUG TRAIN Batch 17/2600 loss 3.858127 loss_att 8.561989 loss_ctc 3.591360 loss_rnnt 2.907394 hw_loss 0.085367 lr 0.00041609 rank 3
2023-02-21 16:56:19,084 DEBUG TRAIN Batch 17/2700 loss 15.695613 loss_att 20.132799 loss_ctc 21.065176 loss_rnnt 14.023640 hw_loss 0.128613 lr 0.00041595 rank 2
2023-02-21 16:56:19,084 DEBUG TRAIN Batch 17/2700 loss 14.298275 loss_att 16.599112 loss_ctc 22.270536 loss_rnnt 12.676168 hw_loss 0.185572 lr 0.00041598 rank 4
2023-02-21 16:56:19,088 DEBUG TRAIN Batch 17/2700 loss 7.847475 loss_att 14.146590 loss_ctc 10.854536 loss_rnnt 6.140298 hw_loss 0.087022 lr 0.00041601 rank 1
2023-02-21 16:56:19,091 DEBUG TRAIN Batch 17/2700 loss 10.500751 loss_att 14.101463 loss_ctc 15.489243 loss_rnnt 9.034618 hw_loss 0.151608 lr 0.00041591 rank 7
2023-02-21 16:56:19,091 DEBUG TRAIN Batch 17/2700 loss 12.161280 loss_att 13.387982 loss_ctc 17.870491 loss_rnnt 11.016518 hw_loss 0.259111 lr 0.00041595 rank 6
2023-02-21 16:56:19,092 DEBUG TRAIN Batch 17/2700 loss 5.316843 loss_att 9.045053 loss_ctc 7.244752 loss_rnnt 4.222663 hw_loss 0.171531 lr 0.00041594 rank 3
2023-02-21 16:56:19,092 DEBUG TRAIN Batch 17/2700 loss 14.445507 loss_att 15.724270 loss_ctc 17.542349 loss_rnnt 13.699644 hw_loss 0.144746 lr 0.00041592 rank 5
2023-02-21 16:56:19,137 DEBUG TRAIN Batch 17/2700 loss 4.832703 loss_att 9.600043 loss_ctc 6.127592 loss_rnnt 3.655224 hw_loss 0.096298 lr 0.00041599 rank 0
2023-02-21 16:57:39,753 DEBUG TRAIN Batch 17/2800 loss 13.703108 loss_att 18.248692 loss_ctc 19.902359 loss_rnnt 11.886736 hw_loss 0.151289 lr 0.00041581 rank 2
2023-02-21 16:57:39,756 DEBUG TRAIN Batch 17/2800 loss 3.044121 loss_att 4.890580 loss_ctc 4.659840 loss_rnnt 2.419173 hw_loss 0.075425 lr 0.00041583 rank 4
2023-02-21 16:57:39,760 DEBUG TRAIN Batch 17/2800 loss 8.490603 loss_att 10.790517 loss_ctc 12.877318 loss_rnnt 7.369989 hw_loss 0.142004 lr 0.00041587 rank 1
2023-02-21 16:57:39,761 DEBUG TRAIN Batch 17/2800 loss 6.528949 loss_att 10.696200 loss_ctc 8.483707 loss_rnnt 5.401545 hw_loss 0.062474 lr 0.00041584 rank 0
2023-02-21 16:57:39,761 DEBUG TRAIN Batch 17/2800 loss 8.986287 loss_att 12.415612 loss_ctc 12.317451 loss_rnnt 7.729506 hw_loss 0.237677 lr 0.00041576 rank 7
2023-02-21 16:57:39,767 DEBUG TRAIN Batch 17/2800 loss 17.872074 loss_att 19.744831 loss_ctc 19.293400 loss_rnnt 17.257515 hw_loss 0.094680 lr 0.00041577 rank 5
2023-02-21 16:57:39,768 DEBUG TRAIN Batch 17/2800 loss 8.624054 loss_att 11.676641 loss_ctc 12.231397 loss_rnnt 7.499334 hw_loss 0.062294 lr 0.00041581 rank 6
2023-02-21 16:57:39,777 DEBUG TRAIN Batch 17/2800 loss 14.167070 loss_att 18.810642 loss_ctc 21.892178 loss_rnnt 12.122428 hw_loss 0.161090 lr 0.00041580 rank 3
2023-02-21 16:58:59,031 DEBUG TRAIN Batch 17/2900 loss 9.230912 loss_att 12.399323 loss_ctc 13.823157 loss_rnnt 7.931429 hw_loss 0.100317 lr 0.00041569 rank 4
2023-02-21 16:58:59,035 DEBUG TRAIN Batch 17/2900 loss 5.717741 loss_att 7.385222 loss_ctc 6.320526 loss_rnnt 5.279486 hw_loss 0.045727 lr 0.00041572 rank 1
2023-02-21 16:58:59,036 DEBUG TRAIN Batch 17/2900 loss 16.717863 loss_att 20.209513 loss_ctc 19.673637 loss_rnnt 15.535128 hw_loss 0.169314 lr 0.00041567 rank 6
2023-02-21 16:58:59,036 DEBUG TRAIN Batch 17/2900 loss 7.389407 loss_att 10.465270 loss_ctc 11.524260 loss_rnnt 6.164554 hw_loss 0.109436 lr 0.00041562 rank 7
2023-02-21 16:58:59,039 DEBUG TRAIN Batch 17/2900 loss 7.708090 loss_att 11.134745 loss_ctc 9.861847 loss_rnnt 6.700234 hw_loss 0.066295 lr 0.00041566 rank 2
2023-02-21 16:58:59,041 DEBUG TRAIN Batch 17/2900 loss 6.893030 loss_att 10.801718 loss_ctc 6.130874 loss_rnnt 6.133381 hw_loss 0.149122 lr 0.00041570 rank 0
2023-02-21 16:58:59,046 DEBUG TRAIN Batch 17/2900 loss 9.107573 loss_att 11.401409 loss_ctc 13.743292 loss_rnnt 7.948149 hw_loss 0.154801 lr 0.00041566 rank 3
2023-02-21 16:58:59,092 DEBUG TRAIN Batch 17/2900 loss 4.884440 loss_att 8.796447 loss_ctc 10.054054 loss_rnnt 3.337517 hw_loss 0.141077 lr 0.00041563 rank 5
2023-02-21 17:00:18,711 DEBUG TRAIN Batch 17/3000 loss 4.397141 loss_att 6.276855 loss_ctc 5.913228 loss_rnnt 3.754014 hw_loss 0.121949 lr 0.00041548 rank 7
2023-02-21 17:00:18,711 DEBUG TRAIN Batch 17/3000 loss 5.488823 loss_att 8.718397 loss_ctc 11.879828 loss_rnnt 3.955670 hw_loss 0.065820 lr 0.00041552 rank 2
2023-02-21 17:00:18,712 DEBUG TRAIN Batch 17/3000 loss 10.683145 loss_att 12.590620 loss_ctc 12.736938 loss_rnnt 9.998968 hw_loss 0.054081 lr 0.00041555 rank 4
2023-02-21 17:00:18,713 DEBUG TRAIN Batch 17/3000 loss 8.330627 loss_att 11.001398 loss_ctc 13.245939 loss_rnnt 7.094172 hw_loss 0.087986 lr 0.00041551 rank 3
2023-02-21 17:00:18,715 DEBUG TRAIN Batch 17/3000 loss 9.442516 loss_att 11.427497 loss_ctc 10.731333 loss_rnnt 8.754871 hw_loss 0.222763 lr 0.00041552 rank 6
2023-02-21 17:00:18,719 DEBUG TRAIN Batch 17/3000 loss 3.612645 loss_att 8.086251 loss_ctc 7.741766 loss_rnnt 2.149202 hw_loss 0.034072 lr 0.00041558 rank 1
2023-02-21 17:00:18,722 DEBUG TRAIN Batch 17/3000 loss 18.347769 loss_att 19.166901 loss_ctc 22.663742 loss_rnnt 17.512217 hw_loss 0.180487 lr 0.00041549 rank 5
2023-02-21 17:00:18,760 DEBUG TRAIN Batch 17/3000 loss 4.719918 loss_att 7.453414 loss_ctc 7.447361 loss_rnnt 3.768474 hw_loss 0.077037 lr 0.00041556 rank 0
2023-02-21 17:01:38,606 DEBUG TRAIN Batch 17/3100 loss 6.890363 loss_att 8.961272 loss_ctc 8.433886 loss_rnnt 6.222814 hw_loss 0.089184 lr 0.00041544 rank 1
2023-02-21 17:01:38,607 DEBUG TRAIN Batch 17/3100 loss 14.772200 loss_att 16.582996 loss_ctc 18.858221 loss_rnnt 13.796976 hw_loss 0.127988 lr 0.00041541 rank 0
2023-02-21 17:01:38,607 DEBUG TRAIN Batch 17/3100 loss 10.056490 loss_att 10.997961 loss_ctc 11.829859 loss_rnnt 9.537436 hw_loss 0.176831 lr 0.00041538 rank 6
2023-02-21 17:01:38,611 DEBUG TRAIN Batch 17/3100 loss 12.916759 loss_att 13.585613 loss_ctc 19.013186 loss_rnnt 11.881736 hw_loss 0.165742 lr 0.00041533 rank 7
2023-02-21 17:01:38,611 DEBUG TRAIN Batch 17/3100 loss 13.255318 loss_att 12.673227 loss_ctc 15.696589 loss_rnnt 12.958055 hw_loss 0.165333 lr 0.00041537 rank 3
2023-02-21 17:01:38,614 DEBUG TRAIN Batch 17/3100 loss 10.547698 loss_att 11.002329 loss_ctc 15.485680 loss_rnnt 9.743500 hw_loss 0.102891 lr 0.00041540 rank 4
2023-02-21 17:01:38,614 DEBUG TRAIN Batch 17/3100 loss 11.165717 loss_att 13.793459 loss_ctc 15.588806 loss_rnnt 9.929770 hw_loss 0.226229 lr 0.00041538 rank 2
2023-02-21 17:01:38,616 DEBUG TRAIN Batch 17/3100 loss 5.607888 loss_att 7.332833 loss_ctc 7.993467 loss_rnnt 4.921110 hw_loss 0.044461 lr 0.00041534 rank 5
2023-02-21 17:03:00,247 DEBUG TRAIN Batch 17/3200 loss 10.186816 loss_att 15.837292 loss_ctc 18.309847 loss_rnnt 7.906820 hw_loss 0.125308 lr 0.00041519 rank 7
2023-02-21 17:03:00,251 DEBUG TRAIN Batch 17/3200 loss 12.507773 loss_att 14.186863 loss_ctc 17.440035 loss_rnnt 11.419963 hw_loss 0.176920 lr 0.00041527 rank 0
2023-02-21 17:03:00,253 DEBUG TRAIN Batch 17/3200 loss 10.680128 loss_att 15.881039 loss_ctc 20.087545 loss_rnnt 8.289341 hw_loss 0.180530 lr 0.00041523 rank 2
2023-02-21 17:03:00,255 DEBUG TRAIN Batch 17/3200 loss 20.219961 loss_att 24.448879 loss_ctc 29.714237 loss_rnnt 18.075937 hw_loss 0.060632 lr 0.00041529 rank 1
2023-02-21 17:03:00,254 DEBUG TRAIN Batch 17/3200 loss 10.066592 loss_att 11.041473 loss_ctc 12.830263 loss_rnnt 9.393572 hw_loss 0.205415 lr 0.00041526 rank 4
2023-02-21 17:03:00,263 DEBUG TRAIN Batch 17/3200 loss 12.518707 loss_att 16.113783 loss_ctc 15.084418 loss_rnnt 11.338400 hw_loss 0.223497 lr 0.00041523 rank 3
2023-02-21 17:03:00,265 DEBUG TRAIN Batch 17/3200 loss 8.727218 loss_att 8.045858 loss_ctc 10.849527 loss_rnnt 8.488132 hw_loss 0.173217 lr 0.00041520 rank 5
2023-02-21 17:03:00,273 DEBUG TRAIN Batch 17/3200 loss 15.455305 loss_att 19.480854 loss_ctc 23.017124 loss_rnnt 13.596762 hw_loss 0.084733 lr 0.00041524 rank 6
2023-02-21 17:04:19,727 DEBUG TRAIN Batch 17/3300 loss 12.672133 loss_att 18.618969 loss_ctc 18.182425 loss_rnnt 10.714514 hw_loss 0.062902 lr 0.00041512 rank 4
2023-02-21 17:04:19,729 DEBUG TRAIN Batch 17/3300 loss 5.008673 loss_att 8.334446 loss_ctc 7.464940 loss_rnnt 3.979591 hw_loss 0.068297 lr 0.00041509 rank 6
2023-02-21 17:04:19,731 DEBUG TRAIN Batch 17/3300 loss 16.427942 loss_att 17.548130 loss_ctc 16.187130 loss_rnnt 16.171440 hw_loss 0.121071 lr 0.00041505 rank 7
2023-02-21 17:04:19,734 DEBUG TRAIN Batch 17/3300 loss 10.294408 loss_att 14.929258 loss_ctc 13.550448 loss_rnnt 8.909655 hw_loss 0.044333 lr 0.00041509 rank 2
2023-02-21 17:04:19,735 DEBUG TRAIN Batch 17/3300 loss 2.222816 loss_att 6.051545 loss_ctc 4.768645 loss_rnnt 1.070433 hw_loss 0.088489 lr 0.00041515 rank 1
2023-02-21 17:04:19,739 DEBUG TRAIN Batch 17/3300 loss 5.194797 loss_att 9.139900 loss_ctc 7.793365 loss_rnnt 3.930157 hw_loss 0.242142 lr 0.00041506 rank 5
2023-02-21 17:04:19,741 DEBUG TRAIN Batch 17/3300 loss 8.592850 loss_att 12.194250 loss_ctc 14.539374 loss_rnnt 6.957907 hw_loss 0.228362 lr 0.00041508 rank 3
2023-02-21 17:04:19,783 DEBUG TRAIN Batch 17/3300 loss 12.854323 loss_att 13.276674 loss_ctc 15.734734 loss_rnnt 12.297637 hw_loss 0.165305 lr 0.00041513 rank 0
2023-02-21 17:05:38,570 DEBUG TRAIN Batch 17/3400 loss 7.992577 loss_att 13.192347 loss_ctc 12.085105 loss_rnnt 6.392854 hw_loss 0.026433 lr 0.00041495 rank 2
2023-02-21 17:05:38,573 DEBUG TRAIN Batch 17/3400 loss 5.808908 loss_att 10.383260 loss_ctc 8.522936 loss_rnnt 4.447763 hw_loss 0.158260 lr 0.00041490 rank 7
2023-02-21 17:05:38,573 DEBUG TRAIN Batch 17/3400 loss 10.196556 loss_att 12.650675 loss_ctc 15.085695 loss_rnnt 8.995797 hw_loss 0.108843 lr 0.00041495 rank 6
2023-02-21 17:05:38,575 DEBUG TRAIN Batch 17/3400 loss 11.861539 loss_att 13.597408 loss_ctc 14.529257 loss_rnnt 11.067817 hw_loss 0.170348 lr 0.00041491 rank 5
2023-02-21 17:05:38,575 DEBUG TRAIN Batch 17/3400 loss 4.563403 loss_att 6.750778 loss_ctc 5.823133 loss_rnnt 3.910327 hw_loss 0.089319 lr 0.00041497 rank 4
2023-02-21 17:05:38,578 DEBUG TRAIN Batch 17/3400 loss 21.041840 loss_att 24.533255 loss_ctc 29.187592 loss_rnnt 19.200502 hw_loss 0.106789 lr 0.00041501 rank 1
2023-02-21 17:05:38,579 DEBUG TRAIN Batch 17/3400 loss 11.985823 loss_att 14.313513 loss_ctc 15.152193 loss_rnnt 11.013554 hw_loss 0.158526 lr 0.00041494 rank 3
2023-02-21 17:05:38,619 DEBUG TRAIN Batch 17/3400 loss 12.607046 loss_att 15.440172 loss_ctc 20.190773 loss_rnnt 10.959659 hw_loss 0.130499 lr 0.00041498 rank 0
2023-02-21 17:06:57,762 DEBUG TRAIN Batch 17/3500 loss 4.558017 loss_att 8.338469 loss_ctc 5.620567 loss_rnnt 3.592502 hw_loss 0.127035 lr 0.00041476 rank 7
2023-02-21 17:06:57,762 DEBUG TRAIN Batch 17/3500 loss 9.523192 loss_att 13.591155 loss_ctc 15.918573 loss_rnnt 7.831826 hw_loss 0.046981 lr 0.00041481 rank 2
2023-02-21 17:06:57,763 DEBUG TRAIN Batch 17/3500 loss 6.538142 loss_att 8.619682 loss_ctc 8.129793 loss_rnnt 5.823000 hw_loss 0.162401 lr 0.00041483 rank 4
2023-02-21 17:06:57,764 DEBUG TRAIN Batch 17/3500 loss 12.046211 loss_att 15.888432 loss_ctc 16.361431 loss_rnnt 10.617880 hw_loss 0.158484 lr 0.00041481 rank 6
2023-02-21 17:06:57,767 DEBUG TRAIN Batch 17/3500 loss 12.817760 loss_att 15.992876 loss_ctc 19.815014 loss_rnnt 11.186443 hw_loss 0.118738 lr 0.00041484 rank 0
2023-02-21 17:06:57,768 DEBUG TRAIN Batch 17/3500 loss 12.985812 loss_att 18.497375 loss_ctc 15.352560 loss_rnnt 11.537365 hw_loss 0.057313 lr 0.00041477 rank 5
2023-02-21 17:06:57,767 DEBUG TRAIN Batch 17/3500 loss 11.115816 loss_att 14.928869 loss_ctc 12.527976 loss_rnnt 10.110843 hw_loss 0.101390 lr 0.00041486 rank 1
2023-02-21 17:06:57,770 DEBUG TRAIN Batch 17/3500 loss 9.524282 loss_att 10.763243 loss_ctc 10.405455 loss_rnnt 9.091276 hw_loss 0.126982 lr 0.00041480 rank 3
2023-02-21 17:08:17,683 DEBUG TRAIN Batch 17/3600 loss 5.007122 loss_att 7.684612 loss_ctc 7.430567 loss_rnnt 4.104601 hw_loss 0.082305 lr 0.00041466 rank 6
2023-02-21 17:08:17,686 DEBUG TRAIN Batch 17/3600 loss 15.401633 loss_att 18.429260 loss_ctc 21.124506 loss_rnnt 13.947347 hw_loss 0.160708 lr 0.00041469 rank 4
2023-02-21 17:08:17,688 DEBUG TRAIN Batch 17/3600 loss 21.843496 loss_att 20.887426 loss_ctc 27.378687 loss_rnnt 21.220661 hw_loss 0.142544 lr 0.00041472 rank 1
2023-02-21 17:08:17,688 DEBUG TRAIN Batch 17/3600 loss 5.715711 loss_att 7.446128 loss_ctc 6.513371 loss_rnnt 5.221275 hw_loss 0.078744 lr 0.00041470 rank 0
2023-02-21 17:08:17,688 DEBUG TRAIN Batch 17/3600 loss 5.918222 loss_att 9.069036 loss_ctc 11.677607 loss_rnnt 4.457754 hw_loss 0.116978 lr 0.00041462 rank 7
2023-02-21 17:08:17,689 DEBUG TRAIN Batch 17/3600 loss 16.565136 loss_att 22.441553 loss_ctc 24.326162 loss_rnnt 14.253060 hw_loss 0.191227 lr 0.00041466 rank 2
2023-02-21 17:08:17,691 DEBUG TRAIN Batch 17/3600 loss 13.486887 loss_att 14.215733 loss_ctc 15.602169 loss_rnnt 12.970260 hw_loss 0.166540 lr 0.00041463 rank 5
2023-02-21 17:08:17,743 DEBUG TRAIN Batch 17/3600 loss 10.398735 loss_att 13.510455 loss_ctc 15.979021 loss_rnnt 8.966394 hw_loss 0.123673 lr 0.00041465 rank 3
2023-02-21 17:09:35,917 DEBUG TRAIN Batch 17/3700 loss 10.551182 loss_att 12.965282 loss_ctc 14.650123 loss_rnnt 9.440281 hw_loss 0.152914 lr 0.00041454 rank 4
2023-02-21 17:09:35,918 DEBUG TRAIN Batch 17/3700 loss 7.634966 loss_att 9.529198 loss_ctc 8.659285 loss_rnnt 7.013424 hw_loss 0.198976 lr 0.00041452 rank 2
2023-02-21 17:09:35,920 DEBUG TRAIN Batch 17/3700 loss 19.563158 loss_att 19.693571 loss_ctc 23.019697 loss_rnnt 19.019827 hw_loss 0.105709 lr 0.00041451 rank 3
2023-02-21 17:09:35,922 DEBUG TRAIN Batch 17/3700 loss 12.904083 loss_att 15.885155 loss_ctc 19.290955 loss_rnnt 11.436327 hw_loss 0.037423 lr 0.00041458 rank 1
2023-02-21 17:09:35,922 DEBUG TRAIN Batch 17/3700 loss 12.736919 loss_att 17.684462 loss_ctc 16.218168 loss_rnnt 11.241370 hw_loss 0.078516 lr 0.00041455 rank 0
2023-02-21 17:09:35,924 DEBUG TRAIN Batch 17/3700 loss 4.541798 loss_att 8.564280 loss_ctc 6.109986 loss_rnnt 3.467740 hw_loss 0.113381 lr 0.00041448 rank 7
2023-02-21 17:09:35,927 DEBUG TRAIN Batch 17/3700 loss 5.352037 loss_att 7.246448 loss_ctc 8.515898 loss_rnnt 4.458838 hw_loss 0.173380 lr 0.00041449 rank 5
2023-02-21 17:09:35,974 DEBUG TRAIN Batch 17/3700 loss 12.639726 loss_att 13.208326 loss_ctc 15.488409 loss_rnnt 12.072502 hw_loss 0.138150 lr 0.00041452 rank 6
2023-02-21 17:10:55,383 DEBUG TRAIN Batch 17/3800 loss 7.662444 loss_att 6.889620 loss_ctc 8.841690 loss_rnnt 7.546688 hw_loss 0.212040 lr 0.00041433 rank 7
2023-02-21 17:10:55,387 DEBUG TRAIN Batch 17/3800 loss 7.098595 loss_att 11.426934 loss_ctc 10.755106 loss_rnnt 5.687556 hw_loss 0.108443 lr 0.00041438 rank 6
2023-02-21 17:10:55,388 DEBUG TRAIN Batch 17/3800 loss 11.024852 loss_att 11.104971 loss_ctc 14.224560 loss_rnnt 10.433331 hw_loss 0.279131 lr 0.00041444 rank 1
2023-02-21 17:10:55,389 DEBUG TRAIN Batch 17/3800 loss 2.109460 loss_att 4.308488 loss_ctc 3.623823 loss_rnnt 1.430942 hw_loss 0.068994 lr 0.00041438 rank 2
2023-02-21 17:10:55,389 DEBUG TRAIN Batch 17/3800 loss 2.637081 loss_att 4.024621 loss_ctc 4.016428 loss_rnnt 1.916279 hw_loss 0.486340 lr 0.00041440 rank 4
2023-02-21 17:10:55,395 DEBUG TRAIN Batch 17/3800 loss 10.315844 loss_att 13.085339 loss_ctc 15.739582 loss_rnnt 8.925077 hw_loss 0.213191 lr 0.00041441 rank 0
2023-02-21 17:10:55,395 DEBUG TRAIN Batch 17/3800 loss 6.864918 loss_att 8.072754 loss_ctc 8.908198 loss_rnnt 6.226611 hw_loss 0.233065 lr 0.00041434 rank 5
2023-02-21 17:10:55,397 DEBUG TRAIN Batch 17/3800 loss 12.662772 loss_att 17.062654 loss_ctc 16.085018 loss_rnnt 11.300295 hw_loss 0.049125 lr 0.00041437 rank 3
2023-02-21 17:12:16,403 DEBUG TRAIN Batch 17/3900 loss 9.642903 loss_att 14.733738 loss_ctc 14.706230 loss_rnnt 7.872278 hw_loss 0.145028 lr 0.00041426 rank 4
2023-02-21 17:12:16,405 DEBUG TRAIN Batch 17/3900 loss 13.902370 loss_att 15.127138 loss_ctc 18.664923 loss_rnnt 12.927275 hw_loss 0.178379 lr 0.00041429 rank 1
2023-02-21 17:12:16,408 DEBUG TRAIN Batch 17/3900 loss 6.615617 loss_att 8.042083 loss_ctc 10.765825 loss_rnnt 5.704220 hw_loss 0.136392 lr 0.00041423 rank 3
2023-02-21 17:12:16,409 DEBUG TRAIN Batch 17/3900 loss 10.697067 loss_att 15.695803 loss_ctc 15.257716 loss_rnnt 9.005965 hw_loss 0.156131 lr 0.00041424 rank 2
2023-02-21 17:12:16,410 DEBUG TRAIN Batch 17/3900 loss 16.661942 loss_att 18.286945 loss_ctc 19.420792 loss_rnnt 15.923850 hw_loss 0.084827 lr 0.00041424 rank 6
2023-02-21 17:12:16,411 DEBUG TRAIN Batch 17/3900 loss 5.510340 loss_att 10.325165 loss_ctc 9.636882 loss_rnnt 3.949306 hw_loss 0.089742 lr 0.00041419 rank 7
2023-02-21 17:12:16,411 DEBUG TRAIN Batch 17/3900 loss 14.527797 loss_att 19.310982 loss_ctc 22.620998 loss_rnnt 12.448101 hw_loss 0.082433 lr 0.00041427 rank 0
2023-02-21 17:12:16,414 DEBUG TRAIN Batch 17/3900 loss 7.111830 loss_att 14.499475 loss_ctc 14.645121 loss_rnnt 4.616129 hw_loss 0.025749 lr 0.00041420 rank 5
2023-02-21 17:13:35,593 DEBUG TRAIN Batch 17/4000 loss 18.800228 loss_att 20.172077 loss_ctc 24.175079 loss_rnnt 17.646395 hw_loss 0.305281 lr 0.00041415 rank 1
2023-02-21 17:13:35,595 DEBUG TRAIN Batch 17/4000 loss 10.621659 loss_att 13.625786 loss_ctc 13.601833 loss_rnnt 9.557479 hw_loss 0.123746 lr 0.00041412 rank 4
2023-02-21 17:13:35,596 DEBUG TRAIN Batch 17/4000 loss 20.033905 loss_att 21.767771 loss_ctc 23.408333 loss_rnnt 19.148453 hw_loss 0.166416 lr 0.00041409 rank 2
2023-02-21 17:13:35,596 DEBUG TRAIN Batch 17/4000 loss 7.335964 loss_att 12.025268 loss_ctc 12.207249 loss_rnnt 5.699388 hw_loss 0.092269 lr 0.00041413 rank 0
2023-02-21 17:13:35,597 DEBUG TRAIN Batch 17/4000 loss 13.753135 loss_att 16.566772 loss_ctc 17.812943 loss_rnnt 12.490161 hw_loss 0.298010 lr 0.00041410 rank 6
2023-02-21 17:13:35,597 DEBUG TRAIN Batch 17/4000 loss 12.296119 loss_att 13.945876 loss_ctc 12.813170 loss_rnnt 11.859897 hw_loss 0.069995 lr 0.00041405 rank 7
2023-02-21 17:13:35,601 DEBUG TRAIN Batch 17/4000 loss 5.393605 loss_att 7.911922 loss_ctc 6.801566 loss_rnnt 4.674919 hw_loss 0.051175 lr 0.00041409 rank 3
2023-02-21 17:13:35,655 DEBUG TRAIN Batch 17/4000 loss 15.976707 loss_att 18.031586 loss_ctc 21.201788 loss_rnnt 14.854541 hw_loss 0.027212 lr 0.00041406 rank 5
2023-02-21 17:14:53,768 DEBUG TRAIN Batch 17/4100 loss 9.194856 loss_att 11.622937 loss_ctc 16.086184 loss_rnnt 7.758565 hw_loss 0.059684 lr 0.00041398 rank 4
2023-02-21 17:14:53,774 DEBUG TRAIN Batch 17/4100 loss 5.558065 loss_att 7.874497 loss_ctc 6.991347 loss_rnnt 4.815264 hw_loss 0.165768 lr 0.00041395 rank 2
2023-02-21 17:14:53,775 DEBUG TRAIN Batch 17/4100 loss 10.444512 loss_att 13.422704 loss_ctc 15.354853 loss_rnnt 9.137165 hw_loss 0.106869 lr 0.00041395 rank 6
2023-02-21 17:14:53,776 DEBUG TRAIN Batch 17/4100 loss 7.605859 loss_att 8.390196 loss_ctc 8.136678 loss_rnnt 7.362385 hw_loss 0.029683 lr 0.00041394 rank 3
2023-02-21 17:14:53,779 DEBUG TRAIN Batch 17/4100 loss 5.928132 loss_att 10.301277 loss_ctc 14.722944 loss_rnnt 3.800042 hw_loss 0.151537 lr 0.00041392 rank 5
2023-02-21 17:14:53,781 DEBUG TRAIN Batch 17/4100 loss 10.981318 loss_att 13.881966 loss_ctc 12.300492 loss_rnnt 10.142838 hw_loss 0.154614 lr 0.00041391 rank 7
2023-02-21 17:14:53,781 DEBUG TRAIN Batch 17/4100 loss 7.164170 loss_att 10.011555 loss_ctc 10.209721 loss_rnnt 6.121727 hw_loss 0.125424 lr 0.00041401 rank 1
2023-02-21 17:14:53,827 DEBUG TRAIN Batch 17/4100 loss 12.406679 loss_att 17.280107 loss_ctc 19.379612 loss_rnnt 10.403992 hw_loss 0.184268 lr 0.00041399 rank 0
2023-02-21 17:16:12,941 DEBUG TRAIN Batch 17/4200 loss 8.693062 loss_att 13.296324 loss_ctc 14.289695 loss_rnnt 6.937877 hw_loss 0.165591 lr 0.00041381 rank 2
2023-02-21 17:16:12,943 DEBUG TRAIN Batch 17/4200 loss 12.775352 loss_att 14.343386 loss_ctc 15.468077 loss_rnnt 12.046200 hw_loss 0.105966 lr 0.00041387 rank 1
2023-02-21 17:16:12,943 DEBUG TRAIN Batch 17/4200 loss 7.173258 loss_att 10.890270 loss_ctc 12.077504 loss_rnnt 5.714171 hw_loss 0.115847 lr 0.00041377 rank 7
2023-02-21 17:16:12,945 DEBUG TRAIN Batch 17/4200 loss 11.951936 loss_att 14.608013 loss_ctc 18.459776 loss_rnnt 10.532533 hw_loss 0.038391 lr 0.00041383 rank 4
2023-02-21 17:16:12,945 DEBUG TRAIN Batch 17/4200 loss 9.337110 loss_att 13.908237 loss_ctc 12.179865 loss_rnnt 7.983632 hw_loss 0.112908 lr 0.00041378 rank 5
2023-02-21 17:16:12,947 DEBUG TRAIN Batch 17/4200 loss 7.447795 loss_att 9.532143 loss_ctc 9.396303 loss_rnnt 6.692581 hw_loss 0.147270 lr 0.00041384 rank 0
2023-02-21 17:16:12,951 DEBUG TRAIN Batch 17/4200 loss 14.228356 loss_att 18.496767 loss_ctc 20.909084 loss_rnnt 12.392599 hw_loss 0.171212 lr 0.00041380 rank 3
2023-02-21 17:16:12,992 DEBUG TRAIN Batch 17/4200 loss 13.886724 loss_att 15.392425 loss_ctc 18.360657 loss_rnnt 12.937280 hw_loss 0.097086 lr 0.00041381 rank 6
2023-02-21 17:17:34,909 DEBUG TRAIN Batch 17/4300 loss 11.457016 loss_att 15.538427 loss_ctc 16.836988 loss_rnnt 9.870166 hw_loss 0.099822 lr 0.00041362 rank 7
2023-02-21 17:17:34,911 DEBUG TRAIN Batch 17/4300 loss 13.891315 loss_att 16.121944 loss_ctc 19.522671 loss_rnnt 12.614777 hw_loss 0.149186 lr 0.00041367 rank 2
2023-02-21 17:17:34,911 DEBUG TRAIN Batch 17/4300 loss 5.784030 loss_att 8.251183 loss_ctc 6.616738 loss_rnnt 5.119918 hw_loss 0.111851 lr 0.00041370 rank 0
2023-02-21 17:17:34,912 DEBUG TRAIN Batch 17/4300 loss 16.841127 loss_att 16.595337 loss_ctc 22.723122 loss_rnnt 16.032427 hw_loss 0.137981 lr 0.00041369 rank 4
2023-02-21 17:17:34,913 DEBUG TRAIN Batch 17/4300 loss 6.351194 loss_att 10.125618 loss_ctc 9.492422 loss_rnnt 5.141371 hw_loss 0.067704 lr 0.00041363 rank 5
2023-02-21 17:17:34,914 DEBUG TRAIN Batch 17/4300 loss 8.590647 loss_att 13.526565 loss_ctc 13.456965 loss_rnnt 6.900066 hw_loss 0.102288 lr 0.00041373 rank 1
2023-02-21 17:17:34,918 DEBUG TRAIN Batch 17/4300 loss 5.262057 loss_att 6.840683 loss_ctc 7.235874 loss_rnnt 4.582621 hw_loss 0.188503 lr 0.00041366 rank 3
2023-02-21 17:17:34,955 DEBUG TRAIN Batch 17/4300 loss 8.782816 loss_att 9.950122 loss_ctc 12.515662 loss_rnnt 8.014497 hw_loss 0.069647 lr 0.00041367 rank 6
2023-02-21 17:18:54,262 DEBUG TRAIN Batch 17/4400 loss 11.671662 loss_att 13.110743 loss_ctc 16.499292 loss_rnnt 10.671928 hw_loss 0.127938 lr 0.00041348 rank 7
2023-02-21 17:18:54,262 DEBUG TRAIN Batch 17/4400 loss 22.121933 loss_att 26.593760 loss_ctc 35.275177 loss_rnnt 19.411320 hw_loss 0.117153 lr 0.00041353 rank 2
2023-02-21 17:18:54,265 DEBUG TRAIN Batch 17/4400 loss 12.962005 loss_att 13.844385 loss_ctc 19.870754 loss_rnnt 11.815838 hw_loss 0.090983 lr 0.00041358 rank 1
2023-02-21 17:18:54,267 DEBUG TRAIN Batch 17/4400 loss 7.780442 loss_att 11.389866 loss_ctc 11.257573 loss_rnnt 6.500787 hw_loss 0.176537 lr 0.00041356 rank 0
2023-02-21 17:18:54,269 DEBUG TRAIN Batch 17/4400 loss 12.321666 loss_att 13.628416 loss_ctc 17.079967 loss_rnnt 11.371367 hw_loss 0.102203 lr 0.00041353 rank 6
2023-02-21 17:18:54,271 DEBUG TRAIN Batch 17/4400 loss 11.875028 loss_att 15.759708 loss_ctc 14.860962 loss_rnnt 10.614490 hw_loss 0.160270 lr 0.00041355 rank 4
2023-02-21 17:18:54,277 DEBUG TRAIN Batch 17/4400 loss 13.976166 loss_att 17.923065 loss_ctc 21.974699 loss_rnnt 12.069563 hw_loss 0.095162 lr 0.00041352 rank 3
2023-02-21 17:18:54,324 DEBUG TRAIN Batch 17/4400 loss 8.410084 loss_att 10.502600 loss_ctc 13.220003 loss_rnnt 7.238019 hw_loss 0.210446 lr 0.00041349 rank 5
2023-02-21 17:20:13,553 DEBUG TRAIN Batch 17/4500 loss 12.170763 loss_att 13.631981 loss_ctc 17.184086 loss_rnnt 11.111101 hw_loss 0.185579 lr 0.00041334 rank 7
2023-02-21 17:20:13,554 DEBUG TRAIN Batch 17/4500 loss 29.966682 loss_att 30.686012 loss_ctc 35.200905 loss_rnnt 29.023029 hw_loss 0.191043 lr 0.00041341 rank 4
2023-02-21 17:20:13,555 DEBUG TRAIN Batch 17/4500 loss 17.743687 loss_att 14.401752 loss_ctc 24.202230 loss_rnnt 17.515869 hw_loss 0.065746 lr 0.00041344 rank 1
2023-02-21 17:20:13,554 DEBUG TRAIN Batch 17/4500 loss 22.008312 loss_att 25.920628 loss_ctc 26.251339 loss_rnnt 20.616201 hw_loss 0.082335 lr 0.00041339 rank 2
2023-02-21 17:20:13,558 DEBUG TRAIN Batch 17/4500 loss 10.907714 loss_att 11.975424 loss_ctc 14.239282 loss_rnnt 10.192995 hw_loss 0.106817 lr 0.00041335 rank 5
2023-02-21 17:20:13,558 DEBUG TRAIN Batch 17/4500 loss 10.560908 loss_att 12.763369 loss_ctc 13.602680 loss_rnnt 9.699702 hw_loss 0.028399 lr 0.00041342 rank 0
2023-02-21 17:20:13,558 DEBUG TRAIN Batch 17/4500 loss 4.677448 loss_att 7.308119 loss_ctc 8.340049 loss_rnnt 3.636883 hw_loss 0.048909 lr 0.00041338 rank 3
2023-02-21 17:20:13,607 DEBUG TRAIN Batch 17/4500 loss 12.959082 loss_att 13.392792 loss_ctc 14.751025 loss_rnnt 12.571104 hw_loss 0.116833 lr 0.00041339 rank 6
2023-02-21 17:21:34,706 DEBUG TRAIN Batch 17/4600 loss 8.387658 loss_att 9.520769 loss_ctc 10.335975 loss_rnnt 7.886556 hw_loss 0.027570 lr 0.00041327 rank 4
2023-02-21 17:21:34,707 DEBUG TRAIN Batch 17/4600 loss 15.423056 loss_att 18.445581 loss_ctc 20.749802 loss_rnnt 14.058589 hw_loss 0.093243 lr 0.00041330 rank 1
2023-02-21 17:21:34,711 DEBUG TRAIN Batch 17/4600 loss 17.439556 loss_att 19.198133 loss_ctc 22.283016 loss_rnnt 16.427526 hw_loss 0.027224 lr 0.00041325 rank 6
2023-02-21 17:21:34,711 DEBUG TRAIN Batch 17/4600 loss 11.233567 loss_att 12.627318 loss_ctc 15.012220 loss_rnnt 10.386093 hw_loss 0.121695 lr 0.00041324 rank 2
2023-02-21 17:21:34,712 DEBUG TRAIN Batch 17/4600 loss 5.492216 loss_att 11.285440 loss_ctc 8.524153 loss_rnnt 3.834840 hw_loss 0.177137 lr 0.00041320 rank 7
2023-02-21 17:21:34,716 DEBUG TRAIN Batch 17/4600 loss 8.270174 loss_att 12.268106 loss_ctc 14.353175 loss_rnnt 6.597750 hw_loss 0.115822 lr 0.00041321 rank 5
2023-02-21 17:21:34,716 DEBUG TRAIN Batch 17/4600 loss 7.477876 loss_att 12.144291 loss_ctc 11.384517 loss_rnnt 6.009840 hw_loss 0.026002 lr 0.00041324 rank 3
2023-02-21 17:21:34,717 DEBUG TRAIN Batch 17/4600 loss 22.185200 loss_att 22.858187 loss_ctc 31.332069 loss_rnnt 20.797295 hw_loss 0.063232 lr 0.00041328 rank 0
2023-02-21 17:22:54,125 DEBUG TRAIN Batch 17/4700 loss 12.002167 loss_att 14.158141 loss_ctc 16.373154 loss_rnnt 10.853473 hw_loss 0.252565 lr 0.00041314 rank 0
2023-02-21 17:22:54,125 DEBUG TRAIN Batch 17/4700 loss 6.072799 loss_att 8.251537 loss_ctc 8.242363 loss_rnnt 5.270086 hw_loss 0.145669 lr 0.00041310 rank 6
2023-02-21 17:22:54,127 DEBUG TRAIN Batch 17/4700 loss 7.265611 loss_att 9.414278 loss_ctc 9.568820 loss_rnnt 6.502954 hw_loss 0.048427 lr 0.00041306 rank 7
2023-02-21 17:22:54,130 DEBUG TRAIN Batch 17/4700 loss 8.138589 loss_att 9.925201 loss_ctc 9.531161 loss_rnnt 7.539727 hw_loss 0.104742 lr 0.00041310 rank 2
2023-02-21 17:22:54,130 DEBUG TRAIN Batch 17/4700 loss 11.287762 loss_att 12.805389 loss_ctc 13.939073 loss_rnnt 10.560415 hw_loss 0.131835 lr 0.00041313 rank 4
2023-02-21 17:22:54,134 DEBUG TRAIN Batch 17/4700 loss 8.634999 loss_att 10.657521 loss_ctc 9.600935 loss_rnnt 8.025914 hw_loss 0.142104 lr 0.00041307 rank 5
2023-02-21 17:22:54,134 DEBUG TRAIN Batch 17/4700 loss 17.686581 loss_att 19.588520 loss_ctc 24.862410 loss_rnnt 16.267714 hw_loss 0.153188 lr 0.00041316 rank 1
2023-02-21 17:22:54,140 DEBUG TRAIN Batch 17/4700 loss 12.943189 loss_att 15.815264 loss_ctc 15.977476 loss_rnnt 11.924803 hw_loss 0.073875 lr 0.00041309 rank 3
2023-02-21 17:24:13,136 DEBUG TRAIN Batch 17/4800 loss 5.866044 loss_att 11.016516 loss_ctc 7.360955 loss_rnnt 4.560253 hw_loss 0.143203 lr 0.00041302 rank 1
2023-02-21 17:24:13,140 DEBUG TRAIN Batch 17/4800 loss 17.651936 loss_att 21.526009 loss_ctc 24.884781 loss_rnnt 15.872866 hw_loss 0.074767 lr 0.00041292 rank 7
2023-02-21 17:24:13,142 DEBUG TRAIN Batch 17/4800 loss 6.037828 loss_att 7.595071 loss_ctc 7.473078 loss_rnnt 5.441695 hw_loss 0.174970 lr 0.00041296 rank 6
2023-02-21 17:24:13,142 DEBUG TRAIN Batch 17/4800 loss 12.034558 loss_att 15.424252 loss_ctc 19.436947 loss_rnnt 10.307676 hw_loss 0.116172 lr 0.00041299 rank 4
2023-02-21 17:24:13,142 DEBUG TRAIN Batch 17/4800 loss 12.611886 loss_att 13.973257 loss_ctc 16.844305 loss_rnnt 11.759177 hw_loss 0.030210 lr 0.00041296 rank 2
2023-02-21 17:24:13,143 DEBUG TRAIN Batch 17/4800 loss 8.320048 loss_att 10.528006 loss_ctc 12.514862 loss_rnnt 7.255658 hw_loss 0.119042 lr 0.00041300 rank 0
2023-02-21 17:24:13,151 DEBUG TRAIN Batch 17/4800 loss 7.539490 loss_att 10.562424 loss_ctc 12.133911 loss_rnnt 6.245993 hw_loss 0.143101 lr 0.00041293 rank 5
2023-02-21 17:24:13,188 DEBUG TRAIN Batch 17/4800 loss 12.058380 loss_att 18.001497 loss_ctc 20.466024 loss_rnnt 9.618355 hw_loss 0.244467 lr 0.00041295 rank 3
2023-02-21 17:25:33,142 DEBUG TRAIN Batch 17/4900 loss 10.860360 loss_att 12.007733 loss_ctc 14.963345 loss_rnnt 10.029704 hw_loss 0.101472 lr 0.00041288 rank 1
2023-02-21 17:25:33,143 DEBUG TRAIN Batch 17/4900 loss 5.875806 loss_att 8.957152 loss_ctc 5.220908 loss_rnnt 5.268899 hw_loss 0.146171 lr 0.00041282 rank 2
2023-02-21 17:25:33,143 DEBUG TRAIN Batch 17/4900 loss 15.678996 loss_att 23.285259 loss_ctc 22.623594 loss_rnnt 13.123108 hw_loss 0.203791 lr 0.00041278 rank 7
2023-02-21 17:25:33,145 DEBUG TRAIN Batch 17/4900 loss 10.553849 loss_att 11.567862 loss_ctc 14.574550 loss_rnnt 9.772750 hw_loss 0.079133 lr 0.00041282 rank 6
2023-02-21 17:25:33,146 DEBUG TRAIN Batch 17/4900 loss 14.496168 loss_att 17.199621 loss_ctc 21.134962 loss_rnnt 12.993735 hw_loss 0.143572 lr 0.00041281 rank 3
2023-02-21 17:25:33,147 DEBUG TRAIN Batch 17/4900 loss 17.666918 loss_att 21.426348 loss_ctc 24.136944 loss_rnnt 15.997375 hw_loss 0.103099 lr 0.00041285 rank 4
2023-02-21 17:25:33,148 DEBUG TRAIN Batch 17/4900 loss 13.450102 loss_att 15.410816 loss_ctc 15.491882 loss_rnnt 12.732695 hw_loss 0.099426 lr 0.00041279 rank 5
2023-02-21 17:25:33,148 DEBUG TRAIN Batch 17/4900 loss 3.492614 loss_att 7.014705 loss_ctc 5.674454 loss_rnnt 2.416363 hw_loss 0.151725 lr 0.00041286 rank 0
2023-02-21 17:26:54,010 DEBUG TRAIN Batch 17/5000 loss 11.660536 loss_att 14.116755 loss_ctc 14.458364 loss_rnnt 10.744891 hw_loss 0.096295 lr 0.00041274 rank 1
2023-02-21 17:26:54,013 DEBUG TRAIN Batch 17/5000 loss 13.720005 loss_att 20.148386 loss_ctc 18.618023 loss_rnnt 11.748652 hw_loss 0.061141 lr 0.00041268 rank 2
2023-02-21 17:26:54,015 DEBUG TRAIN Batch 17/5000 loss 14.341870 loss_att 15.722429 loss_ctc 18.364838 loss_rnnt 13.436804 hw_loss 0.173548 lr 0.00041268 rank 6
2023-02-21 17:26:54,015 DEBUG TRAIN Batch 17/5000 loss 11.725037 loss_att 12.392365 loss_ctc 14.727216 loss_rnnt 11.101084 hw_loss 0.169121 lr 0.00041264 rank 7
2023-02-21 17:26:54,016 DEBUG TRAIN Batch 17/5000 loss 16.265282 loss_att 18.917891 loss_ctc 20.252323 loss_rnnt 15.153732 hw_loss 0.092669 lr 0.00041271 rank 0
2023-02-21 17:26:54,016 DEBUG TRAIN Batch 17/5000 loss 9.177505 loss_att 10.053151 loss_ctc 11.023565 loss_rnnt 8.624344 hw_loss 0.247295 lr 0.00041270 rank 4
2023-02-21 17:26:54,021 DEBUG TRAIN Batch 17/5000 loss 11.545923 loss_att 12.099859 loss_ctc 12.495952 loss_rnnt 11.281729 hw_loss 0.050133 lr 0.00041267 rank 3
2023-02-21 17:26:54,068 DEBUG TRAIN Batch 17/5000 loss 7.865180 loss_att 11.858454 loss_ctc 10.851837 loss_rnnt 6.598547 hw_loss 0.130795 lr 0.00041265 rank 5
2023-02-21 17:28:11,955 DEBUG TRAIN Batch 17/5100 loss 2.957032 loss_att 9.136160 loss_ctc 3.624422 loss_rnnt 1.553891 hw_loss 0.146868 lr 0.00041254 rank 2
2023-02-21 17:28:11,957 DEBUG TRAIN Batch 17/5100 loss 12.236088 loss_att 12.061616 loss_ctc 15.948862 loss_rnnt 11.676618 hw_loss 0.186241 lr 0.00041250 rank 7
2023-02-21 17:28:11,960 DEBUG TRAIN Batch 17/5100 loss 9.862814 loss_att 10.928275 loss_ctc 10.084473 loss_rnnt 9.559139 hw_loss 0.114428 lr 0.00041256 rank 4
2023-02-21 17:28:11,962 DEBUG TRAIN Batch 17/5100 loss 5.839766 loss_att 8.743814 loss_ctc 7.239782 loss_rnnt 5.057175 hw_loss 0.028336 lr 0.00041254 rank 6
2023-02-21 17:28:11,964 DEBUG TRAIN Batch 17/5100 loss 10.816635 loss_att 10.686830 loss_ctc 13.915379 loss_rnnt 10.305560 hw_loss 0.232258 lr 0.00041257 rank 0
2023-02-21 17:28:11,967 DEBUG TRAIN Batch 17/5100 loss 7.572195 loss_att 11.539190 loss_ctc 11.543033 loss_rnnt 6.202155 hw_loss 0.088492 lr 0.00041251 rank 5
2023-02-21 17:28:11,969 DEBUG TRAIN Batch 17/5100 loss 11.299649 loss_att 15.351860 loss_ctc 16.409338 loss_rnnt 9.728243 hw_loss 0.149386 lr 0.00041253 rank 3
2023-02-21 17:28:11,971 DEBUG TRAIN Batch 17/5100 loss 9.592447 loss_att 14.628678 loss_ctc 12.075511 loss_rnnt 8.187285 hw_loss 0.125328 lr 0.00041260 rank 1
2023-02-21 17:29:31,000 DEBUG TRAIN Batch 17/5200 loss 9.186791 loss_att 12.364136 loss_ctc 14.084735 loss_rnnt 7.771110 hw_loss 0.238411 lr 0.00041236 rank 7
2023-02-21 17:29:31,002 DEBUG TRAIN Batch 17/5200 loss 3.532167 loss_att 7.854857 loss_ctc 5.303637 loss_rnnt 2.288385 hw_loss 0.268215 lr 0.00041246 rank 1
2023-02-21 17:29:31,002 DEBUG TRAIN Batch 17/5200 loss 3.500239 loss_att 6.039372 loss_ctc 5.737194 loss_rnnt 2.655668 hw_loss 0.072157 lr 0.00041240 rank 2
2023-02-21 17:29:31,007 DEBUG TRAIN Batch 17/5200 loss 6.970643 loss_att 11.985506 loss_ctc 10.061293 loss_rnnt 5.518482 hw_loss 0.069567 lr 0.00041240 rank 6
2023-02-21 17:29:31,009 DEBUG TRAIN Batch 17/5200 loss 26.105486 loss_att 29.198408 loss_ctc 28.835455 loss_rnnt 24.967405 hw_loss 0.291569 lr 0.00041242 rank 4
2023-02-21 17:29:31,011 DEBUG TRAIN Batch 17/5200 loss 15.644468 loss_att 14.881129 loss_ctc 18.740566 loss_rnnt 15.330816 hw_loss 0.100327 lr 0.00041239 rank 3
2023-02-21 17:29:31,012 DEBUG TRAIN Batch 17/5200 loss 3.728651 loss_att 7.058268 loss_ctc 5.151340 loss_rnnt 2.777030 hw_loss 0.180010 lr 0.00041243 rank 0
2023-02-21 17:29:31,060 DEBUG TRAIN Batch 17/5200 loss 6.488970 loss_att 9.183269 loss_ctc 7.064246 loss_rnnt 5.849108 hw_loss 0.045560 lr 0.00041237 rank 5
2023-02-21 17:30:50,769 DEBUG TRAIN Batch 17/5300 loss 8.018138 loss_att 9.675728 loss_ctc 14.954789 loss_rnnt 6.674762 hw_loss 0.163069 lr 0.00041226 rank 2
2023-02-21 17:30:50,770 DEBUG TRAIN Batch 17/5300 loss 6.939092 loss_att 9.177076 loss_ctc 9.531636 loss_rnnt 6.091534 hw_loss 0.101790 lr 0.00041225 rank 3
2023-02-21 17:30:50,771 DEBUG TRAIN Batch 17/5300 loss 4.320279 loss_att 9.653808 loss_ctc 8.560337 loss_rnnt 2.573903 hw_loss 0.214366 lr 0.00041232 rank 1
2023-02-21 17:30:50,773 DEBUG TRAIN Batch 17/5300 loss 2.806676 loss_att 6.224923 loss_ctc 4.509440 loss_rnnt 1.860347 hw_loss 0.066832 lr 0.00041222 rank 7
2023-02-21 17:30:50,773 DEBUG TRAIN Batch 17/5300 loss 10.471137 loss_att 13.473636 loss_ctc 13.143176 loss_rnnt 9.500505 hw_loss 0.025989 lr 0.00041229 rank 0
2023-02-21 17:30:50,775 DEBUG TRAIN Batch 17/5300 loss 6.990792 loss_att 10.987059 loss_ctc 11.103798 loss_rnnt 5.575439 hw_loss 0.126935 lr 0.00041228 rank 4
2023-02-21 17:30:50,776 DEBUG TRAIN Batch 17/5300 loss 9.270412 loss_att 14.005893 loss_ctc 13.865355 loss_rnnt 7.650542 hw_loss 0.112714 lr 0.00041226 rank 6
2023-02-21 17:30:50,778 DEBUG TRAIN Batch 17/5300 loss 12.195990 loss_att 17.114227 loss_ctc 15.969748 loss_rnnt 10.694944 hw_loss 0.026684 lr 0.00041223 rank 5
2023-02-21 17:32:09,984 DEBUG TRAIN Batch 17/5400 loss 21.373861 loss_att 24.400677 loss_ctc 26.516705 loss_rnnt 20.065481 hw_loss 0.032447 lr 0.00041212 rank 2
2023-02-21 17:32:09,986 DEBUG TRAIN Batch 17/5400 loss 5.257744 loss_att 8.539302 loss_ctc 8.486013 loss_rnnt 4.089802 hw_loss 0.152240 lr 0.00041218 rank 1
2023-02-21 17:32:09,987 DEBUG TRAIN Batch 17/5400 loss 7.109923 loss_att 10.415078 loss_ctc 10.915813 loss_rnnt 5.896094 hw_loss 0.085024 lr 0.00041214 rank 4
2023-02-21 17:32:09,989 DEBUG TRAIN Batch 17/5400 loss 21.961100 loss_att 22.648216 loss_ctc 29.478287 loss_rnnt 20.804319 hw_loss 0.032002 lr 0.00041215 rank 0
2023-02-21 17:32:09,989 DEBUG TRAIN Batch 17/5400 loss 7.615214 loss_att 9.544910 loss_ctc 9.570093 loss_rnnt 6.904929 hw_loss 0.119429 lr 0.00041211 rank 3
2023-02-21 17:32:09,989 DEBUG TRAIN Batch 17/5400 loss 9.070901 loss_att 13.257803 loss_ctc 12.128056 loss_rnnt 7.761479 hw_loss 0.120787 lr 0.00041208 rank 7
2023-02-21 17:32:09,991 DEBUG TRAIN Batch 17/5400 loss 4.176844 loss_att 6.445911 loss_ctc 5.468924 loss_rnnt 3.430812 hw_loss 0.224888 lr 0.00041209 rank 5
2023-02-21 17:32:09,991 DEBUG TRAIN Batch 17/5400 loss 14.608607 loss_att 19.132051 loss_ctc 19.303711 loss_rnnt 12.998945 hw_loss 0.148051 lr 0.00041212 rank 6
2023-02-21 17:33:29,032 DEBUG TRAIN Batch 17/5500 loss 17.702658 loss_att 24.029808 loss_ctc 25.212917 loss_rnnt 15.379412 hw_loss 0.105838 lr 0.00041204 rank 1
2023-02-21 17:33:29,035 DEBUG TRAIN Batch 17/5500 loss 9.969418 loss_att 11.848792 loss_ctc 11.419157 loss_rnnt 9.305975 hw_loss 0.176754 lr 0.00041194 rank 7
2023-02-21 17:33:29,036 DEBUG TRAIN Batch 17/5500 loss 7.924031 loss_att 9.588107 loss_ctc 11.295839 loss_rnnt 7.014771 hw_loss 0.237881 lr 0.00041201 rank 0
2023-02-21 17:33:29,038 DEBUG TRAIN Batch 17/5500 loss 12.882341 loss_att 14.543532 loss_ctc 19.142376 loss_rnnt 11.677012 hw_loss 0.072036 lr 0.00041198 rank 2
2023-02-21 17:33:29,039 DEBUG TRAIN Batch 17/5500 loss 11.919714 loss_att 12.951879 loss_ctc 14.507917 loss_rnnt 11.276619 hw_loss 0.171694 lr 0.00041197 rank 3
2023-02-21 17:33:29,040 DEBUG TRAIN Batch 17/5500 loss 4.736437 loss_att 8.992204 loss_ctc 8.153997 loss_rnnt 3.394222 hw_loss 0.066352 lr 0.00041198 rank 6
2023-02-21 17:33:29,041 DEBUG TRAIN Batch 17/5500 loss 14.587980 loss_att 17.375353 loss_ctc 20.200188 loss_rnnt 13.147624 hw_loss 0.252351 lr 0.00041200 rank 4
2023-02-21 17:33:29,046 DEBUG TRAIN Batch 17/5500 loss 7.899543 loss_att 11.121459 loss_ctc 12.247132 loss_rnnt 6.602451 hw_loss 0.136932 lr 0.00041195 rank 5
2023-02-21 17:34:47,422 DEBUG TRAIN Batch 17/5600 loss 5.196285 loss_att 7.250816 loss_ctc 7.646357 loss_rnnt 4.412728 hw_loss 0.086203 lr 0.00041186 rank 4
2023-02-21 17:34:47,423 DEBUG TRAIN Batch 17/5600 loss 6.029666 loss_att 9.297609 loss_ctc 10.470799 loss_rnnt 4.706547 hw_loss 0.145084 lr 0.00041180 rank 7
2023-02-21 17:34:47,427 DEBUG TRAIN Batch 17/5600 loss 11.526825 loss_att 11.625253 loss_ctc 14.569784 loss_rnnt 11.037404 hw_loss 0.120012 lr 0.00041183 rank 3
2023-02-21 17:34:47,428 DEBUG TRAIN Batch 17/5600 loss 12.487640 loss_att 13.588965 loss_ctc 15.722720 loss_rnnt 11.684746 hw_loss 0.283660 lr 0.00041184 rank 2
2023-02-21 17:34:47,429 DEBUG TRAIN Batch 17/5600 loss 6.772294 loss_att 11.327337 loss_ctc 11.119164 loss_rnnt 5.223738 hw_loss 0.108683 lr 0.00041187 rank 0
2023-02-21 17:34:47,432 DEBUG TRAIN Batch 17/5600 loss 9.831288 loss_att 11.048577 loss_ctc 12.892015 loss_rnnt 9.086261 hw_loss 0.175260 lr 0.00041184 rank 6
2023-02-21 17:34:47,432 DEBUG TRAIN Batch 17/5600 loss 11.079115 loss_att 13.235552 loss_ctc 15.608813 loss_rnnt 9.913324 hw_loss 0.244771 lr 0.00041190 rank 1
2023-02-21 17:34:47,476 DEBUG TRAIN Batch 17/5600 loss 6.561404 loss_att 9.494376 loss_ctc 10.723956 loss_rnnt 5.348921 hw_loss 0.132904 lr 0.00041181 rank 5
2023-02-21 17:36:08,355 DEBUG TRAIN Batch 17/5700 loss 10.001195 loss_att 9.802528 loss_ctc 13.756415 loss_rnnt 9.430747 hw_loss 0.205285 lr 0.00041172 rank 4
2023-02-21 17:36:08,360 DEBUG TRAIN Batch 17/5700 loss 9.575386 loss_att 10.108509 loss_ctc 12.149879 loss_rnnt 8.909752 hw_loss 0.404521 lr 0.00041166 rank 7
2023-02-21 17:36:08,360 DEBUG TRAIN Batch 17/5700 loss 6.036055 loss_att 6.495335 loss_ctc 7.773665 loss_rnnt 5.552722 hw_loss 0.299616 lr 0.00041176 rank 1
2023-02-21 17:36:08,363 DEBUG TRAIN Batch 17/5700 loss 11.219398 loss_att 13.953651 loss_ctc 16.202194 loss_rnnt 9.824877 hw_loss 0.343679 lr 0.00041170 rank 2
2023-02-21 17:36:08,364 DEBUG TRAIN Batch 17/5700 loss 14.387162 loss_att 15.272082 loss_ctc 18.003923 loss_rnnt 13.646640 hw_loss 0.152445 lr 0.00041173 rank 0
2023-02-21 17:36:08,366 DEBUG TRAIN Batch 17/5700 loss 9.625315 loss_att 13.376314 loss_ctc 13.389942 loss_rnnt 8.234550 hw_loss 0.259900 lr 0.00041170 rank 6
2023-02-21 17:36:08,366 DEBUG TRAIN Batch 17/5700 loss 14.025336 loss_att 14.766029 loss_ctc 21.157658 loss_rnnt 12.911300 hw_loss 0.027979 lr 0.00041167 rank 5
2023-02-21 17:36:08,372 DEBUG TRAIN Batch 17/5700 loss 6.852252 loss_att 11.489590 loss_ctc 7.834398 loss_rnnt 5.680360 hw_loss 0.212759 lr 0.00041169 rank 3
2023-02-21 17:37:26,879 DEBUG TRAIN Batch 17/5800 loss 13.567205 loss_att 15.964124 loss_ctc 19.370695 loss_rnnt 12.195614 hw_loss 0.222016 lr 0.00041152 rank 7
2023-02-21 17:37:26,880 DEBUG TRAIN Batch 17/5800 loss 10.151767 loss_att 11.963545 loss_ctc 15.676075 loss_rnnt 9.013647 hw_loss 0.073480 lr 0.00041158 rank 4
2023-02-21 17:37:26,882 DEBUG TRAIN Batch 17/5800 loss 10.898446 loss_att 17.850424 loss_ctc 19.429596 loss_rnnt 8.303720 hw_loss 0.125331 lr 0.00041156 rank 6
2023-02-21 17:37:26,886 DEBUG TRAIN Batch 17/5800 loss 2.513421 loss_att 5.736761 loss_ctc 3.924617 loss_rnnt 1.627690 hw_loss 0.099193 lr 0.00041156 rank 2
2023-02-21 17:37:26,887 DEBUG TRAIN Batch 17/5800 loss 4.825675 loss_att 8.769173 loss_ctc 6.995894 loss_rnnt 3.733128 hw_loss 0.027159 lr 0.00041162 rank 1
2023-02-21 17:37:26,890 DEBUG TRAIN Batch 17/5800 loss 9.354805 loss_att 13.583738 loss_ctc 13.919117 loss_rnnt 7.885808 hw_loss 0.027441 lr 0.00041159 rank 0
2023-02-21 17:37:26,895 DEBUG TRAIN Batch 17/5800 loss 9.238141 loss_att 9.064404 loss_ctc 11.911906 loss_rnnt 8.859224 hw_loss 0.107179 lr 0.00041153 rank 5
2023-02-21 17:37:26,895 DEBUG TRAIN Batch 17/5800 loss 9.610532 loss_att 14.736759 loss_ctc 11.661465 loss_rnnt 8.239309 hw_loss 0.135973 lr 0.00041155 rank 3
2023-02-21 17:38:45,633 DEBUG TRAIN Batch 17/5900 loss 10.057631 loss_att 11.864771 loss_ctc 15.632370 loss_rnnt 8.883261 hw_loss 0.130585 lr 0.00041142 rank 2
2023-02-21 17:38:45,635 DEBUG TRAIN Batch 17/5900 loss 7.922959 loss_att 10.792428 loss_ctc 10.970794 loss_rnnt 6.894245 hw_loss 0.090831 lr 0.00041139 rank 5
2023-02-21 17:38:45,635 DEBUG TRAIN Batch 17/5900 loss 4.783508 loss_att 6.774789 loss_ctc 7.395522 loss_rnnt 3.959526 hw_loss 0.145231 lr 0.00041148 rank 1
2023-02-21 17:38:45,637 DEBUG TRAIN Batch 17/5900 loss 14.097754 loss_att 19.484451 loss_ctc 20.650423 loss_rnnt 12.132571 hw_loss 0.026541 lr 0.00041145 rank 4
2023-02-21 17:38:45,638 DEBUG TRAIN Batch 17/5900 loss 7.131907 loss_att 8.797987 loss_ctc 12.048059 loss_rnnt 6.080362 hw_loss 0.117831 lr 0.00041138 rank 7
2023-02-21 17:38:45,640 DEBUG TRAIN Batch 17/5900 loss 13.685643 loss_att 15.405818 loss_ctc 12.878803 loss_rnnt 13.435802 hw_loss 0.025098 lr 0.00041141 rank 3
2023-02-21 17:38:45,641 DEBUG TRAIN Batch 17/5900 loss 16.710522 loss_att 19.956192 loss_ctc 17.853918 loss_rnnt 15.864481 hw_loss 0.083351 lr 0.00041146 rank 0
2023-02-21 17:38:45,641 DEBUG TRAIN Batch 17/5900 loss 9.407619 loss_att 10.528463 loss_ctc 10.730374 loss_rnnt 8.959729 hw_loss 0.088784 lr 0.00041142 rank 6
2023-02-21 17:40:04,359 DEBUG TRAIN Batch 17/6000 loss 15.450281 loss_att 18.231878 loss_ctc 22.461926 loss_rnnt 13.852590 hw_loss 0.199660 lr 0.00041131 rank 4
2023-02-21 17:40:04,360 DEBUG TRAIN Batch 17/6000 loss 8.907569 loss_att 9.900532 loss_ctc 8.715819 loss_rnnt 8.692058 hw_loss 0.079659 lr 0.00041128 rank 6
2023-02-21 17:40:04,362 DEBUG TRAIN Batch 17/6000 loss 14.290089 loss_att 18.009418 loss_ctc 19.183365 loss_rnnt 12.793415 hw_loss 0.188193 lr 0.00041134 rank 1
2023-02-21 17:40:04,363 DEBUG TRAIN Batch 17/6000 loss 16.775328 loss_att 18.938393 loss_ctc 23.647078 loss_rnnt 15.327744 hw_loss 0.185132 lr 0.00041124 rank 7
2023-02-21 17:40:04,364 DEBUG TRAIN Batch 17/6000 loss 30.586046 loss_att 34.066120 loss_ctc 42.521523 loss_rnnt 28.210487 hw_loss 0.165273 lr 0.00041127 rank 3
2023-02-21 17:40:04,366 DEBUG TRAIN Batch 17/6000 loss 9.527737 loss_att 14.681664 loss_ctc 13.289543 loss_rnnt 7.932168 hw_loss 0.118517 lr 0.00041128 rank 2
2023-02-21 17:40:04,367 DEBUG TRAIN Batch 17/6000 loss 2.219110 loss_att 4.613256 loss_ctc 3.012888 loss_rnnt 1.558621 hw_loss 0.142169 lr 0.00041125 rank 5
2023-02-21 17:40:04,370 DEBUG TRAIN Batch 17/6000 loss 10.449712 loss_att 13.161072 loss_ctc 18.394306 loss_rnnt 8.798839 hw_loss 0.092477 lr 0.00041132 rank 0
2023-02-21 17:41:23,700 DEBUG TRAIN Batch 17/6100 loss 8.764369 loss_att 11.529246 loss_ctc 12.097569 loss_rnnt 7.721608 hw_loss 0.085048 lr 0.00041117 rank 4
2023-02-21 17:41:23,702 DEBUG TRAIN Batch 17/6100 loss 11.689317 loss_att 14.782868 loss_ctc 16.956078 loss_rnnt 10.312008 hw_loss 0.105681 lr 0.00041114 rank 2
2023-02-21 17:41:23,704 DEBUG TRAIN Batch 17/6100 loss 10.979942 loss_att 15.127033 loss_ctc 19.914955 loss_rnnt 8.870708 hw_loss 0.165901 lr 0.00041120 rank 1
2023-02-21 17:41:23,705 DEBUG TRAIN Batch 17/6100 loss 25.852436 loss_att 28.693283 loss_ctc 33.837357 loss_rnnt 24.153618 hw_loss 0.123737 lr 0.00041114 rank 6
2023-02-21 17:41:23,707 DEBUG TRAIN Batch 17/6100 loss 12.049815 loss_att 16.193663 loss_ctc 17.243689 loss_rnnt 10.354279 hw_loss 0.326719 lr 0.00041110 rank 7
2023-02-21 17:41:23,708 DEBUG TRAIN Batch 17/6100 loss 8.056092 loss_att 12.268007 loss_ctc 11.743271 loss_rnnt 6.682405 hw_loss 0.074401 lr 0.00041118 rank 0
2023-02-21 17:41:23,711 DEBUG TRAIN Batch 17/6100 loss 10.331882 loss_att 13.632845 loss_ctc 13.235541 loss_rnnt 9.242914 hw_loss 0.078037 lr 0.00041111 rank 5
2023-02-21 17:41:23,712 DEBUG TRAIN Batch 17/6100 loss 14.132237 loss_att 17.133846 loss_ctc 21.779591 loss_rnnt 12.384406 hw_loss 0.239741 lr 0.00041113 rank 3
2023-02-21 17:42:41,981 DEBUG TRAIN Batch 17/6200 loss 11.777477 loss_att 15.905735 loss_ctc 15.100608 loss_rnnt 10.404737 hw_loss 0.195007 lr 0.00041100 rank 2
2023-02-21 17:42:41,983 DEBUG TRAIN Batch 17/6200 loss 9.299941 loss_att 11.667143 loss_ctc 12.207413 loss_rnnt 8.314936 hw_loss 0.232317 lr 0.00041104 rank 0
2023-02-21 17:42:41,984 DEBUG TRAIN Batch 17/6200 loss 7.359947 loss_att 9.382242 loss_ctc 11.022184 loss_rnnt 6.440072 hw_loss 0.050844 lr 0.00041103 rank 4
2023-02-21 17:42:41,985 DEBUG TRAIN Batch 17/6200 loss 7.979226 loss_att 10.892840 loss_ctc 11.046906 loss_rnnt 6.882350 hw_loss 0.197118 lr 0.00041101 rank 6
2023-02-21 17:42:41,985 DEBUG TRAIN Batch 17/6200 loss 6.805846 loss_att 9.372726 loss_ctc 6.892476 loss_rnnt 6.215364 hw_loss 0.122915 lr 0.00041096 rank 7
2023-02-21 17:42:41,985 DEBUG TRAIN Batch 17/6200 loss 7.398540 loss_att 10.024891 loss_ctc 10.034610 loss_rnnt 6.377923 hw_loss 0.269758 lr 0.00041106 rank 1
2023-02-21 17:42:41,988 DEBUG TRAIN Batch 17/6200 loss 10.199394 loss_att 12.437468 loss_ctc 13.608642 loss_rnnt 9.228108 hw_loss 0.129570 lr 0.00041100 rank 3
2023-02-21 17:42:42,033 DEBUG TRAIN Batch 17/6200 loss 5.993638 loss_att 9.318602 loss_ctc 10.018443 loss_rnnt 4.707483 hw_loss 0.158475 lr 0.00041097 rank 5
2023-02-21 17:44:01,537 DEBUG TRAIN Batch 17/6300 loss 18.041441 loss_att 22.923759 loss_ctc 25.974943 loss_rnnt 15.958208 hw_loss 0.091816 lr 0.00041087 rank 6
2023-02-21 17:44:01,538 DEBUG TRAIN Batch 17/6300 loss 12.794754 loss_att 16.672346 loss_ctc 20.339466 loss_rnnt 10.998192 hw_loss 0.028277 lr 0.00041087 rank 2
2023-02-21 17:44:01,542 DEBUG TRAIN Batch 17/6300 loss 11.755117 loss_att 12.829489 loss_ctc 15.034391 loss_rnnt 11.066530 hw_loss 0.068393 lr 0.00041090 rank 0
2023-02-21 17:44:01,544 DEBUG TRAIN Batch 17/6300 loss 10.086126 loss_att 10.243276 loss_ctc 11.572658 loss_rnnt 9.705503 hw_loss 0.283105 lr 0.00041092 rank 1
2023-02-21 17:44:01,544 DEBUG TRAIN Batch 17/6300 loss 8.555221 loss_att 10.591688 loss_ctc 12.218583 loss_rnnt 7.624455 hw_loss 0.065667 lr 0.00041082 rank 7
2023-02-21 17:44:01,545 DEBUG TRAIN Batch 17/6300 loss 12.279024 loss_att 12.321746 loss_ctc 16.821857 loss_rnnt 11.495282 hw_loss 0.317785 lr 0.00041089 rank 4
2023-02-21 17:44:01,552 DEBUG TRAIN Batch 17/6300 loss 5.334411 loss_att 7.184601 loss_ctc 7.840794 loss_rnnt 4.543111 hw_loss 0.163270 lr 0.00041083 rank 5
2023-02-21 17:44:01,563 DEBUG TRAIN Batch 17/6300 loss 7.567852 loss_att 12.286257 loss_ctc 11.841826 loss_rnnt 5.992126 hw_loss 0.116594 lr 0.00041086 rank 3
2023-02-21 17:45:23,165 DEBUG TRAIN Batch 17/6400 loss 7.077081 loss_att 10.479597 loss_ctc 11.040164 loss_rnnt 5.776390 hw_loss 0.172081 lr 0.00041072 rank 3
2023-02-21 17:45:23,166 DEBUG TRAIN Batch 17/6400 loss 24.840141 loss_att 28.710583 loss_ctc 37.398453 loss_rnnt 22.348349 hw_loss 0.081122 lr 0.00041073 rank 6
2023-02-21 17:45:23,167 DEBUG TRAIN Batch 17/6400 loss 10.836242 loss_att 16.488117 loss_ctc 17.803524 loss_rnnt 8.705160 hw_loss 0.134503 lr 0.00041075 rank 4
2023-02-21 17:45:23,167 DEBUG TRAIN Batch 17/6400 loss 8.779333 loss_att 11.384211 loss_ctc 7.688448 loss_rnnt 8.390364 hw_loss 0.025209 lr 0.00041073 rank 2
2023-02-21 17:45:23,168 DEBUG TRAIN Batch 17/6400 loss 17.273149 loss_att 18.712425 loss_ctc 23.271887 loss_rnnt 16.063002 hw_loss 0.229617 lr 0.00041069 rank 5
2023-02-21 17:45:23,170 DEBUG TRAIN Batch 17/6400 loss 5.882264 loss_att 7.027400 loss_ctc 8.524677 loss_rnnt 5.225995 hw_loss 0.140474 lr 0.00041068 rank 7
2023-02-21 17:45:23,171 DEBUG TRAIN Batch 17/6400 loss 7.424640 loss_att 13.764448 loss_ctc 14.615421 loss_rnnt 5.066408 hw_loss 0.246563 lr 0.00041078 rank 1
2023-02-21 17:45:23,173 DEBUG TRAIN Batch 17/6400 loss 7.903804 loss_att 11.685858 loss_ctc 7.963225 loss_rnnt 7.077724 hw_loss 0.115775 lr 0.00041076 rank 0
2023-02-21 17:46:42,152 DEBUG TRAIN Batch 17/6500 loss 11.463395 loss_att 14.072784 loss_ctc 15.668251 loss_rnnt 10.293279 hw_loss 0.164231 lr 0.00041055 rank 7
2023-02-21 17:46:42,158 DEBUG TRAIN Batch 17/6500 loss 6.042882 loss_att 9.090762 loss_ctc 6.901210 loss_rnnt 5.251163 hw_loss 0.126936 lr 0.00041059 rank 2
2023-02-21 17:46:42,159 DEBUG TRAIN Batch 17/6500 loss 13.525648 loss_att 16.213223 loss_ctc 15.721552 loss_rnnt 12.606743 hw_loss 0.166131 lr 0.00041061 rank 4
2023-02-21 17:46:42,159 DEBUG TRAIN Batch 17/6500 loss 7.051716 loss_att 9.748545 loss_ctc 9.690774 loss_rnnt 6.101053 hw_loss 0.111418 lr 0.00041059 rank 6
2023-02-21 17:46:42,160 DEBUG TRAIN Batch 17/6500 loss 6.761139 loss_att 12.376750 loss_ctc 9.773190 loss_rnnt 5.172802 hw_loss 0.119264 lr 0.00041065 rank 1
2023-02-21 17:46:42,191 DEBUG TRAIN Batch 17/6500 loss 10.658491 loss_att 13.009837 loss_ctc 13.147001 loss_rnnt 9.805086 hw_loss 0.096255 lr 0.00041062 rank 0
2023-02-21 17:46:42,204 DEBUG TRAIN Batch 17/6500 loss 10.652456 loss_att 12.921946 loss_ctc 14.279094 loss_rnnt 9.630883 hw_loss 0.157734 lr 0.00041058 rank 3
2023-02-21 17:46:42,208 DEBUG TRAIN Batch 17/6500 loss 6.740921 loss_att 11.931199 loss_ctc 11.915792 loss_rnnt 4.975751 hw_loss 0.069621 lr 0.00041056 rank 5
2023-02-21 17:48:00,944 DEBUG TRAIN Batch 17/6600 loss 13.620573 loss_att 15.261693 loss_ctc 17.089760 loss_rnnt 12.717634 hw_loss 0.210294 lr 0.00041047 rank 4
2023-02-21 17:48:00,944 DEBUG TRAIN Batch 17/6600 loss 10.204287 loss_att 11.713868 loss_ctc 11.449003 loss_rnnt 9.653308 hw_loss 0.155813 lr 0.00041041 rank 7
2023-02-21 17:48:00,944 DEBUG TRAIN Batch 17/6600 loss 11.725499 loss_att 17.451920 loss_ctc 17.242817 loss_rnnt 9.712883 hw_loss 0.246918 lr 0.00041044 rank 3
2023-02-21 17:48:00,948 DEBUG TRAIN Batch 17/6600 loss 8.299416 loss_att 10.959793 loss_ctc 13.381240 loss_rnnt 7.066993 hw_loss 0.042696 lr 0.00041045 rank 2
2023-02-21 17:48:00,952 DEBUG TRAIN Batch 17/6600 loss 11.075906 loss_att 12.659105 loss_ctc 18.962469 loss_rnnt 9.672632 hw_loss 0.065799 lr 0.00041042 rank 5
2023-02-21 17:48:00,953 DEBUG TRAIN Batch 17/6600 loss 9.526845 loss_att 11.932861 loss_ctc 12.560826 loss_rnnt 8.589454 hw_loss 0.096856 lr 0.00041051 rank 1
2023-02-21 17:48:00,953 DEBUG TRAIN Batch 17/6600 loss 9.086439 loss_att 10.050037 loss_ctc 8.608800 loss_rnnt 8.904807 hw_loss 0.098623 lr 0.00041048 rank 0
2023-02-21 17:48:00,954 DEBUG TRAIN Batch 17/6600 loss 8.439722 loss_att 10.725801 loss_ctc 11.918251 loss_rnnt 7.490897 hw_loss 0.052136 lr 0.00041045 rank 6
2023-02-21 17:49:20,109 DEBUG TRAIN Batch 17/6700 loss 5.166327 loss_att 7.901958 loss_ctc 8.314151 loss_rnnt 4.167520 hw_loss 0.059945 lr 0.00041027 rank 7
2023-02-21 17:49:20,112 DEBUG TRAIN Batch 17/6700 loss 11.631109 loss_att 13.227659 loss_ctc 14.286341 loss_rnnt 10.931532 hw_loss 0.049195 lr 0.00041030 rank 3
2023-02-21 17:49:20,113 DEBUG TRAIN Batch 17/6700 loss 12.985848 loss_att 15.553741 loss_ctc 16.906902 loss_rnnt 11.882402 hw_loss 0.125736 lr 0.00041031 rank 2
2023-02-21 17:49:20,113 DEBUG TRAIN Batch 17/6700 loss 6.550136 loss_att 10.383661 loss_ctc 7.481685 loss_rnnt 5.593851 hw_loss 0.122574 lr 0.00041034 rank 4
2023-02-21 17:49:20,114 DEBUG TRAIN Batch 17/6700 loss 11.322442 loss_att 15.352496 loss_ctc 16.670025 loss_rnnt 9.679656 hw_loss 0.232057 lr 0.00041031 rank 6
2023-02-21 17:49:20,115 DEBUG TRAIN Batch 17/6700 loss 16.416302 loss_att 18.574694 loss_ctc 22.424236 loss_rnnt 15.149074 hw_loss 0.064673 lr 0.00041037 rank 1
2023-02-21 17:49:20,118 DEBUG TRAIN Batch 17/6700 loss 18.381430 loss_att 20.582403 loss_ctc 22.483952 loss_rnnt 17.338327 hw_loss 0.104817 lr 0.00041028 rank 5
2023-02-21 17:49:20,160 DEBUG TRAIN Batch 17/6700 loss 8.448551 loss_att 10.455324 loss_ctc 10.461454 loss_rnnt 7.721678 hw_loss 0.107122 lr 0.00041035 rank 0
2023-02-21 17:50:41,091 DEBUG TRAIN Batch 17/6800 loss 12.356535 loss_att 14.930874 loss_ctc 18.540752 loss_rnnt 10.956379 hw_loss 0.113859 lr 0.00041013 rank 7
2023-02-21 17:50:41,092 DEBUG TRAIN Batch 17/6800 loss 8.251260 loss_att 10.797737 loss_ctc 13.125309 loss_rnnt 6.990335 hw_loss 0.190792 lr 0.00041017 rank 2
2023-02-21 17:50:41,096 DEBUG TRAIN Batch 17/6800 loss 14.388399 loss_att 15.797242 loss_ctc 18.857946 loss_rnnt 13.467878 hw_loss 0.080275 lr 0.00041020 rank 4
2023-02-21 17:50:41,100 DEBUG TRAIN Batch 17/6800 loss 4.641136 loss_att 7.734158 loss_ctc 9.344080 loss_rnnt 3.367078 hw_loss 0.053239 lr 0.00041021 rank 0
2023-02-21 17:50:41,100 DEBUG TRAIN Batch 17/6800 loss 8.035655 loss_att 8.820213 loss_ctc 10.573170 loss_rnnt 7.490654 hw_loss 0.093290 lr 0.00041018 rank 6
2023-02-21 17:50:41,102 DEBUG TRAIN Batch 17/6800 loss 10.716845 loss_att 14.373878 loss_ctc 18.459913 loss_rnnt 8.806639 hw_loss 0.274479 lr 0.00041023 rank 1
2023-02-21 17:50:41,102 DEBUG TRAIN Batch 17/6800 loss 11.835222 loss_att 12.419979 loss_ctc 14.424074 loss_rnnt 11.272534 hw_loss 0.188541 lr 0.00041017 rank 3
2023-02-21 17:50:41,105 DEBUG TRAIN Batch 17/6800 loss 5.277018 loss_att 10.141108 loss_ctc 9.397619 loss_rnnt 3.675786 hw_loss 0.148125 lr 0.00041014 rank 5
2023-02-21 17:52:00,759 DEBUG TRAIN Batch 17/6900 loss 9.414795 loss_att 8.626829 loss_ctc 11.336191 loss_rnnt 9.163639 hw_loss 0.286054 lr 0.00041004 rank 2
2023-02-21 17:52:00,759 DEBUG TRAIN Batch 17/6900 loss 8.192558 loss_att 10.778785 loss_ctc 11.818250 loss_rnnt 7.111309 hw_loss 0.151085 lr 0.00041006 rank 4
2023-02-21 17:52:00,760 DEBUG TRAIN Batch 17/6900 loss 9.831971 loss_att 9.285874 loss_ctc 11.293439 loss_rnnt 9.582239 hw_loss 0.307667 lr 0.00041003 rank 3
2023-02-21 17:52:00,765 DEBUG TRAIN Batch 17/6900 loss 7.564514 loss_att 9.162975 loss_ctc 11.604430 loss_rnnt 6.585193 hw_loss 0.226825 lr 0.00040999 rank 7
2023-02-21 17:52:00,765 DEBUG TRAIN Batch 17/6900 loss 13.468002 loss_att 14.196602 loss_ctc 19.193060 loss_rnnt 12.501305 hw_loss 0.108070 lr 0.00041009 rank 1
2023-02-21 17:52:00,768 DEBUG TRAIN Batch 17/6900 loss 9.171014 loss_att 11.192468 loss_ctc 12.804705 loss_rnnt 8.182857 hw_loss 0.186327 lr 0.00041007 rank 0
2023-02-21 17:52:00,771 DEBUG TRAIN Batch 17/6900 loss 9.681761 loss_att 13.478327 loss_ctc 15.222807 loss_rnnt 8.125088 hw_loss 0.109786 lr 0.00041000 rank 5
2023-02-21 17:52:00,808 DEBUG TRAIN Batch 17/6900 loss 8.295763 loss_att 9.947158 loss_ctc 13.363534 loss_rnnt 7.187112 hw_loss 0.192505 lr 0.00041004 rank 6
2023-02-21 17:53:18,526 DEBUG TRAIN Batch 17/7000 loss 16.738646 loss_att 18.881315 loss_ctc 18.932827 loss_rnnt 15.940505 hw_loss 0.144466 lr 0.00040992 rank 4
2023-02-21 17:53:18,528 DEBUG TRAIN Batch 17/7000 loss 5.339344 loss_att 7.598342 loss_ctc 7.923981 loss_rnnt 4.416069 hw_loss 0.237858 lr 0.00040990 rank 2
2023-02-21 17:53:18,532 DEBUG TRAIN Batch 17/7000 loss 8.210997 loss_att 11.239883 loss_ctc 10.388924 loss_rnnt 7.278064 hw_loss 0.068937 lr 0.00040995 rank 1
2023-02-21 17:53:18,534 DEBUG TRAIN Batch 17/7000 loss 13.017258 loss_att 12.744289 loss_ctc 15.161388 loss_rnnt 12.571438 hw_loss 0.402241 lr 0.00040993 rank 0
2023-02-21 17:53:18,536 DEBUG TRAIN Batch 17/7000 loss 9.321377 loss_att 12.963367 loss_ctc 13.069313 loss_rnnt 8.032136 hw_loss 0.114594 lr 0.00040990 rank 6
2023-02-21 17:53:18,541 DEBUG TRAIN Batch 17/7000 loss 12.041481 loss_att 15.947693 loss_ctc 19.970013 loss_rnnt 10.164813 hw_loss 0.071790 lr 0.00040986 rank 7
2023-02-21 17:53:18,541 DEBUG TRAIN Batch 17/7000 loss 10.882569 loss_att 14.905102 loss_ctc 18.405720 loss_rnnt 8.939486 hw_loss 0.254044 lr 0.00040986 rank 5
2023-02-21 17:53:18,553 DEBUG TRAIN Batch 17/7000 loss 11.392654 loss_att 12.420074 loss_ctc 13.165038 loss_rnnt 10.884230 hw_loss 0.124920 lr 0.00040989 rank 3
2023-02-21 17:54:38,634 DEBUG TRAIN Batch 17/7100 loss 10.784933 loss_att 14.674726 loss_ctc 14.244703 loss_rnnt 9.427455 hw_loss 0.221657 lr 0.00040978 rank 4
2023-02-21 17:54:38,638 DEBUG TRAIN Batch 17/7100 loss 6.157470 loss_att 8.905068 loss_ctc 12.205545 loss_rnnt 4.788673 hw_loss 0.024126 lr 0.00040982 rank 1
2023-02-21 17:54:38,638 DEBUG TRAIN Batch 17/7100 loss 6.043412 loss_att 8.137054 loss_ctc 6.733582 loss_rnnt 5.352998 hw_loss 0.336868 lr 0.00040976 rank 2
2023-02-21 17:54:38,638 DEBUG TRAIN Batch 17/7100 loss 10.618908 loss_att 15.049103 loss_ctc 15.913268 loss_rnnt 8.985863 hw_loss 0.077048 lr 0.00040976 rank 6
2023-02-21 17:54:38,640 DEBUG TRAIN Batch 17/7100 loss 14.858701 loss_att 17.174137 loss_ctc 20.196821 loss_rnnt 13.620255 hw_loss 0.119266 lr 0.00040973 rank 5
2023-02-21 17:54:38,642 DEBUG TRAIN Batch 17/7100 loss 12.601315 loss_att 16.543932 loss_ctc 17.075588 loss_rnnt 11.173729 hw_loss 0.079677 lr 0.00040975 rank 3
2023-02-21 17:54:38,644 DEBUG TRAIN Batch 17/7100 loss 13.127252 loss_att 18.141994 loss_ctc 21.985912 loss_rnnt 10.890857 hw_loss 0.098046 lr 0.00040979 rank 0
2023-02-21 17:54:38,644 DEBUG TRAIN Batch 17/7100 loss 17.921646 loss_att 22.115993 loss_ctc 25.058502 loss_rnnt 16.117819 hw_loss 0.025084 lr 0.00040972 rank 7
2023-02-21 17:55:57,427 DEBUG TRAIN Batch 17/7200 loss 15.058483 loss_att 14.547540 loss_ctc 21.175049 loss_rnnt 14.328894 hw_loss 0.030444 lr 0.00040965 rank 4
2023-02-21 17:55:57,428 DEBUG TRAIN Batch 17/7200 loss 9.741258 loss_att 13.206695 loss_ctc 13.747742 loss_rnnt 8.460896 hw_loss 0.099518 lr 0.00040962 rank 2
2023-02-21 17:55:57,430 DEBUG TRAIN Batch 17/7200 loss 12.524152 loss_att 14.924826 loss_ctc 19.787220 loss_rnnt 11.024750 hw_loss 0.095360 lr 0.00040958 rank 7
2023-02-21 17:55:57,432 DEBUG TRAIN Batch 17/7200 loss 19.320347 loss_att 20.947403 loss_ctc 23.657768 loss_rnnt 18.401310 hw_loss 0.028690 lr 0.00040961 rank 3
2023-02-21 17:55:57,432 DEBUG TRAIN Batch 17/7200 loss 7.377049 loss_att 11.137878 loss_ctc 13.689603 loss_rnnt 5.689211 hw_loss 0.176246 lr 0.00040968 rank 1
2023-02-21 17:55:57,435 DEBUG TRAIN Batch 17/7200 loss 27.681604 loss_att 29.932068 loss_ctc 41.216331 loss_rnnt 25.393661 hw_loss 0.062282 lr 0.00040966 rank 0
2023-02-21 17:55:57,438 DEBUG TRAIN Batch 17/7200 loss 6.545935 loss_att 8.020765 loss_ctc 8.716728 loss_rnnt 5.903528 hw_loss 0.108755 lr 0.00040959 rank 5
2023-02-21 17:55:57,475 DEBUG TRAIN Batch 17/7200 loss 15.890203 loss_att 16.889648 loss_ctc 21.728306 loss_rnnt 14.895862 hw_loss 0.030074 lr 0.00040962 rank 6
2023-02-21 17:57:15,713 DEBUG TRAIN Batch 17/7300 loss 12.518403 loss_att 16.057550 loss_ctc 18.200136 loss_rnnt 10.986090 hw_loss 0.125472 lr 0.00040951 rank 4
2023-02-21 17:57:15,720 DEBUG TRAIN Batch 17/7300 loss 8.909850 loss_att 13.906853 loss_ctc 17.430254 loss_rnnt 6.704417 hw_loss 0.131208 lr 0.00040949 rank 2
2023-02-21 17:57:15,721 DEBUG TRAIN Batch 17/7300 loss 3.257051 loss_att 7.189038 loss_ctc 4.763108 loss_rnnt 2.209237 hw_loss 0.113641 lr 0.00040952 rank 0
2023-02-21 17:57:15,724 DEBUG TRAIN Batch 17/7300 loss 7.370039 loss_att 9.173806 loss_ctc 9.308202 loss_rnnt 6.718992 hw_loss 0.059759 lr 0.00040954 rank 1
2023-02-21 17:57:15,725 DEBUG TRAIN Batch 17/7300 loss 11.490326 loss_att 14.423101 loss_ctc 17.217428 loss_rnnt 10.107264 hw_loss 0.061675 lr 0.00040944 rank 7
2023-02-21 17:57:15,726 DEBUG TRAIN Batch 17/7300 loss 10.817799 loss_att 14.115915 loss_ctc 15.109777 loss_rnnt 9.464346 hw_loss 0.227933 lr 0.00040949 rank 6
2023-02-21 17:57:15,726 DEBUG TRAIN Batch 17/7300 loss 10.014548 loss_att 12.667989 loss_ctc 12.857204 loss_rnnt 9.003323 hw_loss 0.190344 lr 0.00040945 rank 5
2023-02-21 17:57:15,728 DEBUG TRAIN Batch 17/7300 loss 12.526864 loss_att 15.529275 loss_ctc 18.196459 loss_rnnt 11.129972 hw_loss 0.075871 lr 0.00040948 rank 3
2023-02-21 17:58:34,790 DEBUG TRAIN Batch 17/7400 loss 6.212360 loss_att 8.703815 loss_ctc 13.291780 loss_rnnt 4.670843 hw_loss 0.186196 lr 0.00040931 rank 7
2023-02-21 17:58:34,793 DEBUG TRAIN Batch 17/7400 loss 8.591453 loss_att 12.043164 loss_ctc 12.571836 loss_rnnt 7.353768 hw_loss 0.031170 lr 0.00040934 rank 3
2023-02-21 17:58:34,794 DEBUG TRAIN Batch 17/7400 loss 18.646315 loss_att 18.223700 loss_ctc 24.001696 loss_rnnt 17.923351 hw_loss 0.175188 lr 0.00040940 rank 1
2023-02-21 17:58:34,794 DEBUG TRAIN Batch 17/7400 loss 7.320220 loss_att 10.815979 loss_ctc 10.091191 loss_rnnt 6.196561 hw_loss 0.103209 lr 0.00040935 rank 2
2023-02-21 17:58:34,796 DEBUG TRAIN Batch 17/7400 loss 6.583650 loss_att 10.427198 loss_ctc 9.271029 loss_rnnt 5.369757 hw_loss 0.162873 lr 0.00040938 rank 0
2023-02-21 17:58:34,796 DEBUG TRAIN Batch 17/7400 loss 16.825909 loss_att 18.787781 loss_ctc 22.213390 loss_rnnt 15.660398 hw_loss 0.102759 lr 0.00040937 rank 4
2023-02-21 17:58:34,796 DEBUG TRAIN Batch 17/7400 loss 12.980309 loss_att 14.676683 loss_ctc 19.763601 loss_rnnt 11.719193 hw_loss 0.032631 lr 0.00040935 rank 6
2023-02-21 17:58:34,846 DEBUG TRAIN Batch 17/7400 loss 6.640088 loss_att 7.707196 loss_ctc 8.127792 loss_rnnt 6.204594 hw_loss 0.044459 lr 0.00040932 rank 5
2023-02-21 17:59:55,247 DEBUG TRAIN Batch 17/7500 loss 8.316023 loss_att 10.332835 loss_ctc 14.471069 loss_rnnt 6.986806 hw_loss 0.197215 lr 0.00040921 rank 2
2023-02-21 17:59:55,248 DEBUG TRAIN Batch 17/7500 loss 5.395712 loss_att 8.021807 loss_ctc 4.737723 loss_rnnt 4.941182 hw_loss 0.031955 lr 0.00040917 rank 7
2023-02-21 17:59:55,250 DEBUG TRAIN Batch 17/7500 loss 13.962734 loss_att 14.351639 loss_ctc 18.157684 loss_rnnt 13.257872 hw_loss 0.127041 lr 0.00040927 rank 1
2023-02-21 17:59:55,251 DEBUG TRAIN Batch 17/7500 loss 7.939441 loss_att 12.379848 loss_ctc 11.852167 loss_rnnt 6.477895 hw_loss 0.097062 lr 0.00040923 rank 4
2023-02-21 17:59:55,252 DEBUG TRAIN Batch 17/7500 loss 8.235774 loss_att 9.728977 loss_ctc 12.169559 loss_rnnt 7.341398 hw_loss 0.133557 lr 0.00040921 rank 6
2023-02-21 17:59:55,257 DEBUG TRAIN Batch 17/7500 loss 7.851643 loss_att 8.989161 loss_ctc 7.317708 loss_rnnt 7.619047 hw_loss 0.143030 lr 0.00040918 rank 5
2023-02-21 17:59:55,257 DEBUG TRAIN Batch 17/7500 loss 13.009232 loss_att 14.600618 loss_ctc 18.204651 loss_rnnt 11.904887 hw_loss 0.175021 lr 0.00040920 rank 3
2023-02-21 17:59:55,261 DEBUG TRAIN Batch 17/7500 loss 8.875616 loss_att 15.977194 loss_ctc 14.205134 loss_rnnt 6.645840 hw_loss 0.185359 lr 0.00040924 rank 0
2023-02-21 18:01:13,353 DEBUG TRAIN Batch 17/7600 loss 6.685715 loss_att 7.452936 loss_ctc 10.501196 loss_rnnt 6.001852 hw_loss 0.040666 lr 0.00040911 rank 0
2023-02-21 18:01:13,355 DEBUG TRAIN Batch 17/7600 loss 9.848011 loss_att 13.330790 loss_ctc 14.610106 loss_rnnt 8.475889 hw_loss 0.076164 lr 0.00040903 rank 7
2023-02-21 18:01:13,357 DEBUG TRAIN Batch 17/7600 loss 8.458364 loss_att 15.286054 loss_ctc 7.975788 loss_rnnt 7.082803 hw_loss 0.139435 lr 0.00040907 rank 2
2023-02-21 18:01:13,358 DEBUG TRAIN Batch 17/7600 loss 13.423786 loss_att 17.772856 loss_ctc 18.401066 loss_rnnt 11.797656 hw_loss 0.173774 lr 0.00040908 rank 6
2023-02-21 18:01:13,359 DEBUG TRAIN Batch 17/7600 loss 13.766571 loss_att 13.462761 loss_ctc 16.298025 loss_rnnt 13.394612 hw_loss 0.178487 lr 0.00040910 rank 4
2023-02-21 18:01:13,361 DEBUG TRAIN Batch 17/7600 loss 4.820022 loss_att 9.417868 loss_ctc 6.814018 loss_rnnt 3.534817 hw_loss 0.187068 lr 0.00040907 rank 3
2023-02-21 18:01:13,362 DEBUG TRAIN Batch 17/7600 loss 6.526581 loss_att 9.797528 loss_ctc 9.029329 loss_rnnt 5.459756 hw_loss 0.148004 lr 0.00040904 rank 5
2023-02-21 18:01:13,369 DEBUG TRAIN Batch 17/7600 loss 16.215235 loss_att 21.289608 loss_ctc 27.055370 loss_rnnt 13.712531 hw_loss 0.079644 lr 0.00040913 rank 1
2023-02-21 18:02:32,816 DEBUG TRAIN Batch 17/7700 loss 6.290795 loss_att 9.243027 loss_ctc 8.389037 loss_rnnt 5.387874 hw_loss 0.061331 lr 0.00040899 rank 1
2023-02-21 18:02:32,816 DEBUG TRAIN Batch 17/7700 loss 10.113924 loss_att 13.105225 loss_ctc 15.417232 loss_rnnt 8.746992 hw_loss 0.115433 lr 0.00040896 rank 4
2023-02-21 18:02:32,817 DEBUG TRAIN Batch 17/7700 loss 14.087191 loss_att 17.518490 loss_ctc 17.797642 loss_rnnt 12.882057 hw_loss 0.045275 lr 0.00040894 rank 2
2023-02-21 18:02:32,819 DEBUG TRAIN Batch 17/7700 loss 18.242046 loss_att 23.332029 loss_ctc 24.613440 loss_rnnt 16.274588 hw_loss 0.187399 lr 0.00040889 rank 7
2023-02-21 18:02:32,819 DEBUG TRAIN Batch 17/7700 loss 10.911187 loss_att 14.307865 loss_ctc 12.914126 loss_rnnt 9.849170 hw_loss 0.216794 lr 0.00040890 rank 5
2023-02-21 18:02:32,820 DEBUG TRAIN Batch 17/7700 loss 16.963037 loss_att 16.827524 loss_ctc 25.153952 loss_rnnt 15.795173 hw_loss 0.192837 lr 0.00040897 rank 0
2023-02-21 18:02:32,827 DEBUG TRAIN Batch 17/7700 loss 2.379024 loss_att 5.796019 loss_ctc 3.845129 loss_rnnt 1.475045 hw_loss 0.047062 lr 0.00040893 rank 3
2023-02-21 18:02:32,870 DEBUG TRAIN Batch 17/7700 loss 9.853895 loss_att 11.539908 loss_ctc 12.998322 loss_rnnt 9.048434 hw_loss 0.091877 lr 0.00040894 rank 6
2023-02-21 18:03:53,209 DEBUG TRAIN Batch 17/7800 loss 4.691517 loss_att 8.545229 loss_ctc 5.942286 loss_rnnt 3.693875 hw_loss 0.112745 lr 0.00040876 rank 7
2023-02-21 18:03:53,211 DEBUG TRAIN Batch 17/7800 loss 8.759915 loss_att 12.733117 loss_ctc 11.285946 loss_rnnt 7.606278 hw_loss 0.041612 lr 0.00040886 rank 1
2023-02-21 18:03:53,211 DEBUG TRAIN Batch 17/7800 loss 10.237668 loss_att 15.540794 loss_ctc 15.119772 loss_rnnt 8.361694 hw_loss 0.308250 lr 0.00040880 rank 6
2023-02-21 18:03:53,211 DEBUG TRAIN Batch 17/7800 loss 9.438519 loss_att 13.185772 loss_ctc 11.222412 loss_rnnt 8.394930 hw_loss 0.105533 lr 0.00040882 rank 4
2023-02-21 18:03:53,214 DEBUG TRAIN Batch 17/7800 loss 12.652812 loss_att 15.554783 loss_ctc 14.438255 loss_rnnt 11.810988 hw_loss 0.043820 lr 0.00040880 rank 2
2023-02-21 18:03:53,215 DEBUG TRAIN Batch 17/7800 loss 8.068881 loss_att 12.561407 loss_ctc 13.661772 loss_rnnt 6.383651 hw_loss 0.076886 lr 0.00040883 rank 0
2023-02-21 18:03:53,216 DEBUG TRAIN Batch 17/7800 loss 6.275513 loss_att 10.098607 loss_ctc 10.107834 loss_rnnt 4.952440 hw_loss 0.089022 lr 0.00040879 rank 3
2023-02-21 18:03:53,222 DEBUG TRAIN Batch 17/7800 loss 15.878947 loss_att 17.278126 loss_ctc 25.370705 loss_rnnt 14.265845 hw_loss 0.126937 lr 0.00040877 rank 5
2023-02-21 18:05:11,992 DEBUG TRAIN Batch 17/7900 loss 18.674931 loss_att 18.464422 loss_ctc 25.862988 loss_rnnt 17.716429 hw_loss 0.079116 lr 0.00040862 rank 7
2023-02-21 18:05:11,995 DEBUG TRAIN Batch 17/7900 loss 11.517327 loss_att 13.608277 loss_ctc 15.790751 loss_rnnt 10.488484 hw_loss 0.076617 lr 0.00040870 rank 0
2023-02-21 18:05:11,996 DEBUG TRAIN Batch 17/7900 loss 14.911149 loss_att 20.682461 loss_ctc 20.390276 loss_rnnt 12.989115 hw_loss 0.069787 lr 0.00040869 rank 4
2023-02-21 18:05:11,996 DEBUG TRAIN Batch 17/7900 loss 12.838503 loss_att 14.478075 loss_ctc 17.684879 loss_rnnt 11.781641 hw_loss 0.155182 lr 0.00040872 rank 1
2023-02-21 18:05:11,997 DEBUG TRAIN Batch 17/7900 loss 9.073279 loss_att 10.542515 loss_ctc 9.791013 loss_rnnt 8.630997 hw_loss 0.098884 lr 0.00040866 rank 2
2023-02-21 18:05:12,001 DEBUG TRAIN Batch 17/7900 loss 11.858543 loss_att 14.877562 loss_ctc 14.610199 loss_rnnt 10.783028 hw_loss 0.196546 lr 0.00040866 rank 3
2023-02-21 18:05:12,008 DEBUG TRAIN Batch 17/7900 loss 17.491430 loss_att 21.320946 loss_ctc 26.109039 loss_rnnt 15.532535 hw_loss 0.082459 lr 0.00040863 rank 5
2023-02-21 18:05:12,043 DEBUG TRAIN Batch 17/7900 loss 12.907026 loss_att 13.188005 loss_ctc 14.748701 loss_rnnt 12.470322 hw_loss 0.253035 lr 0.00040867 rank 6
2023-02-21 18:06:32,259 DEBUG TRAIN Batch 17/8000 loss 12.462369 loss_att 17.544535 loss_ctc 16.425323 loss_rnnt 10.852674 hw_loss 0.121627 lr 0.00040849 rank 7
2023-02-21 18:06:32,263 DEBUG TRAIN Batch 17/8000 loss 4.734935 loss_att 6.606680 loss_ctc 6.760079 loss_rnnt 4.017353 hw_loss 0.137277 lr 0.00040855 rank 4
2023-02-21 18:06:32,265 DEBUG TRAIN Batch 17/8000 loss 8.659869 loss_att 12.778830 loss_ctc 14.638067 loss_rnnt 6.998582 hw_loss 0.075754 lr 0.00040853 rank 6
2023-02-21 18:06:32,265 DEBUG TRAIN Batch 17/8000 loss 14.613835 loss_att 16.157650 loss_ctc 26.182741 loss_rnnt 12.702615 hw_loss 0.112381 lr 0.00040853 rank 2
2023-02-21 18:06:32,269 DEBUG TRAIN Batch 17/8000 loss 9.800899 loss_att 16.273943 loss_ctc 14.528297 loss_rnnt 7.787189 hw_loss 0.166466 lr 0.00040858 rank 1
2023-02-21 18:06:32,271 DEBUG TRAIN Batch 17/8000 loss 9.800780 loss_att 11.398286 loss_ctc 15.490270 loss_rnnt 8.639164 hw_loss 0.156592 lr 0.00040856 rank 0
2023-02-21 18:06:32,271 DEBUG TRAIN Batch 17/8000 loss 12.028373 loss_att 15.784534 loss_ctc 16.307568 loss_rnnt 10.650780 hw_loss 0.104627 lr 0.00040849 rank 5
2023-02-21 18:06:32,320 DEBUG TRAIN Batch 17/8000 loss 8.974562 loss_att 11.766572 loss_ctc 11.605059 loss_rnnt 7.901306 hw_loss 0.307727 lr 0.00040852 rank 3
2023-02-21 18:07:52,019 DEBUG TRAIN Batch 17/8100 loss 13.926825 loss_att 16.654861 loss_ctc 20.782173 loss_rnnt 12.364569 hw_loss 0.192380 lr 0.00040835 rank 7
2023-02-21 18:07:52,026 DEBUG TRAIN Batch 17/8100 loss 11.213267 loss_att 15.012938 loss_ctc 16.079401 loss_rnnt 9.767774 hw_loss 0.068890 lr 0.00040845 rank 1
2023-02-21 18:07:52,029 DEBUG TRAIN Batch 17/8100 loss 10.109693 loss_att 14.716910 loss_ctc 16.779617 loss_rnnt 8.219922 hw_loss 0.148131 lr 0.00040836 rank 5
2023-02-21 18:07:52,030 DEBUG TRAIN Batch 17/8100 loss 8.488143 loss_att 9.246382 loss_ctc 9.406476 loss_rnnt 8.157277 hw_loss 0.106450 lr 0.00040838 rank 3
2023-02-21 18:07:52,031 DEBUG TRAIN Batch 17/8100 loss 3.486924 loss_att 5.630637 loss_ctc 4.119876 loss_rnnt 2.953322 hw_loss 0.038374 lr 0.00040841 rank 4
2023-02-21 18:07:52,031 DEBUG TRAIN Batch 17/8100 loss 10.927227 loss_att 13.860706 loss_ctc 15.703845 loss_rnnt 9.517181 hw_loss 0.349625 lr 0.00040839 rank 6
2023-02-21 18:07:52,037 DEBUG TRAIN Batch 17/8100 loss 8.739052 loss_att 11.723501 loss_ctc 13.243013 loss_rnnt 7.507700 hw_loss 0.063624 lr 0.00040842 rank 0
2023-02-21 18:07:52,039 DEBUG TRAIN Batch 17/8100 loss 17.616335 loss_att 20.458019 loss_ctc 22.677479 loss_rnnt 16.287458 hw_loss 0.160726 lr 0.00040839 rank 2
2023-02-21 18:09:10,875 DEBUG TRAIN Batch 17/8200 loss 14.395244 loss_att 16.418484 loss_ctc 18.378376 loss_rnnt 13.397917 hw_loss 0.115493 lr 0.00040828 rank 4
2023-02-21 18:09:10,875 DEBUG TRAIN Batch 17/8200 loss 16.175135 loss_att 16.656273 loss_ctc 21.445890 loss_rnnt 15.159940 hw_loss 0.405372 lr 0.00040821 rank 7
2023-02-21 18:09:10,878 DEBUG TRAIN Batch 17/8200 loss 8.525745 loss_att 8.837009 loss_ctc 10.652930 loss_rnnt 8.082766 hw_loss 0.182067 lr 0.00040831 rank 1
2023-02-21 18:09:10,880 DEBUG TRAIN Batch 17/8200 loss 7.364136 loss_att 10.120075 loss_ctc 9.247469 loss_rnnt 6.470037 hw_loss 0.172125 lr 0.00040829 rank 0
2023-02-21 18:09:10,879 DEBUG TRAIN Batch 17/8200 loss 9.875740 loss_att 9.536318 loss_ctc 13.408076 loss_rnnt 9.368034 hw_loss 0.196147 lr 0.00040826 rank 2
2023-02-21 18:09:10,880 DEBUG TRAIN Batch 17/8200 loss 12.815430 loss_att 12.098603 loss_ctc 16.300606 loss_rnnt 12.384041 hw_loss 0.206370 lr 0.00040826 rank 6
2023-02-21 18:09:10,883 DEBUG TRAIN Batch 17/8200 loss 10.501929 loss_att 10.626894 loss_ctc 11.540096 loss_rnnt 10.186008 hw_loss 0.285948 lr 0.00040825 rank 3
2023-02-21 18:09:10,886 DEBUG TRAIN Batch 17/8200 loss 8.107733 loss_att 9.302725 loss_ctc 9.858946 loss_rnnt 7.618929 hw_loss 0.030583 lr 0.00040822 rank 5
2023-02-21 18:10:28,016 DEBUG TRAIN Batch 17/8300 loss 6.767550 loss_att 10.397753 loss_ctc 9.221655 loss_rnnt 5.568820 hw_loss 0.272768 lr 0.00040817 rank 1
2023-02-21 18:10:28,018 DEBUG TRAIN Batch 17/8300 loss 12.376261 loss_att 16.317883 loss_ctc 16.807384 loss_rnnt 10.935345 hw_loss 0.115829 lr 0.00040812 rank 2
2023-02-21 18:10:28,018 DEBUG TRAIN Batch 17/8300 loss 7.314944 loss_att 12.371738 loss_ctc 17.232019 loss_rnnt 4.874074 hw_loss 0.201065 lr 0.00040808 rank 7
2023-02-21 18:10:28,019 DEBUG TRAIN Batch 17/8300 loss 10.006484 loss_att 12.226391 loss_ctc 8.958513 loss_rnnt 9.662990 hw_loss 0.073581 lr 0.00040814 rank 4
2023-02-21 18:10:28,021 DEBUG TRAIN Batch 17/8300 loss 13.216895 loss_att 16.422199 loss_ctc 17.809692 loss_rnnt 11.867763 hw_loss 0.179435 lr 0.00040815 rank 0
2023-02-21 18:10:28,022 DEBUG TRAIN Batch 17/8300 loss 11.013373 loss_att 14.976938 loss_ctc 16.633055 loss_rnnt 9.345650 hw_loss 0.235724 lr 0.00040812 rank 6
2023-02-21 18:10:28,023 DEBUG TRAIN Batch 17/8300 loss 12.840495 loss_att 14.575895 loss_ctc 15.626854 loss_rnnt 12.009762 hw_loss 0.210260 lr 0.00040809 rank 5
2023-02-21 18:10:28,030 DEBUG TRAIN Batch 17/8300 loss 12.119723 loss_att 13.021731 loss_ctc 12.624327 loss_rnnt 11.814308 hw_loss 0.108248 lr 0.00040811 rank 3
2023-02-21 18:11:21,901 DEBUG CV Batch 17/0 loss 2.212567 loss_att 2.203401 loss_ctc 2.817778 loss_rnnt 1.886128 hw_loss 0.464208 history loss 2.130620 rank 6
2023-02-21 18:11:21,907 DEBUG CV Batch 17/0 loss 2.212567 loss_att 2.203401 loss_ctc 2.817778 loss_rnnt 1.886128 hw_loss 0.464209 history loss 2.130620 rank 3
2023-02-21 18:11:21,910 DEBUG CV Batch 17/0 loss 2.212567 loss_att 2.203401 loss_ctc 2.817778 loss_rnnt 1.886128 hw_loss 0.464208 history loss 2.130620 rank 7
2023-02-21 18:11:21,911 DEBUG CV Batch 17/0 loss 2.212567 loss_att 2.203401 loss_ctc 2.817778 loss_rnnt 1.886128 hw_loss 0.464209 history loss 2.130620 rank 5
2023-02-21 18:11:21,914 DEBUG CV Batch 17/0 loss 2.212567 loss_att 2.203401 loss_ctc 2.817778 loss_rnnt 1.886128 hw_loss 0.464208 history loss 2.130620 rank 2
2023-02-21 18:11:21,917 DEBUG CV Batch 17/0 loss 2.212567 loss_att 2.203401 loss_ctc 2.817778 loss_rnnt 1.886128 hw_loss 0.464209 history loss 2.130620 rank 1
2023-02-21 18:11:21,921 DEBUG CV Batch 17/0 loss 2.212567 loss_att 2.203401 loss_ctc 2.817778 loss_rnnt 1.886128 hw_loss 0.464208 history loss 2.130620 rank 4
2023-02-21 18:11:21,937 DEBUG CV Batch 17/0 loss 2.212567 loss_att 2.203401 loss_ctc 2.817778 loss_rnnt 1.886128 hw_loss 0.464209 history loss 2.130620 rank 0
2023-02-21 18:11:32,988 DEBUG CV Batch 17/100 loss 6.576198 loss_att 8.238616 loss_ctc 9.922421 loss_rnnt 5.721073 hw_loss 0.143397 history loss 3.697338 rank 2
2023-02-21 18:11:32,998 DEBUG CV Batch 17/100 loss 6.576198 loss_att 8.238616 loss_ctc 9.922421 loss_rnnt 5.721073 hw_loss 0.143397 history loss 3.697338 rank 7
2023-02-21 18:11:33,112 DEBUG CV Batch 17/100 loss 6.576198 loss_att 8.238616 loss_ctc 9.922421 loss_rnnt 5.721073 hw_loss 0.143397 history loss 3.697338 rank 6
2023-02-21 18:11:33,116 DEBUG CV Batch 17/100 loss 6.576198 loss_att 8.238616 loss_ctc 9.922421 loss_rnnt 5.721073 hw_loss 0.143397 history loss 3.697338 rank 4
2023-02-21 18:11:33,216 DEBUG CV Batch 17/100 loss 6.576198 loss_att 8.238616 loss_ctc 9.922421 loss_rnnt 5.721073 hw_loss 0.143397 history loss 3.697338 rank 3
2023-02-21 18:11:33,249 DEBUG CV Batch 17/100 loss 6.576198 loss_att 8.238616 loss_ctc 9.922421 loss_rnnt 5.721073 hw_loss 0.143397 history loss 3.697338 rank 0
2023-02-21 18:11:33,261 DEBUG CV Batch 17/100 loss 6.576198 loss_att 8.238616 loss_ctc 9.922421 loss_rnnt 5.721073 hw_loss 0.143397 history loss 3.697338 rank 5
2023-02-21 18:11:33,318 DEBUG CV Batch 17/100 loss 6.576198 loss_att 8.238616 loss_ctc 9.922421 loss_rnnt 5.721073 hw_loss 0.143397 history loss 3.697338 rank 1
2023-02-21 18:11:46,253 DEBUG CV Batch 17/200 loss 6.848138 loss_att 18.456482 loss_ctc 8.859657 loss_rnnt 4.199063 hw_loss 0.111005 history loss 4.231150 rank 7
2023-02-21 18:11:46,278 DEBUG CV Batch 17/200 loss 6.848138 loss_att 18.456482 loss_ctc 8.859657 loss_rnnt 4.199063 hw_loss 0.111005 history loss 4.231150 rank 2
2023-02-21 18:11:46,427 DEBUG CV Batch 17/200 loss 6.848138 loss_att 18.456482 loss_ctc 8.859657 loss_rnnt 4.199063 hw_loss 0.111005 history loss 4.231150 rank 4
2023-02-21 18:11:46,502 DEBUG CV Batch 17/200 loss 6.848138 loss_att 18.456482 loss_ctc 8.859657 loss_rnnt 4.199063 hw_loss 0.111005 history loss 4.231150 rank 6
2023-02-21 18:11:46,624 DEBUG CV Batch 17/200 loss 6.848138 loss_att 18.456482 loss_ctc 8.859657 loss_rnnt 4.199063 hw_loss 0.111005 history loss 4.231150 rank 3
2023-02-21 18:11:46,631 DEBUG CV Batch 17/200 loss 6.848138 loss_att 18.456482 loss_ctc 8.859657 loss_rnnt 4.199063 hw_loss 0.111005 history loss 4.231150 rank 1
2023-02-21 18:11:46,670 DEBUG CV Batch 17/200 loss 6.848138 loss_att 18.456482 loss_ctc 8.859657 loss_rnnt 4.199063 hw_loss 0.111005 history loss 4.231150 rank 5
2023-02-21 18:11:46,679 DEBUG CV Batch 17/200 loss 6.848138 loss_att 18.456482 loss_ctc 8.859657 loss_rnnt 4.199063 hw_loss 0.111005 history loss 4.231150 rank 0
2023-02-21 18:11:58,252 DEBUG CV Batch 17/300 loss 5.791294 loss_att 6.053288 loss_ctc 8.906846 loss_rnnt 5.178514 hw_loss 0.271827 history loss 4.386434 rank 7
2023-02-21 18:11:58,329 DEBUG CV Batch 17/300 loss 5.791294 loss_att 6.053288 loss_ctc 8.906846 loss_rnnt 5.178514 hw_loss 0.271827 history loss 4.386434 rank 2
2023-02-21 18:11:58,496 DEBUG CV Batch 17/300 loss 5.791294 loss_att 6.053288 loss_ctc 8.906846 loss_rnnt 5.178514 hw_loss 0.271827 history loss 4.386434 rank 4
2023-02-21 18:11:58,711 DEBUG CV Batch 17/300 loss 5.791294 loss_att 6.053288 loss_ctc 8.906846 loss_rnnt 5.178514 hw_loss 0.271827 history loss 4.386434 rank 1
2023-02-21 18:11:58,726 DEBUG CV Batch 17/300 loss 5.791294 loss_att 6.053288 loss_ctc 8.906846 loss_rnnt 5.178514 hw_loss 0.271827 history loss 4.386434 rank 6
2023-02-21 18:11:58,940 DEBUG CV Batch 17/300 loss 5.791294 loss_att 6.053288 loss_ctc 8.906846 loss_rnnt 5.178514 hw_loss 0.271827 history loss 4.386434 rank 5
2023-02-21 18:11:58,948 DEBUG CV Batch 17/300 loss 5.791294 loss_att 6.053288 loss_ctc 8.906846 loss_rnnt 5.178514 hw_loss 0.271827 history loss 4.386434 rank 0
2023-02-21 18:11:59,715 DEBUG CV Batch 17/300 loss 5.791294 loss_att 6.053288 loss_ctc 8.906846 loss_rnnt 5.178514 hw_loss 0.271827 history loss 4.386434 rank 3
2023-02-21 18:12:10,147 DEBUG CV Batch 17/400 loss 26.788673 loss_att 115.396408 loss_ctc 15.475955 loss_rnnt 10.508067 hw_loss 0.126415 history loss 5.383598 rank 7
2023-02-21 18:12:10,290 DEBUG CV Batch 17/400 loss 26.788673 loss_att 115.396408 loss_ctc 15.475955 loss_rnnt 10.508067 hw_loss 0.126415 history loss 5.383598 rank 2
2023-02-21 18:12:10,416 DEBUG CV Batch 17/400 loss 26.788673 loss_att 115.396408 loss_ctc 15.475955 loss_rnnt 10.508067 hw_loss 0.126415 history loss 5.383598 rank 4
2023-02-21 18:12:10,665 DEBUG CV Batch 17/400 loss 26.788673 loss_att 115.396408 loss_ctc 15.475955 loss_rnnt 10.508067 hw_loss 0.126415 history loss 5.383598 rank 1
2023-02-21 18:12:10,750 DEBUG CV Batch 17/400 loss 26.788673 loss_att 115.396408 loss_ctc 15.475955 loss_rnnt 10.508067 hw_loss 0.126415 history loss 5.383598 rank 6
2023-02-21 18:12:10,921 DEBUG CV Batch 17/400 loss 26.788673 loss_att 115.396408 loss_ctc 15.475955 loss_rnnt 10.508067 hw_loss 0.126415 history loss 5.383598 rank 0
2023-02-21 18:12:10,976 DEBUG CV Batch 17/400 loss 26.788673 loss_att 115.396408 loss_ctc 15.475955 loss_rnnt 10.508067 hw_loss 0.126415 history loss 5.383598 rank 5
2023-02-21 18:12:12,309 DEBUG CV Batch 17/400 loss 26.788673 loss_att 115.396408 loss_ctc 15.475955 loss_rnnt 10.508067 hw_loss 0.126415 history loss 5.383598 rank 3
2023-02-21 18:12:20,662 DEBUG CV Batch 17/500 loss 7.695765 loss_att 7.417252 loss_ctc 9.001230 loss_rnnt 7.559868 hw_loss 0.032883 history loss 6.213002 rank 7
2023-02-21 18:12:20,940 DEBUG CV Batch 17/500 loss 7.695765 loss_att 7.417252 loss_ctc 9.001230 loss_rnnt 7.559868 hw_loss 0.032883 history loss 6.213002 rank 4
2023-02-21 18:12:21,040 DEBUG CV Batch 17/500 loss 7.695765 loss_att 7.417252 loss_ctc 9.001230 loss_rnnt 7.559868 hw_loss 0.032883 history loss 6.213002 rank 2
2023-02-21 18:12:21,115 DEBUG CV Batch 17/500 loss 7.695765 loss_att 7.417252 loss_ctc 9.001230 loss_rnnt 7.559868 hw_loss 0.032883 history loss 6.213002 rank 1
2023-02-21 18:12:21,397 DEBUG CV Batch 17/500 loss 7.695765 loss_att 7.417252 loss_ctc 9.001230 loss_rnnt 7.559868 hw_loss 0.032883 history loss 6.213002 rank 6
2023-02-21 18:12:21,575 DEBUG CV Batch 17/500 loss 7.695765 loss_att 7.417252 loss_ctc 9.001230 loss_rnnt 7.559868 hw_loss 0.032883 history loss 6.213002 rank 0
2023-02-21 18:12:21,652 DEBUG CV Batch 17/500 loss 7.695765 loss_att 7.417252 loss_ctc 9.001230 loss_rnnt 7.559868 hw_loss 0.032883 history loss 6.213002 rank 5
2023-02-21 18:12:23,203 DEBUG CV Batch 17/500 loss 7.695765 loss_att 7.417252 loss_ctc 9.001230 loss_rnnt 7.559868 hw_loss 0.032883 history loss 6.213002 rank 3
2023-02-21 18:12:32,589 DEBUG CV Batch 17/600 loss 7.346078 loss_att 7.267482 loss_ctc 9.569202 loss_rnnt 6.953865 hw_loss 0.209092 history loss 7.193411 rank 7
2023-02-21 18:12:32,862 DEBUG CV Batch 17/600 loss 7.346078 loss_att 7.267482 loss_ctc 9.569202 loss_rnnt 6.953865 hw_loss 0.209092 history loss 7.193411 rank 4
2023-02-21 18:12:33,005 DEBUG CV Batch 17/600 loss 7.346078 loss_att 7.267482 loss_ctc 9.569202 loss_rnnt 6.953865 hw_loss 0.209092 history loss 7.193411 rank 2
2023-02-21 18:12:33,087 DEBUG CV Batch 17/600 loss 7.346078 loss_att 7.267482 loss_ctc 9.569202 loss_rnnt 6.953865 hw_loss 0.209092 history loss 7.193411 rank 1
2023-02-21 18:12:33,503 DEBUG CV Batch 17/600 loss 7.346078 loss_att 7.267482 loss_ctc 9.569202 loss_rnnt 6.953865 hw_loss 0.209092 history loss 7.193411 rank 6
2023-02-21 18:12:33,648 DEBUG CV Batch 17/600 loss 7.346078 loss_att 7.267482 loss_ctc 9.569202 loss_rnnt 6.953865 hw_loss 0.209092 history loss 7.193411 rank 0
2023-02-21 18:12:33,712 DEBUG CV Batch 17/600 loss 7.346078 loss_att 7.267482 loss_ctc 9.569202 loss_rnnt 6.953865 hw_loss 0.209092 history loss 7.193411 rank 5
2023-02-21 18:12:36,592 DEBUG CV Batch 17/600 loss 7.346078 loss_att 7.267482 loss_ctc 9.569202 loss_rnnt 6.953865 hw_loss 0.209092 history loss 7.193411 rank 3
2023-02-21 18:12:43,835 DEBUG CV Batch 17/700 loss 12.058102 loss_att 29.517365 loss_ctc 16.914875 loss_rnnt 7.901142 hw_loss 0.032883 history loss 7.901435 rank 7
2023-02-21 18:12:44,090 DEBUG CV Batch 17/700 loss 12.058102 loss_att 29.517365 loss_ctc 16.914875 loss_rnnt 7.901142 hw_loss 0.032883 history loss 7.901435 rank 4
2023-02-21 18:12:44,333 DEBUG CV Batch 17/700 loss 12.058102 loss_att 29.517365 loss_ctc 16.914875 loss_rnnt 7.901142 hw_loss 0.032883 history loss 7.901435 rank 2
2023-02-21 18:12:44,382 DEBUG CV Batch 17/700 loss 12.058102 loss_att 29.517365 loss_ctc 16.914875 loss_rnnt 7.901142 hw_loss 0.032883 history loss 7.901435 rank 1
2023-02-21 18:12:44,871 DEBUG CV Batch 17/700 loss 12.058102 loss_att 29.517365 loss_ctc 16.914875 loss_rnnt 7.901142 hw_loss 0.032883 history loss 7.901435 rank 6
2023-02-21 18:12:45,128 DEBUG CV Batch 17/700 loss 12.058102 loss_att 29.517365 loss_ctc 16.914875 loss_rnnt 7.901142 hw_loss 0.032883 history loss 7.901435 rank 0
2023-02-21 18:12:45,134 DEBUG CV Batch 17/700 loss 12.058102 loss_att 29.517365 loss_ctc 16.914875 loss_rnnt 7.901142 hw_loss 0.032883 history loss 7.901435 rank 5
2023-02-21 18:12:49,295 DEBUG CV Batch 17/700 loss 12.058102 loss_att 29.517365 loss_ctc 16.914875 loss_rnnt 7.901142 hw_loss 0.032883 history loss 7.901435 rank 3
2023-02-21 18:12:55,129 DEBUG CV Batch 17/800 loss 10.513812 loss_att 10.866863 loss_ctc 17.289526 loss_rnnt 9.419088 hw_loss 0.226283 history loss 7.339781 rank 7
2023-02-21 18:12:55,408 DEBUG CV Batch 17/800 loss 10.513812 loss_att 10.866863 loss_ctc 17.289526 loss_rnnt 9.419088 hw_loss 0.226283 history loss 7.339781 rank 4
2023-02-21 18:12:55,448 DEBUG CV Batch 17/800 loss 10.513812 loss_att 10.866863 loss_ctc 17.289526 loss_rnnt 9.419088 hw_loss 0.226283 history loss 7.339781 rank 1
2023-02-21 18:12:55,519 DEBUG CV Batch 17/800 loss 10.513812 loss_att 10.866863 loss_ctc 17.289526 loss_rnnt 9.419088 hw_loss 0.226283 history loss 7.339781 rank 2
2023-02-21 18:12:56,155 DEBUG CV Batch 17/800 loss 10.513812 loss_att 10.866863 loss_ctc 17.289526 loss_rnnt 9.419088 hw_loss 0.226283 history loss 7.339781 rank 6
2023-02-21 18:12:56,359 DEBUG CV Batch 17/800 loss 10.513812 loss_att 10.866863 loss_ctc 17.289526 loss_rnnt 9.419088 hw_loss 0.226283 history loss 7.339781 rank 5
2023-02-21 18:12:56,484 DEBUG CV Batch 17/800 loss 10.513812 loss_att 10.866863 loss_ctc 17.289526 loss_rnnt 9.419088 hw_loss 0.226283 history loss 7.339781 rank 0
2023-02-21 18:13:00,741 DEBUG CV Batch 17/800 loss 10.513812 loss_att 10.866863 loss_ctc 17.289526 loss_rnnt 9.419088 hw_loss 0.226283 history loss 7.339781 rank 3
2023-02-21 18:13:08,244 DEBUG CV Batch 17/900 loss 12.553480 loss_att 16.621214 loss_ctc 22.200653 loss_rnnt 10.405212 hw_loss 0.090809 history loss 7.117077 rank 7
2023-02-21 18:13:08,595 DEBUG CV Batch 17/900 loss 12.553480 loss_att 16.621214 loss_ctc 22.200653 loss_rnnt 10.405212 hw_loss 0.090809 history loss 7.117077 rank 4
2023-02-21 18:13:08,646 DEBUG CV Batch 17/900 loss 12.553480 loss_att 16.621214 loss_ctc 22.200653 loss_rnnt 10.405212 hw_loss 0.090809 history loss 7.117077 rank 1
2023-02-21 18:13:08,783 DEBUG CV Batch 17/900 loss 12.553480 loss_att 16.621214 loss_ctc 22.200653 loss_rnnt 10.405212 hw_loss 0.090809 history loss 7.117077 rank 2
2023-02-21 18:13:09,433 DEBUG CV Batch 17/900 loss 12.553480 loss_att 16.621214 loss_ctc 22.200653 loss_rnnt 10.405212 hw_loss 0.090809 history loss 7.117077 rank 6
2023-02-21 18:13:09,669 DEBUG CV Batch 17/900 loss 12.553480 loss_att 16.621214 loss_ctc 22.200653 loss_rnnt 10.405212 hw_loss 0.090809 history loss 7.117077 rank 5
2023-02-21 18:13:09,774 DEBUG CV Batch 17/900 loss 12.553480 loss_att 16.621214 loss_ctc 22.200653 loss_rnnt 10.405212 hw_loss 0.090809 history loss 7.117077 rank 0
2023-02-21 18:13:14,106 DEBUG CV Batch 17/900 loss 12.553480 loss_att 16.621214 loss_ctc 22.200653 loss_rnnt 10.405212 hw_loss 0.090809 history loss 7.117077 rank 3
2023-02-21 18:13:20,189 DEBUG CV Batch 17/1000 loss 3.678713 loss_att 4.709741 loss_ctc 5.066075 loss_rnnt 3.211523 hw_loss 0.142506 history loss 6.864534 rank 7
2023-02-21 18:13:20,633 DEBUG CV Batch 17/1000 loss 3.678713 loss_att 4.709741 loss_ctc 5.066075 loss_rnnt 3.211523 hw_loss 0.142506 history loss 6.864534 rank 4
2023-02-21 18:13:20,716 DEBUG CV Batch 17/1000 loss 3.678713 loss_att 4.709741 loss_ctc 5.066075 loss_rnnt 3.211523 hw_loss 0.142506 history loss 6.864534 rank 1
2023-02-21 18:13:20,805 DEBUG CV Batch 17/1000 loss 3.678713 loss_att 4.709741 loss_ctc 5.066075 loss_rnnt 3.211523 hw_loss 0.142506 history loss 6.864534 rank 2
2023-02-21 18:13:21,545 DEBUG CV Batch 17/1000 loss 3.678713 loss_att 4.709741 loss_ctc 5.066075 loss_rnnt 3.211523 hw_loss 0.142506 history loss 6.864534 rank 6
2023-02-21 18:13:21,822 DEBUG CV Batch 17/1000 loss 3.678713 loss_att 4.709741 loss_ctc 5.066075 loss_rnnt 3.211523 hw_loss 0.142506 history loss 6.864534 rank 5
2023-02-21 18:13:22,138 DEBUG CV Batch 17/1000 loss 3.678713 loss_att 4.709741 loss_ctc 5.066075 loss_rnnt 3.211523 hw_loss 0.142506 history loss 6.864534 rank 0
2023-02-21 18:13:26,510 DEBUG CV Batch 17/1000 loss 3.678713 loss_att 4.709741 loss_ctc 5.066075 loss_rnnt 3.211523 hw_loss 0.142506 history loss 6.864534 rank 3
2023-02-21 18:13:31,922 DEBUG CV Batch 17/1100 loss 7.062553 loss_att 6.131643 loss_ctc 8.885904 loss_rnnt 6.748943 hw_loss 0.481273 history loss 6.846912 rank 7
2023-02-21 18:13:32,401 DEBUG CV Batch 17/1100 loss 7.062553 loss_att 6.131643 loss_ctc 8.885904 loss_rnnt 6.748943 hw_loss 0.481273 history loss 6.846912 rank 4
2023-02-21 18:13:32,531 DEBUG CV Batch 17/1100 loss 7.062553 loss_att 6.131643 loss_ctc 8.885904 loss_rnnt 6.748943 hw_loss 0.481273 history loss 6.846912 rank 1
2023-02-21 18:13:32,720 DEBUG CV Batch 17/1100 loss 7.062553 loss_att 6.131643 loss_ctc 8.885904 loss_rnnt 6.748943 hw_loss 0.481273 history loss 6.846912 rank 2
2023-02-21 18:13:33,434 DEBUG CV Batch 17/1100 loss 7.062553 loss_att 6.131643 loss_ctc 8.885904 loss_rnnt 6.748943 hw_loss 0.481273 history loss 6.846912 rank 6
2023-02-21 18:13:33,751 DEBUG CV Batch 17/1100 loss 7.062553 loss_att 6.131643 loss_ctc 8.885904 loss_rnnt 6.748943 hw_loss 0.481273 history loss 6.846912 rank 5
2023-02-21 18:13:34,124 DEBUG CV Batch 17/1100 loss 7.062553 loss_att 6.131643 loss_ctc 8.885904 loss_rnnt 6.748943 hw_loss 0.481273 history loss 6.846912 rank 0
2023-02-21 18:13:38,400 DEBUG CV Batch 17/1100 loss 7.062553 loss_att 6.131643 loss_ctc 8.885904 loss_rnnt 6.748943 hw_loss 0.481273 history loss 6.846912 rank 3
2023-02-21 18:13:42,343 DEBUG CV Batch 17/1200 loss 8.326225 loss_att 9.367073 loss_ctc 10.315231 loss_rnnt 7.811151 hw_loss 0.078194 history loss 7.201599 rank 7
2023-02-21 18:13:42,937 DEBUG CV Batch 17/1200 loss 8.326225 loss_att 9.367073 loss_ctc 10.315231 loss_rnnt 7.811151 hw_loss 0.078194 history loss 7.201599 rank 4
2023-02-21 18:13:43,023 DEBUG CV Batch 17/1200 loss 8.326225 loss_att 9.367073 loss_ctc 10.315231 loss_rnnt 7.811151 hw_loss 0.078194 history loss 7.201599 rank 1
2023-02-21 18:13:43,223 DEBUG CV Batch 17/1200 loss 8.326225 loss_att 9.367073 loss_ctc 10.315231 loss_rnnt 7.811151 hw_loss 0.078194 history loss 7.201599 rank 2
2023-02-21 18:13:44,078 DEBUG CV Batch 17/1200 loss 8.326225 loss_att 9.367073 loss_ctc 10.315231 loss_rnnt 7.811151 hw_loss 0.078194 history loss 7.201599 rank 6
2023-02-21 18:13:44,388 DEBUG CV Batch 17/1200 loss 8.326225 loss_att 9.367073 loss_ctc 10.315231 loss_rnnt 7.811151 hw_loss 0.078194 history loss 7.201599 rank 5
2023-02-21 18:13:44,900 DEBUG CV Batch 17/1200 loss 8.326225 loss_att 9.367073 loss_ctc 10.315231 loss_rnnt 7.811151 hw_loss 0.078194 history loss 7.201599 rank 0
2023-02-21 18:13:49,045 DEBUG CV Batch 17/1200 loss 8.326225 loss_att 9.367073 loss_ctc 10.315231 loss_rnnt 7.811151 hw_loss 0.078194 history loss 7.201599 rank 3
2023-02-21 18:13:54,631 DEBUG CV Batch 17/1300 loss 6.143119 loss_att 5.878477 loss_ctc 8.731739 loss_rnnt 5.668593 hw_loss 0.341821 history loss 7.523149 rank 7
2023-02-21 18:13:54,891 DEBUG CV Batch 17/1300 loss 6.143119 loss_att 5.878477 loss_ctc 8.731739 loss_rnnt 5.668593 hw_loss 0.341821 history loss 7.523149 rank 4
2023-02-21 18:13:54,949 DEBUG CV Batch 17/1300 loss 6.143119 loss_att 5.878477 loss_ctc 8.731739 loss_rnnt 5.668593 hw_loss 0.341821 history loss 7.523149 rank 1
2023-02-21 18:13:55,120 DEBUG CV Batch 17/1300 loss 6.143118 loss_att 5.878477 loss_ctc 8.731739 loss_rnnt 5.668593 hw_loss 0.341821 history loss 7.523149 rank 2
2023-02-21 18:13:56,162 DEBUG CV Batch 17/1300 loss 6.143119 loss_att 5.878477 loss_ctc 8.731739 loss_rnnt 5.668593 hw_loss 0.341821 history loss 7.523149 rank 6
2023-02-21 18:13:56,501 DEBUG CV Batch 17/1300 loss 6.143119 loss_att 5.878477 loss_ctc 8.731739 loss_rnnt 5.668593 hw_loss 0.341821 history loss 7.523149 rank 5
2023-02-21 18:13:57,348 DEBUG CV Batch 17/1300 loss 6.143119 loss_att 5.878477 loss_ctc 8.731739 loss_rnnt 5.668593 hw_loss 0.341821 history loss 7.523149 rank 0
2023-02-21 18:14:01,314 DEBUG CV Batch 17/1300 loss 6.143119 loss_att 5.878477 loss_ctc 8.731739 loss_rnnt 5.668593 hw_loss 0.341821 history loss 7.523149 rank 3
2023-02-21 18:14:05,730 DEBUG CV Batch 17/1400 loss 11.018456 loss_att 28.870039 loss_ctc 10.133337 loss_rnnt 7.548618 hw_loss 0.032883 history loss 7.866144 rank 7
2023-02-21 18:14:05,988 DEBUG CV Batch 17/1400 loss 11.018456 loss_att 28.870039 loss_ctc 10.133337 loss_rnnt 7.548618 hw_loss 0.032883 history loss 7.866144 rank 4
2023-02-21 18:14:06,131 DEBUG CV Batch 17/1400 loss 11.018456 loss_att 28.870039 loss_ctc 10.133337 loss_rnnt 7.548618 hw_loss 0.032883 history loss 7.866144 rank 1
2023-02-21 18:14:06,286 DEBUG CV Batch 17/1400 loss 11.018456 loss_att 28.870039 loss_ctc 10.133337 loss_rnnt 7.548618 hw_loss 0.032883 history loss 7.866144 rank 2
2023-02-21 18:14:07,360 DEBUG CV Batch 17/1400 loss 11.018456 loss_att 28.870039 loss_ctc 10.133337 loss_rnnt 7.548618 hw_loss 0.032883 history loss 7.866144 rank 6
2023-02-21 18:14:07,820 DEBUG CV Batch 17/1400 loss 11.018456 loss_att 28.870039 loss_ctc 10.133337 loss_rnnt 7.548618 hw_loss 0.032883 history loss 7.866144 rank 5
2023-02-21 18:14:08,633 DEBUG CV Batch 17/1400 loss 11.018456 loss_att 28.870039 loss_ctc 10.133337 loss_rnnt 7.548618 hw_loss 0.032883 history loss 7.866144 rank 0
2023-02-21 18:14:12,605 DEBUG CV Batch 17/1400 loss 11.018456 loss_att 28.870039 loss_ctc 10.133337 loss_rnnt 7.548618 hw_loss 0.032883 history loss 7.866144 rank 3
2023-02-21 18:14:17,164 DEBUG CV Batch 17/1500 loss 8.112477 loss_att 8.749985 loss_ctc 8.820978 loss_rnnt 7.811184 hw_loss 0.148735 history loss 7.687248 rank 7
2023-02-21 18:14:17,430 DEBUG CV Batch 17/1500 loss 8.112477 loss_att 8.749985 loss_ctc 8.820978 loss_rnnt 7.811184 hw_loss 0.148735 history loss 7.687248 rank 4
2023-02-21 18:14:17,487 DEBUG CV Batch 17/1500 loss 8.112477 loss_att 8.749985 loss_ctc 8.820978 loss_rnnt 7.811184 hw_loss 0.148735 history loss 7.687248 rank 1
2023-02-21 18:14:17,639 DEBUG CV Batch 17/1500 loss 8.112477 loss_att 8.749985 loss_ctc 8.820978 loss_rnnt 7.811184 hw_loss 0.148735 history loss 7.687248 rank 2
2023-02-21 18:14:18,854 DEBUG CV Batch 17/1500 loss 8.112477 loss_att 8.749985 loss_ctc 8.820978 loss_rnnt 7.811184 hw_loss 0.148735 history loss 7.687248 rank 6
2023-02-21 18:14:19,393 DEBUG CV Batch 17/1500 loss 8.112477 loss_att 8.749985 loss_ctc 8.820978 loss_rnnt 7.811184 hw_loss 0.148735 history loss 7.687248 rank 5
2023-02-21 18:14:20,115 DEBUG CV Batch 17/1500 loss 8.112477 loss_att 8.749985 loss_ctc 8.820978 loss_rnnt 7.811184 hw_loss 0.148735 history loss 7.687248 rank 0
2023-02-21 18:14:24,170 DEBUG CV Batch 17/1500 loss 8.112477 loss_att 8.749985 loss_ctc 8.820978 loss_rnnt 7.811184 hw_loss 0.148735 history loss 7.687248 rank 3
2023-02-21 18:14:30,054 DEBUG CV Batch 17/1600 loss 9.388371 loss_att 14.234833 loss_ctc 11.695438 loss_rnnt 8.037293 hw_loss 0.139080 history loss 7.608456 rank 7
2023-02-21 18:14:30,262 DEBUG CV Batch 17/1600 loss 9.388371 loss_att 14.234833 loss_ctc 11.695438 loss_rnnt 8.037293 hw_loss 0.139080 history loss 7.608456 rank 4
2023-02-21 18:14:30,454 DEBUG CV Batch 17/1600 loss 9.388371 loss_att 14.234833 loss_ctc 11.695438 loss_rnnt 8.037293 hw_loss 0.139080 history loss 7.608456 rank 1
2023-02-21 18:14:30,529 DEBUG CV Batch 17/1600 loss 9.388371 loss_att 14.234833 loss_ctc 11.695438 loss_rnnt 8.037293 hw_loss 0.139080 history loss 7.608456 rank 2
2023-02-21 18:14:31,823 DEBUG CV Batch 17/1600 loss 9.388371 loss_att 14.234833 loss_ctc 11.695438 loss_rnnt 8.037293 hw_loss 0.139080 history loss 7.608456 rank 6
2023-02-21 18:14:32,442 DEBUG CV Batch 17/1600 loss 9.388371 loss_att 14.234833 loss_ctc 11.695438 loss_rnnt 8.037293 hw_loss 0.139080 history loss 7.608456 rank 5
2023-02-21 18:14:33,182 DEBUG CV Batch 17/1600 loss 9.388371 loss_att 14.234833 loss_ctc 11.695438 loss_rnnt 8.037293 hw_loss 0.139080 history loss 7.608456 rank 0
2023-02-21 18:14:37,298 DEBUG CV Batch 17/1600 loss 9.388371 loss_att 14.234833 loss_ctc 11.695438 loss_rnnt 8.037293 hw_loss 0.139080 history loss 7.608456 rank 3
2023-02-21 18:14:42,362 DEBUG CV Batch 17/1700 loss 9.536927 loss_att 9.155833 loss_ctc 14.498683 loss_rnnt 8.824000 hw_loss 0.239210 history loss 7.501418 rank 7
2023-02-21 18:14:42,424 DEBUG CV Batch 17/1700 loss 9.536927 loss_att 9.155833 loss_ctc 14.498683 loss_rnnt 8.824000 hw_loss 0.239210 history loss 7.501418 rank 4
2023-02-21 18:14:42,573 DEBUG CV Batch 17/1700 loss 9.536927 loss_att 9.155833 loss_ctc 14.498683 loss_rnnt 8.824000 hw_loss 0.239210 history loss 7.501418 rank 1
2023-02-21 18:14:42,665 DEBUG CV Batch 17/1700 loss 9.536927 loss_att 9.155833 loss_ctc 14.498683 loss_rnnt 8.824000 hw_loss 0.239210 history loss 7.501418 rank 2
2023-02-21 18:14:44,566 DEBUG CV Batch 17/1700 loss 9.536927 loss_att 9.155833 loss_ctc 14.498683 loss_rnnt 8.824000 hw_loss 0.239210 history loss 7.501418 rank 6
2023-02-21 18:14:45,127 DEBUG CV Batch 17/1700 loss 9.536927 loss_att 9.155833 loss_ctc 14.498683 loss_rnnt 8.824000 hw_loss 0.239210 history loss 7.501418 rank 5
2023-02-21 18:14:45,570 DEBUG CV Batch 17/1700 loss 9.536927 loss_att 9.155833 loss_ctc 14.498683 loss_rnnt 8.824000 hw_loss 0.239210 history loss 7.501418 rank 0
2023-02-21 18:14:49,398 DEBUG CV Batch 17/1700 loss 9.536927 loss_att 9.155833 loss_ctc 14.498683 loss_rnnt 8.824000 hw_loss 0.239210 history loss 7.501418 rank 3
2023-02-21 18:14:51,310 INFO Epoch 17 CV info cv_loss 7.468318646778811
2023-02-21 18:14:51,312 INFO Epoch 18 TRAIN info lr 0.00040802122183356335
2023-02-21 18:14:51,316 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 18:14:51,356 INFO Epoch 17 CV info cv_loss 7.468318646727123
2023-02-21 18:14:51,357 INFO Epoch 18 TRAIN info lr 0.0004080782932503862
2023-02-21 18:14:51,362 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 18:14:51,533 INFO Epoch 17 CV info cv_loss 7.468318646503143
2023-02-21 18:14:51,535 INFO Epoch 18 TRAIN info lr 0.00040809868170381766
2023-02-21 18:14:51,540 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 18:14:51,622 INFO Epoch 17 CV info cv_loss 7.46831864681327
2023-02-21 18:14:51,623 INFO Epoch 18 TRAIN info lr 0.0004080483957121854
2023-02-21 18:14:51,627 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 18:14:54,164 INFO Epoch 17 CV info cv_loss 7.468318647562742
2023-02-21 18:14:54,165 INFO Epoch 18 TRAIN info lr 0.00040806062572899376
2023-02-21 18:14:54,169 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 18:14:54,530 INFO Epoch 17 CV info cv_loss 7.468318647416294
2023-02-21 18:14:54,531 INFO Epoch 18 TRAIN info lr 0.00040803888423709804
2023-02-21 18:14:54,536 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 18:14:54,630 INFO Epoch 17 CV info cv_loss 7.468318647020021
2023-02-21 18:14:54,631 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/17.pt
2023-02-21 18:14:57,870 INFO Epoch 18 TRAIN info lr 0.00040809868170381766
2023-02-21 18:14:57,876 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 18:14:58,483 INFO Epoch 17 CV info cv_loss 7.46831864748521
2023-02-21 18:14:58,484 INFO Epoch 18 TRAIN info lr 0.00040803752550923816
2023-02-21 18:14:58,486 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 18:16:10,838 DEBUG TRAIN Batch 18/0 loss 11.059071 loss_att 10.430696 loss_ctc 15.890143 loss_rnnt 10.417339 hw_loss 0.231118 lr 0.00040804 rank 6
2023-02-21 18:16:10,839 DEBUG TRAIN Batch 18/0 loss 10.616605 loss_att 9.869867 loss_ctc 12.784729 loss_rnnt 10.265141 hw_loss 0.396989 lr 0.00040805 rank 2
2023-02-21 18:16:10,839 DEBUG TRAIN Batch 18/0 loss 11.138019 loss_att 10.791629 loss_ctc 13.189874 loss_rnnt 10.719786 hw_loss 0.401122 lr 0.00040810 rank 1
2023-02-21 18:16:10,848 DEBUG TRAIN Batch 18/0 loss 9.371079 loss_att 8.745556 loss_ctc 10.196904 loss_rnnt 9.240994 hw_loss 0.272024 lr 0.00040806 rank 5
2023-02-21 18:16:10,852 DEBUG TRAIN Batch 18/0 loss 10.529934 loss_att 9.877419 loss_ctc 13.812331 loss_rnnt 10.008104 hw_loss 0.402525 lr 0.00040802 rank 7
2023-02-21 18:16:10,857 DEBUG TRAIN Batch 18/0 loss 8.838066 loss_att 8.088480 loss_ctc 10.180779 loss_rnnt 8.707979 hw_loss 0.189327 lr 0.00040808 rank 4
2023-02-21 18:16:10,870 DEBUG TRAIN Batch 18/0 loss 7.147917 loss_att 6.581454 loss_ctc 8.362531 loss_rnnt 6.874048 hw_loss 0.422274 lr 0.00040804 rank 3
2023-02-21 18:16:10,992 DEBUG TRAIN Batch 18/0 loss 14.173415 loss_att 13.199066 loss_ctc 16.879421 loss_rnnt 13.876668 hw_loss 0.245278 lr 0.00040810 rank 0
2023-02-21 18:17:28,643 DEBUG TRAIN Batch 18/100 loss 3.561747 loss_att 7.128613 loss_ctc 3.025776 loss_rnnt 2.848414 hw_loss 0.133917 lr 0.00040794 rank 4
2023-02-21 18:17:28,644 DEBUG TRAIN Batch 18/100 loss 11.951519 loss_att 16.706905 loss_ctc 16.140104 loss_rnnt 10.383923 hw_loss 0.108826 lr 0.00040796 rank 1
2023-02-21 18:17:28,646 DEBUG TRAIN Batch 18/100 loss 9.705023 loss_att 14.748935 loss_ctc 13.553921 loss_rnnt 8.123333 hw_loss 0.111975 lr 0.00040796 rank 0
2023-02-21 18:17:28,647 DEBUG TRAIN Batch 18/100 loss 18.356310 loss_att 22.355360 loss_ctc 27.462997 loss_rnnt 16.308369 hw_loss 0.063573 lr 0.00040790 rank 6
2023-02-21 18:17:28,648 DEBUG TRAIN Batch 18/100 loss 13.300154 loss_att 14.846429 loss_ctc 13.801620 loss_rnnt 12.886333 hw_loss 0.070691 lr 0.00040791 rank 2
2023-02-21 18:17:28,652 DEBUG TRAIN Batch 18/100 loss 7.081937 loss_att 12.133041 loss_ctc 15.402267 loss_rnnt 4.929545 hw_loss 0.061487 lr 0.00040790 rank 3
2023-02-21 18:17:28,654 DEBUG TRAIN Batch 18/100 loss 3.519025 loss_att 7.460980 loss_ctc 5.242234 loss_rnnt 2.484780 hw_loss 0.030175 lr 0.00040788 rank 7
2023-02-21 18:17:28,656 DEBUG TRAIN Batch 18/100 loss 4.999693 loss_att 9.474391 loss_ctc 6.045167 loss_rnnt 3.870581 hw_loss 0.177705 lr 0.00040792 rank 5
2023-02-21 18:18:47,169 DEBUG TRAIN Batch 18/200 loss 7.401121 loss_att 9.507396 loss_ctc 16.554165 loss_rnnt 5.717922 hw_loss 0.077886 lr 0.00040781 rank 4
2023-02-21 18:18:47,170 DEBUG TRAIN Batch 18/200 loss 10.647130 loss_att 11.359709 loss_ctc 15.047534 loss_rnnt 9.902494 hw_loss 0.028872 lr 0.00040777 rank 6
2023-02-21 18:18:47,171 DEBUG TRAIN Batch 18/200 loss 6.012417 loss_att 9.725381 loss_ctc 7.760364 loss_rnnt 5.021145 hw_loss 0.029287 lr 0.00040775 rank 7
2023-02-21 18:18:47,175 DEBUG TRAIN Batch 18/200 loss 11.065557 loss_att 13.422905 loss_ctc 15.487907 loss_rnnt 9.969337 hw_loss 0.065819 lr 0.00040776 rank 3
2023-02-21 18:18:47,176 DEBUG TRAIN Batch 18/200 loss 5.796996 loss_att 8.017542 loss_ctc 8.565976 loss_rnnt 4.876756 hw_loss 0.200500 lr 0.00040778 rank 2
2023-02-21 18:18:47,180 DEBUG TRAIN Batch 18/200 loss 9.805544 loss_att 11.543011 loss_ctc 14.134193 loss_rnnt 8.865328 hw_loss 0.029193 lr 0.00040783 rank 0
2023-02-21 18:18:47,181 DEBUG TRAIN Batch 18/200 loss 6.183030 loss_att 8.300013 loss_ctc 6.929049 loss_rnnt 5.644827 hw_loss 0.028758 lr 0.00040783 rank 1
2023-02-21 18:18:47,181 DEBUG TRAIN Batch 18/200 loss 4.208534 loss_att 12.816437 loss_ctc 7.137607 loss_rnnt 2.055588 hw_loss 0.076541 lr 0.00040779 rank 5
2023-02-21 18:20:06,315 DEBUG TRAIN Batch 18/300 loss 2.114734 loss_att 4.471849 loss_ctc 5.040675 loss_rnnt 1.121091 hw_loss 0.247677 lr 0.00040767 rank 4
2023-02-21 18:20:06,318 DEBUG TRAIN Batch 18/300 loss 5.183373 loss_att 7.564367 loss_ctc 8.240136 loss_rnnt 4.236981 hw_loss 0.117421 lr 0.00040769 rank 0
2023-02-21 18:20:06,320 DEBUG TRAIN Batch 18/300 loss 16.251263 loss_att 20.990225 loss_ctc 26.104359 loss_rnnt 13.927843 hw_loss 0.116024 lr 0.00040761 rank 7
2023-02-21 18:20:06,324 DEBUG TRAIN Batch 18/300 loss 11.080198 loss_att 14.806627 loss_ctc 14.426008 loss_rnnt 9.856596 hw_loss 0.060390 lr 0.00040769 rank 1
2023-02-21 18:20:06,323 DEBUG TRAIN Batch 18/300 loss 12.564728 loss_att 14.628210 loss_ctc 14.296434 loss_rnnt 11.856941 hw_loss 0.120367 lr 0.00040763 rank 6
2023-02-21 18:20:06,325 DEBUG TRAIN Batch 18/300 loss 4.016184 loss_att 6.819230 loss_ctc 5.652036 loss_rnnt 3.151267 hw_loss 0.161615 lr 0.00040764 rank 2
2023-02-21 18:20:06,324 DEBUG TRAIN Batch 18/300 loss 10.952030 loss_att 13.176231 loss_ctc 12.520378 loss_rnnt 10.220696 hw_loss 0.145088 lr 0.00040765 rank 5
2023-02-21 18:20:06,326 DEBUG TRAIN Batch 18/300 loss 18.964396 loss_att 22.290627 loss_ctc 20.902884 loss_rnnt 17.952072 hw_loss 0.166147 lr 0.00040763 rank 3
2023-02-21 18:21:25,904 DEBUG TRAIN Batch 18/400 loss 2.603639 loss_att 4.742334 loss_ctc 4.526829 loss_rnnt 1.854385 hw_loss 0.122043 lr 0.00040748 rank 7
2023-02-21 18:21:25,904 DEBUG TRAIN Batch 18/400 loss 8.479883 loss_att 10.295025 loss_ctc 8.775930 loss_rnnt 8.040252 hw_loss 0.069617 lr 0.00040750 rank 6
2023-02-21 18:21:25,905 DEBUG TRAIN Batch 18/400 loss 19.251097 loss_att 18.980461 loss_ctc 23.694344 loss_rnnt 18.664272 hw_loss 0.090977 lr 0.00040750 rank 2
2023-02-21 18:21:25,907 DEBUG TRAIN Batch 18/400 loss 18.940512 loss_att 20.205879 loss_ctc 26.633501 loss_rnnt 17.632084 hw_loss 0.055544 lr 0.00040753 rank 4
2023-02-21 18:21:25,910 DEBUG TRAIN Batch 18/400 loss 6.859750 loss_att 9.458137 loss_ctc 9.620653 loss_rnnt 5.914467 hw_loss 0.107785 lr 0.00040755 rank 1
2023-02-21 18:21:25,914 DEBUG TRAIN Batch 18/400 loss 11.518575 loss_att 14.953991 loss_ctc 16.884897 loss_rnnt 10.066509 hw_loss 0.092761 lr 0.00040752 rank 5
2023-02-21 18:21:25,918 DEBUG TRAIN Batch 18/400 loss 22.393833 loss_att 26.546192 loss_ctc 29.762098 loss_rnnt 20.540504 hw_loss 0.075790 lr 0.00040749 rank 3
2023-02-21 18:21:25,961 DEBUG TRAIN Batch 18/400 loss 9.622654 loss_att 13.534728 loss_ctc 12.456888 loss_rnnt 8.391559 hw_loss 0.132719 lr 0.00040755 rank 0
2023-02-21 18:22:44,712 DEBUG TRAIN Batch 18/500 loss 6.830815 loss_att 8.269190 loss_ctc 7.288427 loss_rnnt 6.398647 hw_loss 0.156522 lr 0.00040734 rank 7
2023-02-21 18:22:44,714 DEBUG TRAIN Batch 18/500 loss 14.010397 loss_att 17.428848 loss_ctc 18.131136 loss_rnnt 12.748388 hw_loss 0.054163 lr 0.00040742 rank 1
2023-02-21 18:22:44,715 DEBUG TRAIN Batch 18/500 loss 8.129007 loss_att 11.881826 loss_ctc 9.300986 loss_rnnt 7.116674 hw_loss 0.197823 lr 0.00040740 rank 4
2023-02-21 18:22:44,716 DEBUG TRAIN Batch 18/500 loss 10.716649 loss_att 16.147734 loss_ctc 17.462797 loss_rnnt 8.688110 hw_loss 0.080316 lr 0.00040737 rank 2
2023-02-21 18:22:44,717 DEBUG TRAIN Batch 18/500 loss 10.350293 loss_att 13.433678 loss_ctc 14.082424 loss_rnnt 9.160502 hw_loss 0.141556 lr 0.00040736 rank 6
2023-02-21 18:22:44,718 DEBUG TRAIN Batch 18/500 loss 9.427081 loss_att 13.058916 loss_ctc 14.750405 loss_rnnt 7.923734 hw_loss 0.126007 lr 0.00040742 rank 0
2023-02-21 18:22:44,720 DEBUG TRAIN Batch 18/500 loss 4.773282 loss_att 6.064463 loss_ctc 6.995587 loss_rnnt 4.142786 hw_loss 0.142412 lr 0.00040738 rank 5
2023-02-21 18:22:44,721 DEBUG TRAIN Batch 18/500 loss 7.195183 loss_att 10.264282 loss_ctc 9.485724 loss_rnnt 6.210262 hw_loss 0.123178 lr 0.00040736 rank 3
2023-02-21 18:24:02,405 DEBUG TRAIN Batch 18/600 loss 9.050191 loss_att 9.790743 loss_ctc 12.390408 loss_rnnt 8.322490 hw_loss 0.251678 lr 0.00040723 rank 2
2023-02-21 18:24:02,409 DEBUG TRAIN Batch 18/600 loss 16.545904 loss_att 16.335863 loss_ctc 21.942993 loss_rnnt 15.788824 hw_loss 0.149018 lr 0.00040728 rank 1
2023-02-21 18:24:02,409 DEBUG TRAIN Batch 18/600 loss 14.478749 loss_att 13.767013 loss_ctc 16.130295 loss_rnnt 14.245143 hw_loss 0.292027 lr 0.00040726 rank 4
2023-02-21 18:24:02,410 DEBUG TRAIN Batch 18/600 loss 9.240508 loss_att 9.978134 loss_ctc 12.864774 loss_rnnt 8.472046 hw_loss 0.258191 lr 0.00040722 rank 3
2023-02-21 18:24:02,411 DEBUG TRAIN Batch 18/600 loss 5.631051 loss_att 5.828250 loss_ctc 7.141818 loss_rnnt 5.313462 hw_loss 0.143837 lr 0.00040722 rank 6
2023-02-21 18:24:02,411 DEBUG TRAIN Batch 18/600 loss 15.921434 loss_att 16.782911 loss_ctc 20.881353 loss_rnnt 14.983332 hw_loss 0.195905 lr 0.00040721 rank 7
2023-02-21 18:24:02,416 DEBUG TRAIN Batch 18/600 loss 12.953105 loss_att 14.390484 loss_ctc 18.870525 loss_rnnt 11.796469 hw_loss 0.150320 lr 0.00040728 rank 0
2023-02-21 18:24:02,417 DEBUG TRAIN Batch 18/600 loss 8.426872 loss_att 11.380632 loss_ctc 12.428335 loss_rnnt 7.214339 hw_loss 0.165474 lr 0.00040725 rank 5
2023-02-21 18:25:22,997 DEBUG TRAIN Batch 18/700 loss 8.471005 loss_att 12.936996 loss_ctc 11.759727 loss_rnnt 7.028511 hw_loss 0.207750 lr 0.00040707 rank 7
2023-02-21 18:25:23,001 DEBUG TRAIN Batch 18/700 loss 2.120567 loss_att 4.580892 loss_ctc 4.451468 loss_rnnt 1.276505 hw_loss 0.077270 lr 0.00040709 rank 3
2023-02-21 18:25:23,002 DEBUG TRAIN Batch 18/700 loss 5.838698 loss_att 9.615728 loss_ctc 9.783072 loss_rnnt 4.487872 hw_loss 0.130319 lr 0.00040715 rank 1
2023-02-21 18:25:23,002 DEBUG TRAIN Batch 18/700 loss 6.087454 loss_att 10.451518 loss_ctc 8.220297 loss_rnnt 4.855129 hw_loss 0.140874 lr 0.00040715 rank 0
2023-02-21 18:25:23,002 DEBUG TRAIN Batch 18/700 loss 4.484375 loss_att 6.364042 loss_ctc 7.052739 loss_rnnt 3.664593 hw_loss 0.190127 lr 0.00040709 rank 6
2023-02-21 18:25:23,006 DEBUG TRAIN Batch 18/700 loss 10.651177 loss_att 13.111306 loss_ctc 11.547392 loss_rnnt 9.992188 hw_loss 0.089005 lr 0.00040711 rank 5
2023-02-21 18:25:23,008 DEBUG TRAIN Batch 18/700 loss 9.773127 loss_att 12.313333 loss_ctc 15.768077 loss_rnnt 8.418884 hw_loss 0.087888 lr 0.00040713 rank 4
2023-02-21 18:25:23,020 DEBUG TRAIN Batch 18/700 loss 23.092783 loss_att 22.617649 loss_ctc 28.327892 loss_rnnt 22.389435 hw_loss 0.188179 lr 0.00040710 rank 2
2023-02-21 18:26:40,817 DEBUG TRAIN Batch 18/800 loss 7.463376 loss_att 11.663366 loss_ctc 11.140117 loss_rnnt 6.067140 hw_loss 0.123759 lr 0.00040699 rank 4
2023-02-21 18:26:40,821 DEBUG TRAIN Batch 18/800 loss 5.314480 loss_att 9.961613 loss_ctc 6.975729 loss_rnnt 4.148101 hw_loss 0.028974 lr 0.00040694 rank 7
2023-02-21 18:26:40,825 DEBUG TRAIN Batch 18/800 loss 15.997478 loss_att 18.781000 loss_ctc 27.269897 loss_rnnt 13.890005 hw_loss 0.089589 lr 0.00040701 rank 1
2023-02-21 18:26:40,825 DEBUG TRAIN Batch 18/800 loss 3.508245 loss_att 6.760293 loss_ctc 6.860433 loss_rnnt 2.366129 hw_loss 0.083901 lr 0.00040696 rank 2
2023-02-21 18:26:40,828 DEBUG TRAIN Batch 18/800 loss 23.923124 loss_att 24.550579 loss_ctc 35.022442 loss_rnnt 22.249136 hw_loss 0.128601 lr 0.00040701 rank 0
2023-02-21 18:26:40,828 DEBUG TRAIN Batch 18/800 loss 9.694688 loss_att 12.634812 loss_ctc 13.201747 loss_rnnt 8.500028 hw_loss 0.260676 lr 0.00040695 rank 3
2023-02-21 18:26:40,834 DEBUG TRAIN Batch 18/800 loss 4.009777 loss_att 8.434098 loss_ctc 5.689014 loss_rnnt 2.767000 hw_loss 0.251275 lr 0.00040695 rank 6
2023-02-21 18:26:40,872 DEBUG TRAIN Batch 18/800 loss 14.189731 loss_att 17.437382 loss_ctc 19.905907 loss_rnnt 12.714224 hw_loss 0.119662 lr 0.00040698 rank 5
2023-02-21 18:27:58,313 DEBUG TRAIN Batch 18/900 loss 3.659770 loss_att 6.350048 loss_ctc 4.939428 loss_rnnt 2.881589 hw_loss 0.130320 lr 0.00040682 rank 6
2023-02-21 18:27:58,316 DEBUG TRAIN Batch 18/900 loss 9.354300 loss_att 12.586203 loss_ctc 14.789490 loss_rnnt 7.950387 hw_loss 0.061574 lr 0.00040680 rank 7
2023-02-21 18:27:58,318 DEBUG TRAIN Batch 18/900 loss 12.824023 loss_att 16.175259 loss_ctc 22.870680 loss_rnnt 10.729145 hw_loss 0.159518 lr 0.00040688 rank 1
2023-02-21 18:27:58,319 DEBUG TRAIN Batch 18/900 loss 16.838636 loss_att 17.298489 loss_ctc 17.231697 loss_rnnt 16.610451 hw_loss 0.157142 lr 0.00040686 rank 4
2023-02-21 18:27:58,322 DEBUG TRAIN Batch 18/900 loss 7.289981 loss_att 8.695418 loss_ctc 9.805160 loss_rnnt 6.601346 hw_loss 0.135357 lr 0.00040688 rank 0
2023-02-21 18:27:58,323 DEBUG TRAIN Batch 18/900 loss 10.227407 loss_att 12.559767 loss_ctc 14.282018 loss_rnnt 9.133194 hw_loss 0.163361 lr 0.00040683 rank 2
2023-02-21 18:27:58,325 DEBUG TRAIN Batch 18/900 loss 10.591944 loss_att 13.628645 loss_ctc 14.584926 loss_rnnt 9.404184 hw_loss 0.090041 lr 0.00040682 rank 3
2023-02-21 18:27:58,327 DEBUG TRAIN Batch 18/900 loss 7.985633 loss_att 9.081689 loss_ctc 16.810617 loss_rnnt 6.528420 hw_loss 0.115006 lr 0.00040684 rank 5
2023-02-21 18:29:17,572 DEBUG TRAIN Batch 18/1000 loss 8.682272 loss_att 11.687930 loss_ctc 12.352769 loss_rnnt 7.549709 hw_loss 0.078806 lr 0.00040674 rank 1
2023-02-21 18:29:17,573 DEBUG TRAIN Batch 18/1000 loss 7.185830 loss_att 8.265528 loss_ctc 8.721333 loss_rnnt 6.648886 hw_loss 0.218008 lr 0.00040669 rank 2
2023-02-21 18:29:17,576 DEBUG TRAIN Batch 18/1000 loss 4.397356 loss_att 7.260041 loss_ctc 5.239778 loss_rnnt 3.686548 hw_loss 0.048653 lr 0.00040668 rank 3
2023-02-21 18:29:17,576 DEBUG TRAIN Batch 18/1000 loss 8.265316 loss_att 9.866567 loss_ctc 10.182592 loss_rnnt 7.656260 hw_loss 0.062191 lr 0.00040672 rank 4
2023-02-21 18:29:17,578 DEBUG TRAIN Batch 18/1000 loss 10.047493 loss_att 13.545691 loss_ctc 13.707174 loss_rnnt 8.745729 hw_loss 0.214061 lr 0.00040667 rank 7
2023-02-21 18:29:17,579 DEBUG TRAIN Batch 18/1000 loss 8.700113 loss_att 9.140199 loss_ctc 12.351132 loss_rnnt 8.064270 hw_loss 0.114420 lr 0.00040669 rank 6
2023-02-21 18:29:17,585 DEBUG TRAIN Batch 18/1000 loss 14.198115 loss_att 16.893398 loss_ctc 18.259785 loss_rnnt 13.056436 hw_loss 0.114500 lr 0.00040674 rank 0
2023-02-21 18:29:17,589 DEBUG TRAIN Batch 18/1000 loss 7.057647 loss_att 11.468064 loss_ctc 11.588826 loss_rnnt 5.506973 hw_loss 0.120812 lr 0.00040671 rank 5
2023-02-21 18:30:37,918 DEBUG TRAIN Batch 18/1100 loss 10.074458 loss_att 14.033844 loss_ctc 13.528054 loss_rnnt 8.735155 hw_loss 0.163025 lr 0.00040659 rank 4
2023-02-21 18:30:37,920 DEBUG TRAIN Batch 18/1100 loss 12.035830 loss_att 13.243456 loss_ctc 17.084274 loss_rnnt 11.076206 hw_loss 0.084325 lr 0.00040657 rank 5
2023-02-21 18:30:37,921 DEBUG TRAIN Batch 18/1100 loss 11.190019 loss_att 16.134012 loss_ctc 14.980045 loss_rnnt 9.656965 hw_loss 0.072969 lr 0.00040653 rank 7
2023-02-21 18:30:37,922 DEBUG TRAIN Batch 18/1100 loss 6.272022 loss_att 11.843257 loss_ctc 11.691987 loss_rnnt 4.391673 hw_loss 0.081448 lr 0.00040655 rank 6
2023-02-21 18:30:37,923 DEBUG TRAIN Batch 18/1100 loss 9.357214 loss_att 10.779619 loss_ctc 9.912898 loss_rnnt 8.906178 hw_loss 0.173370 lr 0.00040661 rank 1
2023-02-21 18:30:37,924 DEBUG TRAIN Batch 18/1100 loss 10.877733 loss_att 12.271891 loss_ctc 13.571703 loss_rnnt 10.182311 hw_loss 0.107615 lr 0.00040656 rank 2
2023-02-21 18:30:37,926 DEBUG TRAIN Batch 18/1100 loss 7.876417 loss_att 10.268960 loss_ctc 9.990476 loss_rnnt 7.054598 hw_loss 0.115192 lr 0.00040655 rank 3
2023-02-21 18:30:37,966 DEBUG TRAIN Batch 18/1100 loss 9.666889 loss_att 11.519062 loss_ctc 11.871826 loss_rnnt 8.963970 hw_loss 0.072174 lr 0.00040661 rank 0
2023-02-21 18:31:55,557 DEBUG TRAIN Batch 18/1200 loss 4.815123 loss_att 6.565961 loss_ctc 6.841788 loss_rnnt 4.154986 hw_loss 0.074526 lr 0.00040643 rank 2
2023-02-21 18:31:55,562 DEBUG TRAIN Batch 18/1200 loss 8.643539 loss_att 12.281789 loss_ctc 11.388553 loss_rnnt 7.439105 hw_loss 0.207718 lr 0.00040646 rank 4
2023-02-21 18:31:55,564 DEBUG TRAIN Batch 18/1200 loss 11.629458 loss_att 11.546246 loss_ctc 15.909555 loss_rnnt 10.966406 hw_loss 0.204403 lr 0.00040640 rank 7
2023-02-21 18:31:55,567 DEBUG TRAIN Batch 18/1200 loss 8.000879 loss_att 10.739553 loss_ctc 13.479134 loss_rnnt 6.585635 hw_loss 0.257017 lr 0.00040648 rank 1
2023-02-21 18:31:55,567 DEBUG TRAIN Batch 18/1200 loss 6.717027 loss_att 10.293991 loss_ctc 10.815422 loss_rnnt 5.381761 hw_loss 0.137664 lr 0.00040642 rank 6
2023-02-21 18:31:55,568 DEBUG TRAIN Batch 18/1200 loss 10.390501 loss_att 13.052889 loss_ctc 13.409206 loss_rnnt 9.341808 hw_loss 0.213227 lr 0.00040648 rank 0
2023-02-21 18:31:55,570 DEBUG TRAIN Batch 18/1200 loss 14.749764 loss_att 15.118402 loss_ctc 16.928724 loss_rnnt 14.317126 hw_loss 0.128220 lr 0.00040642 rank 3
2023-02-21 18:31:55,571 DEBUG TRAIN Batch 18/1200 loss 10.174621 loss_att 12.758262 loss_ctc 13.402266 loss_rnnt 9.094351 hw_loss 0.249729 lr 0.00040644 rank 5
2023-02-21 18:33:14,055 DEBUG TRAIN Batch 18/1300 loss 8.699059 loss_att 10.543678 loss_ctc 10.170710 loss_rnnt 8.008631 hw_loss 0.234908 lr 0.00040632 rank 4
2023-02-21 18:33:14,057 DEBUG TRAIN Batch 18/1300 loss 9.548056 loss_att 10.869120 loss_ctc 16.117819 loss_rnnt 8.238198 hw_loss 0.318140 lr 0.00040634 rank 1
2023-02-21 18:33:14,058 DEBUG TRAIN Batch 18/1300 loss 8.653537 loss_att 8.530104 loss_ctc 10.778529 loss_rnnt 8.278238 hw_loss 0.218725 lr 0.00040628 rank 6
2023-02-21 18:33:14,058 DEBUG TRAIN Batch 18/1300 loss 12.623412 loss_att 17.143972 loss_ctc 18.694410 loss_rnnt 10.846487 hw_loss 0.118776 lr 0.00040628 rank 3
2023-02-21 18:33:14,059 DEBUG TRAIN Batch 18/1300 loss 4.264510 loss_att 6.668354 loss_ctc 6.989559 loss_rnnt 3.332064 hw_loss 0.165632 lr 0.00040627 rank 7
2023-02-21 18:33:14,059 DEBUG TRAIN Batch 18/1300 loss 4.773482 loss_att 7.174948 loss_ctc 9.112005 loss_rnnt 3.689846 hw_loss 0.046638 lr 0.00040629 rank 2
2023-02-21 18:33:14,062 DEBUG TRAIN Batch 18/1300 loss 8.588993 loss_att 9.015351 loss_ctc 11.307757 loss_rnnt 8.083091 hw_loss 0.108992 lr 0.00040630 rank 5
2023-02-21 18:33:14,062 DEBUG TRAIN Batch 18/1300 loss 7.575287 loss_att 11.224386 loss_ctc 10.988516 loss_rnnt 6.317762 hw_loss 0.136139 lr 0.00040634 rank 0
2023-02-21 18:34:34,019 DEBUG TRAIN Batch 18/1400 loss 5.011366 loss_att 10.256781 loss_ctc 7.623508 loss_rnnt 3.581825 hw_loss 0.060324 lr 0.00040613 rank 7
2023-02-21 18:34:34,022 DEBUG TRAIN Batch 18/1400 loss 4.658733 loss_att 8.198877 loss_ctc 6.223510 loss_rnnt 3.692667 hw_loss 0.092624 lr 0.00040621 rank 0
2023-02-21 18:34:34,022 DEBUG TRAIN Batch 18/1400 loss 6.319122 loss_att 11.515108 loss_ctc 9.296679 loss_rnnt 4.805840 hw_loss 0.144520 lr 0.00040615 rank 6
2023-02-21 18:34:34,025 DEBUG TRAIN Batch 18/1400 loss 7.001805 loss_att 11.194983 loss_ctc 13.287987 loss_rnnt 5.246923 hw_loss 0.146416 lr 0.00040617 rank 5
2023-02-21 18:34:34,025 DEBUG TRAIN Batch 18/1400 loss 7.805961 loss_att 10.409019 loss_ctc 11.872925 loss_rnnt 6.705323 hw_loss 0.070805 lr 0.00040616 rank 2
2023-02-21 18:34:34,025 DEBUG TRAIN Batch 18/1400 loss 11.108284 loss_att 14.292030 loss_ctc 16.112206 loss_rnnt 9.701223 hw_loss 0.193354 lr 0.00040619 rank 4
2023-02-21 18:34:34,029 DEBUG TRAIN Batch 18/1400 loss 4.718732 loss_att 11.307694 loss_ctc 7.508001 loss_rnnt 3.002456 hw_loss 0.049840 lr 0.00040621 rank 1
2023-02-21 18:34:34,030 DEBUG TRAIN Batch 18/1400 loss 12.150522 loss_att 15.451567 loss_ctc 15.085875 loss_rnnt 11.023081 hw_loss 0.142222 lr 0.00040615 rank 3
2023-02-21 18:35:51,739 DEBUG TRAIN Batch 18/1500 loss 13.019172 loss_att 13.830744 loss_ctc 16.312891 loss_rnnt 12.402984 hw_loss 0.027582 lr 0.00040605 rank 4
2023-02-21 18:35:51,742 DEBUG TRAIN Batch 18/1500 loss 4.023401 loss_att 6.354980 loss_ctc 4.531048 loss_rnnt 3.443428 hw_loss 0.086197 lr 0.00040607 rank 1
2023-02-21 18:35:51,743 DEBUG TRAIN Batch 18/1500 loss 13.318444 loss_att 18.640102 loss_ctc 18.407242 loss_rnnt 11.512381 hw_loss 0.118548 lr 0.00040600 rank 7
2023-02-21 18:35:51,749 DEBUG TRAIN Batch 18/1500 loss 5.503344 loss_att 7.648345 loss_ctc 8.809737 loss_rnnt 4.546804 hw_loss 0.162539 lr 0.00040602 rank 2
2023-02-21 18:35:51,751 DEBUG TRAIN Batch 18/1500 loss 9.381539 loss_att 12.715571 loss_ctc 14.840658 loss_rnnt 7.924149 hw_loss 0.117567 lr 0.00040601 rank 6
2023-02-21 18:35:51,752 DEBUG TRAIN Batch 18/1500 loss 3.450997 loss_att 6.056954 loss_ctc 3.843394 loss_rnnt 2.836193 hw_loss 0.077424 lr 0.00040607 rank 0
2023-02-21 18:35:51,753 DEBUG TRAIN Batch 18/1500 loss 14.325730 loss_att 16.866808 loss_ctc 25.209692 loss_rnnt 12.323478 hw_loss 0.080330 lr 0.00040601 rank 3
2023-02-21 18:35:51,792 DEBUG TRAIN Batch 18/1500 loss 6.462830 loss_att 9.437326 loss_ctc 9.009099 loss_rnnt 5.492739 hw_loss 0.066917 lr 0.00040604 rank 5
2023-02-21 18:37:10,124 DEBUG TRAIN Batch 18/1600 loss 4.626790 loss_att 7.248425 loss_ctc 6.197433 loss_rnnt 3.751148 hw_loss 0.266055 lr 0.00040586 rank 7
2023-02-21 18:37:10,128 DEBUG TRAIN Batch 18/1600 loss 6.786273 loss_att 9.552288 loss_ctc 7.867970 loss_rnnt 6.031709 hw_loss 0.107127 lr 0.00040594 rank 0
2023-02-21 18:37:10,130 DEBUG TRAIN Batch 18/1600 loss 7.064176 loss_att 10.667964 loss_ctc 10.844878 loss_rnnt 5.749887 hw_loss 0.167695 lr 0.00040588 rank 3
2023-02-21 18:37:10,131 DEBUG TRAIN Batch 18/1600 loss 10.010160 loss_att 11.286104 loss_ctc 14.997159 loss_rnnt 9.073995 hw_loss 0.030081 lr 0.00040592 rank 4
2023-02-21 18:37:10,131 DEBUG TRAIN Batch 18/1600 loss 6.584712 loss_att 9.002861 loss_ctc 8.819349 loss_rnnt 5.754994 hw_loss 0.090253 lr 0.00040588 rank 6
2023-02-21 18:37:10,131 DEBUG TRAIN Batch 18/1600 loss 11.214364 loss_att 14.999607 loss_ctc 17.175671 loss_rnnt 9.629534 hw_loss 0.061765 lr 0.00040594 rank 1
2023-02-21 18:37:10,131 DEBUG TRAIN Batch 18/1600 loss 3.561415 loss_att 6.641922 loss_ctc 5.101341 loss_rnnt 2.688830 hw_loss 0.095925 lr 0.00040589 rank 2
2023-02-21 18:37:10,138 DEBUG TRAIN Batch 18/1600 loss 4.272462 loss_att 6.808198 loss_ctc 6.375793 loss_rnnt 3.425411 hw_loss 0.111488 lr 0.00040590 rank 5
2023-02-21 18:38:28,672 DEBUG TRAIN Batch 18/1700 loss 8.953800 loss_att 10.895587 loss_ctc 11.729670 loss_rnnt 8.124094 hw_loss 0.133562 lr 0.00040573 rank 7
2023-02-21 18:38:28,673 DEBUG TRAIN Batch 18/1700 loss 6.921690 loss_att 10.797707 loss_ctc 11.548326 loss_rnnt 5.475429 hw_loss 0.101574 lr 0.00040581 rank 0
2023-02-21 18:38:28,675 DEBUG TRAIN Batch 18/1700 loss 7.734749 loss_att 9.169461 loss_ctc 10.333426 loss_rnnt 6.992118 hw_loss 0.204749 lr 0.00040579 rank 4
2023-02-21 18:38:28,675 DEBUG TRAIN Batch 18/1700 loss 12.751481 loss_att 16.480137 loss_ctc 18.279272 loss_rnnt 11.214743 hw_loss 0.101192 lr 0.00040575 rank 6
2023-02-21 18:38:28,678 DEBUG TRAIN Batch 18/1700 loss 10.500626 loss_att 15.188332 loss_ctc 12.271030 loss_rnnt 9.146777 hw_loss 0.337974 lr 0.00040575 rank 3
2023-02-21 18:38:28,677 DEBUG TRAIN Batch 18/1700 loss 14.638079 loss_att 17.393377 loss_ctc 21.797174 loss_rnnt 13.008935 hw_loss 0.231632 lr 0.00040576 rank 2
2023-02-21 18:38:28,681 DEBUG TRAIN Batch 18/1700 loss 7.825165 loss_att 12.828563 loss_ctc 13.914249 loss_rnnt 5.924695 hw_loss 0.164836 lr 0.00040577 rank 5
2023-02-21 18:38:28,683 DEBUG TRAIN Batch 18/1700 loss 7.636853 loss_att 11.274672 loss_ctc 12.252676 loss_rnnt 6.277997 hw_loss 0.029718 lr 0.00040581 rank 1
2023-02-21 18:39:49,530 DEBUG TRAIN Batch 18/1800 loss 5.854983 loss_att 8.290208 loss_ctc 10.249290 loss_rnnt 4.686910 hw_loss 0.178351 lr 0.00040560 rank 7
2023-02-21 18:39:49,531 DEBUG TRAIN Batch 18/1800 loss 14.231001 loss_att 14.673044 loss_ctc 15.626121 loss_rnnt 13.906380 hw_loss 0.094119 lr 0.00040561 rank 6
2023-02-21 18:39:49,532 DEBUG TRAIN Batch 18/1800 loss 8.372115 loss_att 9.165184 loss_ctc 11.485020 loss_rnnt 7.731070 hw_loss 0.126334 lr 0.00040567 rank 0
2023-02-21 18:39:49,533 DEBUG TRAIN Batch 18/1800 loss 5.062710 loss_att 7.208829 loss_ctc 6.704243 loss_rnnt 4.302379 hw_loss 0.210443 lr 0.00040565 rank 4
2023-02-21 18:39:49,533 DEBUG TRAIN Batch 18/1800 loss 11.210264 loss_att 14.851768 loss_ctc 16.434690 loss_rnnt 9.712624 hw_loss 0.136407 lr 0.00040562 rank 2
2023-02-21 18:39:49,538 DEBUG TRAIN Batch 18/1800 loss 16.558100 loss_att 16.429253 loss_ctc 19.345798 loss_rnnt 16.143518 hw_loss 0.128733 lr 0.00040561 rank 3
2023-02-21 18:39:49,539 DEBUG TRAIN Batch 18/1800 loss 8.184004 loss_att 10.669166 loss_ctc 12.806884 loss_rnnt 7.019570 hw_loss 0.095656 lr 0.00040567 rank 1
2023-02-21 18:39:49,541 DEBUG TRAIN Batch 18/1800 loss 11.877031 loss_att 13.843675 loss_ctc 16.944954 loss_rnnt 10.749887 hw_loss 0.108924 lr 0.00040563 rank 5
2023-02-21 18:41:08,301 DEBUG TRAIN Batch 18/1900 loss 11.529602 loss_att 20.149210 loss_ctc 15.251436 loss_rnnt 9.140343 hw_loss 0.317050 lr 0.00040549 rank 2
2023-02-21 18:41:08,305 DEBUG TRAIN Batch 18/1900 loss 8.778573 loss_att 10.683260 loss_ctc 13.079930 loss_rnnt 7.728742 hw_loss 0.178835 lr 0.00040552 rank 4
2023-02-21 18:41:08,306 DEBUG TRAIN Batch 18/1900 loss 4.662826 loss_att 9.450366 loss_ctc 6.625873 loss_rnnt 3.363280 hw_loss 0.150558 lr 0.00040554 rank 0
2023-02-21 18:41:08,307 DEBUG TRAIN Batch 18/1900 loss 3.580261 loss_att 6.920071 loss_ctc 6.068607 loss_rnnt 2.523921 hw_loss 0.106120 lr 0.00040546 rank 7
2023-02-21 18:41:08,308 DEBUG TRAIN Batch 18/1900 loss 11.201917 loss_att 11.022370 loss_ctc 13.256307 loss_rnnt 10.893332 hw_loss 0.132327 lr 0.00040548 rank 6
2023-02-21 18:41:08,309 DEBUG TRAIN Batch 18/1900 loss 8.895890 loss_att 9.542463 loss_ctc 12.077056 loss_rnnt 8.284966 hw_loss 0.107727 lr 0.00040548 rank 3
2023-02-21 18:41:08,311 DEBUG TRAIN Batch 18/1900 loss 5.797412 loss_att 8.400304 loss_ctc 7.670497 loss_rnnt 4.966256 hw_loss 0.114063 lr 0.00040550 rank 5
2023-02-21 18:41:08,313 DEBUG TRAIN Batch 18/1900 loss 8.494317 loss_att 12.034159 loss_ctc 10.790823 loss_rnnt 7.415203 hw_loss 0.121769 lr 0.00040554 rank 1
2023-02-21 18:42:26,937 DEBUG TRAIN Batch 18/2000 loss 8.583531 loss_att 13.018438 loss_ctc 13.781258 loss_rnnt 6.942650 hw_loss 0.114131 lr 0.00040533 rank 7
2023-02-21 18:42:26,941 DEBUG TRAIN Batch 18/2000 loss 4.222244 loss_att 6.157566 loss_ctc 3.700934 loss_rnnt 3.844670 hw_loss 0.112532 lr 0.00040536 rank 2
2023-02-21 18:42:26,945 DEBUG TRAIN Batch 18/2000 loss 11.430947 loss_att 13.100847 loss_ctc 11.770278 loss_rnnt 11.008700 hw_loss 0.080668 lr 0.00040539 rank 4
2023-02-21 18:42:26,946 DEBUG TRAIN Batch 18/2000 loss 5.725621 loss_att 8.209674 loss_ctc 5.714321 loss_rnnt 5.153714 hw_loss 0.143631 lr 0.00040541 rank 1
2023-02-21 18:42:26,948 DEBUG TRAIN Batch 18/2000 loss 7.700306 loss_att 11.917088 loss_ctc 10.590007 loss_rnnt 6.289287 hw_loss 0.341944 lr 0.00040535 rank 6
2023-02-21 18:42:26,950 DEBUG TRAIN Batch 18/2000 loss 14.772142 loss_att 17.472893 loss_ctc 16.369743 loss_rnnt 13.982658 hw_loss 0.068101 lr 0.00040535 rank 3
2023-02-21 18:42:26,954 DEBUG TRAIN Batch 18/2000 loss 7.206958 loss_att 7.855180 loss_ctc 9.114632 loss_rnnt 6.682334 hw_loss 0.263668 lr 0.00040537 rank 5
2023-02-21 18:42:26,992 DEBUG TRAIN Batch 18/2000 loss 7.135429 loss_att 11.876542 loss_ctc 10.711105 loss_rnnt 5.641745 hw_loss 0.128821 lr 0.00040541 rank 0
2023-02-21 18:43:46,509 DEBUG TRAIN Batch 18/2100 loss 18.177937 loss_att 20.724663 loss_ctc 22.551449 loss_rnnt 17.070393 hw_loss 0.028242 lr 0.00040527 rank 1
2023-02-21 18:43:46,509 DEBUG TRAIN Batch 18/2100 loss 14.107684 loss_att 18.538952 loss_ctc 20.746828 loss_rnnt 12.301697 hw_loss 0.064714 lr 0.00040525 rank 4
2023-02-21 18:43:46,509 DEBUG TRAIN Batch 18/2100 loss 3.117616 loss_att 6.663551 loss_ctc 5.788041 loss_rnnt 1.971982 hw_loss 0.150731 lr 0.00040522 rank 2
2023-02-21 18:43:46,510 DEBUG TRAIN Batch 18/2100 loss 22.348354 loss_att 24.232346 loss_ctc 27.562757 loss_rnnt 21.157776 hw_loss 0.222237 lr 0.00040520 rank 7
2023-02-21 18:43:46,511 DEBUG TRAIN Batch 18/2100 loss 6.656296 loss_att 8.367413 loss_ctc 7.765380 loss_rnnt 6.068395 hw_loss 0.183376 lr 0.00040524 rank 5
2023-02-21 18:43:46,512 DEBUG TRAIN Batch 18/2100 loss 5.358780 loss_att 7.978375 loss_ctc 7.970140 loss_rnnt 4.416223 hw_loss 0.132108 lr 0.00040527 rank 0
2023-02-21 18:43:46,514 DEBUG TRAIN Batch 18/2100 loss 12.524776 loss_att 16.429459 loss_ctc 18.425417 loss_rnnt 10.901234 hw_loss 0.104727 lr 0.00040521 rank 3
2023-02-21 18:43:46,524 DEBUG TRAIN Batch 18/2100 loss 12.213669 loss_att 11.411318 loss_ctc 19.008942 loss_rnnt 11.388819 hw_loss 0.148656 lr 0.00040521 rank 6
2023-02-21 18:45:04,720 DEBUG TRAIN Batch 18/2200 loss 9.539620 loss_att 11.992344 loss_ctc 11.354494 loss_rnnt 8.779758 hw_loss 0.051251 lr 0.00040514 rank 1
2023-02-21 18:45:04,722 DEBUG TRAIN Batch 18/2200 loss 10.523037 loss_att 15.697731 loss_ctc 15.509445 loss_rnnt 8.775434 hw_loss 0.089641 lr 0.00040509 rank 2
2023-02-21 18:45:04,722 DEBUG TRAIN Batch 18/2200 loss 10.008066 loss_att 13.246607 loss_ctc 15.386902 loss_rnnt 8.617477 hw_loss 0.048191 lr 0.00040506 rank 7
2023-02-21 18:45:04,724 DEBUG TRAIN Batch 18/2200 loss 4.457522 loss_att 7.220793 loss_ctc 7.352847 loss_rnnt 3.479974 hw_loss 0.072846 lr 0.00040512 rank 4
2023-02-21 18:45:04,727 DEBUG TRAIN Batch 18/2200 loss 15.268378 loss_att 17.933655 loss_ctc 18.334574 loss_rnnt 14.268299 hw_loss 0.109123 lr 0.00040514 rank 0
2023-02-21 18:45:04,728 DEBUG TRAIN Batch 18/2200 loss 4.588557 loss_att 7.133138 loss_ctc 7.824085 loss_rnnt 3.585300 hw_loss 0.118006 lr 0.00040510 rank 5
2023-02-21 18:45:04,730 DEBUG TRAIN Batch 18/2200 loss 15.061316 loss_att 23.759466 loss_ctc 23.951517 loss_rnnt 12.114097 hw_loss 0.041681 lr 0.00040508 rank 3
2023-02-21 18:45:04,777 DEBUG TRAIN Batch 18/2200 loss 10.621703 loss_att 11.687184 loss_ctc 10.945426 loss_rnnt 10.317064 hw_loss 0.090711 lr 0.00040508 rank 6
2023-02-21 18:46:22,100 DEBUG TRAIN Batch 18/2300 loss 5.594902 loss_att 9.629951 loss_ctc 5.923349 loss_rnnt 4.650838 hw_loss 0.174864 lr 0.00040501 rank 1
2023-02-21 18:46:22,101 DEBUG TRAIN Batch 18/2300 loss 19.563349 loss_att 22.169176 loss_ctc 26.219553 loss_rnnt 18.079617 hw_loss 0.140761 lr 0.00040501 rank 0
2023-02-21 18:46:22,102 DEBUG TRAIN Batch 18/2300 loss 3.357754 loss_att 5.647658 loss_ctc 4.098340 loss_rnnt 2.758057 hw_loss 0.080571 lr 0.00040493 rank 7
2023-02-21 18:46:22,103 DEBUG TRAIN Batch 18/2300 loss 4.564882 loss_att 7.613487 loss_ctc 6.330646 loss_rnnt 3.650961 hw_loss 0.128936 lr 0.00040496 rank 2
2023-02-21 18:46:22,104 DEBUG TRAIN Batch 18/2300 loss 4.297731 loss_att 7.043768 loss_ctc 10.713951 loss_rnnt 2.783096 hw_loss 0.206123 lr 0.00040495 rank 6
2023-02-21 18:46:22,106 DEBUG TRAIN Batch 18/2300 loss 18.443373 loss_att 18.776907 loss_ctc 18.733368 loss_rnnt 18.264839 hw_loss 0.137177 lr 0.00040499 rank 4
2023-02-21 18:46:22,112 DEBUG TRAIN Batch 18/2300 loss 5.961521 loss_att 8.004677 loss_ctc 7.257912 loss_rnnt 5.338584 hw_loss 0.077727 lr 0.00040497 rank 5
2023-02-21 18:46:22,155 DEBUG TRAIN Batch 18/2300 loss 8.579021 loss_att 10.510033 loss_ctc 12.436153 loss_rnnt 7.634194 hw_loss 0.083137 lr 0.00040495 rank 3
2023-02-21 18:47:40,343 DEBUG TRAIN Batch 18/2400 loss 3.331202 loss_att 7.338801 loss_ctc 4.297569 loss_rnnt 2.365261 hw_loss 0.066699 lr 0.00040485 rank 4
2023-02-21 18:47:40,346 DEBUG TRAIN Batch 18/2400 loss 9.414247 loss_att 10.534388 loss_ctc 12.554335 loss_rnnt 8.722695 hw_loss 0.091585 lr 0.00040482 rank 2
2023-02-21 18:47:40,346 DEBUG TRAIN Batch 18/2400 loss 7.563418 loss_att 8.417221 loss_ctc 10.428452 loss_rnnt 6.893887 hw_loss 0.218937 lr 0.00040480 rank 7
2023-02-21 18:47:40,349 DEBUG TRAIN Batch 18/2400 loss 10.011859 loss_att 12.867191 loss_ctc 12.667663 loss_rnnt 8.984037 hw_loss 0.192466 lr 0.00040482 rank 6
2023-02-21 18:47:40,350 DEBUG TRAIN Batch 18/2400 loss 10.849867 loss_att 13.858995 loss_ctc 13.005806 loss_rnnt 9.899170 hw_loss 0.115146 lr 0.00040487 rank 0
2023-02-21 18:47:40,353 DEBUG TRAIN Batch 18/2400 loss 13.255595 loss_att 13.896821 loss_ctc 16.432379 loss_rnnt 12.596669 hw_loss 0.200829 lr 0.00040481 rank 3
2023-02-21 18:47:40,356 DEBUG TRAIN Batch 18/2400 loss 6.820823 loss_att 9.765415 loss_ctc 9.251735 loss_rnnt 5.888154 hw_loss 0.036804 lr 0.00040487 rank 1
2023-02-21 18:47:40,363 DEBUG TRAIN Batch 18/2400 loss 7.876333 loss_att 9.479643 loss_ctc 11.357224 loss_rnnt 7.001673 hw_loss 0.168524 lr 0.00040484 rank 5
2023-02-21 18:49:02,162 DEBUG TRAIN Batch 18/2500 loss 19.237221 loss_att 23.611668 loss_ctc 25.076956 loss_rnnt 17.545424 hw_loss 0.071770 lr 0.00040472 rank 4
2023-02-21 18:49:02,166 DEBUG TRAIN Batch 18/2500 loss 7.072867 loss_att 7.659029 loss_ctc 9.382347 loss_rnnt 6.557866 hw_loss 0.168445 lr 0.00040469 rank 2
2023-02-21 18:49:02,168 DEBUG TRAIN Batch 18/2500 loss 12.787801 loss_att 17.621710 loss_ctc 20.647141 loss_rnnt 10.758049 hw_loss 0.028232 lr 0.00040474 rank 1
2023-02-21 18:49:02,169 DEBUG TRAIN Batch 18/2500 loss 8.935117 loss_att 10.312994 loss_ctc 11.467189 loss_rnnt 8.242179 hw_loss 0.149536 lr 0.00040468 rank 6
2023-02-21 18:49:02,170 DEBUG TRAIN Batch 18/2500 loss 7.746571 loss_att 11.155155 loss_ctc 16.132679 loss_rnnt 5.910456 hw_loss 0.067968 lr 0.00040467 rank 7
2023-02-21 18:49:02,173 DEBUG TRAIN Batch 18/2500 loss 13.035911 loss_att 16.036312 loss_ctc 23.090294 loss_rnnt 11.044614 hw_loss 0.094933 lr 0.00040470 rank 5
2023-02-21 18:49:02,173 DEBUG TRAIN Batch 18/2500 loss 10.800434 loss_att 15.792843 loss_ctc 10.841650 loss_rnnt 9.743067 hw_loss 0.100108 lr 0.00040474 rank 0
2023-02-21 18:49:02,176 DEBUG TRAIN Batch 18/2500 loss 16.227982 loss_att 16.859606 loss_ctc 22.272274 loss_rnnt 15.259993 hw_loss 0.067049 lr 0.00040468 rank 3
2023-02-21 18:50:21,343 DEBUG TRAIN Batch 18/2600 loss 10.777584 loss_att 15.620594 loss_ctc 14.763461 loss_rnnt 9.230659 hw_loss 0.087887 lr 0.00040453 rank 7
2023-02-21 18:50:21,347 DEBUG TRAIN Batch 18/2600 loss 3.128991 loss_att 5.751258 loss_ctc 5.500551 loss_rnnt 2.251268 hw_loss 0.069489 lr 0.00040459 rank 4
2023-02-21 18:50:21,347 DEBUG TRAIN Batch 18/2600 loss 15.303701 loss_att 15.669762 loss_ctc 18.847630 loss_rnnt 14.697902 hw_loss 0.112620 lr 0.00040455 rank 6
2023-02-21 18:50:21,348 DEBUG TRAIN Batch 18/2600 loss 12.229243 loss_att 16.791462 loss_ctc 20.080921 loss_rnnt 10.194520 hw_loss 0.141353 lr 0.00040456 rank 2
2023-02-21 18:50:21,349 DEBUG TRAIN Batch 18/2600 loss 9.050428 loss_att 8.937928 loss_ctc 12.239594 loss_rnnt 8.600113 hw_loss 0.089239 lr 0.00040461 rank 1
2023-02-21 18:50:21,354 DEBUG TRAIN Batch 18/2600 loss 10.309684 loss_att 12.250985 loss_ctc 11.262679 loss_rnnt 9.690020 hw_loss 0.195635 lr 0.00040455 rank 3
2023-02-21 18:50:21,355 DEBUG TRAIN Batch 18/2600 loss 4.794365 loss_att 6.400913 loss_ctc 6.684232 loss_rnnt 4.078436 hw_loss 0.267446 lr 0.00040461 rank 0
2023-02-21 18:50:21,358 DEBUG TRAIN Batch 18/2600 loss 8.974667 loss_att 10.011546 loss_ctc 13.153440 loss_rnnt 8.102140 hw_loss 0.202464 lr 0.00040457 rank 5
2023-02-21 18:51:39,643 DEBUG TRAIN Batch 18/2700 loss 16.738789 loss_att 20.528175 loss_ctc 23.194548 loss_rnnt 15.082973 hw_loss 0.069695 lr 0.00040442 rank 6
2023-02-21 18:51:39,643 DEBUG TRAIN Batch 18/2700 loss 9.594528 loss_att 15.203406 loss_ctc 13.013544 loss_rnnt 7.956178 hw_loss 0.113823 lr 0.00040446 rank 4
2023-02-21 18:51:39,643 DEBUG TRAIN Batch 18/2700 loss 17.905693 loss_att 20.315044 loss_ctc 24.036457 loss_rnnt 16.451176 hw_loss 0.291023 lr 0.00040440 rank 7
2023-02-21 18:51:39,645 DEBUG TRAIN Batch 18/2700 loss 8.605149 loss_att 12.105757 loss_ctc 13.408604 loss_rnnt 7.222027 hw_loss 0.079764 lr 0.00040448 rank 0
2023-02-21 18:51:39,646 DEBUG TRAIN Batch 18/2700 loss 8.819155 loss_att 12.288584 loss_ctc 12.369006 loss_rnnt 7.440624 hw_loss 0.396244 lr 0.00040443 rank 2
2023-02-21 18:51:39,649 DEBUG TRAIN Batch 18/2700 loss 4.893823 loss_att 7.126978 loss_ctc 8.361647 loss_rnnt 3.907805 hw_loss 0.144393 lr 0.00040448 rank 1
2023-02-21 18:51:39,648 DEBUG TRAIN Batch 18/2700 loss 3.070638 loss_att 6.163796 loss_ctc 4.416919 loss_rnnt 2.225843 hw_loss 0.087487 lr 0.00040442 rank 3
2023-02-21 18:51:39,652 DEBUG TRAIN Batch 18/2700 loss 17.342312 loss_att 21.145027 loss_ctc 29.453390 loss_rnnt 14.875799 hw_loss 0.170922 lr 0.00040444 rank 5
2023-02-21 18:52:59,434 DEBUG TRAIN Batch 18/2800 loss 3.717420 loss_att 7.136018 loss_ctc 5.308802 loss_rnnt 2.789183 hw_loss 0.060625 lr 0.00040429 rank 2
2023-02-21 18:52:59,435 DEBUG TRAIN Batch 18/2800 loss 6.086387 loss_att 8.019339 loss_ctc 6.467264 loss_rnnt 5.585394 hw_loss 0.119286 lr 0.00040432 rank 4
2023-02-21 18:52:59,438 DEBUG TRAIN Batch 18/2800 loss 10.155820 loss_att 15.328091 loss_ctc 14.418193 loss_rnnt 8.484608 hw_loss 0.128328 lr 0.00040427 rank 7
2023-02-21 18:52:59,439 DEBUG TRAIN Batch 18/2800 loss 18.700401 loss_att 21.442036 loss_ctc 32.203835 loss_rnnt 16.204262 hw_loss 0.276290 lr 0.00040434 rank 1
2023-02-21 18:52:59,440 DEBUG TRAIN Batch 18/2800 loss 14.394222 loss_att 16.850813 loss_ctc 21.978451 loss_rnnt 12.805451 hw_loss 0.161664 lr 0.00040428 rank 3
2023-02-21 18:52:59,441 DEBUG TRAIN Batch 18/2800 loss 12.835492 loss_att 17.915310 loss_ctc 16.372952 loss_rnnt 11.272614 hw_loss 0.141100 lr 0.00040434 rank 0
2023-02-21 18:52:59,443 DEBUG TRAIN Batch 18/2800 loss 3.570745 loss_att 7.493064 loss_ctc 6.406099 loss_rnnt 2.293723 hw_loss 0.214707 lr 0.00040429 rank 6
2023-02-21 18:52:59,492 DEBUG TRAIN Batch 18/2800 loss 3.902425 loss_att 7.460210 loss_ctc 5.570539 loss_rnnt 2.867189 hw_loss 0.189869 lr 0.00040431 rank 5
2023-02-21 18:54:18,100 DEBUG TRAIN Batch 18/2900 loss 11.767570 loss_att 14.007910 loss_ctc 15.591597 loss_rnnt 10.741888 hw_loss 0.127021 lr 0.00040414 rank 7
2023-02-21 18:54:18,103 DEBUG TRAIN Batch 18/2900 loss 9.775986 loss_att 10.456202 loss_ctc 11.087439 loss_rnnt 9.399200 hw_loss 0.123530 lr 0.00040416 rank 2
2023-02-21 18:54:18,104 DEBUG TRAIN Batch 18/2900 loss 5.290813 loss_att 8.492680 loss_ctc 6.693663 loss_rnnt 4.429462 hw_loss 0.063622 lr 0.00040415 rank 6
2023-02-21 18:54:18,106 DEBUG TRAIN Batch 18/2900 loss 16.348484 loss_att 19.041481 loss_ctc 19.639101 loss_rnnt 15.329734 hw_loss 0.077628 lr 0.00040419 rank 4
2023-02-21 18:54:18,106 DEBUG TRAIN Batch 18/2900 loss 10.647280 loss_att 11.942709 loss_ctc 13.360064 loss_rnnt 9.911735 hw_loss 0.215165 lr 0.00040421 rank 1
2023-02-21 18:54:18,107 DEBUG TRAIN Batch 18/2900 loss 10.506830 loss_att 14.208193 loss_ctc 14.736382 loss_rnnt 9.177145 hw_loss 0.047761 lr 0.00040415 rank 3
2023-02-21 18:54:18,108 DEBUG TRAIN Batch 18/2900 loss 5.992094 loss_att 9.288243 loss_ctc 7.705422 loss_rnnt 5.021955 hw_loss 0.154623 lr 0.00040421 rank 0
2023-02-21 18:54:18,111 DEBUG TRAIN Batch 18/2900 loss 7.609349 loss_att 10.991024 loss_ctc 10.571381 loss_rnnt 6.471770 hw_loss 0.124326 lr 0.00040417 rank 5
2023-02-21 18:55:36,197 DEBUG TRAIN Batch 18/3000 loss 12.485720 loss_att 15.987982 loss_ctc 18.719563 loss_rnnt 10.890056 hw_loss 0.120063 lr 0.00040408 rank 0
2023-02-21 18:55:36,201 DEBUG TRAIN Batch 18/3000 loss 8.548550 loss_att 10.139359 loss_ctc 12.762199 loss_rnnt 7.615872 hw_loss 0.098803 lr 0.00040408 rank 1
2023-02-21 18:55:36,201 DEBUG TRAIN Batch 18/3000 loss 8.420974 loss_att 11.681595 loss_ctc 10.301742 loss_rnnt 7.431283 hw_loss 0.162744 lr 0.00040400 rank 7
2023-02-21 18:55:36,202 DEBUG TRAIN Batch 18/3000 loss 9.699182 loss_att 12.513768 loss_ctc 13.514834 loss_rnnt 8.550541 hw_loss 0.144316 lr 0.00040403 rank 2
2023-02-21 18:55:36,203 DEBUG TRAIN Batch 18/3000 loss 7.633604 loss_att 11.163232 loss_ctc 12.180761 loss_rnnt 6.261613 hw_loss 0.112083 lr 0.00040402 rank 3
2023-02-21 18:55:36,204 DEBUG TRAIN Batch 18/3000 loss 7.555617 loss_att 10.503909 loss_ctc 11.550278 loss_rnnt 6.349970 hw_loss 0.156313 lr 0.00040406 rank 4
2023-02-21 18:55:36,206 DEBUG TRAIN Batch 18/3000 loss 5.927711 loss_att 7.048123 loss_ctc 7.829227 loss_rnnt 5.384195 hw_loss 0.123559 lr 0.00040402 rank 6
2023-02-21 18:55:36,211 DEBUG TRAIN Batch 18/3000 loss 10.845420 loss_att 11.734294 loss_ctc 12.252777 loss_rnnt 10.415625 hw_loss 0.120700 lr 0.00040404 rank 5
2023-02-21 18:56:56,265 DEBUG TRAIN Batch 18/3100 loss 4.394833 loss_att 6.691357 loss_ctc 6.780423 loss_rnnt 3.554050 hw_loss 0.118873 lr 0.00040393 rank 4
2023-02-21 18:56:56,268 DEBUG TRAIN Batch 18/3100 loss 17.130457 loss_att 16.659256 loss_ctc 20.474022 loss_rnnt 16.700521 hw_loss 0.146938 lr 0.00040390 rank 2
2023-02-21 18:56:56,269 DEBUG TRAIN Batch 18/3100 loss 11.406379 loss_att 11.653430 loss_ctc 13.811432 loss_rnnt 10.910262 hw_loss 0.236311 lr 0.00040395 rank 1
2023-02-21 18:56:56,270 DEBUG TRAIN Batch 18/3100 loss 3.711071 loss_att 5.078002 loss_ctc 3.991368 loss_rnnt 3.345505 hw_loss 0.102762 lr 0.00040389 rank 6
2023-02-21 18:56:56,271 DEBUG TRAIN Batch 18/3100 loss 3.747015 loss_att 5.248590 loss_ctc 5.182099 loss_rnnt 3.188737 hw_loss 0.124911 lr 0.00040395 rank 0
2023-02-21 18:56:56,272 DEBUG TRAIN Batch 18/3100 loss 6.483315 loss_att 7.867970 loss_ctc 8.294298 loss_rnnt 5.891033 hw_loss 0.138540 lr 0.00040389 rank 3
2023-02-21 18:56:56,272 DEBUG TRAIN Batch 18/3100 loss 8.375537 loss_att 9.158167 loss_ctc 9.854847 loss_rnnt 7.804404 hw_loss 0.407561 lr 0.00040387 rank 7
2023-02-21 18:56:56,280 DEBUG TRAIN Batch 18/3100 loss 11.457294 loss_att 12.125859 loss_ctc 13.609504 loss_rnnt 10.985622 hw_loss 0.095619 lr 0.00040391 rank 5
2023-02-21 18:58:17,199 DEBUG TRAIN Batch 18/3200 loss 6.949625 loss_att 8.830744 loss_ctc 9.825863 loss_rnnt 6.111139 hw_loss 0.147681 lr 0.00040382 rank 1
2023-02-21 18:58:17,202 DEBUG TRAIN Batch 18/3200 loss 14.499219 loss_att 16.271885 loss_ctc 16.937885 loss_rnnt 13.806384 hw_loss 0.024651 lr 0.00040380 rank 4
2023-02-21 18:58:17,208 DEBUG TRAIN Batch 18/3200 loss 5.126775 loss_att 6.342613 loss_ctc 6.369323 loss_rnnt 4.647533 hw_loss 0.132002 lr 0.00040376 rank 6
2023-02-21 18:58:17,208 DEBUG TRAIN Batch 18/3200 loss 6.945869 loss_att 7.958862 loss_ctc 8.002181 loss_rnnt 6.507384 hw_loss 0.178209 lr 0.00040382 rank 0
2023-02-21 18:58:17,211 DEBUG TRAIN Batch 18/3200 loss 3.641002 loss_att 6.437527 loss_ctc 4.949227 loss_rnnt 2.789326 hw_loss 0.221138 lr 0.00040376 rank 3
2023-02-21 18:58:17,213 DEBUG TRAIN Batch 18/3200 loss 13.427959 loss_att 15.530477 loss_ctc 18.273085 loss_rnnt 12.330788 hw_loss 0.057471 lr 0.00040378 rank 5
2023-02-21 18:58:17,221 DEBUG TRAIN Batch 18/3200 loss 8.562440 loss_att 10.224255 loss_ctc 10.876099 loss_rnnt 7.873285 hw_loss 0.090570 lr 0.00040377 rank 2
2023-02-21 18:58:17,223 DEBUG TRAIN Batch 18/3200 loss 7.750800 loss_att 11.688417 loss_ctc 10.925943 loss_rnnt 6.498571 hw_loss 0.077538 lr 0.00040374 rank 7
2023-02-21 18:59:35,830 DEBUG TRAIN Batch 18/3300 loss 8.781701 loss_att 10.561674 loss_ctc 12.743977 loss_rnnt 7.819002 hw_loss 0.147001 lr 0.00040368 rank 1
2023-02-21 18:59:35,834 DEBUG TRAIN Batch 18/3300 loss 4.565338 loss_att 8.353779 loss_ctc 9.588118 loss_rnnt 3.123628 hw_loss 0.026845 lr 0.00040363 rank 6
2023-02-21 18:59:35,834 DEBUG TRAIN Batch 18/3300 loss 10.308704 loss_att 12.740908 loss_ctc 15.631969 loss_rnnt 9.009933 hw_loss 0.192303 lr 0.00040361 rank 7
2023-02-21 18:59:35,835 DEBUG TRAIN Batch 18/3300 loss 9.377674 loss_att 12.390097 loss_ctc 12.947235 loss_rnnt 8.229213 hw_loss 0.131315 lr 0.00040364 rank 2
2023-02-21 18:59:35,835 DEBUG TRAIN Batch 18/3300 loss 6.789982 loss_att 10.811018 loss_ctc 9.672910 loss_rnnt 5.556508 hw_loss 0.084143 lr 0.00040363 rank 3
2023-02-21 18:59:35,837 DEBUG TRAIN Batch 18/3300 loss 4.072428 loss_att 8.536276 loss_ctc 7.183308 loss_rnnt 2.725726 hw_loss 0.073403 lr 0.00040365 rank 5
2023-02-21 18:59:35,837 DEBUG TRAIN Batch 18/3300 loss 5.580849 loss_att 10.269161 loss_ctc 8.011293 loss_rnnt 4.274302 hw_loss 0.084047 lr 0.00040366 rank 4
2023-02-21 18:59:35,839 DEBUG TRAIN Batch 18/3300 loss 6.888630 loss_att 9.387083 loss_ctc 13.462855 loss_rnnt 5.348355 hw_loss 0.307538 lr 0.00040368 rank 0
2023-02-21 19:00:54,375 DEBUG TRAIN Batch 18/3400 loss 22.613523 loss_att 29.199951 loss_ctc 26.398649 loss_rnnt 20.738337 hw_loss 0.099776 lr 0.00040350 rank 2
2023-02-21 19:00:54,376 DEBUG TRAIN Batch 18/3400 loss 17.967430 loss_att 23.626373 loss_ctc 22.928162 loss_rnnt 16.076180 hw_loss 0.183808 lr 0.00040355 rank 1
2023-02-21 19:00:54,379 DEBUG TRAIN Batch 18/3400 loss 10.987284 loss_att 13.985191 loss_ctc 13.823924 loss_rnnt 9.957321 hw_loss 0.097803 lr 0.00040349 rank 6
2023-02-21 19:00:54,380 DEBUG TRAIN Batch 18/3400 loss 10.471416 loss_att 13.157901 loss_ctc 12.487171 loss_rnnt 9.568617 hw_loss 0.181376 lr 0.00040348 rank 7
2023-02-21 19:00:54,381 DEBUG TRAIN Batch 18/3400 loss 12.702887 loss_att 15.735727 loss_ctc 11.468627 loss_rnnt 12.161488 hw_loss 0.186372 lr 0.00040353 rank 4
2023-02-21 19:00:54,382 DEBUG TRAIN Batch 18/3400 loss 10.759158 loss_att 14.742816 loss_ctc 14.993380 loss_rnnt 9.376068 hw_loss 0.040867 lr 0.00040355 rank 0
2023-02-21 19:00:54,395 DEBUG TRAIN Batch 18/3400 loss 5.094628 loss_att 8.882134 loss_ctc 8.264609 loss_rnnt 3.870609 hw_loss 0.082226 lr 0.00040352 rank 5
2023-02-21 19:00:54,426 DEBUG TRAIN Batch 18/3400 loss 12.834070 loss_att 16.907917 loss_ctc 22.648466 loss_rnnt 10.651936 hw_loss 0.110212 lr 0.00040349 rank 3
2023-02-21 19:02:15,211 DEBUG TRAIN Batch 18/3500 loss 6.202326 loss_att 10.867622 loss_ctc 8.624136 loss_rnnt 4.902889 hw_loss 0.081507 lr 0.00040336 rank 3
2023-02-21 19:02:15,213 DEBUG TRAIN Batch 18/3500 loss 11.781139 loss_att 13.134738 loss_ctc 14.722578 loss_rnnt 11.041094 hw_loss 0.144624 lr 0.00040335 rank 7
2023-02-21 19:02:15,213 DEBUG TRAIN Batch 18/3500 loss 12.488336 loss_att 14.752209 loss_ctc 15.224194 loss_rnnt 11.616867 hw_loss 0.101087 lr 0.00040342 rank 1
2023-02-21 19:02:15,214 DEBUG TRAIN Batch 18/3500 loss 10.483120 loss_att 13.588190 loss_ctc 16.882423 loss_rnnt 8.933552 hw_loss 0.141216 lr 0.00040342 rank 0
2023-02-21 19:02:15,216 DEBUG TRAIN Batch 18/3500 loss 6.956827 loss_att 11.263213 loss_ctc 12.936561 loss_rnnt 5.260551 hw_loss 0.070688 lr 0.00040336 rank 6
2023-02-21 19:02:15,220 DEBUG TRAIN Batch 18/3500 loss 18.261942 loss_att 20.932457 loss_ctc 23.444494 loss_rnnt 17.020363 hw_loss 0.030876 lr 0.00040338 rank 5
2023-02-21 19:02:15,244 DEBUG TRAIN Batch 18/3500 loss 8.754483 loss_att 11.313759 loss_ctc 12.354574 loss_rnnt 7.714263 hw_loss 0.090663 lr 0.00040337 rank 2
2023-02-21 19:02:15,252 DEBUG TRAIN Batch 18/3500 loss 8.241043 loss_att 11.153565 loss_ctc 9.842000 loss_rnnt 7.429904 hw_loss 0.028449 lr 0.00040340 rank 4
2023-02-21 19:03:36,005 DEBUG TRAIN Batch 18/3600 loss 6.018094 loss_att 9.589767 loss_ctc 9.212523 loss_rnnt 4.823484 hw_loss 0.101910 lr 0.00040329 rank 1
2023-02-21 19:03:36,008 DEBUG TRAIN Batch 18/3600 loss 10.276706 loss_att 12.277868 loss_ctc 13.667516 loss_rnnt 9.407699 hw_loss 0.031249 lr 0.00040323 rank 6
2023-02-21 19:03:36,009 DEBUG TRAIN Batch 18/3600 loss 11.209470 loss_att 12.219403 loss_ctc 18.240240 loss_rnnt 10.015053 hw_loss 0.103112 lr 0.00040327 rank 4
2023-02-21 19:03:36,009 DEBUG TRAIN Batch 18/3600 loss 13.454252 loss_att 15.009001 loss_ctc 18.291325 loss_rnnt 12.456024 hw_loss 0.079378 lr 0.00040322 rank 7
2023-02-21 19:03:36,010 DEBUG TRAIN Batch 18/3600 loss 28.217070 loss_att 28.802799 loss_ctc 32.723846 loss_rnnt 27.464664 hw_loss 0.064416 lr 0.00040324 rank 2
2023-02-21 19:03:36,012 DEBUG TRAIN Batch 18/3600 loss 3.899529 loss_att 6.991263 loss_ctc 5.973722 loss_rnnt 2.899576 hw_loss 0.196964 lr 0.00040329 rank 0
2023-02-21 19:03:36,014 DEBUG TRAIN Batch 18/3600 loss 9.269943 loss_att 11.113843 loss_ctc 12.513582 loss_rnnt 8.408581 hw_loss 0.112683 lr 0.00040323 rank 3
2023-02-21 19:03:36,016 DEBUG TRAIN Batch 18/3600 loss 24.531616 loss_att 28.031769 loss_ctc 33.779045 loss_rnnt 22.482550 hw_loss 0.217584 lr 0.00040325 rank 5
2023-02-21 19:04:55,179 DEBUG TRAIN Batch 18/3700 loss 15.195624 loss_att 17.873316 loss_ctc 23.575218 loss_rnnt 13.496989 hw_loss 0.085910 lr 0.00040316 rank 1
2023-02-21 19:04:55,179 DEBUG TRAIN Batch 18/3700 loss 7.646487 loss_att 10.200010 loss_ctc 11.142103 loss_rnnt 6.625415 hw_loss 0.083036 lr 0.00040314 rank 4
2023-02-21 19:04:55,181 DEBUG TRAIN Batch 18/3700 loss 16.086128 loss_att 18.610188 loss_ctc 22.253927 loss_rnnt 14.651966 hw_loss 0.200581 lr 0.00040308 rank 7
2023-02-21 19:04:55,181 DEBUG TRAIN Batch 18/3700 loss 8.595518 loss_att 11.910030 loss_ctc 10.365180 loss_rnnt 7.636045 hw_loss 0.113654 lr 0.00040311 rank 2
2023-02-21 19:04:55,182 DEBUG TRAIN Batch 18/3700 loss 5.502539 loss_att 8.018507 loss_ctc 9.341312 loss_rnnt 4.368938 hw_loss 0.222319 lr 0.00040310 rank 6
2023-02-21 19:04:55,183 DEBUG TRAIN Batch 18/3700 loss 5.345729 loss_att 6.453048 loss_ctc 8.070822 loss_rnnt 4.667209 hw_loss 0.175707 lr 0.00040316 rank 0
2023-02-21 19:04:55,186 DEBUG TRAIN Batch 18/3700 loss 7.349473 loss_att 9.695347 loss_ctc 8.230928 loss_rnnt 6.714220 hw_loss 0.091033 lr 0.00040310 rank 3
2023-02-21 19:04:55,186 DEBUG TRAIN Batch 18/3700 loss 11.062397 loss_att 12.358152 loss_ctc 16.337994 loss_rnnt 10.043101 hw_loss 0.106373 lr 0.00040312 rank 5
2023-02-21 19:06:12,468 DEBUG TRAIN Batch 18/3800 loss 14.521832 loss_att 16.594280 loss_ctc 22.171621 loss_rnnt 13.010080 hw_loss 0.144918 lr 0.00040303 rank 1
2023-02-21 19:06:12,468 DEBUG TRAIN Batch 18/3800 loss 8.875090 loss_att 10.007414 loss_ctc 7.964063 loss_rnnt 8.676604 hw_loss 0.175296 lr 0.00040295 rank 7
2023-02-21 19:06:12,469 DEBUG TRAIN Batch 18/3800 loss 10.166846 loss_att 12.469342 loss_ctc 15.509680 loss_rnnt 8.980658 hw_loss 0.024960 lr 0.00040298 rank 2
2023-02-21 19:06:12,472 DEBUG TRAIN Batch 18/3800 loss 10.588222 loss_att 14.551401 loss_ctc 15.284036 loss_rnnt 9.058914 hw_loss 0.207307 lr 0.00040301 rank 4
2023-02-21 19:06:12,472 DEBUG TRAIN Batch 18/3800 loss 6.602079 loss_att 7.463184 loss_ctc 7.844489 loss_rnnt 6.137050 hw_loss 0.238413 lr 0.00040297 rank 6
2023-02-21 19:06:12,474 DEBUG TRAIN Batch 18/3800 loss 14.349201 loss_att 15.135713 loss_ctc 18.927301 loss_rnnt 13.507660 hw_loss 0.138424 lr 0.00040297 rank 3
2023-02-21 19:06:12,477 DEBUG TRAIN Batch 18/3800 loss 8.402183 loss_att 10.015894 loss_ctc 9.798241 loss_rnnt 7.834607 hw_loss 0.110047 lr 0.00040299 rank 5
2023-02-21 19:06:12,513 DEBUG TRAIN Batch 18/3800 loss 5.346157 loss_att 7.767871 loss_ctc 7.207479 loss_rnnt 4.540913 hw_loss 0.136359 lr 0.00040303 rank 0
2023-02-21 19:07:34,641 DEBUG TRAIN Batch 18/3900 loss 14.785085 loss_att 24.312391 loss_ctc 26.089476 loss_rnnt 11.357946 hw_loss 0.027045 lr 0.00040288 rank 4
2023-02-21 19:07:34,642 DEBUG TRAIN Batch 18/3900 loss 14.395750 loss_att 21.706856 loss_ctc 23.183064 loss_rnnt 11.747279 hw_loss 0.027391 lr 0.00040282 rank 7
2023-02-21 19:07:34,645 DEBUG TRAIN Batch 18/3900 loss 11.630241 loss_att 16.949755 loss_ctc 15.401294 loss_rnnt 10.027428 hw_loss 0.067695 lr 0.00040285 rank 2
2023-02-21 19:07:34,649 DEBUG TRAIN Batch 18/3900 loss 7.675417 loss_att 11.626140 loss_ctc 14.423376 loss_rnnt 5.940670 hw_loss 0.084142 lr 0.00040284 rank 6
2023-02-21 19:07:34,651 DEBUG TRAIN Batch 18/3900 loss 12.481572 loss_att 18.053276 loss_ctc 15.081068 loss_rnnt 11.006254 hw_loss 0.026957 lr 0.00040290 rank 1
2023-02-21 19:07:34,651 DEBUG TRAIN Batch 18/3900 loss 3.079933 loss_att 5.784104 loss_ctc 5.065701 loss_rnnt 2.232634 hw_loss 0.078179 lr 0.00040290 rank 0
2023-02-21 19:07:34,654 DEBUG TRAIN Batch 18/3900 loss 6.856807 loss_att 11.531822 loss_ctc 11.623878 loss_rnnt 5.239753 hw_loss 0.087078 lr 0.00040284 rank 3
2023-02-21 19:07:34,680 DEBUG TRAIN Batch 18/3900 loss 15.031812 loss_att 20.962095 loss_ctc 29.758057 loss_rnnt 11.825502 hw_loss 0.106412 lr 0.00040286 rank 5
2023-02-21 19:08:51,912 DEBUG TRAIN Batch 18/4000 loss 10.665245 loss_att 15.337582 loss_ctc 20.219057 loss_rnnt 8.423517 hw_loss 0.062662 lr 0.00040272 rank 2
2023-02-21 19:08:51,917 DEBUG TRAIN Batch 18/4000 loss 11.578755 loss_att 14.249353 loss_ctc 17.651752 loss_rnnt 10.147438 hw_loss 0.163995 lr 0.00040275 rank 4
2023-02-21 19:08:51,918 DEBUG TRAIN Batch 18/4000 loss 10.590891 loss_att 12.567456 loss_ctc 15.806471 loss_rnnt 9.427299 hw_loss 0.136628 lr 0.00040271 rank 3
2023-02-21 19:08:51,919 DEBUG TRAIN Batch 18/4000 loss 4.155516 loss_att 8.732058 loss_ctc 5.681005 loss_rnnt 2.928171 hw_loss 0.203696 lr 0.00040269 rank 7
2023-02-21 19:08:51,920 DEBUG TRAIN Batch 18/4000 loss 7.714743 loss_att 9.225204 loss_ctc 8.020882 loss_rnnt 7.347480 hw_loss 0.045659 lr 0.00040271 rank 6
2023-02-21 19:08:51,921 DEBUG TRAIN Batch 18/4000 loss 5.224292 loss_att 6.961198 loss_ctc 4.837151 loss_rnnt 4.857933 hw_loss 0.132369 lr 0.00040273 rank 5
2023-02-21 19:08:51,922 DEBUG TRAIN Batch 18/4000 loss 10.536456 loss_att 13.851448 loss_ctc 17.009558 loss_rnnt 8.947294 hw_loss 0.118282 lr 0.00040277 rank 0
2023-02-21 19:08:51,925 DEBUG TRAIN Batch 18/4000 loss 10.807284 loss_att 12.054971 loss_ctc 13.124001 loss_rnnt 10.211626 hw_loss 0.069797 lr 0.00040277 rank 1
2023-02-21 19:10:08,732 DEBUG TRAIN Batch 18/4100 loss 13.605010 loss_att 16.879183 loss_ctc 17.739223 loss_rnnt 12.302217 hw_loss 0.181370 lr 0.00040259 rank 2
2023-02-21 19:10:08,734 DEBUG TRAIN Batch 18/4100 loss 7.359213 loss_att 9.242188 loss_ctc 11.522905 loss_rnnt 6.367671 hw_loss 0.112103 lr 0.00040256 rank 7
2023-02-21 19:10:08,735 DEBUG TRAIN Batch 18/4100 loss 5.663979 loss_att 7.703366 loss_ctc 7.188212 loss_rnnt 4.950189 hw_loss 0.192529 lr 0.00040258 rank 3
2023-02-21 19:10:08,736 DEBUG TRAIN Batch 18/4100 loss 3.125892 loss_att 8.516495 loss_ctc 6.341022 loss_rnnt 1.548621 hw_loss 0.132125 lr 0.00040262 rank 4
2023-02-21 19:10:08,737 DEBUG TRAIN Batch 18/4100 loss 14.574760 loss_att 17.842257 loss_ctc 21.211689 loss_rnnt 12.954660 hw_loss 0.153142 lr 0.00040258 rank 6
2023-02-21 19:10:08,739 DEBUG TRAIN Batch 18/4100 loss 7.912112 loss_att 9.811925 loss_ctc 9.779735 loss_rnnt 7.215436 hw_loss 0.126931 lr 0.00040264 rank 1
2023-02-21 19:10:08,744 DEBUG TRAIN Batch 18/4100 loss 2.970823 loss_att 6.928750 loss_ctc 5.203087 loss_rnnt 1.783066 hw_loss 0.184756 lr 0.00040260 rank 5
2023-02-21 19:10:08,786 DEBUG TRAIN Batch 18/4100 loss 14.264570 loss_att 18.229996 loss_ctc 19.428848 loss_rnnt 12.707588 hw_loss 0.141240 lr 0.00040264 rank 0
2023-02-21 19:11:28,885 DEBUG TRAIN Batch 18/4200 loss 7.950607 loss_att 14.003960 loss_ctc 11.981241 loss_rnnt 6.135598 hw_loss 0.125476 lr 0.00040246 rank 2
2023-02-21 19:11:28,892 DEBUG TRAIN Batch 18/4200 loss 9.503412 loss_att 12.216126 loss_ctc 17.369474 loss_rnnt 7.859494 hw_loss 0.098562 lr 0.00040251 rank 0
2023-02-21 19:11:28,892 DEBUG TRAIN Batch 18/4200 loss 13.102691 loss_att 13.988157 loss_ctc 18.711670 loss_rnnt 12.088759 hw_loss 0.166827 lr 0.00040245 rank 3
2023-02-21 19:11:28,895 DEBUG TRAIN Batch 18/4200 loss 7.065315 loss_att 9.958300 loss_ctc 9.398924 loss_rnnt 6.121421 hw_loss 0.101531 lr 0.00040251 rank 1
2023-02-21 19:11:28,897 DEBUG TRAIN Batch 18/4200 loss 17.592409 loss_att 19.910053 loss_ctc 24.900368 loss_rnnt 16.077785 hw_loss 0.143812 lr 0.00040243 rank 7
2023-02-21 19:11:28,898 DEBUG TRAIN Batch 18/4200 loss 14.194543 loss_att 19.698587 loss_ctc 17.648315 loss_rnnt 12.593539 hw_loss 0.074422 lr 0.00040245 rank 6
2023-02-21 19:11:28,901 DEBUG TRAIN Batch 18/4200 loss 5.183458 loss_att 9.139770 loss_ctc 8.009345 loss_rnnt 3.982473 hw_loss 0.061759 lr 0.00040249 rank 4
2023-02-21 19:11:28,942 DEBUG TRAIN Batch 18/4200 loss 12.112636 loss_att 14.228411 loss_ctc 16.667095 loss_rnnt 11.014364 hw_loss 0.127227 lr 0.00040247 rank 5
2023-02-21 19:12:49,832 DEBUG TRAIN Batch 18/4300 loss 12.328005 loss_att 16.782009 loss_ctc 15.164557 loss_rnnt 11.025576 hw_loss 0.062662 lr 0.00040233 rank 2
2023-02-21 19:12:49,838 DEBUG TRAIN Batch 18/4300 loss 12.401881 loss_att 14.423628 loss_ctc 15.936157 loss_rnnt 11.438107 hw_loss 0.165354 lr 0.00040230 rank 7
2023-02-21 19:12:49,840 DEBUG TRAIN Batch 18/4300 loss 14.481503 loss_att 13.879685 loss_ctc 16.908783 loss_rnnt 14.220379 hw_loss 0.108471 lr 0.00040237 rank 1
2023-02-21 19:12:49,841 DEBUG TRAIN Batch 18/4300 loss 10.561213 loss_att 11.176057 loss_ctc 12.758167 loss_rnnt 10.050430 hw_loss 0.177913 lr 0.00040237 rank 0
2023-02-21 19:12:49,841 DEBUG TRAIN Batch 18/4300 loss 8.149327 loss_att 10.897685 loss_ctc 8.320197 loss_rnnt 7.535396 hw_loss 0.077769 lr 0.00040236 rank 4
2023-02-21 19:12:49,844 DEBUG TRAIN Batch 18/4300 loss 16.514275 loss_att 18.343599 loss_ctc 21.282377 loss_rnnt 15.487166 hw_loss 0.047810 lr 0.00040232 rank 6
2023-02-21 19:12:49,846 DEBUG TRAIN Batch 18/4300 loss 9.404383 loss_att 13.217722 loss_ctc 10.467381 loss_rnnt 8.407018 hw_loss 0.174308 lr 0.00040232 rank 3
2023-02-21 19:12:49,847 DEBUG TRAIN Batch 18/4300 loss 5.187072 loss_att 8.325096 loss_ctc 8.440029 loss_rnnt 4.078112 hw_loss 0.089301 lr 0.00040234 rank 5
2023-02-21 19:14:07,935 DEBUG TRAIN Batch 18/4400 loss 13.733395 loss_att 17.133303 loss_ctc 16.018421 loss_rnnt 12.678432 hw_loss 0.131835 lr 0.00040223 rank 4
2023-02-21 19:14:07,938 DEBUG TRAIN Batch 18/4400 loss 9.739342 loss_att 10.606009 loss_ctc 14.529521 loss_rnnt 8.833580 hw_loss 0.175757 lr 0.00040219 rank 6
2023-02-21 19:14:07,939 DEBUG TRAIN Batch 18/4400 loss 16.975533 loss_att 16.099384 loss_ctc 25.624577 loss_rnnt 15.880866 hw_loss 0.218790 lr 0.00040219 rank 3
2023-02-21 19:14:07,941 DEBUG TRAIN Batch 18/4400 loss 9.012322 loss_att 11.714209 loss_ctc 11.553670 loss_rnnt 8.075462 hw_loss 0.108067 lr 0.00040221 rank 5
2023-02-21 19:14:07,941 DEBUG TRAIN Batch 18/4400 loss 3.611723 loss_att 8.302570 loss_ctc 5.783262 loss_rnnt 2.320289 hw_loss 0.119487 lr 0.00040217 rank 7
2023-02-21 19:14:07,942 DEBUG TRAIN Batch 18/4400 loss 6.623591 loss_att 10.218584 loss_ctc 8.275667 loss_rnnt 5.668664 hw_loss 0.029348 lr 0.00040220 rank 2
2023-02-21 19:14:07,943 DEBUG TRAIN Batch 18/4400 loss 6.535758 loss_att 10.230669 loss_ctc 8.624981 loss_rnnt 5.455923 hw_loss 0.116793 lr 0.00040224 rank 0
2023-02-21 19:14:07,945 DEBUG TRAIN Batch 18/4400 loss 10.484586 loss_att 13.092398 loss_ctc 15.866796 loss_rnnt 9.122611 hw_loss 0.230222 lr 0.00040224 rank 1
2023-02-21 19:15:26,765 DEBUG TRAIN Batch 18/4500 loss 7.433022 loss_att 10.177616 loss_ctc 8.908476 loss_rnnt 6.674176 hw_loss 0.024750 lr 0.00040207 rank 2
2023-02-21 19:15:26,767 DEBUG TRAIN Batch 18/4500 loss 11.259578 loss_att 16.602581 loss_ctc 16.905672 loss_rnnt 9.424777 hw_loss 0.025103 lr 0.00040204 rank 7
2023-02-21 19:15:26,769 DEBUG TRAIN Batch 18/4500 loss 11.897480 loss_att 18.954468 loss_ctc 14.776500 loss_rnnt 10.010737 hw_loss 0.171518 lr 0.00040211 rank 1
2023-02-21 19:15:26,770 DEBUG TRAIN Batch 18/4500 loss 9.593279 loss_att 14.220654 loss_ctc 17.682396 loss_rnnt 7.523635 hw_loss 0.123037 lr 0.00040206 rank 3
2023-02-21 19:15:26,770 DEBUG TRAIN Batch 18/4500 loss 9.344655 loss_att 13.151055 loss_ctc 17.331028 loss_rnnt 7.401845 hw_loss 0.218776 lr 0.00040206 rank 6
2023-02-21 19:15:26,771 DEBUG TRAIN Batch 18/4500 loss 20.394066 loss_att 23.808750 loss_ctc 23.707693 loss_rnnt 19.226774 hw_loss 0.079762 lr 0.00040210 rank 4
2023-02-21 19:15:26,773 DEBUG TRAIN Batch 18/4500 loss 12.953030 loss_att 12.749495 loss_ctc 13.960632 loss_rnnt 12.744391 hw_loss 0.215624 lr 0.00040211 rank 0
2023-02-21 19:15:26,776 DEBUG TRAIN Batch 18/4500 loss 7.677282 loss_att 8.609366 loss_ctc 9.945602 loss_rnnt 7.095634 hw_loss 0.173981 lr 0.00040208 rank 5
2023-02-21 19:16:47,137 DEBUG TRAIN Batch 18/4600 loss 4.070228 loss_att 6.828387 loss_ctc 6.521738 loss_rnnt 3.016502 hw_loss 0.328550 lr 0.00040197 rank 4
2023-02-21 19:16:47,140 DEBUG TRAIN Batch 18/4600 loss 10.935817 loss_att 16.333370 loss_ctc 12.520367 loss_rnnt 9.597109 hw_loss 0.089857 lr 0.00040198 rank 1
2023-02-21 19:16:47,143 DEBUG TRAIN Batch 18/4600 loss 2.023234 loss_att 4.941305 loss_ctc 2.739045 loss_rnnt 1.317678 hw_loss 0.049689 lr 0.00040198 rank 0
2023-02-21 19:16:47,144 DEBUG TRAIN Batch 18/4600 loss 11.166144 loss_att 12.924152 loss_ctc 14.491306 loss_rnnt 10.319258 hw_loss 0.097368 lr 0.00040194 rank 2
2023-02-21 19:16:47,146 DEBUG TRAIN Batch 18/4600 loss 6.882526 loss_att 8.918067 loss_ctc 10.870684 loss_rnnt 5.877855 hw_loss 0.123391 lr 0.00040191 rank 7
2023-02-21 19:16:47,148 DEBUG TRAIN Batch 18/4600 loss 10.117075 loss_att 15.400918 loss_ctc 14.344379 loss_rnnt 8.379042 hw_loss 0.220543 lr 0.00040193 rank 6
2023-02-21 19:16:47,149 DEBUG TRAIN Batch 18/4600 loss 8.508653 loss_att 11.633513 loss_ctc 13.026680 loss_rnnt 7.198575 hw_loss 0.155065 lr 0.00040195 rank 5
2023-02-21 19:16:47,194 DEBUG TRAIN Batch 18/4600 loss 8.819113 loss_att 11.661100 loss_ctc 12.529961 loss_rnnt 7.596574 hw_loss 0.298802 lr 0.00040193 rank 3
2023-02-21 19:18:06,007 DEBUG TRAIN Batch 18/4700 loss 13.292558 loss_att 15.401152 loss_ctc 15.119488 loss_rnnt 12.583765 hw_loss 0.081530 lr 0.00040185 rank 0
2023-02-21 19:18:06,010 DEBUG TRAIN Batch 18/4700 loss 7.228348 loss_att 12.904302 loss_ctc 10.648589 loss_rnnt 5.622859 hw_loss 0.026749 lr 0.00040184 rank 4
2023-02-21 19:18:06,011 DEBUG TRAIN Batch 18/4700 loss 3.852861 loss_att 6.467777 loss_ctc 7.026326 loss_rnnt 2.800516 hw_loss 0.199186 lr 0.00040178 rank 7
2023-02-21 19:18:06,011 DEBUG TRAIN Batch 18/4700 loss 15.176132 loss_att 17.574104 loss_ctc 18.351286 loss_rnnt 14.206200 hw_loss 0.125593 lr 0.00040181 rank 2
2023-02-21 19:18:06,013 DEBUG TRAIN Batch 18/4700 loss 11.869624 loss_att 15.002885 loss_ctc 17.692715 loss_rnnt 10.397312 hw_loss 0.129837 lr 0.00040185 rank 1
2023-02-21 19:18:06,013 DEBUG TRAIN Batch 18/4700 loss 11.901515 loss_att 14.392629 loss_ctc 17.442497 loss_rnnt 10.610112 hw_loss 0.101967 lr 0.00040180 rank 3
2023-02-21 19:18:06,015 DEBUG TRAIN Batch 18/4700 loss 6.874109 loss_att 10.753563 loss_ctc 8.390881 loss_rnnt 5.850188 hw_loss 0.085864 lr 0.00040182 rank 5
2023-02-21 19:18:06,057 DEBUG TRAIN Batch 18/4700 loss 5.368505 loss_att 8.290893 loss_ctc 5.919510 loss_rnnt 4.666221 hw_loss 0.083137 lr 0.00040180 rank 6
2023-02-21 19:19:24,312 DEBUG TRAIN Batch 18/4800 loss 11.573175 loss_att 14.363543 loss_ctc 14.315143 loss_rnnt 10.605135 hw_loss 0.083195 lr 0.00040167 rank 6
2023-02-21 19:19:24,312 DEBUG TRAIN Batch 18/4800 loss 8.409903 loss_att 10.847638 loss_ctc 12.485275 loss_rnnt 7.313139 hw_loss 0.123438 lr 0.00040168 rank 2
2023-02-21 19:19:24,316 DEBUG TRAIN Batch 18/4800 loss 8.078700 loss_att 11.968910 loss_ctc 11.473856 loss_rnnt 6.784707 hw_loss 0.118622 lr 0.00040173 rank 1
2023-02-21 19:19:24,318 DEBUG TRAIN Batch 18/4800 loss 10.649164 loss_att 13.518175 loss_ctc 14.949001 loss_rnnt 9.437085 hw_loss 0.121810 lr 0.00040165 rank 7
2023-02-21 19:19:24,318 DEBUG TRAIN Batch 18/4800 loss 12.535933 loss_att 19.197931 loss_ctc 18.456623 loss_rnnt 10.355128 hw_loss 0.110591 lr 0.00040171 rank 4
2023-02-21 19:19:24,320 DEBUG TRAIN Batch 18/4800 loss 16.842207 loss_att 20.869507 loss_ctc 23.756306 loss_rnnt 15.049533 hw_loss 0.122502 lr 0.00040167 rank 3
2023-02-21 19:19:24,323 DEBUG TRAIN Batch 18/4800 loss 4.678440 loss_att 6.437878 loss_ctc 6.082497 loss_rnnt 4.098015 hw_loss 0.077494 lr 0.00040169 rank 5
2023-02-21 19:19:24,364 DEBUG TRAIN Batch 18/4800 loss 13.958611 loss_att 20.156227 loss_ctc 21.532064 loss_rnnt 11.668589 hw_loss 0.076319 lr 0.00040173 rank 0
2023-02-21 19:20:43,707 DEBUG TRAIN Batch 18/4900 loss 9.311156 loss_att 11.450038 loss_ctc 14.469666 loss_rnnt 8.116359 hw_loss 0.148538 lr 0.00040155 rank 2
2023-02-21 19:20:43,707 DEBUG TRAIN Batch 18/4900 loss 15.139927 loss_att 18.031721 loss_ctc 23.206177 loss_rnnt 13.418904 hw_loss 0.125933 lr 0.00040158 rank 4
2023-02-21 19:20:43,713 DEBUG TRAIN Batch 18/4900 loss 8.205999 loss_att 10.489707 loss_ctc 11.877606 loss_rnnt 7.181765 hw_loss 0.146145 lr 0.00040152 rank 7
2023-02-21 19:20:43,714 DEBUG TRAIN Batch 18/4900 loss 15.200400 loss_att 18.689018 loss_ctc 21.084305 loss_rnnt 13.650217 hw_loss 0.127384 lr 0.00040160 rank 1
2023-02-21 19:20:43,717 DEBUG TRAIN Batch 18/4900 loss 9.421584 loss_att 11.204327 loss_ctc 11.931307 loss_rnnt 8.592648 hw_loss 0.258299 lr 0.00040160 rank 0
2023-02-21 19:20:43,717 DEBUG TRAIN Batch 18/4900 loss 19.334784 loss_att 20.609413 loss_ctc 23.813036 loss_rnnt 18.364923 hw_loss 0.220937 lr 0.00040154 rank 3
2023-02-21 19:20:43,719 DEBUG TRAIN Batch 18/4900 loss 7.589854 loss_att 11.046241 loss_ctc 9.872108 loss_rnnt 6.554268 hw_loss 0.075015 lr 0.00040156 rank 5
2023-02-21 19:20:43,725 DEBUG TRAIN Batch 18/4900 loss 17.400675 loss_att 19.287424 loss_ctc 21.106516 loss_rnnt 16.451267 hw_loss 0.146145 lr 0.00040154 rank 6
2023-02-21 19:22:05,966 DEBUG TRAIN Batch 18/5000 loss 9.655365 loss_att 12.853680 loss_ctc 9.625751 loss_rnnt 8.915466 hw_loss 0.195345 lr 0.00040147 rank 1
2023-02-21 19:22:05,967 DEBUG TRAIN Batch 18/5000 loss 9.028054 loss_att 8.861501 loss_ctc 12.780390 loss_rnnt 8.441554 hw_loss 0.224060 lr 0.00040145 rank 4
2023-02-21 19:22:05,968 DEBUG TRAIN Batch 18/5000 loss 8.959584 loss_att 9.647714 loss_ctc 11.543745 loss_rnnt 8.348939 hw_loss 0.240871 lr 0.00040147 rank 0
2023-02-21 19:22:05,971 DEBUG TRAIN Batch 18/5000 loss 12.166925 loss_att 14.369718 loss_ctc 15.125223 loss_rnnt 11.290220 hw_loss 0.078202 lr 0.00040139 rank 7
2023-02-21 19:22:05,971 DEBUG TRAIN Batch 18/5000 loss 16.172033 loss_att 16.632603 loss_ctc 21.207897 loss_rnnt 15.372155 hw_loss 0.068094 lr 0.00040143 rank 5
2023-02-21 19:22:05,975 DEBUG TRAIN Batch 18/5000 loss 7.213236 loss_att 8.171153 loss_ctc 9.644954 loss_rnnt 6.643567 hw_loss 0.100981 lr 0.00040141 rank 6
2023-02-21 19:22:05,976 DEBUG TRAIN Batch 18/5000 loss 7.163777 loss_att 7.294388 loss_ctc 9.909481 loss_rnnt 6.644206 hw_loss 0.238791 lr 0.00040142 rank 2
2023-02-21 19:22:05,980 DEBUG TRAIN Batch 18/5000 loss 8.510046 loss_att 10.942317 loss_ctc 10.135424 loss_rnnt 7.723751 hw_loss 0.155857 lr 0.00040141 rank 3
2023-02-21 19:23:23,860 DEBUG TRAIN Batch 18/5100 loss 5.670205 loss_att 6.740889 loss_ctc 8.864007 loss_rnnt 4.931365 hw_loss 0.185368 lr 0.00040134 rank 1
2023-02-21 19:23:23,864 DEBUG TRAIN Batch 18/5100 loss 6.402056 loss_att 8.980553 loss_ctc 8.881712 loss_rnnt 5.510530 hw_loss 0.084761 lr 0.00040128 rank 6
2023-02-21 19:23:23,867 DEBUG TRAIN Batch 18/5100 loss 18.501051 loss_att 23.318142 loss_ctc 29.626316 loss_rnnt 15.997252 hw_loss 0.106904 lr 0.00040126 rank 7
2023-02-21 19:23:23,869 DEBUG TRAIN Batch 18/5100 loss 9.997908 loss_att 12.590615 loss_ctc 13.108452 loss_rnnt 9.018393 hw_loss 0.086688 lr 0.00040132 rank 4
2023-02-21 19:23:23,870 DEBUG TRAIN Batch 18/5100 loss 21.692427 loss_att 22.809025 loss_ctc 35.244774 loss_rnnt 19.586300 hw_loss 0.142172 lr 0.00040129 rank 2
2023-02-21 19:23:23,871 DEBUG TRAIN Batch 18/5100 loss 6.286394 loss_att 7.464405 loss_ctc 8.240419 loss_rnnt 5.676090 hw_loss 0.214061 lr 0.00040130 rank 5
2023-02-21 19:23:23,871 DEBUG TRAIN Batch 18/5100 loss 2.144651 loss_att 5.396760 loss_ctc 2.885098 loss_rnnt 1.320707 hw_loss 0.140241 lr 0.00040134 rank 0
2023-02-21 19:23:23,876 DEBUG TRAIN Batch 18/5100 loss 10.197293 loss_att 15.472782 loss_ctc 17.278141 loss_rnnt 8.096760 hw_loss 0.189979 lr 0.00040128 rank 3
2023-02-21 19:24:41,438 DEBUG TRAIN Batch 18/5200 loss 5.957570 loss_att 9.598751 loss_ctc 9.487720 loss_rnnt 4.745129 hw_loss 0.025347 lr 0.00040115 rank 6
2023-02-21 19:24:41,443 DEBUG TRAIN Batch 18/5200 loss 18.791281 loss_att 22.587482 loss_ctc 31.992336 loss_rnnt 16.144077 hw_loss 0.239663 lr 0.00040121 rank 1
2023-02-21 19:24:41,443 DEBUG TRAIN Batch 18/5200 loss 3.663604 loss_att 10.640732 loss_ctc 6.015005 loss_rnnt 1.913641 hw_loss 0.076908 lr 0.00040119 rank 4
2023-02-21 19:24:41,444 DEBUG TRAIN Batch 18/5200 loss 14.629928 loss_att 16.541578 loss_ctc 17.545059 loss_rnnt 13.845230 hw_loss 0.025656 lr 0.00040113 rank 7
2023-02-21 19:24:41,445 DEBUG TRAIN Batch 18/5200 loss 6.899712 loss_att 9.712456 loss_ctc 7.554526 loss_rnnt 6.125768 hw_loss 0.232662 lr 0.00040116 rank 2
2023-02-21 19:24:41,451 DEBUG TRAIN Batch 18/5200 loss 4.147987 loss_att 6.663171 loss_ctc 5.010324 loss_rnnt 3.516322 hw_loss 0.025595 lr 0.00040115 rank 3
2023-02-21 19:24:41,451 DEBUG TRAIN Batch 18/5200 loss 9.231934 loss_att 12.929647 loss_ctc 11.432574 loss_rnnt 8.156252 hw_loss 0.080098 lr 0.00040121 rank 0
2023-02-21 19:24:41,451 DEBUG TRAIN Batch 18/5200 loss 10.297713 loss_att 10.920805 loss_ctc 13.673978 loss_rnnt 9.634760 hw_loss 0.165312 lr 0.00040117 rank 5
2023-02-21 19:26:01,327 DEBUG TRAIN Batch 18/5300 loss 8.445818 loss_att 10.544960 loss_ctc 10.143536 loss_rnnt 7.755871 hw_loss 0.082042 lr 0.00040100 rank 7
2023-02-21 19:26:01,329 DEBUG TRAIN Batch 18/5300 loss 10.677925 loss_att 14.826748 loss_ctc 14.233286 loss_rnnt 9.260524 hw_loss 0.212979 lr 0.00040103 rank 2
2023-02-21 19:26:01,331 DEBUG TRAIN Batch 18/5300 loss 6.393900 loss_att 8.633888 loss_ctc 8.187413 loss_rnnt 5.550882 hw_loss 0.292285 lr 0.00040106 rank 4
2023-02-21 19:26:01,331 DEBUG TRAIN Batch 18/5300 loss 10.781648 loss_att 14.088577 loss_ctc 17.163126 loss_rnnt 9.082796 hw_loss 0.349878 lr 0.00040102 rank 3
2023-02-21 19:26:01,333 DEBUG TRAIN Batch 18/5300 loss 10.068545 loss_att 11.255459 loss_ctc 17.480772 loss_rnnt 8.778170 hw_loss 0.121304 lr 0.00040104 rank 5
2023-02-21 19:26:01,333 DEBUG TRAIN Batch 18/5300 loss 5.128633 loss_att 7.428394 loss_ctc 5.905639 loss_rnnt 4.494282 hw_loss 0.132746 lr 0.00040108 rank 0
2023-02-21 19:26:01,351 DEBUG TRAIN Batch 18/5300 loss 15.266547 loss_att 18.530922 loss_ctc 21.012411 loss_rnnt 13.764389 hw_loss 0.155941 lr 0.00040108 rank 1
2023-02-21 19:26:01,380 DEBUG TRAIN Batch 18/5300 loss 7.185210 loss_att 10.483947 loss_ctc 12.564869 loss_rnnt 5.718711 hw_loss 0.167744 lr 0.00040102 rank 6
2023-02-21 19:27:20,171 DEBUG TRAIN Batch 18/5400 loss 23.643166 loss_att 25.539288 loss_ctc 28.216316 loss_rnnt 22.636398 hw_loss 0.033355 lr 0.00040093 rank 4
2023-02-21 19:27:20,177 DEBUG TRAIN Batch 18/5400 loss 20.404224 loss_att 20.935020 loss_ctc 24.588718 loss_rnnt 19.620674 hw_loss 0.223983 lr 0.00040089 rank 6
2023-02-21 19:27:20,178 DEBUG TRAIN Batch 18/5400 loss 17.766634 loss_att 18.276165 loss_ctc 23.012831 loss_rnnt 16.893898 hw_loss 0.133756 lr 0.00040095 rank 0
2023-02-21 19:27:20,179 DEBUG TRAIN Batch 18/5400 loss 14.858700 loss_att 17.485153 loss_ctc 20.220097 loss_rnnt 13.521614 hw_loss 0.181765 lr 0.00040090 rank 2
2023-02-21 19:27:20,182 DEBUG TRAIN Batch 18/5400 loss 12.731798 loss_att 12.514250 loss_ctc 15.904720 loss_rnnt 12.334223 hw_loss 0.033802 lr 0.00040088 rank 7
2023-02-21 19:27:20,183 DEBUG TRAIN Batch 18/5400 loss 4.904881 loss_att 6.719859 loss_ctc 6.102555 loss_rnnt 4.347405 hw_loss 0.065233 lr 0.00040095 rank 1
2023-02-21 19:27:20,186 DEBUG TRAIN Batch 18/5400 loss 15.366691 loss_att 18.334818 loss_ctc 24.079052 loss_rnnt 13.549208 hw_loss 0.116642 lr 0.00040089 rank 3
2023-02-21 19:27:20,187 DEBUG TRAIN Batch 18/5400 loss 15.815790 loss_att 18.681353 loss_ctc 27.211399 loss_rnnt 13.658134 hw_loss 0.122119 lr 0.00040091 rank 5
2023-02-21 19:28:38,744 DEBUG TRAIN Batch 18/5500 loss 10.725744 loss_att 13.445023 loss_ctc 14.916771 loss_rnnt 9.607029 hw_loss 0.030106 lr 0.00040077 rank 2
2023-02-21 19:28:38,748 DEBUG TRAIN Batch 18/5500 loss 24.768269 loss_att 27.809538 loss_ctc 38.043129 loss_rnnt 22.358490 hw_loss 0.059141 lr 0.00040080 rank 4
2023-02-21 19:28:38,747 DEBUG TRAIN Batch 18/5500 loss 9.239858 loss_att 11.638561 loss_ctc 17.205400 loss_rnnt 7.627913 hw_loss 0.131498 lr 0.00040082 rank 1
2023-02-21 19:28:38,748 DEBUG TRAIN Batch 18/5500 loss 11.704593 loss_att 17.344595 loss_ctc 16.858742 loss_rnnt 9.800694 hw_loss 0.166273 lr 0.00040076 rank 3
2023-02-21 19:28:38,750 DEBUG TRAIN Batch 18/5500 loss 8.235492 loss_att 9.844096 loss_ctc 12.892005 loss_rnnt 7.238812 hw_loss 0.101417 lr 0.00040075 rank 7
2023-02-21 19:28:38,752 DEBUG TRAIN Batch 18/5500 loss 4.318953 loss_att 6.456102 loss_ctc 6.447623 loss_rnnt 3.494756 hw_loss 0.211769 lr 0.00040076 rank 6
2023-02-21 19:28:38,753 DEBUG TRAIN Batch 18/5500 loss 10.870902 loss_att 13.879368 loss_ctc 14.149129 loss_rnnt 9.769523 hw_loss 0.117354 lr 0.00040082 rank 0
2023-02-21 19:28:38,757 DEBUG TRAIN Batch 18/5500 loss 18.391989 loss_att 20.774086 loss_ctc 22.367121 loss_rnnt 17.352219 hw_loss 0.062498 lr 0.00040078 rank 5
2023-02-21 19:29:58,136 DEBUG TRAIN Batch 18/5600 loss 7.914369 loss_att 9.322474 loss_ctc 10.842829 loss_rnnt 7.135452 hw_loss 0.200315 lr 0.00040064 rank 2
2023-02-21 19:29:58,137 DEBUG TRAIN Batch 18/5600 loss 6.772362 loss_att 7.825355 loss_ctc 8.283785 loss_rnnt 6.238182 hw_loss 0.228859 lr 0.00040067 rank 4
2023-02-21 19:29:58,137 DEBUG TRAIN Batch 18/5600 loss 11.271614 loss_att 14.653294 loss_ctc 14.568664 loss_rnnt 10.035522 hw_loss 0.225282 lr 0.00040063 rank 3
2023-02-21 19:29:58,139 DEBUG TRAIN Batch 18/5600 loss 7.227066 loss_att 10.035103 loss_ctc 10.119092 loss_rnnt 6.195365 hw_loss 0.158420 lr 0.00040069 rank 1
2023-02-21 19:29:58,141 DEBUG TRAIN Batch 18/5600 loss 11.380521 loss_att 13.652672 loss_ctc 15.763657 loss_rnnt 10.295373 hw_loss 0.086812 lr 0.00040069 rank 0
2023-02-21 19:29:58,141 DEBUG TRAIN Batch 18/5600 loss 9.671710 loss_att 13.630204 loss_ctc 14.670241 loss_rnnt 8.111407 hw_loss 0.191501 lr 0.00040064 rank 6
2023-02-21 19:29:58,142 DEBUG TRAIN Batch 18/5600 loss 7.773492 loss_att 8.175228 loss_ctc 10.241414 loss_rnnt 7.227356 hw_loss 0.256375 lr 0.00040062 rank 7
2023-02-21 19:29:58,146 DEBUG TRAIN Batch 18/5600 loss 12.306373 loss_att 13.836651 loss_ctc 14.818237 loss_rnnt 11.586498 hw_loss 0.147944 lr 0.00040066 rank 5
2023-02-21 19:31:19,184 DEBUG TRAIN Batch 18/5700 loss 7.407588 loss_att 7.933825 loss_ctc 9.252574 loss_rnnt 6.849745 hw_loss 0.387372 lr 0.00040054 rank 4
2023-02-21 19:31:19,184 DEBUG TRAIN Batch 18/5700 loss 8.746762 loss_att 10.703180 loss_ctc 11.728253 loss_rnnt 7.871540 hw_loss 0.162013 lr 0.00040056 rank 1
2023-02-21 19:31:19,185 DEBUG TRAIN Batch 18/5700 loss 14.093106 loss_att 19.615751 loss_ctc 24.858894 loss_rnnt 11.470407 hw_loss 0.155120 lr 0.00040052 rank 2
2023-02-21 19:31:19,186 DEBUG TRAIN Batch 18/5700 loss 12.730575 loss_att 13.417503 loss_ctc 14.481508 loss_rnnt 12.328328 hw_loss 0.058880 lr 0.00040053 rank 5
2023-02-21 19:31:19,186 DEBUG TRAIN Batch 18/5700 loss 13.321198 loss_att 14.037971 loss_ctc 18.203548 loss_rnnt 12.463255 hw_loss 0.119269 lr 0.00040056 rank 0
2023-02-21 19:31:19,186 DEBUG TRAIN Batch 18/5700 loss 10.365456 loss_att 14.505647 loss_ctc 15.584342 loss_rnnt 8.773767 hw_loss 0.127122 lr 0.00040049 rank 7
2023-02-21 19:31:19,189 DEBUG TRAIN Batch 18/5700 loss 16.813040 loss_att 20.695633 loss_ctc 21.129478 loss_rnnt 15.436037 hw_loss 0.046795 lr 0.00040051 rank 3
2023-02-21 19:31:19,229 DEBUG TRAIN Batch 18/5700 loss 7.963827 loss_att 9.474833 loss_ctc 11.227755 loss_rnnt 7.123082 hw_loss 0.193787 lr 0.00040051 rank 6
2023-02-21 19:32:38,167 DEBUG TRAIN Batch 18/5800 loss 4.751182 loss_att 7.726857 loss_ctc 9.469172 loss_rnnt 3.513129 hw_loss 0.025975 lr 0.00040039 rank 2
2023-02-21 19:32:38,169 DEBUG TRAIN Batch 18/5800 loss 7.288193 loss_att 11.875031 loss_ctc 8.480300 loss_rnnt 6.122669 hw_loss 0.167265 lr 0.00040036 rank 7
2023-02-21 19:32:38,169 DEBUG TRAIN Batch 18/5800 loss 10.014223 loss_att 16.635645 loss_ctc 21.017756 loss_rnnt 7.125466 hw_loss 0.182503 lr 0.00040043 rank 1
2023-02-21 19:32:38,170 DEBUG TRAIN Batch 18/5800 loss 9.998627 loss_att 14.229180 loss_ctc 11.811736 loss_rnnt 8.797400 hw_loss 0.212566 lr 0.00040042 rank 4
2023-02-21 19:32:38,172 DEBUG TRAIN Batch 18/5800 loss 9.543368 loss_att 13.753824 loss_ctc 13.073431 loss_rnnt 8.194753 hw_loss 0.067217 lr 0.00040043 rank 0
2023-02-21 19:32:38,174 DEBUG TRAIN Batch 18/5800 loss 11.865472 loss_att 14.935759 loss_ctc 19.766262 loss_rnnt 10.162135 hw_loss 0.067201 lr 0.00040038 rank 3
2023-02-21 19:32:38,176 DEBUG TRAIN Batch 18/5800 loss 12.661575 loss_att 16.276491 loss_ctc 19.165600 loss_rnnt 10.987685 hw_loss 0.156944 lr 0.00040040 rank 5
2023-02-21 19:32:38,216 DEBUG TRAIN Batch 18/5800 loss 7.478271 loss_att 10.839022 loss_ctc 9.481737 loss_rnnt 6.490497 hw_loss 0.090930 lr 0.00040038 rank 6
2023-02-21 19:33:56,388 DEBUG TRAIN Batch 18/5900 loss 9.640513 loss_att 12.174445 loss_ctc 13.423636 loss_rnnt 8.483528 hw_loss 0.273342 lr 0.00040025 rank 6
2023-02-21 19:33:56,389 DEBUG TRAIN Batch 18/5900 loss 8.125798 loss_att 8.056549 loss_ctc 8.730116 loss_rnnt 7.934375 hw_loss 0.233808 lr 0.00040031 rank 1
2023-02-21 19:33:56,389 DEBUG TRAIN Batch 18/5900 loss 10.454622 loss_att 11.378349 loss_ctc 13.362196 loss_rnnt 9.758471 hw_loss 0.231992 lr 0.00040029 rank 4
2023-02-21 19:33:56,389 DEBUG TRAIN Batch 18/5900 loss 17.779306 loss_att 20.041752 loss_ctc 26.352211 loss_rnnt 16.087406 hw_loss 0.180668 lr 0.00040031 rank 0
2023-02-21 19:33:56,390 DEBUG TRAIN Batch 18/5900 loss 6.189240 loss_att 7.708509 loss_ctc 10.962679 loss_rnnt 5.144322 hw_loss 0.196133 lr 0.00040023 rank 7
2023-02-21 19:33:56,394 DEBUG TRAIN Batch 18/5900 loss 13.116673 loss_att 16.009672 loss_ctc 17.578648 loss_rnnt 11.864055 hw_loss 0.148290 lr 0.00040026 rank 2
2023-02-21 19:33:56,399 DEBUG TRAIN Batch 18/5900 loss 12.183650 loss_att 17.342028 loss_ctc 16.426304 loss_rnnt 10.475199 hw_loss 0.208293 lr 0.00040025 rank 3
2023-02-21 19:33:56,399 DEBUG TRAIN Batch 18/5900 loss 8.196269 loss_att 13.229534 loss_ctc 10.300558 loss_rnnt 6.815624 hw_loss 0.175161 lr 0.00040027 rank 5
2023-02-21 19:35:16,900 DEBUG TRAIN Batch 18/6000 loss 14.529398 loss_att 21.353077 loss_ctc 22.670176 loss_rnnt 12.048241 hw_loss 0.058096 lr 0.00040012 rank 6
2023-02-21 19:35:16,901 DEBUG TRAIN Batch 18/6000 loss 7.924648 loss_att 12.236803 loss_ctc 9.903095 loss_rnnt 6.777289 hw_loss 0.039627 lr 0.00040013 rank 2
2023-02-21 19:35:16,903 DEBUG TRAIN Batch 18/6000 loss 17.406418 loss_att 19.269697 loss_ctc 21.946926 loss_rnnt 16.310698 hw_loss 0.220617 lr 0.00040018 rank 1
2023-02-21 19:35:16,903 DEBUG TRAIN Batch 18/6000 loss 9.965111 loss_att 13.147393 loss_ctc 14.743261 loss_rnnt 8.647723 hw_loss 0.082209 lr 0.00040011 rank 7
2023-02-21 19:35:16,904 DEBUG TRAIN Batch 18/6000 loss 6.247658 loss_att 9.102113 loss_ctc 13.798624 loss_rnnt 4.643077 hw_loss 0.050427 lr 0.00040014 rank 5
2023-02-21 19:35:16,904 DEBUG TRAIN Batch 18/6000 loss 4.718699 loss_att 6.357307 loss_ctc 6.547182 loss_rnnt 4.116125 hw_loss 0.058228 lr 0.00040012 rank 3
2023-02-21 19:35:16,907 DEBUG TRAIN Batch 18/6000 loss 5.804024 loss_att 8.514389 loss_ctc 8.534311 loss_rnnt 4.789086 hw_loss 0.204051 lr 0.00040016 rank 4
2023-02-21 19:35:16,907 DEBUG TRAIN Batch 18/6000 loss 11.573954 loss_att 12.239090 loss_ctc 16.098309 loss_rnnt 10.762164 hw_loss 0.141592 lr 0.00040018 rank 0
2023-02-21 19:36:36,448 DEBUG TRAIN Batch 18/6100 loss 7.047710 loss_att 8.805722 loss_ctc 10.022707 loss_rnnt 6.262729 hw_loss 0.068837 lr 0.00040005 rank 0
2023-02-21 19:36:36,449 DEBUG TRAIN Batch 18/6100 loss 11.424388 loss_att 14.953121 loss_ctc 16.815382 loss_rnnt 9.965573 hw_loss 0.064251 lr 0.00039998 rank 7
2023-02-21 19:36:36,450 DEBUG TRAIN Batch 18/6100 loss 11.566423 loss_att 14.757729 loss_ctc 15.837878 loss_rnnt 10.274202 hw_loss 0.158312 lr 0.00040000 rank 2
2023-02-21 19:36:36,451 DEBUG TRAIN Batch 18/6100 loss 8.541864 loss_att 9.882196 loss_ctc 12.207648 loss_rnnt 7.701369 hw_loss 0.156858 lr 0.00040003 rank 4
2023-02-21 19:36:36,453 DEBUG TRAIN Batch 18/6100 loss 12.975973 loss_att 18.351696 loss_ctc 20.705578 loss_rnnt 10.803748 hw_loss 0.124627 lr 0.00040005 rank 1
2023-02-21 19:36:36,455 DEBUG TRAIN Batch 18/6100 loss 10.382050 loss_att 12.024359 loss_ctc 12.702566 loss_rnnt 9.652423 hw_loss 0.172055 lr 0.00039999 rank 6
2023-02-21 19:36:36,457 DEBUG TRAIN Batch 18/6100 loss 5.493543 loss_att 7.294322 loss_ctc 5.522668 loss_rnnt 4.987312 hw_loss 0.266609 lr 0.00040001 rank 5
2023-02-21 19:36:36,463 DEBUG TRAIN Batch 18/6100 loss 13.314140 loss_att 14.798450 loss_ctc 16.681488 loss_rnnt 12.450809 hw_loss 0.220294 lr 0.00039999 rank 3
2023-02-21 19:37:55,118 DEBUG TRAIN Batch 18/6200 loss 6.799585 loss_att 10.922617 loss_ctc 11.354130 loss_rnnt 5.293373 hw_loss 0.139374 lr 0.00039990 rank 4
2023-02-21 19:37:55,122 DEBUG TRAIN Batch 18/6200 loss 12.954494 loss_att 13.896551 loss_ctc 17.331366 loss_rnnt 12.103997 hw_loss 0.147191 lr 0.00039985 rank 7
2023-02-21 19:37:55,122 DEBUG TRAIN Batch 18/6200 loss 11.716436 loss_att 13.980244 loss_ctc 17.101189 loss_rnnt 10.487861 hw_loss 0.108465 lr 0.00039992 rank 0
2023-02-21 19:37:55,126 DEBUG TRAIN Batch 18/6200 loss 11.086964 loss_att 14.422785 loss_ctc 14.334666 loss_rnnt 9.937423 hw_loss 0.092531 lr 0.00039987 rank 2
2023-02-21 19:37:55,126 DEBUG TRAIN Batch 18/6200 loss 16.793369 loss_att 18.654625 loss_ctc 26.325020 loss_rnnt 15.107038 hw_loss 0.080988 lr 0.00039989 rank 5
2023-02-21 19:37:55,127 DEBUG TRAIN Batch 18/6200 loss 8.606502 loss_att 9.652381 loss_ctc 10.227268 loss_rnnt 8.161593 hw_loss 0.036807 lr 0.00039986 rank 3
2023-02-21 19:37:55,128 DEBUG TRAIN Batch 18/6200 loss 12.674361 loss_att 12.464998 loss_ctc 14.266323 loss_rnnt 12.440627 hw_loss 0.118772 lr 0.00039992 rank 1
2023-02-21 19:37:55,176 DEBUG TRAIN Batch 18/6200 loss 16.086645 loss_att 18.299526 loss_ctc 22.468391 loss_rnnt 14.753118 hw_loss 0.075096 lr 0.00039987 rank 6
2023-02-21 19:39:13,302 DEBUG TRAIN Batch 18/6300 loss 9.258787 loss_att 21.680004 loss_ctc 9.456438 loss_rnnt 6.721893 hw_loss 0.049308 lr 0.00039972 rank 7
2023-02-21 19:39:13,305 DEBUG TRAIN Batch 18/6300 loss 12.380514 loss_att 14.011196 loss_ctc 20.435730 loss_rnnt 10.927590 hw_loss 0.098921 lr 0.00039974 rank 6
2023-02-21 19:39:13,307 DEBUG TRAIN Batch 18/6300 loss 12.461554 loss_att 14.145664 loss_ctc 17.475609 loss_rnnt 11.256618 hw_loss 0.374197 lr 0.00039975 rank 2
2023-02-21 19:39:13,310 DEBUG TRAIN Batch 18/6300 loss 14.439521 loss_att 22.488964 loss_ctc 18.486607 loss_rnnt 12.252474 hw_loss 0.070401 lr 0.00039979 rank 0
2023-02-21 19:39:13,310 DEBUG TRAIN Batch 18/6300 loss 5.894407 loss_att 7.355074 loss_ctc 7.853020 loss_rnnt 5.305246 hw_loss 0.067273 lr 0.00039974 rank 3
2023-02-21 19:39:13,311 DEBUG TRAIN Batch 18/6300 loss 7.829856 loss_att 11.435526 loss_ctc 9.069907 loss_rnnt 6.853246 hw_loss 0.169005 lr 0.00039977 rank 4
2023-02-21 19:39:13,312 DEBUG TRAIN Batch 18/6300 loss 12.650035 loss_att 12.643703 loss_ctc 15.183432 loss_rnnt 12.227010 hw_loss 0.162196 lr 0.00039976 rank 5
2023-02-21 19:39:13,313 DEBUG TRAIN Batch 18/6300 loss 10.263747 loss_att 12.692698 loss_ctc 12.934826 loss_rnnt 9.354908 hw_loss 0.125447 lr 0.00039979 rank 1
2023-02-21 19:40:34,963 DEBUG TRAIN Batch 18/6400 loss 5.047317 loss_att 8.432298 loss_ctc 6.715668 loss_rnnt 4.058564 hw_loss 0.167457 lr 0.00039959 rank 7
2023-02-21 19:40:34,967 DEBUG TRAIN Batch 18/6400 loss 12.066677 loss_att 12.744175 loss_ctc 17.438770 loss_rnnt 11.178438 hw_loss 0.068359 lr 0.00039967 rank 1
2023-02-21 19:40:34,971 DEBUG TRAIN Batch 18/6400 loss 10.047696 loss_att 12.730764 loss_ctc 12.536469 loss_rnnt 9.117851 hw_loss 0.115117 lr 0.00039965 rank 4
2023-02-21 19:40:34,973 DEBUG TRAIN Batch 18/6400 loss 6.179515 loss_att 8.695696 loss_ctc 8.686620 loss_rnnt 5.230959 hw_loss 0.208199 lr 0.00039961 rank 6
2023-02-21 19:40:34,974 DEBUG TRAIN Batch 18/6400 loss 6.534311 loss_att 10.924767 loss_ctc 8.292296 loss_rnnt 5.397855 hw_loss 0.044937 lr 0.00039962 rank 2
2023-02-21 19:40:34,975 DEBUG TRAIN Batch 18/6400 loss 12.534889 loss_att 14.502623 loss_ctc 15.368868 loss_rnnt 11.688223 hw_loss 0.141107 lr 0.00039963 rank 5
2023-02-21 19:40:34,978 DEBUG TRAIN Batch 18/6400 loss 4.715217 loss_att 10.692438 loss_ctc 10.000298 loss_rnnt 2.767223 hw_loss 0.089759 lr 0.00039967 rank 0
2023-02-21 19:40:35,002 DEBUG TRAIN Batch 18/6400 loss 1.825680 loss_att 4.845756 loss_ctc 3.178159 loss_rnnt 0.967586 hw_loss 0.138280 lr 0.00039961 rank 3
2023-02-21 19:41:52,352 DEBUG TRAIN Batch 18/6500 loss 4.881554 loss_att 7.974724 loss_ctc 4.492275 loss_rnnt 4.208205 hw_loss 0.199909 lr 0.00039949 rank 2
2023-02-21 19:41:52,354 DEBUG TRAIN Batch 18/6500 loss 11.342060 loss_att 13.966055 loss_ctc 15.643629 loss_rnnt 10.197525 hw_loss 0.086612 lr 0.00039947 rank 7
2023-02-21 19:41:52,356 DEBUG TRAIN Batch 18/6500 loss 24.563536 loss_att 22.109024 loss_ctc 31.586700 loss_rnnt 24.065250 hw_loss 0.098931 lr 0.00039948 rank 6
2023-02-21 19:41:52,359 DEBUG TRAIN Batch 18/6500 loss 7.251176 loss_att 11.044824 loss_ctc 8.880710 loss_rnnt 6.220651 hw_loss 0.102234 lr 0.00039952 rank 4
2023-02-21 19:41:52,358 DEBUG TRAIN Batch 18/6500 loss 13.240473 loss_att 14.435547 loss_ctc 16.877300 loss_rnnt 12.394348 hw_loss 0.229124 lr 0.00039954 rank 0
2023-02-21 19:41:52,359 DEBUG TRAIN Batch 18/6500 loss 6.456111 loss_att 10.535924 loss_ctc 11.328974 loss_rnnt 4.927691 hw_loss 0.117641 lr 0.00039948 rank 3
2023-02-21 19:41:52,359 DEBUG TRAIN Batch 18/6500 loss 12.950541 loss_att 17.412941 loss_ctc 16.383728 loss_rnnt 11.491983 hw_loss 0.203099 lr 0.00039954 rank 1
2023-02-21 19:41:52,363 DEBUG TRAIN Batch 18/6500 loss 7.240784 loss_att 10.226409 loss_ctc 10.420528 loss_rnnt 6.113347 hw_loss 0.199398 lr 0.00039950 rank 5
2023-02-21 19:43:11,466 DEBUG TRAIN Batch 18/6600 loss 16.657248 loss_att 19.703701 loss_ctc 25.109615 loss_rnnt 14.889345 hw_loss 0.059303 lr 0.00039936 rank 2
2023-02-21 19:43:11,466 DEBUG TRAIN Batch 18/6600 loss 16.188677 loss_att 20.138374 loss_ctc 21.066376 loss_rnnt 14.684547 hw_loss 0.119683 lr 0.00039935 rank 3
2023-02-21 19:43:11,468 DEBUG TRAIN Batch 18/6600 loss 12.262184 loss_att 15.642365 loss_ctc 14.419949 loss_rnnt 11.248966 hw_loss 0.092776 lr 0.00039939 rank 4
2023-02-21 19:43:11,468 DEBUG TRAIN Batch 18/6600 loss 5.111746 loss_att 7.716995 loss_ctc 8.496182 loss_rnnt 4.070019 hw_loss 0.130160 lr 0.00039934 rank 7
2023-02-21 19:43:11,468 DEBUG TRAIN Batch 18/6600 loss 9.607510 loss_att 15.034357 loss_ctc 18.630079 loss_rnnt 7.304134 hw_loss 0.028119 lr 0.00039936 rank 6
2023-02-21 19:43:11,469 DEBUG TRAIN Batch 18/6600 loss 11.776731 loss_att 13.551343 loss_ctc 14.691296 loss_rnnt 10.995646 hw_loss 0.070411 lr 0.00039941 rank 1
2023-02-21 19:43:11,474 DEBUG TRAIN Batch 18/6600 loss 6.183452 loss_att 11.227409 loss_ctc 8.934134 loss_rnnt 4.721813 hw_loss 0.161417 lr 0.00039941 rank 0
2023-02-21 19:43:11,517 DEBUG TRAIN Batch 18/6600 loss 14.808730 loss_att 17.504105 loss_ctc 22.445295 loss_rnnt 13.203781 hw_loss 0.089371 lr 0.00039938 rank 5
2023-02-21 19:44:30,177 DEBUG TRAIN Batch 18/6700 loss 14.973564 loss_att 14.930401 loss_ctc 18.726849 loss_rnnt 14.409577 hw_loss 0.135341 lr 0.00039926 rank 4
2023-02-21 19:44:30,178 DEBUG TRAIN Batch 18/6700 loss 25.716278 loss_att 28.344372 loss_ctc 36.730904 loss_rnnt 23.620884 hw_loss 0.189673 lr 0.00039924 rank 2
2023-02-21 19:44:30,178 DEBUG TRAIN Batch 18/6700 loss 7.036280 loss_att 11.314013 loss_ctc 10.099991 loss_rnnt 5.755941 hw_loss 0.030557 lr 0.00039928 rank 1
2023-02-21 19:44:30,178 DEBUG TRAIN Batch 18/6700 loss 5.840626 loss_att 10.028569 loss_ctc 7.943139 loss_rnnt 4.690118 hw_loss 0.061097 lr 0.00039923 rank 6
2023-02-21 19:44:30,179 DEBUG TRAIN Batch 18/6700 loss 6.752372 loss_att 10.297846 loss_ctc 9.842846 loss_rnnt 5.577543 hw_loss 0.100636 lr 0.00039925 rank 5
2023-02-21 19:44:30,180 DEBUG TRAIN Batch 18/6700 loss 12.786220 loss_att 16.720203 loss_ctc 18.004002 loss_rnnt 11.229548 hw_loss 0.139071 lr 0.00039928 rank 0
2023-02-21 19:44:30,181 DEBUG TRAIN Batch 18/6700 loss 7.608150 loss_att 9.516817 loss_ctc 9.287199 loss_rnnt 6.893766 hw_loss 0.203958 lr 0.00039923 rank 3
2023-02-21 19:44:30,186 DEBUG TRAIN Batch 18/6700 loss 9.640733 loss_att 13.057637 loss_ctc 12.727600 loss_rnnt 8.478002 hw_loss 0.127066 lr 0.00039921 rank 7
2023-02-21 19:45:49,455 DEBUG TRAIN Batch 18/6800 loss 11.059731 loss_att 15.209646 loss_ctc 18.074215 loss_rnnt 9.216413 hw_loss 0.146380 lr 0.00039911 rank 2
2023-02-21 19:45:49,458 DEBUG TRAIN Batch 18/6800 loss 7.370278 loss_att 9.596832 loss_ctc 10.333019 loss_rnnt 6.455743 hw_loss 0.139110 lr 0.00039914 rank 4
2023-02-21 19:45:49,459 DEBUG TRAIN Batch 18/6800 loss 9.642106 loss_att 11.836895 loss_ctc 11.000351 loss_rnnt 8.954326 hw_loss 0.126982 lr 0.00039916 rank 1
2023-02-21 19:45:49,463 DEBUG TRAIN Batch 18/6800 loss 19.374237 loss_att 21.653780 loss_ctc 28.181524 loss_rnnt 17.633312 hw_loss 0.207581 lr 0.00039916 rank 0
2023-02-21 19:45:49,463 DEBUG TRAIN Batch 18/6800 loss 16.941021 loss_att 16.834913 loss_ctc 22.873642 loss_rnnt 16.057854 hw_loss 0.212575 lr 0.00039908 rank 7
2023-02-21 19:45:49,464 DEBUG TRAIN Batch 18/6800 loss 5.292577 loss_att 6.775508 loss_ctc 7.371294 loss_rnnt 4.662652 hw_loss 0.105332 lr 0.00039910 rank 3
2023-02-21 19:45:49,486 DEBUG TRAIN Batch 18/6800 loss 10.269963 loss_att 11.871714 loss_ctc 13.504382 loss_rnnt 9.349888 hw_loss 0.315880 lr 0.00039910 rank 6
2023-02-21 19:45:49,496 DEBUG TRAIN Batch 18/6800 loss 13.112836 loss_att 13.311982 loss_ctc 16.450125 loss_rnnt 12.589049 hw_loss 0.073098 lr 0.00039912 rank 5
2023-02-21 19:47:08,514 DEBUG TRAIN Batch 18/6900 loss 8.791317 loss_att 10.779957 loss_ctc 12.042315 loss_rnnt 7.920305 hw_loss 0.074658 lr 0.00039901 rank 4
2023-02-21 19:47:08,517 DEBUG TRAIN Batch 18/6900 loss 3.540971 loss_att 7.628817 loss_ctc 6.404121 loss_rnnt 2.280072 hw_loss 0.115457 lr 0.00039896 rank 7
2023-02-21 19:47:08,518 DEBUG TRAIN Batch 18/6900 loss 8.037652 loss_att 8.774556 loss_ctc 10.957865 loss_rnnt 7.316172 hw_loss 0.346382 lr 0.00039903 rank 0
2023-02-21 19:47:08,524 DEBUG TRAIN Batch 18/6900 loss 15.777993 loss_att 19.471359 loss_ctc 22.544359 loss_rnnt 14.119499 hw_loss 0.033071 lr 0.00039903 rank 1
2023-02-21 19:47:08,524 DEBUG TRAIN Batch 18/6900 loss 7.082510 loss_att 10.506536 loss_ctc 10.723066 loss_rnnt 5.841863 hw_loss 0.132066 lr 0.00039897 rank 3
2023-02-21 19:47:08,525 DEBUG TRAIN Batch 18/6900 loss 7.437930 loss_att 9.282850 loss_ctc 10.518270 loss_rnnt 6.566593 hw_loss 0.171826 lr 0.00039897 rank 6
2023-02-21 19:47:08,527 DEBUG TRAIN Batch 18/6900 loss 12.804678 loss_att 16.079599 loss_ctc 17.471678 loss_rnnt 11.450708 hw_loss 0.143847 lr 0.00039898 rank 2
2023-02-21 19:47:08,575 DEBUG TRAIN Batch 18/6900 loss 5.713986 loss_att 7.011254 loss_ctc 7.412502 loss_rnnt 5.089437 hw_loss 0.259927 lr 0.00039899 rank 5
2023-02-21 19:48:26,862 DEBUG TRAIN Batch 18/7000 loss 7.010944 loss_att 9.569470 loss_ctc 10.841342 loss_rnnt 5.926717 hw_loss 0.115878 lr 0.00039883 rank 7
2023-02-21 19:48:26,863 DEBUG TRAIN Batch 18/7000 loss 11.307853 loss_att 17.885994 loss_ctc 14.166736 loss_rnnt 9.530464 hw_loss 0.151078 lr 0.00039886 rank 2
2023-02-21 19:48:26,864 DEBUG TRAIN Batch 18/7000 loss 13.608081 loss_att 14.309639 loss_ctc 17.859684 loss_rnnt 12.784203 hw_loss 0.218785 lr 0.00039888 rank 4
2023-02-21 19:48:26,866 DEBUG TRAIN Batch 18/7000 loss 21.340069 loss_att 23.053045 loss_ctc 25.893103 loss_rnnt 20.317848 hw_loss 0.136040 lr 0.00039885 rank 6
2023-02-21 19:48:26,868 DEBUG TRAIN Batch 18/7000 loss 10.668791 loss_att 12.575439 loss_ctc 9.930256 loss_rnnt 10.339497 hw_loss 0.087067 lr 0.00039885 rank 3
2023-02-21 19:48:26,868 DEBUG TRAIN Batch 18/7000 loss 7.129436 loss_att 8.604303 loss_ctc 11.498729 loss_rnnt 6.124546 hw_loss 0.238773 lr 0.00039890 rank 1
2023-02-21 19:48:26,869 DEBUG TRAIN Batch 18/7000 loss 4.956470 loss_att 7.053056 loss_ctc 8.853857 loss_rnnt 3.968159 hw_loss 0.092516 lr 0.00039890 rank 0
2023-02-21 19:48:26,869 DEBUG TRAIN Batch 18/7000 loss 4.971579 loss_att 8.571962 loss_ctc 7.184268 loss_rnnt 3.864311 hw_loss 0.172812 lr 0.00039887 rank 5
2023-02-21 19:49:48,023 DEBUG TRAIN Batch 18/7100 loss 8.725575 loss_att 10.169250 loss_ctc 12.459833 loss_rnnt 7.872943 hw_loss 0.123741 lr 0.00039876 rank 4
2023-02-21 19:49:48,026 DEBUG TRAIN Batch 18/7100 loss 12.423918 loss_att 15.968548 loss_ctc 21.475765 loss_rnnt 10.493749 hw_loss 0.026870 lr 0.00039870 rank 7
2023-02-21 19:49:48,027 DEBUG TRAIN Batch 18/7100 loss 7.481766 loss_att 10.808846 loss_ctc 11.538860 loss_rnnt 6.215323 hw_loss 0.112653 lr 0.00039872 rank 3
2023-02-21 19:49:48,028 DEBUG TRAIN Batch 18/7100 loss 6.585295 loss_att 11.371079 loss_ctc 10.447597 loss_rnnt 5.099008 hw_loss 0.026543 lr 0.00039872 rank 6
2023-02-21 19:49:48,029 DEBUG TRAIN Batch 18/7100 loss 5.807850 loss_att 8.173348 loss_ctc 10.378428 loss_rnnt 4.619303 hw_loss 0.198820 lr 0.00039878 rank 1
2023-02-21 19:49:48,032 DEBUG TRAIN Batch 18/7100 loss 13.792838 loss_att 13.357374 loss_ctc 16.748428 loss_rnnt 13.383842 hw_loss 0.191272 lr 0.00039874 rank 5
2023-02-21 19:49:48,033 DEBUG TRAIN Batch 18/7100 loss 5.747963 loss_att 8.192541 loss_ctc 9.221725 loss_rnnt 4.781595 hw_loss 0.026783 lr 0.00039878 rank 0
2023-02-21 19:49:48,043 DEBUG TRAIN Batch 18/7100 loss 19.567419 loss_att 26.254238 loss_ctc 27.104084 loss_rnnt 17.131889 hw_loss 0.174894 lr 0.00039873 rank 2
2023-02-21 19:51:06,367 DEBUG TRAIN Batch 18/7200 loss 13.717216 loss_att 16.891014 loss_ctc 18.029047 loss_rnnt 12.385135 hw_loss 0.229519 lr 0.00039865 rank 1
2023-02-21 19:51:06,372 DEBUG TRAIN Batch 18/7200 loss 11.060157 loss_att 13.530315 loss_ctc 17.993595 loss_rnnt 9.583827 hw_loss 0.108450 lr 0.00039860 rank 2
2023-02-21 19:51:06,372 DEBUG TRAIN Batch 18/7200 loss 4.734664 loss_att 7.882213 loss_ctc 8.665794 loss_rnnt 3.565981 hw_loss 0.028168 lr 0.00039858 rank 7
2023-02-21 19:51:06,374 DEBUG TRAIN Batch 18/7200 loss 10.799124 loss_att 14.190078 loss_ctc 16.218948 loss_rnnt 9.306877 hw_loss 0.171400 lr 0.00039863 rank 4
2023-02-21 19:51:06,374 DEBUG TRAIN Batch 18/7200 loss 14.225756 loss_att 18.538776 loss_ctc 19.633680 loss_rnnt 12.565870 hw_loss 0.142922 lr 0.00039859 rank 6
2023-02-21 19:51:06,384 DEBUG TRAIN Batch 18/7200 loss 11.200275 loss_att 13.006393 loss_ctc 13.098859 loss_rnnt 10.556561 hw_loss 0.055021 lr 0.00039865 rank 0
2023-02-21 19:51:06,414 DEBUG TRAIN Batch 18/7200 loss 16.750177 loss_att 21.368967 loss_ctc 21.536840 loss_rnnt 15.173244 hw_loss 0.028037 lr 0.00039859 rank 3
2023-02-21 19:51:06,421 DEBUG TRAIN Batch 18/7200 loss 8.225164 loss_att 10.869807 loss_ctc 9.071474 loss_rnnt 7.543506 hw_loss 0.074791 lr 0.00039861 rank 5
2023-02-21 19:52:24,343 DEBUG TRAIN Batch 18/7300 loss 16.485296 loss_att 17.264843 loss_ctc 20.777969 loss_rnnt 15.711059 hw_loss 0.086199 lr 0.00039850 rank 4
2023-02-21 19:52:24,348 DEBUG TRAIN Batch 18/7300 loss 8.066680 loss_att 11.141974 loss_ctc 7.672485 loss_rnnt 7.292110 hw_loss 0.397632 lr 0.00039845 rank 7
2023-02-21 19:52:24,349 DEBUG TRAIN Batch 18/7300 loss 4.935879 loss_att 5.809601 loss_ctc 6.717889 loss_rnnt 4.437793 hw_loss 0.160762 lr 0.00039848 rank 2
2023-02-21 19:52:24,350 DEBUG TRAIN Batch 18/7300 loss 8.945541 loss_att 11.118310 loss_ctc 14.075822 loss_rnnt 7.812079 hw_loss 0.027883 lr 0.00039852 rank 0
2023-02-21 19:52:24,353 DEBUG TRAIN Batch 18/7300 loss 5.466643 loss_att 10.354440 loss_ctc 7.019931 loss_rnnt 4.233204 hw_loss 0.091454 lr 0.00039847 rank 6
2023-02-21 19:52:24,355 DEBUG TRAIN Batch 18/7300 loss 8.032399 loss_att 8.910067 loss_ctc 9.145650 loss_rnnt 7.551826 hw_loss 0.293635 lr 0.00039849 rank 5
2023-02-21 19:52:24,355 DEBUG TRAIN Batch 18/7300 loss 12.346270 loss_att 15.743165 loss_ctc 17.322054 loss_rnnt 10.961764 hw_loss 0.078166 lr 0.00039852 rank 1
2023-02-21 19:52:24,360 DEBUG TRAIN Batch 18/7300 loss 7.831400 loss_att 8.742453 loss_ctc 9.333493 loss_rnnt 7.377631 hw_loss 0.133650 lr 0.00039847 rank 3
2023-02-21 19:53:43,998 DEBUG TRAIN Batch 18/7400 loss 6.379206 loss_att 8.084898 loss_ctc 10.585708 loss_rnnt 5.375242 hw_loss 0.191172 lr 0.00039835 rank 2
2023-02-21 19:53:44,006 DEBUG TRAIN Batch 18/7400 loss 8.479116 loss_att 11.164168 loss_ctc 10.867313 loss_rnnt 7.530367 hw_loss 0.174962 lr 0.00039832 rank 7
2023-02-21 19:53:44,007 DEBUG TRAIN Batch 18/7400 loss 5.087708 loss_att 7.494272 loss_ctc 7.848496 loss_rnnt 4.166358 hw_loss 0.134874 lr 0.00039840 rank 0
2023-02-21 19:53:44,007 DEBUG TRAIN Batch 18/7400 loss 9.342643 loss_att 12.039367 loss_ctc 12.849088 loss_rnnt 8.263858 hw_loss 0.134841 lr 0.00039840 rank 1
2023-02-21 19:53:44,007 DEBUG TRAIN Batch 18/7400 loss 9.649434 loss_att 9.868227 loss_ctc 12.920239 loss_rnnt 9.099154 hw_loss 0.132026 lr 0.00039834 rank 6
2023-02-21 19:53:44,010 DEBUG TRAIN Batch 18/7400 loss 8.980743 loss_att 12.844496 loss_ctc 13.829908 loss_rnnt 7.519994 hw_loss 0.077705 lr 0.00039838 rank 4
2023-02-21 19:53:44,013 DEBUG TRAIN Batch 18/7400 loss 9.605142 loss_att 13.232219 loss_ctc 15.159056 loss_rnnt 8.112368 hw_loss 0.050317 lr 0.00039836 rank 5
2023-02-21 19:53:44,016 DEBUG TRAIN Batch 18/7400 loss 6.443471 loss_att 9.115612 loss_ctc 11.115196 loss_rnnt 5.245408 hw_loss 0.076385 lr 0.00039834 rank 3
2023-02-21 19:55:04,870 DEBUG TRAIN Batch 18/7500 loss 10.596380 loss_att 10.709770 loss_ctc 14.553691 loss_rnnt 9.928061 hw_loss 0.221250 lr 0.00039820 rank 7
2023-02-21 19:55:04,873 DEBUG TRAIN Batch 18/7500 loss 10.118117 loss_att 11.368126 loss_ctc 11.952559 loss_rnnt 9.560654 hw_loss 0.117878 lr 0.00039822 rank 2
2023-02-21 19:55:04,879 DEBUG TRAIN Batch 18/7500 loss 6.031654 loss_att 9.788086 loss_ctc 10.338482 loss_rnnt 4.681002 hw_loss 0.047104 lr 0.00039827 rank 1
2023-02-21 19:55:04,880 DEBUG TRAIN Batch 18/7500 loss 8.956656 loss_att 10.377003 loss_ctc 14.757623 loss_rnnt 7.862247 hw_loss 0.069144 lr 0.00039821 rank 6
2023-02-21 19:55:04,881 DEBUG TRAIN Batch 18/7500 loss 17.303913 loss_att 20.375298 loss_ctc 23.208782 loss_rnnt 15.851437 hw_loss 0.095407 lr 0.00039825 rank 4
2023-02-21 19:55:04,883 DEBUG TRAIN Batch 18/7500 loss 11.661149 loss_att 12.398170 loss_ctc 16.321611 loss_rnnt 10.839861 hw_loss 0.098416 lr 0.00039821 rank 3
2023-02-21 19:55:04,887 DEBUG TRAIN Batch 18/7500 loss 11.299003 loss_att 14.835838 loss_ctc 14.491153 loss_rnnt 10.117963 hw_loss 0.090096 lr 0.00039823 rank 5
2023-02-21 19:55:04,923 DEBUG TRAIN Batch 18/7500 loss 7.123182 loss_att 9.509758 loss_ctc 8.558112 loss_rnnt 6.410004 hw_loss 0.083511 lr 0.00039827 rank 0
2023-02-21 19:56:23,691 DEBUG TRAIN Batch 18/7600 loss 5.663139 loss_att 7.985442 loss_ctc 8.853152 loss_rnnt 4.631237 hw_loss 0.266449 lr 0.00039812 rank 4
2023-02-21 19:56:23,694 DEBUG TRAIN Batch 18/7600 loss 7.296870 loss_att 10.471981 loss_ctc 8.125075 loss_rnnt 6.493852 hw_loss 0.107941 lr 0.00039814 rank 0
2023-02-21 19:56:23,697 DEBUG TRAIN Batch 18/7600 loss 17.557648 loss_att 19.281343 loss_ctc 24.929199 loss_rnnt 16.165804 hw_loss 0.120430 lr 0.00039814 rank 1
2023-02-21 19:56:23,698 DEBUG TRAIN Batch 18/7600 loss 6.650459 loss_att 9.309914 loss_ctc 7.315142 loss_rnnt 5.940537 hw_loss 0.167637 lr 0.00039807 rank 7
2023-02-21 19:56:23,701 DEBUG TRAIN Batch 18/7600 loss 6.784177 loss_att 8.987417 loss_ctc 10.575273 loss_rnnt 5.799141 hw_loss 0.072954 lr 0.00039810 rank 2
2023-02-21 19:56:23,702 DEBUG TRAIN Batch 18/7600 loss 13.095219 loss_att 15.697158 loss_ctc 20.481806 loss_rnnt 11.531493 hw_loss 0.109611 lr 0.00039809 rank 6
2023-02-21 19:56:23,703 DEBUG TRAIN Batch 18/7600 loss 4.575037 loss_att 5.544410 loss_ctc 7.323410 loss_rnnt 3.877100 hw_loss 0.258022 lr 0.00039809 rank 3
2023-02-21 19:56:23,703 DEBUG TRAIN Batch 18/7600 loss 18.155363 loss_att 20.412138 loss_ctc 22.307522 loss_rnnt 17.085079 hw_loss 0.122452 lr 0.00039811 rank 5
2023-02-21 19:57:41,823 DEBUG TRAIN Batch 18/7700 loss 6.850888 loss_att 7.541490 loss_ctc 8.702398 loss_rnnt 6.376963 hw_loss 0.166755 lr 0.00039802 rank 1
2023-02-21 19:57:41,825 DEBUG TRAIN Batch 18/7700 loss 9.094887 loss_att 12.073890 loss_ctc 12.517375 loss_rnnt 7.937726 hw_loss 0.196926 lr 0.00039800 rank 4
2023-02-21 19:57:41,825 DEBUG TRAIN Batch 18/7700 loss 11.125134 loss_att 15.784517 loss_ctc 17.753662 loss_rnnt 9.266259 hw_loss 0.080990 lr 0.00039802 rank 0
2023-02-21 19:57:41,826 DEBUG TRAIN Batch 18/7700 loss 14.161459 loss_att 14.983843 loss_ctc 18.666376 loss_rnnt 13.278101 hw_loss 0.221671 lr 0.00039796 rank 6
2023-02-21 19:57:41,826 DEBUG TRAIN Batch 18/7700 loss 11.165483 loss_att 15.423595 loss_ctc 11.823608 loss_rnnt 10.190092 hw_loss 0.067534 lr 0.00039797 rank 2
2023-02-21 19:57:41,827 DEBUG TRAIN Batch 18/7700 loss 10.184031 loss_att 17.525288 loss_ctc 11.930098 loss_rnnt 8.442322 hw_loss 0.076219 lr 0.00039794 rank 7
2023-02-21 19:57:41,827 DEBUG TRAIN Batch 18/7700 loss 17.675556 loss_att 18.541142 loss_ctc 22.506964 loss_rnnt 16.718964 hw_loss 0.261162 lr 0.00039798 rank 5
2023-02-21 19:57:41,831 DEBUG TRAIN Batch 18/7700 loss 14.898400 loss_att 21.973734 loss_ctc 21.634575 loss_rnnt 12.534836 hw_loss 0.094388 lr 0.00039796 rank 3
2023-02-21 19:59:02,193 DEBUG TRAIN Batch 18/7800 loss 8.325607 loss_att 16.491028 loss_ctc 9.727285 loss_rnnt 6.457300 hw_loss 0.090624 lr 0.00039784 rank 2
2023-02-21 19:59:02,196 DEBUG TRAIN Batch 18/7800 loss 2.068872 loss_att 5.978122 loss_ctc 4.206230 loss_rnnt 0.903918 hw_loss 0.183980 lr 0.00039789 rank 1
2023-02-21 19:59:02,201 DEBUG TRAIN Batch 18/7800 loss 17.187159 loss_att 19.336927 loss_ctc 20.421463 loss_rnnt 16.254089 hw_loss 0.134765 lr 0.00039787 rank 4
2023-02-21 19:59:02,201 DEBUG TRAIN Batch 18/7800 loss 8.331027 loss_att 9.220765 loss_ctc 9.967649 loss_rnnt 7.848884 hw_loss 0.161211 lr 0.00039784 rank 6
2023-02-21 19:59:02,203 DEBUG TRAIN Batch 18/7800 loss 14.547790 loss_att 16.249588 loss_ctc 24.459915 loss_rnnt 12.823463 hw_loss 0.116907 lr 0.00039789 rank 0
2023-02-21 19:59:02,205 DEBUG TRAIN Batch 18/7800 loss 9.111056 loss_att 12.899143 loss_ctc 17.245953 loss_rnnt 7.182901 hw_loss 0.161034 lr 0.00039782 rank 7
2023-02-21 19:59:02,206 DEBUG TRAIN Batch 18/7800 loss 21.053726 loss_att 23.580313 loss_ctc 30.209965 loss_rnnt 19.293125 hw_loss 0.064595 lr 0.00039783 rank 3
2023-02-21 19:59:02,212 DEBUG TRAIN Batch 18/7800 loss 14.213799 loss_att 17.612913 loss_ctc 20.150545 loss_rnnt 12.695440 hw_loss 0.088069 lr 0.00039786 rank 5
2023-02-21 20:00:21,028 DEBUG TRAIN Batch 18/7900 loss 13.079813 loss_att 18.395939 loss_ctc 18.413155 loss_rnnt 11.203856 hw_loss 0.190536 lr 0.00039771 rank 3
2023-02-21 20:00:21,029 DEBUG TRAIN Batch 18/7900 loss 12.455057 loss_att 12.502028 loss_ctc 17.623352 loss_rnnt 11.692527 hw_loss 0.120057 lr 0.00039772 rank 2
2023-02-21 20:00:21,033 DEBUG TRAIN Batch 18/7900 loss 10.799748 loss_att 11.537874 loss_ctc 10.553543 loss_rnnt 10.626922 hw_loss 0.108805 lr 0.00039769 rank 7
2023-02-21 20:00:21,034 DEBUG TRAIN Batch 18/7900 loss 14.236791 loss_att 18.481653 loss_ctc 20.508961 loss_rnnt 12.445620 hw_loss 0.198579 lr 0.00039776 rank 1
2023-02-21 20:00:21,034 DEBUG TRAIN Batch 18/7900 loss 12.279995 loss_att 12.148866 loss_ctc 12.047705 loss_rnnt 12.201934 hw_loss 0.253612 lr 0.00039775 rank 4
2023-02-21 20:00:21,035 DEBUG TRAIN Batch 18/7900 loss 19.326588 loss_att 23.137091 loss_ctc 29.828232 loss_rnnt 17.148874 hw_loss 0.028862 lr 0.00039773 rank 5
2023-02-21 20:00:21,035 DEBUG TRAIN Batch 18/7900 loss 15.350039 loss_att 21.109467 loss_ctc 24.685392 loss_rnnt 12.939128 hw_loss 0.026835 lr 0.00039771 rank 6
2023-02-21 20:00:21,040 DEBUG TRAIN Batch 18/7900 loss 13.179103 loss_att 14.129128 loss_ctc 15.285056 loss_rnnt 12.633934 hw_loss 0.139444 lr 0.00039776 rank 0
2023-02-21 20:01:40,195 DEBUG TRAIN Batch 18/8000 loss 7.437270 loss_att 12.569502 loss_ctc 11.045967 loss_rnnt 5.890378 hw_loss 0.073661 lr 0.00039762 rank 4
2023-02-21 20:01:40,198 DEBUG TRAIN Batch 18/8000 loss 4.157116 loss_att 6.909942 loss_ctc 5.300564 loss_rnnt 3.399678 hw_loss 0.102024 lr 0.00039764 rank 1
2023-02-21 20:01:40,201 DEBUG TRAIN Batch 18/8000 loss 3.788879 loss_att 7.096056 loss_ctc 6.442063 loss_rnnt 2.727841 hw_loss 0.085959 lr 0.00039757 rank 7
2023-02-21 20:01:40,202 DEBUG TRAIN Batch 18/8000 loss 10.415564 loss_att 13.425377 loss_ctc 13.455125 loss_rnnt 9.340476 hw_loss 0.127220 lr 0.00039759 rank 2
2023-02-21 20:01:40,204 DEBUG TRAIN Batch 18/8000 loss 6.901983 loss_att 11.368605 loss_ctc 11.620584 loss_rnnt 5.336938 hw_loss 0.079825 lr 0.00039758 rank 6
2023-02-21 20:01:40,206 DEBUG TRAIN Batch 18/8000 loss 13.045007 loss_att 15.694394 loss_ctc 18.258888 loss_rnnt 11.784419 hw_loss 0.066611 lr 0.00039760 rank 5
2023-02-21 20:01:40,206 DEBUG TRAIN Batch 18/8000 loss 6.981634 loss_att 10.621603 loss_ctc 11.151480 loss_rnnt 5.648360 hw_loss 0.092439 lr 0.00039764 rank 0
2023-02-21 20:01:40,210 DEBUG TRAIN Batch 18/8000 loss 5.921210 loss_att 10.629198 loss_ctc 8.949364 loss_rnnt 4.535338 hw_loss 0.075974 lr 0.00039758 rank 3
2023-02-21 20:03:00,842 DEBUG TRAIN Batch 18/8100 loss 9.215954 loss_att 9.280848 loss_ctc 11.401070 loss_rnnt 8.886447 hw_loss 0.047212 lr 0.00039744 rank 7
2023-02-21 20:03:00,843 DEBUG TRAIN Batch 18/8100 loss 15.368391 loss_att 16.974789 loss_ctc 16.024498 loss_rnnt 14.870920 hw_loss 0.166329 lr 0.00039751 rank 0
2023-02-21 20:03:00,844 DEBUG TRAIN Batch 18/8100 loss 5.120894 loss_att 7.476087 loss_ctc 6.533805 loss_rnnt 4.402143 hw_loss 0.111233 lr 0.00039751 rank 1
2023-02-21 20:03:00,845 DEBUG TRAIN Batch 18/8100 loss 7.472221 loss_att 11.351603 loss_ctc 8.568371 loss_rnnt 6.483947 hw_loss 0.124208 lr 0.00039749 rank 4
2023-02-21 20:03:00,848 DEBUG TRAIN Batch 18/8100 loss 6.209311 loss_att 8.215842 loss_ctc 9.301702 loss_rnnt 5.310929 hw_loss 0.158919 lr 0.00039746 rank 6
2023-02-21 20:03:00,849 DEBUG TRAIN Batch 18/8100 loss 4.881100 loss_att 6.903267 loss_ctc 6.561274 loss_rnnt 4.158240 hw_loss 0.177005 lr 0.00039747 rank 2
2023-02-21 20:03:00,851 DEBUG TRAIN Batch 18/8100 loss 11.222225 loss_att 14.462826 loss_ctc 13.839770 loss_rnnt 10.146096 hw_loss 0.148130 lr 0.00039746 rank 3
2023-02-21 20:03:00,869 DEBUG TRAIN Batch 18/8100 loss 4.445835 loss_att 8.360430 loss_ctc 7.031066 loss_rnnt 3.209090 hw_loss 0.204615 lr 0.00039748 rank 5
2023-02-21 20:04:20,392 DEBUG TRAIN Batch 18/8200 loss 9.286373 loss_att 13.844743 loss_ctc 13.422480 loss_rnnt 7.776355 hw_loss 0.087869 lr 0.00039737 rank 4
2023-02-21 20:04:20,395 DEBUG TRAIN Batch 18/8200 loss 5.690013 loss_att 7.045151 loss_ctc 6.734587 loss_rnnt 5.229495 hw_loss 0.094151 lr 0.00039734 rank 2
2023-02-21 20:04:20,395 DEBUG TRAIN Batch 18/8200 loss 9.031450 loss_att 9.294138 loss_ctc 13.163803 loss_rnnt 8.387851 hw_loss 0.075154 lr 0.00039732 rank 7
2023-02-21 20:04:20,396 DEBUG TRAIN Batch 18/8200 loss 6.270307 loss_att 7.918527 loss_ctc 7.457068 loss_rnnt 5.709177 hw_loss 0.137344 lr 0.00039735 rank 5
2023-02-21 20:04:20,397 DEBUG TRAIN Batch 18/8200 loss 12.216979 loss_att 14.293941 loss_ctc 18.618567 loss_rnnt 10.892101 hw_loss 0.104886 lr 0.00039733 rank 6
2023-02-21 20:04:20,398 DEBUG TRAIN Batch 18/8200 loss 8.640995 loss_att 10.243071 loss_ctc 9.892748 loss_rnnt 8.027793 hw_loss 0.236038 lr 0.00039739 rank 0
2023-02-21 20:04:20,399 DEBUG TRAIN Batch 18/8200 loss 11.220965 loss_att 11.738729 loss_ctc 12.308643 loss_rnnt 10.899327 hw_loss 0.136991 lr 0.00039739 rank 1
2023-02-21 20:04:20,449 DEBUG TRAIN Batch 18/8200 loss 11.374705 loss_att 12.266002 loss_ctc 14.835089 loss_rnnt 10.655588 hw_loss 0.149013 lr 0.00039733 rank 3
2023-02-21 20:05:37,939 DEBUG TRAIN Batch 18/8300 loss 9.817597 loss_att 13.964726 loss_ctc 16.072010 loss_rnnt 8.076832 hw_loss 0.145158 lr 0.00039726 rank 1
2023-02-21 20:05:37,940 DEBUG TRAIN Batch 18/8300 loss 25.457125 loss_att 30.096533 loss_ctc 40.702457 loss_rnnt 22.482346 hw_loss 0.026597 lr 0.00039721 rank 3
2023-02-21 20:05:37,942 DEBUG TRAIN Batch 18/8300 loss 5.126895 loss_att 10.127674 loss_ctc 8.326571 loss_rnnt 3.665929 hw_loss 0.064102 lr 0.00039722 rank 2
2023-02-21 20:05:37,942 DEBUG TRAIN Batch 18/8300 loss 6.323718 loss_att 10.958267 loss_ctc 13.603217 loss_rnnt 4.389053 hw_loss 0.069665 lr 0.00039719 rank 7
2023-02-21 20:05:37,944 DEBUG TRAIN Batch 18/8300 loss 9.481111 loss_att 11.564259 loss_ctc 13.193653 loss_rnnt 8.512831 hw_loss 0.106208 lr 0.00039723 rank 5
2023-02-21 20:05:37,945 DEBUG TRAIN Batch 18/8300 loss 10.439505 loss_att 10.410872 loss_ctc 13.117447 loss_rnnt 9.883749 hw_loss 0.383294 lr 0.00039724 rank 4
2023-02-21 20:05:37,947 DEBUG TRAIN Batch 18/8300 loss 4.067925 loss_att 8.202607 loss_ctc 7.555982 loss_rnnt 2.713061 hw_loss 0.117851 lr 0.00039726 rank 0
2023-02-21 20:05:37,992 DEBUG TRAIN Batch 18/8300 loss 4.554085 loss_att 5.867660 loss_ctc 5.021286 loss_rnnt 4.103166 hw_loss 0.236082 lr 0.00039721 rank 6
2023-02-21 20:06:35,950 DEBUG CV Batch 18/0 loss 1.697836 loss_att 1.690742 loss_ctc 2.180454 loss_rnnt 1.371640 hw_loss 0.493623 history loss 1.634953 rank 0
2023-02-21 20:06:35,950 DEBUG CV Batch 18/0 loss 1.697836 loss_att 1.690742 loss_ctc 2.180454 loss_rnnt 1.371640 hw_loss 0.493623 history loss 1.634953 rank 4
2023-02-21 20:06:35,950 DEBUG CV Batch 18/0 loss 1.697836 loss_att 1.690742 loss_ctc 2.180454 loss_rnnt 1.371640 hw_loss 0.493623 history loss 1.634953 rank 2
2023-02-21 20:06:35,953 DEBUG CV Batch 18/0 loss 1.697836 loss_att 1.690742 loss_ctc 2.180454 loss_rnnt 1.371640 hw_loss 0.493623 history loss 1.634953 rank 6
2023-02-21 20:06:35,954 DEBUG CV Batch 18/0 loss 1.697836 loss_att 1.690742 loss_ctc 2.180454 loss_rnnt 1.371640 hw_loss 0.493623 history loss 1.634953 rank 3
2023-02-21 20:06:35,970 DEBUG CV Batch 18/0 loss 1.697836 loss_att 1.690742 loss_ctc 2.180454 loss_rnnt 1.371640 hw_loss 0.493623 history loss 1.634953 rank 7
2023-02-21 20:06:35,971 DEBUG CV Batch 18/0 loss 1.697836 loss_att 1.690742 loss_ctc 2.180454 loss_rnnt 1.371640 hw_loss 0.493623 history loss 1.634953 rank 1
2023-02-21 20:06:35,974 DEBUG CV Batch 18/0 loss 1.697836 loss_att 1.690742 loss_ctc 2.180454 loss_rnnt 1.371640 hw_loss 0.493623 history loss 1.634953 rank 5
2023-02-21 20:06:47,034 DEBUG CV Batch 18/100 loss 7.446734 loss_att 8.378979 loss_ctc 9.644451 loss_rnnt 6.913711 hw_loss 0.100397 history loss 3.688063 rank 7
2023-02-21 20:06:47,106 DEBUG CV Batch 18/100 loss 7.446734 loss_att 8.378979 loss_ctc 9.644451 loss_rnnt 6.913711 hw_loss 0.100397 history loss 3.688062 rank 2
2023-02-21 20:06:47,129 DEBUG CV Batch 18/100 loss 7.446734 loss_att 8.378979 loss_ctc 9.644451 loss_rnnt 6.913711 hw_loss 0.100397 history loss 3.688062 rank 4
2023-02-21 20:06:47,180 DEBUG CV Batch 18/100 loss 7.446734 loss_att 8.378979 loss_ctc 9.644451 loss_rnnt 6.913711 hw_loss 0.100397 history loss 3.688062 rank 1
2023-02-21 20:06:47,204 DEBUG CV Batch 18/100 loss 7.446734 loss_att 8.378979 loss_ctc 9.644451 loss_rnnt 6.913711 hw_loss 0.100397 history loss 3.688063 rank 6
2023-02-21 20:06:47,221 DEBUG CV Batch 18/100 loss 7.446734 loss_att 8.378979 loss_ctc 9.644451 loss_rnnt 6.913711 hw_loss 0.100397 history loss 3.688063 rank 5
2023-02-21 20:06:47,241 DEBUG CV Batch 18/100 loss 7.446734 loss_att 8.378979 loss_ctc 9.644451 loss_rnnt 6.913711 hw_loss 0.100397 history loss 3.688063 rank 0
2023-02-21 20:06:47,334 DEBUG CV Batch 18/100 loss 7.446734 loss_att 8.378979 loss_ctc 9.644451 loss_rnnt 6.913711 hw_loss 0.100397 history loss 3.688062 rank 3
2023-02-21 20:07:00,312 DEBUG CV Batch 18/200 loss 7.864884 loss_att 14.709191 loss_ctc 11.088937 loss_rnnt 5.996634 hw_loss 0.130341 history loss 4.233503 rank 7
2023-02-21 20:07:00,380 DEBUG CV Batch 18/200 loss 7.864884 loss_att 14.709191 loss_ctc 11.088937 loss_rnnt 5.996634 hw_loss 0.130341 history loss 4.233503 rank 2
2023-02-21 20:07:00,424 DEBUG CV Batch 18/200 loss 7.864884 loss_att 14.709191 loss_ctc 11.088937 loss_rnnt 5.996634 hw_loss 0.130341 history loss 4.233502 rank 4
2023-02-21 20:07:00,541 DEBUG CV Batch 18/200 loss 7.864884 loss_att 14.709191 loss_ctc 11.088937 loss_rnnt 5.996634 hw_loss 0.130341 history loss 4.233503 rank 1
2023-02-21 20:07:00,550 DEBUG CV Batch 18/200 loss 7.864884 loss_att 14.709191 loss_ctc 11.088937 loss_rnnt 5.996634 hw_loss 0.130341 history loss 4.233503 rank 5
2023-02-21 20:07:00,662 DEBUG CV Batch 18/200 loss 7.864884 loss_att 14.709191 loss_ctc 11.088937 loss_rnnt 5.996634 hw_loss 0.130341 history loss 4.233502 rank 0
2023-02-21 20:07:00,679 DEBUG CV Batch 18/200 loss 7.864884 loss_att 14.709191 loss_ctc 11.088937 loss_rnnt 5.996634 hw_loss 0.130341 history loss 4.233502 rank 6
2023-02-21 20:07:00,760 DEBUG CV Batch 18/200 loss 7.864884 loss_att 14.709191 loss_ctc 11.088937 loss_rnnt 5.996634 hw_loss 0.130341 history loss 4.233503 rank 3
2023-02-21 20:07:12,328 DEBUG CV Batch 18/300 loss 4.777349 loss_att 5.303841 loss_ctc 6.679145 loss_rnnt 4.299684 hw_loss 0.222741 history loss 4.381996 rank 7
2023-02-21 20:07:12,452 DEBUG CV Batch 18/300 loss 4.777349 loss_att 5.303841 loss_ctc 6.679145 loss_rnnt 4.299684 hw_loss 0.222741 history loss 4.381996 rank 4
2023-02-21 20:07:12,490 DEBUG CV Batch 18/300 loss 4.777349 loss_att 5.303841 loss_ctc 6.679145 loss_rnnt 4.299684 hw_loss 0.222741 history loss 4.381996 rank 2
2023-02-21 20:07:12,569 DEBUG CV Batch 18/300 loss 4.777349 loss_att 5.303841 loss_ctc 6.679145 loss_rnnt 4.299684 hw_loss 0.222741 history loss 4.381996 rank 1
2023-02-21 20:07:12,723 DEBUG CV Batch 18/300 loss 4.777349 loss_att 5.303841 loss_ctc 6.679145 loss_rnnt 4.299684 hw_loss 0.222741 history loss 4.381996 rank 5
2023-02-21 20:07:12,855 DEBUG CV Batch 18/300 loss 4.777349 loss_att 5.303841 loss_ctc 6.679145 loss_rnnt 4.299684 hw_loss 0.222741 history loss 4.381996 rank 6
2023-02-21 20:07:12,874 DEBUG CV Batch 18/300 loss 4.777349 loss_att 5.303841 loss_ctc 6.679145 loss_rnnt 4.299684 hw_loss 0.222741 history loss 4.381996 rank 0
2023-02-21 20:07:13,899 DEBUG CV Batch 18/300 loss 4.777349 loss_att 5.303841 loss_ctc 6.679145 loss_rnnt 4.299684 hw_loss 0.222741 history loss 4.381996 rank 3
2023-02-21 20:07:24,269 DEBUG CV Batch 18/400 loss 28.764484 loss_att 116.389252 loss_ctc 17.172003 loss_rnnt 12.770245 hw_loss 0.028030 history loss 5.402937 rank 7
2023-02-21 20:07:24,404 DEBUG CV Batch 18/400 loss 28.764484 loss_att 116.389252 loss_ctc 17.172003 loss_rnnt 12.770245 hw_loss 0.028030 history loss 5.402937 rank 4
2023-02-21 20:07:24,409 DEBUG CV Batch 18/400 loss 28.764484 loss_att 116.389252 loss_ctc 17.172003 loss_rnnt 12.770245 hw_loss 0.028030 history loss 5.402937 rank 2
2023-02-21 20:07:24,509 DEBUG CV Batch 18/400 loss 28.764484 loss_att 116.389252 loss_ctc 17.172003 loss_rnnt 12.770245 hw_loss 0.028030 history loss 5.402937 rank 1
2023-02-21 20:07:24,770 DEBUG CV Batch 18/400 loss 28.764484 loss_att 116.389252 loss_ctc 17.172003 loss_rnnt 12.770245 hw_loss 0.028030 history loss 5.402937 rank 5
2023-02-21 20:07:24,893 DEBUG CV Batch 18/400 loss 28.764484 loss_att 116.389252 loss_ctc 17.172003 loss_rnnt 12.770245 hw_loss 0.028030 history loss 5.402937 rank 6
2023-02-21 20:07:24,924 DEBUG CV Batch 18/400 loss 28.764484 loss_att 116.389252 loss_ctc 17.172003 loss_rnnt 12.770245 hw_loss 0.028030 history loss 5.402937 rank 0
2023-02-21 20:07:26,582 DEBUG CV Batch 18/400 loss 28.764484 loss_att 116.389252 loss_ctc 17.172003 loss_rnnt 12.770245 hw_loss 0.028030 history loss 5.402937 rank 3
2023-02-21 20:07:34,701 DEBUG CV Batch 18/500 loss 5.340402 loss_att 5.455756 loss_ctc 5.655828 loss_rnnt 5.192810 hw_loss 0.154623 history loss 6.228216 rank 7
2023-02-21 20:07:34,968 DEBUG CV Batch 18/500 loss 5.340402 loss_att 5.455756 loss_ctc 5.655828 loss_rnnt 5.192810 hw_loss 0.154623 history loss 6.228216 rank 4
2023-02-21 20:07:35,004 DEBUG CV Batch 18/500 loss 5.340402 loss_att 5.455756 loss_ctc 5.655828 loss_rnnt 5.192810 hw_loss 0.154623 history loss 6.228216 rank 1
2023-02-21 20:07:35,049 DEBUG CV Batch 18/500 loss 5.340402 loss_att 5.455756 loss_ctc 5.655828 loss_rnnt 5.192810 hw_loss 0.154623 history loss 6.228216 rank 2
2023-02-21 20:07:35,374 DEBUG CV Batch 18/500 loss 5.340402 loss_att 5.455756 loss_ctc 5.655828 loss_rnnt 5.192810 hw_loss 0.154623 history loss 6.228216 rank 5
2023-02-21 20:07:35,599 DEBUG CV Batch 18/500 loss 5.340402 loss_att 5.455756 loss_ctc 5.655828 loss_rnnt 5.192810 hw_loss 0.154623 history loss 6.228216 rank 0
2023-02-21 20:07:35,619 DEBUG CV Batch 18/500 loss 5.340402 loss_att 5.455756 loss_ctc 5.655828 loss_rnnt 5.192810 hw_loss 0.154623 history loss 6.228216 rank 6
2023-02-21 20:07:37,377 DEBUG CV Batch 18/500 loss 5.340402 loss_att 5.455756 loss_ctc 5.655828 loss_rnnt 5.192810 hw_loss 0.154623 history loss 6.228216 rank 3
2023-02-21 20:07:46,678 DEBUG CV Batch 18/600 loss 7.888530 loss_att 8.264102 loss_ctc 10.061767 loss_rnnt 7.339910 hw_loss 0.344512 history loss 7.284804 rank 7
2023-02-21 20:07:46,952 DEBUG CV Batch 18/600 loss 7.888530 loss_att 8.264102 loss_ctc 10.061767 loss_rnnt 7.339910 hw_loss 0.344512 history loss 7.284804 rank 4
2023-02-21 20:07:46,953 DEBUG CV Batch 18/600 loss 7.888530 loss_att 8.264102 loss_ctc 10.061767 loss_rnnt 7.339910 hw_loss 0.344512 history loss 7.284804 rank 2
2023-02-21 20:07:47,059 DEBUG CV Batch 18/600 loss 7.888530 loss_att 8.264102 loss_ctc 10.061767 loss_rnnt 7.339910 hw_loss 0.344512 history loss 7.284804 rank 1
2023-02-21 20:07:47,573 DEBUG CV Batch 18/600 loss 7.888530 loss_att 8.264102 loss_ctc 10.061767 loss_rnnt 7.339910 hw_loss 0.344512 history loss 7.284804 rank 5
2023-02-21 20:07:47,705 DEBUG CV Batch 18/600 loss 7.888530 loss_att 8.264102 loss_ctc 10.061767 loss_rnnt 7.339910 hw_loss 0.344512 history loss 7.284804 rank 0
2023-02-21 20:07:47,814 DEBUG CV Batch 18/600 loss 7.888530 loss_att 8.264102 loss_ctc 10.061767 loss_rnnt 7.339910 hw_loss 0.344512 history loss 7.284804 rank 6
2023-02-21 20:07:49,544 DEBUG CV Batch 18/600 loss 7.888530 loss_att 8.264102 loss_ctc 10.061767 loss_rnnt 7.339910 hw_loss 0.344512 history loss 7.284804 rank 3
2023-02-21 20:07:57,970 DEBUG CV Batch 18/700 loss 16.282175 loss_att 43.097977 loss_ctc 18.829382 loss_rnnt 10.503835 hw_loss 0.141661 history loss 8.014965 rank 7
2023-02-21 20:07:58,235 DEBUG CV Batch 18/700 loss 16.282175 loss_att 43.097977 loss_ctc 18.829382 loss_rnnt 10.503835 hw_loss 0.141661 history loss 8.014965 rank 2
2023-02-21 20:07:58,396 DEBUG CV Batch 18/700 loss 16.282175 loss_att 43.097977 loss_ctc 18.829382 loss_rnnt 10.503835 hw_loss 0.141661 history loss 8.014965 rank 1
2023-02-21 20:07:58,448 DEBUG CV Batch 18/700 loss 16.282175 loss_att 43.097977 loss_ctc 18.829382 loss_rnnt 10.503835 hw_loss 0.141661 history loss 8.014965 rank 4
2023-02-21 20:07:58,961 DEBUG CV Batch 18/700 loss 16.282175 loss_att 43.097977 loss_ctc 18.829382 loss_rnnt 10.503835 hw_loss 0.141661 history loss 8.014965 rank 5
2023-02-21 20:07:59,196 DEBUG CV Batch 18/700 loss 16.282175 loss_att 43.097977 loss_ctc 18.829382 loss_rnnt 10.503835 hw_loss 0.141661 history loss 8.014965 rank 6
2023-02-21 20:07:59,579 DEBUG CV Batch 18/700 loss 16.282175 loss_att 43.097977 loss_ctc 18.829382 loss_rnnt 10.503835 hw_loss 0.141661 history loss 8.014965 rank 0
2023-02-21 20:08:01,041 DEBUG CV Batch 18/700 loss 16.282175 loss_att 43.097977 loss_ctc 18.829382 loss_rnnt 10.503835 hw_loss 0.141661 history loss 8.014965 rank 3
2023-02-21 20:08:09,102 DEBUG CV Batch 18/800 loss 10.273964 loss_att 10.318851 loss_ctc 15.290104 loss_rnnt 9.519465 hw_loss 0.143816 history loss 7.440615 rank 7
2023-02-21 20:08:09,388 DEBUG CV Batch 18/800 loss 10.273964 loss_att 10.318851 loss_ctc 15.290104 loss_rnnt 9.519465 hw_loss 0.143816 history loss 7.440615 rank 2
2023-02-21 20:08:09,541 DEBUG CV Batch 18/800 loss 10.273964 loss_att 10.318851 loss_ctc 15.290104 loss_rnnt 9.519465 hw_loss 0.143816 history loss 7.440615 rank 1
2023-02-21 20:08:09,613 DEBUG CV Batch 18/800 loss 10.273964 loss_att 10.318851 loss_ctc 15.290104 loss_rnnt 9.519465 hw_loss 0.143816 history loss 7.440615 rank 4
2023-02-21 20:08:10,295 DEBUG CV Batch 18/800 loss 10.273964 loss_att 10.318851 loss_ctc 15.290104 loss_rnnt 9.519465 hw_loss 0.143816 history loss 7.440615 rank 5
2023-02-21 20:08:10,516 DEBUG CV Batch 18/800 loss 10.273964 loss_att 10.318851 loss_ctc 15.290104 loss_rnnt 9.519465 hw_loss 0.143816 history loss 7.440615 rank 6
2023-02-21 20:08:11,146 DEBUG CV Batch 18/800 loss 10.273964 loss_att 10.318851 loss_ctc 15.290104 loss_rnnt 9.519465 hw_loss 0.143816 history loss 7.440615 rank 0
2023-02-21 20:08:12,327 DEBUG CV Batch 18/800 loss 10.273964 loss_att 10.318851 loss_ctc 15.290104 loss_rnnt 9.519465 hw_loss 0.143816 history loss 7.440615 rank 3
2023-02-21 20:08:22,162 DEBUG CV Batch 18/900 loss 12.590292 loss_att 20.408165 loss_ctc 19.430408 loss_rnnt 10.088963 hw_loss 0.048260 history loss 7.205785 rank 7
2023-02-21 20:08:22,583 DEBUG CV Batch 18/900 loss 12.590292 loss_att 20.408165 loss_ctc 19.430408 loss_rnnt 10.088963 hw_loss 0.048260 history loss 7.205785 rank 2
2023-02-21 20:08:22,703 DEBUG CV Batch 18/900 loss 12.590292 loss_att 20.408165 loss_ctc 19.430408 loss_rnnt 10.088963 hw_loss 0.048260 history loss 7.205785 rank 1
2023-02-21 20:08:22,735 DEBUG CV Batch 18/900 loss 12.590292 loss_att 20.408165 loss_ctc 19.430408 loss_rnnt 10.088963 hw_loss 0.048260 history loss 7.205785 rank 4
2023-02-21 20:08:23,547 DEBUG CV Batch 18/900 loss 12.590292 loss_att 20.408165 loss_ctc 19.430408 loss_rnnt 10.088963 hw_loss 0.048260 history loss 7.205785 rank 5
2023-02-21 20:08:23,736 DEBUG CV Batch 18/900 loss 12.590292 loss_att 20.408165 loss_ctc 19.430408 loss_rnnt 10.088963 hw_loss 0.048260 history loss 7.205785 rank 6
2023-02-21 20:08:24,474 DEBUG CV Batch 18/900 loss 12.590292 loss_att 20.408165 loss_ctc 19.430408 loss_rnnt 10.088963 hw_loss 0.048260 history loss 7.205785 rank 0
2023-02-21 20:08:25,710 DEBUG CV Batch 18/900 loss 12.590292 loss_att 20.408165 loss_ctc 19.430408 loss_rnnt 10.088963 hw_loss 0.048260 history loss 7.205785 rank 3
2023-02-21 20:08:34,156 DEBUG CV Batch 18/1000 loss 4.277482 loss_att 5.127693 loss_ctc 4.670522 loss_rnnt 3.963516 hw_loss 0.171596 history loss 6.947982 rank 7
2023-02-21 20:08:34,641 DEBUG CV Batch 18/1000 loss 4.277482 loss_att 5.127693 loss_ctc 4.670522 loss_rnnt 3.963516 hw_loss 0.171596 history loss 6.947982 rank 2
2023-02-21 20:08:34,683 DEBUG CV Batch 18/1000 loss 4.277482 loss_att 5.127693 loss_ctc 4.670522 loss_rnnt 3.963516 hw_loss 0.171596 history loss 6.947982 rank 1
2023-02-21 20:08:34,717 DEBUG CV Batch 18/1000 loss 4.277482 loss_att 5.127693 loss_ctc 4.670522 loss_rnnt 3.963516 hw_loss 0.171596 history loss 6.947982 rank 4
2023-02-21 20:08:35,659 DEBUG CV Batch 18/1000 loss 4.277482 loss_att 5.127693 loss_ctc 4.670522 loss_rnnt 3.963516 hw_loss 0.171596 history loss 6.947982 rank 5
2023-02-21 20:08:35,983 DEBUG CV Batch 18/1000 loss 4.277482 loss_att 5.127693 loss_ctc 4.670522 loss_rnnt 3.963516 hw_loss 0.171596 history loss 6.947982 rank 6
2023-02-21 20:08:36,751 DEBUG CV Batch 18/1000 loss 4.277482 loss_att 5.127693 loss_ctc 4.670522 loss_rnnt 3.963516 hw_loss 0.171596 history loss 6.947982 rank 0
2023-02-21 20:08:37,978 DEBUG CV Batch 18/1000 loss 4.277482 loss_att 5.127693 loss_ctc 4.670522 loss_rnnt 3.963516 hw_loss 0.171596 history loss 6.947982 rank 3
2023-02-21 20:08:45,950 DEBUG CV Batch 18/1100 loss 6.949543 loss_att 6.243146 loss_ctc 9.320911 loss_rnnt 6.542109 hw_loss 0.435995 history loss 6.918986 rank 7
2023-02-21 20:08:46,466 DEBUG CV Batch 18/1100 loss 6.949543 loss_att 6.243146 loss_ctc 9.320911 loss_rnnt 6.542109 hw_loss 0.435995 history loss 6.918986 rank 1
2023-02-21 20:08:46,559 DEBUG CV Batch 18/1100 loss 6.949543 loss_att 6.243146 loss_ctc 9.320911 loss_rnnt 6.542109 hw_loss 0.435995 history loss 6.918986 rank 4
2023-02-21 20:08:46,788 DEBUG CV Batch 18/1100 loss 6.949543 loss_att 6.243146 loss_ctc 9.320911 loss_rnnt 6.542109 hw_loss 0.435995 history loss 6.918986 rank 2
2023-02-21 20:08:47,556 DEBUG CV Batch 18/1100 loss 6.949543 loss_att 6.243146 loss_ctc 9.320911 loss_rnnt 6.542109 hw_loss 0.435995 history loss 6.918986 rank 5
2023-02-21 20:08:47,995 DEBUG CV Batch 18/1100 loss 6.949543 loss_att 6.243146 loss_ctc 9.320911 loss_rnnt 6.542109 hw_loss 0.435995 history loss 6.918986 rank 6
2023-02-21 20:08:48,743 DEBUG CV Batch 18/1100 loss 6.949543 loss_att 6.243146 loss_ctc 9.320911 loss_rnnt 6.542109 hw_loss 0.435995 history loss 6.918986 rank 0
2023-02-21 20:08:49,932 DEBUG CV Batch 18/1100 loss 6.949543 loss_att 6.243146 loss_ctc 9.320911 loss_rnnt 6.542109 hw_loss 0.435995 history loss 6.918986 rank 3
2023-02-21 20:08:56,357 DEBUG CV Batch 18/1200 loss 8.924911 loss_att 8.483466 loss_ctc 9.375085 loss_rnnt 8.862271 hw_loss 0.170447 history loss 7.269239 rank 7
2023-02-21 20:08:56,904 DEBUG CV Batch 18/1200 loss 8.924911 loss_att 8.483466 loss_ctc 9.375085 loss_rnnt 8.862271 hw_loss 0.170447 history loss 7.269239 rank 1
2023-02-21 20:08:56,974 DEBUG CV Batch 18/1200 loss 8.924911 loss_att 8.483466 loss_ctc 9.375085 loss_rnnt 8.862271 hw_loss 0.170447 history loss 7.269239 rank 4
2023-02-21 20:08:57,315 DEBUG CV Batch 18/1200 loss 8.924911 loss_att 8.483466 loss_ctc 9.375085 loss_rnnt 8.862271 hw_loss 0.170447 history loss 7.269239 rank 2
2023-02-21 20:08:58,201 DEBUG CV Batch 18/1200 loss 8.924911 loss_att 8.483466 loss_ctc 9.375085 loss_rnnt 8.862271 hw_loss 0.170447 history loss 7.269239 rank 5
2023-02-21 20:08:58,733 DEBUG CV Batch 18/1200 loss 8.924911 loss_att 8.483466 loss_ctc 9.375085 loss_rnnt 8.862271 hw_loss 0.170447 history loss 7.269239 rank 6
2023-02-21 20:08:59,376 DEBUG CV Batch 18/1200 loss 8.924911 loss_att 8.483466 loss_ctc 9.375085 loss_rnnt 8.862271 hw_loss 0.170447 history loss 7.269239 rank 0
2023-02-21 20:09:00,564 DEBUG CV Batch 18/1200 loss 8.924911 loss_att 8.483466 loss_ctc 9.375085 loss_rnnt 8.862271 hw_loss 0.170447 history loss 7.269239 rank 3
2023-02-21 20:09:08,272 DEBUG CV Batch 18/1300 loss 5.758694 loss_att 5.820057 loss_ctc 8.652411 loss_rnnt 5.221567 hw_loss 0.260671 history loss 7.626476 rank 7
2023-02-21 20:09:08,815 DEBUG CV Batch 18/1300 loss 5.758694 loss_att 5.820057 loss_ctc 8.652411 loss_rnnt 5.221567 hw_loss 0.260671 history loss 7.626476 rank 1
2023-02-21 20:09:08,835 DEBUG CV Batch 18/1300 loss 5.758694 loss_att 5.820057 loss_ctc 8.652411 loss_rnnt 5.221567 hw_loss 0.260671 history loss 7.626476 rank 4
2023-02-21 20:09:09,264 DEBUG CV Batch 18/1300 loss 5.758694 loss_att 5.820057 loss_ctc 8.652411 loss_rnnt 5.221567 hw_loss 0.260671 history loss 7.626476 rank 2
2023-02-21 20:09:10,236 DEBUG CV Batch 18/1300 loss 5.758694 loss_att 5.820057 loss_ctc 8.652411 loss_rnnt 5.221567 hw_loss 0.260671 history loss 7.626476 rank 5
2023-02-21 20:09:10,729 DEBUG CV Batch 18/1300 loss 5.758694 loss_att 5.820057 loss_ctc 8.652411 loss_rnnt 5.221567 hw_loss 0.260671 history loss 7.626476 rank 6
2023-02-21 20:09:11,422 DEBUG CV Batch 18/1300 loss 5.758694 loss_att 5.820057 loss_ctc 8.652411 loss_rnnt 5.221567 hw_loss 0.260671 history loss 7.626476 rank 0
2023-02-21 20:09:12,688 DEBUG CV Batch 18/1300 loss 5.758694 loss_att 5.820057 loss_ctc 8.652411 loss_rnnt 5.221567 hw_loss 0.260671 history loss 7.626476 rank 3
2023-02-21 20:09:19,377 DEBUG CV Batch 18/1400 loss 6.094102 loss_att 24.000509 loss_ctc 3.823271 loss_rnnt 2.800648 hw_loss 0.028030 history loss 7.978681 rank 7
2023-02-21 20:09:19,953 DEBUG CV Batch 18/1400 loss 6.094102 loss_att 24.000509 loss_ctc 3.823271 loss_rnnt 2.800648 hw_loss 0.028030 history loss 7.978681 rank 1
2023-02-21 20:09:20,033 DEBUG CV Batch 18/1400 loss 6.094102 loss_att 24.000509 loss_ctc 3.823271 loss_rnnt 2.800648 hw_loss 0.028030 history loss 7.978681 rank 4
2023-02-21 20:09:20,426 DEBUG CV Batch 18/1400 loss 6.094102 loss_att 24.000509 loss_ctc 3.823271 loss_rnnt 2.800648 hw_loss 0.028030 history loss 7.978681 rank 2
2023-02-21 20:09:21,403 DEBUG CV Batch 18/1400 loss 6.094102 loss_att 24.000509 loss_ctc 3.823271 loss_rnnt 2.800648 hw_loss 0.028030 history loss 7.978681 rank 5
2023-02-21 20:09:21,936 DEBUG CV Batch 18/1400 loss 6.094102 loss_att 24.000509 loss_ctc 3.823271 loss_rnnt 2.800648 hw_loss 0.028030 history loss 7.978681 rank 6
2023-02-21 20:09:23,395 DEBUG CV Batch 18/1400 loss 6.094102 loss_att 24.000509 loss_ctc 3.823271 loss_rnnt 2.800648 hw_loss 0.028030 history loss 7.978681 rank 0
2023-02-21 20:09:24,105 DEBUG CV Batch 18/1400 loss 6.094102 loss_att 24.000509 loss_ctc 3.823271 loss_rnnt 2.800648 hw_loss 0.028030 history loss 7.978681 rank 3
2023-02-21 20:09:30,842 DEBUG CV Batch 18/1500 loss 7.507203 loss_att 7.790036 loss_ctc 6.708715 loss_rnnt 7.477416 hw_loss 0.149408 history loss 7.791618 rank 7
2023-02-21 20:09:31,298 DEBUG CV Batch 18/1500 loss 7.507203 loss_att 7.790036 loss_ctc 6.708715 loss_rnnt 7.477416 hw_loss 0.149408 history loss 7.791618 rank 1
2023-02-21 20:09:31,389 DEBUG CV Batch 18/1500 loss 7.507203 loss_att 7.790036 loss_ctc 6.708715 loss_rnnt 7.477416 hw_loss 0.149408 history loss 7.791618 rank 4
2023-02-21 20:09:31,750 DEBUG CV Batch 18/1500 loss 7.507203 loss_att 7.790036 loss_ctc 6.708715 loss_rnnt 7.477416 hw_loss 0.149408 history loss 7.791618 rank 2
2023-02-21 20:09:32,963 DEBUG CV Batch 18/1500 loss 7.507203 loss_att 7.790036 loss_ctc 6.708715 loss_rnnt 7.477416 hw_loss 0.149408 history loss 7.791618 rank 5
2023-02-21 20:09:33,366 DEBUG CV Batch 18/1500 loss 7.507203 loss_att 7.790036 loss_ctc 6.708715 loss_rnnt 7.477416 hw_loss 0.149408 history loss 7.791618 rank 6
2023-02-21 20:09:35,567 DEBUG CV Batch 18/1500 loss 7.507203 loss_att 7.790036 loss_ctc 6.708715 loss_rnnt 7.477416 hw_loss 0.149408 history loss 7.791618 rank 3
2023-02-21 20:09:35,754 DEBUG CV Batch 18/1500 loss 7.507203 loss_att 7.790036 loss_ctc 6.708715 loss_rnnt 7.477416 hw_loss 0.149408 history loss 7.791618 rank 0
2023-02-21 20:09:44,155 DEBUG CV Batch 18/1600 loss 10.056517 loss_att 14.465180 loss_ctc 13.952333 loss_rnnt 8.592920 hw_loss 0.117041 history loss 7.707549 rank 7
2023-02-21 20:09:44,262 DEBUG CV Batch 18/1600 loss 10.056517 loss_att 14.465180 loss_ctc 13.952333 loss_rnnt 8.592920 hw_loss 0.117041 history loss 7.707549 rank 4
2023-02-21 20:09:44,337 DEBUG CV Batch 18/1600 loss 10.056517 loss_att 14.465180 loss_ctc 13.952333 loss_rnnt 8.592920 hw_loss 0.117041 history loss 7.707549 rank 1
2023-02-21 20:09:44,677 DEBUG CV Batch 18/1600 loss 10.056517 loss_att 14.465180 loss_ctc 13.952333 loss_rnnt 8.592920 hw_loss 0.117041 history loss 7.707549 rank 2
2023-02-21 20:09:45,965 DEBUG CV Batch 18/1600 loss 10.056517 loss_att 14.465180 loss_ctc 13.952333 loss_rnnt 8.592920 hw_loss 0.117041 history loss 7.707549 rank 5
2023-02-21 20:09:46,317 DEBUG CV Batch 18/1600 loss 10.056517 loss_att 14.465180 loss_ctc 13.952333 loss_rnnt 8.592920 hw_loss 0.117041 history loss 7.707549 rank 6
2023-02-21 20:09:48,516 DEBUG CV Batch 18/1600 loss 10.056517 loss_att 14.465180 loss_ctc 13.952333 loss_rnnt 8.592920 hw_loss 0.117041 history loss 7.707549 rank 3
2023-02-21 20:09:48,761 DEBUG CV Batch 18/1600 loss 10.056517 loss_att 14.465180 loss_ctc 13.952333 loss_rnnt 8.592920 hw_loss 0.117041 history loss 7.707549 rank 0
2023-02-21 20:09:56,383 DEBUG CV Batch 18/1700 loss 8.845935 loss_att 8.468597 loss_ctc 13.589794 loss_rnnt 8.138304 hw_loss 0.282346 history loss 7.594742 rank 7
2023-02-21 20:09:56,403 DEBUG CV Batch 18/1700 loss 8.845935 loss_att 8.468597 loss_ctc 13.589794 loss_rnnt 8.138304 hw_loss 0.282346 history loss 7.594742 rank 4
2023-02-21 20:09:56,554 DEBUG CV Batch 18/1700 loss 8.845935 loss_att 8.468597 loss_ctc 13.589794 loss_rnnt 8.138304 hw_loss 0.282346 history loss 7.594742 rank 1
2023-02-21 20:09:56,915 DEBUG CV Batch 18/1700 loss 8.845935 loss_att 8.468597 loss_ctc 13.589794 loss_rnnt 8.138304 hw_loss 0.282346 history loss 7.594742 rank 2
2023-02-21 20:09:58,409 DEBUG CV Batch 18/1700 loss 8.845935 loss_att 8.468597 loss_ctc 13.589794 loss_rnnt 8.138304 hw_loss 0.282346 history loss 7.594742 rank 5
2023-02-21 20:09:58,571 DEBUG CV Batch 18/1700 loss 8.845935 loss_att 8.468597 loss_ctc 13.589794 loss_rnnt 8.138304 hw_loss 0.282346 history loss 7.594742 rank 6
2023-02-21 20:10:00,706 DEBUG CV Batch 18/1700 loss 8.845935 loss_att 8.468597 loss_ctc 13.589794 loss_rnnt 8.138304 hw_loss 0.282346 history loss 7.594742 rank 3
2023-02-21 20:10:01,010 DEBUG CV Batch 18/1700 loss 8.845935 loss_att 8.468597 loss_ctc 13.589794 loss_rnnt 8.138304 hw_loss 0.282346 history loss 7.594742 rank 0
2023-02-21 20:10:05,359 INFO Epoch 18 CV info cv_loss 7.5542779382162655
2023-02-21 20:10:05,360 INFO Epoch 19 TRAIN info lr 0.00039719709864530537
2023-02-21 20:10:05,364 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 20:10:05,517 INFO Epoch 18 CV info cv_loss 7.554277938974353
2023-02-21 20:10:05,518 INFO Epoch 19 TRAIN info lr 0.00039723219514365733
2023-02-21 20:10:05,518 INFO Epoch 18 CV info cv_loss 7.554277939336168
2023-02-21 20:10:05,519 INFO Epoch 19 TRAIN info lr 0.00039710814563778256
2023-02-21 20:10:05,520 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 20:10:05,524 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 20:10:05,886 INFO Epoch 18 CV info cv_loss 7.55427793881929
2023-02-21 20:10:05,887 INFO Epoch 19 TRAIN info lr 0.00039715449397243385
2023-02-21 20:10:05,890 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 20:10:07,378 INFO Epoch 18 CV info cv_loss 7.554277939374934
2023-02-21 20:10:07,379 INFO Epoch 19 TRAIN info lr 0.0003972058718977681
2023-02-21 20:10:07,383 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 20:10:07,517 INFO Epoch 18 CV info cv_loss 7.554277938147348
2023-02-21 20:10:07,519 INFO Epoch 19 TRAIN info lr 0.0003971670233353908
2023-02-21 20:10:07,523 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 20:10:09,715 INFO Epoch 18 CV info cv_loss 7.554277940421612
2023-02-21 20:10:09,716 INFO Epoch 19 TRAIN info lr 0.00039714948255926047
2023-02-21 20:10:09,719 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 20:10:11,024 INFO Epoch 18 CV info cv_loss 7.554277938267953
2023-02-21 20:10:11,025 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_finetune/18.pt
2023-02-21 20:10:16,168 INFO Epoch 19 TRAIN info lr 0.00039720461854039486
2023-02-21 20:10:16,174 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-21 20:11:27,562 DEBUG TRAIN Batch 19/0 loss 9.763719 loss_att 9.219738 loss_ctc 13.159427 loss_rnnt 9.186785 hw_loss 0.436819 lr 0.00039720 rank 4
2023-02-21 20:11:27,566 DEBUG TRAIN Batch 19/0 loss 12.835820 loss_att 12.212243 loss_ctc 15.843362 loss_rnnt 12.363436 hw_loss 0.367675 lr 0.00039711 rank 7
2023-02-21 20:11:27,571 DEBUG TRAIN Batch 19/0 loss 7.223146 loss_att 7.225937 loss_ctc 9.213058 loss_rnnt 6.768563 hw_loss 0.353820 lr 0.00039723 rank 1
2023-02-21 20:11:27,573 DEBUG TRAIN Batch 19/0 loss 9.809684 loss_att 8.617809 loss_ctc 10.985619 loss_rnnt 9.623133 hw_loss 0.502753 lr 0.00039720 rank 5
2023-02-21 20:11:27,574 DEBUG TRAIN Batch 19/0 loss 11.585006 loss_att 11.909355 loss_ctc 15.190607 loss_rnnt 10.935824 hw_loss 0.194183 lr 0.00039717 rank 6
2023-02-21 20:11:27,575 DEBUG TRAIN Batch 19/0 loss 8.848732 loss_att 9.950907 loss_ctc 12.260737 loss_rnnt 7.939311 hw_loss 0.438848 lr 0.00039715 rank 2
2023-02-21 20:11:27,575 DEBUG TRAIN Batch 19/0 loss 7.907786 loss_att 7.788970 loss_ctc 10.466109 loss_rnnt 7.417249 hw_loss 0.324732 lr 0.00039715 rank 3
2023-02-21 20:11:27,591 DEBUG TRAIN Batch 19/0 loss 10.284275 loss_att 10.131433 loss_ctc 13.313982 loss_rnnt 9.756309 hw_loss 0.289826 lr 0.00039720 rank 0
2023-02-21 20:12:45,048 DEBUG TRAIN Batch 19/100 loss 5.111554 loss_att 8.421713 loss_ctc 9.557788 loss_rnnt 3.808740 hw_loss 0.089907 lr 0.00039707 rank 4
2023-02-21 20:12:45,054 DEBUG TRAIN Batch 19/100 loss 10.444468 loss_att 12.336054 loss_ctc 10.924902 loss_rnnt 9.873420 hw_loss 0.241260 lr 0.00039702 rank 3
2023-02-21 20:12:45,055 DEBUG TRAIN Batch 19/100 loss 7.420766 loss_att 8.602084 loss_ctc 10.411545 loss_rnnt 6.721896 hw_loss 0.119692 lr 0.00039711 rank 1
2023-02-21 20:12:45,055 DEBUG TRAIN Batch 19/100 loss 9.363246 loss_att 12.428652 loss_ctc 11.848145 loss_rnnt 8.384642 hw_loss 0.064132 lr 0.00039698 rank 7
2023-02-21 20:12:45,056 DEBUG TRAIN Batch 19/100 loss 10.749734 loss_att 10.955884 loss_ctc 14.385303 loss_rnnt 10.199436 hw_loss 0.045610 lr 0.00039703 rank 2
2023-02-21 20:12:45,058 DEBUG TRAIN Batch 19/100 loss 5.078388 loss_att 10.379959 loss_ctc 8.775187 loss_rnnt 3.469068 hw_loss 0.105185 lr 0.00039708 rank 5
2023-02-21 20:12:45,059 DEBUG TRAIN Batch 19/100 loss 8.096074 loss_att 11.843383 loss_ctc 10.907219 loss_rnnt 6.934095 hw_loss 0.070681 lr 0.00039708 rank 0
2023-02-21 20:12:45,102 DEBUG TRAIN Batch 19/100 loss 8.179992 loss_att 11.859183 loss_ctc 9.584229 loss_rnnt 7.130107 hw_loss 0.237778 lr 0.00039704 rank 6
2023-02-21 20:14:02,156 DEBUG TRAIN Batch 19/200 loss 7.075766 loss_att 6.848649 loss_ctc 8.071892 loss_rnnt 6.936980 hw_loss 0.096361 lr 0.00039698 rank 1
2023-02-21 20:14:02,157 DEBUG TRAIN Batch 19/200 loss 6.787620 loss_att 11.903159 loss_ctc 9.817141 loss_rnnt 5.311723 hw_loss 0.091598 lr 0.00039695 rank 4
2023-02-21 20:14:02,158 DEBUG TRAIN Batch 19/200 loss 13.566815 loss_att 14.619097 loss_ctc 17.819843 loss_rnnt 12.676031 hw_loss 0.212360 lr 0.00039690 rank 2
2023-02-21 20:14:02,164 DEBUG TRAIN Batch 19/200 loss 4.191506 loss_att 7.171754 loss_ctc 9.213006 loss_rnnt 2.854463 hw_loss 0.133988 lr 0.00039695 rank 0
2023-02-21 20:14:02,165 DEBUG TRAIN Batch 19/200 loss 11.148545 loss_att 14.451050 loss_ctc 16.365187 loss_rnnt 9.777479 hw_loss 0.028147 lr 0.00039686 rank 7
2023-02-21 20:14:02,167 DEBUG TRAIN Batch 19/200 loss 14.605153 loss_att 17.365101 loss_ctc 17.282469 loss_rnnt 13.622854 hw_loss 0.137504 lr 0.00039695 rank 5
2023-02-21 20:14:02,169 DEBUG TRAIN Batch 19/200 loss 5.333230 loss_att 7.602879 loss_ctc 9.318630 loss_rnnt 4.279323 hw_loss 0.128608 lr 0.00039690 rank 3
2023-02-21 20:14:02,209 DEBUG TRAIN Batch 19/200 loss 7.417440 loss_att 9.789058 loss_ctc 11.716159 loss_rnnt 6.320463 hw_loss 0.092796 lr 0.00039692 rank 6
2023-02-21 20:15:22,014 DEBUG TRAIN Batch 19/300 loss 13.765699 loss_att 15.343029 loss_ctc 17.771017 loss_rnnt 12.806602 hw_loss 0.205479 lr 0.00039682 rank 4
2023-02-21 20:15:22,017 DEBUG TRAIN Batch 19/300 loss 14.734187 loss_att 16.879992 loss_ctc 19.334568 loss_rnnt 13.591399 hw_loss 0.187956 lr 0.00039683 rank 0
2023-02-21 20:15:22,018 DEBUG TRAIN Batch 19/300 loss 7.826734 loss_att 11.381614 loss_ctc 11.350881 loss_rnnt 6.606242 hw_loss 0.074306 lr 0.00039679 rank 6
2023-02-21 20:15:22,019 DEBUG TRAIN Batch 19/300 loss 12.411754 loss_att 13.885891 loss_ctc 16.607037 loss_rnnt 11.524805 hw_loss 0.061404 lr 0.00039673 rank 7
2023-02-21 20:15:22,020 DEBUG TRAIN Batch 19/300 loss 6.307821 loss_att 8.148024 loss_ctc 7.706227 loss_rnnt 5.699073 hw_loss 0.101724 lr 0.00039683 rank 5
2023-02-21 20:15:22,019 DEBUG TRAIN Batch 19/300 loss 6.535516 loss_att 9.780568 loss_ctc 10.807693 loss_rnnt 5.279922 hw_loss 0.069300 lr 0.00039686 rank 1
2023-02-21 20:15:22,022 DEBUG TRAIN Batch 19/300 loss 15.245570 loss_att 20.107897 loss_ctc 26.630795 loss_rnnt 12.696497 hw_loss 0.109833 lr 0.00039678 rank 2
2023-02-21 20:15:22,022 DEBUG TRAIN Batch 19/300 loss 12.954018 loss_att 14.106362 loss_ctc 18.036602 loss_rnnt 11.940072 hw_loss 0.198371 lr 0.00039677 rank 3
2023-02-21 20:16:41,202 DEBUG TRAIN Batch 19/400 loss 11.530400 loss_att 13.390969 loss_ctc 13.556364 loss_rnnt 10.816753 hw_loss 0.133884 lr 0.00039661 rank 7
2023-02-21 20:16:41,202 DEBUG TRAIN Batch 19/400 loss 5.988251 loss_att 9.454813 loss_ctc 7.931127 loss_rnnt 4.976878 hw_loss 0.110644 lr 0.00039670 rank 4
2023-02-21 20:16:41,204 DEBUG TRAIN Batch 19/400 loss 16.754608 loss_att 18.814949 loss_ctc 21.843281 loss_rnnt 15.567279 hw_loss 0.181447 lr 0.00039665 rank 2
2023-02-21 20:16:41,208 DEBUG TRAIN Batch 19/400 loss 12.150624 loss_att 17.568344 loss_ctc 15.182243 loss_rnnt 10.570038 hw_loss 0.174048 lr 0.00039665 rank 3
2023-02-21 20:16:41,211 DEBUG TRAIN Batch 19/400 loss 13.011694 loss_att 14.614547 loss_ctc 17.676294 loss_rnnt 11.984623 hw_loss 0.158539 lr 0.00039667 rank 6
2023-02-21 20:16:41,211 DEBUG TRAIN Batch 19/400 loss 4.147274 loss_att 8.262772 loss_ctc 6.579554 loss_rnnt 2.944958 hw_loss 0.102960 lr 0.00039673 rank 1
2023-02-21 20:16:41,215 DEBUG TRAIN Batch 19/400 loss 5.690724 loss_att 10.602131 loss_ctc 6.458548 loss_rnnt 4.579195 hw_loss 0.050384 lr 0.00039670 rank 5
2023-02-21 20:16:41,257 DEBUG TRAIN Batch 19/400 loss 4.276406 loss_att 6.499578 loss_ctc 4.848325 loss_rnnt 3.656762 hw_loss 0.185163 lr 0.00039670 rank 0
2023-02-21 20:17:59,629 DEBUG TRAIN Batch 19/500 loss 7.410421 loss_att 9.820089 loss_ctc 12.933169 loss_rnnt 6.157334 hw_loss 0.065224 lr 0.00039653 rank 2
2023-02-21 20:17:59,630 DEBUG TRAIN Batch 19/500 loss 7.220014 loss_att 11.335494 loss_ctc 9.046175 loss_rnnt 6.071228 hw_loss 0.154129 lr 0.00039658 rank 0
2023-02-21 20:17:59,633 DEBUG TRAIN Batch 19/500 loss 10.842369 loss_att 15.538600 loss_ctc 17.209282 loss_rnnt 8.938293 hw_loss 0.217328 lr 0.00039657 rank 4
2023-02-21 20:17:59,633 DEBUG TRAIN Batch 19/500 loss 4.468376 loss_att 7.009253 loss_ctc 6.555324 loss_rnnt 3.612789 hw_loss 0.129660 lr 0.00039648 rank 7
2023-02-21 20:17:59,633 DEBUG TRAIN Batch 19/500 loss 5.473994 loss_att 7.900197 loss_ctc 7.144899 loss_rnnt 4.659976 hw_loss 0.198731 lr 0.00039654 rank 6
2023-02-21 20:17:59,636 DEBUG TRAIN Batch 19/500 loss 12.125946 loss_att 14.212801 loss_ctc 16.933491 loss_rnnt 11.008694 hw_loss 0.110392 lr 0.00039661 rank 1
2023-02-21 20:17:59,638 DEBUG TRAIN Batch 19/500 loss 12.009969 loss_att 13.751942 loss_ctc 16.646479 loss_rnnt 10.901711 hw_loss 0.265616 lr 0.00039658 rank 5
2023-02-21 20:17:59,638 DEBUG TRAIN Batch 19/500 loss 5.121041 loss_att 7.169347 loss_ctc 5.860541 loss_rnnt 4.556414 hw_loss 0.105686 lr 0.00039652 rank 3
2023-02-21 20:19:18,098 DEBUG TRAIN Batch 19/600 loss 9.375283 loss_att 10.388693 loss_ctc 12.682049 loss_rnnt 8.670706 hw_loss 0.114362 lr 0.00039640 rank 2
2023-02-21 20:19:18,099 DEBUG TRAIN Batch 19/600 loss 8.102399 loss_att 11.139163 loss_ctc 13.814001 loss_rnnt 6.601655 hw_loss 0.247208 lr 0.00039645 rank 4
2023-02-21 20:19:18,100 DEBUG TRAIN Batch 19/600 loss 18.186415 loss_att 23.884821 loss_ctc 25.286398 loss_rnnt 16.012379 hw_loss 0.164420 lr 0.00039648 rank 1
2023-02-21 20:19:18,103 DEBUG TRAIN Batch 19/600 loss 10.091014 loss_att 12.210982 loss_ctc 16.185720 loss_rnnt 8.803640 hw_loss 0.095160 lr 0.00039636 rank 7
2023-02-21 20:19:18,105 DEBUG TRAIN Batch 19/600 loss 8.478031 loss_att 8.279015 loss_ctc 10.682184 loss_rnnt 8.082642 hw_loss 0.264950 lr 0.00039645 rank 5
2023-02-21 20:19:18,105 DEBUG TRAIN Batch 19/600 loss 12.885241 loss_att 13.521204 loss_ctc 17.394436 loss_rnnt 12.093037 hw_loss 0.119595 lr 0.00039642 rank 6
2023-02-21 20:19:18,110 DEBUG TRAIN Batch 19/600 loss 11.886980 loss_att 12.718040 loss_ctc 16.955805 loss_rnnt 10.907441 hw_loss 0.257780 lr 0.00039640 rank 3
2023-02-21 20:19:18,152 DEBUG TRAIN Batch 19/600 loss 11.107774 loss_att 11.967173 loss_ctc 14.350195 loss_rnnt 10.448031 hw_loss 0.104136 lr 0.00039645 rank 0
2023-02-21 20:20:39,435 DEBUG TRAIN Batch 19/700 loss 7.141629 loss_att 11.722742 loss_ctc 10.655214 loss_rnnt 5.665270 hw_loss 0.171859 lr 0.00039623 rank 7
2023-02-21 20:20:39,438 DEBUG TRAIN Batch 19/700 loss 5.939962 loss_att 9.181152 loss_ctc 8.010586 loss_rnnt 4.945106 hw_loss 0.132255 lr 0.00039632 rank 4
2023-02-21 20:20:39,437 DEBUG TRAIN Batch 19/700 loss 6.810466 loss_att 8.655751 loss_ctc 7.406037 loss_rnnt 6.312095 hw_loss 0.093572 lr 0.00039629 rank 6
2023-02-21 20:20:39,441 DEBUG TRAIN Batch 19/700 loss 8.433728 loss_att 13.818942 loss_ctc 13.195968 loss_rnnt 6.677957 hw_loss 0.082056 lr 0.00039636 rank 1
2023-02-21 20:20:39,444 DEBUG TRAIN Batch 19/700 loss 7.910796 loss_att 11.076637 loss_ctc 13.688773 loss_rnnt 6.429863 hw_loss 0.145065 lr 0.00039627 rank 3
2023-02-21 20:20:39,445 DEBUG TRAIN Batch 19/700 loss 9.129934 loss_att 11.162794 loss_ctc 15.879752 loss_rnnt 7.775232 hw_loss 0.090288 lr 0.00039633 rank 5
2023-02-21 20:20:39,452 DEBUG TRAIN Batch 19/700 loss 5.516658 loss_att 10.829696 loss_ctc 7.862058 loss_rnnt 4.086973 hw_loss 0.101922 lr 0.00039633 rank 0
2023-02-21 20:20:39,470 DEBUG TRAIN Batch 19/700 loss 7.958179 loss_att 12.128102 loss_ctc 12.072893 loss_rnnt 6.523602 hw_loss 0.097431 lr 0.00039628 rank 2
2023-02-21 20:21:56,967 DEBUG TRAIN Batch 19/800 loss 9.324527 loss_att 15.576024 loss_ctc 15.371145 loss_rnnt 7.243594 hw_loss 0.045784 lr 0.00039611 rank 7
2023-02-21 20:21:56,968 DEBUG TRAIN Batch 19/800 loss 5.854609 loss_att 8.675302 loss_ctc 9.278312 loss_rnnt 4.789436 hw_loss 0.083512 lr 0.00039620 rank 0
2023-02-21 20:21:56,973 DEBUG TRAIN Batch 19/800 loss 6.057491 loss_att 10.572174 loss_ctc 9.661154 loss_rnnt 4.534206 hw_loss 0.262237 lr 0.00039620 rank 4
2023-02-21 20:21:56,973 DEBUG TRAIN Batch 19/800 loss 17.581114 loss_att 25.866743 loss_ctc 24.883053 loss_rnnt 14.809605 hw_loss 0.263981 lr 0.00039621 rank 5
2023-02-21 20:21:56,973 DEBUG TRAIN Batch 19/800 loss 8.255533 loss_att 11.235932 loss_ctc 10.921677 loss_rnnt 7.192094 hw_loss 0.209763 lr 0.00039615 rank 2
2023-02-21 20:21:56,975 DEBUG TRAIN Batch 19/800 loss 5.279442 loss_att 9.052998 loss_ctc 6.519270 loss_rnnt 4.316310 hw_loss 0.080831 lr 0.00039615 rank 3
2023-02-21 20:21:56,975 DEBUG TRAIN Batch 19/800 loss 12.511991 loss_att 13.682865 loss_ctc 17.156733 loss_rnnt 11.622264 hw_loss 0.067973 lr 0.00039623 rank 1
2023-02-21 20:21:57,020 DEBUG TRAIN Batch 19/800 loss 7.314921 loss_att 9.146288 loss_ctc 7.783128 loss_rnnt 6.835838 hw_loss 0.094469 lr 0.00039617 rank 6
2023-02-21 20:23:15,063 DEBUG TRAIN Batch 19/900 loss 13.399565 loss_att 16.096813 loss_ctc 23.471851 loss_rnnt 11.433242 hw_loss 0.157315 lr 0.00039607 rank 4
2023-02-21 20:23:15,072 DEBUG TRAIN Batch 19/900 loss 4.534333 loss_att 7.368589 loss_ctc 6.823333 loss_rnnt 3.562930 hw_loss 0.186286 lr 0.00039598 rank 7
2023-02-21 20:23:15,071 DEBUG TRAIN Batch 19/900 loss 4.061453 loss_att 7.082152 loss_ctc 5.146017 loss_rnnt 3.296818 hw_loss 0.029789 lr 0.00039603 rank 2
2023-02-21 20:23:15,073 DEBUG TRAIN Batch 19/900 loss 7.448642 loss_att 9.583029 loss_ctc 10.433862 loss_rnnt 6.586866 hw_loss 0.069130 lr 0.00039611 rank 1
2023-02-21 20:23:15,072 DEBUG TRAIN Batch 19/900 loss 6.108500 loss_att 8.394299 loss_ctc 8.686202 loss_rnnt 5.254092 hw_loss 0.100414 lr 0.00039608 rank 0
2023-02-21 20:23:15,074 DEBUG TRAIN Batch 19/900 loss 2.191433 loss_att 5.663396 loss_ctc 4.093043 loss_rnnt 1.190971 hw_loss 0.098476 lr 0.00039608 rank 5
2023-02-21 20:23:15,101 DEBUG TRAIN Batch 19/900 loss 7.663476 loss_att 11.964738 loss_ctc 10.523365 loss_rnnt 6.392447 hw_loss 0.055233 lr 0.00039603 rank 3
2023-02-21 20:23:15,105 DEBUG TRAIN Batch 19/900 loss 10.849251 loss_att 12.939714 loss_ctc 14.772884 loss_rnnt 9.859324 hw_loss 0.091280 lr 0.00039604 rank 6
2023-02-21 20:24:34,294 DEBUG TRAIN Batch 19/1000 loss 6.399802 loss_att 9.734089 loss_ctc 8.283764 loss_rnnt 5.401834 hw_loss 0.149841 lr 0.00039592 rank 6
2023-02-21 20:24:34,297 DEBUG TRAIN Batch 19/1000 loss 9.010963 loss_att 10.525061 loss_ctc 10.515354 loss_rnnt 8.462554 hw_loss 0.084383 lr 0.00039595 rank 4
2023-02-21 20:24:34,298 DEBUG TRAIN Batch 19/1000 loss 9.266993 loss_att 10.469726 loss_ctc 10.966413 loss_rnnt 8.682911 hw_loss 0.219273 lr 0.00039586 rank 7
2023-02-21 20:24:34,300 DEBUG TRAIN Batch 19/1000 loss 7.280663 loss_att 10.260227 loss_ctc 7.942416 loss_rnnt 6.580800 hw_loss 0.029466 lr 0.00039591 rank 2
2023-02-21 20:24:34,300 DEBUG TRAIN Batch 19/1000 loss 13.460866 loss_att 16.135509 loss_ctc 20.232286 loss_rnnt 11.925743 hw_loss 0.182509 lr 0.00039596 rank 0
2023-02-21 20:24:34,305 DEBUG TRAIN Batch 19/1000 loss 15.792220 loss_att 17.107616 loss_ctc 23.107822 loss_rnnt 14.505798 hw_loss 0.089868 lr 0.00039598 rank 1
2023-02-21 20:24:34,310 DEBUG TRAIN Batch 19/1000 loss 13.002885 loss_att 15.326559 loss_ctc 13.505609 loss_rnnt 12.419880 hw_loss 0.096075 lr 0.00039596 rank 5
2023-02-21 20:24:34,351 DEBUG TRAIN Batch 19/1000 loss 10.110089 loss_att 13.972591 loss_ctc 13.925324 loss_rnnt 8.795774 hw_loss 0.062093 lr 0.00039590 rank 3
2023-02-21 20:25:53,438 DEBUG TRAIN Batch 19/1100 loss 6.186677 loss_att 7.906090 loss_ctc 6.162730 loss_rnnt 5.809338 hw_loss 0.068720 lr 0.00039578 rank 2
2023-02-21 20:25:53,445 DEBUG TRAIN Batch 19/1100 loss 7.168747 loss_att 10.407748 loss_ctc 13.084549 loss_rnnt 5.662050 hw_loss 0.131479 lr 0.00039582 rank 4
2023-02-21 20:25:53,448 DEBUG TRAIN Batch 19/1100 loss 10.799773 loss_att 13.459095 loss_ctc 15.389923 loss_rnnt 9.600506 hw_loss 0.103844 lr 0.00039574 rank 7
2023-02-21 20:25:53,448 DEBUG TRAIN Batch 19/1100 loss 10.261685 loss_att 12.943680 loss_ctc 16.673883 loss_rnnt 8.818871 hw_loss 0.096478 lr 0.00039583 rank 0
2023-02-21 20:25:53,452 DEBUG TRAIN Batch 19/1100 loss 12.626219 loss_att 13.891641 loss_ctc 15.917685 loss_rnnt 11.887337 hw_loss 0.088006 lr 0.00039578 rank 3
2023-02-21 20:25:53,452 DEBUG TRAIN Batch 19/1100 loss 10.797641 loss_att 12.511755 loss_ctc 14.035933 loss_rnnt 9.954777 hw_loss 0.128004 lr 0.00039579 rank 6
2023-02-21 20:25:53,453 DEBUG TRAIN Batch 19/1100 loss 5.175976 loss_att 7.582856 loss_ctc 8.878271 loss_rnnt 4.145012 hw_loss 0.104903 lr 0.00039586 rank 1
2023-02-21 20:25:53,458 DEBUG TRAIN Batch 19/1100 loss 5.130818 loss_att 9.093398 loss_ctc 10.092587 loss_rnnt 3.656637 hw_loss 0.037679 lr 0.00039583 rank 5
2023-02-21 20:27:12,726 DEBUG TRAIN Batch 19/1200 loss 5.964834 loss_att 9.949408 loss_ctc 9.359627 loss_rnnt 4.668836 hw_loss 0.087084 lr 0.00039570 rank 4
2023-02-21 20:27:12,727 DEBUG TRAIN Batch 19/1200 loss 13.316053 loss_att 16.809967 loss_ctc 16.890799 loss_rnnt 12.030906 hw_loss 0.205748 lr 0.00039561 rank 7
2023-02-21 20:27:12,729 DEBUG TRAIN Batch 19/1200 loss 4.721485 loss_att 5.299664 loss_ctc 5.949973 loss_rnnt 4.215424 hw_loss 0.424926 lr 0.00039567 rank 6
2023-02-21 20:27:12,729 DEBUG TRAIN Batch 19/1200 loss 17.977301 loss_att 20.682959 loss_ctc 27.583815 loss_rnnt 16.066500 hw_loss 0.166496 lr 0.00039566 rank 2
2023-02-21 20:27:12,730 DEBUG TRAIN Batch 19/1200 loss 8.355753 loss_att 9.117481 loss_ctc 9.787425 loss_rnnt 7.940634 hw_loss 0.134782 lr 0.00039574 rank 1
2023-02-21 20:27:12,733 DEBUG TRAIN Batch 19/1200 loss 8.065578 loss_att 9.488842 loss_ctc 10.552177 loss_rnnt 7.329705 hw_loss 0.224385 lr 0.00039565 rank 3
2023-02-21 20:27:12,737 DEBUG TRAIN Batch 19/1200 loss 11.712028 loss_att 14.372172 loss_ctc 15.888110 loss_rnnt 10.573474 hw_loss 0.093216 lr 0.00039571 rank 0
2023-02-21 20:27:12,738 DEBUG TRAIN Batch 19/1200 loss 5.537003 loss_att 7.243955 loss_ctc 8.847895 loss_rnnt 4.642425 hw_loss 0.209505 lr 0.00039571 rank 5
2023-02-21 20:28:30,971 DEBUG TRAIN Batch 19/1300 loss 4.813385 loss_att 8.108915 loss_ctc 3.500290 loss_rnnt 4.246383 hw_loss 0.155577 lr 0.00039559 rank 5
2023-02-21 20:28:30,972 DEBUG TRAIN Batch 19/1300 loss 7.782049 loss_att 13.638318 loss_ctc 16.629698 loss_rnnt 5.391998 hw_loss 0.073332 lr 0.00039549 rank 7
2023-02-21 20:28:30,972 DEBUG TRAIN Batch 19/1300 loss 5.457980 loss_att 9.691187 loss_ctc 5.520063 loss_rnnt 4.563293 hw_loss 0.074565 lr 0.00039555 rank 6
2023-02-21 20:28:30,973 DEBUG TRAIN Batch 19/1300 loss 13.111251 loss_att 13.752046 loss_ctc 16.703770 loss_rnnt 12.393064 hw_loss 0.208174 lr 0.00039561 rank 1
2023-02-21 20:28:30,973 DEBUG TRAIN Batch 19/1300 loss 6.249744 loss_att 8.995484 loss_ctc 5.910253 loss_rnnt 5.720708 hw_loss 0.047162 lr 0.00039553 rank 2
2023-02-21 20:28:30,973 DEBUG TRAIN Batch 19/1300 loss 17.231758 loss_att 24.955395 loss_ctc 26.934864 loss_rnnt 14.258194 hw_loss 0.253291 lr 0.00039558 rank 4
2023-02-21 20:28:30,977 DEBUG TRAIN Batch 19/1300 loss 5.671207 loss_att 11.704896 loss_ctc 11.602905 loss_rnnt 3.544976 hw_loss 0.241127 lr 0.00039558 rank 0
2023-02-21 20:28:30,979 DEBUG TRAIN Batch 19/1300 loss 5.130867 loss_att 9.796636 loss_ctc 6.824878 loss_rnnt 3.874561 hw_loss 0.182409 lr 0.00039553 rank 3
2023-02-21 20:29:51,602 DEBUG TRAIN Batch 19/1400 loss 6.038514 loss_att 10.506516 loss_ctc 10.876198 loss_rnnt 4.428090 hw_loss 0.134624 lr 0.00039537 rank 7
2023-02-21 20:29:51,602 DEBUG TRAIN Batch 19/1400 loss 7.539485 loss_att 11.498956 loss_ctc 9.405654 loss_rnnt 6.455215 hw_loss 0.081662 lr 0.00039541 rank 2
2023-02-21 20:29:51,603 DEBUG TRAIN Batch 19/1400 loss 12.385390 loss_att 19.201385 loss_ctc 18.474672 loss_rnnt 10.174010 hw_loss 0.068019 lr 0.00039549 rank 1
2023-02-21 20:29:51,606 DEBUG TRAIN Batch 19/1400 loss 6.394954 loss_att 9.300716 loss_ctc 8.124783 loss_rnnt 5.510468 hw_loss 0.136293 lr 0.00039542 rank 6
2023-02-21 20:29:51,609 DEBUG TRAIN Batch 19/1400 loss 12.688826 loss_att 13.042393 loss_ctc 19.423697 loss_rnnt 11.653252 hw_loss 0.125397 lr 0.00039546 rank 0
2023-02-21 20:29:51,612 DEBUG TRAIN Batch 19/1400 loss 13.688864 loss_att 15.945982 loss_ctc 20.292318 loss_rnnt 12.289207 hw_loss 0.127074 lr 0.00039545 rank 4
2023-02-21 20:29:51,616 DEBUG TRAIN Batch 19/1400 loss 6.835223 loss_att 9.624336 loss_ctc 7.758596 loss_rnnt 6.084696 hw_loss 0.130478 lr 0.00039546 rank 5
2023-02-21 20:29:51,616 DEBUG TRAIN Batch 19/1400 loss 5.008376 loss_att 8.971241 loss_ctc 9.210460 loss_rnnt 3.608886 hw_loss 0.087446 lr 0.00039541 rank 3
2023-02-21 20:31:10,516 DEBUG TRAIN Batch 19/1500 loss 10.250339 loss_att 10.869451 loss_ctc 10.545622 loss_rnnt 10.029419 hw_loss 0.108235 lr 0.00039524 rank 7
2023-02-21 20:31:10,517 DEBUG TRAIN Batch 19/1500 loss 18.668171 loss_att 26.342749 loss_ctc 25.891392 loss_rnnt 16.096630 hw_loss 0.137868 lr 0.00039529 rank 2
2023-02-21 20:31:10,519 DEBUG TRAIN Batch 19/1500 loss 12.686777 loss_att 17.423677 loss_ctc 19.655827 loss_rnnt 10.769974 hw_loss 0.075407 lr 0.00039536 rank 1
2023-02-21 20:31:10,520 DEBUG TRAIN Batch 19/1500 loss 14.129474 loss_att 21.281731 loss_ctc 15.852083 loss_rnnt 12.425582 hw_loss 0.082050 lr 0.00039533 rank 4
2023-02-21 20:31:10,521 DEBUG TRAIN Batch 19/1500 loss 12.216408 loss_att 15.815130 loss_ctc 17.508568 loss_rnnt 10.761642 hw_loss 0.055125 lr 0.00039534 rank 5
2023-02-21 20:31:10,522 DEBUG TRAIN Batch 19/1500 loss 11.697623 loss_att 13.479503 loss_ctc 21.947254 loss_rnnt 9.902023 hw_loss 0.136140 lr 0.00039534 rank 0
2023-02-21 20:31:10,524 DEBUG TRAIN Batch 19/1500 loss 11.610470 loss_att 13.140001 loss_ctc 19.071953 loss_rnnt 10.255388 hw_loss 0.101832 lr 0.00039530 rank 6
2023-02-21 20:31:10,527 DEBUG TRAIN Batch 19/1500 loss 12.572282 loss_att 16.565777 loss_ctc 20.126459 loss_rnnt 10.737244 hw_loss 0.054591 lr 0.00039528 rank 3
2023-02-21 20:32:28,817 DEBUG TRAIN Batch 19/1600 loss 20.592550 loss_att 22.315901 loss_ctc 25.907103 loss_rnnt 19.448635 hw_loss 0.169947 lr 0.00039516 rank 2
2023-02-21 20:32:28,821 DEBUG TRAIN Batch 19/1600 loss 10.431180 loss_att 16.062551 loss_ctc 13.570524 loss_rnnt 8.719554 hw_loss 0.312699 lr 0.00039521 rank 4
2023-02-21 20:32:28,824 DEBUG TRAIN Batch 19/1600 loss 8.144609 loss_att 11.059839 loss_ctc 10.101532 loss_rnnt 7.268739 hw_loss 0.059814 lr 0.00039512 rank 7
2023-02-21 20:32:28,825 DEBUG TRAIN Batch 19/1600 loss 16.504234 loss_att 19.386698 loss_ctc 23.477798 loss_rnnt 14.898665 hw_loss 0.186129 lr 0.00039518 rank 6
2023-02-21 20:32:28,826 DEBUG TRAIN Batch 19/1600 loss 6.853385 loss_att 10.314686 loss_ctc 9.972125 loss_rnnt 5.673085 hw_loss 0.135389 lr 0.00039524 rank 1
2023-02-21 20:32:28,826 DEBUG TRAIN Batch 19/1600 loss 14.852221 loss_att 18.109505 loss_ctc 25.950974 loss_rnnt 12.667470 hw_loss 0.100239 lr 0.00039521 rank 0
2023-02-21 20:32:28,829 DEBUG TRAIN Batch 19/1600 loss 14.713213 loss_att 16.713503 loss_ctc 17.776049 loss_rnnt 13.864202 hw_loss 0.076077 lr 0.00039521 rank 5
2023-02-21 20:32:28,877 DEBUG TRAIN Batch 19/1600 loss 8.035469 loss_att 9.582502 loss_ctc 9.652103 loss_rnnt 7.478401 hw_loss 0.060208 lr 0.00039516 rank 3
2023-02-21 20:33:47,345 DEBUG TRAIN Batch 19/1700 loss 6.759686 loss_att 7.585563 loss_ctc 7.029901 loss_rnnt 6.538465 hw_loss 0.037529 lr 0.00039499 rank 7
2023-02-21 20:33:47,348 DEBUG TRAIN Batch 19/1700 loss 8.000474 loss_att 10.389471 loss_ctc 8.727309 loss_rnnt 7.405789 hw_loss 0.037450 lr 0.00039509 rank 0
2023-02-21 20:33:47,352 DEBUG TRAIN Batch 19/1700 loss 10.462853 loss_att 13.794111 loss_ctc 16.126896 loss_rnnt 8.980351 hw_loss 0.114459 lr 0.00039505 rank 6
2023-02-21 20:33:47,355 DEBUG TRAIN Batch 19/1700 loss 3.682345 loss_att 6.518922 loss_ctc 5.237616 loss_rnnt 2.790867 hw_loss 0.218985 lr 0.00039504 rank 3
2023-02-21 20:33:47,355 DEBUG TRAIN Batch 19/1700 loss 2.606492 loss_att 6.432873 loss_ctc 2.868105 loss_rnnt 1.741109 hw_loss 0.122298 lr 0.00039512 rank 1
2023-02-21 20:33:47,357 DEBUG TRAIN Batch 19/1700 loss 15.069373 loss_att 15.710738 loss_ctc 17.911572 loss_rnnt 14.501666 hw_loss 0.113390 lr 0.00039509 rank 5
2023-02-21 20:33:47,387 DEBUG TRAIN Batch 19/1700 loss 8.271435 loss_att 10.681902 loss_ctc 10.796633 loss_rnnt 7.366942 hw_loss 0.160699 lr 0.00039508 rank 4
2023-02-21 20:33:47,396 DEBUG TRAIN Batch 19/1700 loss 7.267058 loss_att 9.712925 loss_ctc 9.868961 loss_rnnt 6.348787 hw_loss 0.154083 lr 0.00039504 rank 2
2023-02-21 20:35:08,046 DEBUG TRAIN Batch 19/1800 loss 6.821630 loss_att 6.972885 loss_ctc 7.517367 loss_rnnt 6.648733 hw_loss 0.093530 lr 0.00039496 rank 4
2023-02-21 20:35:08,045 DEBUG TRAIN Batch 19/1800 loss 8.831128 loss_att 10.545172 loss_ctc 11.963012 loss_rnnt 8.018769 hw_loss 0.097434 lr 0.00039492 rank 2
2023-02-21 20:35:08,047 DEBUG TRAIN Batch 19/1800 loss 16.138527 loss_att 17.779886 loss_ctc 21.387938 loss_rnnt 14.965292 hw_loss 0.271953 lr 0.00039497 rank 0
2023-02-21 20:35:08,048 DEBUG TRAIN Batch 19/1800 loss 6.354618 loss_att 9.036563 loss_ctc 12.820602 loss_rnnt 4.842816 hw_loss 0.212403 lr 0.00039499 rank 1
2023-02-21 20:35:08,052 DEBUG TRAIN Batch 19/1800 loss 9.748922 loss_att 12.779776 loss_ctc 13.755925 loss_rnnt 8.514279 hw_loss 0.176635 lr 0.00039487 rank 7
2023-02-21 20:35:08,053 DEBUG TRAIN Batch 19/1800 loss 11.582249 loss_att 11.775778 loss_ctc 14.273471 loss_rnnt 11.057319 hw_loss 0.238864 lr 0.00039493 rank 6
2023-02-21 20:35:08,057 DEBUG TRAIN Batch 19/1800 loss 11.497411 loss_att 12.253384 loss_ctc 16.818924 loss_rnnt 10.549402 hw_loss 0.163646 lr 0.00039491 rank 3
2023-02-21 20:35:08,058 DEBUG TRAIN Batch 19/1800 loss 7.249885 loss_att 10.714577 loss_ctc 10.149281 loss_rnnt 6.112600 hw_loss 0.108303 lr 0.00039497 rank 5
2023-02-21 20:36:26,828 DEBUG TRAIN Batch 19/1900 loss 6.365991 loss_att 8.951725 loss_ctc 11.213353 loss_rnnt 5.156037 hw_loss 0.087172 lr 0.00039487 rank 1
2023-02-21 20:36:26,829 DEBUG TRAIN Batch 19/1900 loss 9.842897 loss_att 10.102576 loss_ctc 13.381773 loss_rnnt 9.096962 hw_loss 0.416530 lr 0.00039484 rank 4
2023-02-21 20:36:26,832 DEBUG TRAIN Batch 19/1900 loss 15.143032 loss_att 18.196217 loss_ctc 16.836708 loss_rnnt 14.209076 hw_loss 0.182806 lr 0.00039481 rank 6
2023-02-21 20:36:26,834 DEBUG TRAIN Batch 19/1900 loss 14.494553 loss_att 15.037851 loss_ctc 22.341408 loss_rnnt 13.284109 hw_loss 0.104131 lr 0.00039475 rank 7
2023-02-21 20:36:26,834 DEBUG TRAIN Batch 19/1900 loss 12.867060 loss_att 15.759377 loss_ctc 20.291859 loss_rnnt 11.178213 hw_loss 0.225767 lr 0.00039484 rank 0
2023-02-21 20:36:26,837 DEBUG TRAIN Batch 19/1900 loss 7.029679 loss_att 9.970386 loss_ctc 9.432917 loss_rnnt 6.048601 hw_loss 0.135948 lr 0.00039479 rank 2
2023-02-21 20:36:26,846 DEBUG TRAIN Batch 19/1900 loss 1.923806 loss_att 6.677029 loss_ctc 2.760726 loss_rnnt 0.748352 hw_loss 0.212289 lr 0.00039479 rank 3
2023-02-21 20:36:26,880 DEBUG TRAIN Batch 19/1900 loss 10.233402 loss_att 16.004444 loss_ctc 12.213282 loss_rnnt 8.645385 hw_loss 0.318423 lr 0.00039484 rank 5
2023-02-21 20:37:46,012 DEBUG TRAIN Batch 19/2000 loss 5.175925 loss_att 7.983846 loss_ctc 12.537054 loss_rnnt 3.597610 hw_loss 0.066088 lr 0.00039463 rank 7
2023-02-21 20:37:46,014 DEBUG TRAIN Batch 19/2000 loss 6.277111 loss_att 9.368538 loss_ctc 8.151590 loss_rnnt 5.318423 hw_loss 0.169635 lr 0.00039467 rank 2
2023-02-21 20:37:46,016 DEBUG TRAIN Batch 19/2000 loss 10.724536 loss_att 15.778385 loss_ctc 15.908714 loss_rnnt 8.964722 hw_loss 0.108412 lr 0.00039472 rank 0
2023-02-21 20:37:46,017 DEBUG TRAIN Batch 19/2000 loss 13.650281 loss_att 20.195187 loss_ctc 17.154675 loss_rnnt 11.788666 hw_loss 0.160092 lr 0.00039467 rank 3
2023-02-21 20:37:46,019 DEBUG TRAIN Batch 19/2000 loss 6.222436 loss_att 9.470532 loss_ctc 9.244297 loss_rnnt 5.025582 hw_loss 0.270600 lr 0.00039471 rank 4
2023-02-21 20:37:46,019 DEBUG TRAIN Batch 19/2000 loss 11.511146 loss_att 13.713249 loss_ctc 23.326254 loss_rnnt 9.447876 hw_loss 0.089063 lr 0.00039475 rank 1
2023-02-21 20:37:46,020 DEBUG TRAIN Batch 19/2000 loss 4.025500 loss_att 7.811463 loss_ctc 6.745519 loss_rnnt 2.856119 hw_loss 0.092849 lr 0.00039472 rank 5
2023-02-21 20:37:46,023 DEBUG TRAIN Batch 19/2000 loss 7.521179 loss_att 11.421193 loss_ctc 10.869104 loss_rnnt 6.196092 hw_loss 0.185051 lr 0.00039468 rank 6
2023-02-21 20:39:06,960 DEBUG TRAIN Batch 19/2100 loss 9.875872 loss_att 10.078968 loss_ctc 13.322081 loss_rnnt 9.322334 hw_loss 0.100168 lr 0.00039462 rank 1
2023-02-21 20:39:06,963 DEBUG TRAIN Batch 19/2100 loss 8.042911 loss_att 11.131058 loss_ctc 10.194195 loss_rnnt 7.124386 hw_loss 0.026357 lr 0.00039450 rank 7
2023-02-21 20:39:06,963 DEBUG TRAIN Batch 19/2100 loss 11.959955 loss_att 13.640484 loss_ctc 17.158058 loss_rnnt 10.894437 hw_loss 0.068122 lr 0.00039456 rank 6
2023-02-21 20:39:06,968 DEBUG TRAIN Batch 19/2100 loss 3.226271 loss_att 5.868231 loss_ctc 4.746244 loss_rnnt 2.409561 hw_loss 0.160604 lr 0.00039454 rank 3
2023-02-21 20:39:06,968 DEBUG TRAIN Batch 19/2100 loss 7.879006 loss_att 11.359264 loss_ctc 13.488887 loss_rnnt 6.394094 hw_loss 0.076643 lr 0.00039460 rank 0
2023-02-21 20:39:06,971 DEBUG TRAIN Batch 19/2100 loss 10.979699 loss_att 13.136975 loss_ctc 9.793945 loss_rnnt 10.614172 hw_loss 0.172822 lr 0.00039459 rank 4
2023-02-21 20:39:06,971 DEBUG TRAIN Batch 19/2100 loss 5.962219 loss_att 8.546596 loss_ctc 6.825265 loss_rnnt 5.316377 hw_loss 0.026051 lr 0.00039455 rank 2
2023-02-21 20:39:06,975 DEBUG TRAIN Batch 19/2100 loss 12.638287 loss_att 15.586497 loss_ctc 14.872005 loss_rnnt 11.700882 hw_loss 0.093623 lr 0.00039460 rank 5
2023-02-21 20:40:26,391 DEBUG TRAIN Batch 19/2200 loss 5.048388 loss_att 9.517200 loss_ctc 8.603030 loss_rnnt 3.623878 hw_loss 0.106489 lr 0.00039438 rank 7
2023-02-21 20:40:26,391 DEBUG TRAIN Batch 19/2200 loss 18.475344 loss_att 21.674675 loss_ctc 24.286171 loss_rnnt 17.011930 hw_loss 0.091443 lr 0.00039444 rank 6
2023-02-21 20:40:26,393 DEBUG TRAIN Batch 19/2200 loss 16.455296 loss_att 20.771767 loss_ctc 22.145798 loss_rnnt 14.784996 hw_loss 0.090506 lr 0.00039447 rank 4
2023-02-21 20:40:26,394 DEBUG TRAIN Batch 19/2200 loss 8.661154 loss_att 12.062012 loss_ctc 14.426019 loss_rnnt 7.092473 hw_loss 0.224737 lr 0.00039442 rank 3
2023-02-21 20:40:26,396 DEBUG TRAIN Batch 19/2200 loss 4.436786 loss_att 8.394363 loss_ctc 7.361399 loss_rnnt 3.173639 hw_loss 0.153155 lr 0.00039443 rank 2
2023-02-21 20:40:26,398 DEBUG TRAIN Batch 19/2200 loss 6.323319 loss_att 10.476809 loss_ctc 9.132427 loss_rnnt 5.060302 hw_loss 0.108323 lr 0.00039450 rank 1
2023-02-21 20:40:26,401 DEBUG TRAIN Batch 19/2200 loss 6.398672 loss_att 8.515949 loss_ctc 6.253571 loss_rnnt 5.979857 hw_loss 0.027573 lr 0.00039447 rank 0
2023-02-21 20:40:26,402 DEBUG TRAIN Batch 19/2200 loss 3.921409 loss_att 7.622663 loss_ctc 7.578753 loss_rnnt 2.650584 hw_loss 0.080490 lr 0.00039448 rank 5
2023-02-21 20:41:45,264 DEBUG TRAIN Batch 19/2300 loss 12.324915 loss_att 13.016239 loss_ctc 14.632320 loss_rnnt 11.810992 hw_loss 0.127505 lr 0.00039426 rank 7
2023-02-21 20:41:45,266 DEBUG TRAIN Batch 19/2300 loss 11.026999 loss_att 16.071293 loss_ctc 14.907463 loss_rnnt 9.455081 hw_loss 0.085620 lr 0.00039435 rank 0
2023-02-21 20:41:45,267 DEBUG TRAIN Batch 19/2300 loss 9.511360 loss_att 12.020510 loss_ctc 14.361353 loss_rnnt 8.322639 hw_loss 0.075424 lr 0.00039434 rank 4
2023-02-21 20:41:45,268 DEBUG TRAIN Batch 19/2300 loss 6.764816 loss_att 9.039093 loss_ctc 8.045443 loss_rnnt 6.052082 hw_loss 0.163365 lr 0.00039431 rank 6
2023-02-21 20:41:45,269 DEBUG TRAIN Batch 19/2300 loss 9.904944 loss_att 13.637699 loss_ctc 18.462887 loss_rnnt 7.974419 hw_loss 0.080467 lr 0.00039438 rank 1
2023-02-21 20:41:45,270 DEBUG TRAIN Batch 19/2300 loss 12.558331 loss_att 15.881811 loss_ctc 21.110291 loss_rnnt 10.645500 hw_loss 0.202265 lr 0.00039430 rank 2
2023-02-21 20:41:45,272 DEBUG TRAIN Batch 19/2300 loss 6.336699 loss_att 10.220236 loss_ctc 10.769614 loss_rnnt 4.926848 hw_loss 0.078916 lr 0.00039435 rank 5
2023-02-21 20:41:45,321 DEBUG TRAIN Batch 19/2300 loss 4.871446 loss_att 8.465702 loss_ctc 7.626976 loss_rnnt 3.725507 hw_loss 0.111907 lr 0.00039430 rank 3
2023-02-21 20:43:02,598 DEBUG TRAIN Batch 19/2400 loss 3.923521 loss_att 6.696904 loss_ctc 6.866122 loss_rnnt 2.915662 hw_loss 0.114067 lr 0.00039419 rank 6
2023-02-21 20:43:02,601 DEBUG TRAIN Batch 19/2400 loss 15.545178 loss_att 21.157228 loss_ctc 24.360512 loss_rnnt 13.192770 hw_loss 0.102414 lr 0.00039422 rank 4
2023-02-21 20:43:02,602 DEBUG TRAIN Batch 19/2400 loss 7.715592 loss_att 8.911945 loss_ctc 9.448090 loss_rnnt 7.162827 hw_loss 0.154679 lr 0.00039418 rank 3
2023-02-21 20:43:02,603 DEBUG TRAIN Batch 19/2400 loss 20.024221 loss_att 25.774269 loss_ctc 29.301380 loss_rnnt 17.591087 hw_loss 0.086569 lr 0.00039423 rank 0
2023-02-21 20:43:02,606 DEBUG TRAIN Batch 19/2400 loss 3.671619 loss_att 4.630173 loss_ctc 4.153401 loss_rnnt 3.328687 hw_loss 0.163093 lr 0.00039413 rank 7
2023-02-21 20:43:02,606 DEBUG TRAIN Batch 19/2400 loss 8.884022 loss_att 13.524395 loss_ctc 12.949110 loss_rnnt 7.314660 hw_loss 0.186141 lr 0.00039418 rank 2
2023-02-21 20:43:02,606 DEBUG TRAIN Batch 19/2400 loss 6.061337 loss_att 7.974291 loss_ctc 10.751267 loss_rnnt 5.011571 hw_loss 0.078471 lr 0.00039423 rank 5
2023-02-21 20:43:02,607 DEBUG TRAIN Batch 19/2400 loss 11.432872 loss_att 15.057771 loss_ctc 14.211343 loss_rnnt 10.311062 hw_loss 0.049439 lr 0.00039426 rank 1
2023-02-21 20:44:23,926 DEBUG TRAIN Batch 19/2500 loss 2.957487 loss_att 4.935561 loss_ctc 4.224103 loss_rnnt 2.341664 hw_loss 0.096237 lr 0.00039401 rank 7
2023-02-21 20:44:23,927 DEBUG TRAIN Batch 19/2500 loss 18.076714 loss_att 22.028034 loss_ctc 25.657629 loss_rnnt 16.203421 hw_loss 0.135448 lr 0.00039406 rank 2
2023-02-21 20:44:23,929 DEBUG TRAIN Batch 19/2500 loss 7.447645 loss_att 9.654534 loss_ctc 11.940338 loss_rnnt 6.391744 hw_loss 0.029059 lr 0.00039405 rank 3
2023-02-21 20:44:23,930 DEBUG TRAIN Batch 19/2500 loss 16.113836 loss_att 16.172216 loss_ctc 22.411425 loss_rnnt 15.193285 hw_loss 0.129743 lr 0.00039413 rank 1
2023-02-21 20:44:23,931 DEBUG TRAIN Batch 19/2500 loss 7.595427 loss_att 9.044710 loss_ctc 12.038603 loss_rnnt 6.642696 hw_loss 0.132094 lr 0.00039407 rank 6
2023-02-21 20:44:23,932 DEBUG TRAIN Batch 19/2500 loss 2.535952 loss_att 3.830733 loss_ctc 3.157072 loss_rnnt 2.124985 hw_loss 0.129741 lr 0.00039410 rank 4
2023-02-21 20:44:23,937 DEBUG TRAIN Batch 19/2500 loss 9.297896 loss_att 8.973103 loss_ctc 11.444362 loss_rnnt 8.793303 hw_loss 0.531295 lr 0.00039411 rank 5
2023-02-21 20:44:23,937 DEBUG TRAIN Batch 19/2500 loss 6.086445 loss_att 9.387210 loss_ctc 10.179739 loss_rnnt 4.806297 hw_loss 0.139167 lr 0.00039411 rank 0
2023-02-21 20:45:42,655 DEBUG TRAIN Batch 19/2600 loss 13.028010 loss_att 14.878058 loss_ctc 17.909607 loss_rnnt 11.962596 hw_loss 0.083488 lr 0.00039398 rank 4
2023-02-21 20:45:42,660 DEBUG TRAIN Batch 19/2600 loss 2.064913 loss_att 4.650768 loss_ctc 3.620387 loss_rnnt 1.301232 hw_loss 0.073337 lr 0.00039389 rank 7
2023-02-21 20:45:42,661 DEBUG TRAIN Batch 19/2600 loss 8.761785 loss_att 9.351076 loss_ctc 10.959990 loss_rnnt 8.215385 hw_loss 0.253964 lr 0.00039398 rank 0
2023-02-21 20:45:42,662 DEBUG TRAIN Batch 19/2600 loss 12.693741 loss_att 17.355137 loss_ctc 19.430958 loss_rnnt 10.798584 hw_loss 0.121090 lr 0.00039395 rank 6
2023-02-21 20:45:42,665 DEBUG TRAIN Batch 19/2600 loss 27.399611 loss_att 31.012318 loss_ctc 32.172817 loss_rnnt 26.027113 hw_loss 0.025368 lr 0.00039393 rank 3
2023-02-21 20:45:42,667 DEBUG TRAIN Batch 19/2600 loss 18.972330 loss_att 27.771814 loss_ctc 24.570831 loss_rnnt 16.420788 hw_loss 0.084708 lr 0.00039399 rank 5
2023-02-21 20:45:42,692 DEBUG TRAIN Batch 19/2600 loss 7.278433 loss_att 7.248299 loss_ctc 10.599110 loss_rnnt 6.704205 hw_loss 0.257809 lr 0.00039401 rank 1
2023-02-21 20:45:42,698 DEBUG TRAIN Batch 19/2600 loss 6.342537 loss_att 10.838144 loss_ctc 11.726706 loss_rnnt 4.664135 hw_loss 0.115110 lr 0.00039394 rank 2
2023-02-21 20:47:02,327 DEBUG TRAIN Batch 19/2700 loss 8.423269 loss_att 10.867493 loss_ctc 11.379375 loss_rnnt 7.502157 hw_loss 0.071477 lr 0.00039377 rank 7
2023-02-21 20:47:02,331 DEBUG TRAIN Batch 19/2700 loss 21.558302 loss_att 30.924917 loss_ctc 36.826729 loss_rnnt 17.588497 hw_loss 0.113799 lr 0.00039389 rank 1
2023-02-21 20:47:02,340 DEBUG TRAIN Batch 19/2700 loss 11.729434 loss_att 15.530374 loss_ctc 12.245495 loss_rnnt 10.886894 hw_loss 0.025395 lr 0.00039386 rank 0
2023-02-21 20:47:02,340 DEBUG TRAIN Batch 19/2700 loss 12.614276 loss_att 15.714694 loss_ctc 15.372642 loss_rnnt 11.612948 hw_loss 0.025241 lr 0.00039385 rank 4
2023-02-21 20:47:02,341 DEBUG TRAIN Batch 19/2700 loss 5.308674 loss_att 7.975708 loss_ctc 6.916114 loss_rnnt 4.525031 hw_loss 0.067331 lr 0.00039381 rank 2
2023-02-21 20:47:02,342 DEBUG TRAIN Batch 19/2700 loss 3.233246 loss_att 5.877348 loss_ctc 5.062644 loss_rnnt 2.397846 hw_loss 0.117488 lr 0.00039386 rank 5
2023-02-21 20:47:02,343 DEBUG TRAIN Batch 19/2700 loss 6.347644 loss_att 9.472367 loss_ctc 9.953879 loss_rnnt 5.131336 hw_loss 0.207247 lr 0.00039381 rank 3
2023-02-21 20:47:02,380 DEBUG TRAIN Batch 19/2700 loss 9.446999 loss_att 13.184403 loss_ctc 15.074100 loss_rnnt 7.916276 hw_loss 0.061804 lr 0.00039383 rank 6
2023-02-21 20:48:24,029 DEBUG TRAIN Batch 19/2800 loss 8.383740 loss_att 12.867436 loss_ctc 13.996527 loss_rnnt 6.722860 hw_loss 0.029568 lr 0.00039369 rank 2
2023-02-21 20:48:24,031 DEBUG TRAIN Batch 19/2800 loss 3.819264 loss_att 6.223010 loss_ctc 6.307148 loss_rnnt 2.928704 hw_loss 0.146424 lr 0.00039373 rank 4
2023-02-21 20:48:24,033 DEBUG TRAIN Batch 19/2800 loss 8.297361 loss_att 12.369358 loss_ctc 13.581111 loss_rnnt 6.745001 hw_loss 0.062740 lr 0.00039377 rank 1
2023-02-21 20:48:24,034 DEBUG TRAIN Batch 19/2800 loss 8.810972 loss_att 11.337353 loss_ctc 13.528694 loss_rnnt 7.605245 hw_loss 0.133915 lr 0.00039365 rank 7
2023-02-21 20:48:24,035 DEBUG TRAIN Batch 19/2800 loss 7.336340 loss_att 11.855555 loss_ctc 6.416440 loss_rnnt 6.495609 hw_loss 0.111640 lr 0.00039374 rank 0
2023-02-21 20:48:24,038 DEBUG TRAIN Batch 19/2800 loss 5.790941 loss_att 8.270512 loss_ctc 8.352404 loss_rnnt 4.856356 hw_loss 0.182143 lr 0.00039374 rank 5
2023-02-21 20:48:24,038 DEBUG TRAIN Batch 19/2800 loss 9.534422 loss_att 10.861568 loss_ctc 11.554344 loss_rnnt 8.944919 hw_loss 0.102657 lr 0.00039369 rank 3
2023-02-21 20:48:24,045 DEBUG TRAIN Batch 19/2800 loss 8.164035 loss_att 10.110850 loss_ctc 10.085178 loss_rnnt 7.480407 hw_loss 0.071461 lr 0.00039370 rank 6
2023-02-21 20:49:45,352 DEBUG TRAIN Batch 19/2900 loss 10.828572 loss_att 13.372477 loss_ctc 18.971146 loss_rnnt 9.182628 hw_loss 0.096538 lr 0.00039361 rank 4
2023-02-21 20:49:45,353 DEBUG TRAIN Batch 19/2900 loss 6.890087 loss_att 11.458258 loss_ctc 9.551240 loss_rnnt 5.562491 hw_loss 0.110890 lr 0.00039352 rank 7
2023-02-21 20:49:45,354 DEBUG TRAIN Batch 19/2900 loss 13.217851 loss_att 13.487447 loss_ctc 16.017361 loss_rnnt 12.728025 hw_loss 0.117448 lr 0.00039357 rank 2
2023-02-21 20:49:45,356 DEBUG TRAIN Batch 19/2900 loss 8.244793 loss_att 11.279766 loss_ctc 13.271172 loss_rnnt 6.909211 hw_loss 0.109505 lr 0.00039364 rank 1
2023-02-21 20:49:45,356 DEBUG TRAIN Batch 19/2900 loss 5.437314 loss_att 7.744791 loss_ctc 8.761934 loss_rnnt 4.488264 hw_loss 0.083008 lr 0.00039362 rank 0
2023-02-21 20:49:45,359 DEBUG TRAIN Batch 19/2900 loss 7.644614 loss_att 12.461103 loss_ctc 10.685558 loss_rnnt 6.232160 hw_loss 0.081931 lr 0.00039356 rank 3
2023-02-21 20:49:45,363 DEBUG TRAIN Batch 19/2900 loss 5.516654 loss_att 9.861295 loss_ctc 8.395627 loss_rnnt 4.190861 hw_loss 0.136878 lr 0.00039362 rank 5
2023-02-21 20:49:45,406 DEBUG TRAIN Batch 19/2900 loss 10.959234 loss_att 13.241938 loss_ctc 17.561192 loss_rnnt 9.536764 hw_loss 0.160627 lr 0.00039358 rank 6
2023-02-21 20:51:03,782 DEBUG TRAIN Batch 19/3000 loss 12.575474 loss_att 16.306934 loss_ctc 18.181122 loss_rnnt 11.023027 hw_loss 0.110127 lr 0.00039340 rank 7
2023-02-21 20:51:03,782 DEBUG TRAIN Batch 19/3000 loss 9.872627 loss_att 11.595195 loss_ctc 13.451832 loss_rnnt 8.988049 hw_loss 0.117821 lr 0.00039346 rank 6
2023-02-21 20:51:03,789 DEBUG TRAIN Batch 19/3000 loss 5.472213 loss_att 7.521070 loss_ctc 9.584746 loss_rnnt 4.458883 hw_loss 0.103541 lr 0.00039345 rank 2
2023-02-21 20:51:03,789 DEBUG TRAIN Batch 19/3000 loss 15.947635 loss_att 18.267900 loss_ctc 22.596949 loss_rnnt 14.534237 hw_loss 0.117690 lr 0.00039352 rank 1
2023-02-21 20:51:03,790 DEBUG TRAIN Batch 19/3000 loss 8.541746 loss_att 13.773914 loss_ctc 12.989718 loss_rnnt 6.836187 hw_loss 0.123868 lr 0.00039349 rank 4
2023-02-21 20:51:03,792 DEBUG TRAIN Batch 19/3000 loss 8.342819 loss_att 10.414417 loss_ctc 10.424459 loss_rnnt 7.548440 hw_loss 0.192202 lr 0.00039350 rank 0
2023-02-21 20:51:03,793 DEBUG TRAIN Batch 19/3000 loss 3.935220 loss_att 7.650226 loss_ctc 7.901597 loss_rnnt 2.576096 hw_loss 0.163634 lr 0.00039344 rank 3
2023-02-21 20:51:03,842 DEBUG TRAIN Batch 19/3000 loss 15.880074 loss_att 17.017275 loss_ctc 20.665565 loss_rnnt 14.988729 hw_loss 0.048447 lr 0.00039350 rank 5
2023-02-21 20:52:22,669 DEBUG TRAIN Batch 19/3100 loss 12.176394 loss_att 14.596601 loss_ctc 14.981995 loss_rnnt 11.276267 hw_loss 0.078761 lr 0.00039328 rank 7
2023-02-21 20:52:22,671 DEBUG TRAIN Batch 19/3100 loss 11.128542 loss_att 13.366116 loss_ctc 16.696680 loss_rnnt 9.853173 hw_loss 0.160192 lr 0.00039337 rank 0
2023-02-21 20:52:22,674 DEBUG TRAIN Batch 19/3100 loss 5.954341 loss_att 8.408432 loss_ctc 11.494324 loss_rnnt 4.674905 hw_loss 0.093664 lr 0.00039340 rank 1
2023-02-21 20:52:22,676 DEBUG TRAIN Batch 19/3100 loss 10.155868 loss_att 9.330061 loss_ctc 12.936996 loss_rnnt 9.729713 hw_loss 0.413434 lr 0.00039332 rank 3
2023-02-21 20:52:22,676 DEBUG TRAIN Batch 19/3100 loss 3.958235 loss_att 7.520191 loss_ctc 5.054273 loss_rnnt 3.012807 hw_loss 0.162934 lr 0.00039333 rank 2
2023-02-21 20:52:22,679 DEBUG TRAIN Batch 19/3100 loss 7.380124 loss_att 10.365731 loss_ctc 11.509554 loss_rnnt 6.192441 hw_loss 0.074946 lr 0.00039337 rank 4
2023-02-21 20:52:22,680 DEBUG TRAIN Batch 19/3100 loss 4.164872 loss_att 5.244165 loss_ctc 4.701017 loss_rnnt 3.843464 hw_loss 0.063869 lr 0.00039334 rank 6
2023-02-21 20:52:22,684 DEBUG TRAIN Batch 19/3100 loss 9.579199 loss_att 11.133297 loss_ctc 15.957341 loss_rnnt 8.275943 hw_loss 0.266283 lr 0.00039338 rank 5
2023-02-21 20:53:44,744 DEBUG TRAIN Batch 19/3200 loss 6.972089 loss_att 9.599094 loss_ctc 9.498514 loss_rnnt 6.084376 hw_loss 0.047728 lr 0.00039316 rank 7
2023-02-21 20:53:44,745 DEBUG TRAIN Batch 19/3200 loss 12.185685 loss_att 18.723368 loss_ctc 18.956650 loss_rnnt 9.903658 hw_loss 0.134431 lr 0.00039325 rank 4
2023-02-21 20:53:44,751 DEBUG TRAIN Batch 19/3200 loss 11.968334 loss_att 12.332623 loss_ctc 14.128726 loss_rnnt 11.538856 hw_loss 0.128568 lr 0.00039325 rank 0
2023-02-21 20:53:44,751 DEBUG TRAIN Batch 19/3200 loss 11.151047 loss_att 13.096058 loss_ctc 16.414417 loss_rnnt 9.978030 hw_loss 0.154186 lr 0.00039328 rank 1
2023-02-21 20:53:44,753 DEBUG TRAIN Batch 19/3200 loss 8.167569 loss_att 11.951752 loss_ctc 13.185478 loss_rnnt 6.598125 hw_loss 0.269163 lr 0.00039325 rank 5
2023-02-21 20:53:44,752 DEBUG TRAIN Batch 19/3200 loss 5.889213 loss_att 7.456365 loss_ctc 6.594856 loss_rnnt 5.329201 hw_loss 0.285929 lr 0.00039320 rank 3
2023-02-21 20:53:44,754 DEBUG TRAIN Batch 19/3200 loss 4.987174 loss_att 5.568209 loss_ctc 6.315446 loss_rnnt 4.540303 hw_loss 0.287927 lr 0.00039320 rank 2
2023-02-21 20:53:44,779 DEBUG TRAIN Batch 19/3200 loss 8.640627 loss_att 10.971129 loss_ctc 10.051373 loss_rnnt 7.865394 hw_loss 0.226936 lr 0.00039322 rank 6
2023-02-21 20:55:03,927 DEBUG TRAIN Batch 19/3300 loss 8.475298 loss_att 9.594149 loss_ctc 11.487971 loss_rnnt 7.788970 hw_loss 0.114129 lr 0.00039312 rank 4
2023-02-21 20:55:03,929 DEBUG TRAIN Batch 19/3300 loss 9.031214 loss_att 12.346958 loss_ctc 14.674759 loss_rnnt 7.559449 hw_loss 0.105265 lr 0.00039304 rank 7
2023-02-21 20:55:03,931 DEBUG TRAIN Batch 19/3300 loss 10.730902 loss_att 15.079473 loss_ctc 11.943848 loss_rnnt 9.615783 hw_loss 0.156899 lr 0.00039308 rank 2
2023-02-21 20:55:03,931 DEBUG TRAIN Batch 19/3300 loss 11.292415 loss_att 16.655008 loss_ctc 12.987656 loss_rnnt 9.899386 hw_loss 0.177148 lr 0.00039316 rank 1
2023-02-21 20:55:03,933 DEBUG TRAIN Batch 19/3300 loss 7.889132 loss_att 11.635519 loss_ctc 10.894447 loss_rnnt 6.679967 hw_loss 0.110962 lr 0.00039308 rank 3
2023-02-21 20:55:03,935 DEBUG TRAIN Batch 19/3300 loss 10.627915 loss_att 18.558914 loss_ctc 13.630503 loss_rnnt 8.608665 hw_loss 0.061322 lr 0.00039313 rank 5
2023-02-21 20:55:03,943 DEBUG TRAIN Batch 19/3300 loss 7.520979 loss_att 14.667398 loss_ctc 11.610766 loss_rnnt 5.386678 hw_loss 0.299460 lr 0.00039313 rank 0
2023-02-21 20:55:03,982 DEBUG TRAIN Batch 19/3300 loss 7.624370 loss_att 8.570705 loss_ctc 6.553432 loss_rnnt 7.456714 hw_loss 0.227213 lr 0.00039309 rank 6
2023-02-21 20:56:22,857 DEBUG TRAIN Batch 19/3400 loss 4.383649 loss_att 7.926316 loss_ctc 5.131189 loss_rnnt 3.501556 hw_loss 0.138537 lr 0.00039296 rank 2
2023-02-21 20:56:22,862 DEBUG TRAIN Batch 19/3400 loss 6.735425 loss_att 11.843448 loss_ctc 12.685751 loss_rnnt 4.861114 hw_loss 0.111245 lr 0.00039292 rank 7
2023-02-21 20:56:22,862 DEBUG TRAIN Batch 19/3400 loss 11.883473 loss_att 12.987320 loss_ctc 17.559603 loss_rnnt 10.717648 hw_loss 0.352948 lr 0.00039297 rank 6
2023-02-21 20:56:22,862 DEBUG TRAIN Batch 19/3400 loss 21.593147 loss_att 23.594639 loss_ctc 28.232456 loss_rnnt 20.215988 hw_loss 0.171783 lr 0.00039304 rank 1
2023-02-21 20:56:22,863 DEBUG TRAIN Batch 19/3400 loss 8.002172 loss_att 9.867443 loss_ctc 11.891655 loss_rnnt 7.056907 hw_loss 0.100522 lr 0.00039300 rank 4
2023-02-21 20:56:22,863 DEBUG TRAIN Batch 19/3400 loss 14.638935 loss_att 18.182560 loss_ctc 22.654898 loss_rnnt 12.824411 hw_loss 0.069380 lr 0.00039301 rank 0
2023-02-21 20:56:22,867 DEBUG TRAIN Batch 19/3400 loss 7.354313 loss_att 11.439727 loss_ctc 9.110893 loss_rnnt 6.280794 hw_loss 0.041672 lr 0.00039301 rank 5
2023-02-21 20:56:22,909 DEBUG TRAIN Batch 19/3400 loss 10.701359 loss_att 13.038824 loss_ctc 13.235105 loss_rnnt 9.764038 hw_loss 0.247491 lr 0.00039296 rank 3
2023-02-21 20:57:43,747 DEBUG TRAIN Batch 19/3500 loss 6.058862 loss_att 10.166348 loss_ctc 9.855775 loss_rnnt 4.697566 hw_loss 0.062894 lr 0.00039279 rank 7
2023-02-21 20:57:43,751 DEBUG TRAIN Batch 19/3500 loss 13.357250 loss_att 16.135351 loss_ctc 19.898394 loss_rnnt 11.868400 hw_loss 0.114523 lr 0.00039291 rank 1
2023-02-21 20:57:43,753 DEBUG TRAIN Batch 19/3500 loss 14.472320 loss_att 15.841252 loss_ctc 20.235964 loss_rnnt 13.345042 hw_loss 0.159384 lr 0.00039288 rank 4
2023-02-21 20:57:43,754 DEBUG TRAIN Batch 19/3500 loss 10.836329 loss_att 13.965309 loss_ctc 19.889177 loss_rnnt 8.942474 hw_loss 0.114398 lr 0.00039284 rank 2
2023-02-21 20:57:43,757 DEBUG TRAIN Batch 19/3500 loss 10.115454 loss_att 15.605635 loss_ctc 14.809836 loss_rnnt 8.210602 hw_loss 0.339184 lr 0.00039285 rank 6
2023-02-21 20:57:43,757 DEBUG TRAIN Batch 19/3500 loss 10.179950 loss_att 13.708630 loss_ctc 15.346198 loss_rnnt 8.648017 hw_loss 0.257558 lr 0.00039289 rank 5
2023-02-21 20:57:43,758 DEBUG TRAIN Batch 19/3500 loss 9.824014 loss_att 10.475480 loss_ctc 19.443693 loss_rnnt 8.361898 hw_loss 0.092245 lr 0.00039283 rank 3
2023-02-21 20:57:43,804 DEBUG TRAIN Batch 19/3500 loss 5.757123 loss_att 8.431777 loss_ctc 6.827893 loss_rnnt 5.038966 hw_loss 0.075855 lr 0.00039289 rank 0
2023-02-21 20:59:04,344 DEBUG TRAIN Batch 19/3600 loss 8.757268 loss_att 12.720324 loss_ctc 11.919984 loss_rnnt 7.469800 hw_loss 0.137176 lr 0.00039267 rank 7
2023-02-21 20:59:04,344 DEBUG TRAIN Batch 19/3600 loss 5.889288 loss_att 7.462303 loss_ctc 6.297155 loss_rnnt 5.455705 hw_loss 0.121121 lr 0.00039276 rank 4
2023-02-21 20:59:04,346 DEBUG TRAIN Batch 19/3600 loss 9.525423 loss_att 13.684536 loss_ctc 15.396321 loss_rnnt 7.859303 hw_loss 0.096583 lr 0.00039272 rank 2
2023-02-21 20:59:04,347 DEBUG TRAIN Batch 19/3600 loss 8.483494 loss_att 10.276186 loss_ctc 11.292470 loss_rnnt 7.702107 hw_loss 0.090597 lr 0.00039279 rank 1
2023-02-21 20:59:04,347 DEBUG TRAIN Batch 19/3600 loss 18.696278 loss_att 21.037632 loss_ctc 24.843054 loss_rnnt 17.310030 hw_loss 0.184513 lr 0.00039271 rank 3
2023-02-21 20:59:04,347 DEBUG TRAIN Batch 19/3600 loss 10.288975 loss_att 10.379849 loss_ctc 12.310372 loss_rnnt 9.928513 hw_loss 0.136439 lr 0.00039273 rank 6
2023-02-21 20:59:04,352 DEBUG TRAIN Batch 19/3600 loss 9.638830 loss_att 10.990182 loss_ctc 13.770134 loss_rnnt 8.737116 hw_loss 0.151132 lr 0.00039277 rank 5
2023-02-21 20:59:04,357 DEBUG TRAIN Batch 19/3600 loss 5.790413 loss_att 8.361357 loss_ctc 7.894952 loss_rnnt 4.895394 hw_loss 0.187922 lr 0.00039277 rank 0
2023-02-21 21:00:22,533 DEBUG TRAIN Batch 19/3700 loss 7.797787 loss_att 10.656424 loss_ctc 11.766165 loss_rnnt 6.591151 hw_loss 0.198359 lr 0.00039260 rank 2
2023-02-21 21:00:22,534 DEBUG TRAIN Batch 19/3700 loss 8.540730 loss_att 10.730408 loss_ctc 14.555435 loss_rnnt 7.227654 hw_loss 0.137214 lr 0.00039255 rank 7
2023-02-21 21:00:22,539 DEBUG TRAIN Batch 19/3700 loss 9.134098 loss_att 11.803557 loss_ctc 13.537034 loss_rnnt 7.979105 hw_loss 0.063831 lr 0.00039264 rank 4
2023-02-21 21:00:22,542 DEBUG TRAIN Batch 19/3700 loss 11.010667 loss_att 12.868814 loss_ctc 15.216319 loss_rnnt 10.010730 hw_loss 0.126661 lr 0.00039261 rank 6
2023-02-21 21:00:22,543 DEBUG TRAIN Batch 19/3700 loss 8.806236 loss_att 12.173564 loss_ctc 16.445992 loss_rnnt 6.985451 hw_loss 0.241287 lr 0.00039267 rank 1
2023-02-21 21:00:22,546 DEBUG TRAIN Batch 19/3700 loss 11.728848 loss_att 12.617559 loss_ctc 15.890014 loss_rnnt 10.914751 hw_loss 0.152870 lr 0.00039259 rank 3
2023-02-21 21:00:22,546 DEBUG TRAIN Batch 19/3700 loss 8.929741 loss_att 9.884861 loss_ctc 10.537731 loss_rnnt 8.424780 hw_loss 0.186633 lr 0.00039265 rank 5
2023-02-21 21:00:22,594 DEBUG TRAIN Batch 19/3700 loss 8.075932 loss_att 10.653117 loss_ctc 17.962168 loss_rnnt 6.226576 hw_loss 0.029537 lr 0.00039265 rank 0
2023-02-21 21:01:41,492 DEBUG TRAIN Batch 19/3800 loss 8.204313 loss_att 10.837245 loss_ctc 11.787933 loss_rnnt 7.121152 hw_loss 0.147671 lr 0.00039255 rank 1
2023-02-21 21:01:41,492 DEBUG TRAIN Batch 19/3800 loss 5.800835 loss_att 7.730148 loss_ctc 9.834476 loss_rnnt 4.827864 hw_loss 0.092418 lr 0.00039243 rank 7
2023-02-21 21:01:41,494 DEBUG TRAIN Batch 19/3800 loss 7.058527 loss_att 9.801105 loss_ctc 8.228006 loss_rnnt 6.282489 hw_loss 0.134236 lr 0.00039247 rank 3
2023-02-21 21:01:41,495 DEBUG TRAIN Batch 19/3800 loss 13.720840 loss_att 15.728513 loss_ctc 19.063269 loss_rnnt 12.424075 hw_loss 0.342952 lr 0.00039252 rank 4
2023-02-21 21:01:41,497 DEBUG TRAIN Batch 19/3800 loss 6.348319 loss_att 8.735281 loss_ctc 9.328496 loss_rnnt 5.395463 hw_loss 0.146450 lr 0.00039252 rank 0
2023-02-21 21:01:41,498 DEBUG TRAIN Batch 19/3800 loss 4.456805 loss_att 7.861578 loss_ctc 5.888680 loss_rnnt 3.504106 hw_loss 0.151551 lr 0.00039253 rank 5
2023-02-21 21:01:41,495 DEBUG TRAIN Batch 19/3800 loss 5.371855 loss_att 6.745380 loss_ctc 8.347953 loss_rnnt 4.612845 hw_loss 0.164047 lr 0.00039248 rank 2
2023-02-21 21:01:41,541 DEBUG TRAIN Batch 19/3800 loss 9.643976 loss_att 13.789164 loss_ctc 15.084702 loss_rnnt 8.055152 hw_loss 0.064420 lr 0.00039249 rank 6
2023-02-21 21:03:02,977 DEBUG TRAIN Batch 19/3900 loss 28.840700 loss_att 33.888046 loss_ctc 45.109459 loss_rnnt 25.626877 hw_loss 0.065969 lr 0.00039236 rank 2
2023-02-21 21:03:02,979 DEBUG TRAIN Batch 19/3900 loss 6.560163 loss_att 8.781733 loss_ctc 8.240248 loss_rnnt 5.818211 hw_loss 0.138052 lr 0.00039231 rank 7
2023-02-21 21:03:02,980 DEBUG TRAIN Batch 19/3900 loss 7.403224 loss_att 9.744544 loss_ctc 8.706753 loss_rnnt 6.675651 hw_loss 0.160326 lr 0.00039240 rank 4
2023-02-21 21:03:02,981 DEBUG TRAIN Batch 19/3900 loss 6.066346 loss_att 8.038361 loss_ctc 6.911516 loss_rnnt 5.482122 hw_loss 0.144621 lr 0.00039235 rank 3
2023-02-21 21:03:02,984 DEBUG TRAIN Batch 19/3900 loss 8.796728 loss_att 7.934481 loss_ctc 10.372786 loss_rnnt 8.676298 hw_loss 0.155135 lr 0.00039243 rank 1
2023-02-21 21:03:03,013 DEBUG TRAIN Batch 19/3900 loss 11.588253 loss_att 17.874353 loss_ctc 21.953487 loss_rnnt 8.915220 hw_loss 0.063342 lr 0.00039237 rank 6
2023-02-21 21:03:03,027 DEBUG TRAIN Batch 19/3900 loss 8.491335 loss_att 11.158379 loss_ctc 11.634865 loss_rnnt 7.429412 hw_loss 0.205080 lr 0.00039240 rank 5
2023-02-21 21:03:03,093 DEBUG TRAIN Batch 19/3900 loss 9.812550 loss_att 12.597347 loss_ctc 15.902922 loss_rnnt 8.376703 hw_loss 0.125317 lr 0.00039240 rank 0
2023-02-21 21:04:22,467 DEBUG TRAIN Batch 19/4000 loss 4.776455 loss_att 9.776149 loss_ctc 8.734852 loss_rnnt 3.207144 hw_loss 0.077973 lr 0.00039228 rank 4
2023-02-21 21:04:22,472 DEBUG TRAIN Batch 19/4000 loss 15.095132 loss_att 15.319517 loss_ctc 22.794935 loss_rnnt 14.010597 hw_loss 0.024407 lr 0.00039219 rank 7
2023-02-21 21:04:22,473 DEBUG TRAIN Batch 19/4000 loss 8.849575 loss_att 12.206165 loss_ctc 15.553856 loss_rnnt 7.195657 hw_loss 0.166305 lr 0.00039231 rank 1
2023-02-21 21:04:22,474 DEBUG TRAIN Batch 19/4000 loss 14.940092 loss_att 17.596476 loss_ctc 20.968390 loss_rnnt 13.581646 hw_loss 0.043869 lr 0.00039223 rank 3
2023-02-21 21:04:22,476 DEBUG TRAIN Batch 19/4000 loss 5.232984 loss_att 7.559148 loss_ctc 6.458146 loss_rnnt 4.579859 hw_loss 0.046006 lr 0.00039225 rank 6
2023-02-21 21:04:22,477 DEBUG TRAIN Batch 19/4000 loss 12.679500 loss_att 18.583727 loss_ctc 18.240576 loss_rnnt 10.680446 hw_loss 0.143875 lr 0.00039223 rank 2
2023-02-21 21:04:22,480 DEBUG TRAIN Batch 19/4000 loss 8.759160 loss_att 13.139071 loss_ctc 14.659557 loss_rnnt 7.075602 hw_loss 0.039106 lr 0.00039228 rank 5
2023-02-21 21:04:22,481 DEBUG TRAIN Batch 19/4000 loss 10.067826 loss_att 11.843063 loss_ctc 11.373645 loss_rnnt 9.512774 hw_loss 0.048555 lr 0.00039228 rank 0
2023-02-21 21:05:41,288 DEBUG TRAIN Batch 19/4100 loss 9.191067 loss_att 11.687829 loss_ctc 14.689717 loss_rnnt 7.890848 hw_loss 0.126963 lr 0.00039211 rank 3
2023-02-21 21:05:41,290 DEBUG TRAIN Batch 19/4100 loss 0.967355 loss_att 3.705099 loss_ctc 1.465884 loss_rnnt 0.302284 hw_loss 0.095720 lr 0.00039216 rank 4
2023-02-21 21:05:41,291 DEBUG TRAIN Batch 19/4100 loss 12.021667 loss_att 14.753426 loss_ctc 17.094810 loss_rnnt 10.762159 hw_loss 0.068882 lr 0.00039213 rank 6
2023-02-21 21:05:41,292 DEBUG TRAIN Batch 19/4100 loss 4.912155 loss_att 7.348697 loss_ctc 8.179804 loss_rnnt 3.892615 hw_loss 0.181021 lr 0.00039216 rank 0
2023-02-21 21:05:41,293 DEBUG TRAIN Batch 19/4100 loss 6.119582 loss_att 9.040190 loss_ctc 10.174574 loss_rnnt 4.856114 hw_loss 0.260026 lr 0.00039207 rank 7
2023-02-21 21:05:41,293 DEBUG TRAIN Batch 19/4100 loss 5.357594 loss_att 6.922692 loss_ctc 8.262128 loss_rnnt 4.564950 hw_loss 0.173164 lr 0.00039211 rank 2
2023-02-21 21:05:41,294 DEBUG TRAIN Batch 19/4100 loss 4.530833 loss_att 8.272166 loss_ctc 7.698628 loss_rnnt 3.275091 hw_loss 0.159568 lr 0.00039219 rank 1
2023-02-21 21:05:41,340 DEBUG TRAIN Batch 19/4100 loss 7.606201 loss_att 9.861311 loss_ctc 10.567329 loss_rnnt 6.717393 hw_loss 0.080565 lr 0.00039216 rank 5
2023-02-21 21:07:00,266 DEBUG TRAIN Batch 19/4200 loss 4.021790 loss_att 7.278581 loss_ctc 8.034253 loss_rnnt 2.774316 hw_loss 0.114600 lr 0.00039195 rank 7
2023-02-21 21:07:00,266 DEBUG TRAIN Batch 19/4200 loss 12.095467 loss_att 11.191319 loss_ctc 17.222042 loss_rnnt 11.568192 hw_loss 0.046050 lr 0.00039207 rank 1
2023-02-21 21:07:00,270 DEBUG TRAIN Batch 19/4200 loss 5.677026 loss_att 9.286774 loss_ctc 8.133469 loss_rnnt 4.546450 hw_loss 0.152063 lr 0.00039204 rank 5
2023-02-21 21:07:00,271 DEBUG TRAIN Batch 19/4200 loss 6.491375 loss_att 9.410962 loss_ctc 8.302069 loss_rnnt 5.582417 hw_loss 0.156777 lr 0.00039201 rank 6
2023-02-21 21:07:00,273 DEBUG TRAIN Batch 19/4200 loss 7.469075 loss_att 10.208773 loss_ctc 13.297168 loss_rnnt 6.107711 hw_loss 0.068149 lr 0.00039203 rank 4
2023-02-21 21:07:00,274 DEBUG TRAIN Batch 19/4200 loss 4.722872 loss_att 6.667439 loss_ctc 7.405776 loss_rnnt 3.917536 hw_loss 0.110066 lr 0.00039199 rank 3
2023-02-21 21:07:00,277 DEBUG TRAIN Batch 19/4200 loss 6.721500 loss_att 10.868653 loss_ctc 11.990849 loss_rnnt 5.171933 hw_loss 0.032917 lr 0.00039204 rank 0
2023-02-21 21:07:00,277 DEBUG TRAIN Batch 19/4200 loss 4.088648 loss_att 7.853267 loss_ctc 5.439803 loss_rnnt 3.067687 hw_loss 0.164781 lr 0.00039199 rank 2
2023-02-21 21:08:20,598 DEBUG TRAIN Batch 19/4300 loss 7.722128 loss_att 9.490762 loss_ctc 9.688009 loss_rnnt 7.088964 hw_loss 0.032475 lr 0.00039187 rank 2
2023-02-21 21:08:20,601 DEBUG TRAIN Batch 19/4300 loss 13.092137 loss_att 14.336872 loss_ctc 17.011618 loss_rnnt 12.282958 hw_loss 0.070567 lr 0.00039189 rank 6
2023-02-21 21:08:20,602 DEBUG TRAIN Batch 19/4300 loss 10.561449 loss_att 13.063799 loss_ctc 16.273888 loss_rnnt 9.209284 hw_loss 0.168819 lr 0.00039183 rank 7
2023-02-21 21:08:20,604 DEBUG TRAIN Batch 19/4300 loss 10.020775 loss_att 11.739889 loss_ctc 10.640753 loss_rnnt 9.526423 hw_loss 0.127249 lr 0.00039191 rank 4
2023-02-21 21:08:20,603 DEBUG TRAIN Batch 19/4300 loss 5.197384 loss_att 8.016451 loss_ctc 7.021829 loss_rnnt 4.366136 hw_loss 0.045329 lr 0.00039195 rank 1
2023-02-21 21:08:20,606 DEBUG TRAIN Batch 19/4300 loss 10.897360 loss_att 12.990179 loss_ctc 17.043030 loss_rnnt 9.587857 hw_loss 0.134092 lr 0.00039187 rank 3
2023-02-21 21:08:20,611 DEBUG TRAIN Batch 19/4300 loss 8.712610 loss_att 10.083357 loss_ctc 12.972879 loss_rnnt 7.834043 hw_loss 0.068216 lr 0.00039192 rank 5
2023-02-21 21:08:20,652 DEBUG TRAIN Batch 19/4300 loss 15.834731 loss_att 21.451815 loss_ctc 22.007969 loss_rnnt 13.861828 hw_loss 0.049476 lr 0.00039192 rank 0
2023-02-21 21:09:40,758 DEBUG TRAIN Batch 19/4400 loss 4.566457 loss_att 6.577385 loss_ctc 5.753289 loss_rnnt 3.934162 hw_loss 0.134748 lr 0.00039183 rank 1
2023-02-21 21:09:40,762 DEBUG TRAIN Batch 19/4400 loss 11.392037 loss_att 12.125590 loss_ctc 15.875478 loss_rnnt 10.499654 hw_loss 0.277278 lr 0.00039175 rank 3
2023-02-21 21:09:40,763 DEBUG TRAIN Batch 19/4400 loss 6.267881 loss_att 9.134457 loss_ctc 9.971863 loss_rnnt 5.126790 hw_loss 0.138584 lr 0.00039175 rank 2
2023-02-21 21:09:40,763 DEBUG TRAIN Batch 19/4400 loss 5.426333 loss_att 7.596274 loss_ctc 8.481425 loss_rnnt 4.494779 hw_loss 0.169162 lr 0.00039179 rank 4
2023-02-21 21:09:40,764 DEBUG TRAIN Batch 19/4400 loss 9.046260 loss_att 10.851037 loss_ctc 11.949669 loss_rnnt 8.237647 hw_loss 0.113507 lr 0.00039171 rank 7
2023-02-21 21:09:40,771 DEBUG TRAIN Batch 19/4400 loss 15.521888 loss_att 22.508625 loss_ctc 23.072605 loss_rnnt 13.001221 hw_loss 0.218545 lr 0.00039180 rank 5
2023-02-21 21:09:40,771 DEBUG TRAIN Batch 19/4400 loss 14.489196 loss_att 18.412354 loss_ctc 23.291622 loss_rnnt 12.468422 hw_loss 0.117158 lr 0.00039176 rank 6
2023-02-21 21:09:40,808 DEBUG TRAIN Batch 19/4400 loss 17.014381 loss_att 21.067528 loss_ctc 25.669214 loss_rnnt 14.990817 hw_loss 0.110545 lr 0.00039180 rank 0
2023-02-21 21:10:59,188 DEBUG TRAIN Batch 19/4500 loss 15.743606 loss_att 16.775543 loss_ctc 18.228338 loss_rnnt 15.144529 hw_loss 0.115109 lr 0.00039163 rank 2
2023-02-21 21:10:59,189 DEBUG TRAIN Batch 19/4500 loss 5.552884 loss_att 9.879190 loss_ctc 8.677787 loss_rnnt 4.224765 hw_loss 0.086632 lr 0.00039168 rank 5
2023-02-21 21:10:59,189 DEBUG TRAIN Batch 19/4500 loss 16.869205 loss_att 16.507423 loss_ctc 27.859421 loss_rnnt 15.382571 hw_loss 0.175552 lr 0.00039164 rank 6
2023-02-21 21:10:59,190 DEBUG TRAIN Batch 19/4500 loss 2.910300 loss_att 5.686860 loss_ctc 3.819085 loss_rnnt 2.195687 hw_loss 0.071494 lr 0.00039159 rank 7
2023-02-21 21:10:59,190 DEBUG TRAIN Batch 19/4500 loss 6.129888 loss_att 8.133227 loss_ctc 8.164942 loss_rnnt 5.340478 hw_loss 0.220128 lr 0.00039171 rank 1
2023-02-21 21:10:59,191 DEBUG TRAIN Batch 19/4500 loss 7.970678 loss_att 8.653015 loss_ctc 9.669718 loss_rnnt 7.347886 hw_loss 0.487100 lr 0.00039167 rank 4
2023-02-21 21:10:59,192 DEBUG TRAIN Batch 19/4500 loss 6.059307 loss_att 7.826259 loss_ctc 9.394524 loss_rnnt 5.101109 hw_loss 0.300210 lr 0.00039168 rank 0
2023-02-21 21:10:59,199 DEBUG TRAIN Batch 19/4500 loss 2.348763 loss_att 6.246804 loss_ctc 2.993865 loss_rnnt 1.393656 hw_loss 0.167782 lr 0.00039163 rank 3
2023-02-21 21:12:19,702 DEBUG TRAIN Batch 19/4600 loss 6.823736 loss_att 10.877308 loss_ctc 11.862830 loss_rnnt 5.298972 hw_loss 0.079070 lr 0.00039152 rank 6
2023-02-21 21:12:19,704 DEBUG TRAIN Batch 19/4600 loss 15.426047 loss_att 19.408379 loss_ctc 23.791113 loss_rnnt 13.409427 hw_loss 0.196525 lr 0.00039159 rank 1
2023-02-21 21:12:19,704 DEBUG TRAIN Batch 19/4600 loss 9.156497 loss_att 14.346227 loss_ctc 17.733908 loss_rnnt 6.942409 hw_loss 0.060914 lr 0.00039147 rank 7
2023-02-21 21:12:19,710 DEBUG TRAIN Batch 19/4600 loss 7.593164 loss_att 10.743553 loss_ctc 11.312588 loss_rnnt 6.383178 hw_loss 0.157472 lr 0.00039155 rank 4
2023-02-21 21:12:19,710 DEBUG TRAIN Batch 19/4600 loss 6.711349 loss_att 8.345034 loss_ctc 8.723915 loss_rnnt 6.042017 hw_loss 0.139225 lr 0.00039151 rank 2
2023-02-21 21:12:19,711 DEBUG TRAIN Batch 19/4600 loss 4.410542 loss_att 8.401626 loss_ctc 4.364438 loss_rnnt 3.544373 hw_loss 0.138936 lr 0.00039156 rank 5
2023-02-21 21:12:19,719 DEBUG TRAIN Batch 19/4600 loss 6.042726 loss_att 10.131841 loss_ctc 7.179410 loss_rnnt 4.929471 hw_loss 0.269765 lr 0.00039151 rank 3
2023-02-21 21:12:19,752 DEBUG TRAIN Batch 19/4600 loss 15.560923 loss_att 18.487410 loss_ctc 19.555954 loss_rnnt 14.396130 hw_loss 0.087798 lr 0.00039156 rank 0
2023-02-21 21:13:38,922 DEBUG TRAIN Batch 19/4700 loss 7.408642 loss_att 10.042837 loss_ctc 8.823966 loss_rnnt 6.514710 hw_loss 0.334466 lr 0.00039147 rank 1
2023-02-21 21:13:38,924 DEBUG TRAIN Batch 19/4700 loss 9.174346 loss_att 12.207565 loss_ctc 11.267631 loss_rnnt 8.225931 hw_loss 0.117499 lr 0.00039143 rank 4
2023-02-21 21:13:38,925 DEBUG TRAIN Batch 19/4700 loss 9.811712 loss_att 12.050682 loss_ctc 14.033508 loss_rnnt 8.713026 hw_loss 0.164974 lr 0.00039139 rank 2
2023-02-21 21:13:38,925 DEBUG TRAIN Batch 19/4700 loss 9.124152 loss_att 15.639058 loss_ctc 11.995574 loss_rnnt 7.400900 hw_loss 0.070152 lr 0.00039135 rank 7
2023-02-21 21:13:38,928 DEBUG TRAIN Batch 19/4700 loss 3.775543 loss_att 8.279285 loss_ctc 6.605030 loss_rnnt 2.466792 hw_loss 0.057632 lr 0.00039144 rank 5
2023-02-21 21:13:38,930 DEBUG TRAIN Batch 19/4700 loss 9.813328 loss_att 13.060078 loss_ctc 12.671124 loss_rnnt 8.768778 hw_loss 0.026550 lr 0.00039144 rank 0
2023-02-21 21:13:38,931 DEBUG TRAIN Batch 19/4700 loss 8.234907 loss_att 12.023895 loss_ctc 10.849325 loss_rnnt 7.041321 hw_loss 0.163498 lr 0.00039140 rank 6
2023-02-21 21:13:38,934 DEBUG TRAIN Batch 19/4700 loss 7.665882 loss_att 8.884665 loss_ctc 7.426387 loss_rnnt 7.366332 hw_loss 0.164486 lr 0.00039139 rank 3
2023-02-21 21:14:56,450 DEBUG TRAIN Batch 19/4800 loss 14.129748 loss_att 15.351972 loss_ctc 20.663494 loss_rnnt 12.941942 hw_loss 0.135368 lr 0.00039128 rank 6
2023-02-21 21:14:56,452 DEBUG TRAIN Batch 19/4800 loss 6.615164 loss_att 11.099057 loss_ctc 9.772667 loss_rnnt 5.191591 hw_loss 0.198364 lr 0.00039131 rank 4
2023-02-21 21:14:56,453 DEBUG TRAIN Batch 19/4800 loss 12.304756 loss_att 14.883391 loss_ctc 13.999144 loss_rnnt 11.493696 hw_loss 0.130153 lr 0.00039127 rank 2
2023-02-21 21:14:56,454 DEBUG TRAIN Batch 19/4800 loss 22.137568 loss_att 24.929914 loss_ctc 34.395519 loss_rnnt 19.872511 hw_loss 0.135364 lr 0.00039123 rank 7
2023-02-21 21:14:56,455 DEBUG TRAIN Batch 19/4800 loss 7.207325 loss_att 11.346098 loss_ctc 9.282712 loss_rnnt 6.020199 hw_loss 0.154973 lr 0.00039135 rank 1
2023-02-21 21:14:56,455 DEBUG TRAIN Batch 19/4800 loss 6.868174 loss_att 12.059081 loss_ctc 11.749441 loss_rnnt 5.105838 hw_loss 0.137474 lr 0.00039127 rank 3
2023-02-21 21:14:56,459 DEBUG TRAIN Batch 19/4800 loss 16.401628 loss_att 17.316319 loss_ctc 18.002686 loss_rnnt 15.953387 hw_loss 0.097181 lr 0.00039132 rank 5
2023-02-21 21:14:56,503 DEBUG TRAIN Batch 19/4800 loss 6.444606 loss_att 7.773832 loss_ctc 7.392620 loss_rnnt 5.949365 hw_loss 0.193114 lr 0.00039132 rank 0
2023-02-21 21:16:15,943 DEBUG TRAIN Batch 19/4900 loss 14.546411 loss_att 16.653313 loss_ctc 16.738464 loss_rnnt 13.785544 hw_loss 0.088523 lr 0.00039111 rank 7
2023-02-21 21:16:15,944 DEBUG TRAIN Batch 19/4900 loss 10.344499 loss_att 14.925777 loss_ctc 13.982007 loss_rnnt 8.882593 hw_loss 0.113717 lr 0.00039115 rank 2
2023-02-21 21:16:15,948 DEBUG TRAIN Batch 19/4900 loss 6.209278 loss_att 9.534541 loss_ctc 10.788412 loss_rnnt 4.905778 hw_loss 0.052303 lr 0.00039119 rank 4
2023-02-21 21:16:15,950 DEBUG TRAIN Batch 19/4900 loss 6.405850 loss_att 8.353286 loss_ctc 9.781517 loss_rnnt 5.547425 hw_loss 0.035342 lr 0.00039115 rank 3
2023-02-21 21:16:15,953 DEBUG TRAIN Batch 19/4900 loss 8.890548 loss_att 11.083290 loss_ctc 13.206661 loss_rnnt 7.820508 hw_loss 0.105016 lr 0.00039120 rank 5
2023-02-21 21:16:15,956 DEBUG TRAIN Batch 19/4900 loss 7.396372 loss_att 10.112228 loss_ctc 10.009885 loss_rnnt 6.314847 hw_loss 0.356035 lr 0.00039123 rank 1
2023-02-21 21:16:15,957 DEBUG TRAIN Batch 19/4900 loss 6.916103 loss_att 8.430863 loss_ctc 9.666142 loss_rnnt 6.130145 hw_loss 0.218127 lr 0.00039120 rank 0
2023-02-21 21:16:15,977 DEBUG TRAIN Batch 19/4900 loss 14.731861 loss_att 18.586346 loss_ctc 21.114235 loss_rnnt 13.011069 hw_loss 0.185462 lr 0.00039116 rank 6
2023-02-21 21:17:37,079 DEBUG TRAIN Batch 19/5000 loss 8.346178 loss_att 9.042494 loss_ctc 9.917775 loss_rnnt 7.935070 hw_loss 0.116812 lr 0.00039107 rank 4
2023-02-21 21:17:37,083 DEBUG TRAIN Batch 19/5000 loss 10.873109 loss_att 16.724752 loss_ctc 15.430097 loss_rnnt 9.054277 hw_loss 0.076697 lr 0.00039103 rank 2
2023-02-21 21:17:37,087 DEBUG TRAIN Batch 19/5000 loss 4.792940 loss_att 7.600700 loss_ctc 7.168601 loss_rnnt 3.871629 hw_loss 0.080632 lr 0.00039103 rank 3
2023-02-21 21:17:37,087 DEBUG TRAIN Batch 19/5000 loss 7.115248 loss_att 9.472734 loss_ctc 10.391083 loss_rnnt 6.171185 hw_loss 0.067099 lr 0.00039099 rank 7
2023-02-21 21:17:37,088 DEBUG TRAIN Batch 19/5000 loss 6.030496 loss_att 9.149910 loss_ctc 9.033829 loss_rnnt 4.923975 hw_loss 0.154113 lr 0.00039111 rank 1
2023-02-21 21:17:37,090 DEBUG TRAIN Batch 19/5000 loss 10.920746 loss_att 12.070278 loss_ctc 14.291095 loss_rnnt 10.105020 hw_loss 0.255823 lr 0.00039108 rank 0
2023-02-21 21:17:37,094 DEBUG TRAIN Batch 19/5000 loss 5.688735 loss_att 6.956260 loss_ctc 7.986180 loss_rnnt 5.052851 hw_loss 0.142602 lr 0.00039105 rank 6
2023-02-21 21:17:37,100 DEBUG TRAIN Batch 19/5000 loss 13.949144 loss_att 15.598997 loss_ctc 17.858532 loss_rnnt 13.000961 hw_loss 0.181801 lr 0.00039108 rank 5
2023-02-21 21:18:53,971 DEBUG TRAIN Batch 19/5100 loss 14.927863 loss_att 16.046101 loss_ctc 22.960386 loss_rnnt 13.599826 hw_loss 0.062601 lr 0.00039087 rank 7
2023-02-21 21:18:53,977 DEBUG TRAIN Batch 19/5100 loss 7.683122 loss_att 10.374247 loss_ctc 12.334428 loss_rnnt 6.461123 hw_loss 0.119248 lr 0.00039096 rank 0
2023-02-21 21:18:53,980 DEBUG TRAIN Batch 19/5100 loss 6.697428 loss_att 11.223505 loss_ctc 11.765806 loss_rnnt 5.057205 hw_loss 0.111044 lr 0.00039093 rank 6
2023-02-21 21:18:53,982 DEBUG TRAIN Batch 19/5100 loss 9.805291 loss_att 10.224021 loss_ctc 11.533182 loss_rnnt 9.307975 hw_loss 0.343472 lr 0.00039091 rank 2
2023-02-21 21:18:53,983 DEBUG TRAIN Batch 19/5100 loss 13.941945 loss_att 15.103602 loss_ctc 15.746804 loss_rnnt 13.396731 hw_loss 0.135441 lr 0.00039095 rank 4
2023-02-21 21:18:53,986 DEBUG TRAIN Batch 19/5100 loss 13.947445 loss_att 14.109840 loss_ctc 18.012787 loss_rnnt 13.237209 hw_loss 0.254456 lr 0.00039099 rank 1
2023-02-21 21:18:53,991 DEBUG TRAIN Batch 19/5100 loss 12.573907 loss_att 14.925856 loss_ctc 19.972637 loss_rnnt 11.102935 hw_loss 0.026409 lr 0.00039096 rank 5
2023-02-21 21:18:54,032 DEBUG TRAIN Batch 19/5100 loss 4.812854 loss_att 8.797314 loss_ctc 10.004339 loss_rnnt 3.271343 hw_loss 0.098289 lr 0.00039091 rank 3
run_2_20_rnnt_bias_both_2_class_more_layers_fintune.sh: line 166: 13812 Terminated              python wenet/bin/train.py --gpu $gpu_id --config $train_config --data_type raw --symbol_table $dict --bpe_model ${bpemodel}.model --train_data $wave_data/$train_set/data.list --cv_data $wave_data/$dev_set/data.list ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --ddp.init_method $init_method --ddp.world_size $num_gpus --ddp.rank $i --ddp.dist_backend $dist_backend --num_workers 1 $cmvn_opts --pin_memory
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [127.0.0.1]:2202: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [127.0.1.1]:35763: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:25995
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:23988
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:5001
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:6560
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.1.1]:45447
