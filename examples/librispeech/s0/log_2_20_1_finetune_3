/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_20_rnnt_bias_both_2_class_more_layers_0-3word_fintune.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/ddp_init
2023-02-22 22:30:10,948 INFO training on multiple gpus, this gpu 4
2023-02-22 22:30:10,948 INFO training on multiple gpus, this gpu 6
2023-02-22 22:30:10,950 INFO training on multiple gpus, this gpu 5
2023-02-22 22:30:10,951 INFO training on multiple gpus, this gpu 0
2023-02-22 22:30:10,952 INFO training on multiple gpus, this gpu 2
2023-02-22 22:30:10,953 INFO training on multiple gpus, this gpu 1
2023-02-22 22:30:11,077 INFO training on multiple gpus, this gpu 3
2023-02-22 22:30:11,110 INFO training on multiple gpus, this gpu 7
2023-02-22 22:30:17,059 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-22 22:30:18,136 INFO Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-22 22:30:19,150 INFO Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-22 22:30:19,158 INFO Added key: store_based_barrier_key:1 to store for rank: 5
2023-02-22 22:30:20,138 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-22 22:30:20,174 INFO Added key: store_based_barrier_key:1 to store for rank: 7
2023-02-22 22:30:22,195 INFO Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-22 22:30:26,448 INFO Added key: store_based_barrier_key:1 to store for rank: 6
2023-02-22 22:30:26,449 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:30:26,450 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:30:26,474 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:30:27,465 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:30:27,729 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:30:28,750 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:30:29,481 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:30:33,612 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-22 22:30:36,550 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/10.pt for GPU
2023-02-22 22:30:36,572 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/10.pt for GPU
2023-02-22 22:30:36,595 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/10.pt for GPU
2023-02-22 22:30:36,619 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/10.pt for GPU
2023-02-22 22:30:36,642 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/10.pt for GPU
2023-02-22 22:30:36,667 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/10.pt for GPU
2023-02-22 22:30:36,691 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/10.pt for GPU
2023-02-22 22:30:36,717 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/10.pt for GPU
2023-02-22 22:30:52,328 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:30:52,330 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-22 22:30:52,338 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:30:52,340 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-22 22:30:52,360 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:30:52,361 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-22 22:30:52,386 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:30:52,388 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-22 22:30:52,399 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:30:52,401 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-22 22:30:52,442 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:30:52,443 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:30:52,444 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-22 22:30:52,454 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-22 22:30:52,465 INFO Epoch 11 TRAIN info lr 4e-08
2023-02-22 22:30:52,468 INFO using accumulate grad, new batch size is 1 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-02-22 22:31:55,298 DEBUG TRAIN Batch 11/0 loss 238.124832 loss_att 72.836639 loss_ctc 939.256470 loss_rnnt 177.312332 hw_loss 0.723563 lr 0.00052195 rank 3
2023-02-22 22:31:55,338 DEBUG TRAIN Batch 11/0 loss 352.157440 loss_att 77.064438 loss_ctc 692.446106 loss_rnnt 361.429901 hw_loss 0.701935 lr 0.00052195 rank 7
2023-02-22 22:31:55,378 DEBUG TRAIN Batch 11/0 loss 181.520660 loss_att 78.969650 loss_ctc 315.674072 loss_rnnt 183.767258 hw_loss 0.705897 lr 0.00052195 rank 2
2023-02-22 22:31:55,380 DEBUG TRAIN Batch 11/0 loss 231.906113 loss_att 79.490616 loss_ctc 869.420593 loss_rnnt 176.984772 hw_loss 0.754645 lr 0.00052195 rank 5
2023-02-22 22:31:55,399 DEBUG TRAIN Batch 11/0 loss 298.327179 loss_att 83.733780 loss_ctc 731.040649 loss_rnnt 283.156219 hw_loss 0.739743 lr 0.00052195 rank 0
2023-02-22 22:31:55,405 DEBUG TRAIN Batch 11/0 loss 289.480286 loss_att 79.756813 loss_ctc 1007.154846 loss_rnnt 235.339066 hw_loss 0.742398 lr 0.00052195 rank 4
2023-02-22 22:31:55,414 DEBUG TRAIN Batch 11/0 loss 200.486237 loss_att 71.173340 loss_ctc 597.392090 loss_rnnt 172.978348 hw_loss 0.843144 lr 0.00052195 rank 6
2023-02-22 22:31:55,425 DEBUG TRAIN Batch 11/0 loss 225.240677 loss_att 77.936249 loss_ctc 400.515076 loss_rnnt 230.967545 hw_loss 0.682625 lr 0.00052195 rank 1
2023-02-22 22:33:11,725 DEBUG TRAIN Batch 11/100 loss 228.554077 loss_att 270.170654 loss_ctc 243.474487 loss_rnnt 217.991364 hw_loss 0.468760 lr 0.00052167 rank 7
2023-02-22 22:33:11,733 DEBUG TRAIN Batch 11/100 loss 203.175644 loss_att 239.393448 loss_ctc 194.698242 loss_rnnt 196.877441 hw_loss 0.346776 lr 0.00052167 rank 1
2023-02-22 22:33:11,736 DEBUG TRAIN Batch 11/100 loss 232.751495 loss_att 291.558746 loss_ctc 230.096909 loss_rnnt 221.129501 hw_loss 0.402166 lr 0.00052167 rank 3
2023-02-22 22:33:11,738 DEBUG TRAIN Batch 11/100 loss 259.118744 loss_att 294.807465 loss_ctc 264.598114 loss_rnnt 251.061752 hw_loss 0.353791 lr 0.00052167 rank 6
2023-02-22 22:33:11,738 DEBUG TRAIN Batch 11/100 loss 257.323486 loss_att 310.078766 loss_ctc 253.596237 loss_rnnt 247.077759 hw_loss 0.359345 lr 0.00052167 rank 0
2023-02-22 22:33:11,740 DEBUG TRAIN Batch 11/100 loss 299.370117 loss_att 333.044769 loss_ctc 299.932861 loss_rnnt 292.370026 hw_loss 0.356425 lr 0.00052167 rank 4
2023-02-22 22:33:11,744 DEBUG TRAIN Batch 11/100 loss 268.790253 loss_att 320.035126 loss_ctc 250.948044 loss_rnnt 260.762726 hw_loss 0.295322 lr 0.00052167 rank 5
2023-02-22 22:33:11,748 DEBUG TRAIN Batch 11/100 loss 211.590958 loss_att 256.361664 loss_ctc 213.656372 loss_rnnt 202.159607 hw_loss 0.378414 lr 0.00052167 rank 2
2023-02-22 22:34:24,933 DEBUG TRAIN Batch 11/200 loss 137.027374 loss_att 257.510681 loss_ctc 118.554901 loss_rnnt 115.167923 hw_loss 0.423342 lr 0.00052139 rank 0
2023-02-22 22:34:24,945 DEBUG TRAIN Batch 11/200 loss 142.244995 loss_att 253.457092 loss_ctc 122.445114 loss_rnnt 122.448242 hw_loss 0.364360 lr 0.00052139 rank 7
2023-02-22 22:34:24,946 DEBUG TRAIN Batch 11/200 loss 176.268463 loss_att 273.042847 loss_ctc 157.553711 loss_rnnt 159.213242 hw_loss 0.366816 lr 0.00052139 rank 4
2023-02-22 22:34:24,948 DEBUG TRAIN Batch 11/200 loss 152.122696 loss_att 286.467560 loss_ctc 139.470398 loss_rnnt 126.720886 hw_loss 0.412138 lr 0.00052139 rank 5
2023-02-22 22:34:24,949 DEBUG TRAIN Batch 11/200 loss 135.795395 loss_att 235.291382 loss_ctc 113.971153 loss_rnnt 118.535515 hw_loss 0.507320 lr 0.00052139 rank 3
2023-02-22 22:34:24,950 DEBUG TRAIN Batch 11/200 loss 189.335464 loss_att 312.700623 loss_ctc 168.319885 loss_rnnt 167.282791 hw_loss 0.340736 lr 0.00052139 rank 6
2023-02-22 22:34:24,953 DEBUG TRAIN Batch 11/200 loss 175.268127 loss_att 288.803650 loss_ctc 150.933411 loss_rnnt 155.611359 hw_loss 0.364315 lr 0.00052139 rank 1
2023-02-22 22:34:24,959 DEBUG TRAIN Batch 11/200 loss 213.777618 loss_att 290.353577 loss_ctc 208.479736 loss_rnnt 198.902374 hw_loss 0.499520 lr 0.00052139 rank 2
2023-02-22 22:35:40,236 DEBUG TRAIN Batch 11/300 loss 135.680206 loss_att 277.163147 loss_ctc 118.430260 loss_rnnt 109.488159 hw_loss 0.366481 lr 0.00052110 rank 4
2023-02-22 22:35:40,242 DEBUG TRAIN Batch 11/300 loss 111.848633 loss_att 245.146347 loss_ctc 88.260376 loss_rnnt 88.121765 hw_loss 0.398297 lr 0.00052110 rank 6
2023-02-22 22:35:40,242 DEBUG TRAIN Batch 11/300 loss 126.317757 loss_att 256.395538 loss_ctc 118.795578 loss_rnnt 101.108276 hw_loss 0.369172 lr 0.00052110 rank 7
2023-02-22 22:35:40,257 DEBUG TRAIN Batch 11/300 loss 158.674408 loss_att 301.050903 loss_ctc 135.361420 loss_rnnt 133.108154 hw_loss 0.373755 lr 0.00052110 rank 3
2023-02-22 22:35:40,260 DEBUG TRAIN Batch 11/300 loss 122.009140 loss_att 245.569870 loss_ctc 110.636520 loss_rnnt 98.573898 hw_loss 0.448952 lr 0.00052110 rank 1
2023-02-22 22:35:40,270 DEBUG TRAIN Batch 11/300 loss 147.412872 loss_att 292.801300 loss_ctc 130.455765 loss_rnnt 120.379509 hw_loss 0.406196 lr 0.00052110 rank 0
2023-02-22 22:35:40,274 DEBUG TRAIN Batch 11/300 loss 132.811203 loss_att 258.137238 loss_ctc 114.659348 loss_rnnt 109.953796 hw_loss 0.398320 lr 0.00052110 rank 2
2023-02-22 22:35:40,303 DEBUG TRAIN Batch 11/300 loss 136.973297 loss_att 275.650818 loss_ctc 126.061256 loss_rnnt 110.448830 hw_loss 0.457297 lr 0.00052110 rank 5
2023-02-22 22:36:54,365 DEBUG TRAIN Batch 11/400 loss 95.925369 loss_att 235.165680 loss_ctc 74.856628 loss_rnnt 70.664879 hw_loss 0.415492 lr 0.00052082 rank 3
2023-02-22 22:36:54,366 DEBUG TRAIN Batch 11/400 loss 100.768677 loss_att 230.451355 loss_ctc 79.997719 loss_rnnt 77.329605 hw_loss 0.509996 lr 0.00052082 rank 7
2023-02-22 22:36:54,366 DEBUG TRAIN Batch 11/400 loss 107.209129 loss_att 251.046082 loss_ctc 86.571030 loss_rnnt 80.953293 hw_loss 0.450357 lr 0.00052082 rank 6
2023-02-22 22:36:54,368 DEBUG TRAIN Batch 11/400 loss 89.176819 loss_att 220.457657 loss_ctc 70.353668 loss_rnnt 65.163788 hw_loss 0.499923 lr 0.00052082 rank 5
2023-02-22 22:36:54,374 DEBUG TRAIN Batch 11/400 loss 142.311249 loss_att 270.371796 loss_ctc 137.062424 loss_rnnt 117.191711 hw_loss 0.388651 lr 0.00052082 rank 1
2023-02-22 22:36:54,373 DEBUG TRAIN Batch 11/400 loss 140.350235 loss_att 279.206146 loss_ctc 134.027527 loss_rnnt 113.232544 hw_loss 0.355394 lr 0.00052082 rank 0
2023-02-22 22:36:54,375 DEBUG TRAIN Batch 11/400 loss 85.087044 loss_att 217.729401 loss_ctc 71.324585 loss_rnnt 60.164268 hw_loss 0.429940 lr 0.00052082 rank 4
2023-02-22 22:36:54,378 DEBUG TRAIN Batch 11/400 loss 108.507187 loss_att 258.069946 loss_ctc 94.290756 loss_rnnt 80.288589 hw_loss 0.377947 lr 0.00052082 rank 2
2023-02-22 22:38:08,025 DEBUG TRAIN Batch 11/500 loss 66.204941 loss_att 173.658356 loss_ctc 58.249214 loss_rnnt 45.521023 hw_loss 0.476240 lr 0.00052054 rank 7
2023-02-22 22:38:08,028 DEBUG TRAIN Batch 11/500 loss 77.722313 loss_att 180.880310 loss_ctc 62.736431 loss_rnnt 58.826778 hw_loss 0.491353 lr 0.00052054 rank 1
2023-02-22 22:38:08,032 DEBUG TRAIN Batch 11/500 loss 98.193352 loss_att 233.372192 loss_ctc 87.199142 loss_rnnt 72.397873 hw_loss 0.423014 lr 0.00052054 rank 0
2023-02-22 22:38:08,033 DEBUG TRAIN Batch 11/500 loss 94.110283 loss_att 202.298126 loss_ctc 91.022743 loss_rnnt 72.661789 hw_loss 0.417366 lr 0.00052054 rank 3
2023-02-22 22:38:08,034 DEBUG TRAIN Batch 11/500 loss 62.543713 loss_att 157.097473 loss_ctc 51.499275 loss_rnnt 44.872021 hw_loss 0.437875 lr 0.00052054 rank 6
2023-02-22 22:38:08,037 DEBUG TRAIN Batch 11/500 loss 64.897476 loss_att 168.293716 loss_ctc 55.572433 loss_rnnt 45.249420 hw_loss 0.397771 lr 0.00052054 rank 5
2023-02-22 22:38:08,039 DEBUG TRAIN Batch 11/500 loss 112.253639 loss_att 223.139313 loss_ctc 103.586945 loss_rnnt 91.039856 hw_loss 0.360391 lr 0.00052054 rank 4
2023-02-22 22:38:08,042 DEBUG TRAIN Batch 11/500 loss 67.764999 loss_att 176.164459 loss_ctc 50.134686 loss_rnnt 48.197716 hw_loss 0.446435 lr 0.00052054 rank 2
2023-02-22 22:39:21,728 DEBUG TRAIN Batch 11/600 loss 60.786922 loss_att 106.993134 loss_ctc 60.369179 loss_rnnt 51.324272 hw_loss 0.519571 lr 0.00052026 rank 3
2023-02-22 22:39:21,737 DEBUG TRAIN Batch 11/600 loss 65.752312 loss_att 137.664902 loss_ctc 59.335930 loss_rnnt 51.925270 hw_loss 0.562579 lr 0.00052026 rank 4
2023-02-22 22:39:21,737 DEBUG TRAIN Batch 11/600 loss 52.791019 loss_att 131.065048 loss_ctc 45.166351 loss_rnnt 37.895306 hw_loss 0.482874 lr 0.00052026 rank 1
2023-02-22 22:39:21,739 DEBUG TRAIN Batch 11/600 loss 65.343361 loss_att 130.992371 loss_ctc 54.650307 loss_rnnt 53.389275 hw_loss 0.468813 lr 0.00052026 rank 7
2023-02-22 22:39:21,751 DEBUG TRAIN Batch 11/600 loss 50.794338 loss_att 98.933243 loss_ctc 51.506535 loss_rnnt 40.806526 hw_loss 0.497003 lr 0.00052026 rank 5
2023-02-22 22:39:21,755 DEBUG TRAIN Batch 11/600 loss 48.710720 loss_att 125.181885 loss_ctc 41.222237 loss_rnnt 34.165356 hw_loss 0.467997 lr 0.00052026 rank 2
2023-02-22 22:39:21,761 DEBUG TRAIN Batch 11/600 loss 53.692841 loss_att 130.690582 loss_ctc 44.329006 loss_rnnt 39.274601 hw_loss 0.501005 lr 0.00052026 rank 6
2023-02-22 22:39:21,783 DEBUG TRAIN Batch 11/600 loss 85.417175 loss_att 169.301865 loss_ctc 77.749794 loss_rnnt 69.442543 hw_loss 0.412516 lr 0.00052026 rank 0
2023-02-22 22:40:38,233 DEBUG TRAIN Batch 11/700 loss 42.304176 loss_att 106.916412 loss_ctc 33.776703 loss_rnnt 30.265537 hw_loss 0.474722 lr 0.00051997 rank 7
2023-02-22 22:40:38,240 DEBUG TRAIN Batch 11/700 loss 91.310936 loss_att 220.860291 loss_ctc 77.054596 loss_rnnt 67.113556 hw_loss 0.353156 lr 0.00051997 rank 3
2023-02-22 22:40:38,243 DEBUG TRAIN Batch 11/700 loss 63.329582 loss_att 174.789230 loss_ctc 58.502178 loss_rnnt 41.436996 hw_loss 0.458087 lr 0.00051997 rank 1
2023-02-22 22:40:38,244 DEBUG TRAIN Batch 11/700 loss 76.277672 loss_att 197.071289 loss_ctc 72.429520 loss_rnnt 52.471584 hw_loss 0.300850 lr 0.00051997 rank 6
2023-02-22 22:40:38,244 DEBUG TRAIN Batch 11/700 loss 53.931091 loss_att 143.810715 loss_ctc 51.642670 loss_rnnt 36.046921 hw_loss 0.400064 lr 0.00051997 rank 0
2023-02-22 22:40:38,251 DEBUG TRAIN Batch 11/700 loss 82.479134 loss_att 194.363525 loss_ctc 66.585838 loss_rnnt 61.988228 hw_loss 0.437103 lr 0.00051997 rank 5
2023-02-22 22:40:38,259 DEBUG TRAIN Batch 11/700 loss 77.266342 loss_att 171.820099 loss_ctc 75.724594 loss_rnnt 58.367493 hw_loss 0.363119 lr 0.00051997 rank 4
2023-02-22 22:40:38,292 DEBUG TRAIN Batch 11/700 loss 76.987198 loss_att 168.110596 loss_ctc 71.087372 loss_rnnt 59.301739 hw_loss 0.463914 lr 0.00051997 rank 2
2023-02-22 22:41:51,416 DEBUG TRAIN Batch 11/800 loss 49.099121 loss_att 107.038223 loss_ctc 48.345043 loss_rnnt 37.390289 hw_loss 0.415422 lr 0.00051969 rank 7
2023-02-22 22:41:51,422 DEBUG TRAIN Batch 11/800 loss 68.421082 loss_att 128.785461 loss_ctc 70.066895 loss_rnnt 55.911713 hw_loss 0.406970 lr 0.00051969 rank 0
2023-02-22 22:41:51,422 DEBUG TRAIN Batch 11/800 loss 62.798843 loss_att 131.779694 loss_ctc 65.741653 loss_rnnt 48.414360 hw_loss 0.367396 lr 0.00051969 rank 6
2023-02-22 22:41:51,423 DEBUG TRAIN Batch 11/800 loss 91.786346 loss_att 176.380371 loss_ctc 85.427170 loss_rnnt 75.527992 hw_loss 0.351456 lr 0.00051969 rank 3
2023-02-22 22:41:51,424 DEBUG TRAIN Batch 11/800 loss 82.831757 loss_att 164.329849 loss_ctc 67.065399 loss_rnnt 68.384705 hw_loss 0.468023 lr 0.00051969 rank 4
2023-02-22 22:41:51,426 DEBUG TRAIN Batch 11/800 loss 66.701653 loss_att 160.763275 loss_ctc 55.967842 loss_rnnt 49.119610 hw_loss 0.376690 lr 0.00051969 rank 5
2023-02-22 22:41:51,425 DEBUG TRAIN Batch 11/800 loss 80.710045 loss_att 157.888474 loss_ctc 93.393921 loss_rnnt 63.414391 hw_loss 0.316470 lr 0.00051969 rank 2
2023-02-22 22:41:51,427 DEBUG TRAIN Batch 11/800 loss 50.940434 loss_att 96.751945 loss_ctc 43.338898 loss_rnnt 42.589008 hw_loss 0.379993 lr 0.00051969 rank 1
2023-02-22 22:43:05,686 DEBUG TRAIN Batch 11/900 loss 46.261623 loss_att 107.902733 loss_ctc 46.594967 loss_rnnt 33.705257 hw_loss 0.344435 lr 0.00051941 rank 7
2023-02-22 22:43:05,688 DEBUG TRAIN Batch 11/900 loss 70.854736 loss_att 143.324997 loss_ctc 63.283764 loss_rnnt 57.184334 hw_loss 0.348403 lr 0.00051941 rank 3
2023-02-22 22:43:05,696 DEBUG TRAIN Batch 11/900 loss 47.152248 loss_att 112.884613 loss_ctc 45.516922 loss_rnnt 33.985909 hw_loss 0.446078 lr 0.00051941 rank 5
2023-02-22 22:43:05,697 DEBUG TRAIN Batch 11/900 loss 54.022320 loss_att 115.658539 loss_ctc 46.148773 loss_rnnt 42.513962 hw_loss 0.432979 lr 0.00051941 rank 6
2023-02-22 22:43:05,697 DEBUG TRAIN Batch 11/900 loss 56.335712 loss_att 125.094818 loss_ctc 47.232597 loss_rnnt 43.594223 hw_loss 0.381405 lr 0.00051941 rank 4
2023-02-22 22:43:05,697 DEBUG TRAIN Batch 11/900 loss 44.443577 loss_att 102.244560 loss_ctc 42.112186 loss_rnnt 32.997673 hw_loss 0.368544 lr 0.00051941 rank 1
2023-02-22 22:43:05,704 DEBUG TRAIN Batch 11/900 loss 61.578472 loss_att 123.896057 loss_ctc 49.477104 loss_rnnt 50.539013 hw_loss 0.355236 lr 0.00051941 rank 2
2023-02-22 22:43:05,706 DEBUG TRAIN Batch 11/900 loss 67.131187 loss_att 125.347542 loss_ctc 68.161224 loss_rnnt 55.172234 hw_loss 0.334400 lr 0.00051941 rank 0
2023-02-22 22:44:19,227 DEBUG TRAIN Batch 11/1000 loss 50.567020 loss_att 105.105133 loss_ctc 47.062805 loss_rnnt 39.934937 hw_loss 0.359425 lr 0.00051913 rank 1
2023-02-22 22:44:19,247 DEBUG TRAIN Batch 11/1000 loss 58.615967 loss_att 95.492210 loss_ctc 69.953430 loss_rnnt 49.473640 hw_loss 0.478897 lr 0.00051913 rank 7
2023-02-22 22:44:19,247 DEBUG TRAIN Batch 11/1000 loss 46.087036 loss_att 85.252769 loss_ctc 52.496536 loss_rnnt 37.213623 hw_loss 0.348129 lr 0.00051913 rank 4
2023-02-22 22:44:19,247 DEBUG TRAIN Batch 11/1000 loss 74.331329 loss_att 114.203918 loss_ctc 72.825584 loss_rnnt 66.373024 hw_loss 0.346027 lr 0.00051913 rank 3
2023-02-22 22:44:19,247 DEBUG TRAIN Batch 11/1000 loss 49.650127 loss_att 103.287796 loss_ctc 53.711578 loss_rnnt 38.189705 hw_loss 0.358804 lr 0.00051913 rank 0
2023-02-22 22:44:19,249 DEBUG TRAIN Batch 11/1000 loss 47.211956 loss_att 100.510101 loss_ctc 45.279469 loss_rnnt 36.576107 hw_loss 0.438537 lr 0.00051913 rank 6
2023-02-22 22:44:19,251 DEBUG TRAIN Batch 11/1000 loss 65.658302 loss_att 102.529335 loss_ctc 71.301788 loss_rnnt 57.309139 hw_loss 0.417178 lr 0.00051913 rank 2
2023-02-22 22:44:19,260 DEBUG TRAIN Batch 11/1000 loss 55.663754 loss_att 107.142921 loss_ctc 61.555847 loss_rnnt 44.382034 hw_loss 0.375514 lr 0.00051913 rank 5
2023-02-22 22:45:34,641 DEBUG TRAIN Batch 11/1100 loss 57.349739 loss_att 87.282776 loss_ctc 62.629368 loss_rnnt 50.466492 hw_loss 0.361291 lr 0.00051885 rank 7
2023-02-22 22:45:34,641 DEBUG TRAIN Batch 11/1100 loss 45.503700 loss_att 78.481682 loss_ctc 50.192535 loss_rnnt 38.047073 hw_loss 0.442220 lr 0.00051885 rank 3
2023-02-22 22:45:34,646 DEBUG TRAIN Batch 11/1100 loss 62.792126 loss_att 101.087936 loss_ctc 65.097992 loss_rnnt 54.628723 hw_loss 0.368988 lr 0.00051885 rank 0
2023-02-22 22:45:34,646 DEBUG TRAIN Batch 11/1100 loss 53.307819 loss_att 94.482468 loss_ctc 46.729294 loss_rnnt 45.749390 hw_loss 0.376197 lr 0.00051885 rank 4
2023-02-22 22:45:34,650 DEBUG TRAIN Batch 11/1100 loss 50.108738 loss_att 73.985237 loss_ctc 50.417812 loss_rnnt 45.062881 hw_loss 0.430025 lr 0.00051885 rank 5
2023-02-22 22:45:34,650 DEBUG TRAIN Batch 11/1100 loss 37.674488 loss_att 66.068436 loss_ctc 42.665531 loss_rnnt 31.102211 hw_loss 0.427527 lr 0.00051885 rank 1
2023-02-22 22:45:34,655 DEBUG TRAIN Batch 11/1100 loss 75.660271 loss_att 129.318954 loss_ctc 86.791611 loss_rnnt 63.237892 hw_loss 0.387121 lr 0.00051885 rank 2
2023-02-22 22:45:34,689 DEBUG TRAIN Batch 11/1100 loss 55.399620 loss_att 85.911545 loss_ctc 54.414726 loss_rnnt 49.217560 hw_loss 0.395624 lr 0.00051885 rank 6
2023-02-22 22:46:48,230 DEBUG TRAIN Batch 11/1200 loss 46.704674 loss_att 71.818626 loss_ctc 52.022388 loss_rnnt 40.752853 hw_loss 0.412507 lr 0.00051857 rank 3
2023-02-22 22:46:48,235 DEBUG TRAIN Batch 11/1200 loss 33.061935 loss_att 70.774971 loss_ctc 27.344099 loss_rnnt 26.051529 hw_loss 0.431585 lr 0.00051857 rank 7
2023-02-22 22:46:48,236 DEBUG TRAIN Batch 11/1200 loss 53.071484 loss_att 73.851494 loss_ctc 52.488354 loss_rnnt 48.720177 hw_loss 0.511983 lr 0.00051857 rank 0
2023-02-22 22:46:48,241 DEBUG TRAIN Batch 11/1200 loss 45.891563 loss_att 74.923256 loss_ctc 46.441853 loss_rnnt 39.781200 hw_loss 0.432474 lr 0.00051857 rank 5
2023-02-22 22:46:48,242 DEBUG TRAIN Batch 11/1200 loss 44.185215 loss_att 76.708015 loss_ctc 45.044918 loss_rnnt 37.350491 hw_loss 0.404137 lr 0.00051857 rank 1
2023-02-22 22:46:48,245 DEBUG TRAIN Batch 11/1200 loss 38.388779 loss_att 69.457474 loss_ctc 33.507507 loss_rnnt 32.575039 hw_loss 0.470320 lr 0.00051857 rank 4
2023-02-22 22:46:48,251 DEBUG TRAIN Batch 11/1200 loss 51.834263 loss_att 80.552666 loss_ctc 53.629005 loss_rnnt 45.639896 hw_loss 0.396351 lr 0.00051857 rank 2
2023-02-22 22:46:48,280 DEBUG TRAIN Batch 11/1200 loss 37.723385 loss_att 72.042465 loss_ctc 34.613884 loss_rnnt 31.062843 hw_loss 0.396241 lr 0.00051857 rank 6
2023-02-22 22:48:02,780 DEBUG TRAIN Batch 11/1300 loss 74.249329 loss_att 112.571320 loss_ctc 81.927109 loss_rnnt 65.355576 hw_loss 0.385586 lr 0.00051829 rank 3
2023-02-22 22:48:02,784 DEBUG TRAIN Batch 11/1300 loss 37.015896 loss_att 48.283554 loss_ctc 36.015289 loss_rnnt 34.655190 hw_loss 0.451107 lr 0.00051829 rank 1
2023-02-22 22:48:02,784 DEBUG TRAIN Batch 11/1300 loss 38.857533 loss_att 76.920578 loss_ctc 40.766090 loss_rnnt 30.808508 hw_loss 0.341143 lr 0.00051829 rank 7
2023-02-22 22:48:02,785 DEBUG TRAIN Batch 11/1300 loss 29.864368 loss_att 64.014450 loss_ctc 27.240574 loss_rnnt 23.212357 hw_loss 0.322188 lr 0.00051829 rank 4
2023-02-22 22:48:02,787 DEBUG TRAIN Batch 11/1300 loss 43.733643 loss_att 68.298843 loss_ctc 45.049744 loss_rnnt 38.430676 hw_loss 0.402084 lr 0.00051829 rank 5
2023-02-22 22:48:02,788 DEBUG TRAIN Batch 11/1300 loss 85.753372 loss_att 115.891129 loss_ctc 90.390892 loss_rnnt 78.912041 hw_loss 0.366480 lr 0.00051829 rank 0
2023-02-22 22:48:02,789 DEBUG TRAIN Batch 11/1300 loss 42.278584 loss_att 60.349915 loss_ctc 45.937534 loss_rnnt 37.916546 hw_loss 0.487333 lr 0.00051829 rank 6
2023-02-22 22:48:02,792 DEBUG TRAIN Batch 11/1300 loss 20.690853 loss_att 22.570976 loss_ctc 23.716663 loss_rnnt 19.580645 hw_loss 0.620142 lr 0.00051829 rank 2
2023-02-22 22:49:18,428 DEBUG TRAIN Batch 11/1400 loss 34.451862 loss_att 81.205635 loss_ctc 31.147055 loss_rnnt 25.363796 hw_loss 0.333664 lr 0.00051802 rank 2
2023-02-22 22:49:18,431 DEBUG TRAIN Batch 11/1400 loss 28.307388 loss_att 63.223652 loss_ctc 33.027802 loss_rnnt 20.539286 hw_loss 0.291491 lr 0.00051802 rank 7
2023-02-22 22:49:18,435 DEBUG TRAIN Batch 11/1400 loss 44.628849 loss_att 93.671555 loss_ctc 43.203991 loss_rnnt 34.821514 hw_loss 0.353956 lr 0.00051802 rank 3
2023-02-22 22:49:18,437 DEBUG TRAIN Batch 11/1400 loss 28.955265 loss_att 44.991749 loss_ctc 29.199415 loss_rnnt 25.492041 hw_loss 0.418832 lr 0.00051802 rank 0
2023-02-22 22:49:18,438 DEBUG TRAIN Batch 11/1400 loss 40.202351 loss_att 85.873863 loss_ctc 34.005993 loss_rnnt 31.708759 hw_loss 0.347754 lr 0.00051802 rank 5
2023-02-22 22:49:18,458 DEBUG TRAIN Batch 11/1400 loss 19.732241 loss_att 43.475792 loss_ctc 16.317390 loss_rnnt 15.232806 hw_loss 0.386319 lr 0.00051802 rank 4
2023-02-22 22:49:18,470 DEBUG TRAIN Batch 11/1400 loss 27.010096 loss_att 61.060486 loss_ctc 25.345089 loss_rnnt 20.202250 hw_loss 0.412064 lr 0.00051802 rank 6
2023-02-22 22:49:18,488 DEBUG TRAIN Batch 11/1400 loss 57.175915 loss_att 91.400604 loss_ctc 63.554390 loss_rnnt 49.294765 hw_loss 0.348277 lr 0.00051802 rank 1
2023-02-22 22:50:32,457 DEBUG TRAIN Batch 11/1500 loss 22.848492 loss_att 41.273037 loss_ctc 25.709602 loss_rnnt 18.560513 hw_loss 0.415478 lr 0.00051774 rank 1
2023-02-22 22:50:32,461 DEBUG TRAIN Batch 11/1500 loss 42.365349 loss_att 81.166550 loss_ctc 42.323830 loss_rnnt 34.421185 hw_loss 0.355230 lr 0.00051774 rank 0
2023-02-22 22:50:32,462 DEBUG TRAIN Batch 11/1500 loss 39.870022 loss_att 67.363823 loss_ctc 47.102180 loss_rnnt 33.205322 hw_loss 0.378095 lr 0.00051774 rank 4
2023-02-22 22:50:32,463 DEBUG TRAIN Batch 11/1500 loss 45.877338 loss_att 61.857594 loss_ctc 43.387741 loss_rnnt 42.806992 hw_loss 0.386697 lr 0.00051774 rank 2
2023-02-22 22:50:32,465 DEBUG TRAIN Batch 11/1500 loss 33.348751 loss_att 52.348770 loss_ctc 35.675667 loss_rnnt 29.054472 hw_loss 0.345043 lr 0.00051774 rank 7
2023-02-22 22:50:32,465 DEBUG TRAIN Batch 11/1500 loss 59.824543 loss_att 83.229393 loss_ctc 64.012894 loss_rnnt 54.359467 hw_loss 0.423106 lr 0.00051774 rank 3
2023-02-22 22:50:32,465 DEBUG TRAIN Batch 11/1500 loss 21.510990 loss_att 33.086124 loss_ctc 21.817814 loss_rnnt 18.905418 hw_loss 0.468066 lr 0.00051774 rank 5
2023-02-22 22:50:32,511 DEBUG TRAIN Batch 11/1500 loss 31.694708 loss_att 58.476730 loss_ctc 31.317989 loss_rnnt 26.091568 hw_loss 0.556806 lr 0.00051774 rank 6
2023-02-22 22:51:44,928 DEBUG TRAIN Batch 11/1600 loss 34.574425 loss_att 60.884743 loss_ctc 34.307983 loss_rnnt 29.154964 hw_loss 0.361733 lr 0.00051746 rank 7
2023-02-22 22:51:44,940 DEBUG TRAIN Batch 11/1600 loss 40.338436 loss_att 62.681358 loss_ctc 43.774506 loss_rnnt 35.180485 hw_loss 0.433543 lr 0.00051746 rank 1
2023-02-22 22:51:44,940 DEBUG TRAIN Batch 11/1600 loss 27.930510 loss_att 41.104664 loss_ctc 29.199272 loss_rnnt 24.929237 hw_loss 0.369889 lr 0.00051746 rank 3
2023-02-22 22:51:44,942 DEBUG TRAIN Batch 11/1600 loss 33.344769 loss_att 61.952545 loss_ctc 33.351620 loss_rnnt 27.419096 hw_loss 0.381011 lr 0.00051746 rank 0
2023-02-22 22:51:44,943 DEBUG TRAIN Batch 11/1600 loss 49.450264 loss_att 64.335167 loss_ctc 51.378586 loss_rnnt 46.064354 hw_loss 0.284665 lr 0.00051746 rank 6
2023-02-22 22:51:44,943 DEBUG TRAIN Batch 11/1600 loss 29.920214 loss_att 51.584110 loss_ctc 28.687685 loss_rnnt 25.518925 hw_loss 0.436581 lr 0.00051746 rank 4
2023-02-22 22:51:44,950 DEBUG TRAIN Batch 11/1600 loss 44.943565 loss_att 78.308922 loss_ctc 44.188538 loss_rnnt 38.190475 hw_loss 0.338797 lr 0.00051746 rank 2
2023-02-22 22:51:44,952 DEBUG TRAIN Batch 11/1600 loss 29.957808 loss_att 51.323734 loss_ctc 27.406176 loss_rnnt 25.813126 hw_loss 0.396962 lr 0.00051746 rank 5
2023-02-22 22:52:58,800 DEBUG TRAIN Batch 11/1700 loss 51.632107 loss_att 73.386108 loss_ctc 56.255463 loss_rnnt 46.478333 hw_loss 0.349740 lr 0.00051718 rank 1
2023-02-22 22:52:58,809 DEBUG TRAIN Batch 11/1700 loss 29.427605 loss_att 43.919640 loss_ctc 34.051674 loss_rnnt 25.697197 hw_loss 0.403987 lr 0.00051718 rank 7
2023-02-22 22:52:58,810 DEBUG TRAIN Batch 11/1700 loss 35.530209 loss_att 51.043945 loss_ctc 36.293400 loss_rnnt 32.093605 hw_loss 0.435185 lr 0.00051718 rank 3
2023-02-22 22:52:58,812 DEBUG TRAIN Batch 11/1700 loss 27.808538 loss_att 49.663311 loss_ctc 31.107058 loss_rnnt 22.786774 hw_loss 0.395635 lr 0.00051718 rank 6
2023-02-22 22:52:58,811 DEBUG TRAIN Batch 11/1700 loss 51.620445 loss_att 63.040459 loss_ctc 53.695354 loss_rnnt 48.871368 hw_loss 0.353275 lr 0.00051718 rank 4
2023-02-22 22:52:58,817 DEBUG TRAIN Batch 11/1700 loss 32.230309 loss_att 56.181042 loss_ctc 31.303032 loss_rnnt 27.317276 hw_loss 0.462223 lr 0.00051718 rank 0
2023-02-22 22:52:58,826 DEBUG TRAIN Batch 11/1700 loss 33.940487 loss_att 47.816299 loss_ctc 37.765182 loss_rnnt 30.446606 hw_loss 0.391430 lr 0.00051718 rank 5
2023-02-22 22:52:58,828 DEBUG TRAIN Batch 11/1700 loss 26.558355 loss_att 47.594952 loss_ctc 26.748077 loss_rnnt 22.137457 hw_loss 0.353025 lr 0.00051718 rank 2
2023-02-22 22:54:14,841 DEBUG TRAIN Batch 11/1800 loss 36.098309 loss_att 52.652531 loss_ctc 44.835773 loss_rnnt 31.352964 hw_loss 0.505320 lr 0.00051691 rank 7
2023-02-22 22:54:14,845 DEBUG TRAIN Batch 11/1800 loss 34.821430 loss_att 52.205288 loss_ctc 31.681168 loss_rnnt 31.541798 hw_loss 0.415427 lr 0.00051691 rank 3
2023-02-22 22:54:14,848 DEBUG TRAIN Batch 11/1800 loss 39.223873 loss_att 47.843132 loss_ctc 41.649715 loss_rnnt 36.945782 hw_loss 0.432741 lr 0.00051691 rank 5
2023-02-22 22:54:14,848 DEBUG TRAIN Batch 11/1800 loss 40.622311 loss_att 58.391144 loss_ctc 45.839096 loss_rnnt 36.126957 hw_loss 0.461271 lr 0.00051691 rank 6
2023-02-22 22:54:14,853 DEBUG TRAIN Batch 11/1800 loss 36.094006 loss_att 48.858086 loss_ctc 42.031013 loss_rnnt 32.502651 hw_loss 0.463010 lr 0.00051691 rank 4
2023-02-22 22:54:14,856 DEBUG TRAIN Batch 11/1800 loss 32.596619 loss_att 44.298145 loss_ctc 35.588486 loss_rnnt 29.626425 hw_loss 0.433071 lr 0.00051691 rank 1
2023-02-22 22:54:14,857 DEBUG TRAIN Batch 11/1800 loss 36.227970 loss_att 58.423470 loss_ctc 42.831715 loss_rnnt 30.669888 hw_loss 0.447153 lr 0.00051691 rank 2
2023-02-22 22:54:14,899 DEBUG TRAIN Batch 11/1800 loss 38.542442 loss_att 59.942451 loss_ctc 39.711964 loss_rnnt 33.872253 hw_loss 0.439211 lr 0.00051691 rank 0
2023-02-22 22:55:28,342 DEBUG TRAIN Batch 11/1900 loss 25.176138 loss_att 34.393639 loss_ctc 26.585262 loss_rnnt 22.897198 hw_loss 0.464170 lr 0.00051663 rank 6
2023-02-22 22:55:28,359 DEBUG TRAIN Batch 11/1900 loss 24.844614 loss_att 26.099525 loss_ctc 29.518856 loss_rnnt 23.692167 hw_loss 0.521688 lr 0.00051663 rank 7
2023-02-22 22:55:28,362 DEBUG TRAIN Batch 11/1900 loss 17.105165 loss_att 18.589676 loss_ctc 20.156223 loss_rnnt 16.126240 hw_loss 0.516029 lr 0.00051663 rank 0
2023-02-22 22:55:28,362 DEBUG TRAIN Batch 11/1900 loss 23.266283 loss_att 38.512848 loss_ctc 23.447075 loss_rnnt 19.964470 hw_loss 0.428240 lr 0.00051663 rank 4
2023-02-22 22:55:28,364 DEBUG TRAIN Batch 11/1900 loss 30.805908 loss_att 40.935516 loss_ctc 33.140167 loss_rnnt 28.246176 hw_loss 0.417333 lr 0.00051663 rank 3
2023-02-22 22:55:28,364 DEBUG TRAIN Batch 11/1900 loss 24.304382 loss_att 31.231264 loss_ctc 24.420841 loss_rnnt 22.595352 hw_loss 0.577735 lr 0.00051663 rank 5
2023-02-22 22:55:28,364 DEBUG TRAIN Batch 11/1900 loss 17.751657 loss_att 29.967567 loss_ctc 18.059565 loss_rnnt 15.039576 hw_loss 0.427208 lr 0.00051663 rank 1
2023-02-22 22:55:28,371 DEBUG TRAIN Batch 11/1900 loss 17.342972 loss_att 26.436596 loss_ctc 21.756504 loss_rnnt 14.682658 hw_loss 0.474595 lr 0.00051663 rank 2
2023-02-22 22:56:41,263 DEBUG TRAIN Batch 11/2000 loss 42.167866 loss_att 64.109436 loss_ctc 45.676388 loss_rnnt 37.077179 hw_loss 0.439814 lr 0.00051636 rank 7
2023-02-22 22:56:41,266 DEBUG TRAIN Batch 11/2000 loss 37.569599 loss_att 49.355824 loss_ctc 40.386429 loss_rnnt 34.675648 hw_loss 0.302111 lr 0.00051636 rank 0
2023-02-22 22:56:41,268 DEBUG TRAIN Batch 11/2000 loss 31.133553 loss_att 53.046967 loss_ctc 31.868727 loss_rnnt 26.480198 hw_loss 0.323716 lr 0.00051636 rank 1
2023-02-22 22:56:41,271 DEBUG TRAIN Batch 11/2000 loss 28.755760 loss_att 40.965855 loss_ctc 25.784443 loss_rnnt 26.438496 hw_loss 0.508912 lr 0.00051636 rank 3
2023-02-22 22:56:41,272 DEBUG TRAIN Batch 11/2000 loss 40.531322 loss_att 75.319786 loss_ctc 43.998047 loss_rnnt 32.913971 hw_loss 0.370177 lr 0.00051636 rank 2
2023-02-22 22:56:41,273 DEBUG TRAIN Batch 11/2000 loss 37.981232 loss_att 56.969826 loss_ctc 40.357384 loss_rnnt 33.696602 hw_loss 0.318915 lr 0.00051636 rank 5
2023-02-22 22:56:41,274 DEBUG TRAIN Batch 11/2000 loss 23.769236 loss_att 37.276066 loss_ctc 25.340416 loss_rnnt 20.613541 hw_loss 0.459068 lr 0.00051636 rank 4
2023-02-22 22:56:41,305 DEBUG TRAIN Batch 11/2000 loss 51.364346 loss_att 70.805832 loss_ctc 61.084183 loss_rnnt 45.996944 hw_loss 0.343358 lr 0.00051636 rank 6
2023-02-22 22:57:57,132 DEBUG TRAIN Batch 11/2100 loss 14.262259 loss_att 33.068398 loss_ctc 13.677089 loss_rnnt 10.322307 hw_loss 0.481404 lr 0.00051608 rank 5
2023-02-22 22:57:57,147 DEBUG TRAIN Batch 11/2100 loss 27.200216 loss_att 43.290394 loss_ctc 26.954824 loss_rnnt 23.843153 hw_loss 0.322025 lr 0.00051608 rank 7
2023-02-22 22:57:57,149 DEBUG TRAIN Batch 11/2100 loss 36.302208 loss_att 53.312889 loss_ctc 34.601986 loss_rnnt 32.890244 hw_loss 0.443480 lr 0.00051608 rank 4
2023-02-22 22:57:57,150 DEBUG TRAIN Batch 11/2100 loss 28.416119 loss_att 44.529690 loss_ctc 26.775539 loss_rnnt 25.210958 hw_loss 0.377233 lr 0.00051608 rank 1
2023-02-22 22:57:57,152 DEBUG TRAIN Batch 11/2100 loss 26.922422 loss_att 43.433826 loss_ctc 28.916876 loss_rnnt 23.151604 hw_loss 0.379897 lr 0.00051608 rank 0
2023-02-22 22:57:57,153 DEBUG TRAIN Batch 11/2100 loss 32.453247 loss_att 45.539440 loss_ctc 31.653080 loss_rnnt 29.745771 hw_loss 0.369236 lr 0.00051608 rank 3
2023-02-22 22:57:57,154 DEBUG TRAIN Batch 11/2100 loss 31.541332 loss_att 53.896942 loss_ctc 28.897884 loss_rnnt 27.222656 hw_loss 0.375026 lr 0.00051608 rank 6
2023-02-22 22:57:57,201 DEBUG TRAIN Batch 11/2100 loss 37.099442 loss_att 51.573490 loss_ctc 41.541332 loss_rnnt 33.406982 hw_loss 0.385122 lr 0.00051608 rank 2
2023-02-22 22:59:11,198 DEBUG TRAIN Batch 11/2200 loss 45.037827 loss_att 48.161850 loss_ctc 46.123108 loss_rnnt 44.086700 hw_loss 0.340524 lr 0.00051581 rank 3
2023-02-22 22:59:11,210 DEBUG TRAIN Batch 11/2200 loss 33.600067 loss_att 56.768543 loss_ctc 36.079319 loss_rnnt 28.425442 hw_loss 0.394440 lr 0.00051581 rank 7
2023-02-22 22:59:11,210 DEBUG TRAIN Batch 11/2200 loss 28.195593 loss_att 40.949455 loss_ctc 25.231075 loss_rnnt 25.838135 hw_loss 0.378667 lr 0.00051581 rank 6
2023-02-22 22:59:11,211 DEBUG TRAIN Batch 11/2200 loss 30.141459 loss_att 49.108009 loss_ctc 35.264320 loss_rnnt 25.461803 hw_loss 0.381186 lr 0.00051581 rank 5
2023-02-22 22:59:11,212 DEBUG TRAIN Batch 11/2200 loss 51.507076 loss_att 63.363697 loss_ctc 56.607872 loss_rnnt 48.213501 hw_loss 0.454027 lr 0.00051581 rank 0
2023-02-22 22:59:11,213 DEBUG TRAIN Batch 11/2200 loss 43.387291 loss_att 64.766624 loss_ctc 47.109184 loss_rnnt 38.385033 hw_loss 0.431503 lr 0.00051581 rank 4
2023-02-22 22:59:11,214 DEBUG TRAIN Batch 11/2200 loss 38.666466 loss_att 42.379089 loss_ctc 40.744576 loss_rnnt 37.377468 hw_loss 0.505107 lr 0.00051581 rank 1
2023-02-22 22:59:11,214 DEBUG TRAIN Batch 11/2200 loss 37.411285 loss_att 42.877647 loss_ctc 38.681988 loss_rnnt 35.957027 hw_loss 0.359174 lr 0.00051581 rank 2
2023-02-22 23:00:24,438 DEBUG TRAIN Batch 11/2300 loss 20.169882 loss_att 30.295530 loss_ctc 19.965809 loss_rnnt 17.907295 hw_loss 0.496250 lr 0.00051553 rank 7
2023-02-22 23:00:24,443 DEBUG TRAIN Batch 11/2300 loss 29.667336 loss_att 38.218094 loss_ctc 32.698914 loss_rnnt 27.289715 hw_loss 0.493610 lr 0.00051553 rank 1
2023-02-22 23:00:24,443 DEBUG TRAIN Batch 11/2300 loss 28.465046 loss_att 36.707264 loss_ctc 27.916729 loss_rnnt 26.637890 hw_loss 0.472162 lr 0.00051553 rank 4
2023-02-22 23:00:24,445 DEBUG TRAIN Batch 11/2300 loss 22.716040 loss_att 31.113708 loss_ctc 25.529011 loss_rnnt 20.413794 hw_loss 0.464344 lr 0.00051553 rank 0
2023-02-22 23:00:24,445 DEBUG TRAIN Batch 11/2300 loss 39.354866 loss_att 47.584404 loss_ctc 44.365814 loss_rnnt 36.840954 hw_loss 0.374775 lr 0.00051553 rank 3
2023-02-22 23:00:24,450 DEBUG TRAIN Batch 11/2300 loss 32.927559 loss_att 45.952953 loss_ctc 37.854622 loss_rnnt 29.459969 hw_loss 0.385442 lr 0.00051553 rank 6
2023-02-22 23:00:24,450 DEBUG TRAIN Batch 11/2300 loss 25.883451 loss_att 37.018795 loss_ctc 33.529602 loss_rnnt 22.439228 hw_loss 0.370622 lr 0.00051553 rank 2
2023-02-22 23:00:24,450 DEBUG TRAIN Batch 11/2300 loss 30.967445 loss_att 47.177242 loss_ctc 32.094204 loss_rnnt 27.412724 hw_loss 0.304737 lr 0.00051553 rank 5
2023-02-22 23:01:38,280 DEBUG TRAIN Batch 11/2400 loss 24.706812 loss_att 38.140656 loss_ctc 29.346189 loss_rnnt 21.141735 hw_loss 0.486984 lr 0.00051526 rank 2
2023-02-22 23:01:38,285 DEBUG TRAIN Batch 11/2400 loss 34.698986 loss_att 43.029644 loss_ctc 37.826305 loss_rnnt 32.379845 hw_loss 0.442559 lr 0.00051526 rank 5
2023-02-22 23:01:38,291 DEBUG TRAIN Batch 11/2400 loss 38.090645 loss_att 46.205566 loss_ctc 47.704781 loss_rnnt 34.967625 hw_loss 0.409032 lr 0.00051526 rank 4
2023-02-22 23:01:38,295 DEBUG TRAIN Batch 11/2400 loss 15.823476 loss_att 31.665810 loss_ctc 15.845057 loss_rnnt 12.429626 hw_loss 0.417196 lr 0.00051526 rank 1
2023-02-22 23:01:38,313 DEBUG TRAIN Batch 11/2400 loss 28.184490 loss_att 37.826294 loss_ctc 29.480324 loss_rnnt 25.836811 hw_loss 0.462260 lr 0.00051526 rank 6
2023-02-22 23:01:38,321 DEBUG TRAIN Batch 11/2400 loss 23.076500 loss_att 28.736698 loss_ctc 27.942106 loss_rnnt 21.028706 hw_loss 0.500636 lr 0.00051526 rank 7
2023-02-22 23:01:38,324 DEBUG TRAIN Batch 11/2400 loss 40.570091 loss_att 58.511024 loss_ctc 41.597717 loss_rnnt 36.618317 hw_loss 0.424827 lr 0.00051526 rank 3
2023-02-22 23:01:38,341 DEBUG TRAIN Batch 11/2400 loss 20.781691 loss_att 27.832653 loss_ctc 25.876438 loss_rnnt 18.438879 hw_loss 0.474973 lr 0.00051526 rank 0
2023-02-22 23:02:54,465 DEBUG TRAIN Batch 11/2500 loss 22.774210 loss_att 30.649418 loss_ctc 25.325008 loss_rnnt 20.622597 hw_loss 0.443367 lr 0.00051499 rank 7
2023-02-22 23:02:54,468 DEBUG TRAIN Batch 11/2500 loss 43.820244 loss_att 49.139828 loss_ctc 55.130596 loss_rnnt 41.020523 hw_loss 0.427046 lr 0.00051499 rank 3
2023-02-22 23:02:54,477 DEBUG TRAIN Batch 11/2500 loss 44.909153 loss_att 55.505955 loss_ctc 54.297230 loss_rnnt 41.308254 hw_loss 0.430874 lr 0.00051499 rank 4
2023-02-22 23:02:54,478 DEBUG TRAIN Batch 11/2500 loss 25.963314 loss_att 36.379723 loss_ctc 25.606403 loss_rnnt 23.688532 hw_loss 0.448289 lr 0.00051499 rank 6
2023-02-22 23:02:54,478 DEBUG TRAIN Batch 11/2500 loss 14.274205 loss_att 18.901505 loss_ctc 17.605309 loss_rnnt 12.659381 hw_loss 0.459783 lr 0.00051499 rank 5
2023-02-22 23:02:54,482 DEBUG TRAIN Batch 11/2500 loss 21.334122 loss_att 30.571552 loss_ctc 20.444212 loss_rnnt 19.343782 hw_loss 0.490327 lr 0.00051499 rank 1
2023-02-22 23:02:54,487 DEBUG TRAIN Batch 11/2500 loss 15.094072 loss_att 19.664093 loss_ctc 16.431273 loss_rnnt 13.739126 hw_loss 0.492467 lr 0.00051499 rank 2
2023-02-22 23:02:54,537 DEBUG TRAIN Batch 11/2500 loss 30.277170 loss_att 33.681400 loss_ctc 37.186676 loss_rnnt 28.430857 hw_loss 0.457874 lr 0.00051499 rank 0
2023-02-22 23:04:07,957 DEBUG TRAIN Batch 11/2600 loss 15.807989 loss_att 29.374374 loss_ctc 19.477186 loss_rnnt 12.403553 hw_loss 0.378622 lr 0.00051471 rank 6
2023-02-22 23:04:07,959 DEBUG TRAIN Batch 11/2600 loss 22.740793 loss_att 40.603329 loss_ctc 22.283054 loss_rnnt 19.024904 hw_loss 0.383272 lr 0.00051471 rank 4
2023-02-22 23:04:07,963 DEBUG TRAIN Batch 11/2600 loss 22.546381 loss_att 28.480371 loss_ctc 23.270124 loss_rnnt 20.985527 hw_loss 0.520417 lr 0.00051471 rank 2
2023-02-22 23:04:07,962 DEBUG TRAIN Batch 11/2600 loss 28.541065 loss_att 49.659981 loss_ctc 30.632351 loss_rnnt 23.833675 hw_loss 0.383943 lr 0.00051471 rank 5
2023-02-22 23:04:07,963 DEBUG TRAIN Batch 11/2600 loss 13.223893 loss_att 15.995755 loss_ctc 14.344657 loss_rnnt 12.170410 hw_loss 0.655642 lr 0.00051471 rank 3
2023-02-22 23:04:07,963 DEBUG TRAIN Batch 11/2600 loss 12.668619 loss_att 22.606113 loss_ctc 10.732219 loss_rnnt 10.709692 hw_loss 0.430529 lr 0.00051471 rank 7
2023-02-22 23:04:07,965 DEBUG TRAIN Batch 11/2600 loss 26.139547 loss_att 41.508987 loss_ctc 30.780910 loss_rnnt 22.223394 hw_loss 0.418908 lr 0.00051471 rank 0
2023-02-22 23:04:07,969 DEBUG TRAIN Batch 11/2600 loss 29.030209 loss_att 32.748291 loss_ctc 29.801933 loss_rnnt 27.957081 hw_loss 0.424902 lr 0.00051471 rank 1
2023-02-22 23:05:21,543 DEBUG TRAIN Batch 11/2700 loss 19.018219 loss_att 25.807823 loss_ctc 17.685833 loss_rnnt 17.663391 hw_loss 0.327297 lr 0.00051444 rank 1
2023-02-22 23:05:21,543 DEBUG TRAIN Batch 11/2700 loss 24.664555 loss_att 44.707508 loss_ctc 28.337194 loss_rnnt 19.994911 hw_loss 0.321312 lr 0.00051444 rank 0
2023-02-22 23:05:21,546 DEBUG TRAIN Batch 11/2700 loss 45.287983 loss_att 59.913410 loss_ctc 57.182934 loss_rnnt 40.526192 hw_loss 0.470089 lr 0.00051444 rank 3
2023-02-22 23:05:21,546 DEBUG TRAIN Batch 11/2700 loss 14.678950 loss_att 25.858974 loss_ctc 21.745510 loss_rnnt 11.328596 hw_loss 0.322767 lr 0.00051444 rank 7
2023-02-22 23:05:21,549 DEBUG TRAIN Batch 11/2700 loss 18.996544 loss_att 29.685354 loss_ctc 18.145802 loss_rnnt 16.702881 hw_loss 0.504999 lr 0.00051444 rank 4
2023-02-22 23:05:21,549 DEBUG TRAIN Batch 11/2700 loss 72.974129 loss_att 83.883347 loss_ctc 80.191788 loss_rnnt 69.575623 hw_loss 0.476815 lr 0.00051444 rank 5
2023-02-22 23:05:21,551 DEBUG TRAIN Batch 11/2700 loss 38.159565 loss_att 46.463940 loss_ctc 39.305439 loss_rnnt 36.122627 hw_loss 0.418651 lr 0.00051444 rank 2
2023-02-22 23:05:21,597 DEBUG TRAIN Batch 11/2700 loss 12.900925 loss_att 25.232285 loss_ctc 10.743023 loss_rnnt 10.523220 hw_loss 0.373411 lr 0.00051444 rank 6
2023-02-22 23:06:36,804 DEBUG TRAIN Batch 11/2800 loss 22.637131 loss_att 35.364998 loss_ctc 24.622341 loss_rnnt 19.651272 hw_loss 0.329230 lr 0.00051417 rank 0
2023-02-22 23:06:36,808 DEBUG TRAIN Batch 11/2800 loss 24.428974 loss_att 40.096336 loss_ctc 24.790718 loss_rnnt 21.045498 hw_loss 0.378322 lr 0.00051417 rank 4
2023-02-22 23:06:36,818 DEBUG TRAIN Batch 11/2800 loss 11.254656 loss_att 18.217644 loss_ctc 12.779043 loss_rnnt 9.446533 hw_loss 0.398010 lr 0.00051417 rank 2
2023-02-22 23:06:36,824 DEBUG TRAIN Batch 11/2800 loss 25.192137 loss_att 36.005856 loss_ctc 29.372234 loss_rnnt 22.268911 hw_loss 0.380880 lr 0.00051417 rank 7
2023-02-22 23:06:36,824 DEBUG TRAIN Batch 11/2800 loss 34.381351 loss_att 57.886009 loss_ctc 39.962486 loss_rnnt 28.743294 hw_loss 0.361834 lr 0.00051417 rank 1
2023-02-22 23:06:36,826 DEBUG TRAIN Batch 11/2800 loss 22.593664 loss_att 29.432774 loss_ctc 25.134583 loss_rnnt 20.707417 hw_loss 0.336813 lr 0.00051417 rank 5
2023-02-22 23:06:36,827 DEBUG TRAIN Batch 11/2800 loss 21.473267 loss_att 25.115871 loss_ctc 25.563437 loss_rnnt 19.911480 hw_loss 0.539827 lr 0.00051417 rank 6
2023-02-22 23:06:36,828 DEBUG TRAIN Batch 11/2800 loss 31.481651 loss_att 50.527840 loss_ctc 38.961319 loss_rnnt 26.466307 hw_loss 0.391534 lr 0.00051417 rank 3
2023-02-22 23:07:50,152 DEBUG TRAIN Batch 11/2900 loss 23.906092 loss_att 36.035278 loss_ctc 23.912319 loss_rnnt 21.223867 hw_loss 0.479171 lr 0.00051390 rank 7
2023-02-22 23:07:50,155 DEBUG TRAIN Batch 11/2900 loss 29.788044 loss_att 34.295532 loss_ctc 29.670649 loss_rnnt 28.676025 hw_loss 0.424078 lr 0.00051390 rank 6
2023-02-22 23:07:50,158 DEBUG TRAIN Batch 11/2900 loss 38.467979 loss_att 55.219162 loss_ctc 42.142498 loss_rnnt 34.426743 hw_loss 0.376984 lr 0.00051390 rank 1
2023-02-22 23:07:50,158 DEBUG TRAIN Batch 11/2900 loss 23.002598 loss_att 33.742146 loss_ctc 20.836565 loss_rnnt 20.944801 hw_loss 0.372542 lr 0.00051390 rank 5
2023-02-22 23:07:50,159 DEBUG TRAIN Batch 11/2900 loss 34.636124 loss_att 40.951698 loss_ctc 37.628056 loss_rnnt 32.708954 hw_loss 0.497125 lr 0.00051390 rank 4
2023-02-22 23:07:50,161 DEBUG TRAIN Batch 11/2900 loss 19.109028 loss_att 25.286812 loss_ctc 18.602983 loss_rnnt 17.702673 hw_loss 0.446757 lr 0.00051390 rank 2
2023-02-22 23:07:50,162 DEBUG TRAIN Batch 11/2900 loss 22.055264 loss_att 36.639423 loss_ctc 20.165203 loss_rnnt 19.173399 hw_loss 0.406953 lr 0.00051390 rank 3
2023-02-22 23:07:50,165 DEBUG TRAIN Batch 11/2900 loss 39.658630 loss_att 50.448544 loss_ctc 39.892384 loss_rnnt 37.226555 hw_loss 0.455484 lr 0.00051390 rank 0
2023-02-22 23:09:02,851 DEBUG TRAIN Batch 11/3000 loss 30.669388 loss_att 44.093346 loss_ctc 33.791561 loss_rnnt 27.366180 hw_loss 0.378983 lr 0.00051362 rank 3
2023-02-22 23:09:02,854 DEBUG TRAIN Batch 11/3000 loss 14.806451 loss_att 19.131466 loss_ctc 16.796257 loss_rnnt 13.449606 hw_loss 0.424756 lr 0.00051362 rank 7
2023-02-22 23:09:02,858 DEBUG TRAIN Batch 11/3000 loss 30.432337 loss_att 39.311966 loss_ctc 34.348080 loss_rnnt 27.941515 hw_loss 0.361489 lr 0.00051362 rank 1
2023-02-22 23:09:02,859 DEBUG TRAIN Batch 11/3000 loss 18.051786 loss_att 22.139927 loss_ctc 16.302402 loss_rnnt 17.224819 hw_loss 0.454854 lr 0.00051362 rank 4
2023-02-22 23:09:02,858 DEBUG TRAIN Batch 11/3000 loss 28.639435 loss_att 38.553246 loss_ctc 31.651451 loss_rnnt 26.044613 hw_loss 0.394607 lr 0.00051362 rank 6
2023-02-22 23:09:02,860 DEBUG TRAIN Batch 11/3000 loss 18.150997 loss_att 35.984894 loss_ctc 17.681520 loss_rnnt 14.455292 hw_loss 0.359105 lr 0.00051362 rank 0
2023-02-22 23:09:02,862 DEBUG TRAIN Batch 11/3000 loss 17.864935 loss_att 24.192242 loss_ctc 18.997246 loss_rnnt 16.237400 hw_loss 0.395808 lr 0.00051362 rank 5
2023-02-22 23:09:02,868 DEBUG TRAIN Batch 11/3000 loss 36.263863 loss_att 48.961445 loss_ctc 45.718277 loss_rnnt 32.247795 hw_loss 0.404930 lr 0.00051362 rank 2
2023-02-22 23:10:15,988 DEBUG TRAIN Batch 11/3100 loss 28.637486 loss_att 35.855766 loss_ctc 29.411926 loss_rnnt 26.852209 hw_loss 0.446931 lr 0.00051335 rank 4
2023-02-22 23:10:15,992 DEBUG TRAIN Batch 11/3100 loss 20.755796 loss_att 29.881046 loss_ctc 24.595398 loss_rnnt 18.225548 hw_loss 0.362350 lr 0.00051335 rank 0
2023-02-22 23:10:15,992 DEBUG TRAIN Batch 11/3100 loss 26.976812 loss_att 30.625025 loss_ctc 26.778070 loss_rnnt 26.006491 hw_loss 0.500963 lr 0.00051335 rank 1
2023-02-22 23:10:15,993 DEBUG TRAIN Batch 11/3100 loss 26.695021 loss_att 29.241821 loss_ctc 30.866270 loss_rnnt 25.399834 hw_loss 0.430619 lr 0.00051335 rank 7
2023-02-22 23:10:15,994 DEBUG TRAIN Batch 11/3100 loss 35.044556 loss_att 38.707977 loss_ctc 40.965740 loss_rnnt 33.314037 hw_loss 0.390646 lr 0.00051335 rank 5
2023-02-22 23:10:15,996 DEBUG TRAIN Batch 11/3100 loss 24.220980 loss_att 27.267372 loss_ctc 27.581093 loss_rnnt 22.909084 hw_loss 0.477376 lr 0.00051335 rank 3
2023-02-22 23:10:16,000 DEBUG TRAIN Batch 11/3100 loss 28.073469 loss_att 33.971481 loss_ctc 29.485266 loss_rnnt 26.457275 hw_loss 0.465661 lr 0.00051335 rank 2
2023-02-22 23:10:16,041 DEBUG TRAIN Batch 11/3100 loss 25.542551 loss_att 34.303135 loss_ctc 33.482449 loss_rnnt 22.504593 hw_loss 0.425973 lr 0.00051335 rank 6
2023-02-22 23:11:32,994 DEBUG TRAIN Batch 11/3200 loss 16.069794 loss_att 33.954239 loss_ctc 18.739429 loss_rnnt 11.922099 hw_loss 0.402850 lr 0.00051308 rank 7
2023-02-22 23:11:32,999 DEBUG TRAIN Batch 11/3200 loss 10.213556 loss_att 11.576872 loss_ctc 11.191206 loss_rnnt 9.466768 hw_loss 0.644573 lr 0.00051308 rank 1
2023-02-22 23:11:33,002 DEBUG TRAIN Batch 11/3200 loss 28.397453 loss_att 30.753550 loss_ctc 31.927450 loss_rnnt 27.138971 hw_loss 0.593615 lr 0.00051308 rank 3
2023-02-22 23:11:33,002 DEBUG TRAIN Batch 11/3200 loss 54.347374 loss_att 69.438400 loss_ctc 55.436516 loss_rnnt 51.005943 hw_loss 0.333771 lr 0.00051308 rank 6
2023-02-22 23:11:33,004 DEBUG TRAIN Batch 11/3200 loss 9.320836 loss_att 13.532827 loss_ctc 11.563951 loss_rnnt 7.907807 hw_loss 0.509151 lr 0.00051308 rank 0
2023-02-22 23:11:33,010 DEBUG TRAIN Batch 11/3200 loss 18.419701 loss_att 24.161633 loss_ctc 21.467993 loss_rnnt 16.612556 hw_loss 0.473097 lr 0.00051308 rank 2
2023-02-22 23:11:33,033 DEBUG TRAIN Batch 11/3200 loss 24.276651 loss_att 37.083035 loss_ctc 27.902243 loss_rnnt 21.052279 hw_loss 0.336909 lr 0.00051308 rank 5
2023-02-22 23:11:33,050 DEBUG TRAIN Batch 11/3200 loss 8.215407 loss_att 22.655550 loss_ctc 8.061993 loss_rnnt 5.116836 hw_loss 0.433124 lr 0.00051308 rank 4
2023-02-22 23:12:46,089 DEBUG TRAIN Batch 11/3300 loss 13.953471 loss_att 22.534605 loss_ctc 15.140484 loss_rnnt 11.831644 hw_loss 0.463745 lr 0.00051281 rank 3
2023-02-22 23:12:46,090 DEBUG TRAIN Batch 11/3300 loss 32.864109 loss_att 37.086468 loss_ctc 29.533005 loss_rnnt 32.285309 hw_loss 0.334645 lr 0.00051281 rank 7
2023-02-22 23:12:46,092 DEBUG TRAIN Batch 11/3300 loss 28.933821 loss_att 36.665676 loss_ctc 31.488949 loss_rnnt 26.833794 hw_loss 0.399322 lr 0.00051281 rank 4
2023-02-22 23:12:46,094 DEBUG TRAIN Batch 11/3300 loss 21.697691 loss_att 28.577641 loss_ctc 20.314589 loss_rnnt 20.234407 hw_loss 0.509454 lr 0.00051281 rank 5
2023-02-22 23:12:46,095 DEBUG TRAIN Batch 11/3300 loss 19.093969 loss_att 27.912163 loss_ctc 20.176870 loss_rnnt 17.014957 hw_loss 0.320601 lr 0.00051281 rank 1
2023-02-22 23:12:46,097 DEBUG TRAIN Batch 11/3300 loss 23.763962 loss_att 30.961367 loss_ctc 26.729034 loss_rnnt 21.741262 hw_loss 0.352269 lr 0.00051281 rank 0
2023-02-22 23:12:46,104 DEBUG TRAIN Batch 11/3300 loss 18.689529 loss_att 30.767502 loss_ctc 21.159191 loss_rnnt 15.714731 hw_loss 0.431091 lr 0.00051281 rank 2
2023-02-22 23:12:46,142 DEBUG TRAIN Batch 11/3300 loss 24.687002 loss_att 38.411613 loss_ctc 29.477362 loss_rnnt 21.094900 hw_loss 0.390874 lr 0.00051281 rank 6
2023-02-22 23:13:59,415 DEBUG TRAIN Batch 11/3400 loss 22.651245 loss_att 33.442554 loss_ctc 23.365585 loss_rnnt 20.194336 hw_loss 0.381373 lr 0.00051254 rank 4
2023-02-22 23:13:59,422 DEBUG TRAIN Batch 11/3400 loss 17.230728 loss_att 26.312178 loss_ctc 17.777632 loss_rnnt 15.140278 hw_loss 0.377325 lr 0.00051254 rank 3
2023-02-22 23:13:59,423 DEBUG TRAIN Batch 11/3400 loss 12.508976 loss_att 22.257490 loss_ctc 12.396426 loss_rnnt 10.296606 hw_loss 0.520637 lr 0.00051254 rank 7
2023-02-22 23:13:59,423 DEBUG TRAIN Batch 11/3400 loss 24.470510 loss_att 29.241850 loss_ctc 29.107168 loss_rnnt 22.627882 hw_loss 0.506513 lr 0.00051254 rank 1
2023-02-22 23:13:59,426 DEBUG TRAIN Batch 11/3400 loss 19.970968 loss_att 29.925022 loss_ctc 23.587858 loss_rnnt 17.298187 hw_loss 0.374474 lr 0.00051254 rank 0
2023-02-22 23:13:59,438 DEBUG TRAIN Batch 11/3400 loss 26.816788 loss_att 32.494812 loss_ctc 28.359838 loss_rnnt 25.283674 hw_loss 0.359568 lr 0.00051254 rank 2
2023-02-22 23:13:59,459 DEBUG TRAIN Batch 11/3400 loss 20.132826 loss_att 35.621006 loss_ctc 23.903717 loss_rnnt 16.294567 hw_loss 0.445942 lr 0.00051254 rank 6
2023-02-22 23:13:59,470 DEBUG TRAIN Batch 11/3400 loss 20.679823 loss_att 29.183266 loss_ctc 21.157261 loss_rnnt 18.658640 hw_loss 0.481563 lr 0.00051254 rank 5
2023-02-22 23:15:14,603 DEBUG TRAIN Batch 11/3500 loss 29.417746 loss_att 35.045269 loss_ctc 38.725845 loss_rnnt 26.825157 hw_loss 0.423757 lr 0.00051228 rank 5
2023-02-22 23:15:14,611 DEBUG TRAIN Batch 11/3500 loss 31.478312 loss_att 40.061440 loss_ctc 32.929607 loss_rnnt 29.336411 hw_loss 0.434566 lr 0.00051228 rank 0
2023-02-22 23:15:14,613 DEBUG TRAIN Batch 11/3500 loss 22.308851 loss_att 28.622440 loss_ctc 22.336884 loss_rnnt 20.854645 hw_loss 0.352035 lr 0.00051228 rank 6
2023-02-22 23:15:14,614 DEBUG TRAIN Batch 11/3500 loss 25.433811 loss_att 28.554308 loss_ctc 22.114281 loss_rnnt 25.046398 hw_loss 0.386102 lr 0.00051228 rank 7
2023-02-22 23:15:14,616 DEBUG TRAIN Batch 11/3500 loss 24.456926 loss_att 35.153496 loss_ctc 25.164112 loss_rnnt 21.996527 hw_loss 0.425238 lr 0.00051228 rank 3
2023-02-22 23:15:14,623 DEBUG TRAIN Batch 11/3500 loss 24.123976 loss_att 33.732254 loss_ctc 25.726248 loss_rnnt 21.789650 hw_loss 0.373188 lr 0.00051228 rank 2
2023-02-22 23:15:14,631 DEBUG TRAIN Batch 11/3500 loss 17.646191 loss_att 22.384644 loss_ctc 16.084135 loss_rnnt 16.680771 hw_loss 0.423757 lr 0.00051228 rank 1
2023-02-22 23:15:14,642 DEBUG TRAIN Batch 11/3500 loss 29.724777 loss_att 41.342972 loss_ctc 32.692768 loss_rnnt 26.796730 hw_loss 0.391263 lr 0.00051228 rank 4
2023-02-22 23:16:28,981 DEBUG TRAIN Batch 11/3600 loss 13.449431 loss_att 20.578407 loss_ctc 16.809610 loss_rnnt 11.371383 hw_loss 0.382930 lr 0.00051201 rank 4
2023-02-22 23:16:28,981 DEBUG TRAIN Batch 11/3600 loss 22.068512 loss_att 27.177099 loss_ctc 22.493410 loss_rnnt 20.803608 hw_loss 0.349750 lr 0.00051201 rank 7
2023-02-22 23:16:28,985 DEBUG TRAIN Batch 11/3600 loss 31.691420 loss_att 43.016212 loss_ctc 31.626610 loss_rnnt 29.259558 hw_loss 0.329145 lr 0.00051201 rank 0
2023-02-22 23:16:28,985 DEBUG TRAIN Batch 11/3600 loss 21.481365 loss_att 28.840822 loss_ctc 27.159618 loss_rnnt 19.065275 hw_loss 0.350807 lr 0.00051201 rank 6
2023-02-22 23:16:28,988 DEBUG TRAIN Batch 11/3600 loss 26.143009 loss_att 30.468094 loss_ctc 32.507645 loss_rnnt 24.260855 hw_loss 0.315974 lr 0.00051201 rank 3
2023-02-22 23:16:29,010 DEBUG TRAIN Batch 11/3600 loss 9.459692 loss_att 15.898750 loss_ctc 11.005595 loss_rnnt 7.774909 hw_loss 0.357846 lr 0.00051201 rank 1
2023-02-22 23:16:29,011 DEBUG TRAIN Batch 11/3600 loss 30.496853 loss_att 36.551861 loss_ctc 34.944721 loss_rnnt 28.488979 hw_loss 0.382164 lr 0.00051201 rank 5
2023-02-22 23:16:29,043 DEBUG TRAIN Batch 11/3600 loss 18.741886 loss_att 19.462370 loss_ctc 18.129171 loss_rnnt 18.496651 hw_loss 0.342812 lr 0.00051201 rank 2
2023-02-22 23:17:45,391 DEBUG TRAIN Batch 11/3700 loss 12.071679 loss_att 13.417968 loss_ctc 12.228653 loss_rnnt 11.547840 hw_loss 0.438096 lr 0.00051174 rank 1
2023-02-22 23:17:45,392 DEBUG TRAIN Batch 11/3700 loss 11.918697 loss_att 22.264462 loss_ctc 12.162148 loss_rnnt 9.568800 hw_loss 0.465533 lr 0.00051174 rank 0
2023-02-22 23:17:45,393 DEBUG TRAIN Batch 11/3700 loss 21.618851 loss_att 26.062328 loss_ctc 23.083281 loss_rnnt 20.330627 hw_loss 0.383006 lr 0.00051174 rank 7
2023-02-22 23:17:45,398 DEBUG TRAIN Batch 11/3700 loss 26.942345 loss_att 36.606209 loss_ctc 32.073437 loss_rnnt 24.163338 hw_loss 0.303916 lr 0.00051174 rank 4
2023-02-22 23:17:45,397 DEBUG TRAIN Batch 11/3700 loss 20.430592 loss_att 22.600418 loss_ctc 27.823685 loss_rnnt 18.796492 hw_loss 0.401983 lr 0.00051174 rank 3
2023-02-22 23:17:45,399 DEBUG TRAIN Batch 11/3700 loss 27.088831 loss_att 32.211311 loss_ctc 28.188261 loss_rnnt 25.575430 hw_loss 0.641838 lr 0.00051174 rank 5
2023-02-22 23:17:45,414 DEBUG TRAIN Batch 11/3700 loss 20.807501 loss_att 24.203716 loss_ctc 24.035278 loss_rnnt 19.488865 hw_loss 0.391915 lr 0.00051174 rank 2
2023-02-22 23:17:45,465 DEBUG TRAIN Batch 11/3700 loss 30.661589 loss_att 40.256035 loss_ctc 32.565468 loss_rnnt 28.235491 hw_loss 0.475049 lr 0.00051174 rank 6
2023-02-22 23:18:59,411 DEBUG TRAIN Batch 11/3800 loss 23.147915 loss_att 26.954674 loss_ctc 29.235088 loss_rnnt 21.331028 hw_loss 0.457334 lr 0.00051147 rank 4
2023-02-22 23:18:59,415 DEBUG TRAIN Batch 11/3800 loss 41.527584 loss_att 58.228561 loss_ctc 48.838726 loss_rnnt 37.002865 hw_loss 0.393201 lr 0.00051147 rank 5
2023-02-22 23:18:59,429 DEBUG TRAIN Batch 11/3800 loss 19.265121 loss_att 23.621138 loss_ctc 22.999329 loss_rnnt 17.613577 hw_loss 0.529588 lr 0.00051147 rank 7
2023-02-22 23:18:59,433 DEBUG TRAIN Batch 11/3800 loss 18.825964 loss_att 22.281689 loss_ctc 23.218891 loss_rnnt 17.294119 hw_loss 0.478082 lr 0.00051147 rank 0
2023-02-22 23:18:59,434 DEBUG TRAIN Batch 11/3800 loss 23.768999 loss_att 26.351326 loss_ctc 27.392601 loss_rnnt 22.474297 hw_loss 0.553294 lr 0.00051147 rank 6
2023-02-22 23:18:59,434 DEBUG TRAIN Batch 11/3800 loss 30.957947 loss_att 38.058319 loss_ctc 34.438171 loss_rnnt 28.840994 hw_loss 0.436591 lr 0.00051147 rank 1
2023-02-22 23:18:59,434 DEBUG TRAIN Batch 11/3800 loss 31.319248 loss_att 31.907785 loss_ctc 32.741920 loss_rnnt 30.791067 hw_loss 0.413972 lr 0.00051147 rank 2
2023-02-22 23:18:59,438 DEBUG TRAIN Batch 11/3800 loss 17.893555 loss_att 21.568737 loss_ctc 23.072334 loss_rnnt 16.230627 hw_loss 0.445099 lr 0.00051147 rank 3
2023-02-22 23:20:15,091 DEBUG TRAIN Batch 11/3900 loss 18.433025 loss_att 32.225540 loss_ctc 19.484810 loss_rnnt 15.278893 hw_loss 0.478858 lr 0.00051120 rank 7
2023-02-22 23:20:15,092 DEBUG TRAIN Batch 11/3900 loss 22.903299 loss_att 22.318192 loss_ctc 23.970449 loss_rnnt 22.567158 hw_loss 0.582892 lr 0.00051120 rank 3
2023-02-22 23:20:15,097 DEBUG TRAIN Batch 11/3900 loss 15.167364 loss_att 25.031593 loss_ctc 14.169140 loss_rnnt 13.145255 hw_loss 0.341927 lr 0.00051120 rank 0
2023-02-22 23:20:15,099 DEBUG TRAIN Batch 11/3900 loss 53.068863 loss_att 58.007568 loss_ctc 55.571350 loss_rnnt 51.565639 hw_loss 0.340900 lr 0.00051120 rank 5
2023-02-22 23:20:15,099 DEBUG TRAIN Batch 11/3900 loss 18.684111 loss_att 23.261810 loss_ctc 20.944122 loss_rnnt 17.235910 hw_loss 0.433734 lr 0.00051120 rank 6
2023-02-22 23:20:15,102 DEBUG TRAIN Batch 11/3900 loss 13.118133 loss_att 22.213289 loss_ctc 15.507651 loss_rnnt 10.787017 hw_loss 0.362779 lr 0.00051120 rank 1
2023-02-22 23:20:15,103 DEBUG TRAIN Batch 11/3900 loss 15.244176 loss_att 29.783518 loss_ctc 17.639816 loss_rnnt 11.860336 hw_loss 0.293534 lr 0.00051120 rank 2
2023-02-22 23:20:15,106 DEBUG TRAIN Batch 11/3900 loss 26.208767 loss_att 38.751236 loss_ctc 28.998734 loss_rnnt 23.147545 hw_loss 0.338874 lr 0.00051120 rank 4
2023-02-22 23:21:28,520 DEBUG TRAIN Batch 11/4000 loss 22.076586 loss_att 22.610073 loss_ctc 24.012024 loss_rnnt 21.524244 hw_loss 0.351720 lr 0.00051094 rank 1
2023-02-22 23:21:28,527 DEBUG TRAIN Batch 11/4000 loss 12.827833 loss_att 21.511488 loss_ctc 16.422075 loss_rnnt 10.384531 hw_loss 0.426259 lr 0.00051094 rank 7
2023-02-22 23:21:28,529 DEBUG TRAIN Batch 11/4000 loss 16.760044 loss_att 19.992468 loss_ctc 17.251678 loss_rnnt 15.837164 hw_loss 0.395334 lr 0.00051094 rank 3
2023-02-22 23:21:28,529 DEBUG TRAIN Batch 11/4000 loss 12.115948 loss_att 19.004223 loss_ctc 14.332829 loss_rnnt 10.228981 hw_loss 0.400739 lr 0.00051094 rank 2
2023-02-22 23:21:28,529 DEBUG TRAIN Batch 11/4000 loss 10.957565 loss_att 15.961377 loss_ctc 11.841072 loss_rnnt 9.655790 hw_loss 0.343519 lr 0.00051094 rank 5
2023-02-22 23:21:28,529 DEBUG TRAIN Batch 11/4000 loss 34.059040 loss_att 36.907879 loss_ctc 40.043388 loss_rnnt 32.541355 hw_loss 0.281256 lr 0.00051094 rank 6
2023-02-22 23:21:28,534 DEBUG TRAIN Batch 11/4000 loss 31.099953 loss_att 36.189667 loss_ctc 35.972794 loss_rnnt 29.255125 hw_loss 0.332202 lr 0.00051094 rank 0
2023-02-22 23:21:28,578 DEBUG TRAIN Batch 11/4000 loss 26.533945 loss_att 36.075024 loss_ctc 31.642052 loss_rnnt 23.779272 hw_loss 0.310084 lr 0.00051094 rank 4
2023-02-22 23:22:41,470 DEBUG TRAIN Batch 11/4100 loss 11.998318 loss_att 17.520210 loss_ctc 13.120625 loss_rnnt 10.515098 hw_loss 0.429751 lr 0.00051067 rank 5
2023-02-22 23:22:41,471 DEBUG TRAIN Batch 11/4100 loss 35.139244 loss_att 42.595474 loss_ctc 39.284100 loss_rnnt 32.829803 hw_loss 0.497903 lr 0.00051067 rank 7
2023-02-22 23:22:41,474 DEBUG TRAIN Batch 11/4100 loss 23.952332 loss_att 30.488674 loss_ctc 22.055531 loss_rnnt 22.675076 hw_loss 0.417927 lr 0.00051067 rank 3
2023-02-22 23:22:41,474 DEBUG TRAIN Batch 11/4100 loss 23.479626 loss_att 32.724617 loss_ctc 32.561165 loss_rnnt 20.199417 hw_loss 0.413130 lr 0.00051067 rank 0
2023-02-22 23:22:41,474 DEBUG TRAIN Batch 11/4100 loss 24.916121 loss_att 29.123230 loss_ctc 27.304771 loss_rnnt 23.599031 hw_loss 0.294713 lr 0.00051067 rank 4
2023-02-22 23:22:41,478 DEBUG TRAIN Batch 11/4100 loss 37.175652 loss_att 48.317951 loss_ctc 42.915382 loss_rnnt 34.008892 hw_loss 0.324387 lr 0.00051067 rank 2
2023-02-22 23:22:41,478 DEBUG TRAIN Batch 11/4100 loss 17.378832 loss_att 27.494247 loss_ctc 16.771435 loss_rnnt 15.223955 hw_loss 0.398962 lr 0.00051067 rank 6
2023-02-22 23:22:41,524 DEBUG TRAIN Batch 11/4100 loss 35.491508 loss_att 47.458885 loss_ctc 42.313869 loss_rnnt 31.970139 hw_loss 0.409213 lr 0.00051067 rank 1
2023-02-22 23:23:55,773 DEBUG TRAIN Batch 11/4200 loss 20.577496 loss_att 29.295485 loss_ctc 21.180653 loss_rnnt 18.510923 hw_loss 0.454789 lr 0.00051040 rank 1
2023-02-22 23:23:55,783 DEBUG TRAIN Batch 11/4200 loss 28.082661 loss_att 34.195515 loss_ctc 34.598896 loss_rnnt 25.764866 hw_loss 0.424478 lr 0.00051040 rank 5
2023-02-22 23:23:55,789 DEBUG TRAIN Batch 11/4200 loss 21.114038 loss_att 28.160503 loss_ctc 27.137646 loss_rnnt 18.709770 hw_loss 0.359676 lr 0.00051040 rank 4
2023-02-22 23:23:55,789 DEBUG TRAIN Batch 11/4200 loss 18.479261 loss_att 30.245356 loss_ctc 22.461922 loss_rnnt 15.380505 hw_loss 0.402219 lr 0.00051040 rank 6
2023-02-22 23:23:55,788 DEBUG TRAIN Batch 11/4200 loss 42.971889 loss_att 58.513641 loss_ctc 42.639145 loss_rnnt 39.696671 hw_loss 0.396062 lr 0.00051040 rank 3
2023-02-22 23:23:55,791 DEBUG TRAIN Batch 11/4200 loss 18.842058 loss_att 28.268030 loss_ctc 20.613529 loss_rnnt 16.531052 hw_loss 0.355530 lr 0.00051040 rank 0
2023-02-22 23:23:55,792 DEBUG TRAIN Batch 11/4200 loss 27.145632 loss_att 36.886421 loss_ctc 26.667519 loss_rnnt 25.018600 hw_loss 0.454915 lr 0.00051040 rank 7
2023-02-22 23:23:55,798 DEBUG TRAIN Batch 11/4200 loss 15.605289 loss_att 19.244202 loss_ctc 20.932203 loss_rnnt 13.963329 hw_loss 0.382354 lr 0.00051040 rank 2
2023-02-22 23:25:10,519 DEBUG TRAIN Batch 11/4300 loss 35.959618 loss_att 38.827065 loss_ctc 39.648376 loss_rnnt 34.632492 hw_loss 0.490878 lr 0.00051014 rank 6
2023-02-22 23:25:10,519 DEBUG TRAIN Batch 11/4300 loss 25.672024 loss_att 31.705105 loss_ctc 28.968534 loss_rnnt 23.835735 hw_loss 0.356508 lr 0.00051014 rank 3
2023-02-22 23:25:10,520 DEBUG TRAIN Batch 11/4300 loss 18.123554 loss_att 27.164751 loss_ctc 20.607967 loss_rnnt 15.737890 hw_loss 0.461570 lr 0.00051014 rank 7
2023-02-22 23:25:10,520 DEBUG TRAIN Batch 11/4300 loss 20.672188 loss_att 30.641850 loss_ctc 24.404039 loss_rnnt 17.983179 hw_loss 0.370300 lr 0.00051014 rank 0
2023-02-22 23:25:10,525 DEBUG TRAIN Batch 11/4300 loss 36.607952 loss_att 41.438629 loss_ctc 35.286156 loss_rnnt 35.590847 hw_loss 0.426018 lr 0.00051014 rank 1
2023-02-22 23:25:10,525 DEBUG TRAIN Batch 11/4300 loss 32.151642 loss_att 39.525955 loss_ctc 38.326317 loss_rnnt 29.638062 hw_loss 0.403922 lr 0.00051014 rank 5
2023-02-22 23:25:10,533 DEBUG TRAIN Batch 11/4300 loss 27.036428 loss_att 33.341114 loss_ctc 30.053602 loss_rnnt 25.169645 hw_loss 0.381669 lr 0.00051014 rank 2
2023-02-22 23:25:10,580 DEBUG TRAIN Batch 11/4300 loss 12.532306 loss_att 17.725241 loss_ctc 13.276979 loss_rnnt 11.152723 hw_loss 0.453198 lr 0.00051014 rank 4
2023-02-22 23:26:23,893 DEBUG TRAIN Batch 11/4400 loss 22.135284 loss_att 23.037998 loss_ctc 24.148581 loss_rnnt 21.460653 hw_loss 0.423091 lr 0.00050987 rank 7
2023-02-22 23:26:23,895 DEBUG TRAIN Batch 11/4400 loss 25.078350 loss_att 36.369598 loss_ctc 26.125071 loss_rnnt 22.418266 hw_loss 0.491759 lr 0.00050987 rank 3
2023-02-22 23:26:23,898 DEBUG TRAIN Batch 11/4400 loss 15.453586 loss_att 17.101051 loss_ctc 17.241909 loss_rnnt 14.605813 hw_loss 0.524694 lr 0.00050987 rank 5
2023-02-22 23:26:23,899 DEBUG TRAIN Batch 11/4400 loss 35.569908 loss_att 41.599144 loss_ctc 35.865837 loss_rnnt 34.079685 hw_loss 0.459211 lr 0.00050987 rank 0
2023-02-22 23:26:23,900 DEBUG TRAIN Batch 11/4400 loss 29.555578 loss_att 38.401451 loss_ctc 40.093937 loss_rnnt 26.122272 hw_loss 0.485656 lr 0.00050987 rank 6
2023-02-22 23:26:23,902 DEBUG TRAIN Batch 11/4400 loss 14.573709 loss_att 20.223595 loss_ctc 18.654047 loss_rnnt 12.650434 hw_loss 0.467349 lr 0.00050987 rank 1
2023-02-22 23:26:23,903 DEBUG TRAIN Batch 11/4400 loss 16.198883 loss_att 19.079712 loss_ctc 20.026062 loss_rnnt 14.831739 hw_loss 0.526290 lr 0.00050987 rank 4
2023-02-22 23:26:23,916 DEBUG TRAIN Batch 11/4400 loss 25.808979 loss_att 27.444832 loss_ctc 27.550293 loss_rnnt 24.993149 hw_loss 0.480908 lr 0.00050987 rank 2
2023-02-22 23:27:37,298 DEBUG TRAIN Batch 11/4500 loss 16.340916 loss_att 17.458580 loss_ctc 18.276459 loss_rnnt 15.594587 hw_loss 0.496354 lr 0.00050961 rank 1
2023-02-22 23:27:37,303 DEBUG TRAIN Batch 11/4500 loss 31.375809 loss_att 38.129292 loss_ctc 31.234459 loss_rnnt 29.827688 hw_loss 0.405513 lr 0.00050961 rank 7
2023-02-22 23:27:37,306 DEBUG TRAIN Batch 11/4500 loss 29.892168 loss_att 31.550669 loss_ctc 30.309517 loss_rnnt 29.285027 hw_loss 0.412109 lr 0.00050961 rank 5
2023-02-22 23:27:37,306 DEBUG TRAIN Batch 11/4500 loss 14.192863 loss_att 17.760231 loss_ctc 13.988930 loss_rnnt 13.288055 hw_loss 0.409735 lr 0.00050961 rank 3
2023-02-22 23:27:37,307 DEBUG TRAIN Batch 11/4500 loss 27.177124 loss_att 27.671894 loss_ctc 31.398083 loss_rnnt 26.262060 hw_loss 0.474968 lr 0.00050961 rank 2
2023-02-22 23:27:37,308 DEBUG TRAIN Batch 11/4500 loss 23.242060 loss_att 29.559486 loss_ctc 26.097153 loss_rnnt 21.365860 hw_loss 0.435068 lr 0.00050961 rank 6
2023-02-22 23:27:37,313 DEBUG TRAIN Batch 11/4500 loss 13.722327 loss_att 15.789999 loss_ctc 15.479380 loss_rnnt 12.802332 hw_loss 0.510350 lr 0.00050961 rank 0
2023-02-22 23:27:37,346 DEBUG TRAIN Batch 11/4500 loss 24.837198 loss_att 29.461525 loss_ctc 28.100586 loss_rnnt 23.167171 hw_loss 0.581333 lr 0.00050961 rank 4
2023-02-22 23:28:53,005 DEBUG TRAIN Batch 11/4600 loss 8.233879 loss_att 13.977186 loss_ctc 9.440027 loss_rnnt 6.716341 hw_loss 0.390105 lr 0.00050934 rank 6
2023-02-22 23:28:53,007 DEBUG TRAIN Batch 11/4600 loss 23.232870 loss_att 33.164230 loss_ctc 24.242079 loss_rnnt 20.840088 hw_loss 0.509907 lr 0.00050934 rank 5
2023-02-22 23:28:53,011 DEBUG TRAIN Batch 11/4600 loss 15.390499 loss_att 20.941406 loss_ctc 17.214657 loss_rnnt 13.807339 hw_loss 0.430795 lr 0.00050934 rank 0
2023-02-22 23:28:53,023 DEBUG TRAIN Batch 11/4600 loss 30.147989 loss_att 31.950390 loss_ctc 30.346579 loss_rnnt 29.529959 hw_loss 0.433258 lr 0.00050934 rank 7
2023-02-22 23:28:53,024 DEBUG TRAIN Batch 11/4600 loss 25.837431 loss_att 32.667725 loss_ctc 25.989304 loss_rnnt 24.290070 hw_loss 0.301975 lr 0.00050934 rank 1
2023-02-22 23:28:53,059 DEBUG TRAIN Batch 11/4600 loss 28.556648 loss_att 38.404129 loss_ctc 29.833370 loss_rnnt 26.136351 hw_loss 0.526069 lr 0.00050934 rank 4
2023-02-22 23:28:53,058 DEBUG TRAIN Batch 11/4600 loss 16.785437 loss_att 26.427620 loss_ctc 20.486893 loss_rnnt 14.101957 hw_loss 0.490342 lr 0.00050934 rank 2
2023-02-22 23:28:53,062 DEBUG TRAIN Batch 11/4600 loss 31.184189 loss_att 42.692650 loss_ctc 28.355377 loss_rnnt 29.042233 hw_loss 0.407699 lr 0.00050934 rank 3
2023-02-22 23:30:06,424 DEBUG TRAIN Batch 11/4700 loss 11.433055 loss_att 22.501625 loss_ctc 10.221745 loss_rnnt 9.175623 hw_loss 0.384797 lr 0.00050908 rank 3
2023-02-22 23:30:06,426 DEBUG TRAIN Batch 11/4700 loss 17.697807 loss_att 21.548765 loss_ctc 21.258444 loss_rnnt 16.243410 hw_loss 0.392724 lr 0.00050908 rank 4
2023-02-22 23:30:06,428 DEBUG TRAIN Batch 11/4700 loss 16.083284 loss_att 19.536606 loss_ctc 18.335705 loss_rnnt 14.874480 hw_loss 0.408406 lr 0.00050908 rank 7
2023-02-22 23:30:06,430 DEBUG TRAIN Batch 11/4700 loss 21.868931 loss_att 25.773193 loss_ctc 24.488251 loss_rnnt 20.533607 hw_loss 0.384797 lr 0.00050908 rank 0
2023-02-22 23:30:06,434 DEBUG TRAIN Batch 11/4700 loss 10.244192 loss_att 14.337461 loss_ctc 11.578588 loss_rnnt 9.048511 hw_loss 0.373327 lr 0.00050908 rank 2
2023-02-22 23:30:06,435 DEBUG TRAIN Batch 11/4700 loss 30.240368 loss_att 34.534618 loss_ctc 30.495171 loss_rnnt 29.167465 hw_loss 0.337650 lr 0.00050908 rank 1
2023-02-22 23:30:06,435 DEBUG TRAIN Batch 11/4700 loss 30.422365 loss_att 37.323929 loss_ctc 31.610065 loss_rnnt 28.633423 hw_loss 0.469257 lr 0.00050908 rank 6
2023-02-22 23:30:06,435 DEBUG TRAIN Batch 11/4700 loss 11.736757 loss_att 15.603900 loss_ctc 12.530219 loss_rnnt 10.618961 hw_loss 0.447324 lr 0.00050908 rank 5
2023-02-22 23:31:19,557 DEBUG TRAIN Batch 11/4800 loss 54.079659 loss_att 53.545219 loss_ctc 60.090397 loss_rnnt 53.162464 hw_loss 0.417465 lr 0.00050882 rank 6
2023-02-22 23:31:19,559 DEBUG TRAIN Batch 11/4800 loss 20.609369 loss_att 30.026108 loss_ctc 24.601147 loss_rnnt 17.967264 hw_loss 0.424724 lr 0.00050882 rank 5
2023-02-22 23:31:19,562 DEBUG TRAIN Batch 11/4800 loss 19.459915 loss_att 26.541677 loss_ctc 24.725695 loss_rnnt 17.081562 hw_loss 0.487309 lr 0.00050882 rank 4
2023-02-22 23:31:19,565 DEBUG TRAIN Batch 11/4800 loss 18.904854 loss_att 25.312555 loss_ctc 19.999519 loss_rnnt 17.319290 hw_loss 0.296380 lr 0.00050882 rank 7
2023-02-22 23:31:19,566 DEBUG TRAIN Batch 11/4800 loss 7.124213 loss_att 16.012186 loss_ctc 6.431158 loss_rnnt 5.189738 hw_loss 0.467415 lr 0.00050882 rank 0
2023-02-22 23:31:19,567 DEBUG TRAIN Batch 11/4800 loss 15.731478 loss_att 17.799101 loss_ctc 18.148943 loss_rnnt 14.802685 hw_loss 0.361761 lr 0.00050882 rank 3
2023-02-22 23:31:19,567 DEBUG TRAIN Batch 11/4800 loss 23.546370 loss_att 31.223026 loss_ctc 30.708302 loss_rnnt 20.854763 hw_loss 0.377532 lr 0.00050882 rank 1
2023-02-22 23:31:19,572 DEBUG TRAIN Batch 11/4800 loss 14.625413 loss_att 23.508205 loss_ctc 16.955040 loss_rnnt 12.367378 hw_loss 0.320360 lr 0.00050882 rank 2
2023-02-22 23:32:33,134 DEBUG TRAIN Batch 11/4900 loss 28.305168 loss_att 33.313061 loss_ctc 30.404202 loss_rnnt 26.846819 hw_loss 0.331683 lr 0.00050855 rank 7
2023-02-22 23:32:33,136 DEBUG TRAIN Batch 11/4900 loss 30.420696 loss_att 36.129745 loss_ctc 33.766174 loss_rnnt 28.627245 hw_loss 0.385456 lr 0.00050855 rank 0
2023-02-22 23:32:33,138 DEBUG TRAIN Batch 11/4900 loss 25.007010 loss_att 28.919331 loss_ctc 26.826778 loss_rnnt 23.784924 hw_loss 0.369349 lr 0.00050855 rank 2
2023-02-22 23:32:33,139 DEBUG TRAIN Batch 11/4900 loss 18.238106 loss_att 22.005917 loss_ctc 20.937849 loss_rnnt 16.918837 hw_loss 0.385764 lr 0.00050855 rank 3
2023-02-22 23:32:33,139 DEBUG TRAIN Batch 11/4900 loss 17.945087 loss_att 25.531498 loss_ctc 21.358673 loss_rnnt 15.749889 hw_loss 0.417694 lr 0.00050855 rank 5
2023-02-22 23:32:33,140 DEBUG TRAIN Batch 11/4900 loss 21.298332 loss_att 23.785851 loss_ctc 22.060356 loss_rnnt 20.434505 hw_loss 0.496347 lr 0.00050855 rank 4
2023-02-22 23:32:33,141 DEBUG TRAIN Batch 11/4900 loss 11.683338 loss_att 19.234989 loss_ctc 12.402421 loss_rnnt 9.906590 hw_loss 0.319764 lr 0.00050855 rank 1
2023-02-22 23:32:33,185 DEBUG TRAIN Batch 11/4900 loss 18.117531 loss_att 22.878590 loss_ctc 18.138603 loss_rnnt 16.946096 hw_loss 0.405775 lr 0.00050855 rank 6
2023-02-22 23:33:49,832 DEBUG TRAIN Batch 11/5000 loss 19.705877 loss_att 26.381039 loss_ctc 23.742949 loss_rnnt 17.574257 hw_loss 0.484335 lr 0.00050829 rank 7
2023-02-22 23:33:49,833 DEBUG TRAIN Batch 11/5000 loss 45.283009 loss_att 55.846909 loss_ctc 47.891899 loss_rnnt 42.617233 hw_loss 0.384646 lr 0.00050829 rank 1
2023-02-22 23:33:49,834 DEBUG TRAIN Batch 11/5000 loss 28.796495 loss_att 28.440659 loss_ctc 31.420841 loss_rnnt 28.231165 hw_loss 0.537341 lr 0.00050829 rank 5
2023-02-22 23:33:49,837 DEBUG TRAIN Batch 11/5000 loss 35.878857 loss_att 37.539902 loss_ctc 37.545078 loss_rnnt 35.103882 hw_loss 0.413636 lr 0.00050829 rank 3
2023-02-22 23:33:49,839 DEBUG TRAIN Batch 11/5000 loss 28.106119 loss_att 33.956333 loss_ctc 31.681606 loss_rnnt 26.193930 hw_loss 0.497652 lr 0.00050829 rank 4
2023-02-22 23:33:49,841 DEBUG TRAIN Batch 11/5000 loss 23.839169 loss_att 33.696587 loss_ctc 29.689445 loss_rnnt 20.825163 hw_loss 0.492163 lr 0.00050829 rank 0
2023-02-22 23:33:49,845 DEBUG TRAIN Batch 11/5000 loss 13.551341 loss_att 17.216339 loss_ctc 14.546378 loss_rnnt 12.438528 hw_loss 0.463392 lr 0.00050829 rank 2
2023-02-22 23:33:49,887 DEBUG TRAIN Batch 11/5000 loss 31.516479 loss_att 34.216995 loss_ctc 36.752319 loss_rnnt 30.041971 hw_loss 0.443049 lr 0.00050829 rank 6
2023-02-22 23:35:03,188 DEBUG TRAIN Batch 11/5100 loss 10.237806 loss_att 10.262950 loss_ctc 10.447544 loss_rnnt 9.783150 hw_loss 0.790621 lr 0.00050803 rank 7
2023-02-22 23:35:03,194 DEBUG TRAIN Batch 11/5100 loss 27.157215 loss_att 32.133499 loss_ctc 32.775471 loss_rnnt 25.164137 hw_loss 0.466353 lr 0.00050803 rank 1
2023-02-22 23:35:03,194 DEBUG TRAIN Batch 11/5100 loss 22.731655 loss_att 26.431267 loss_ctc 24.409727 loss_rnnt 21.527691 hw_loss 0.450562 lr 0.00050803 rank 3
2023-02-22 23:35:03,195 DEBUG TRAIN Batch 11/5100 loss 32.104649 loss_att 35.271179 loss_ctc 36.846066 loss_rnnt 30.605213 hw_loss 0.438631 lr 0.00050803 rank 0
2023-02-22 23:35:03,197 DEBUG TRAIN Batch 11/5100 loss 17.683996 loss_att 26.707664 loss_ctc 16.667744 loss_rnnt 15.743782 hw_loss 0.508089 lr 0.00050803 rank 5
2023-02-22 23:35:03,197 DEBUG TRAIN Batch 11/5100 loss 29.106905 loss_att 42.169556 loss_ctc 40.765953 loss_rnnt 24.744749 hw_loss 0.365784 lr 0.00050803 rank 4
2023-02-22 23:35:03,199 DEBUG TRAIN Batch 11/5100 loss 12.261398 loss_att 22.383015 loss_ctc 15.498047 loss_rnnt 9.596542 hw_loss 0.391837 lr 0.00050803 rank 6
2023-02-22 23:35:03,211 DEBUG TRAIN Batch 11/5100 loss 15.478448 loss_att 15.477038 loss_ctc 16.755217 loss_rnnt 14.903921 hw_loss 0.758575 lr 0.00050803 rank 2
2023-02-22 23:36:16,722 DEBUG TRAIN Batch 11/5200 loss 48.070362 loss_att 52.008362 loss_ctc 49.242188 loss_rnnt 46.898220 hw_loss 0.428066 lr 0.00050776 rank 1
2023-02-22 23:36:16,724 DEBUG TRAIN Batch 11/5200 loss 18.089048 loss_att 24.416706 loss_ctc 15.223059 loss_rnnt 17.005426 hw_loss 0.375420 lr 0.00050776 rank 7
2023-02-22 23:36:16,725 DEBUG TRAIN Batch 11/5200 loss 32.759304 loss_att 41.417248 loss_ctc 40.507317 loss_rnnt 29.791737 hw_loss 0.380447 lr 0.00050776 rank 4
2023-02-22 23:36:16,727 DEBUG TRAIN Batch 11/5200 loss 11.192362 loss_att 14.486591 loss_ctc 8.876421 loss_rnnt 10.621839 hw_loss 0.413380 lr 0.00050776 rank 3
2023-02-22 23:36:16,729 DEBUG TRAIN Batch 11/5200 loss 17.459150 loss_att 23.006308 loss_ctc 21.201694 loss_rnnt 15.618472 hw_loss 0.435453 lr 0.00050776 rank 5
2023-02-22 23:36:16,730 DEBUG TRAIN Batch 11/5200 loss 13.639143 loss_att 20.763798 loss_ctc 19.127823 loss_rnnt 11.266403 hw_loss 0.404970 lr 0.00050776 rank 6
2023-02-22 23:36:16,731 DEBUG TRAIN Batch 11/5200 loss 28.464056 loss_att 31.051109 loss_ctc 33.453495 loss_rnnt 27.111891 hw_loss 0.317803 lr 0.00050776 rank 0
2023-02-22 23:36:16,736 DEBUG TRAIN Batch 11/5200 loss 11.467724 loss_att 18.038368 loss_ctc 12.930721 loss_rnnt 9.761166 hw_loss 0.370056 lr 0.00050776 rank 2
2023-02-22 23:37:31,642 DEBUG TRAIN Batch 11/5300 loss 10.226113 loss_att 19.467327 loss_ctc 14.213836 loss_rnnt 7.658084 hw_loss 0.352670 lr 0.00050750 rank 6
2023-02-22 23:37:31,653 DEBUG TRAIN Batch 11/5300 loss 33.619968 loss_att 38.985737 loss_ctc 34.540943 loss_rnnt 32.187317 hw_loss 0.443811 lr 0.00050750 rank 7
2023-02-22 23:37:31,657 DEBUG TRAIN Batch 11/5300 loss 16.316648 loss_att 23.664841 loss_ctc 15.733891 loss_rnnt 14.746371 hw_loss 0.334389 lr 0.00050750 rank 1
2023-02-22 23:37:31,658 DEBUG TRAIN Batch 11/5300 loss 8.912884 loss_att 10.906887 loss_ctc 11.440814 loss_rnnt 7.925450 hw_loss 0.471701 lr 0.00050750 rank 5
2023-02-22 23:37:31,658 DEBUG TRAIN Batch 11/5300 loss 26.701660 loss_att 32.722115 loss_ctc 28.616341 loss_rnnt 25.060890 hw_loss 0.340102 lr 0.00050750 rank 0
2023-02-22 23:37:31,663 DEBUG TRAIN Batch 11/5300 loss 8.858508 loss_att 16.349979 loss_ctc 11.869014 loss_rnnt 6.739173 hw_loss 0.411825 lr 0.00050750 rank 3
2023-02-22 23:37:31,669 DEBUG TRAIN Batch 11/5300 loss 32.514328 loss_att 31.745768 loss_ctc 33.169048 loss_rnnt 32.371201 hw_loss 0.392896 lr 0.00050750 rank 4
2023-02-22 23:37:31,671 DEBUG TRAIN Batch 11/5300 loss 42.292614 loss_att 56.449360 loss_ctc 48.411804 loss_rnnt 38.440781 hw_loss 0.383615 lr 0.00050750 rank 2
2023-02-22 23:38:45,813 DEBUG TRAIN Batch 11/5400 loss 23.503408 loss_att 30.368940 loss_ctc 26.667250 loss_rnnt 21.523386 hw_loss 0.347004 lr 0.00050724 rank 7
2023-02-22 23:38:45,819 DEBUG TRAIN Batch 11/5400 loss 21.864714 loss_att 32.025055 loss_ctc 28.833054 loss_rnnt 18.694437 hw_loss 0.392056 lr 0.00050724 rank 5
2023-02-22 23:38:45,820 DEBUG TRAIN Batch 11/5400 loss 24.502604 loss_att 27.016937 loss_ctc 23.911007 loss_rnnt 23.894430 hw_loss 0.345347 lr 0.00050724 rank 3
2023-02-22 23:38:45,820 DEBUG TRAIN Batch 11/5400 loss 8.553362 loss_att 15.858173 loss_ctc 10.865438 loss_rnnt 6.595840 hw_loss 0.353032 lr 0.00050724 rank 1
2023-02-22 23:38:45,822 DEBUG TRAIN Batch 11/5400 loss 11.572957 loss_att 17.009489 loss_ctc 13.111439 loss_rnnt 10.050858 hw_loss 0.430615 lr 0.00050724 rank 2
2023-02-22 23:38:45,822 DEBUG TRAIN Batch 11/5400 loss 15.721066 loss_att 24.433315 loss_ctc 20.706116 loss_rnnt 13.099936 hw_loss 0.401260 lr 0.00050724 rank 6
2023-02-22 23:38:45,823 DEBUG TRAIN Batch 11/5400 loss 27.049332 loss_att 32.690975 loss_ctc 29.726414 loss_rnnt 25.356358 hw_loss 0.389437 lr 0.00050724 rank 4
2023-02-22 23:38:45,828 DEBUG TRAIN Batch 11/5400 loss 27.602070 loss_att 32.882469 loss_ctc 34.511734 loss_rnnt 25.431339 hw_loss 0.362547 lr 0.00050724 rank 0
2023-02-22 23:39:58,763 DEBUG TRAIN Batch 11/5500 loss 25.836969 loss_att 32.118736 loss_ctc 31.450274 loss_rnnt 23.630550 hw_loss 0.378049 lr 0.00050698 rank 3
2023-02-22 23:39:58,762 DEBUG TRAIN Batch 11/5500 loss 23.689720 loss_att 29.229710 loss_ctc 25.023392 loss_rnnt 22.179104 hw_loss 0.421496 lr 0.00050698 rank 4
2023-02-22 23:39:58,764 DEBUG TRAIN Batch 11/5500 loss 14.528782 loss_att 18.517347 loss_ctc 17.669651 loss_rnnt 13.081925 hw_loss 0.431925 lr 0.00050698 rank 7
2023-02-22 23:39:58,765 DEBUG TRAIN Batch 11/5500 loss 23.508354 loss_att 31.548130 loss_ctc 28.654734 loss_rnnt 20.969973 hw_loss 0.457952 lr 0.00050698 rank 1
2023-02-22 23:39:58,766 DEBUG TRAIN Batch 11/5500 loss 13.426100 loss_att 20.635998 loss_ctc 15.546056 loss_rnnt 11.486531 hw_loss 0.402990 lr 0.00050698 rank 0
2023-02-22 23:39:58,769 DEBUG TRAIN Batch 11/5500 loss 14.119459 loss_att 18.253620 loss_ctc 17.135649 loss_rnnt 12.626377 hw_loss 0.495172 lr 0.00050698 rank 6
2023-02-22 23:39:58,779 DEBUG TRAIN Batch 11/5500 loss 10.581861 loss_att 16.349905 loss_ctc 14.432531 loss_rnnt 8.684904 hw_loss 0.431109 lr 0.00050698 rank 5
2023-02-22 23:39:58,781 DEBUG TRAIN Batch 11/5500 loss 24.370117 loss_att 30.784115 loss_ctc 33.201374 loss_rnnt 21.653769 hw_loss 0.480087 lr 0.00050698 rank 2
2023-02-22 23:41:12,689 DEBUG TRAIN Batch 11/5600 loss 15.802989 loss_att 20.706921 loss_ctc 22.009750 loss_rnnt 13.754208 hw_loss 0.450800 lr 0.00050672 rank 4
2023-02-22 23:41:12,699 DEBUG TRAIN Batch 11/5600 loss 20.106102 loss_att 21.616879 loss_ctc 21.298576 loss_rnnt 19.447666 hw_loss 0.369905 lr 0.00050672 rank 2
2023-02-22 23:41:12,699 DEBUG TRAIN Batch 11/5600 loss 16.090754 loss_att 21.857021 loss_ctc 19.585114 loss_rnnt 14.291249 hw_loss 0.338131 lr 0.00050672 rank 1
2023-02-22 23:41:12,700 DEBUG TRAIN Batch 11/5600 loss 28.366123 loss_att 30.627136 loss_ctc 32.393906 loss_rnnt 27.186298 hw_loss 0.357342 lr 0.00050672 rank 3
2023-02-22 23:41:12,701 DEBUG TRAIN Batch 11/5600 loss 25.620316 loss_att 32.510468 loss_ctc 26.316191 loss_rnnt 23.936884 hw_loss 0.398660 lr 0.00050672 rank 7
2023-02-22 23:41:12,705 DEBUG TRAIN Batch 11/5600 loss 11.384524 loss_att 18.439589 loss_ctc 12.231380 loss_rnnt 9.614467 hw_loss 0.461495 lr 0.00050672 rank 5
2023-02-22 23:41:12,709 DEBUG TRAIN Batch 11/5600 loss 16.268208 loss_att 18.369299 loss_ctc 20.881273 loss_rnnt 14.958781 hw_loss 0.514000 lr 0.00050672 rank 6
2023-02-22 23:41:12,737 DEBUG TRAIN Batch 11/5600 loss 31.378874 loss_att 34.746723 loss_ctc 37.470261 loss_rnnt 29.661961 hw_loss 0.433421 lr 0.00050672 rank 0
2023-02-22 23:42:28,782 DEBUG TRAIN Batch 11/5700 loss 19.146729 loss_att 18.693733 loss_ctc 23.049835 loss_rnnt 18.431486 hw_loss 0.535177 lr 0.00050646 rank 7
2023-02-22 23:42:28,788 DEBUG TRAIN Batch 11/5700 loss 24.573233 loss_att 25.588242 loss_ctc 24.585718 loss_rnnt 24.142294 hw_loss 0.424262 lr 0.00050646 rank 3
2023-02-22 23:42:28,789 DEBUG TRAIN Batch 11/5700 loss 15.967075 loss_att 19.265692 loss_ctc 20.908766 loss_rnnt 14.385908 hw_loss 0.492287 lr 0.00050646 rank 4
2023-02-22 23:42:28,789 DEBUG TRAIN Batch 11/5700 loss 20.289162 loss_att 19.748125 loss_ctc 23.693163 loss_rnnt 19.631922 hw_loss 0.584210 lr 0.00050646 rank 5
2023-02-22 23:42:28,789 DEBUG TRAIN Batch 11/5700 loss 17.864645 loss_att 23.243738 loss_ctc 19.861155 loss_rnnt 16.326290 hw_loss 0.368126 lr 0.00050646 rank 1
2023-02-22 23:42:28,790 DEBUG TRAIN Batch 11/5700 loss 15.800456 loss_att 20.209442 loss_ctc 14.908286 loss_rnnt 14.843398 hw_loss 0.364159 lr 0.00050646 rank 0
2023-02-22 23:42:28,825 DEBUG TRAIN Batch 11/5700 loss 16.400305 loss_att 18.169230 loss_ctc 17.665976 loss_rnnt 15.607459 hw_loss 0.506823 lr 0.00050646 rank 2
2023-02-22 23:42:28,870 DEBUG TRAIN Batch 11/5700 loss 31.431286 loss_att 39.631027 loss_ctc 31.423742 loss_rnnt 29.563734 hw_loss 0.428644 lr 0.00050646 rank 6
2023-02-22 23:43:42,291 DEBUG TRAIN Batch 11/5800 loss 13.608431 loss_att 15.834505 loss_ctc 16.540041 loss_rnnt 12.480577 hw_loss 0.547042 lr 0.00050620 rank 1
2023-02-22 23:43:42,292 DEBUG TRAIN Batch 11/5800 loss 15.567322 loss_att 17.062990 loss_ctc 17.859339 loss_rnnt 14.644806 hw_loss 0.595838 lr 0.00050620 rank 3
2023-02-22 23:43:42,293 DEBUG TRAIN Batch 11/5800 loss 20.642792 loss_att 25.654144 loss_ctc 21.255049 loss_rnnt 19.376759 hw_loss 0.341491 lr 0.00050620 rank 7
2023-02-22 23:43:42,295 DEBUG TRAIN Batch 11/5800 loss 39.548016 loss_att 39.515327 loss_ctc 35.044304 loss_rnnt 39.946678 hw_loss 0.390689 lr 0.00050620 rank 6
2023-02-22 23:43:42,296 DEBUG TRAIN Batch 11/5800 loss 15.287563 loss_att 21.091856 loss_ctc 12.682669 loss_rnnt 14.273970 hw_loss 0.375104 lr 0.00050620 rank 4
2023-02-22 23:43:42,300 DEBUG TRAIN Batch 11/5800 loss 23.213011 loss_att 31.882160 loss_ctc 20.865181 loss_rnnt 21.604397 hw_loss 0.352181 lr 0.00050620 rank 5
2023-02-22 23:43:42,304 DEBUG TRAIN Batch 11/5800 loss 35.325329 loss_att 39.107967 loss_ctc 44.235413 loss_rnnt 33.201290 hw_loss 0.336560 lr 0.00050620 rank 2
2023-02-22 23:43:42,305 DEBUG TRAIN Batch 11/5800 loss 15.430783 loss_att 24.051426 loss_ctc 18.361229 loss_rnnt 13.106441 hw_loss 0.392793 lr 0.00050620 rank 0
2023-02-22 23:44:55,973 DEBUG TRAIN Batch 11/5900 loss 17.665857 loss_att 21.588009 loss_ctc 23.547628 loss_rnnt 15.864992 hw_loss 0.435371 lr 0.00050594 rank 3
2023-02-22 23:44:55,974 DEBUG TRAIN Batch 11/5900 loss 23.422813 loss_att 28.333454 loss_ctc 26.506168 loss_rnnt 21.783583 hw_loss 0.461224 lr 0.00050594 rank 0
2023-02-22 23:44:55,974 DEBUG TRAIN Batch 11/5900 loss 27.773926 loss_att 30.754993 loss_ctc 29.432053 loss_rnnt 26.771528 hw_loss 0.347068 lr 0.00050594 rank 7
2023-02-22 23:44:55,977 DEBUG TRAIN Batch 11/5900 loss 13.127656 loss_att 19.625126 loss_ctc 16.101244 loss_rnnt 11.224878 hw_loss 0.387757 lr 0.00050594 rank 1
2023-02-22 23:44:55,978 DEBUG TRAIN Batch 11/5900 loss 25.162727 loss_att 27.188755 loss_ctc 24.127468 loss_rnnt 24.646055 hw_loss 0.467816 lr 0.00050594 rank 6
2023-02-22 23:44:55,978 DEBUG TRAIN Batch 11/5900 loss 16.972435 loss_att 27.943127 loss_ctc 20.967915 loss_rnnt 14.073271 hw_loss 0.323055 lr 0.00050594 rank 4
2023-02-22 23:44:55,979 DEBUG TRAIN Batch 11/5900 loss 15.023681 loss_att 23.097134 loss_ctc 15.083812 loss_rnnt 13.166197 hw_loss 0.440207 lr 0.00050594 rank 5
2023-02-22 23:44:55,989 DEBUG TRAIN Batch 11/5900 loss 8.935525 loss_att 12.459546 loss_ctc 10.264682 loss_rnnt 7.837343 hw_loss 0.405296 lr 0.00050594 rank 2
2023-02-22 23:46:10,575 DEBUG TRAIN Batch 11/6000 loss 8.663572 loss_att 12.482376 loss_ctc 10.295246 loss_rnnt 7.462562 hw_loss 0.411922 lr 0.00050568 rank 7
2023-02-22 23:46:10,577 DEBUG TRAIN Batch 11/6000 loss 10.945318 loss_att 17.316738 loss_ctc 12.284887 loss_rnnt 9.281537 hw_loss 0.395413 lr 0.00050568 rank 3
2023-02-22 23:46:10,579 DEBUG TRAIN Batch 11/6000 loss 27.093508 loss_att 33.544338 loss_ctc 33.915531 loss_rnnt 24.680965 hw_loss 0.398951 lr 0.00050568 rank 4
2023-02-22 23:46:10,581 DEBUG TRAIN Batch 11/6000 loss 19.787119 loss_att 21.718044 loss_ctc 24.261549 loss_rnnt 18.591274 hw_loss 0.399502 lr 0.00050568 rank 0
2023-02-22 23:46:10,582 DEBUG TRAIN Batch 11/6000 loss 14.355059 loss_att 18.057728 loss_ctc 16.332661 loss_rnnt 13.113653 hw_loss 0.444734 lr 0.00050568 rank 6
2023-02-22 23:46:10,582 DEBUG TRAIN Batch 11/6000 loss 13.178888 loss_att 19.316530 loss_ctc 14.008247 loss_rnnt 11.661774 hw_loss 0.335636 lr 0.00050568 rank 5
2023-02-22 23:46:10,584 DEBUG TRAIN Batch 11/6000 loss 19.005640 loss_att 26.298693 loss_ctc 24.896627 loss_rnnt 16.505678 hw_loss 0.479782 lr 0.00050568 rank 2
2023-02-22 23:46:10,603 DEBUG TRAIN Batch 11/6000 loss 17.029308 loss_att 23.626440 loss_ctc 21.511475 loss_rnnt 14.887126 hw_loss 0.422124 lr 0.00050568 rank 1
2023-02-22 23:47:25,006 DEBUG TRAIN Batch 11/6100 loss 18.702875 loss_att 22.434942 loss_ctc 26.167095 loss_rnnt 16.790955 hw_loss 0.319273 lr 0.00050542 rank 3
2023-02-22 23:47:25,008 DEBUG TRAIN Batch 11/6100 loss 21.875292 loss_att 22.918755 loss_ctc 24.817520 loss_rnnt 21.088017 hw_loss 0.349286 lr 0.00050542 rank 7
2023-02-22 23:47:25,010 DEBUG TRAIN Batch 11/6100 loss 20.236069 loss_att 27.170746 loss_ctc 26.580212 loss_rnnt 17.819172 hw_loss 0.345145 lr 0.00050542 rank 1
2023-02-22 23:47:25,017 DEBUG TRAIN Batch 11/6100 loss 21.549149 loss_att 27.783775 loss_ctc 23.489414 loss_rnnt 19.832367 hw_loss 0.395912 lr 0.00050542 rank 6
2023-02-22 23:47:25,018 DEBUG TRAIN Batch 11/6100 loss 17.935266 loss_att 27.391438 loss_ctc 23.953827 loss_rnnt 15.071458 hw_loss 0.318932 lr 0.00050542 rank 0
2023-02-22 23:47:25,019 DEBUG TRAIN Batch 11/6100 loss 18.006550 loss_att 18.859932 loss_ctc 24.351097 loss_rnnt 16.798717 hw_loss 0.358527 lr 0.00050542 rank 5
2023-02-22 23:47:25,048 DEBUG TRAIN Batch 11/6100 loss 22.055214 loss_att 31.059336 loss_ctc 25.199860 loss_rnnt 19.656572 hw_loss 0.334743 lr 0.00050542 rank 4
2023-02-22 23:47:25,095 DEBUG TRAIN Batch 11/6100 loss 11.899497 loss_att 17.073635 loss_ctc 13.615046 loss_rnnt 10.438836 hw_loss 0.369550 lr 0.00050542 rank 2
2023-02-22 23:48:38,664 DEBUG TRAIN Batch 11/6200 loss 12.859065 loss_att 16.988689 loss_ctc 13.734545 loss_rnnt 11.642860 hw_loss 0.512905 lr 0.00050517 rank 7
2023-02-22 23:48:38,669 DEBUG TRAIN Batch 11/6200 loss 16.914507 loss_att 19.792379 loss_ctc 19.507608 loss_rnnt 15.765490 hw_loss 0.426928 lr 0.00050517 rank 3
2023-02-22 23:48:38,672 DEBUG TRAIN Batch 11/6200 loss 11.685885 loss_att 16.750549 loss_ctc 13.033741 loss_rnnt 10.224315 hw_loss 0.504232 lr 0.00050517 rank 0
2023-02-22 23:48:38,675 DEBUG TRAIN Batch 11/6200 loss 15.396439 loss_att 17.626919 loss_ctc 17.804436 loss_rnnt 14.422072 hw_loss 0.388507 lr 0.00050517 rank 6
2023-02-22 23:48:38,676 DEBUG TRAIN Batch 11/6200 loss 26.330900 loss_att 33.030518 loss_ctc 33.943413 loss_rnnt 23.780384 hw_loss 0.366731 lr 0.00050517 rank 4
2023-02-22 23:48:38,679 DEBUG TRAIN Batch 11/6200 loss 9.766466 loss_att 16.883942 loss_ctc 10.537134 loss_rnnt 7.984010 hw_loss 0.480386 lr 0.00050517 rank 2
2023-02-22 23:48:38,680 DEBUG TRAIN Batch 11/6200 loss 21.421719 loss_att 25.233105 loss_ctc 22.797907 loss_rnnt 20.222820 hw_loss 0.474619 lr 0.00050517 rank 5
2023-02-22 23:48:38,720 DEBUG TRAIN Batch 11/6200 loss 19.838799 loss_att 26.638985 loss_ctc 22.243650 loss_rnnt 17.913017 hw_loss 0.459558 lr 0.00050517 rank 1
2023-02-22 23:49:52,759 DEBUG TRAIN Batch 11/6300 loss 15.585968 loss_att 18.036324 loss_ctc 18.896942 loss_rnnt 14.405952 hw_loss 0.465903 lr 0.00050491 rank 1
2023-02-22 23:49:52,762 DEBUG TRAIN Batch 11/6300 loss 23.670425 loss_att 25.708435 loss_ctc 29.549749 loss_rnnt 22.194117 hw_loss 0.533990 lr 0.00050491 rank 0
2023-02-22 23:49:52,763 DEBUG TRAIN Batch 11/6300 loss 20.953053 loss_att 20.448320 loss_ctc 25.978941 loss_rnnt 20.072517 hw_loss 0.583806 lr 0.00050491 rank 7
2023-02-22 23:49:52,764 DEBUG TRAIN Batch 11/6300 loss 30.042665 loss_att 32.096733 loss_ctc 36.107140 loss_rnnt 28.514669 hw_loss 0.578598 lr 0.00050491 rank 6
2023-02-22 23:49:52,764 DEBUG TRAIN Batch 11/6300 loss 16.340376 loss_att 23.460609 loss_ctc 18.439333 loss_rnnt 14.395119 hw_loss 0.452531 lr 0.00050491 rank 3
2023-02-22 23:49:52,765 DEBUG TRAIN Batch 11/6300 loss 14.998804 loss_att 18.239288 loss_ctc 15.908356 loss_rnnt 13.982504 hw_loss 0.462993 lr 0.00050491 rank 4
2023-02-22 23:49:52,768 DEBUG TRAIN Batch 11/6300 loss 18.126371 loss_att 22.240538 loss_ctc 24.105906 loss_rnnt 16.260750 hw_loss 0.460346 lr 0.00050491 rank 2
2023-02-22 23:49:52,815 DEBUG TRAIN Batch 11/6300 loss 16.861080 loss_att 24.062618 loss_ctc 23.935591 loss_rnnt 14.191230 hw_loss 0.536766 lr 0.00050491 rank 5
2023-02-22 23:51:09,223 DEBUG TRAIN Batch 11/6400 loss 16.196283 loss_att 17.435440 loss_ctc 19.239290 loss_rnnt 15.198812 hw_loss 0.644821 lr 0.00050465 rank 1
2023-02-22 23:51:09,224 DEBUG TRAIN Batch 11/6400 loss 10.842392 loss_att 12.654841 loss_ctc 14.903824 loss_rnnt 9.721135 hw_loss 0.407330 lr 0.00050465 rank 3
2023-02-22 23:51:09,227 DEBUG TRAIN Batch 11/6400 loss 14.957552 loss_att 17.160488 loss_ctc 17.639309 loss_rnnt 13.823342 hw_loss 0.630103 lr 0.00050465 rank 0
2023-02-22 23:51:09,227 DEBUG TRAIN Batch 11/6400 loss 18.107531 loss_att 21.452938 loss_ctc 20.742239 loss_rnnt 16.895473 hw_loss 0.359401 lr 0.00050465 rank 6
2023-02-22 23:51:09,229 DEBUG TRAIN Batch 11/6400 loss 16.648335 loss_att 24.746292 loss_ctc 19.553917 loss_rnnt 14.464183 hw_loss 0.332154 lr 0.00050465 rank 4
2023-02-22 23:51:09,231 DEBUG TRAIN Batch 11/6400 loss 10.815951 loss_att 21.217663 loss_ctc 16.194065 loss_rnnt 7.842529 hw_loss 0.329996 lr 0.00050465 rank 2
2023-02-22 23:51:09,237 DEBUG TRAIN Batch 11/6400 loss 15.499637 loss_att 19.052044 loss_ctc 12.788218 loss_rnnt 14.962481 hw_loss 0.352868 lr 0.00050465 rank 7
2023-02-22 23:51:09,253 DEBUG TRAIN Batch 11/6400 loss 24.310379 loss_att 28.796732 loss_ctc 27.556950 loss_rnnt 22.773132 hw_loss 0.388312 lr 0.00050465 rank 5
2023-02-22 23:52:22,286 DEBUG TRAIN Batch 11/6500 loss 21.017925 loss_att 25.867260 loss_ctc 30.678440 loss_rnnt 18.561459 hw_loss 0.372249 lr 0.00050439 rank 3
2023-02-22 23:52:22,287 DEBUG TRAIN Batch 11/6500 loss 17.613783 loss_att 24.345486 loss_ctc 25.618279 loss_rnnt 14.960840 hw_loss 0.448756 lr 0.00050439 rank 7
2023-02-22 23:52:22,291 DEBUG TRAIN Batch 11/6500 loss 16.733986 loss_att 24.445717 loss_ctc 16.703136 loss_rnnt 14.968432 hw_loss 0.426227 lr 0.00050439 rank 4
2023-02-22 23:52:22,293 DEBUG TRAIN Batch 11/6500 loss 27.376858 loss_att 34.086311 loss_ctc 24.873938 loss_rnnt 26.145910 hw_loss 0.417709 lr 0.00050439 rank 1
2023-02-22 23:52:22,294 DEBUG TRAIN Batch 11/6500 loss 25.092834 loss_att 32.904274 loss_ctc 26.652050 loss_rnnt 23.102278 hw_loss 0.413200 lr 0.00050439 rank 6
2023-02-22 23:52:22,297 DEBUG TRAIN Batch 11/6500 loss 11.770417 loss_att 16.267353 loss_ctc 17.369297 loss_rnnt 9.933132 hw_loss 0.358837 lr 0.00050439 rank 2
2023-02-22 23:52:22,305 DEBUG TRAIN Batch 11/6500 loss 18.840748 loss_att 20.797592 loss_ctc 19.953352 loss_rnnt 18.110155 hw_loss 0.357889 lr 0.00050439 rank 5
2023-02-22 23:52:22,313 DEBUG TRAIN Batch 11/6500 loss 30.750860 loss_att 32.250854 loss_ctc 31.364567 loss_rnnt 30.149303 hw_loss 0.411993 lr 0.00050439 rank 0
2023-02-22 23:53:36,112 DEBUG TRAIN Batch 11/6600 loss 10.095851 loss_att 16.352684 loss_ctc 11.942588 loss_rnnt 8.364268 hw_loss 0.438721 lr 0.00050414 rank 7
2023-02-22 23:53:36,114 DEBUG TRAIN Batch 11/6600 loss 6.338659 loss_att 10.484488 loss_ctc 5.227853 loss_rnnt 5.467355 hw_loss 0.356709 lr 0.00050414 rank 0
2023-02-22 23:53:36,117 DEBUG TRAIN Batch 11/6600 loss 8.087048 loss_att 13.521713 loss_ctc 6.166012 loss_rnnt 7.046705 hw_loss 0.392901 lr 0.00050414 rank 4
2023-02-22 23:53:36,118 DEBUG TRAIN Batch 11/6600 loss 18.760155 loss_att 24.371475 loss_ctc 21.222372 loss_rnnt 17.070913 hw_loss 0.447533 lr 0.00050414 rank 1
2023-02-22 23:53:36,119 DEBUG TRAIN Batch 11/6600 loss 23.284784 loss_att 27.220901 loss_ctc 27.298986 loss_rnnt 21.731709 hw_loss 0.432421 lr 0.00050414 rank 3
2023-02-22 23:53:36,120 DEBUG TRAIN Batch 11/6600 loss 35.465382 loss_att 47.737759 loss_ctc 39.412849 loss_rnnt 32.237984 hw_loss 0.462363 lr 0.00050414 rank 5
2023-02-22 23:53:36,138 DEBUG TRAIN Batch 11/6600 loss 18.461294 loss_att 20.471752 loss_ctc 21.460417 loss_rnnt 17.473381 hw_loss 0.348636 lr 0.00050414 rank 6
2023-02-22 23:53:36,152 DEBUG TRAIN Batch 11/6600 loss 20.058920 loss_att 26.642853 loss_ctc 22.305798 loss_rnnt 18.224487 hw_loss 0.408867 lr 0.00050414 rank 2
2023-02-22 23:54:50,970 DEBUG TRAIN Batch 11/6700 loss 10.196939 loss_att 20.488581 loss_ctc 10.132656 loss_rnnt 7.926438 hw_loss 0.413895 lr 0.00050388 rank 3
2023-02-22 23:54:50,971 DEBUG TRAIN Batch 11/6700 loss 16.910667 loss_att 25.412241 loss_ctc 24.563696 loss_rnnt 13.953360 hw_loss 0.443606 lr 0.00050388 rank 7
2023-02-22 23:54:50,971 DEBUG TRAIN Batch 11/6700 loss 16.612162 loss_att 23.828705 loss_ctc 19.683926 loss_rnnt 14.532387 hw_loss 0.425435 lr 0.00050388 rank 0
2023-02-22 23:54:50,972 DEBUG TRAIN Batch 11/6700 loss 8.328055 loss_att 12.394039 loss_ctc 8.733013 loss_rnnt 7.301029 hw_loss 0.299689 lr 0.00050388 rank 6
2023-02-22 23:54:50,974 DEBUG TRAIN Batch 11/6700 loss 38.028095 loss_att 43.991379 loss_ctc 46.578266 loss_rnnt 35.471291 hw_loss 0.420236 lr 0.00050388 rank 1
2023-02-22 23:54:50,974 DEBUG TRAIN Batch 11/6700 loss 27.282946 loss_att 26.150986 loss_ctc 28.309958 loss_rnnt 27.207708 hw_loss 0.308806 lr 0.00050388 rank 4
2023-02-22 23:54:50,975 DEBUG TRAIN Batch 11/6700 loss 17.387463 loss_att 22.892906 loss_ctc 23.862209 loss_rnnt 15.194357 hw_loss 0.428848 lr 0.00050388 rank 2
2023-02-22 23:54:50,975 DEBUG TRAIN Batch 11/6700 loss 13.263157 loss_att 25.701241 loss_ctc 14.279929 loss_rnnt 10.451631 hw_loss 0.353135 lr 0.00050388 rank 5
2023-02-22 23:56:05,467 DEBUG TRAIN Batch 11/6800 loss 10.191829 loss_att 18.226437 loss_ctc 8.982367 loss_rnnt 8.515655 hw_loss 0.432214 lr 0.00050363 rank 7
2023-02-22 23:56:05,469 DEBUG TRAIN Batch 11/6800 loss 22.002300 loss_att 27.955601 loss_ctc 30.054413 loss_rnnt 19.524456 hw_loss 0.400444 lr 0.00050363 rank 3
2023-02-22 23:56:05,474 DEBUG TRAIN Batch 11/6800 loss 19.878242 loss_att 29.066111 loss_ctc 24.802788 loss_rnnt 17.152361 hw_loss 0.434436 lr 0.00050363 rank 6
2023-02-22 23:56:05,477 DEBUG TRAIN Batch 11/6800 loss 26.657637 loss_att 31.579077 loss_ctc 30.543198 loss_rnnt 24.911205 hw_loss 0.457630 lr 0.00050363 rank 1
2023-02-22 23:56:05,479 DEBUG TRAIN Batch 11/6800 loss 19.850908 loss_att 22.760509 loss_ctc 19.592545 loss_rnnt 19.088486 hw_loss 0.403033 lr 0.00050363 rank 5
2023-02-22 23:56:05,480 DEBUG TRAIN Batch 11/6800 loss 16.798813 loss_att 22.736082 loss_ctc 19.047491 loss_rnnt 15.033928 hw_loss 0.520518 lr 0.00050363 rank 0
2023-02-22 23:56:05,485 DEBUG TRAIN Batch 11/6800 loss 25.261415 loss_att 30.800797 loss_ctc 27.795496 loss_rnnt 23.637817 hw_loss 0.333462 lr 0.00050363 rank 2
2023-02-22 23:56:05,518 DEBUG TRAIN Batch 11/6800 loss 20.390316 loss_att 30.266361 loss_ctc 24.524027 loss_rnnt 17.623039 hw_loss 0.451700 lr 0.00050363 rank 4
2023-02-22 23:57:19,231 DEBUG TRAIN Batch 11/6900 loss 28.685627 loss_att 34.706463 loss_ctc 33.924484 loss_rnnt 26.550978 hw_loss 0.434944 lr 0.00050337 rank 1
2023-02-22 23:57:19,232 DEBUG TRAIN Batch 11/6900 loss 28.275616 loss_att 30.682827 loss_ctc 30.417133 loss_rnnt 27.293421 hw_loss 0.403528 lr 0.00050337 rank 4
2023-02-22 23:57:19,233 DEBUG TRAIN Batch 11/6900 loss 23.221844 loss_att 27.060795 loss_ctc 27.165714 loss_rnnt 21.756182 hw_loss 0.322543 lr 0.00050337 rank 3
2023-02-22 23:57:19,233 DEBUG TRAIN Batch 11/6900 loss 15.872013 loss_att 19.181103 loss_ctc 17.041321 loss_rnnt 14.831485 hw_loss 0.417754 lr 0.00050337 rank 6
2023-02-22 23:57:19,235 DEBUG TRAIN Batch 11/6900 loss 22.451971 loss_att 26.002426 loss_ctc 29.551937 loss_rnnt 20.571404 hw_loss 0.419652 lr 0.00050337 rank 0
2023-02-22 23:57:19,238 DEBUG TRAIN Batch 11/6900 loss 11.788472 loss_att 14.324951 loss_ctc 13.358417 loss_rnnt 10.850110 hw_loss 0.415761 lr 0.00050337 rank 7
2023-02-22 23:57:19,239 DEBUG TRAIN Batch 11/6900 loss 20.454700 loss_att 19.791332 loss_ctc 24.504541 loss_rnnt 19.811176 hw_loss 0.442910 lr 0.00050337 rank 2
2023-02-22 23:57:19,285 DEBUG TRAIN Batch 11/6900 loss 15.860480 loss_att 16.069006 loss_ctc 16.648706 loss_rnnt 15.486232 hw_loss 0.426464 lr 0.00050337 rank 5
2023-02-22 23:58:33,891 DEBUG TRAIN Batch 11/7000 loss 13.369701 loss_att 14.703045 loss_ctc 12.657641 loss_rnnt 12.919238 hw_loss 0.522631 lr 0.00050312 rank 4
2023-02-22 23:58:33,895 DEBUG TRAIN Batch 11/7000 loss 14.541251 loss_att 17.956261 loss_ctc 18.650549 loss_rnnt 13.122461 hw_loss 0.352276 lr 0.00050312 rank 0
2023-02-22 23:58:33,896 DEBUG TRAIN Batch 11/7000 loss 11.892240 loss_att 15.843681 loss_ctc 12.846460 loss_rnnt 10.691564 hw_loss 0.530920 lr 0.00050312 rank 1
2023-02-22 23:58:33,898 DEBUG TRAIN Batch 11/7000 loss 5.519044 loss_att 7.722386 loss_ctc 5.914885 loss_rnnt 4.761408 hw_loss 0.495354 lr 0.00050312 rank 7
2023-02-22 23:58:33,899 DEBUG TRAIN Batch 11/7000 loss 14.525887 loss_att 15.296970 loss_ctc 15.210618 loss_rnnt 13.940331 hw_loss 0.637578 lr 0.00050312 rank 5
2023-02-22 23:58:33,902 DEBUG TRAIN Batch 11/7000 loss 21.292467 loss_att 19.604553 loss_ctc 26.082634 loss_rnnt 20.586369 hw_loss 0.759360 lr 0.00050312 rank 6
2023-02-22 23:58:33,905 DEBUG TRAIN Batch 11/7000 loss 11.684448 loss_att 14.913132 loss_ctc 14.494557 loss_rnnt 10.405065 hw_loss 0.485561 lr 0.00050312 rank 3
2023-02-22 23:58:33,911 DEBUG TRAIN Batch 11/7000 loss 22.589699 loss_att 37.453320 loss_ctc 32.678154 loss_rnnt 18.070387 hw_loss 0.377741 lr 0.00050312 rank 2
2023-02-22 23:59:49,478 DEBUG TRAIN Batch 11/7100 loss 14.204942 loss_att 22.882500 loss_ctc 15.590391 loss_rnnt 12.054411 hw_loss 0.431801 lr 0.00050286 rank 7
2023-02-22 23:59:49,487 DEBUG TRAIN Batch 11/7100 loss 11.481935 loss_att 18.220613 loss_ctc 13.182264 loss_rnnt 9.682140 hw_loss 0.422525 lr 0.00050286 rank 2
2023-02-22 23:59:49,488 DEBUG TRAIN Batch 11/7100 loss 23.019985 loss_att 36.969398 loss_ctc 23.980114 loss_rnnt 19.904827 hw_loss 0.369858 lr 0.00050286 rank 5
2023-02-22 23:59:49,488 DEBUG TRAIN Batch 11/7100 loss 18.812365 loss_att 19.734970 loss_ctc 20.872242 loss_rnnt 18.023634 hw_loss 0.617922 lr 0.00050286 rank 1
2023-02-22 23:59:49,490 DEBUG TRAIN Batch 11/7100 loss 27.451048 loss_att 37.764084 loss_ctc 37.928860 loss_rnnt 23.742102 hw_loss 0.467428 lr 0.00050286 rank 3
2023-02-22 23:59:49,490 DEBUG TRAIN Batch 11/7100 loss 17.725204 loss_att 20.172369 loss_ctc 21.712914 loss_rnnt 16.449810 hw_loss 0.476751 lr 0.00050286 rank 6
2023-02-22 23:59:49,515 DEBUG TRAIN Batch 11/7100 loss 18.357029 loss_att 30.099373 loss_ctc 26.003033 loss_rnnt 14.759315 hw_loss 0.430832 lr 0.00050286 rank 4
2023-02-22 23:59:49,541 DEBUG TRAIN Batch 11/7100 loss 11.010687 loss_att 11.616506 loss_ctc 13.545102 loss_rnnt 10.142400 hw_loss 0.767252 lr 0.00050286 rank 0
2023-02-23 00:01:03,269 DEBUG TRAIN Batch 11/7200 loss 5.501830 loss_att 13.174288 loss_ctc 4.145482 loss_rnnt 3.953011 hw_loss 0.365949 lr 0.00050261 rank 3
2023-02-23 00:01:03,270 DEBUG TRAIN Batch 11/7200 loss 26.173964 loss_att 30.333290 loss_ctc 31.241377 loss_rnnt 24.464649 hw_loss 0.378363 lr 0.00050261 rank 4
2023-02-23 00:01:03,271 DEBUG TRAIN Batch 11/7200 loss 15.581109 loss_att 18.182503 loss_ctc 18.170099 loss_rnnt 14.476340 hw_loss 0.448672 lr 0.00050261 rank 6
2023-02-23 00:01:03,272 DEBUG TRAIN Batch 11/7200 loss 24.280565 loss_att 31.913689 loss_ctc 27.788305 loss_rnnt 22.080786 hw_loss 0.385228 lr 0.00050261 rank 0
2023-02-23 00:01:03,273 DEBUG TRAIN Batch 11/7200 loss 32.984531 loss_att 39.914341 loss_ctc 35.342850 loss_rnnt 31.083019 hw_loss 0.377084 lr 0.00050261 rank 5
2023-02-23 00:01:03,275 DEBUG TRAIN Batch 11/7200 loss 21.254595 loss_att 30.898237 loss_ctc 26.220879 loss_rnnt 18.493620 hw_loss 0.318892 lr 0.00050261 rank 7
2023-02-23 00:01:03,280 DEBUG TRAIN Batch 11/7200 loss 20.458813 loss_att 23.902205 loss_ctc 25.052868 loss_rnnt 18.972357 hw_loss 0.347320 lr 0.00050261 rank 1
2023-02-23 00:01:03,283 DEBUG TRAIN Batch 11/7200 loss 10.007971 loss_att 14.180366 loss_ctc 13.976493 loss_rnnt 8.452838 hw_loss 0.359097 lr 0.00050261 rank 2
2023-02-23 00:02:16,650 DEBUG TRAIN Batch 11/7300 loss 18.523537 loss_att 24.262583 loss_ctc 19.747194 loss_rnnt 17.009081 hw_loss 0.381548 lr 0.00050235 rank 7
2023-02-23 00:02:16,656 DEBUG TRAIN Batch 11/7300 loss 24.733658 loss_att 24.896461 loss_ctc 26.852173 loss_rnnt 24.212547 hw_loss 0.386402 lr 0.00050235 rank 1
2023-02-23 00:02:16,657 DEBUG TRAIN Batch 11/7300 loss 22.792582 loss_att 30.831999 loss_ctc 24.082489 loss_rnnt 20.804188 hw_loss 0.390978 lr 0.00050235 rank 3
2023-02-23 00:02:16,662 DEBUG TRAIN Batch 11/7300 loss 14.083632 loss_att 21.079453 loss_ctc 15.795824 loss_rnnt 12.213374 hw_loss 0.455252 lr 0.00050235 rank 5
2023-02-23 00:02:16,662 DEBUG TRAIN Batch 11/7300 loss 18.354706 loss_att 26.496349 loss_ctc 19.743610 loss_rnnt 16.311077 hw_loss 0.431460 lr 0.00050235 rank 0
2023-02-23 00:02:16,664 DEBUG TRAIN Batch 11/7300 loss 9.071060 loss_att 16.339176 loss_ctc 10.688562 loss_rnnt 7.195848 hw_loss 0.386104 lr 0.00050235 rank 2
2023-02-23 00:02:16,666 DEBUG TRAIN Batch 11/7300 loss 25.138887 loss_att 25.934153 loss_ctc 28.691532 loss_rnnt 24.311014 hw_loss 0.365874 lr 0.00050235 rank 4
2023-02-23 00:02:16,668 DEBUG TRAIN Batch 11/7300 loss 23.186420 loss_att 31.074512 loss_ctc 33.049099 loss_rnnt 20.068220 hw_loss 0.422923 lr 0.00050235 rank 6
2023-02-23 00:03:31,024 DEBUG TRAIN Batch 11/7400 loss 13.965113 loss_att 26.867626 loss_ctc 18.630966 loss_rnnt 10.584794 hw_loss 0.333190 lr 0.00050210 rank 3
2023-02-23 00:03:31,025 DEBUG TRAIN Batch 11/7400 loss 27.182632 loss_att 28.618015 loss_ctc 31.038662 loss_rnnt 26.156691 hw_loss 0.421363 lr 0.00050210 rank 7
2023-02-23 00:03:31,030 DEBUG TRAIN Batch 11/7400 loss 8.385757 loss_att 13.597536 loss_ctc 8.364863 loss_rnnt 7.132343 hw_loss 0.400959 lr 0.00050210 rank 2
2023-02-23 00:03:31,033 DEBUG TRAIN Batch 11/7400 loss 12.387239 loss_att 21.499279 loss_ctc 19.398623 loss_rnnt 9.374705 hw_loss 0.478643 lr 0.00050210 rank 0
2023-02-23 00:03:31,058 DEBUG TRAIN Batch 11/7400 loss 14.669662 loss_att 18.183722 loss_ctc 17.501522 loss_rnnt 13.349341 hw_loss 0.449862 lr 0.00050210 rank 1
2023-02-23 00:03:31,061 DEBUG TRAIN Batch 11/7400 loss 20.325348 loss_att 24.771641 loss_ctc 22.233505 loss_rnnt 18.952114 hw_loss 0.430417 lr 0.00050210 rank 5
2023-02-23 00:03:31,062 DEBUG TRAIN Batch 11/7400 loss 19.361195 loss_att 20.530788 loss_ctc 21.376810 loss_rnnt 18.612808 hw_loss 0.460726 lr 0.00050210 rank 6
2023-02-23 00:03:31,076 DEBUG TRAIN Batch 11/7400 loss 10.362542 loss_att 13.299500 loss_ctc 10.698064 loss_rnnt 9.539659 hw_loss 0.357667 lr 0.00050210 rank 4
2023-02-23 00:04:46,234 DEBUG TRAIN Batch 11/7500 loss 22.570328 loss_att 26.554844 loss_ctc 27.414858 loss_rnnt 20.945930 hw_loss 0.340418 lr 0.00050185 rank 3
2023-02-23 00:04:46,236 DEBUG TRAIN Batch 11/7500 loss 17.195595 loss_att 21.787563 loss_ctc 22.008272 loss_rnnt 15.419797 hw_loss 0.404464 lr 0.00050185 rank 7
2023-02-23 00:04:46,238 DEBUG TRAIN Batch 11/7500 loss 17.140179 loss_att 26.401598 loss_ctc 17.569906 loss_rnnt 15.034421 hw_loss 0.367832 lr 0.00050185 rank 1
2023-02-23 00:04:46,238 DEBUG TRAIN Batch 11/7500 loss 30.328386 loss_att 32.839111 loss_ctc 37.540207 loss_rnnt 28.600521 hw_loss 0.495274 lr 0.00050185 rank 2
2023-02-23 00:04:46,240 DEBUG TRAIN Batch 11/7500 loss 26.152262 loss_att 28.345732 loss_ctc 34.326118 loss_rnnt 24.407635 hw_loss 0.405163 lr 0.00050185 rank 6
2023-02-23 00:04:46,239 DEBUG TRAIN Batch 11/7500 loss 30.796963 loss_att 34.127563 loss_ctc 41.843765 loss_rnnt 28.474304 hw_loss 0.344311 lr 0.00050185 rank 0
2023-02-23 00:04:46,245 DEBUG TRAIN Batch 11/7500 loss 22.605194 loss_att 23.873837 loss_ctc 21.680330 loss_rnnt 22.277849 hw_loss 0.369245 lr 0.00050185 rank 5
2023-02-23 00:04:46,246 DEBUG TRAIN Batch 11/7500 loss 18.125360 loss_att 22.869976 loss_ctc 17.278963 loss_rnnt 17.098957 hw_loss 0.356876 lr 0.00050185 rank 4
2023-02-23 00:06:01,479 DEBUG TRAIN Batch 11/7600 loss 12.524484 loss_att 15.743084 loss_ctc 15.751345 loss_rnnt 11.186676 hw_loss 0.494700 lr 0.00050160 rank 6
2023-02-23 00:06:01,479 DEBUG TRAIN Batch 11/7600 loss 22.535053 loss_att 28.319159 loss_ctc 24.518433 loss_rnnt 20.878160 hw_loss 0.441787 lr 0.00050160 rank 1
2023-02-23 00:06:01,481 DEBUG TRAIN Batch 11/7600 loss 16.658039 loss_att 20.754391 loss_ctc 19.211643 loss_rnnt 15.264496 hw_loss 0.438363 lr 0.00050160 rank 3
2023-02-23 00:06:01,483 DEBUG TRAIN Batch 11/7600 loss 15.572012 loss_att 14.118073 loss_ctc 19.288754 loss_rnnt 15.050486 hw_loss 0.593902 lr 0.00050160 rank 7
2023-02-23 00:06:01,484 DEBUG TRAIN Batch 11/7600 loss 17.596619 loss_att 19.903065 loss_ctc 18.097239 loss_rnnt 16.840553 hw_loss 0.427550 lr 0.00050160 rank 5
2023-02-23 00:06:01,486 DEBUG TRAIN Batch 11/7600 loss 20.327623 loss_att 21.021210 loss_ctc 22.756855 loss_rnnt 19.634068 hw_loss 0.433015 lr 0.00050160 rank 4
2023-02-23 00:06:01,488 DEBUG TRAIN Batch 11/7600 loss 15.055249 loss_att 19.612539 loss_ctc 18.142387 loss_rnnt 13.482480 hw_loss 0.468173 lr 0.00050160 rank 0
2023-02-23 00:06:01,491 DEBUG TRAIN Batch 11/7600 loss 10.555761 loss_att 11.458815 loss_ctc 12.146080 loss_rnnt 9.799089 hw_loss 0.682535 lr 0.00050160 rank 2
2023-02-23 00:07:15,499 DEBUG TRAIN Batch 11/7700 loss 14.196260 loss_att 14.853780 loss_ctc 15.037132 loss_rnnt 13.555164 hw_loss 0.745266 lr 0.00050134 rank 3
2023-02-23 00:07:15,511 DEBUG TRAIN Batch 11/7700 loss 22.069035 loss_att 24.759216 loss_ctc 27.755234 loss_rnnt 20.574383 hw_loss 0.372103 lr 0.00050134 rank 7
2023-02-23 00:07:15,517 DEBUG TRAIN Batch 11/7700 loss 14.588558 loss_att 23.082071 loss_ctc 20.656166 loss_rnnt 11.823780 hw_loss 0.481988 lr 0.00050134 rank 6
2023-02-23 00:07:15,519 DEBUG TRAIN Batch 11/7700 loss 19.223801 loss_att 21.275742 loss_ctc 22.873749 loss_rnnt 18.019192 hw_loss 0.576678 lr 0.00050134 rank 0
2023-02-23 00:07:15,519 DEBUG TRAIN Batch 11/7700 loss 12.806712 loss_att 16.395775 loss_ctc 14.204290 loss_rnnt 11.681356 hw_loss 0.414751 lr 0.00050134 rank 4
2023-02-23 00:07:15,520 DEBUG TRAIN Batch 11/7700 loss 19.744093 loss_att 20.208935 loss_ctc 22.883856 loss_rnnt 18.973515 hw_loss 0.485576 lr 0.00050134 rank 1
2023-02-23 00:07:15,522 DEBUG TRAIN Batch 11/7700 loss 6.797775 loss_att 15.532339 loss_ctc 7.455792 loss_rnnt 4.777683 hw_loss 0.347706 lr 0.00050134 rank 5
2023-02-23 00:07:15,522 DEBUG TRAIN Batch 11/7700 loss 6.126810 loss_att 11.595901 loss_ctc 6.367311 loss_rnnt 4.780219 hw_loss 0.413824 lr 0.00050134 rank 2
2023-02-23 00:08:31,463 DEBUG TRAIN Batch 11/7800 loss 27.779779 loss_att 30.601801 loss_ctc 33.253853 loss_rnnt 26.290617 hw_loss 0.365401 lr 0.00050109 rank 1
2023-02-23 00:08:31,479 DEBUG TRAIN Batch 11/7800 loss 11.498356 loss_att 16.103733 loss_ctc 13.028454 loss_rnnt 10.174902 hw_loss 0.371936 lr 0.00050109 rank 3
2023-02-23 00:08:31,480 DEBUG TRAIN Batch 11/7800 loss 11.183326 loss_att 17.557436 loss_ctc 14.538017 loss_rnnt 9.282414 hw_loss 0.335242 lr 0.00050109 rank 7
2023-02-23 00:08:31,480 DEBUG TRAIN Batch 11/7800 loss 13.872743 loss_att 18.905584 loss_ctc 17.777702 loss_rnnt 12.105919 hw_loss 0.449240 lr 0.00050109 rank 4
2023-02-23 00:08:31,483 DEBUG TRAIN Batch 11/7800 loss 39.382904 loss_att 51.985058 loss_ctc 43.819519 loss_rnnt 36.025909 hw_loss 0.459405 lr 0.00050109 rank 6
2023-02-23 00:08:31,488 DEBUG TRAIN Batch 11/7800 loss 12.156637 loss_att 18.593395 loss_ctc 15.051227 loss_rnnt 10.263503 hw_loss 0.412196 lr 0.00050109 rank 2
2023-02-23 00:08:31,497 DEBUG TRAIN Batch 11/7800 loss 15.719357 loss_att 23.972130 loss_ctc 23.681458 loss_rnnt 12.821611 hw_loss 0.347955 lr 0.00050109 rank 5
2023-02-23 00:08:31,508 DEBUG TRAIN Batch 11/7800 loss 20.585835 loss_att 31.036495 loss_ctc 25.908209 loss_rnnt 17.578802 hw_loss 0.388596 lr 0.00050109 rank 0
2023-02-23 00:09:45,538 DEBUG TRAIN Batch 11/7900 loss 21.265753 loss_att 26.141115 loss_ctc 25.084793 loss_rnnt 19.559874 hw_loss 0.415500 lr 0.00050084 rank 7
2023-02-23 00:09:45,541 DEBUG TRAIN Batch 11/7900 loss 27.713858 loss_att 31.087082 loss_ctc 28.641050 loss_rnnt 26.638573 hw_loss 0.519397 lr 0.00050084 rank 0
2023-02-23 00:09:45,540 DEBUG TRAIN Batch 11/7900 loss 27.940220 loss_att 32.061092 loss_ctc 36.385815 loss_rnnt 25.768646 hw_loss 0.414970 lr 0.00050084 rank 6
2023-02-23 00:09:45,542 DEBUG TRAIN Batch 11/7900 loss 18.031710 loss_att 20.709583 loss_ctc 25.040670 loss_rnnt 16.313530 hw_loss 0.465142 lr 0.00050084 rank 1
2023-02-23 00:09:45,543 DEBUG TRAIN Batch 11/7900 loss 4.129858 loss_att 7.997898 loss_ctc 4.404095 loss_rnnt 3.101926 hw_loss 0.408298 lr 0.00050084 rank 4
2023-02-23 00:09:45,543 DEBUG TRAIN Batch 11/7900 loss 8.142522 loss_att 11.103257 loss_ctc 8.860271 loss_rnnt 7.236916 hw_loss 0.408298 lr 0.00050084 rank 3
2023-02-23 00:09:45,545 DEBUG TRAIN Batch 11/7900 loss 22.017834 loss_att 35.848907 loss_ctc 24.129379 loss_rnnt 18.740765 hw_loss 0.429964 lr 0.00050084 rank 5
2023-02-23 00:09:45,551 DEBUG TRAIN Batch 11/7900 loss 15.650509 loss_att 19.475307 loss_ctc 17.324001 loss_rnnt 14.431826 hw_loss 0.432358 lr 0.00050084 rank 2
2023-02-23 00:10:58,077 DEBUG TRAIN Batch 11/8000 loss 10.222864 loss_att 20.722080 loss_ctc 12.990351 loss_rnnt 7.554183 hw_loss 0.374698 lr 0.00050059 rank 1
2023-02-23 00:10:58,077 DEBUG TRAIN Batch 11/8000 loss 16.060354 loss_att 19.677790 loss_ctc 21.015289 loss_rnnt 14.437490 hw_loss 0.447596 lr 0.00050059 rank 7
2023-02-23 00:10:58,079 DEBUG TRAIN Batch 11/8000 loss 22.997480 loss_att 23.464762 loss_ctc 29.643738 loss_rnnt 21.778385 hw_loss 0.449006 lr 0.00050059 rank 6
2023-02-23 00:10:58,082 DEBUG TRAIN Batch 11/8000 loss 28.212927 loss_att 31.316935 loss_ctc 29.477436 loss_rnnt 27.196068 hw_loss 0.426476 lr 0.00050059 rank 3
2023-02-23 00:10:58,082 DEBUG TRAIN Batch 11/8000 loss 21.183870 loss_att 28.174807 loss_ctc 26.970558 loss_rnnt 18.826721 hw_loss 0.351382 lr 0.00050059 rank 4
2023-02-23 00:10:58,083 DEBUG TRAIN Batch 11/8000 loss 9.527866 loss_att 13.453849 loss_ctc 11.943508 loss_rnnt 8.200958 hw_loss 0.411798 lr 0.00050059 rank 5
2023-02-23 00:10:58,088 DEBUG TRAIN Batch 11/8000 loss 22.754120 loss_att 27.360390 loss_ctc 23.772362 loss_rnnt 21.491028 hw_loss 0.386384 lr 0.00050059 rank 0
2023-02-23 00:10:58,139 DEBUG TRAIN Batch 11/8000 loss 15.117244 loss_att 19.048529 loss_ctc 18.253311 loss_rnnt 13.697906 hw_loss 0.403010 lr 0.00050059 rank 2
2023-02-23 00:12:12,138 DEBUG TRAIN Batch 11/8100 loss 13.367078 loss_att 18.284647 loss_ctc 18.819038 loss_rnnt 11.432890 hw_loss 0.419524 lr 0.00050034 rank 0
2023-02-23 00:12:12,150 DEBUG TRAIN Batch 11/8100 loss 19.655928 loss_att 20.213081 loss_ctc 21.315861 loss_rnnt 19.078367 hw_loss 0.459010 lr 0.00050034 rank 1
2023-02-23 00:12:12,153 DEBUG TRAIN Batch 11/8100 loss 14.150901 loss_att 16.310015 loss_ctc 14.429261 loss_rnnt 13.419535 hw_loss 0.492050 lr 0.00050034 rank 7
2023-02-23 00:12:12,157 DEBUG TRAIN Batch 11/8100 loss 12.870452 loss_att 21.140711 loss_ctc 17.372982 loss_rnnt 10.413601 hw_loss 0.379617 lr 0.00050034 rank 2
2023-02-23 00:12:12,158 DEBUG TRAIN Batch 11/8100 loss 23.103910 loss_att 28.221085 loss_ctc 25.439072 loss_rnnt 21.584980 hw_loss 0.345262 lr 0.00050034 rank 3
2023-02-23 00:12:12,159 DEBUG TRAIN Batch 11/8100 loss 20.242674 loss_att 22.772999 loss_ctc 22.786955 loss_rnnt 19.183018 hw_loss 0.401910 lr 0.00050034 rank 5
2023-02-23 00:12:12,159 DEBUG TRAIN Batch 11/8100 loss 12.944366 loss_att 19.228302 loss_ctc 16.889790 loss_rnnt 10.918150 hw_loss 0.456320 lr 0.00050034 rank 6
2023-02-23 00:12:12,214 DEBUG TRAIN Batch 11/8100 loss 13.678951 loss_att 18.009670 loss_ctc 15.267109 loss_rnnt 12.405097 hw_loss 0.367420 lr 0.00050034 rank 4
2023-02-23 00:13:26,130 DEBUG TRAIN Batch 11/8200 loss 13.822472 loss_att 15.147146 loss_ctc 16.427408 loss_rnnt 12.949681 hw_loss 0.488493 lr 0.00050009 rank 7
2023-02-23 00:13:26,132 DEBUG TRAIN Batch 11/8200 loss 13.495153 loss_att 16.435837 loss_ctc 14.058692 loss_rnnt 12.591080 hw_loss 0.451495 lr 0.00050009 rank 3
2023-02-23 00:13:26,134 DEBUG TRAIN Batch 11/8200 loss 10.577288 loss_att 12.512918 loss_ctc 12.472157 loss_rnnt 9.725898 hw_loss 0.396778 lr 0.00050009 rank 2
2023-02-23 00:13:26,136 DEBUG TRAIN Batch 11/8200 loss 22.107122 loss_att 24.837280 loss_ctc 24.876558 loss_rnnt 20.964735 hw_loss 0.425810 lr 0.00050009 rank 5
2023-02-23 00:13:26,136 DEBUG TRAIN Batch 11/8200 loss 34.309021 loss_att 38.632023 loss_ctc 43.209438 loss_rnnt 31.989948 hw_loss 0.502030 lr 0.00050009 rank 1
2023-02-23 00:13:26,136 DEBUG TRAIN Batch 11/8200 loss 35.458992 loss_att 44.043980 loss_ctc 40.745651 loss_rnnt 32.855286 hw_loss 0.340906 lr 0.00050009 rank 0
2023-02-23 00:13:26,136 DEBUG TRAIN Batch 11/8200 loss 19.882477 loss_att 21.275213 loss_ctc 27.900949 loss_rnnt 18.267805 hw_loss 0.500619 lr 0.00050009 rank 6
2023-02-23 00:13:26,182 DEBUG TRAIN Batch 11/8200 loss 11.233559 loss_att 14.326266 loss_ctc 13.001200 loss_rnnt 10.147210 hw_loss 0.435227 lr 0.00050009 rank 4
2023-02-23 00:14:38,616 DEBUG TRAIN Batch 11/8300 loss 17.378942 loss_att 21.515276 loss_ctc 21.948353 loss_rnnt 15.766983 hw_loss 0.328946 lr 0.00049984 rank 3
2023-02-23 00:14:38,618 DEBUG TRAIN Batch 11/8300 loss 8.586800 loss_att 17.431379 loss_ctc 10.855579 loss_rnnt 6.297496 hw_loss 0.408533 lr 0.00049984 rank 4
2023-02-23 00:14:38,620 DEBUG TRAIN Batch 11/8300 loss 9.133424 loss_att 14.782848 loss_ctc 13.311268 loss_rnnt 7.197400 hw_loss 0.467050 lr 0.00049984 rank 1
2023-02-23 00:14:38,621 DEBUG TRAIN Batch 11/8300 loss 12.776511 loss_att 18.747236 loss_ctc 10.714157 loss_rnnt 11.672857 hw_loss 0.345918 lr 0.00049984 rank 7
2023-02-23 00:14:38,621 DEBUG TRAIN Batch 11/8300 loss 13.584711 loss_att 19.430325 loss_ctc 12.898795 loss_rnnt 12.335840 hw_loss 0.321005 lr 0.00049984 rank 0
2023-02-23 00:14:38,627 DEBUG TRAIN Batch 11/8300 loss 9.265609 loss_att 9.304042 loss_ctc 11.186736 loss_rnnt 8.618187 hw_loss 0.719221 lr 0.00049984 rank 6
2023-02-23 00:14:38,627 DEBUG TRAIN Batch 11/8300 loss 15.117745 loss_att 20.908566 loss_ctc 16.003357 loss_rnnt 13.655142 hw_loss 0.349420 lr 0.00049984 rank 5
2023-02-23 00:14:38,628 DEBUG TRAIN Batch 11/8300 loss 22.262018 loss_att 26.120274 loss_ctc 23.289371 loss_rnnt 21.158537 hw_loss 0.365343 lr 0.00049984 rank 2
2023-02-23 00:15:21,333 DEBUG CV Batch 11/0 loss 3.111210 loss_att 3.271327 loss_ctc 3.416749 loss_rnnt 2.547509 hw_loss 0.920510 history loss 2.995980 rank 7
2023-02-23 00:15:21,346 DEBUG CV Batch 11/0 loss 3.111210 loss_att 3.271327 loss_ctc 3.416749 loss_rnnt 2.547509 hw_loss 0.920510 history loss 2.995980 rank 1
2023-02-23 00:15:21,349 DEBUG CV Batch 11/0 loss 3.111210 loss_att 3.271327 loss_ctc 3.416749 loss_rnnt 2.547509 hw_loss 0.920510 history loss 2.995980 rank 0
2023-02-23 00:15:21,350 DEBUG CV Batch 11/0 loss 3.111210 loss_att 3.271327 loss_ctc 3.416749 loss_rnnt 2.547509 hw_loss 0.920510 history loss 2.995980 rank 6
2023-02-23 00:15:21,353 DEBUG CV Batch 11/0 loss 3.111210 loss_att 3.271327 loss_ctc 3.416749 loss_rnnt 2.547509 hw_loss 0.920510 history loss 2.995980 rank 2
2023-02-23 00:15:21,357 DEBUG CV Batch 11/0 loss 3.111210 loss_att 3.271327 loss_ctc 3.416749 loss_rnnt 2.547509 hw_loss 0.920510 history loss 2.995980 rank 3
2023-02-23 00:15:21,358 DEBUG CV Batch 11/0 loss 3.111209 loss_att 3.271327 loss_ctc 3.416749 loss_rnnt 2.547509 hw_loss 0.920510 history loss 2.995979 rank 5
2023-02-23 00:15:21,369 DEBUG CV Batch 11/0 loss 3.111210 loss_att 3.271327 loss_ctc 3.416749 loss_rnnt 2.547509 hw_loss 0.920510 history loss 2.995980 rank 4
2023-02-23 00:15:32,716 DEBUG CV Batch 11/100 loss 11.245663 loss_att 12.796744 loss_ctc 13.002081 loss_rnnt 10.421064 hw_loss 0.525361 history loss 5.512508 rank 7
2023-02-23 00:15:32,838 DEBUG CV Batch 11/100 loss 11.245663 loss_att 12.796744 loss_ctc 13.002081 loss_rnnt 10.421064 hw_loss 0.525361 history loss 5.512508 rank 3
2023-02-23 00:15:33,004 DEBUG CV Batch 11/100 loss 11.245663 loss_att 12.796744 loss_ctc 13.002081 loss_rnnt 10.421064 hw_loss 0.525361 history loss 5.512508 rank 0
2023-02-23 00:15:33,027 DEBUG CV Batch 11/100 loss 11.245663 loss_att 12.796744 loss_ctc 13.002081 loss_rnnt 10.421064 hw_loss 0.525361 history loss 5.512508 rank 6
2023-02-23 00:15:33,085 DEBUG CV Batch 11/100 loss 11.245663 loss_att 12.796744 loss_ctc 13.002081 loss_rnnt 10.421064 hw_loss 0.525361 history loss 5.512508 rank 4
2023-02-23 00:15:33,326 DEBUG CV Batch 11/100 loss 11.245663 loss_att 12.796744 loss_ctc 13.002081 loss_rnnt 10.421064 hw_loss 0.525361 history loss 5.512508 rank 5
2023-02-23 00:15:33,335 DEBUG CV Batch 11/100 loss 11.245663 loss_att 12.796744 loss_ctc 13.002081 loss_rnnt 10.421064 hw_loss 0.525361 history loss 5.512508 rank 1
2023-02-23 00:15:33,396 DEBUG CV Batch 11/100 loss 11.245663 loss_att 12.796744 loss_ctc 13.002081 loss_rnnt 10.421064 hw_loss 0.525361 history loss 5.512508 rank 2
2023-02-23 00:15:46,244 DEBUG CV Batch 11/200 loss 13.501499 loss_att 24.738155 loss_ctc 13.902453 loss_rnnt 11.027313 hw_loss 0.325112 history loss 6.336530 rank 7
2023-02-23 00:15:46,422 DEBUG CV Batch 11/200 loss 13.501499 loss_att 24.738155 loss_ctc 13.902453 loss_rnnt 11.027313 hw_loss 0.325112 history loss 6.336530 rank 3
2023-02-23 00:15:46,822 DEBUG CV Batch 11/200 loss 13.501499 loss_att 24.738155 loss_ctc 13.902453 loss_rnnt 11.027313 hw_loss 0.325112 history loss 6.336530 rank 0
2023-02-23 00:15:47,283 DEBUG CV Batch 11/200 loss 13.501499 loss_att 24.738155 loss_ctc 13.902453 loss_rnnt 11.027313 hw_loss 0.325112 history loss 6.336530 rank 4
2023-02-23 00:15:47,312 DEBUG CV Batch 11/200 loss 13.501499 loss_att 24.738155 loss_ctc 13.902453 loss_rnnt 11.027313 hw_loss 0.325112 history loss 6.336530 rank 1
2023-02-23 00:15:47,394 DEBUG CV Batch 11/200 loss 13.501499 loss_att 24.738155 loss_ctc 13.902453 loss_rnnt 11.027313 hw_loss 0.325112 history loss 6.336530 rank 2
2023-02-23 00:15:47,412 DEBUG CV Batch 11/200 loss 13.501499 loss_att 24.738155 loss_ctc 13.902453 loss_rnnt 11.027313 hw_loss 0.325112 history loss 6.336530 rank 5
2023-02-23 00:15:47,706 DEBUG CV Batch 11/200 loss 13.501499 loss_att 24.738155 loss_ctc 13.902453 loss_rnnt 11.027313 hw_loss 0.325112 history loss 6.336530 rank 6
2023-02-23 00:15:58,645 DEBUG CV Batch 11/300 loss 9.410401 loss_att 9.081548 loss_ctc 11.099447 loss_rnnt 9.022286 hw_loss 0.428774 history loss 6.510469 rank 7
2023-02-23 00:15:58,942 DEBUG CV Batch 11/300 loss 9.410401 loss_att 9.081548 loss_ctc 11.099447 loss_rnnt 9.022286 hw_loss 0.428774 history loss 6.510469 rank 3
2023-02-23 00:15:59,465 DEBUG CV Batch 11/300 loss 9.410401 loss_att 9.081548 loss_ctc 11.099447 loss_rnnt 9.022286 hw_loss 0.428774 history loss 6.510469 rank 0
2023-02-23 00:15:59,711 DEBUG CV Batch 11/300 loss 9.410401 loss_att 9.081548 loss_ctc 11.099447 loss_rnnt 9.022286 hw_loss 0.428774 history loss 6.510469 rank 5
2023-02-23 00:15:59,834 DEBUG CV Batch 11/300 loss 9.410401 loss_att 9.081548 loss_ctc 11.099447 loss_rnnt 9.022286 hw_loss 0.428774 history loss 6.510469 rank 1
2023-02-23 00:16:00,076 DEBUG CV Batch 11/300 loss 9.410401 loss_att 9.081548 loss_ctc 11.099447 loss_rnnt 9.022286 hw_loss 0.428774 history loss 6.510469 rank 4
2023-02-23 00:16:00,305 DEBUG CV Batch 11/300 loss 9.410401 loss_att 9.081548 loss_ctc 11.099447 loss_rnnt 9.022286 hw_loss 0.428774 history loss 6.510469 rank 2
2023-02-23 00:16:00,529 DEBUG CV Batch 11/300 loss 9.410401 loss_att 9.081548 loss_ctc 11.099447 loss_rnnt 9.022286 hw_loss 0.428774 history loss 6.510469 rank 6
2023-02-23 00:16:10,788 DEBUG CV Batch 11/400 loss 30.935635 loss_att 119.403915 loss_ctc 19.079514 loss_rnnt 14.673993 hw_loss 0.279001 history loss 7.775061 rank 7
2023-02-23 00:16:11,244 DEBUG CV Batch 11/400 loss 30.935635 loss_att 119.403915 loss_ctc 19.079514 loss_rnnt 14.673993 hw_loss 0.279001 history loss 7.775061 rank 3
2023-02-23 00:16:11,978 DEBUG CV Batch 11/400 loss 30.935635 loss_att 119.403915 loss_ctc 19.079514 loss_rnnt 14.673993 hw_loss 0.279001 history loss 7.775061 rank 5
2023-02-23 00:16:12,247 DEBUG CV Batch 11/400 loss 30.935635 loss_att 119.403915 loss_ctc 19.079514 loss_rnnt 14.673993 hw_loss 0.279001 history loss 7.775061 rank 0
2023-02-23 00:16:12,346 DEBUG CV Batch 11/400 loss 30.935635 loss_att 119.403915 loss_ctc 19.079514 loss_rnnt 14.673993 hw_loss 0.279001 history loss 7.775061 rank 4
2023-02-23 00:16:12,494 DEBUG CV Batch 11/400 loss 30.935635 loss_att 119.403915 loss_ctc 19.079514 loss_rnnt 14.673993 hw_loss 0.279001 history loss 7.775061 rank 1
2023-02-23 00:16:12,835 DEBUG CV Batch 11/400 loss 30.935635 loss_att 119.403915 loss_ctc 19.079514 loss_rnnt 14.673993 hw_loss 0.279001 history loss 7.775061 rank 6
2023-02-23 00:16:13,235 DEBUG CV Batch 11/400 loss 30.935635 loss_att 119.403915 loss_ctc 19.079514 loss_rnnt 14.673993 hw_loss 0.279001 history loss 7.775061 rank 2
2023-02-23 00:16:21,308 DEBUG CV Batch 11/500 loss 7.920422 loss_att 8.634485 loss_ctc 8.230029 loss_rnnt 7.504397 hw_loss 0.434872 history loss 8.896759 rank 7
2023-02-23 00:16:21,947 DEBUG CV Batch 11/500 loss 7.920422 loss_att 8.634485 loss_ctc 8.230029 loss_rnnt 7.504397 hw_loss 0.434872 history loss 8.896759 rank 3
2023-02-23 00:16:22,675 DEBUG CV Batch 11/500 loss 7.920422 loss_att 8.634485 loss_ctc 8.230029 loss_rnnt 7.504397 hw_loss 0.434872 history loss 8.896759 rank 5
2023-02-23 00:16:22,895 DEBUG CV Batch 11/500 loss 7.920422 loss_att 8.634485 loss_ctc 8.230029 loss_rnnt 7.504397 hw_loss 0.434872 history loss 8.896759 rank 4
2023-02-23 00:16:23,205 DEBUG CV Batch 11/500 loss 7.920422 loss_att 8.634485 loss_ctc 8.230029 loss_rnnt 7.504397 hw_loss 0.434872 history loss 8.896759 rank 0
2023-02-23 00:16:23,494 DEBUG CV Batch 11/500 loss 7.920422 loss_att 8.634485 loss_ctc 8.230029 loss_rnnt 7.504397 hw_loss 0.434872 history loss 8.896759 rank 1
2023-02-23 00:16:23,557 DEBUG CV Batch 11/500 loss 7.920422 loss_att 8.634485 loss_ctc 8.230029 loss_rnnt 7.504397 hw_loss 0.434872 history loss 8.896759 rank 6
2023-02-23 00:16:24,625 DEBUG CV Batch 11/500 loss 7.920422 loss_att 8.634485 loss_ctc 8.230029 loss_rnnt 7.504397 hw_loss 0.434872 history loss 8.896759 rank 2
2023-02-23 00:16:33,522 DEBUG CV Batch 11/600 loss 10.043309 loss_att 9.150011 loss_ctc 11.900890 loss_rnnt 9.551245 hw_loss 0.793214 history loss 10.054748 rank 7
2023-02-23 00:16:34,145 DEBUG CV Batch 11/600 loss 10.043309 loss_att 9.150011 loss_ctc 11.900890 loss_rnnt 9.551245 hw_loss 0.793214 history loss 10.054748 rank 3
2023-02-23 00:16:34,992 DEBUG CV Batch 11/600 loss 10.043309 loss_att 9.150011 loss_ctc 11.900890 loss_rnnt 9.551245 hw_loss 0.793214 history loss 10.054748 rank 5
2023-02-23 00:16:35,132 DEBUG CV Batch 11/600 loss 10.043309 loss_att 9.150011 loss_ctc 11.900890 loss_rnnt 9.551245 hw_loss 0.793214 history loss 10.054748 rank 4
2023-02-23 00:16:35,917 DEBUG CV Batch 11/600 loss 10.043309 loss_att 9.150011 loss_ctc 11.900890 loss_rnnt 9.551245 hw_loss 0.793214 history loss 10.054748 rank 6
2023-02-23 00:16:36,083 DEBUG CV Batch 11/600 loss 10.043309 loss_att 9.150011 loss_ctc 11.900890 loss_rnnt 9.551245 hw_loss 0.793214 history loss 10.054748 rank 0
2023-02-23 00:16:36,116 DEBUG CV Batch 11/600 loss 10.043309 loss_att 9.150011 loss_ctc 11.900890 loss_rnnt 9.551245 hw_loss 0.793214 history loss 10.054748 rank 1
2023-02-23 00:16:37,397 DEBUG CV Batch 11/600 loss 10.043309 loss_att 9.150011 loss_ctc 11.900890 loss_rnnt 9.551245 hw_loss 0.793214 history loss 10.054748 rank 2
2023-02-23 00:16:45,073 DEBUG CV Batch 11/700 loss 36.009319 loss_att 118.960297 loss_ctc 21.882044 loss_rnnt 21.152874 hw_loss 0.281038 history loss 10.928424 rank 7
2023-02-23 00:16:45,679 DEBUG CV Batch 11/700 loss 36.009319 loss_att 118.960297 loss_ctc 21.882044 loss_rnnt 21.152874 hw_loss 0.281038 history loss 10.928424 rank 3
2023-02-23 00:16:46,713 DEBUG CV Batch 11/700 loss 36.009319 loss_att 118.960297 loss_ctc 21.882044 loss_rnnt 21.152874 hw_loss 0.281038 history loss 10.928424 rank 5
2023-02-23 00:16:46,896 DEBUG CV Batch 11/700 loss 36.009319 loss_att 118.960297 loss_ctc 21.882044 loss_rnnt 21.152874 hw_loss 0.281038 history loss 10.928424 rank 4
2023-02-23 00:16:47,343 DEBUG CV Batch 11/700 loss 36.009319 loss_att 118.960297 loss_ctc 21.882044 loss_rnnt 21.152874 hw_loss 0.281038 history loss 10.928424 rank 6
2023-02-23 00:16:48,085 DEBUG CV Batch 11/700 loss 36.009319 loss_att 118.960297 loss_ctc 21.882044 loss_rnnt 21.152874 hw_loss 0.281038 history loss 10.928424 rank 1
2023-02-23 00:16:48,160 DEBUG CV Batch 11/700 loss 36.009319 loss_att 118.960297 loss_ctc 21.882044 loss_rnnt 21.152874 hw_loss 0.281038 history loss 10.928424 rank 0
2023-02-23 00:16:49,415 DEBUG CV Batch 11/700 loss 36.009319 loss_att 118.960297 loss_ctc 21.882044 loss_rnnt 21.152874 hw_loss 0.281038 history loss 10.928424 rank 2
2023-02-23 00:16:56,256 DEBUG CV Batch 11/800 loss 17.063313 loss_att 15.666170 loss_ctc 19.873566 loss_rnnt 16.659056 hw_loss 0.579346 history loss 10.199033 rank 7
2023-02-23 00:16:56,939 DEBUG CV Batch 11/800 loss 17.063313 loss_att 15.666170 loss_ctc 19.873566 loss_rnnt 16.659056 hw_loss 0.579346 history loss 10.199033 rank 3
2023-02-23 00:16:58,778 DEBUG CV Batch 11/800 loss 17.063313 loss_att 15.666170 loss_ctc 19.873566 loss_rnnt 16.659056 hw_loss 0.579346 history loss 10.199033 rank 4
2023-02-23 00:16:58,931 DEBUG CV Batch 11/800 loss 17.063313 loss_att 15.666170 loss_ctc 19.873566 loss_rnnt 16.659056 hw_loss 0.579346 history loss 10.199033 rank 6
2023-02-23 00:16:59,322 DEBUG CV Batch 11/800 loss 17.063313 loss_att 15.666170 loss_ctc 19.873566 loss_rnnt 16.659056 hw_loss 0.579346 history loss 10.199033 rank 5
2023-02-23 00:16:59,814 DEBUG CV Batch 11/800 loss 17.063313 loss_att 15.666170 loss_ctc 19.873566 loss_rnnt 16.659056 hw_loss 0.579346 history loss 10.199033 rank 0
2023-02-23 00:17:00,760 DEBUG CV Batch 11/800 loss 17.063313 loss_att 15.666170 loss_ctc 19.873566 loss_rnnt 16.659056 hw_loss 0.579346 history loss 10.199033 rank 1
2023-02-23 00:17:01,677 DEBUG CV Batch 11/800 loss 17.063313 loss_att 15.666170 loss_ctc 19.873566 loss_rnnt 16.659056 hw_loss 0.579346 history loss 10.199033 rank 2
2023-02-23 00:17:09,655 DEBUG CV Batch 11/900 loss 19.869049 loss_att 32.161819 loss_ctc 23.222097 loss_rnnt 16.722162 hw_loss 0.452358 history loss 9.940397 rank 7
2023-02-23 00:17:10,499 DEBUG CV Batch 11/900 loss 19.869049 loss_att 32.161819 loss_ctc 23.222097 loss_rnnt 16.722162 hw_loss 0.452358 history loss 9.940397 rank 3
2023-02-23 00:17:12,818 DEBUG CV Batch 11/900 loss 19.869049 loss_att 32.161819 loss_ctc 23.222097 loss_rnnt 16.722162 hw_loss 0.452358 history loss 9.940397 rank 4
2023-02-23 00:17:12,887 DEBUG CV Batch 11/900 loss 19.869049 loss_att 32.161819 loss_ctc 23.222097 loss_rnnt 16.722162 hw_loss 0.452358 history loss 9.940397 rank 6
2023-02-23 00:17:13,416 DEBUG CV Batch 11/900 loss 19.869049 loss_att 32.161819 loss_ctc 23.222097 loss_rnnt 16.722162 hw_loss 0.452358 history loss 9.940397 rank 5
2023-02-23 00:17:13,942 DEBUG CV Batch 11/900 loss 19.869049 loss_att 32.161819 loss_ctc 23.222097 loss_rnnt 16.722162 hw_loss 0.452358 history loss 9.940397 rank 0
2023-02-23 00:17:14,902 DEBUG CV Batch 11/900 loss 19.869049 loss_att 32.161819 loss_ctc 23.222097 loss_rnnt 16.722162 hw_loss 0.452358 history loss 9.940397 rank 1
2023-02-23 00:17:15,829 DEBUG CV Batch 11/900 loss 19.869049 loss_att 32.161819 loss_ctc 23.222097 loss_rnnt 16.722162 hw_loss 0.452358 history loss 9.940397 rank 2
2023-02-23 00:17:22,058 DEBUG CV Batch 11/1000 loss 6.762084 loss_att 7.540218 loss_ctc 6.153178 loss_rnnt 6.331593 hw_loss 0.667597 history loss 9.630701 rank 7
2023-02-23 00:17:23,139 DEBUG CV Batch 11/1000 loss 6.762084 loss_att 7.540218 loss_ctc 6.153178 loss_rnnt 6.331593 hw_loss 0.667597 history loss 9.630701 rank 3
2023-02-23 00:17:25,273 DEBUG CV Batch 11/1000 loss 6.762084 loss_att 7.540218 loss_ctc 6.153178 loss_rnnt 6.331593 hw_loss 0.667597 history loss 9.630701 rank 6
2023-02-23 00:17:25,455 DEBUG CV Batch 11/1000 loss 6.762084 loss_att 7.540218 loss_ctc 6.153178 loss_rnnt 6.331593 hw_loss 0.667597 history loss 9.630701 rank 4
2023-02-23 00:17:25,720 DEBUG CV Batch 11/1000 loss 6.762084 loss_att 7.540218 loss_ctc 6.153178 loss_rnnt 6.331593 hw_loss 0.667597 history loss 9.630701 rank 5
2023-02-23 00:17:27,238 DEBUG CV Batch 11/1000 loss 6.762084 loss_att 7.540218 loss_ctc 6.153178 loss_rnnt 6.331593 hw_loss 0.667597 history loss 9.630701 rank 0
2023-02-23 00:17:27,305 DEBUG CV Batch 11/1000 loss 6.762084 loss_att 7.540218 loss_ctc 6.153178 loss_rnnt 6.331593 hw_loss 0.667597 history loss 9.630701 rank 1
2023-02-23 00:17:29,212 DEBUG CV Batch 11/1000 loss 6.762084 loss_att 7.540218 loss_ctc 6.153178 loss_rnnt 6.331593 hw_loss 0.667597 history loss 9.630701 rank 2
2023-02-23 00:17:34,136 DEBUG CV Batch 11/1100 loss 8.991394 loss_att 7.288295 loss_ctc 10.206742 loss_rnnt 8.726538 hw_loss 0.831431 history loss 9.603380 rank 7
2023-02-23 00:17:35,225 DEBUG CV Batch 11/1100 loss 8.991394 loss_att 7.288295 loss_ctc 10.206742 loss_rnnt 8.726538 hw_loss 0.831431 history loss 9.603380 rank 3
2023-02-23 00:17:37,593 DEBUG CV Batch 11/1100 loss 8.991394 loss_att 7.288295 loss_ctc 10.206742 loss_rnnt 8.726538 hw_loss 0.831431 history loss 9.603380 rank 4
2023-02-23 00:17:37,925 DEBUG CV Batch 11/1100 loss 8.991394 loss_att 7.288295 loss_ctc 10.206742 loss_rnnt 8.726538 hw_loss 0.831431 history loss 9.603380 rank 5
2023-02-23 00:17:38,038 DEBUG CV Batch 11/1100 loss 8.991394 loss_att 7.288295 loss_ctc 10.206742 loss_rnnt 8.726538 hw_loss 0.831431 history loss 9.603380 rank 6
2023-02-23 00:17:39,406 DEBUG CV Batch 11/1100 loss 8.991394 loss_att 7.288295 loss_ctc 10.206742 loss_rnnt 8.726538 hw_loss 0.831431 history loss 9.603380 rank 1
2023-02-23 00:17:39,957 DEBUG CV Batch 11/1100 loss 8.991394 loss_att 7.288295 loss_ctc 10.206742 loss_rnnt 8.726538 hw_loss 0.831431 history loss 9.603380 rank 0
2023-02-23 00:17:41,842 DEBUG CV Batch 11/1100 loss 8.991394 loss_att 7.288295 loss_ctc 10.206742 loss_rnnt 8.726538 hw_loss 0.831431 history loss 9.603380 rank 2
2023-02-23 00:17:45,043 DEBUG CV Batch 11/1200 loss 9.363350 loss_att 11.366534 loss_ctc 9.514335 loss_rnnt 8.724143 hw_loss 0.409573 history loss 10.075584 rank 7
2023-02-23 00:17:46,128 DEBUG CV Batch 11/1200 loss 9.363350 loss_att 11.366534 loss_ctc 9.514335 loss_rnnt 8.724143 hw_loss 0.409573 history loss 10.075584 rank 3
2023-02-23 00:17:48,145 DEBUG CV Batch 11/1200 loss 9.363350 loss_att 11.366534 loss_ctc 9.514335 loss_rnnt 8.724143 hw_loss 0.409573 history loss 10.075584 rank 4
2023-02-23 00:17:48,527 DEBUG CV Batch 11/1200 loss 9.363350 loss_att 11.366534 loss_ctc 9.514335 loss_rnnt 8.724143 hw_loss 0.409573 history loss 10.075584 rank 6
2023-02-23 00:17:48,698 DEBUG CV Batch 11/1200 loss 9.363350 loss_att 11.366534 loss_ctc 9.514335 loss_rnnt 8.724143 hw_loss 0.409573 history loss 10.075584 rank 5
2023-02-23 00:17:50,272 DEBUG CV Batch 11/1200 loss 9.363350 loss_att 11.366534 loss_ctc 9.514335 loss_rnnt 8.724143 hw_loss 0.409573 history loss 10.075584 rank 1
2023-02-23 00:17:51,473 DEBUG CV Batch 11/1200 loss 9.363350 loss_att 11.366534 loss_ctc 9.514335 loss_rnnt 8.724143 hw_loss 0.409573 history loss 10.075584 rank 0
2023-02-23 00:17:53,210 DEBUG CV Batch 11/1200 loss 9.363350 loss_att 11.366534 loss_ctc 9.514335 loss_rnnt 8.724143 hw_loss 0.409573 history loss 10.075584 rank 2
2023-02-23 00:17:57,175 DEBUG CV Batch 11/1300 loss 9.116866 loss_att 7.437313 loss_ctc 10.022091 loss_rnnt 9.016088 hw_loss 0.592485 history loss 10.450040 rank 7
2023-02-23 00:17:58,329 DEBUG CV Batch 11/1300 loss 9.116866 loss_att 7.437313 loss_ctc 10.022091 loss_rnnt 9.016088 hw_loss 0.592485 history loss 10.450040 rank 3
2023-02-23 00:18:00,538 DEBUG CV Batch 11/1300 loss 9.116866 loss_att 7.437313 loss_ctc 10.022091 loss_rnnt 9.016088 hw_loss 0.592485 history loss 10.450040 rank 4
2023-02-23 00:18:00,842 DEBUG CV Batch 11/1300 loss 9.116866 loss_att 7.437313 loss_ctc 10.022091 loss_rnnt 9.016088 hw_loss 0.592485 history loss 10.450040 rank 6
2023-02-23 00:18:00,963 DEBUG CV Batch 11/1300 loss 9.116866 loss_att 7.437313 loss_ctc 10.022091 loss_rnnt 9.016088 hw_loss 0.592485 history loss 10.450040 rank 5
2023-02-23 00:18:02,516 DEBUG CV Batch 11/1300 loss 9.116866 loss_att 7.437313 loss_ctc 10.022091 loss_rnnt 9.016088 hw_loss 0.592485 history loss 10.450040 rank 1
2023-02-23 00:18:04,170 DEBUG CV Batch 11/1300 loss 9.116866 loss_att 7.437313 loss_ctc 10.022091 loss_rnnt 9.016088 hw_loss 0.592485 history loss 10.450040 rank 0
2023-02-23 00:18:05,937 DEBUG CV Batch 11/1300 loss 9.116866 loss_att 7.437313 loss_ctc 10.022091 loss_rnnt 9.016088 hw_loss 0.592485 history loss 10.450040 rank 2
2023-02-23 00:18:08,529 DEBUG CV Batch 11/1400 loss 15.792410 loss_att 54.419827 loss_ctc 12.121425 loss_rnnt 8.425643 hw_loss 0.245152 history loss 10.875450 rank 7
2023-02-23 00:18:09,822 DEBUG CV Batch 11/1400 loss 15.792410 loss_att 54.419827 loss_ctc 12.121425 loss_rnnt 8.425643 hw_loss 0.245152 history loss 10.875450 rank 3
2023-02-23 00:18:12,126 DEBUG CV Batch 11/1400 loss 15.792410 loss_att 54.419827 loss_ctc 12.121425 loss_rnnt 8.425643 hw_loss 0.245152 history loss 10.875450 rank 4
2023-02-23 00:18:12,368 DEBUG CV Batch 11/1400 loss 15.792410 loss_att 54.419827 loss_ctc 12.121425 loss_rnnt 8.425643 hw_loss 0.245152 history loss 10.875450 rank 6
2023-02-23 00:18:13,723 DEBUG CV Batch 11/1400 loss 15.792410 loss_att 54.419827 loss_ctc 12.121425 loss_rnnt 8.425643 hw_loss 0.245152 history loss 10.875450 rank 5
2023-02-23 00:18:15,141 DEBUG CV Batch 11/1400 loss 15.792410 loss_att 54.419827 loss_ctc 12.121425 loss_rnnt 8.425643 hw_loss 0.245152 history loss 10.875450 rank 1
2023-02-23 00:18:15,996 DEBUG CV Batch 11/1400 loss 15.792410 loss_att 54.419827 loss_ctc 12.121425 loss_rnnt 8.425643 hw_loss 0.245152 history loss 10.875450 rank 0
2023-02-23 00:18:17,859 DEBUG CV Batch 11/1400 loss 15.792410 loss_att 54.419827 loss_ctc 12.121425 loss_rnnt 8.425643 hw_loss 0.245152 history loss 10.875450 rank 2
2023-02-23 00:18:20,260 DEBUG CV Batch 11/1500 loss 10.491437 loss_att 12.279552 loss_ctc 10.705915 loss_rnnt 9.869709 hw_loss 0.441579 history loss 10.616531 rank 7
2023-02-23 00:18:21,527 DEBUG CV Batch 11/1500 loss 10.491437 loss_att 12.279552 loss_ctc 10.705915 loss_rnnt 9.869709 hw_loss 0.441579 history loss 10.616531 rank 3
2023-02-23 00:18:24,955 DEBUG CV Batch 11/1500 loss 10.491437 loss_att 12.279552 loss_ctc 10.705915 loss_rnnt 9.869709 hw_loss 0.441579 history loss 10.616531 rank 4
2023-02-23 00:18:24,981 DEBUG CV Batch 11/1500 loss 10.491437 loss_att 12.279552 loss_ctc 10.705915 loss_rnnt 9.869709 hw_loss 0.441579 history loss 10.616531 rank 6
2023-02-23 00:18:27,207 DEBUG CV Batch 11/1500 loss 10.491437 loss_att 12.279552 loss_ctc 10.705915 loss_rnnt 9.869709 hw_loss 0.441579 history loss 10.616531 rank 5
2023-02-23 00:18:28,166 DEBUG CV Batch 11/1500 loss 10.491437 loss_att 12.279552 loss_ctc 10.705915 loss_rnnt 9.869709 hw_loss 0.441579 history loss 10.616531 rank 1
2023-02-23 00:18:28,324 DEBUG CV Batch 11/1500 loss 10.491437 loss_att 12.279552 loss_ctc 10.705915 loss_rnnt 9.869709 hw_loss 0.441579 history loss 10.616531 rank 0
2023-02-23 00:18:30,316 DEBUG CV Batch 11/1500 loss 10.491437 loss_att 12.279552 loss_ctc 10.705915 loss_rnnt 9.869709 hw_loss 0.441579 history loss 10.616531 rank 2
2023-02-23 00:18:33,501 DEBUG CV Batch 11/1600 loss 17.902760 loss_att 24.590950 loss_ctc 16.835379 loss_rnnt 16.512747 hw_loss 0.365047 history loss 10.501955 rank 7
2023-02-23 00:18:34,728 DEBUG CV Batch 11/1600 loss 17.902760 loss_att 24.590950 loss_ctc 16.835379 loss_rnnt 16.512747 hw_loss 0.365047 history loss 10.501955 rank 3
2023-02-23 00:18:38,676 DEBUG CV Batch 11/1600 loss 17.902760 loss_att 24.590950 loss_ctc 16.835379 loss_rnnt 16.512747 hw_loss 0.365047 history loss 10.501955 rank 4
2023-02-23 00:18:39,216 DEBUG CV Batch 11/1600 loss 17.902760 loss_att 24.590950 loss_ctc 16.835379 loss_rnnt 16.512747 hw_loss 0.365047 history loss 10.501955 rank 6
2023-02-23 00:18:41,337 DEBUG CV Batch 11/1600 loss 17.902760 loss_att 24.590950 loss_ctc 16.835379 loss_rnnt 16.512747 hw_loss 0.365047 history loss 10.501955 rank 5
2023-02-23 00:18:41,954 DEBUG CV Batch 11/1600 loss 17.902760 loss_att 24.590950 loss_ctc 16.835379 loss_rnnt 16.512747 hw_loss 0.365047 history loss 10.501955 rank 0
2023-02-23 00:18:42,050 DEBUG CV Batch 11/1600 loss 17.902760 loss_att 24.590950 loss_ctc 16.835379 loss_rnnt 16.512747 hw_loss 0.365047 history loss 10.501955 rank 1
2023-02-23 00:18:44,095 DEBUG CV Batch 11/1600 loss 17.902760 loss_att 24.590950 loss_ctc 16.835379 loss_rnnt 16.512747 hw_loss 0.365047 history loss 10.501955 rank 2
2023-02-23 00:18:46,054 DEBUG CV Batch 11/1700 loss 13.292814 loss_att 13.378074 loss_ctc 16.730339 loss_rnnt 12.518843 hw_loss 0.559845 history loss 10.354805 rank 7
2023-02-23 00:18:47,272 DEBUG CV Batch 11/1700 loss 13.292814 loss_att 13.378074 loss_ctc 16.730339 loss_rnnt 12.518843 hw_loss 0.559845 history loss 10.354805 rank 3
2023-02-23 00:18:51,241 DEBUG CV Batch 11/1700 loss 13.292814 loss_att 13.378074 loss_ctc 16.730339 loss_rnnt 12.518843 hw_loss 0.559845 history loss 10.354805 rank 4
2023-02-23 00:18:52,324 DEBUG CV Batch 11/1700 loss 13.292814 loss_att 13.378074 loss_ctc 16.730339 loss_rnnt 12.518843 hw_loss 0.559845 history loss 10.354805 rank 6
2023-02-23 00:18:54,003 DEBUG CV Batch 11/1700 loss 13.292814 loss_att 13.378074 loss_ctc 16.730339 loss_rnnt 12.518843 hw_loss 0.559845 history loss 10.354805 rank 5
2023-02-23 00:18:54,544 DEBUG CV Batch 11/1700 loss 13.292814 loss_att 13.378074 loss_ctc 16.730339 loss_rnnt 12.518843 hw_loss 0.559845 history loss 10.354805 rank 1
2023-02-23 00:18:55,096 DEBUG CV Batch 11/1700 loss 13.292814 loss_att 13.378074 loss_ctc 16.730339 loss_rnnt 12.518843 hw_loss 0.559845 history loss 10.354805 rank 0
2023-02-23 00:18:55,349 INFO Epoch 11 CV info cv_loss 10.29583560965588
2023-02-23 00:18:55,350 INFO Epoch 12 TRAIN info lr 0.0004997127477313521
2023-02-23 00:18:55,352 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:18:56,507 INFO Epoch 11 CV info cv_loss 10.295835609289758
2023-02-23 00:18:56,508 INFO Epoch 12 TRAIN info lr 0.0004997352104890762
2023-02-23 00:18:56,513 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:18:56,971 DEBUG CV Batch 11/1700 loss 13.292814 loss_att 13.378074 loss_ctc 16.730339 loss_rnnt 12.518843 hw_loss 0.559845 history loss 10.354805 rank 2
2023-02-23 00:19:00,681 INFO Epoch 11 CV info cv_loss 10.29583560797172
2023-02-23 00:19:00,681 INFO Epoch 12 TRAIN info lr 0.0004997377065380372
2023-02-23 00:19:00,686 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:19:01,827 INFO Epoch 11 CV info cv_loss 10.295835610202909
2023-02-23 00:19:01,827 INFO Epoch 12 TRAIN info lr 0.0004997501873438866
2023-02-23 00:19:01,829 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:19:03,344 INFO Epoch 11 CV info cv_loss 10.295835608626431
2023-02-23 00:19:03,344 INFO Epoch 12 TRAIN info lr 0.0004997352104890762
2023-02-23 00:19:03,346 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:19:03,867 INFO Epoch 11 CV info cv_loss 10.29583560976787
2023-02-23 00:19:03,868 INFO Epoch 12 TRAIN info lr 0.000499785138575691
2023-02-23 00:19:03,873 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:19:04,622 INFO Epoch 11 CV info cv_loss 10.295835606808744
2023-02-23 00:19:04,623 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/11.pt
2023-02-23 00:19:08,216 INFO Epoch 11 CV info cv_loss 10.295835611848304
2023-02-23 00:19:08,217 INFO Epoch 12 TRAIN info lr 0.0004997177391935201
2023-02-23 00:19:08,222 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:19:08,291 INFO Epoch 12 TRAIN info lr 0.0004997951259889105
2023-02-23 00:19:08,295 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 00:20:10,493 DEBUG TRAIN Batch 12/0 loss 17.897106 loss_att 15.724298 loss_ctc 20.815819 loss_rnnt 17.554714 hw_loss 0.727109 lr 0.00049971 rank 7
2023-02-23 00:20:10,495 DEBUG TRAIN Batch 12/0 loss 12.794080 loss_att 11.055611 loss_ctc 13.874916 loss_rnnt 12.585854 hw_loss 0.772143 lr 0.00049974 rank 4
2023-02-23 00:20:10,497 DEBUG TRAIN Batch 12/0 loss 15.337605 loss_att 14.987134 loss_ctc 16.511347 loss_rnnt 14.801924 hw_loss 0.842398 lr 0.00049973 rank 5
2023-02-23 00:20:10,498 DEBUG TRAIN Batch 12/0 loss 12.562434 loss_att 11.475457 loss_ctc 13.985631 loss_rnnt 12.165289 hw_loss 0.796466 lr 0.00049973 rank 3
2023-02-23 00:20:10,502 DEBUG TRAIN Batch 12/0 loss 15.329427 loss_att 14.281380 loss_ctc 17.922209 loss_rnnt 14.832555 hw_loss 0.676457 lr 0.00049978 rank 1
2023-02-23 00:20:10,524 DEBUG TRAIN Batch 12/0 loss 17.326620 loss_att 15.648458 loss_ctc 19.952686 loss_rnnt 16.912207 hw_loss 0.749821 lr 0.00049972 rank 2
2023-02-23 00:20:10,532 DEBUG TRAIN Batch 12/0 loss 11.482651 loss_att 10.885396 loss_ctc 12.721366 loss_rnnt 11.071505 hw_loss 0.685188 lr 0.00049979 rank 0
2023-02-23 00:20:10,540 DEBUG TRAIN Batch 12/0 loss 11.895226 loss_att 11.154546 loss_ctc 14.863151 loss_rnnt 11.251487 hw_loss 0.742788 lr 0.00049975 rank 6
2023-02-23 00:21:22,667 DEBUG TRAIN Batch 12/100 loss 29.292471 loss_att 32.308041 loss_ctc 31.239655 loss_rnnt 28.195608 hw_loss 0.438982 lr 0.00049949 rank 4
2023-02-23 00:21:22,682 DEBUG TRAIN Batch 12/100 loss 23.673740 loss_att 24.583670 loss_ctc 28.562363 loss_rnnt 22.632416 hw_loss 0.389107 lr 0.00049946 rank 7
2023-02-23 00:21:22,683 DEBUG TRAIN Batch 12/100 loss 13.952747 loss_att 17.487789 loss_ctc 16.346169 loss_rnnt 12.656279 hw_loss 0.506883 lr 0.00049948 rank 3
2023-02-23 00:21:22,689 DEBUG TRAIN Batch 12/100 loss 16.869127 loss_att 23.006636 loss_ctc 21.959625 loss_rnnt 14.783644 hw_loss 0.336091 lr 0.00049950 rank 6
2023-02-23 00:21:22,689 DEBUG TRAIN Batch 12/100 loss 12.654014 loss_att 15.506653 loss_ctc 14.973291 loss_rnnt 11.576561 hw_loss 0.370664 lr 0.00049954 rank 0
2023-02-23 00:21:22,690 DEBUG TRAIN Batch 12/100 loss 14.216634 loss_att 19.327061 loss_ctc 16.083916 loss_rnnt 12.751381 hw_loss 0.364118 lr 0.00049947 rank 2
2023-02-23 00:21:22,695 DEBUG TRAIN Batch 12/100 loss 16.870571 loss_att 17.806786 loss_ctc 20.421755 loss_rnnt 15.967800 hw_loss 0.453819 lr 0.00049953 rank 1
2023-02-23 00:21:22,699 DEBUG TRAIN Batch 12/100 loss 28.270309 loss_att 34.324608 loss_ctc 35.849785 loss_rnnt 25.841373 hw_loss 0.389022 lr 0.00049948 rank 5
2023-02-23 00:22:35,968 DEBUG TRAIN Batch 12/200 loss 9.628863 loss_att 17.276787 loss_ctc 11.053691 loss_rnnt 7.655451 hw_loss 0.475970 lr 0.00049921 rank 7
2023-02-23 00:22:35,968 DEBUG TRAIN Batch 12/200 loss 17.298622 loss_att 26.892170 loss_ctc 20.518202 loss_rnnt 14.760267 hw_loss 0.356939 lr 0.00049928 rank 1
2023-02-23 00:22:35,968 DEBUG TRAIN Batch 12/200 loss 10.505769 loss_att 17.201164 loss_ctc 10.664101 loss_rnnt 8.922294 hw_loss 0.418662 lr 0.00049924 rank 4
2023-02-23 00:22:35,970 DEBUG TRAIN Batch 12/200 loss 13.688989 loss_att 23.936958 loss_ctc 13.645491 loss_rnnt 11.439156 hw_loss 0.386324 lr 0.00049925 rank 6
2023-02-23 00:22:35,969 DEBUG TRAIN Batch 12/200 loss 18.735373 loss_att 22.018702 loss_ctc 21.929716 loss_rnnt 17.470253 hw_loss 0.342265 lr 0.00049923 rank 3
2023-02-23 00:22:35,976 DEBUG TRAIN Batch 12/200 loss 21.134087 loss_att 26.537291 loss_ctc 24.696297 loss_rnnt 19.396132 hw_loss 0.341910 lr 0.00049929 rank 0
2023-02-23 00:22:35,977 DEBUG TRAIN Batch 12/200 loss 21.449234 loss_att 24.336262 loss_ctc 25.557989 loss_rnnt 20.143930 hw_loss 0.337617 lr 0.00049923 rank 5
2023-02-23 00:22:35,979 DEBUG TRAIN Batch 12/200 loss 12.864402 loss_att 18.184233 loss_ctc 16.243778 loss_rnnt 11.105688 hw_loss 0.457809 lr 0.00049922 rank 2
2023-02-23 00:23:51,129 DEBUG TRAIN Batch 12/300 loss 19.182398 loss_att 25.263432 loss_ctc 22.852463 loss_rnnt 17.242500 hw_loss 0.439406 lr 0.00049904 rank 1
2023-02-23 00:23:51,134 DEBUG TRAIN Batch 12/300 loss 13.477635 loss_att 18.115095 loss_ctc 14.751765 loss_rnnt 12.183598 hw_loss 0.368740 lr 0.00049899 rank 3
2023-02-23 00:23:51,135 DEBUG TRAIN Batch 12/300 loss 24.985464 loss_att 32.849960 loss_ctc 30.785950 loss_rnnt 22.425278 hw_loss 0.401045 lr 0.00049897 rank 2
2023-02-23 00:23:51,136 DEBUG TRAIN Batch 12/300 loss 16.837610 loss_att 23.008204 loss_ctc 23.940773 loss_rnnt 14.408276 hw_loss 0.465238 lr 0.00049896 rank 7
2023-02-23 00:23:51,136 DEBUG TRAIN Batch 12/300 loss 25.930668 loss_att 30.853502 loss_ctc 33.414749 loss_rnnt 23.684824 hw_loss 0.493875 lr 0.00049905 rank 0
2023-02-23 00:23:51,154 DEBUG TRAIN Batch 12/300 loss 16.203745 loss_att 20.383022 loss_ctc 18.857475 loss_rnnt 14.758126 hw_loss 0.479870 lr 0.00049899 rank 5
2023-02-23 00:23:51,171 DEBUG TRAIN Batch 12/300 loss 9.303024 loss_att 14.329016 loss_ctc 12.682201 loss_rnnt 7.629210 hw_loss 0.408860 lr 0.00049900 rank 6
2023-02-23 00:23:51,177 DEBUG TRAIN Batch 12/300 loss 18.639376 loss_att 24.385695 loss_ctc 19.159040 loss_rnnt 17.209389 hw_loss 0.396437 lr 0.00049899 rank 4
2023-02-23 00:25:05,299 DEBUG TRAIN Batch 12/400 loss 19.499252 loss_att 29.565025 loss_ctc 24.849815 loss_rnnt 16.588291 hw_loss 0.345747 lr 0.00049871 rank 7
2023-02-23 00:25:05,302 DEBUG TRAIN Batch 12/400 loss 18.114027 loss_att 22.629210 loss_ctc 20.647919 loss_rnnt 16.656261 hw_loss 0.406642 lr 0.00049875 rank 6
2023-02-23 00:25:05,303 DEBUG TRAIN Batch 12/400 loss 17.356771 loss_att 17.944366 loss_ctc 19.213306 loss_rnnt 16.771374 hw_loss 0.413138 lr 0.00049874 rank 3
2023-02-23 00:25:05,308 DEBUG TRAIN Batch 12/400 loss 14.771322 loss_att 20.316271 loss_ctc 19.351984 loss_rnnt 12.874783 hw_loss 0.331491 lr 0.00049874 rank 5
2023-02-23 00:25:05,308 DEBUG TRAIN Batch 12/400 loss 16.479893 loss_att 24.525415 loss_ctc 20.962318 loss_rnnt 14.049940 hw_loss 0.418486 lr 0.00049880 rank 0
2023-02-23 00:25:05,319 DEBUG TRAIN Batch 12/400 loss 16.933468 loss_att 17.679073 loss_ctc 18.753101 loss_rnnt 16.333614 hw_loss 0.390214 lr 0.00049874 rank 4
2023-02-23 00:25:05,330 DEBUG TRAIN Batch 12/400 loss 23.797316 loss_att 24.563519 loss_ctc 30.479816 loss_rnnt 22.581161 hw_loss 0.322335 lr 0.00049872 rank 2
2023-02-23 00:25:05,348 DEBUG TRAIN Batch 12/400 loss 11.568869 loss_att 19.105764 loss_ctc 14.192450 loss_rnnt 9.481119 hw_loss 0.432298 lr 0.00049879 rank 1
2023-02-23 00:26:18,137 DEBUG TRAIN Batch 12/500 loss 21.111856 loss_att 23.675753 loss_ctc 24.352043 loss_rnnt 19.945419 hw_loss 0.415563 lr 0.00049849 rank 4
2023-02-23 00:26:18,139 DEBUG TRAIN Batch 12/500 loss 13.030752 loss_att 18.764290 loss_ctc 16.285925 loss_rnnt 11.270878 hw_loss 0.335893 lr 0.00049847 rank 7
2023-02-23 00:26:18,139 DEBUG TRAIN Batch 12/500 loss 12.163907 loss_att 14.902579 loss_ctc 11.848337 loss_rnnt 11.390980 hw_loss 0.501129 lr 0.00049855 rank 0
2023-02-23 00:26:18,141 DEBUG TRAIN Batch 12/500 loss 14.912859 loss_att 18.392536 loss_ctc 19.775196 loss_rnnt 13.309944 hw_loss 0.485000 lr 0.00049849 rank 3
2023-02-23 00:26:18,143 DEBUG TRAIN Batch 12/500 loss 24.611025 loss_att 26.252312 loss_ctc 28.036823 loss_rnnt 23.625414 hw_loss 0.376088 lr 0.00049849 rank 5
2023-02-23 00:26:18,143 DEBUG TRAIN Batch 12/500 loss 14.518825 loss_att 19.568272 loss_ctc 20.565214 loss_rnnt 12.510531 hw_loss 0.360410 lr 0.00049850 rank 6
2023-02-23 00:26:18,145 DEBUG TRAIN Batch 12/500 loss 17.986509 loss_att 18.600016 loss_ctc 20.356140 loss_rnnt 17.289124 hw_loss 0.485125 lr 0.00049854 rank 1
2023-02-23 00:26:18,151 DEBUG TRAIN Batch 12/500 loss 32.302643 loss_att 33.778038 loss_ctc 38.473824 loss_rnnt 30.960232 hw_loss 0.420949 lr 0.00049847 rank 2
2023-02-23 00:27:31,503 DEBUG TRAIN Batch 12/600 loss 7.699887 loss_att 11.510398 loss_ctc 10.480444 loss_rnnt 6.324245 hw_loss 0.455247 lr 0.00049822 rank 7
2023-02-23 00:27:31,507 DEBUG TRAIN Batch 12/600 loss 24.841347 loss_att 24.369385 loss_ctc 27.214657 loss_rnnt 24.346310 hw_loss 0.511856 lr 0.00049824 rank 4
2023-02-23 00:27:31,510 DEBUG TRAIN Batch 12/600 loss 17.499767 loss_att 15.701998 loss_ctc 18.967598 loss_rnnt 17.317741 hw_loss 0.648503 lr 0.00049824 rank 5
2023-02-23 00:27:31,511 DEBUG TRAIN Batch 12/600 loss 30.389349 loss_att 30.582043 loss_ctc 36.267593 loss_rnnt 29.334665 hw_loss 0.435712 lr 0.00049824 rank 3
2023-02-23 00:27:31,513 DEBUG TRAIN Batch 12/600 loss 15.714590 loss_att 15.022089 loss_ctc 16.750378 loss_rnnt 15.458729 hw_loss 0.480481 lr 0.00049830 rank 0
2023-02-23 00:27:31,517 DEBUG TRAIN Batch 12/600 loss 11.819141 loss_att 12.425189 loss_ctc 14.790666 loss_rnnt 11.067813 hw_loss 0.438592 lr 0.00049826 rank 6
2023-02-23 00:27:31,519 DEBUG TRAIN Batch 12/600 loss 12.048210 loss_att 12.695406 loss_ctc 11.868991 loss_rnnt 11.682195 hw_loss 0.488384 lr 0.00049829 rank 1
2023-02-23 00:27:31,525 DEBUG TRAIN Batch 12/600 loss 27.629740 loss_att 25.894905 loss_ctc 31.952232 loss_rnnt 27.109224 hw_loss 0.545907 lr 0.00049822 rank 2
2023-02-23 00:28:46,993 DEBUG TRAIN Batch 12/700 loss 17.682497 loss_att 25.573471 loss_ctc 21.137997 loss_rnnt 15.451499 hw_loss 0.360131 lr 0.00049801 rank 6
2023-02-23 00:28:46,992 DEBUG TRAIN Batch 12/700 loss 15.273575 loss_att 17.249834 loss_ctc 13.699108 loss_rnnt 14.868735 hw_loss 0.411591 lr 0.00049805 rank 0
2023-02-23 00:28:47,004 DEBUG TRAIN Batch 12/700 loss 17.446447 loss_att 28.527231 loss_ctc 20.971981 loss_rnnt 14.533050 hw_loss 0.425946 lr 0.00049797 rank 7
2023-02-23 00:28:47,005 DEBUG TRAIN Batch 12/700 loss 10.965946 loss_att 12.184623 loss_ctc 12.411769 loss_rnnt 10.318729 hw_loss 0.395073 lr 0.00049799 rank 5
2023-02-23 00:28:47,011 DEBUG TRAIN Batch 12/700 loss 15.241426 loss_att 18.796783 loss_ctc 16.208399 loss_rnnt 14.171518 hw_loss 0.431076 lr 0.00049800 rank 4
2023-02-23 00:28:47,011 DEBUG TRAIN Batch 12/700 loss 16.315784 loss_att 23.783562 loss_ctc 17.679771 loss_rnnt 14.329945 hw_loss 0.582039 lr 0.00049799 rank 3
2023-02-23 00:28:47,042 DEBUG TRAIN Batch 12/700 loss 35.463551 loss_att 33.890480 loss_ctc 40.180012 loss_rnnt 34.928288 hw_loss 0.414402 lr 0.00049798 rank 2
2023-02-23 00:28:47,058 DEBUG TRAIN Batch 12/700 loss 11.389915 loss_att 16.185253 loss_ctc 13.490275 loss_rnnt 9.915374 hw_loss 0.441422 lr 0.00049804 rank 1
2023-02-23 00:30:00,223 DEBUG TRAIN Batch 12/800 loss 6.363478 loss_att 11.574258 loss_ctc 6.499427 loss_rnnt 5.131781 hw_loss 0.321401 lr 0.00049773 rank 7
2023-02-23 00:30:00,232 DEBUG TRAIN Batch 12/800 loss 19.168688 loss_att 19.241627 loss_ctc 19.215225 loss_rnnt 18.917767 hw_loss 0.431488 lr 0.00049775 rank 4
2023-02-23 00:30:00,232 DEBUG TRAIN Batch 12/800 loss 3.663367 loss_att 7.837555 loss_ctc 4.711701 loss_rnnt 2.505089 hw_loss 0.344365 lr 0.00049775 rank 3
2023-02-23 00:30:00,233 DEBUG TRAIN Batch 12/800 loss 25.801933 loss_att 27.472973 loss_ctc 31.664768 loss_rnnt 24.453037 hw_loss 0.436829 lr 0.00049781 rank 0
2023-02-23 00:30:00,237 DEBUG TRAIN Batch 12/800 loss 7.762277 loss_att 13.567410 loss_ctc 9.528641 loss_rnnt 6.131043 hw_loss 0.440047 lr 0.00049775 rank 5
2023-02-23 00:30:00,238 DEBUG TRAIN Batch 12/800 loss 12.336920 loss_att 16.625214 loss_ctc 12.190084 loss_rnnt 11.270510 hw_loss 0.428117 lr 0.00049780 rank 1
2023-02-23 00:30:00,238 DEBUG TRAIN Batch 12/800 loss 11.244900 loss_att 17.900591 loss_ctc 15.753494 loss_rnnt 9.050034 hw_loss 0.492339 lr 0.00049773 rank 2
2023-02-23 00:30:00,238 DEBUG TRAIN Batch 12/800 loss 22.598904 loss_att 29.101097 loss_ctc 30.895824 loss_rnnt 19.951990 hw_loss 0.450409 lr 0.00049776 rank 6
2023-02-23 00:31:13,371 DEBUG TRAIN Batch 12/900 loss 20.219467 loss_att 24.393993 loss_ctc 19.801559 loss_rnnt 19.269855 hw_loss 0.319546 lr 0.00049748 rank 7
2023-02-23 00:31:13,372 DEBUG TRAIN Batch 12/900 loss 13.421625 loss_att 15.173571 loss_ctc 17.251507 loss_rnnt 12.345665 hw_loss 0.402976 lr 0.00049750 rank 3
2023-02-23 00:31:13,374 DEBUG TRAIN Batch 12/900 loss 10.944385 loss_att 15.621407 loss_ctc 11.762651 loss_rnnt 9.681707 hw_loss 0.409069 lr 0.00049752 rank 6
2023-02-23 00:31:13,373 DEBUG TRAIN Batch 12/900 loss 24.513096 loss_att 29.927326 loss_ctc 30.592653 loss_rnnt 22.359779 hw_loss 0.487242 lr 0.00049750 rank 4
2023-02-23 00:31:13,376 DEBUG TRAIN Batch 12/900 loss 30.400078 loss_att 32.070564 loss_ctc 30.911762 loss_rnnt 29.814365 hw_loss 0.343851 lr 0.00049756 rank 0
2023-02-23 00:31:13,376 DEBUG TRAIN Batch 12/900 loss 7.693487 loss_att 12.336093 loss_ctc 8.608416 loss_rnnt 6.369662 hw_loss 0.512460 lr 0.00049750 rank 5
2023-02-23 00:31:13,377 DEBUG TRAIN Batch 12/900 loss 4.907856 loss_att 9.862480 loss_ctc 5.198551 loss_rnnt 3.692393 hw_loss 0.348334 lr 0.00049755 rank 1
2023-02-23 00:31:13,381 DEBUG TRAIN Batch 12/900 loss 30.138554 loss_att 34.310005 loss_ctc 39.840492 loss_rnnt 27.763729 hw_loss 0.463017 lr 0.00049748 rank 2
2023-02-23 00:32:27,329 DEBUG TRAIN Batch 12/1000 loss 8.497319 loss_att 14.017813 loss_ctc 11.357608 loss_rnnt 6.837326 hw_loss 0.327229 lr 0.00049730 rank 1
2023-02-23 00:32:27,335 DEBUG TRAIN Batch 12/1000 loss 24.735498 loss_att 27.314558 loss_ctc 31.001348 loss_rnnt 23.164448 hw_loss 0.412106 lr 0.00049726 rank 4
2023-02-23 00:32:27,336 DEBUG TRAIN Batch 12/1000 loss 29.600973 loss_att 34.397804 loss_ctc 33.173523 loss_rnnt 27.965755 hw_loss 0.374085 lr 0.00049726 rank 5
2023-02-23 00:32:27,339 DEBUG TRAIN Batch 12/1000 loss 14.804488 loss_att 17.919535 loss_ctc 19.455772 loss_rnnt 13.382065 hw_loss 0.336081 lr 0.00049723 rank 7
2023-02-23 00:32:27,342 DEBUG TRAIN Batch 12/1000 loss 13.243258 loss_att 17.844139 loss_ctc 17.090130 loss_rnnt 11.569942 hw_loss 0.450415 lr 0.00049726 rank 3
2023-02-23 00:32:27,349 DEBUG TRAIN Batch 12/1000 loss 17.644463 loss_att 20.772419 loss_ctc 20.394238 loss_rnnt 16.412926 hw_loss 0.448706 lr 0.00049727 rank 6
2023-02-23 00:32:27,359 DEBUG TRAIN Batch 12/1000 loss 12.544596 loss_att 23.465021 loss_ctc 12.999531 loss_rnnt 10.120005 hw_loss 0.337215 lr 0.00049731 rank 0
2023-02-23 00:32:27,387 DEBUG TRAIN Batch 12/1000 loss 17.304745 loss_att 19.181156 loss_ctc 19.265656 loss_rnnt 16.485165 hw_loss 0.342832 lr 0.00049724 rank 2
2023-02-23 00:33:42,918 DEBUG TRAIN Batch 12/1100 loss 27.159685 loss_att 32.206642 loss_ctc 33.432076 loss_rnnt 25.111443 hw_loss 0.379750 lr 0.00049699 rank 7
2023-02-23 00:33:42,920 DEBUG TRAIN Batch 12/1100 loss 10.694171 loss_att 17.248184 loss_ctc 15.311346 loss_rnnt 8.553596 hw_loss 0.401528 lr 0.00049701 rank 3
2023-02-23 00:33:42,922 DEBUG TRAIN Batch 12/1100 loss 18.275654 loss_att 24.732124 loss_ctc 25.123665 loss_rnnt 15.870249 hw_loss 0.376954 lr 0.00049702 rank 6
2023-02-23 00:33:42,922 DEBUG TRAIN Batch 12/1100 loss 32.048103 loss_att 36.181641 loss_ctc 40.119286 loss_rnnt 29.912170 hw_loss 0.437000 lr 0.00049707 rank 0
2023-02-23 00:33:42,927 DEBUG TRAIN Batch 12/1100 loss 19.221807 loss_att 24.723101 loss_ctc 27.186787 loss_rnnt 16.822159 hw_loss 0.445112 lr 0.00049701 rank 5
2023-02-23 00:33:42,927 DEBUG TRAIN Batch 12/1100 loss 8.453388 loss_att 13.336636 loss_ctc 14.611814 loss_rnnt 6.429228 hw_loss 0.424476 lr 0.00049706 rank 1
2023-02-23 00:33:42,928 DEBUG TRAIN Batch 12/1100 loss 27.026127 loss_att 29.530720 loss_ctc 28.090967 loss_rnnt 26.174564 hw_loss 0.391244 lr 0.00049701 rank 4
2023-02-23 00:33:42,928 DEBUG TRAIN Batch 12/1100 loss 22.558382 loss_att 25.056635 loss_ctc 28.412518 loss_rnnt 21.065895 hw_loss 0.398034 lr 0.00049699 rank 2
2023-02-23 00:34:56,601 DEBUG TRAIN Batch 12/1200 loss 19.608091 loss_att 19.405031 loss_ctc 22.268009 loss_rnnt 19.054991 hw_loss 0.448230 lr 0.00049681 rank 1
2023-02-23 00:34:56,605 DEBUG TRAIN Batch 12/1200 loss 15.751495 loss_att 19.535820 loss_ctc 17.742960 loss_rnnt 14.487732 hw_loss 0.452567 lr 0.00049677 rank 4
2023-02-23 00:34:56,619 DEBUG TRAIN Batch 12/1200 loss 11.612974 loss_att 14.638489 loss_ctc 10.845344 loss_rnnt 10.870655 hw_loss 0.449187 lr 0.00049674 rank 7
2023-02-23 00:34:56,619 DEBUG TRAIN Batch 12/1200 loss 12.193732 loss_att 14.814556 loss_ctc 14.326034 loss_rnnt 11.111415 hw_loss 0.513460 lr 0.00049682 rank 0
2023-02-23 00:34:56,621 DEBUG TRAIN Batch 12/1200 loss 26.933933 loss_att 25.123865 loss_ctc 30.568890 loss_rnnt 26.481857 hw_loss 0.617683 lr 0.00049676 rank 5
2023-02-23 00:34:56,622 DEBUG TRAIN Batch 12/1200 loss 18.027782 loss_att 19.234936 loss_ctc 23.111450 loss_rnnt 16.870218 hw_loss 0.446836 lr 0.00049675 rank 2
2023-02-23 00:34:56,623 DEBUG TRAIN Batch 12/1200 loss 27.780245 loss_att 25.528074 loss_ctc 33.752525 loss_rnnt 27.183834 hw_loss 0.469766 lr 0.00049676 rank 3
2023-02-23 00:34:56,667 DEBUG TRAIN Batch 12/1200 loss 17.521786 loss_att 20.869160 loss_ctc 23.115515 loss_rnnt 15.869153 hw_loss 0.444985 lr 0.00049678 rank 6
2023-02-23 00:36:09,832 DEBUG TRAIN Batch 12/1300 loss 15.861913 loss_att 18.590591 loss_ctc 19.914764 loss_rnnt 14.598351 hw_loss 0.332713 lr 0.00049652 rank 5
2023-02-23 00:36:09,836 DEBUG TRAIN Batch 12/1300 loss 27.991964 loss_att 29.534325 loss_ctc 30.629910 loss_rnnt 27.100235 hw_loss 0.434122 lr 0.00049652 rank 3
2023-02-23 00:36:09,839 DEBUG TRAIN Batch 12/1300 loss 9.852230 loss_att 13.149122 loss_ctc 11.291497 loss_rnnt 8.732028 hw_loss 0.504226 lr 0.00049650 rank 7
2023-02-23 00:36:09,844 DEBUG TRAIN Batch 12/1300 loss 15.032843 loss_att 15.022725 loss_ctc 16.767565 loss_rnnt 14.431188 hw_loss 0.698214 lr 0.00049658 rank 0
2023-02-23 00:36:09,845 DEBUG TRAIN Batch 12/1300 loss 14.273862 loss_att 15.050528 loss_ctc 17.440989 loss_rnnt 13.268491 hw_loss 0.802038 lr 0.00049657 rank 1
2023-02-23 00:36:09,845 DEBUG TRAIN Batch 12/1300 loss 17.952301 loss_att 23.951555 loss_ctc 22.422211 loss_rnnt 15.993496 hw_loss 0.305564 lr 0.00049650 rank 2
2023-02-23 00:36:09,845 DEBUG TRAIN Batch 12/1300 loss 8.118601 loss_att 15.876173 loss_ctc 10.512423 loss_rnnt 6.025439 hw_loss 0.417133 lr 0.00049652 rank 4
2023-02-23 00:36:09,848 DEBUG TRAIN Batch 12/1300 loss 9.253749 loss_att 11.651472 loss_ctc 10.819051 loss_rnnt 8.264898 hw_loss 0.563624 lr 0.00049653 rank 6
2023-02-23 00:37:25,516 DEBUG TRAIN Batch 12/1400 loss 7.653738 loss_att 14.113555 loss_ctc 11.743282 loss_rnnt 5.598606 hw_loss 0.408558 lr 0.00049626 rank 2
2023-02-23 00:37:25,523 DEBUG TRAIN Batch 12/1400 loss 7.697208 loss_att 14.099283 loss_ctc 10.294346 loss_rnnt 5.856079 hw_loss 0.402055 lr 0.00049628 rank 4
2023-02-23 00:37:25,527 DEBUG TRAIN Batch 12/1400 loss 11.660972 loss_att 16.788372 loss_ctc 16.805710 loss_rnnt 9.745609 hw_loss 0.382345 lr 0.00049627 rank 3
2023-02-23 00:37:25,528 DEBUG TRAIN Batch 12/1400 loss 28.491198 loss_att 32.926895 loss_ctc 30.030767 loss_rnnt 27.188347 hw_loss 0.394567 lr 0.00049629 rank 6
2023-02-23 00:37:25,530 DEBUG TRAIN Batch 12/1400 loss 18.114441 loss_att 25.730024 loss_ctc 17.277168 loss_rnnt 16.484779 hw_loss 0.409090 lr 0.00049632 rank 1
2023-02-23 00:37:25,530 DEBUG TRAIN Batch 12/1400 loss 13.582674 loss_att 21.295948 loss_ctc 19.603462 loss_rnnt 11.060271 hw_loss 0.331830 lr 0.00049627 rank 5
2023-02-23 00:37:25,555 DEBUG TRAIN Batch 12/1400 loss 28.682579 loss_att 29.289408 loss_ctc 33.851311 loss_rnnt 27.656277 hw_loss 0.404573 lr 0.00049625 rank 7
2023-02-23 00:37:25,560 DEBUG TRAIN Batch 12/1400 loss 34.393909 loss_att 39.596554 loss_ctc 38.078846 loss_rnnt 32.652412 hw_loss 0.393087 lr 0.00049633 rank 0
2023-02-23 00:38:39,570 DEBUG TRAIN Batch 12/1500 loss 11.880794 loss_att 16.641479 loss_ctc 10.878646 loss_rnnt 10.799129 hw_loss 0.493399 lr 0.00049608 rank 1
2023-02-23 00:38:39,571 DEBUG TRAIN Batch 12/1500 loss 7.243454 loss_att 11.037050 loss_ctc 12.477936 loss_rnnt 5.548547 hw_loss 0.446731 lr 0.00049601 rank 7
2023-02-23 00:38:39,574 DEBUG TRAIN Batch 12/1500 loss 24.607216 loss_att 29.349613 loss_ctc 36.872192 loss_rnnt 21.799866 hw_loss 0.419142 lr 0.00049603 rank 3
2023-02-23 00:38:39,577 DEBUG TRAIN Batch 12/1500 loss 26.247736 loss_att 34.694115 loss_ctc 27.777145 loss_rnnt 24.194204 hw_loss 0.300631 lr 0.00049604 rank 6
2023-02-23 00:38:39,577 DEBUG TRAIN Batch 12/1500 loss 19.609556 loss_att 23.256155 loss_ctc 20.626892 loss_rnnt 18.524338 hw_loss 0.412977 lr 0.00049603 rank 5
2023-02-23 00:38:39,577 DEBUG TRAIN Batch 12/1500 loss 12.877462 loss_att 16.239807 loss_ctc 17.867462 loss_rnnt 11.319210 hw_loss 0.413344 lr 0.00049609 rank 0
2023-02-23 00:38:39,578 DEBUG TRAIN Batch 12/1500 loss 13.277963 loss_att 20.980486 loss_ctc 17.843231 loss_rnnt 10.911293 hw_loss 0.407744 lr 0.00049603 rank 4
2023-02-23 00:38:39,583 DEBUG TRAIN Batch 12/1500 loss 11.814235 loss_att 14.314207 loss_ctc 14.964828 loss_rnnt 10.680434 hw_loss 0.400740 lr 0.00049601 rank 2
2023-02-23 00:39:52,671 DEBUG TRAIN Batch 12/1600 loss 21.824091 loss_att 24.649282 loss_ctc 22.304890 loss_rnnt 20.993160 hw_loss 0.378349 lr 0.00049576 rank 7
2023-02-23 00:39:52,680 DEBUG TRAIN Batch 12/1600 loss 12.494000 loss_att 19.470661 loss_ctc 15.234865 loss_rnnt 10.475856 hw_loss 0.482558 lr 0.00049580 rank 6
2023-02-23 00:39:52,682 DEBUG TRAIN Batch 12/1600 loss 23.974976 loss_att 28.358568 loss_ctc 27.737553 loss_rnnt 22.432455 hw_loss 0.307731 lr 0.00049579 rank 4
2023-02-23 00:39:52,682 DEBUG TRAIN Batch 12/1600 loss 25.178129 loss_att 27.675983 loss_ctc 28.363152 loss_rnnt 24.085363 hw_loss 0.315984 lr 0.00049579 rank 3
2023-02-23 00:39:52,683 DEBUG TRAIN Batch 12/1600 loss 12.182721 loss_att 18.069111 loss_ctc 14.749084 loss_rnnt 10.430825 hw_loss 0.435817 lr 0.00049577 rank 2
2023-02-23 00:39:52,684 DEBUG TRAIN Batch 12/1600 loss 10.249996 loss_att 16.058212 loss_ctc 10.667516 loss_rnnt 8.856022 hw_loss 0.331241 lr 0.00049584 rank 0
2023-02-23 00:39:52,685 DEBUG TRAIN Batch 12/1600 loss 14.009987 loss_att 20.310366 loss_ctc 17.242939 loss_rnnt 12.092289 hw_loss 0.424806 lr 0.00049579 rank 5
2023-02-23 00:39:52,735 DEBUG TRAIN Batch 12/1600 loss 31.374109 loss_att 39.417034 loss_ctc 40.188118 loss_rnnt 28.390816 hw_loss 0.374076 lr 0.00049584 rank 1
2023-02-23 00:41:05,328 DEBUG TRAIN Batch 12/1700 loss 18.342510 loss_att 22.076580 loss_ctc 20.884182 loss_rnnt 17.037447 hw_loss 0.411300 lr 0.00049555 rank 4
2023-02-23 00:41:05,329 DEBUG TRAIN Batch 12/1700 loss 19.870800 loss_att 23.137421 loss_ctc 22.416735 loss_rnnt 18.637115 hw_loss 0.451695 lr 0.00049552 rank 7
2023-02-23 00:41:05,334 DEBUG TRAIN Batch 12/1700 loss 8.160872 loss_att 11.152047 loss_ctc 11.867546 loss_rnnt 6.861566 hw_loss 0.387841 lr 0.00049553 rank 2
2023-02-23 00:41:05,334 DEBUG TRAIN Batch 12/1700 loss 18.561832 loss_att 22.747358 loss_ctc 25.394627 loss_rnnt 16.645927 hw_loss 0.314552 lr 0.00049554 rank 3
2023-02-23 00:41:05,334 DEBUG TRAIN Batch 12/1700 loss 13.542148 loss_att 18.634270 loss_ctc 14.226833 loss_rnnt 12.217598 hw_loss 0.402813 lr 0.00049560 rank 0
2023-02-23 00:41:05,368 DEBUG TRAIN Batch 12/1700 loss 13.889533 loss_att 16.646296 loss_ctc 17.390867 loss_rnnt 12.651976 hw_loss 0.411300 lr 0.00049554 rank 5
2023-02-23 00:41:05,373 DEBUG TRAIN Batch 12/1700 loss 18.366354 loss_att 26.356394 loss_ctc 19.580538 loss_rnnt 16.381771 hw_loss 0.421281 lr 0.00049559 rank 1
2023-02-23 00:41:05,377 DEBUG TRAIN Batch 12/1700 loss 14.255663 loss_att 19.086620 loss_ctc 19.073008 loss_rnnt 12.458558 hw_loss 0.353625 lr 0.00049556 rank 6
2023-02-23 00:42:20,486 DEBUG TRAIN Batch 12/1800 loss 17.139273 loss_att 21.524427 loss_ctc 19.310072 loss_rnnt 15.758406 hw_loss 0.401994 lr 0.00049528 rank 7
2023-02-23 00:42:20,488 DEBUG TRAIN Batch 12/1800 loss 12.960952 loss_att 14.232466 loss_ctc 16.245726 loss_rnnt 12.050330 hw_loss 0.409406 lr 0.00049530 rank 3
2023-02-23 00:42:20,492 DEBUG TRAIN Batch 12/1800 loss 6.234973 loss_att 11.756895 loss_ctc 9.006563 loss_rnnt 4.499315 hw_loss 0.490743 lr 0.00049535 rank 1
2023-02-23 00:42:20,491 DEBUG TRAIN Batch 12/1800 loss 22.824263 loss_att 20.074675 loss_ctc 28.069860 loss_rnnt 22.412395 hw_loss 0.491950 lr 0.00049536 rank 0
2023-02-23 00:42:20,494 DEBUG TRAIN Batch 12/1800 loss 8.553596 loss_att 10.657974 loss_ctc 9.153724 loss_rnnt 7.828061 hw_loss 0.421202 lr 0.00049530 rank 5
2023-02-23 00:42:20,496 DEBUG TRAIN Batch 12/1800 loss 13.205289 loss_att 17.721943 loss_ctc 13.234786 loss_rnnt 12.045684 hw_loss 0.473140 lr 0.00049530 rank 4
2023-02-23 00:42:20,497 DEBUG TRAIN Batch 12/1800 loss 27.817032 loss_att 26.246941 loss_ctc 33.997948 loss_rnnt 27.042187 hw_loss 0.496388 lr 0.00049531 rank 6
2023-02-23 00:42:20,500 DEBUG TRAIN Batch 12/1800 loss 23.235691 loss_att 30.960464 loss_ctc 23.786022 loss_rnnt 21.422998 hw_loss 0.364421 lr 0.00049528 rank 2
2023-02-23 00:43:34,141 DEBUG TRAIN Batch 12/1900 loss 15.597595 loss_att 21.170460 loss_ctc 17.654371 loss_rnnt 13.960467 hw_loss 0.465599 lr 0.00049503 rank 7
2023-02-23 00:43:34,142 DEBUG TRAIN Batch 12/1900 loss 12.702536 loss_att 24.831348 loss_ctc 13.543598 loss_rnnt 9.931849 hw_loss 0.436468 lr 0.00049506 rank 5
2023-02-23 00:43:34,143 DEBUG TRAIN Batch 12/1900 loss 13.435776 loss_att 12.894809 loss_ctc 14.842672 loss_rnnt 13.046614 hw_loss 0.580818 lr 0.00049506 rank 4
2023-02-23 00:43:34,144 DEBUG TRAIN Batch 12/1900 loss 18.435486 loss_att 23.717651 loss_ctc 19.753601 loss_rnnt 16.970266 hw_loss 0.436945 lr 0.00049506 rank 3
2023-02-23 00:43:34,144 DEBUG TRAIN Batch 12/1900 loss 20.022264 loss_att 19.313477 loss_ctc 25.820745 loss_rnnt 19.162807 hw_loss 0.427654 lr 0.00049511 rank 1
2023-02-23 00:43:34,147 DEBUG TRAIN Batch 12/1900 loss 15.747231 loss_att 16.206322 loss_ctc 17.311438 loss_rnnt 15.209983 hw_loss 0.444130 lr 0.00049512 rank 0
2023-02-23 00:43:34,148 DEBUG TRAIN Batch 12/1900 loss 14.144190 loss_att 18.661570 loss_ctc 18.695148 loss_rnnt 12.350420 hw_loss 0.531561 lr 0.00049507 rank 6
2023-02-23 00:43:34,153 DEBUG TRAIN Batch 12/1900 loss 16.484867 loss_att 15.804592 loss_ctc 18.059822 loss_rnnt 16.106689 hw_loss 0.570447 lr 0.00049504 rank 2
2023-02-23 00:44:46,684 DEBUG TRAIN Batch 12/2000 loss 20.087671 loss_att 26.002647 loss_ctc 25.174847 loss_rnnt 18.055969 hw_loss 0.319534 lr 0.00049479 rank 7
2023-02-23 00:44:46,686 DEBUG TRAIN Batch 12/2000 loss 13.868221 loss_att 16.971710 loss_ctc 12.743568 loss_rnnt 13.159060 hw_loss 0.447033 lr 0.00049482 rank 4
2023-02-23 00:44:46,688 DEBUG TRAIN Batch 12/2000 loss 5.404093 loss_att 12.044661 loss_ctc 5.913760 loss_rnnt 3.759218 hw_loss 0.466512 lr 0.00049483 rank 6
2023-02-23 00:44:46,688 DEBUG TRAIN Batch 12/2000 loss 40.372738 loss_att 45.434467 loss_ctc 48.905113 loss_rnnt 37.977867 hw_loss 0.459133 lr 0.00049481 rank 3
2023-02-23 00:44:46,694 DEBUG TRAIN Batch 12/2000 loss 15.790923 loss_att 24.867336 loss_ctc 18.630238 loss_rnnt 13.392703 hw_loss 0.383179 lr 0.00049481 rank 5
2023-02-23 00:44:46,694 DEBUG TRAIN Batch 12/2000 loss 21.484064 loss_att 24.629225 loss_ctc 20.043993 loss_rnnt 20.865761 hw_loss 0.339897 lr 0.00049480 rank 2
2023-02-23 00:44:46,695 DEBUG TRAIN Batch 12/2000 loss 30.290512 loss_att 31.660461 loss_ctc 39.408195 loss_rnnt 28.583645 hw_loss 0.407230 lr 0.00049487 rank 0
2023-02-23 00:44:46,742 DEBUG TRAIN Batch 12/2000 loss 6.987527 loss_att 13.335224 loss_ctc 6.867343 loss_rnnt 5.464219 hw_loss 0.505861 lr 0.00049486 rank 1
2023-02-23 00:46:02,552 DEBUG TRAIN Batch 12/2100 loss 15.647647 loss_att 18.803959 loss_ctc 16.597507 loss_rnnt 14.698883 hw_loss 0.357850 lr 0.00049459 rank 6
2023-02-23 00:46:02,564 DEBUG TRAIN Batch 12/2100 loss 11.148703 loss_att 16.356981 loss_ctc 13.624760 loss_rnnt 9.617072 hw_loss 0.299688 lr 0.00049462 rank 1
2023-02-23 00:46:02,565 DEBUG TRAIN Batch 12/2100 loss 31.279058 loss_att 33.986286 loss_ctc 31.203430 loss_rnnt 30.553101 hw_loss 0.364871 lr 0.00049463 rank 0
2023-02-23 00:46:02,565 DEBUG TRAIN Batch 12/2100 loss 21.925142 loss_att 27.354256 loss_ctc 26.172142 loss_rnnt 20.060604 hw_loss 0.398345 lr 0.00049457 rank 4
2023-02-23 00:46:02,568 DEBUG TRAIN Batch 12/2100 loss 6.882530 loss_att 11.364735 loss_ctc 11.538891 loss_rnnt 5.103144 hw_loss 0.491433 lr 0.00049455 rank 7
2023-02-23 00:46:02,577 DEBUG TRAIN Batch 12/2100 loss 11.640942 loss_att 17.160608 loss_ctc 16.829191 loss_rnnt 9.612541 hw_loss 0.436310 lr 0.00049456 rank 2
2023-02-23 00:46:02,584 DEBUG TRAIN Batch 12/2100 loss 20.979918 loss_att 23.594030 loss_ctc 24.614868 loss_rnnt 19.782440 hw_loss 0.356239 lr 0.00049457 rank 3
2023-02-23 00:46:02,585 DEBUG TRAIN Batch 12/2100 loss 12.970635 loss_att 20.564331 loss_ctc 17.629366 loss_rnnt 10.580936 hw_loss 0.468367 lr 0.00049457 rank 5
2023-02-23 00:47:16,337 DEBUG TRAIN Batch 12/2200 loss 16.062576 loss_att 20.648621 loss_ctc 17.422817 loss_rnnt 14.740716 hw_loss 0.418659 lr 0.00049431 rank 7
2023-02-23 00:47:16,338 DEBUG TRAIN Batch 12/2200 loss 10.014075 loss_att 16.458065 loss_ctc 8.630730 loss_rnnt 8.653908 hw_loss 0.479654 lr 0.00049434 rank 6
2023-02-23 00:47:16,340 DEBUG TRAIN Batch 12/2200 loss 6.414200 loss_att 10.842187 loss_ctc 8.156755 loss_rnnt 5.055580 hw_loss 0.451279 lr 0.00049438 rank 1
2023-02-23 00:47:16,341 DEBUG TRAIN Batch 12/2200 loss 7.142678 loss_att 11.584854 loss_ctc 6.637851 loss_rnnt 6.125157 hw_loss 0.368241 lr 0.00049439 rank 0
2023-02-23 00:47:16,341 DEBUG TRAIN Batch 12/2200 loss 7.442440 loss_att 13.283072 loss_ctc 8.622358 loss_rnnt 5.877874 hw_loss 0.448343 lr 0.00049433 rank 3
2023-02-23 00:47:16,348 DEBUG TRAIN Batch 12/2200 loss 8.022260 loss_att 12.399660 loss_ctc 10.325273 loss_rnnt 6.655073 hw_loss 0.346195 lr 0.00049433 rank 5
2023-02-23 00:47:16,350 DEBUG TRAIN Batch 12/2200 loss 25.168840 loss_att 26.816433 loss_ctc 30.029003 loss_rnnt 23.997236 hw_loss 0.363867 lr 0.00049431 rank 2
2023-02-23 00:47:16,387 DEBUG TRAIN Batch 12/2200 loss 21.142157 loss_att 26.988192 loss_ctc 29.211252 loss_rnnt 18.697880 hw_loss 0.373484 lr 0.00049433 rank 4
2023-02-23 00:48:29,482 DEBUG TRAIN Batch 12/2300 loss 21.176537 loss_att 25.030148 loss_ctc 24.820835 loss_rnnt 19.707872 hw_loss 0.397562 lr 0.00049407 rank 7
2023-02-23 00:48:29,502 DEBUG TRAIN Batch 12/2300 loss 14.330660 loss_att 15.762818 loss_ctc 17.528528 loss_rnnt 13.413969 hw_loss 0.382270 lr 0.00049414 rank 1
2023-02-23 00:48:29,504 DEBUG TRAIN Batch 12/2300 loss 22.689821 loss_att 25.564772 loss_ctc 26.944832 loss_rnnt 21.332415 hw_loss 0.403275 lr 0.00049407 rank 2
2023-02-23 00:48:29,504 DEBUG TRAIN Batch 12/2300 loss 22.519516 loss_att 26.598221 loss_ctc 30.230705 loss_rnnt 20.456886 hw_loss 0.410116 lr 0.00049409 rank 4
2023-02-23 00:48:29,506 DEBUG TRAIN Batch 12/2300 loss 16.000544 loss_att 22.678905 loss_ctc 18.856577 loss_rnnt 14.076700 hw_loss 0.388814 lr 0.00049409 rank 5
2023-02-23 00:48:29,508 DEBUG TRAIN Batch 12/2300 loss 21.305441 loss_att 22.168009 loss_ctc 23.060911 loss_rnnt 20.639738 hw_loss 0.485862 lr 0.00049409 rank 3
2023-02-23 00:48:29,508 DEBUG TRAIN Batch 12/2300 loss 18.074520 loss_att 23.703838 loss_ctc 24.634819 loss_rnnt 15.858132 hw_loss 0.404656 lr 0.00049410 rank 6
2023-02-23 00:48:29,511 DEBUG TRAIN Batch 12/2300 loss 19.661667 loss_att 21.967920 loss_ctc 24.406252 loss_rnnt 18.384468 hw_loss 0.343751 lr 0.00049415 rank 0
2023-02-23 00:49:42,677 DEBUG TRAIN Batch 12/2400 loss 22.470522 loss_att 30.187449 loss_ctc 28.153286 loss_rnnt 19.931868 hw_loss 0.445439 lr 0.00049383 rank 7
2023-02-23 00:49:42,678 DEBUG TRAIN Batch 12/2400 loss 7.070334 loss_att 7.829506 loss_ctc 7.544363 loss_rnnt 6.631639 hw_loss 0.419356 lr 0.00049385 rank 3
2023-02-23 00:49:42,681 DEBUG TRAIN Batch 12/2400 loss 27.006956 loss_att 26.658340 loss_ctc 29.804754 loss_rnnt 26.452896 hw_loss 0.470142 lr 0.00049390 rank 1
2023-02-23 00:49:42,683 DEBUG TRAIN Batch 12/2400 loss 18.260365 loss_att 21.625515 loss_ctc 22.526741 loss_rnnt 16.765247 hw_loss 0.474818 lr 0.00049385 rank 4
2023-02-23 00:49:42,691 DEBUG TRAIN Batch 12/2400 loss 14.831630 loss_att 18.879734 loss_ctc 18.072773 loss_rnnt 13.381510 hw_loss 0.390647 lr 0.00049383 rank 2
2023-02-23 00:49:42,705 DEBUG TRAIN Batch 12/2400 loss 14.345894 loss_att 18.560766 loss_ctc 14.691052 loss_rnnt 13.237105 hw_loss 0.412112 lr 0.00049391 rank 0
2023-02-23 00:49:42,706 DEBUG TRAIN Batch 12/2400 loss 18.870310 loss_att 22.886843 loss_ctc 19.977972 loss_rnnt 17.711243 hw_loss 0.390134 lr 0.00049385 rank 5
2023-02-23 00:49:42,716 DEBUG TRAIN Batch 12/2400 loss 22.763811 loss_att 27.084858 loss_ctc 26.829079 loss_rnnt 21.134148 hw_loss 0.418909 lr 0.00049386 rank 6
2023-02-23 00:50:58,910 DEBUG TRAIN Batch 12/2500 loss 25.833525 loss_att 30.145256 loss_ctc 30.548325 loss_rnnt 24.146845 hw_loss 0.366920 lr 0.00049359 rank 7
2023-02-23 00:50:58,916 DEBUG TRAIN Batch 12/2500 loss 15.053661 loss_att 16.950577 loss_ctc 16.171604 loss_rnnt 14.261127 hw_loss 0.495174 lr 0.00049366 rank 0
2023-02-23 00:50:58,916 DEBUG TRAIN Batch 12/2500 loss 11.880290 loss_att 13.419899 loss_ctc 16.531116 loss_rnnt 10.718471 hw_loss 0.438352 lr 0.00049361 rank 5
2023-02-23 00:50:58,919 DEBUG TRAIN Batch 12/2500 loss 17.912884 loss_att 18.207382 loss_ctc 21.239519 loss_rnnt 17.134157 hw_loss 0.518015 lr 0.00049361 rank 4
2023-02-23 00:50:58,919 DEBUG TRAIN Batch 12/2500 loss 14.788024 loss_att 16.149311 loss_ctc 17.755188 loss_rnnt 13.888155 hw_loss 0.434981 lr 0.00049366 rank 1
2023-02-23 00:50:58,921 DEBUG TRAIN Batch 12/2500 loss 20.222561 loss_att 25.161404 loss_ctc 23.108818 loss_rnnt 18.591444 hw_loss 0.484713 lr 0.00049362 rank 6
2023-02-23 00:50:58,923 DEBUG TRAIN Batch 12/2500 loss 16.790735 loss_att 14.685757 loss_ctc 19.743132 loss_rnnt 16.561655 hw_loss 0.480792 lr 0.00049359 rank 2
2023-02-23 00:50:58,931 DEBUG TRAIN Batch 12/2500 loss 8.964918 loss_att 13.697931 loss_ctc 10.058462 loss_rnnt 7.638723 hw_loss 0.438347 lr 0.00049361 rank 3
2023-02-23 00:52:12,218 DEBUG TRAIN Batch 12/2600 loss 22.233675 loss_att 25.689251 loss_ctc 27.623823 loss_rnnt 20.603407 hw_loss 0.413377 lr 0.00049337 rank 3
2023-02-23 00:52:12,220 DEBUG TRAIN Batch 12/2600 loss 14.871587 loss_att 22.044218 loss_ctc 17.328243 loss_rnnt 12.901613 hw_loss 0.389797 lr 0.00049335 rank 7
2023-02-23 00:52:12,226 DEBUG TRAIN Batch 12/2600 loss 17.474480 loss_att 23.923326 loss_ctc 17.817984 loss_rnnt 15.940227 hw_loss 0.372532 lr 0.00049337 rank 4
2023-02-23 00:52:12,227 DEBUG TRAIN Batch 12/2600 loss 19.262016 loss_att 23.882654 loss_ctc 21.511169 loss_rnnt 17.841827 hw_loss 0.367829 lr 0.00049341 rank 1
2023-02-23 00:52:12,228 DEBUG TRAIN Batch 12/2600 loss 11.264492 loss_att 14.458868 loss_ctc 14.172771 loss_rnnt 9.971419 hw_loss 0.499548 lr 0.00049338 rank 6
2023-02-23 00:52:12,232 DEBUG TRAIN Batch 12/2600 loss 6.525155 loss_att 9.779095 loss_ctc 10.179756 loss_rnnt 5.205164 hw_loss 0.341105 lr 0.00049335 rank 2
2023-02-23 00:52:12,234 DEBUG TRAIN Batch 12/2600 loss 18.689217 loss_att 18.529398 loss_ctc 19.747696 loss_rnnt 18.402105 hw_loss 0.333650 lr 0.00049337 rank 5
2023-02-23 00:52:12,271 DEBUG TRAIN Batch 12/2600 loss 18.199825 loss_att 17.128784 loss_ctc 21.281193 loss_rnnt 17.698715 hw_loss 0.570879 lr 0.00049342 rank 0
2023-02-23 00:53:26,078 DEBUG TRAIN Batch 12/2700 loss 20.606825 loss_att 31.593887 loss_ctc 21.496288 loss_rnnt 18.072037 hw_loss 0.410215 lr 0.00049311 rank 7
2023-02-23 00:53:26,084 DEBUG TRAIN Batch 12/2700 loss 9.220196 loss_att 15.049084 loss_ctc 15.750049 loss_rnnt 6.979169 hw_loss 0.383628 lr 0.00049318 rank 0
2023-02-23 00:53:26,085 DEBUG TRAIN Batch 12/2700 loss 12.332300 loss_att 16.586529 loss_ctc 13.981548 loss_rnnt 11.083191 hw_loss 0.334429 lr 0.00049313 rank 3
2023-02-23 00:53:26,085 DEBUG TRAIN Batch 12/2700 loss 19.945335 loss_att 24.185434 loss_ctc 27.618126 loss_rnnt 17.863773 hw_loss 0.394694 lr 0.00049313 rank 4
2023-02-23 00:53:26,085 DEBUG TRAIN Batch 12/2700 loss 14.917749 loss_att 20.116966 loss_ctc 19.472307 loss_rnnt 12.999205 hw_loss 0.508929 lr 0.00049317 rank 1
2023-02-23 00:53:26,087 DEBUG TRAIN Batch 12/2700 loss 9.961787 loss_att 18.337379 loss_ctc 14.098445 loss_rnnt 7.570025 hw_loss 0.309542 lr 0.00049314 rank 6
2023-02-23 00:53:26,089 DEBUG TRAIN Batch 12/2700 loss 14.826227 loss_att 20.805849 loss_ctc 20.481764 loss_rnnt 12.660830 hw_loss 0.403877 lr 0.00049311 rank 2
2023-02-23 00:53:26,096 DEBUG TRAIN Batch 12/2700 loss 13.006933 loss_att 18.722471 loss_ctc 11.036663 loss_rnnt 11.889937 hw_loss 0.443608 lr 0.00049313 rank 5
2023-02-23 00:54:41,004 DEBUG TRAIN Batch 12/2800 loss 13.542137 loss_att 14.053269 loss_ctc 13.939627 loss_rnnt 13.109462 hw_loss 0.520221 lr 0.00049289 rank 5
2023-02-23 00:54:41,016 DEBUG TRAIN Batch 12/2800 loss 19.972397 loss_att 20.479841 loss_ctc 23.740696 loss_rnnt 19.155973 hw_loss 0.398431 lr 0.00049287 rank 2
2023-02-23 00:54:41,018 DEBUG TRAIN Batch 12/2800 loss 5.321392 loss_att 10.706624 loss_ctc 8.133297 loss_rnnt 3.657453 hw_loss 0.397447 lr 0.00049287 rank 7
2023-02-23 00:54:41,019 DEBUG TRAIN Batch 12/2800 loss 7.185303 loss_att 10.592961 loss_ctc 7.825988 loss_rnnt 6.185112 hw_loss 0.437314 lr 0.00049294 rank 0
2023-02-23 00:54:41,020 DEBUG TRAIN Batch 12/2800 loss 11.904584 loss_att 17.421000 loss_ctc 15.111998 loss_rnnt 10.152817 hw_loss 0.414054 lr 0.00049294 rank 1
2023-02-23 00:54:41,024 DEBUG TRAIN Batch 12/2800 loss 16.706860 loss_att 18.304924 loss_ctc 28.867861 loss_rnnt 14.540813 hw_loss 0.421809 lr 0.00049289 rank 4
2023-02-23 00:54:41,025 DEBUG TRAIN Batch 12/2800 loss 12.455798 loss_att 18.927111 loss_ctc 12.550077 loss_rnnt 10.942646 hw_loss 0.386849 lr 0.00049290 rank 6
2023-02-23 00:54:41,046 DEBUG TRAIN Batch 12/2800 loss 26.785236 loss_att 29.096071 loss_ctc 31.765882 loss_rnnt 25.483849 hw_loss 0.328378 lr 0.00049289 rank 3
2023-02-23 00:55:55,417 DEBUG TRAIN Batch 12/2900 loss 13.277652 loss_att 16.011703 loss_ctc 14.181060 loss_rnnt 12.414864 hw_loss 0.366607 lr 0.00049271 rank 0
2023-02-23 00:55:55,418 DEBUG TRAIN Batch 12/2900 loss 17.322258 loss_att 18.788490 loss_ctc 21.186445 loss_rnnt 16.280165 hw_loss 0.438044 lr 0.00049265 rank 3
2023-02-23 00:55:55,418 DEBUG TRAIN Batch 12/2900 loss 11.990697 loss_att 14.862464 loss_ctc 14.691755 loss_rnnt 10.854233 hw_loss 0.378692 lr 0.00049270 rank 1
2023-02-23 00:55:55,420 DEBUG TRAIN Batch 12/2900 loss 19.772396 loss_att 25.365120 loss_ctc 24.946922 loss_rnnt 17.754675 hw_loss 0.392326 lr 0.00049263 rank 7
2023-02-23 00:55:55,421 DEBUG TRAIN Batch 12/2900 loss 25.587799 loss_att 25.830830 loss_ctc 27.309534 loss_rnnt 25.095022 hw_loss 0.402389 lr 0.00049265 rank 4
2023-02-23 00:55:55,421 DEBUG TRAIN Batch 12/2900 loss 21.598948 loss_att 27.076675 loss_ctc 25.361038 loss_rnnt 19.770283 hw_loss 0.434079 lr 0.00049266 rank 6
2023-02-23 00:55:55,428 DEBUG TRAIN Batch 12/2900 loss 21.388897 loss_att 25.352047 loss_ctc 31.426043 loss_rnnt 19.050074 hw_loss 0.389822 lr 0.00049263 rank 2
2023-02-23 00:55:55,429 DEBUG TRAIN Batch 12/2900 loss 11.717955 loss_att 16.252651 loss_ctc 16.056166 loss_rnnt 10.040619 hw_loss 0.359940 lr 0.00049265 rank 5
2023-02-23 00:57:08,364 DEBUG TRAIN Batch 12/3000 loss 9.310561 loss_att 10.115858 loss_ctc 9.530855 loss_rnnt 8.823611 hw_loss 0.555971 lr 0.00049241 rank 3
2023-02-23 00:57:08,386 DEBUG TRAIN Batch 12/3000 loss 19.514338 loss_att 21.386375 loss_ctc 27.759996 loss_rnnt 17.837963 hw_loss 0.379770 lr 0.00049239 rank 2
2023-02-23 00:57:08,387 DEBUG TRAIN Batch 12/3000 loss 11.909429 loss_att 15.741790 loss_ctc 16.732660 loss_rnnt 10.223261 hw_loss 0.518619 lr 0.00049246 rank 1
2023-02-23 00:57:08,388 DEBUG TRAIN Batch 12/3000 loss 35.662781 loss_att 40.280365 loss_ctc 44.424889 loss_rnnt 33.378479 hw_loss 0.360943 lr 0.00049247 rank 0
2023-02-23 00:57:08,389 DEBUG TRAIN Batch 12/3000 loss 13.321482 loss_att 18.488506 loss_ctc 19.402927 loss_rnnt 11.252558 hw_loss 0.421235 lr 0.00049239 rank 7
2023-02-23 00:57:08,389 DEBUG TRAIN Batch 12/3000 loss 25.736530 loss_att 26.503689 loss_ctc 27.837152 loss_rnnt 25.112356 hw_loss 0.357489 lr 0.00049241 rank 4
2023-02-23 00:57:08,394 DEBUG TRAIN Batch 12/3000 loss 18.337502 loss_att 25.522434 loss_ctc 26.966997 loss_rnnt 15.512934 hw_loss 0.444337 lr 0.00049242 rank 6
2023-02-23 00:57:08,441 DEBUG TRAIN Batch 12/3000 loss 12.202058 loss_att 18.515488 loss_ctc 15.322116 loss_rnnt 10.297901 hw_loss 0.422742 lr 0.00049241 rank 5
2023-02-23 00:58:23,679 DEBUG TRAIN Batch 12/3100 loss 13.010709 loss_att 17.850458 loss_ctc 15.270751 loss_rnnt 11.513756 hw_loss 0.426871 lr 0.00049222 rank 1
2023-02-23 00:58:23,691 DEBUG TRAIN Batch 12/3100 loss 12.652673 loss_att 18.476484 loss_ctc 15.031466 loss_rnnt 10.953345 hw_loss 0.407613 lr 0.00049215 rank 7
2023-02-23 00:58:23,692 DEBUG TRAIN Batch 12/3100 loss 16.290869 loss_att 15.111367 loss_ctc 20.418119 loss_rnnt 15.756114 hw_loss 0.413167 lr 0.00049217 rank 4
2023-02-23 00:58:23,694 DEBUG TRAIN Batch 12/3100 loss 23.979240 loss_att 28.475498 loss_ctc 28.006254 loss_rnnt 22.328672 hw_loss 0.401962 lr 0.00049217 rank 3
2023-02-23 00:58:23,694 DEBUG TRAIN Batch 12/3100 loss 26.971563 loss_att 31.208490 loss_ctc 31.535591 loss_rnnt 25.298161 hw_loss 0.407773 lr 0.00049218 rank 6
2023-02-23 00:58:23,697 DEBUG TRAIN Batch 12/3100 loss 11.680136 loss_att 15.626749 loss_ctc 13.229708 loss_rnnt 10.474285 hw_loss 0.393598 lr 0.00049223 rank 0
2023-02-23 00:58:23,698 DEBUG TRAIN Batch 12/3100 loss 7.272832 loss_att 9.846642 loss_ctc 10.256028 loss_rnnt 6.108483 hw_loss 0.472175 lr 0.00049217 rank 5
2023-02-23 00:58:23,699 DEBUG TRAIN Batch 12/3100 loss 23.273670 loss_att 22.980747 loss_ctc 24.444136 loss_rnnt 22.970068 hw_loss 0.386481 lr 0.00049215 rank 2
2023-02-23 00:59:40,844 DEBUG TRAIN Batch 12/3200 loss 19.275187 loss_att 20.248657 loss_ctc 22.235844 loss_rnnt 18.415543 hw_loss 0.506614 lr 0.00049199 rank 0
2023-02-23 00:59:40,845 DEBUG TRAIN Batch 12/3200 loss 11.709534 loss_att 10.595449 loss_ctc 14.836304 loss_rnnt 11.204422 hw_loss 0.583173 lr 0.00049192 rank 2
2023-02-23 00:59:40,845 DEBUG TRAIN Batch 12/3200 loss 21.689560 loss_att 30.210154 loss_ctc 27.379185 loss_rnnt 19.046055 hw_loss 0.338948 lr 0.00049193 rank 3
2023-02-23 00:59:40,849 DEBUG TRAIN Batch 12/3200 loss 21.492636 loss_att 21.111660 loss_ctc 26.473198 loss_rnnt 20.671150 hw_loss 0.438008 lr 0.00049191 rank 7
2023-02-23 00:59:40,865 DEBUG TRAIN Batch 12/3200 loss 26.721497 loss_att 28.452467 loss_ctc 33.578403 loss_rnnt 25.274664 hw_loss 0.349474 lr 0.00049195 rank 6
2023-02-23 00:59:40,870 DEBUG TRAIN Batch 12/3200 loss 26.871346 loss_att 28.228199 loss_ctc 29.667158 loss_rnnt 26.005545 hw_loss 0.415599 lr 0.00049193 rank 5
2023-02-23 00:59:40,892 DEBUG TRAIN Batch 12/3200 loss 17.939295 loss_att 18.975033 loss_ctc 17.961782 loss_rnnt 17.419838 hw_loss 0.579958 lr 0.00049198 rank 1
2023-02-23 00:59:40,911 DEBUG TRAIN Batch 12/3200 loss 9.350973 loss_att 13.522272 loss_ctc 14.901077 loss_rnnt 7.572555 hw_loss 0.382773 lr 0.00049193 rank 4
2023-02-23 01:00:54,243 DEBUG TRAIN Batch 12/3300 loss 11.082193 loss_att 14.656941 loss_ctc 14.666430 loss_rnnt 9.690287 hw_loss 0.373235 lr 0.00049167 rank 7
2023-02-23 01:00:54,250 DEBUG TRAIN Batch 12/3300 loss 25.686085 loss_att 32.052589 loss_ctc 28.845379 loss_rnnt 23.814987 hw_loss 0.331045 lr 0.00049169 rank 3
2023-02-23 01:00:54,253 DEBUG TRAIN Batch 12/3300 loss 8.727394 loss_att 14.235487 loss_ctc 13.904106 loss_rnnt 6.734939 hw_loss 0.376138 lr 0.00049169 rank 5
2023-02-23 01:00:54,252 DEBUG TRAIN Batch 12/3300 loss 10.759732 loss_att 12.206657 loss_ctc 12.132605 loss_rnnt 10.052782 hw_loss 0.439717 lr 0.00049170 rank 4
2023-02-23 01:00:54,253 DEBUG TRAIN Batch 12/3300 loss 5.808369 loss_att 9.680429 loss_ctc 6.682573 loss_rnnt 4.714310 hw_loss 0.380788 lr 0.00049175 rank 0
2023-02-23 01:00:54,254 DEBUG TRAIN Batch 12/3300 loss 12.077857 loss_att 15.093657 loss_ctc 12.838008 loss_rnnt 11.140724 hw_loss 0.436161 lr 0.00049174 rank 1
2023-02-23 01:00:54,264 DEBUG TRAIN Batch 12/3300 loss 19.559576 loss_att 22.584932 loss_ctc 29.594061 loss_rnnt 17.426697 hw_loss 0.356019 lr 0.00049168 rank 2
2023-02-23 01:00:54,303 DEBUG TRAIN Batch 12/3300 loss 13.627838 loss_att 12.731246 loss_ctc 16.038792 loss_rnnt 13.153182 hw_loss 0.623464 lr 0.00049171 rank 6
2023-02-23 01:02:07,447 DEBUG TRAIN Batch 12/3400 loss 13.081774 loss_att 18.694958 loss_ctc 12.748388 loss_rnnt 11.802559 hw_loss 0.376929 lr 0.00049144 rank 7
2023-02-23 01:02:07,450 DEBUG TRAIN Batch 12/3400 loss 23.102617 loss_att 26.455723 loss_ctc 33.577675 loss_rnnt 20.814285 hw_loss 0.414444 lr 0.00049150 rank 1
2023-02-23 01:02:07,451 DEBUG TRAIN Batch 12/3400 loss 14.340979 loss_att 16.285639 loss_ctc 17.848797 loss_rnnt 13.308922 hw_loss 0.328904 lr 0.00049146 rank 5
2023-02-23 01:02:07,450 DEBUG TRAIN Batch 12/3400 loss 16.941031 loss_att 19.355761 loss_ctc 15.970959 loss_rnnt 16.373459 hw_loss 0.401190 lr 0.00049146 rank 4
2023-02-23 01:02:07,451 DEBUG TRAIN Batch 12/3400 loss 18.338137 loss_att 20.513817 loss_ctc 20.532074 loss_rnnt 17.401001 hw_loss 0.392764 lr 0.00049146 rank 3
2023-02-23 01:02:07,455 DEBUG TRAIN Batch 12/3400 loss 24.534533 loss_att 27.782478 loss_ctc 34.795879 loss_rnnt 22.350094 hw_loss 0.312507 lr 0.00049151 rank 0
2023-02-23 01:02:07,458 DEBUG TRAIN Batch 12/3400 loss 15.730521 loss_att 20.524328 loss_ctc 16.421354 loss_rnnt 14.465214 hw_loss 0.402063 lr 0.00049144 rank 2
2023-02-23 01:02:07,503 DEBUG TRAIN Batch 12/3400 loss 17.066751 loss_att 24.635944 loss_ctc 20.349302 loss_rnnt 14.870047 hw_loss 0.459738 lr 0.00049147 rank 6
2023-02-23 01:03:22,273 DEBUG TRAIN Batch 12/3500 loss 12.151023 loss_att 16.654087 loss_ctc 19.158415 loss_rnnt 10.092917 hw_loss 0.418450 lr 0.00049122 rank 5
2023-02-23 01:03:22,275 DEBUG TRAIN Batch 12/3500 loss 22.365700 loss_att 22.739939 loss_ctc 25.210030 loss_rnnt 21.697636 hw_loss 0.401190 lr 0.00049127 rank 1
2023-02-23 01:03:22,277 DEBUG TRAIN Batch 12/3500 loss 20.898247 loss_att 25.782768 loss_ctc 23.610733 loss_rnnt 19.346466 hw_loss 0.399769 lr 0.00049120 rank 7
2023-02-23 01:03:22,279 DEBUG TRAIN Batch 12/3500 loss 21.297501 loss_att 26.146025 loss_ctc 24.491714 loss_rnnt 19.695894 hw_loss 0.386261 lr 0.00049122 rank 3
2023-02-23 01:03:22,279 DEBUG TRAIN Batch 12/3500 loss 5.247728 loss_att 10.659925 loss_ctc 4.438612 loss_rnnt 4.103840 hw_loss 0.317495 lr 0.00049123 rank 6
2023-02-23 01:03:22,281 DEBUG TRAIN Batch 12/3500 loss 16.705797 loss_att 20.528145 loss_ctc 21.602827 loss_rnnt 15.046786 hw_loss 0.453006 lr 0.00049120 rank 2
2023-02-23 01:03:22,282 DEBUG TRAIN Batch 12/3500 loss 15.086480 loss_att 16.134933 loss_ctc 15.115511 loss_rnnt 14.643470 hw_loss 0.430218 lr 0.00049128 rank 0
2023-02-23 01:03:22,294 DEBUG TRAIN Batch 12/3500 loss 5.277658 loss_att 10.199614 loss_ctc 6.473912 loss_rnnt 3.914964 hw_loss 0.410252 lr 0.00049122 rank 4
2023-02-23 01:04:37,122 DEBUG TRAIN Batch 12/3600 loss 17.424980 loss_att 20.246746 loss_ctc 19.924889 loss_rnnt 16.268135 hw_loss 0.485945 lr 0.00049096 rank 7
2023-02-23 01:04:37,130 DEBUG TRAIN Batch 12/3600 loss 29.808407 loss_att 28.141172 loss_ctc 30.700308 loss_rnnt 29.822601 hw_loss 0.375623 lr 0.00049103 rank 1
2023-02-23 01:04:37,130 DEBUG TRAIN Batch 12/3600 loss 12.068567 loss_att 14.732643 loss_ctc 15.433845 loss_rnnt 10.867701 hw_loss 0.411277 lr 0.00049104 rank 0
2023-02-23 01:04:37,131 DEBUG TRAIN Batch 12/3600 loss 12.351841 loss_att 14.999038 loss_ctc 14.756880 loss_rnnt 11.322038 hw_loss 0.336922 lr 0.00049097 rank 2
2023-02-23 01:04:37,132 DEBUG TRAIN Batch 12/3600 loss 12.888039 loss_att 18.851273 loss_ctc 18.682699 loss_rnnt 10.707652 hw_loss 0.403348 lr 0.00049098 rank 3
2023-02-23 01:04:37,133 DEBUG TRAIN Batch 12/3600 loss 40.220486 loss_att 44.824745 loss_ctc 48.319862 loss_rnnt 37.991661 hw_loss 0.427596 lr 0.00049100 rank 6
2023-02-23 01:04:37,134 DEBUG TRAIN Batch 12/3600 loss 24.734262 loss_att 29.722681 loss_ctc 29.947611 loss_rnnt 22.809780 hw_loss 0.434412 lr 0.00049098 rank 5
2023-02-23 01:04:37,135 DEBUG TRAIN Batch 12/3600 loss 20.976427 loss_att 23.648853 loss_ctc 26.934860 loss_rnnt 19.422781 hw_loss 0.421319 lr 0.00049098 rank 4
2023-02-23 01:05:50,135 DEBUG TRAIN Batch 12/3700 loss 10.226611 loss_att 14.830878 loss_ctc 11.092442 loss_rnnt 8.959635 hw_loss 0.432521 lr 0.00049076 rank 6
2023-02-23 01:05:50,137 DEBUG TRAIN Batch 12/3700 loss 9.093438 loss_att 13.739717 loss_ctc 10.279016 loss_rnnt 7.798383 hw_loss 0.389478 lr 0.00049080 rank 0
2023-02-23 01:05:50,137 DEBUG TRAIN Batch 12/3700 loss 27.752789 loss_att 35.479740 loss_ctc 39.420898 loss_rnnt 24.320644 hw_loss 0.620644 lr 0.00049072 rank 7
2023-02-23 01:05:50,137 DEBUG TRAIN Batch 12/3700 loss 20.769312 loss_att 24.397301 loss_ctc 24.679472 loss_rnnt 19.263338 hw_loss 0.485663 lr 0.00049075 rank 4
2023-02-23 01:05:50,140 DEBUG TRAIN Batch 12/3700 loss 11.944396 loss_att 16.559500 loss_ctc 12.918280 loss_rnnt 10.686509 hw_loss 0.384403 lr 0.00049073 rank 2
2023-02-23 01:05:50,141 DEBUG TRAIN Batch 12/3700 loss 9.759607 loss_att 14.493502 loss_ctc 9.120689 loss_rnnt 8.760900 hw_loss 0.257096 lr 0.00049075 rank 3
2023-02-23 01:05:50,141 DEBUG TRAIN Batch 12/3700 loss 17.516314 loss_att 20.799850 loss_ctc 17.272062 loss_rnnt 16.679502 hw_loss 0.398759 lr 0.00049075 rank 5
2023-02-23 01:05:50,142 DEBUG TRAIN Batch 12/3700 loss 14.290366 loss_att 18.703545 loss_ctc 17.343096 loss_rnnt 12.798227 hw_loss 0.379635 lr 0.00049079 rank 1
2023-02-23 01:07:03,977 DEBUG TRAIN Batch 12/3800 loss 8.087257 loss_att 11.885399 loss_ctc 10.403625 loss_rnnt 6.825146 hw_loss 0.363065 lr 0.00049051 rank 3
2023-02-23 01:07:03,977 DEBUG TRAIN Batch 12/3800 loss 12.130548 loss_att 12.673905 loss_ctc 14.764609 loss_rnnt 11.380472 hw_loss 0.544116 lr 0.00049051 rank 4
2023-02-23 01:07:03,980 DEBUG TRAIN Batch 12/3800 loss 14.217865 loss_att 15.453857 loss_ctc 17.668343 loss_rnnt 13.300658 hw_loss 0.393646 lr 0.00049049 rank 7
2023-02-23 01:07:03,982 DEBUG TRAIN Batch 12/3800 loss 10.247321 loss_att 10.934529 loss_ctc 11.135169 loss_rnnt 9.592606 hw_loss 0.747927 lr 0.00049051 rank 5
2023-02-23 01:07:03,983 DEBUG TRAIN Batch 12/3800 loss 9.138206 loss_att 10.243578 loss_ctc 11.049763 loss_rnnt 8.377287 hw_loss 0.534318 lr 0.00049057 rank 0
2023-02-23 01:07:03,986 DEBUG TRAIN Batch 12/3800 loss 13.036021 loss_att 13.956363 loss_ctc 15.682256 loss_rnnt 12.252420 hw_loss 0.462564 lr 0.00049049 rank 2
2023-02-23 01:07:03,988 DEBUG TRAIN Batch 12/3800 loss 11.391487 loss_att 12.815262 loss_ctc 12.279797 loss_rnnt 10.705447 hw_loss 0.530334 lr 0.00049056 rank 1
2023-02-23 01:07:04,028 DEBUG TRAIN Batch 12/3800 loss 9.582157 loss_att 12.224691 loss_ctc 14.993362 loss_rnnt 8.043305 hw_loss 0.541594 lr 0.00049052 rank 6
2023-02-23 01:08:20,248 DEBUG TRAIN Batch 12/3900 loss 4.831191 loss_att 9.961790 loss_ctc 3.917839 loss_rnnt 3.713936 hw_loss 0.399214 lr 0.00049025 rank 7
2023-02-23 01:08:20,253 DEBUG TRAIN Batch 12/3900 loss 5.767209 loss_att 10.837720 loss_ctc 7.760482 loss_rnnt 4.172480 hw_loss 0.590356 lr 0.00049027 rank 5
2023-02-23 01:08:20,254 DEBUG TRAIN Batch 12/3900 loss 25.079235 loss_att 32.679813 loss_ctc 30.690306 loss_rnnt 22.634933 hw_loss 0.330079 lr 0.00049027 rank 3
2023-02-23 01:08:20,254 DEBUG TRAIN Batch 12/3900 loss 18.740265 loss_att 21.204124 loss_ctc 20.909000 loss_rnnt 17.733879 hw_loss 0.420840 lr 0.00049028 rank 4
2023-02-23 01:08:20,255 DEBUG TRAIN Batch 12/3900 loss 13.020643 loss_att 13.750932 loss_ctc 16.119431 loss_rnnt 12.163104 hw_loss 0.559330 lr 0.00049032 rank 1
2023-02-23 01:08:20,256 DEBUG TRAIN Batch 12/3900 loss 22.322657 loss_att 28.819878 loss_ctc 24.761868 loss_rnnt 20.508511 hw_loss 0.355256 lr 0.00049033 rank 0
2023-02-23 01:08:20,276 DEBUG TRAIN Batch 12/3900 loss 14.948858 loss_att 17.606453 loss_ctc 21.530910 loss_rnnt 13.297835 hw_loss 0.453556 lr 0.00049029 rank 6
2023-02-23 01:08:20,311 DEBUG TRAIN Batch 12/3900 loss 20.094032 loss_att 26.066948 loss_ctc 24.361832 loss_rnnt 18.127371 hw_loss 0.380700 lr 0.00049026 rank 2
2023-02-23 01:09:33,507 DEBUG TRAIN Batch 12/4000 loss 13.502652 loss_att 19.973244 loss_ctc 16.383392 loss_rnnt 11.598576 hw_loss 0.423486 lr 0.00049002 rank 7
2023-02-23 01:09:33,507 DEBUG TRAIN Batch 12/4000 loss 20.443363 loss_att 21.517052 loss_ctc 20.920322 loss_rnnt 19.973278 hw_loss 0.359536 lr 0.00049009 rank 1
2023-02-23 01:09:33,511 DEBUG TRAIN Batch 12/4000 loss 24.371849 loss_att 31.481712 loss_ctc 35.476997 loss_rnnt 21.266096 hw_loss 0.380804 lr 0.00049009 rank 0
2023-02-23 01:09:33,513 DEBUG TRAIN Batch 12/4000 loss 15.503107 loss_att 18.904270 loss_ctc 22.639910 loss_rnnt 13.643064 hw_loss 0.427943 lr 0.00049004 rank 3
2023-02-23 01:09:33,516 DEBUG TRAIN Batch 12/4000 loss 17.464142 loss_att 30.001431 loss_ctc 23.918118 loss_rnnt 13.926313 hw_loss 0.318453 lr 0.00049002 rank 2
2023-02-23 01:09:33,518 DEBUG TRAIN Batch 12/4000 loss 10.459955 loss_att 15.153111 loss_ctc 11.451454 loss_rnnt 9.172134 hw_loss 0.406856 lr 0.00049004 rank 5
2023-02-23 01:09:33,542 DEBUG TRAIN Batch 12/4000 loss 10.676080 loss_att 19.943064 loss_ctc 11.887748 loss_rnnt 8.453854 hw_loss 0.388638 lr 0.00049005 rank 6
2023-02-23 01:09:33,549 DEBUG TRAIN Batch 12/4000 loss 13.891049 loss_att 20.045061 loss_ctc 18.563828 loss_rnnt 11.823242 hw_loss 0.401190 lr 0.00049004 rank 4
2023-02-23 01:10:46,678 DEBUG TRAIN Batch 12/4100 loss 30.667057 loss_att 31.267941 loss_ctc 36.317135 loss_rnnt 29.579880 hw_loss 0.400607 lr 0.00048978 rank 7
2023-02-23 01:10:46,687 DEBUG TRAIN Batch 12/4100 loss 14.763366 loss_att 15.626778 loss_ctc 15.761057 loss_rnnt 14.222212 hw_loss 0.441462 lr 0.00048980 rank 3
2023-02-23 01:10:46,687 DEBUG TRAIN Batch 12/4100 loss 19.399517 loss_att 26.206526 loss_ctc 23.974697 loss_rnnt 17.221565 hw_loss 0.387235 lr 0.00048985 rank 1
2023-02-23 01:10:46,688 DEBUG TRAIN Batch 12/4100 loss 16.964399 loss_att 20.638351 loss_ctc 19.781761 loss_rnnt 15.655024 hw_loss 0.373006 lr 0.00048981 rank 4
2023-02-23 01:10:46,691 DEBUG TRAIN Batch 12/4100 loss 10.252849 loss_att 15.133207 loss_ctc 12.941666 loss_rnnt 8.685356 hw_loss 0.436708 lr 0.00048980 rank 5
2023-02-23 01:10:46,691 DEBUG TRAIN Batch 12/4100 loss 12.533906 loss_att 16.522156 loss_ctc 17.946115 loss_rnnt 10.804913 hw_loss 0.393216 lr 0.00048979 rank 2
2023-02-23 01:10:46,693 DEBUG TRAIN Batch 12/4100 loss 16.479870 loss_att 20.323431 loss_ctc 16.351700 loss_rnnt 15.513746 hw_loss 0.402187 lr 0.00048986 rank 0
2023-02-23 01:10:46,694 DEBUG TRAIN Batch 12/4100 loss 22.141870 loss_att 21.960020 loss_ctc 28.746725 loss_rnnt 21.065002 hw_loss 0.436112 lr 0.00048982 rank 6
2023-02-23 01:12:01,080 DEBUG TRAIN Batch 12/4200 loss 11.840095 loss_att 16.044327 loss_ctc 12.386326 loss_rnnt 10.750388 hw_loss 0.330054 lr 0.00048958 rank 6
2023-02-23 01:12:01,087 DEBUG TRAIN Batch 12/4200 loss 19.879614 loss_att 23.227343 loss_ctc 24.017822 loss_rnnt 18.465420 hw_loss 0.361661 lr 0.00048955 rank 2
2023-02-23 01:12:01,090 DEBUG TRAIN Batch 12/4200 loss 8.640955 loss_att 13.604234 loss_ctc 13.032214 loss_rnnt 6.844045 hw_loss 0.410160 lr 0.00048957 rank 3
2023-02-23 01:12:01,096 DEBUG TRAIN Batch 12/4200 loss 21.451488 loss_att 22.136538 loss_ctc 24.075073 loss_rnnt 20.741489 hw_loss 0.418458 lr 0.00048955 rank 7
2023-02-23 01:12:01,097 DEBUG TRAIN Batch 12/4200 loss 11.035205 loss_att 13.446877 loss_ctc 12.369652 loss_rnnt 10.180385 hw_loss 0.364800 lr 0.00048957 rank 4
2023-02-23 01:12:01,100 DEBUG TRAIN Batch 12/4200 loss 14.898033 loss_att 18.120995 loss_ctc 15.257959 loss_rnnt 13.982744 hw_loss 0.417573 lr 0.00048962 rank 0
2023-02-23 01:12:01,102 DEBUG TRAIN Batch 12/4200 loss 10.369740 loss_att 14.429464 loss_ctc 14.870784 loss_rnnt 8.711132 hw_loss 0.462234 lr 0.00048962 rank 1
2023-02-23 01:12:01,113 DEBUG TRAIN Batch 12/4200 loss 11.291511 loss_att 21.913139 loss_ctc 14.598410 loss_rnnt 8.553066 hw_loss 0.324748 lr 0.00048957 rank 5
2023-02-23 01:13:16,308 DEBUG TRAIN Batch 12/4300 loss 10.230519 loss_att 12.670942 loss_ctc 11.786906 loss_rnnt 9.357100 hw_loss 0.333407 lr 0.00048935 rank 6
2023-02-23 01:13:16,308 DEBUG TRAIN Batch 12/4300 loss 11.619139 loss_att 16.376202 loss_ctc 17.725903 loss_rnnt 9.678470 hw_loss 0.328164 lr 0.00048934 rank 4
2023-02-23 01:13:16,308 DEBUG TRAIN Batch 12/4300 loss 19.192923 loss_att 24.298794 loss_ctc 26.995306 loss_rnnt 16.847872 hw_loss 0.531668 lr 0.00048931 rank 7
2023-02-23 01:13:16,309 DEBUG TRAIN Batch 12/4300 loss 14.524580 loss_att 14.972519 loss_ctc 16.821178 loss_rnnt 13.933841 hw_loss 0.365509 lr 0.00048932 rank 2
2023-02-23 01:13:16,311 DEBUG TRAIN Batch 12/4300 loss 32.366261 loss_att 35.098896 loss_ctc 35.717793 loss_rnnt 31.152538 hw_loss 0.413115 lr 0.00048938 rank 1
2023-02-23 01:13:16,312 DEBUG TRAIN Batch 12/4300 loss 15.889342 loss_att 17.012846 loss_ctc 21.286839 loss_rnnt 14.717717 hw_loss 0.426109 lr 0.00048939 rank 0
2023-02-23 01:13:16,312 DEBUG TRAIN Batch 12/4300 loss 22.258877 loss_att 26.040920 loss_ctc 25.685041 loss_rnnt 20.832804 hw_loss 0.399080 lr 0.00048933 rank 3
2023-02-23 01:13:16,314 DEBUG TRAIN Batch 12/4300 loss 10.654378 loss_att 16.807793 loss_ctc 16.637188 loss_rnnt 8.410975 hw_loss 0.403148 lr 0.00048933 rank 5
2023-02-23 01:14:29,248 DEBUG TRAIN Batch 12/4400 loss 18.499081 loss_att 25.495695 loss_ctc 28.249523 loss_rnnt 15.585498 hw_loss 0.401624 lr 0.00048910 rank 3
2023-02-23 01:14:29,250 DEBUG TRAIN Batch 12/4400 loss 14.197727 loss_att 17.589844 loss_ctc 15.106521 loss_rnnt 13.156492 hw_loss 0.453075 lr 0.00048910 rank 4
2023-02-23 01:14:29,250 DEBUG TRAIN Batch 12/4400 loss 14.803910 loss_att 16.329710 loss_ctc 16.955442 loss_rnnt 13.967244 hw_loss 0.458692 lr 0.00048908 rank 7
2023-02-23 01:14:29,251 DEBUG TRAIN Batch 12/4400 loss 18.568989 loss_att 22.002333 loss_ctc 22.566420 loss_rnnt 17.112627 hw_loss 0.443817 lr 0.00048916 rank 0
2023-02-23 01:14:29,255 DEBUG TRAIN Batch 12/4400 loss 15.395323 loss_att 18.044502 loss_ctc 21.450737 loss_rnnt 13.849899 hw_loss 0.390371 lr 0.00048915 rank 1
2023-02-23 01:14:29,257 DEBUG TRAIN Batch 12/4400 loss 19.594025 loss_att 20.498133 loss_ctc 27.774979 loss_rnnt 18.111582 hw_loss 0.395300 lr 0.00048911 rank 6
2023-02-23 01:14:29,256 DEBUG TRAIN Batch 12/4400 loss 18.074926 loss_att 17.702854 loss_ctc 21.236923 loss_rnnt 17.482994 hw_loss 0.458900 lr 0.00048910 rank 5
2023-02-23 01:14:29,300 DEBUG TRAIN Batch 12/4400 loss 11.344945 loss_att 13.369621 loss_ctc 15.090884 loss_rnnt 10.211479 hw_loss 0.429509 lr 0.00048908 rank 2
2023-02-23 01:15:42,774 DEBUG TRAIN Batch 12/4500 loss 24.957846 loss_att 29.449764 loss_ctc 28.097210 loss_rnnt 23.417723 hw_loss 0.418420 lr 0.00048887 rank 3
2023-02-23 01:15:42,782 DEBUG TRAIN Batch 12/4500 loss 14.824102 loss_att 14.677755 loss_ctc 15.083964 loss_rnnt 14.539952 hw_loss 0.522695 lr 0.00048884 rank 7
2023-02-23 01:15:42,786 DEBUG TRAIN Batch 12/4500 loss 9.343660 loss_att 10.855945 loss_ctc 12.013741 loss_rnnt 8.483065 hw_loss 0.378990 lr 0.00048891 rank 1
2023-02-23 01:15:42,788 DEBUG TRAIN Batch 12/4500 loss 11.790442 loss_att 16.491184 loss_ctc 12.238262 loss_rnnt 10.580719 hw_loss 0.393495 lr 0.00048887 rank 4
2023-02-23 01:15:42,789 DEBUG TRAIN Batch 12/4500 loss 14.893320 loss_att 13.850960 loss_ctc 16.089115 loss_rnnt 14.612114 hw_loss 0.619197 lr 0.00048892 rank 0
2023-02-23 01:15:42,792 DEBUG TRAIN Batch 12/4500 loss 20.628654 loss_att 23.009075 loss_ctc 24.628531 loss_rnnt 19.395496 hw_loss 0.419547 lr 0.00048885 rank 2
2023-02-23 01:15:42,794 DEBUG TRAIN Batch 12/4500 loss 15.745173 loss_att 22.646832 loss_ctc 18.434080 loss_rnnt 13.778488 hw_loss 0.427185 lr 0.00048887 rank 5
2023-02-23 01:15:42,794 DEBUG TRAIN Batch 12/4500 loss 8.404711 loss_att 12.561825 loss_ctc 10.157620 loss_rnnt 7.089286 hw_loss 0.469278 lr 0.00048888 rank 6
2023-02-23 01:16:57,496 DEBUG TRAIN Batch 12/4600 loss 14.031463 loss_att 17.675354 loss_ctc 14.659296 loss_rnnt 12.964278 hw_loss 0.477554 lr 0.00048868 rank 1
2023-02-23 01:16:57,508 DEBUG TRAIN Batch 12/4600 loss 11.707154 loss_att 12.014471 loss_ctc 13.429090 loss_rnnt 11.165245 hw_loss 0.470353 lr 0.00048862 rank 2
2023-02-23 01:16:57,513 DEBUG TRAIN Batch 12/4600 loss 15.044538 loss_att 30.121853 loss_ctc 23.114876 loss_rnnt 10.774870 hw_loss 0.334052 lr 0.00048863 rank 4
2023-02-23 01:16:57,515 DEBUG TRAIN Batch 12/4600 loss 23.493317 loss_att 23.556808 loss_ctc 25.272820 loss_rnnt 23.093838 hw_loss 0.280339 lr 0.00048861 rank 7
2023-02-23 01:16:57,518 DEBUG TRAIN Batch 12/4600 loss 15.875916 loss_att 24.515188 loss_ctc 19.606476 loss_rnnt 13.445913 hw_loss 0.383886 lr 0.00048863 rank 5
2023-02-23 01:16:57,521 DEBUG TRAIN Batch 12/4600 loss 36.411602 loss_att 39.989006 loss_ctc 40.877472 loss_rnnt 34.911945 hw_loss 0.353860 lr 0.00048863 rank 3
2023-02-23 01:16:57,534 DEBUG TRAIN Batch 12/4600 loss 24.307337 loss_att 28.475544 loss_ctc 30.528667 loss_rnnt 22.370022 hw_loss 0.514058 lr 0.00048869 rank 0
2023-02-23 01:16:57,564 DEBUG TRAIN Batch 12/4600 loss 6.222649 loss_att 10.109128 loss_ctc 7.421186 loss_rnnt 5.092699 hw_loss 0.361592 lr 0.00048865 rank 6
2023-02-23 01:18:11,357 DEBUG TRAIN Batch 12/4700 loss 23.832605 loss_att 24.478760 loss_ctc 26.543465 loss_rnnt 23.158785 hw_loss 0.343389 lr 0.00048840 rank 3
2023-02-23 01:18:11,365 DEBUG TRAIN Batch 12/4700 loss 5.851397 loss_att 6.999171 loss_ctc 6.202938 loss_rnnt 5.315382 hw_loss 0.486728 lr 0.00048838 rank 7
2023-02-23 01:18:11,366 DEBUG TRAIN Batch 12/4700 loss 19.594423 loss_att 22.103895 loss_ctc 23.201002 loss_rnnt 18.396101 hw_loss 0.404156 lr 0.00048846 rank 0
2023-02-23 01:18:11,366 DEBUG TRAIN Batch 12/4700 loss 18.027937 loss_att 16.922079 loss_ctc 18.000908 loss_rnnt 18.085361 hw_loss 0.313782 lr 0.00048841 rank 6
2023-02-23 01:18:11,371 DEBUG TRAIN Batch 12/4700 loss 29.952955 loss_att 30.460564 loss_ctc 30.554401 loss_rnnt 29.559561 hw_loss 0.396898 lr 0.00048838 rank 2
2023-02-23 01:18:11,374 DEBUG TRAIN Batch 12/4700 loss 12.759323 loss_att 17.658875 loss_ctc 18.137264 loss_rnnt 10.842660 hw_loss 0.411925 lr 0.00048840 rank 5
2023-02-23 01:18:11,379 DEBUG TRAIN Batch 12/4700 loss 17.228319 loss_att 20.990776 loss_ctc 19.763336 loss_rnnt 15.969337 hw_loss 0.315914 lr 0.00048845 rank 1
2023-02-23 01:18:11,412 DEBUG TRAIN Batch 12/4700 loss 10.940329 loss_att 22.192404 loss_ctc 13.245616 loss_rnnt 8.209924 hw_loss 0.323657 lr 0.00048840 rank 4
2023-02-23 01:19:23,897 DEBUG TRAIN Batch 12/4800 loss 13.094441 loss_att 18.312191 loss_ctc 15.963528 loss_rnnt 11.457014 hw_loss 0.396249 lr 0.00048815 rank 7
2023-02-23 01:19:23,898 DEBUG TRAIN Batch 12/4800 loss 13.885000 loss_att 16.763649 loss_ctc 14.161481 loss_rnnt 13.037549 hw_loss 0.440356 lr 0.00048817 rank 4
2023-02-23 01:19:23,901 DEBUG TRAIN Batch 12/4800 loss 9.996426 loss_att 11.448565 loss_ctc 10.887117 loss_rnnt 9.294962 hw_loss 0.548018 lr 0.00048817 rank 3
2023-02-23 01:19:23,905 DEBUG TRAIN Batch 12/4800 loss 21.893930 loss_att 26.534126 loss_ctc 27.281765 loss_rnnt 20.046097 hw_loss 0.377656 lr 0.00048822 rank 0
2023-02-23 01:19:23,908 DEBUG TRAIN Batch 12/4800 loss 13.173587 loss_att 16.132078 loss_ctc 16.158422 loss_rnnt 12.007658 hw_loss 0.330474 lr 0.00048818 rank 6
2023-02-23 01:19:23,937 DEBUG TRAIN Batch 12/4800 loss 12.177131 loss_att 17.008551 loss_ctc 17.558960 loss_rnnt 10.250444 hw_loss 0.455295 lr 0.00048821 rank 1
2023-02-23 01:19:23,938 DEBUG TRAIN Batch 12/4800 loss 10.328468 loss_att 14.484756 loss_ctc 14.989641 loss_rnnt 8.683594 hw_loss 0.360238 lr 0.00048815 rank 2
2023-02-23 01:19:23,950 DEBUG TRAIN Batch 12/4800 loss 11.897978 loss_att 14.939175 loss_ctc 15.808711 loss_rnnt 10.571420 hw_loss 0.369165 lr 0.00048817 rank 5
2023-02-23 01:20:37,398 DEBUG TRAIN Batch 12/4900 loss 13.198685 loss_att 14.535546 loss_ctc 14.215680 loss_rnnt 12.558916 hw_loss 0.443994 lr 0.00048791 rank 7
2023-02-23 01:20:37,409 DEBUG TRAIN Batch 12/4900 loss 9.353956 loss_att 16.074110 loss_ctc 13.293438 loss_rnnt 7.259215 hw_loss 0.422712 lr 0.00048799 rank 0
2023-02-23 01:20:37,411 DEBUG TRAIN Batch 12/4900 loss 17.847315 loss_att 20.772808 loss_ctc 19.535709 loss_rnnt 16.851097 hw_loss 0.348751 lr 0.00048794 rank 4
2023-02-23 01:20:37,412 DEBUG TRAIN Batch 12/4900 loss 14.910144 loss_att 16.308905 loss_ctc 21.214048 loss_rnnt 13.548235 hw_loss 0.453068 lr 0.00048798 rank 1
2023-02-23 01:20:37,431 DEBUG TRAIN Batch 12/4900 loss 22.724142 loss_att 27.955650 loss_ctc 29.836170 loss_rnnt 20.515575 hw_loss 0.401238 lr 0.00048793 rank 3
2023-02-23 01:20:37,450 DEBUG TRAIN Batch 12/4900 loss 9.978406 loss_att 13.328930 loss_ctc 8.037328 loss_rnnt 9.349674 hw_loss 0.407694 lr 0.00048793 rank 5
2023-02-23 01:20:37,459 DEBUG TRAIN Batch 12/4900 loss 18.314499 loss_att 20.705395 loss_ctc 20.464577 loss_rnnt 17.347733 hw_loss 0.378581 lr 0.00048792 rank 2
2023-02-23 01:20:37,459 DEBUG TRAIN Batch 12/4900 loss 10.168083 loss_att 18.506794 loss_ctc 10.889183 loss_rnnt 8.189775 hw_loss 0.402035 lr 0.00048795 rank 6
2023-02-23 01:21:53,080 DEBUG TRAIN Batch 12/5000 loss 21.559965 loss_att 22.060871 loss_ctc 25.512592 loss_rnnt 20.706728 hw_loss 0.423815 lr 0.00048768 rank 7
2023-02-23 01:21:53,085 DEBUG TRAIN Batch 12/5000 loss 19.561409 loss_att 21.161289 loss_ctc 23.873701 loss_rnnt 18.458977 hw_loss 0.389031 lr 0.00048770 rank 5
2023-02-23 01:21:53,089 DEBUG TRAIN Batch 12/5000 loss 12.044682 loss_att 15.021372 loss_ctc 13.444906 loss_rnnt 10.969156 hw_loss 0.550293 lr 0.00048770 rank 3
2023-02-23 01:21:53,092 DEBUG TRAIN Batch 12/5000 loss 26.345335 loss_att 28.916630 loss_ctc 37.050404 loss_rnnt 24.195261 hw_loss 0.390889 lr 0.00048775 rank 1
2023-02-23 01:21:53,093 DEBUG TRAIN Batch 12/5000 loss 9.712883 loss_att 12.317900 loss_ctc 11.967311 loss_rnnt 8.638803 hw_loss 0.473411 lr 0.00048769 rank 2
2023-02-23 01:21:53,122 DEBUG TRAIN Batch 12/5000 loss 21.384581 loss_att 25.637604 loss_ctc 28.825731 loss_rnnt 19.294041 hw_loss 0.464593 lr 0.00048776 rank 0
2023-02-23 01:21:53,130 DEBUG TRAIN Batch 12/5000 loss 58.873081 loss_att 65.923370 loss_ctc 69.484467 loss_rnnt 55.850643 hw_loss 0.370357 lr 0.00048772 rank 6
2023-02-23 01:21:53,135 DEBUG TRAIN Batch 12/5000 loss 10.055875 loss_att 12.312032 loss_ctc 13.237693 loss_rnnt 8.939386 hw_loss 0.451903 lr 0.00048770 rank 4
2023-02-23 01:23:06,326 DEBUG TRAIN Batch 12/5100 loss 18.599627 loss_att 22.829771 loss_ctc 16.355419 loss_rnnt 17.835854 hw_loss 0.406823 lr 0.00048747 rank 4
2023-02-23 01:23:06,337 DEBUG TRAIN Batch 12/5100 loss 16.124258 loss_att 17.230989 loss_ctc 19.439857 loss_rnnt 15.266482 hw_loss 0.364405 lr 0.00048747 rank 3
2023-02-23 01:23:06,338 DEBUG TRAIN Batch 12/5100 loss 12.059668 loss_att 13.015005 loss_ctc 14.383282 loss_rnnt 11.228091 hw_loss 0.620052 lr 0.00048747 rank 5
2023-02-23 01:23:06,338 DEBUG TRAIN Batch 12/5100 loss 9.416813 loss_att 12.136124 loss_ctc 11.423763 loss_rnnt 8.274494 hw_loss 0.620367 lr 0.00048745 rank 7
2023-02-23 01:23:06,340 DEBUG TRAIN Batch 12/5100 loss 15.580679 loss_att 16.784092 loss_ctc 19.832092 loss_rnnt 14.490780 hw_loss 0.529428 lr 0.00048752 rank 1
2023-02-23 01:23:06,342 DEBUG TRAIN Batch 12/5100 loss 14.586305 loss_att 16.698683 loss_ctc 18.176619 loss_rnnt 13.497444 hw_loss 0.351891 lr 0.00048748 rank 6
2023-02-23 01:23:06,344 DEBUG TRAIN Batch 12/5100 loss 10.278145 loss_att 11.926291 loss_ctc 11.897441 loss_rnnt 9.472467 hw_loss 0.487766 lr 0.00048753 rank 0
2023-02-23 01:23:06,396 DEBUG TRAIN Batch 12/5100 loss 21.154856 loss_att 23.499434 loss_ctc 27.915333 loss_rnnt 19.592083 hw_loss 0.360859 lr 0.00048745 rank 2
2023-02-23 01:24:19,491 DEBUG TRAIN Batch 12/5200 loss 16.217321 loss_att 19.252224 loss_ctc 17.721478 loss_rnnt 15.227427 hw_loss 0.341927 lr 0.00048724 rank 3
2023-02-23 01:24:19,493 DEBUG TRAIN Batch 12/5200 loss 15.267597 loss_att 17.411964 loss_ctc 14.903831 loss_rnnt 14.578539 hw_loss 0.578787 lr 0.00048725 rank 6
2023-02-23 01:24:19,496 DEBUG TRAIN Batch 12/5200 loss 9.171887 loss_att 15.163291 loss_ctc 14.268557 loss_rnnt 7.070266 hw_loss 0.419598 lr 0.00048722 rank 7
2023-02-23 01:24:19,500 DEBUG TRAIN Batch 12/5200 loss 13.685407 loss_att 17.644140 loss_ctc 15.187823 loss_rnnt 12.481654 hw_loss 0.396908 lr 0.00048724 rank 4
2023-02-23 01:24:19,501 DEBUG TRAIN Batch 12/5200 loss 4.173064 loss_att 8.704127 loss_ctc 4.037956 loss_rnnt 3.091005 hw_loss 0.363488 lr 0.00048728 rank 1
2023-02-23 01:24:19,501 DEBUG TRAIN Batch 12/5200 loss 16.182892 loss_att 17.727207 loss_ctc 14.507410 loss_rnnt 15.852214 hw_loss 0.459772 lr 0.00048724 rank 5
2023-02-23 01:24:19,502 DEBUG TRAIN Batch 12/5200 loss 16.576445 loss_att 21.829382 loss_ctc 21.517120 loss_rnnt 14.659615 hw_loss 0.389036 lr 0.00048729 rank 0
2023-02-23 01:24:19,503 DEBUG TRAIN Batch 12/5200 loss 24.072733 loss_att 30.551376 loss_ctc 31.728910 loss_rnnt 21.533995 hw_loss 0.416599 lr 0.00048722 rank 2
2023-02-23 01:25:34,833 DEBUG TRAIN Batch 12/5300 loss 11.996813 loss_att 14.605051 loss_ctc 14.192444 loss_rnnt 10.991130 hw_loss 0.358659 lr 0.00048705 rank 1
2023-02-23 01:25:34,840 DEBUG TRAIN Batch 12/5300 loss 11.664693 loss_att 13.399467 loss_ctc 12.077698 loss_rnnt 11.028664 hw_loss 0.438764 lr 0.00048701 rank 4
2023-02-23 01:25:34,843 DEBUG TRAIN Batch 12/5300 loss 18.686649 loss_att 20.791689 loss_ctc 25.986240 loss_rnnt 17.114155 hw_loss 0.334138 lr 0.00048701 rank 5
2023-02-23 01:25:34,845 DEBUG TRAIN Batch 12/5300 loss 21.343513 loss_att 28.160524 loss_ctc 24.878241 loss_rnnt 19.293480 hw_loss 0.403750 lr 0.00048701 rank 3
2023-02-23 01:25:34,846 DEBUG TRAIN Batch 12/5300 loss 4.197343 loss_att 8.156231 loss_ctc 7.686629 loss_rnnt 2.740730 hw_loss 0.374245 lr 0.00048706 rank 0
2023-02-23 01:25:34,848 DEBUG TRAIN Batch 12/5300 loss 24.969751 loss_att 26.271793 loss_ctc 27.610409 loss_rnnt 24.130154 hw_loss 0.425816 lr 0.00048699 rank 7
2023-02-23 01:25:34,848 DEBUG TRAIN Batch 12/5300 loss 30.671146 loss_att 40.676956 loss_ctc 38.663559 loss_rnnt 27.401253 hw_loss 0.380771 lr 0.00048702 rank 6
2023-02-23 01:25:34,851 DEBUG TRAIN Batch 12/5300 loss 17.241106 loss_att 20.041739 loss_ctc 19.696001 loss_rnnt 16.130379 hw_loss 0.418652 lr 0.00048699 rank 2
2023-02-23 01:26:49,126 DEBUG TRAIN Batch 12/5400 loss 8.190114 loss_att 14.242970 loss_ctc 9.897661 loss_rnnt 6.534280 hw_loss 0.407979 lr 0.00048683 rank 0
2023-02-23 01:26:49,141 DEBUG TRAIN Batch 12/5400 loss 11.166104 loss_att 16.532764 loss_ctc 12.578352 loss_rnnt 9.618807 hw_loss 0.535625 lr 0.00048676 rank 7
2023-02-23 01:26:49,144 DEBUG TRAIN Batch 12/5400 loss 14.700901 loss_att 18.701456 loss_ctc 16.799408 loss_rnnt 13.386342 hw_loss 0.439961 lr 0.00048682 rank 1
2023-02-23 01:26:49,146 DEBUG TRAIN Batch 12/5400 loss 19.277021 loss_att 22.979086 loss_ctc 26.799664 loss_rnnt 17.320017 hw_loss 0.400446 lr 0.00048678 rank 3
2023-02-23 01:26:49,147 DEBUG TRAIN Batch 12/5400 loss 9.985421 loss_att 14.137356 loss_ctc 12.244594 loss_rnnt 8.639751 hw_loss 0.401360 lr 0.00048679 rank 6
2023-02-23 01:26:49,147 DEBUG TRAIN Batch 12/5400 loss 33.708511 loss_att 35.991768 loss_ctc 40.947491 loss_rnnt 32.095097 hw_loss 0.359184 lr 0.00048678 rank 4
2023-02-23 01:26:49,150 DEBUG TRAIN Batch 12/5400 loss 12.065322 loss_att 16.604166 loss_ctc 14.255810 loss_rnnt 10.623468 hw_loss 0.453785 lr 0.00048676 rank 2
2023-02-23 01:26:49,158 DEBUG TRAIN Batch 12/5400 loss 12.279375 loss_att 16.034313 loss_ctc 17.862167 loss_rnnt 10.554187 hw_loss 0.430927 lr 0.00048678 rank 5
2023-02-23 01:28:01,903 DEBUG TRAIN Batch 12/5500 loss 15.254137 loss_att 20.605145 loss_ctc 17.439526 loss_rnnt 13.686950 hw_loss 0.385502 lr 0.00048655 rank 3
2023-02-23 01:28:01,906 DEBUG TRAIN Batch 12/5500 loss 13.797172 loss_att 15.363478 loss_ctc 15.073417 loss_rnnt 13.074471 hw_loss 0.448639 lr 0.00048655 rank 5
2023-02-23 01:28:01,906 DEBUG TRAIN Batch 12/5500 loss 16.709850 loss_att 20.523468 loss_ctc 26.169804 loss_rnnt 14.514229 hw_loss 0.321692 lr 0.00048652 rank 7
2023-02-23 01:28:01,911 DEBUG TRAIN Batch 12/5500 loss 10.011313 loss_att 11.765524 loss_ctc 13.189850 loss_rnnt 9.022339 hw_loss 0.401864 lr 0.00048653 rank 2
2023-02-23 01:28:01,912 DEBUG TRAIN Batch 12/5500 loss 22.005926 loss_att 24.063004 loss_ctc 25.861950 loss_rnnt 20.883331 hw_loss 0.369454 lr 0.00048655 rank 4
2023-02-23 01:28:01,913 DEBUG TRAIN Batch 12/5500 loss 25.787277 loss_att 26.458054 loss_ctc 29.633135 loss_rnnt 24.850765 hw_loss 0.542955 lr 0.00048659 rank 1
2023-02-23 01:28:01,913 DEBUG TRAIN Batch 12/5500 loss 7.044649 loss_att 9.846933 loss_ctc 7.994599 loss_rnnt 6.118730 hw_loss 0.447753 lr 0.00048660 rank 0
2023-02-23 01:28:01,958 DEBUG TRAIN Batch 12/5500 loss 11.825741 loss_att 14.795347 loss_ctc 13.482269 loss_rnnt 10.759729 hw_loss 0.471037 lr 0.00048656 rank 6
2023-02-23 01:29:15,253 DEBUG TRAIN Batch 12/5600 loss 12.955265 loss_att 15.743608 loss_ctc 14.553424 loss_rnnt 11.948306 hw_loss 0.442881 lr 0.00048632 rank 4
2023-02-23 01:29:15,259 DEBUG TRAIN Batch 12/5600 loss 9.964168 loss_att 11.362147 loss_ctc 13.251342 loss_rnnt 8.968723 hw_loss 0.520423 lr 0.00048636 rank 1
2023-02-23 01:29:15,260 DEBUG TRAIN Batch 12/5600 loss 7.288749 loss_att 11.531440 loss_ctc 9.252395 loss_rnnt 5.978323 hw_loss 0.375126 lr 0.00048637 rank 0
2023-02-23 01:29:15,260 DEBUG TRAIN Batch 12/5600 loss 16.932575 loss_att 22.540380 loss_ctc 19.949366 loss_rnnt 15.193203 hw_loss 0.404197 lr 0.00048629 rank 7
2023-02-23 01:29:15,261 DEBUG TRAIN Batch 12/5600 loss 15.846797 loss_att 18.274981 loss_ctc 15.971450 loss_rnnt 15.124894 hw_loss 0.411835 lr 0.00048633 rank 6
2023-02-23 01:29:15,269 DEBUG TRAIN Batch 12/5600 loss 23.720495 loss_att 24.658052 loss_ctc 26.565235 loss_rnnt 22.925678 hw_loss 0.427514 lr 0.00048632 rank 3
2023-02-23 01:29:15,299 DEBUG TRAIN Batch 12/5600 loss 7.688786 loss_att 10.832386 loss_ctc 8.225723 loss_rnnt 6.718991 hw_loss 0.505279 lr 0.00048630 rank 2
2023-02-23 01:29:15,307 DEBUG TRAIN Batch 12/5600 loss 9.941503 loss_att 14.420947 loss_ctc 14.566114 loss_rnnt 8.188549 hw_loss 0.450843 lr 0.00048632 rank 5
2023-02-23 01:30:31,702 DEBUG TRAIN Batch 12/5700 loss 12.135757 loss_att 12.136653 loss_ctc 15.205487 loss_rnnt 11.450186 hw_loss 0.517678 lr 0.00048609 rank 3
2023-02-23 01:30:31,703 DEBUG TRAIN Batch 12/5700 loss 22.214073 loss_att 21.950138 loss_ctc 28.124796 loss_rnnt 21.235668 hw_loss 0.455806 lr 0.00048614 rank 0
2023-02-23 01:30:31,702 DEBUG TRAIN Batch 12/5700 loss 15.473178 loss_att 14.583959 loss_ctc 17.855898 loss_rnnt 15.033801 hw_loss 0.561606 lr 0.00048609 rank 4
2023-02-23 01:30:31,703 DEBUG TRAIN Batch 12/5700 loss 18.769484 loss_att 20.307325 loss_ctc 20.072830 loss_rnnt 18.050539 hw_loss 0.445492 lr 0.00048606 rank 7
2023-02-23 01:30:31,705 DEBUG TRAIN Batch 12/5700 loss 16.415237 loss_att 15.769700 loss_ctc 17.967438 loss_rnnt 16.078400 hw_loss 0.485597 lr 0.00048613 rank 1
2023-02-23 01:30:31,706 DEBUG TRAIN Batch 12/5700 loss 19.627403 loss_att 23.326935 loss_ctc 20.316681 loss_rnnt 18.553955 hw_loss 0.453073 lr 0.00048610 rank 6
2023-02-23 01:30:31,708 DEBUG TRAIN Batch 12/5700 loss 8.765072 loss_att 11.687469 loss_ctc 9.919929 loss_rnnt 7.863013 hw_loss 0.306746 lr 0.00048607 rank 2
2023-02-23 01:30:31,708 DEBUG TRAIN Batch 12/5700 loss 14.660901 loss_att 16.497459 loss_ctc 16.984013 loss_rnnt 13.704612 hw_loss 0.523555 lr 0.00048609 rank 5
2023-02-23 01:31:45,358 DEBUG TRAIN Batch 12/5800 loss 11.264672 loss_att 13.817459 loss_ctc 13.372284 loss_rnnt 10.223059 hw_loss 0.468829 lr 0.00048584 rank 7
2023-02-23 01:31:45,359 DEBUG TRAIN Batch 12/5800 loss 17.821697 loss_att 18.514433 loss_ctc 21.530302 loss_rnnt 16.989286 hw_loss 0.373847 lr 0.00048590 rank 1
2023-02-23 01:31:45,360 DEBUG TRAIN Batch 12/5800 loss 13.211516 loss_att 19.265446 loss_ctc 17.003473 loss_rnnt 11.307502 hw_loss 0.351815 lr 0.00048586 rank 3
2023-02-23 01:31:45,361 DEBUG TRAIN Batch 12/5800 loss 8.636172 loss_att 13.967936 loss_ctc 10.653599 loss_rnnt 7.121217 hw_loss 0.336772 lr 0.00048586 rank 5
2023-02-23 01:31:45,362 DEBUG TRAIN Batch 12/5800 loss 12.578431 loss_att 16.598003 loss_ctc 18.480732 loss_rnnt 10.763789 hw_loss 0.419541 lr 0.00048591 rank 0
2023-02-23 01:31:45,364 DEBUG TRAIN Batch 12/5800 loss 21.980993 loss_att 27.499893 loss_ctc 22.728943 loss_rnnt 20.567032 hw_loss 0.394600 lr 0.00048584 rank 2
2023-02-23 01:31:45,365 DEBUG TRAIN Batch 12/5800 loss 11.753696 loss_att 12.351698 loss_ctc 15.741754 loss_rnnt 10.698151 hw_loss 0.757885 lr 0.00048587 rank 6
2023-02-23 01:31:45,366 DEBUG TRAIN Batch 12/5800 loss 13.622625 loss_att 16.286285 loss_ctc 18.280371 loss_rnnt 12.224047 hw_loss 0.459024 lr 0.00048586 rank 4
2023-02-23 01:32:58,936 DEBUG TRAIN Batch 12/5900 loss 24.132135 loss_att 29.020000 loss_ctc 25.353491 loss_rnnt 22.759659 hw_loss 0.435102 lr 0.00048564 rank 6
2023-02-23 01:32:58,937 DEBUG TRAIN Batch 12/5900 loss 9.846332 loss_att 15.079068 loss_ctc 11.195639 loss_rnnt 8.418907 hw_loss 0.376817 lr 0.00048561 rank 7
2023-02-23 01:32:58,940 DEBUG TRAIN Batch 12/5900 loss 13.169497 loss_att 21.750122 loss_ctc 15.423695 loss_rnnt 10.978374 hw_loss 0.327074 lr 0.00048563 rank 3
2023-02-23 01:32:58,944 DEBUG TRAIN Batch 12/5900 loss 16.675999 loss_att 19.922199 loss_ctc 25.644363 loss_rnnt 14.556396 hw_loss 0.514841 lr 0.00048563 rank 4
2023-02-23 01:32:58,944 DEBUG TRAIN Batch 12/5900 loss 14.384542 loss_att 17.133028 loss_ctc 17.380424 loss_rnnt 13.227035 hw_loss 0.390671 lr 0.00048563 rank 5
2023-02-23 01:32:58,945 DEBUG TRAIN Batch 12/5900 loss 14.162556 loss_att 18.663845 loss_ctc 17.508331 loss_rnnt 12.546658 hw_loss 0.505379 lr 0.00048567 rank 1
2023-02-23 01:32:58,957 DEBUG TRAIN Batch 12/5900 loss 8.383368 loss_att 11.472072 loss_ctc 7.171996 loss_rnnt 7.755081 hw_loss 0.322615 lr 0.00048568 rank 0
2023-02-23 01:32:59,005 DEBUG TRAIN Batch 12/5900 loss 8.907769 loss_att 11.537945 loss_ctc 10.194288 loss_rnnt 7.982658 hw_loss 0.426638 lr 0.00048561 rank 2
2023-02-23 01:34:13,447 DEBUG TRAIN Batch 12/6000 loss 20.067938 loss_att 27.402718 loss_ctc 24.297043 loss_rnnt 17.781578 hw_loss 0.479104 lr 0.00048538 rank 7
2023-02-23 01:34:13,448 DEBUG TRAIN Batch 12/6000 loss 13.111739 loss_att 18.299494 loss_ctc 16.679111 loss_rnnt 11.399203 hw_loss 0.373753 lr 0.00048541 rank 6
2023-02-23 01:34:13,450 DEBUG TRAIN Batch 12/6000 loss 14.721130 loss_att 18.555187 loss_ctc 17.498672 loss_rnnt 13.379438 hw_loss 0.383516 lr 0.00048545 rank 0
2023-02-23 01:34:13,450 DEBUG TRAIN Batch 12/6000 loss 9.742791 loss_att 12.250154 loss_ctc 12.050650 loss_rnnt 8.683987 hw_loss 0.468033 lr 0.00048540 rank 3
2023-02-23 01:34:13,451 DEBUG TRAIN Batch 12/6000 loss 15.568546 loss_att 21.765547 loss_ctc 18.096882 loss_rnnt 13.756717 hw_loss 0.441219 lr 0.00048540 rank 4
2023-02-23 01:34:13,451 DEBUG TRAIN Batch 12/6000 loss 11.092702 loss_att 18.842485 loss_ctc 13.112618 loss_rnnt 9.049920 hw_loss 0.419067 lr 0.00048544 rank 1
2023-02-23 01:34:13,453 DEBUG TRAIN Batch 12/6000 loss 17.007338 loss_att 20.619846 loss_ctc 21.351139 loss_rnnt 15.497707 hw_loss 0.389913 lr 0.00048538 rank 2
2023-02-23 01:34:13,472 DEBUG TRAIN Batch 12/6000 loss 14.394487 loss_att 19.488390 loss_ctc 26.855114 loss_rnnt 11.524611 hw_loss 0.355648 lr 0.00048540 rank 5
2023-02-23 01:35:27,554 DEBUG TRAIN Batch 12/6100 loss 14.769015 loss_att 18.915216 loss_ctc 20.025532 loss_rnnt 13.024420 hw_loss 0.402161 lr 0.00048517 rank 3
2023-02-23 01:35:27,554 DEBUG TRAIN Batch 12/6100 loss 20.928204 loss_att 28.549904 loss_ctc 28.284384 loss_rnnt 18.193268 hw_loss 0.430824 lr 0.00048515 rank 7
2023-02-23 01:35:27,557 DEBUG TRAIN Batch 12/6100 loss 9.760785 loss_att 12.707705 loss_ctc 7.035088 loss_rnnt 9.348416 hw_loss 0.349520 lr 0.00048522 rank 1
2023-02-23 01:35:27,557 DEBUG TRAIN Batch 12/6100 loss 7.637567 loss_att 12.591532 loss_ctc 9.641312 loss_rnnt 6.173382 hw_loss 0.386672 lr 0.00048517 rank 4
2023-02-23 01:35:27,558 DEBUG TRAIN Batch 12/6100 loss 11.268529 loss_att 15.121771 loss_ctc 16.773539 loss_rnnt 9.539228 hw_loss 0.421218 lr 0.00048515 rank 2
2023-02-23 01:35:27,561 DEBUG TRAIN Batch 12/6100 loss 10.317980 loss_att 14.261553 loss_ctc 14.915026 loss_rnnt 8.736599 hw_loss 0.336985 lr 0.00048522 rank 0
2023-02-23 01:35:27,561 DEBUG TRAIN Batch 12/6100 loss 9.006754 loss_att 12.811401 loss_ctc 11.488125 loss_rnnt 7.666726 hw_loss 0.465468 lr 0.00048517 rank 5
2023-02-23 01:35:27,564 DEBUG TRAIN Batch 12/6100 loss 15.126969 loss_att 21.774881 loss_ctc 19.947691 loss_rnnt 12.944201 hw_loss 0.394542 lr 0.00048518 rank 6
2023-02-23 01:36:40,782 DEBUG TRAIN Batch 12/6200 loss 25.001600 loss_att 22.967449 loss_ctc 33.378300 loss_rnnt 24.112457 hw_loss 0.335772 lr 0.00048492 rank 7
2023-02-23 01:36:40,787 DEBUG TRAIN Batch 12/6200 loss 13.691623 loss_att 21.368038 loss_ctc 16.422918 loss_rnnt 11.539747 hw_loss 0.473287 lr 0.00048495 rank 6
2023-02-23 01:36:40,786 DEBUG TRAIN Batch 12/6200 loss 14.593010 loss_att 18.035406 loss_ctc 22.168041 loss_rnnt 12.632951 hw_loss 0.490453 lr 0.00048494 rank 3
2023-02-23 01:36:40,787 DEBUG TRAIN Batch 12/6200 loss 15.724360 loss_att 18.168339 loss_ctc 17.974167 loss_rnnt 14.754780 hw_loss 0.339017 lr 0.00048499 rank 1
2023-02-23 01:36:40,788 DEBUG TRAIN Batch 12/6200 loss 12.251963 loss_att 14.459242 loss_ctc 13.128704 loss_rnnt 11.441470 hw_loss 0.472759 lr 0.00048500 rank 0
2023-02-23 01:36:40,792 DEBUG TRAIN Batch 12/6200 loss 18.156107 loss_att 18.924622 loss_ctc 22.845688 loss_rnnt 17.164148 hw_loss 0.399332 lr 0.00048494 rank 5
2023-02-23 01:36:40,794 DEBUG TRAIN Batch 12/6200 loss 19.402758 loss_att 21.479908 loss_ctc 18.593853 loss_rnnt 18.875748 hw_loss 0.411437 lr 0.00048493 rank 2
2023-02-23 01:36:40,801 DEBUG TRAIN Batch 12/6200 loss 18.932375 loss_att 23.576283 loss_ctc 22.037003 loss_rnnt 17.394638 hw_loss 0.365635 lr 0.00048494 rank 4
2023-02-23 01:37:53,811 DEBUG TRAIN Batch 12/6300 loss 28.544386 loss_att 30.594093 loss_ctc 33.587658 loss_rnnt 27.220692 hw_loss 0.452465 lr 0.00048471 rank 3
2023-02-23 01:37:53,813 DEBUG TRAIN Batch 12/6300 loss 9.814574 loss_att 11.052353 loss_ctc 11.625983 loss_rnnt 9.120055 hw_loss 0.385202 lr 0.00048469 rank 7
2023-02-23 01:37:53,815 DEBUG TRAIN Batch 12/6300 loss 13.243048 loss_att 13.667421 loss_ctc 13.818544 loss_rnnt 12.834943 hw_loss 0.462183 lr 0.00048476 rank 1
2023-02-23 01:37:53,816 DEBUG TRAIN Batch 12/6300 loss 11.182940 loss_att 14.593954 loss_ctc 17.923306 loss_rnnt 9.389413 hw_loss 0.398641 lr 0.00048471 rank 5
2023-02-23 01:37:53,817 DEBUG TRAIN Batch 12/6300 loss 11.730245 loss_att 14.547211 loss_ctc 16.647877 loss_rnnt 10.219645 hw_loss 0.546606 lr 0.00048472 rank 4
2023-02-23 01:37:53,818 DEBUG TRAIN Batch 12/6300 loss 15.015200 loss_att 17.138971 loss_ctc 17.686882 loss_rnnt 13.999635 hw_loss 0.439849 lr 0.00048473 rank 6
2023-02-23 01:37:53,847 DEBUG TRAIN Batch 12/6300 loss 14.362423 loss_att 15.716453 loss_ctc 20.740036 loss_rnnt 12.982867 hw_loss 0.484505 lr 0.00048477 rank 0
2023-02-23 01:37:53,891 DEBUG TRAIN Batch 12/6300 loss 13.322197 loss_att 13.737676 loss_ctc 16.322071 loss_rnnt 12.518260 hw_loss 0.601608 lr 0.00048470 rank 2
2023-02-23 01:39:10,207 DEBUG TRAIN Batch 12/6400 loss 39.059116 loss_att 43.238758 loss_ctc 46.774162 loss_rnnt 36.992432 hw_loss 0.378904 lr 0.00048447 rank 7
2023-02-23 01:39:10,214 DEBUG TRAIN Batch 12/6400 loss 8.922932 loss_att 13.720951 loss_ctc 12.924517 loss_rnnt 7.223961 hw_loss 0.385916 lr 0.00048449 rank 4
2023-02-23 01:39:10,214 DEBUG TRAIN Batch 12/6400 loss 17.503889 loss_att 17.165657 loss_ctc 22.741133 loss_rnnt 16.558659 hw_loss 0.589832 lr 0.00048449 rank 5
2023-02-23 01:39:10,218 DEBUG TRAIN Batch 12/6400 loss 16.413839 loss_att 17.708946 loss_ctc 21.626637 loss_rnnt 15.214600 hw_loss 0.459709 lr 0.00048450 rank 6
2023-02-23 01:39:10,218 DEBUG TRAIN Batch 12/6400 loss 14.044635 loss_att 13.344624 loss_ctc 17.405052 loss_rnnt 13.355445 hw_loss 0.714631 lr 0.00048453 rank 1
2023-02-23 01:39:10,241 DEBUG TRAIN Batch 12/6400 loss 5.646808 loss_att 11.185478 loss_ctc 5.846324 loss_rnnt 4.304037 hw_loss 0.390816 lr 0.00048449 rank 3
2023-02-23 01:39:10,258 DEBUG TRAIN Batch 12/6400 loss 10.209568 loss_att 16.848648 loss_ctc 13.067637 loss_rnnt 8.314593 hw_loss 0.348905 lr 0.00048447 rank 2
2023-02-23 01:39:10,267 DEBUG TRAIN Batch 12/6400 loss 14.111233 loss_att 14.042166 loss_ctc 16.507059 loss_rnnt 13.439868 hw_loss 0.685751 lr 0.00048454 rank 0
2023-02-23 01:40:23,034 DEBUG TRAIN Batch 12/6500 loss 22.138243 loss_att 22.405121 loss_ctc 23.417309 loss_rnnt 21.710857 hw_loss 0.381504 lr 0.00048427 rank 6
2023-02-23 01:40:23,035 DEBUG TRAIN Batch 12/6500 loss 6.695551 loss_att 11.165167 loss_ctc 7.978695 loss_rnnt 5.455793 hw_loss 0.327655 lr 0.00048426 rank 3
2023-02-23 01:40:23,036 DEBUG TRAIN Batch 12/6500 loss 17.991049 loss_att 22.173574 loss_ctc 23.315557 loss_rnnt 16.263165 hw_loss 0.340207 lr 0.00048424 rank 7
2023-02-23 01:40:23,036 DEBUG TRAIN Batch 12/6500 loss 25.254885 loss_att 25.053371 loss_ctc 22.506798 loss_rnnt 25.434380 hw_loss 0.426041 lr 0.00048426 rank 4
2023-02-23 01:40:23,037 DEBUG TRAIN Batch 12/6500 loss 17.693602 loss_att 20.212446 loss_ctc 19.728207 loss_rnnt 16.676601 hw_loss 0.453658 lr 0.00048430 rank 1
2023-02-23 01:40:23,042 DEBUG TRAIN Batch 12/6500 loss 13.943819 loss_att 14.565510 loss_ctc 15.226722 loss_rnnt 13.432433 hw_loss 0.404989 lr 0.00048424 rank 2
2023-02-23 01:40:23,046 DEBUG TRAIN Batch 12/6500 loss 19.622099 loss_att 29.271572 loss_ctc 27.073513 loss_rnnt 16.518177 hw_loss 0.338444 lr 0.00048426 rank 5
2023-02-23 01:40:23,092 DEBUG TRAIN Batch 12/6500 loss 16.849939 loss_att 22.291492 loss_ctc 24.320454 loss_rnnt 14.570894 hw_loss 0.365000 lr 0.00048431 rank 0
2023-02-23 01:41:35,877 DEBUG TRAIN Batch 12/6600 loss 19.530443 loss_att 25.781166 loss_ctc 29.406651 loss_rnnt 16.776930 hw_loss 0.349764 lr 0.00048401 rank 7
2023-02-23 01:41:35,890 DEBUG TRAIN Batch 12/6600 loss 14.612514 loss_att 20.471033 loss_ctc 17.137680 loss_rnnt 12.931929 hw_loss 0.322862 lr 0.00048403 rank 5
2023-02-23 01:41:35,891 DEBUG TRAIN Batch 12/6600 loss 16.927555 loss_att 17.424976 loss_ctc 20.957138 loss_rnnt 16.112089 hw_loss 0.335072 lr 0.00048409 rank 0
2023-02-23 01:41:35,891 DEBUG TRAIN Batch 12/6600 loss 11.273296 loss_att 16.810272 loss_ctc 17.460409 loss_rnnt 9.111742 hw_loss 0.429770 lr 0.00048403 rank 3
2023-02-23 01:41:35,893 DEBUG TRAIN Batch 12/6600 loss 27.794563 loss_att 32.413712 loss_ctc 33.999401 loss_rnnt 25.782024 hw_loss 0.490124 lr 0.00048408 rank 1
2023-02-23 01:41:35,893 DEBUG TRAIN Batch 12/6600 loss 13.753661 loss_att 19.581593 loss_ctc 21.198973 loss_rnnt 11.364717 hw_loss 0.432465 lr 0.00048405 rank 6
2023-02-23 01:41:35,895 DEBUG TRAIN Batch 12/6600 loss 18.601608 loss_att 21.934361 loss_ctc 24.783524 loss_rnnt 16.891434 hw_loss 0.411315 lr 0.00048403 rank 4
2023-02-23 01:41:35,906 DEBUG TRAIN Batch 12/6600 loss 10.928109 loss_att 13.519082 loss_ctc 14.060315 loss_rnnt 9.790710 hw_loss 0.377957 lr 0.00048402 rank 2
2023-02-23 01:42:50,603 DEBUG TRAIN Batch 12/6700 loss 8.821342 loss_att 12.051112 loss_ctc 10.300131 loss_rnnt 7.767430 hw_loss 0.395222 lr 0.00048382 rank 6
2023-02-23 01:42:50,615 DEBUG TRAIN Batch 12/6700 loss 17.874903 loss_att 22.201357 loss_ctc 27.991913 loss_rnnt 15.440196 hw_loss 0.413403 lr 0.00048380 rank 5
2023-02-23 01:42:50,619 DEBUG TRAIN Batch 12/6700 loss 17.041265 loss_att 21.601025 loss_ctc 21.362366 loss_rnnt 15.345418 hw_loss 0.389530 lr 0.00048386 rank 0
2023-02-23 01:42:50,620 DEBUG TRAIN Batch 12/6700 loss 12.948703 loss_att 13.695203 loss_ctc 9.549389 loss_rnnt 13.062744 hw_loss 0.356063 lr 0.00048379 rank 2
2023-02-23 01:42:50,621 DEBUG TRAIN Batch 12/6700 loss 22.600471 loss_att 30.109726 loss_ctc 26.233780 loss_rnnt 20.408943 hw_loss 0.384819 lr 0.00048385 rank 1
2023-02-23 01:42:50,622 DEBUG TRAIN Batch 12/6700 loss 24.532801 loss_att 24.405106 loss_ctc 30.170380 loss_rnnt 23.576927 hw_loss 0.430750 lr 0.00048378 rank 7
2023-02-23 01:42:50,623 DEBUG TRAIN Batch 12/6700 loss 14.835147 loss_att 15.711346 loss_ctc 18.973997 loss_rnnt 13.882677 hw_loss 0.422593 lr 0.00048381 rank 4
2023-02-23 01:42:50,629 DEBUG TRAIN Batch 12/6700 loss 4.684697 loss_att 8.878940 loss_ctc 6.469651 loss_rnnt 3.406823 hw_loss 0.376933 lr 0.00048380 rank 3
2023-02-23 01:44:05,353 DEBUG TRAIN Batch 12/6800 loss 7.517503 loss_att 14.320583 loss_ctc 8.111995 loss_rnnt 5.885681 hw_loss 0.359888 lr 0.00048358 rank 5
2023-02-23 01:44:05,368 DEBUG TRAIN Batch 12/6800 loss 14.077659 loss_att 17.498947 loss_ctc 16.275295 loss_rnnt 12.904643 hw_loss 0.367014 lr 0.00048363 rank 0
2023-02-23 01:44:05,371 DEBUG TRAIN Batch 12/6800 loss 13.838805 loss_att 16.877144 loss_ctc 17.425024 loss_rnnt 12.549656 hw_loss 0.381223 lr 0.00048359 rank 6
2023-02-23 01:44:05,373 DEBUG TRAIN Batch 12/6800 loss 13.789975 loss_att 16.869705 loss_ctc 16.030300 loss_rnnt 12.617231 hw_loss 0.483916 lr 0.00048358 rank 4
2023-02-23 01:44:05,374 DEBUG TRAIN Batch 12/6800 loss 14.340755 loss_att 16.694355 loss_ctc 13.604452 loss_rnnt 13.748522 hw_loss 0.411912 lr 0.00048356 rank 7
2023-02-23 01:44:05,378 DEBUG TRAIN Batch 12/6800 loss 13.439652 loss_att 15.953392 loss_ctc 17.886135 loss_rnnt 12.153552 hw_loss 0.357165 lr 0.00048356 rank 2
2023-02-23 01:44:05,379 DEBUG TRAIN Batch 12/6800 loss 27.218225 loss_att 27.172245 loss_ctc 32.903801 loss_rnnt 26.265532 hw_loss 0.382152 lr 0.00048358 rank 3
2023-02-23 01:44:05,429 DEBUG TRAIN Batch 12/6800 loss 10.080239 loss_att 12.578171 loss_ctc 10.887183 loss_rnnt 9.266626 hw_loss 0.387066 lr 0.00048362 rank 1
2023-02-23 01:45:18,358 DEBUG TRAIN Batch 12/6900 loss 11.745984 loss_att 16.239384 loss_ctc 14.325756 loss_rnnt 10.269810 hw_loss 0.437860 lr 0.00048333 rank 7
2023-02-23 01:45:18,360 DEBUG TRAIN Batch 12/6900 loss 31.219437 loss_att 34.241673 loss_ctc 39.438213 loss_rnnt 29.267960 hw_loss 0.470987 lr 0.00048335 rank 3
2023-02-23 01:45:18,362 DEBUG TRAIN Batch 12/6900 loss 22.932673 loss_att 25.341557 loss_ctc 29.805182 loss_rnnt 21.346964 hw_loss 0.351741 lr 0.00048335 rank 4
2023-02-23 01:45:18,364 DEBUG TRAIN Batch 12/6900 loss 18.412865 loss_att 21.631104 loss_ctc 22.318600 loss_rnnt 16.978378 hw_loss 0.506385 lr 0.00048340 rank 1
2023-02-23 01:45:18,364 DEBUG TRAIN Batch 12/6900 loss 10.364395 loss_att 13.278463 loss_ctc 13.322481 loss_rnnt 9.175415 hw_loss 0.397041 lr 0.00048337 rank 6
2023-02-23 01:45:18,364 DEBUG TRAIN Batch 12/6900 loss 23.114082 loss_att 26.158745 loss_ctc 28.953739 loss_rnnt 21.537392 hw_loss 0.354634 lr 0.00048335 rank 5
2023-02-23 01:45:18,366 DEBUG TRAIN Batch 12/6900 loss 14.165195 loss_att 14.585867 loss_ctc 13.874487 loss_rnnt 13.853434 hw_loss 0.499476 lr 0.00048334 rank 2
2023-02-23 01:45:18,366 DEBUG TRAIN Batch 12/6900 loss 14.205288 loss_att 17.576035 loss_ctc 15.944992 loss_rnnt 13.109725 hw_loss 0.355225 lr 0.00048341 rank 0
2023-02-23 01:46:33,856 DEBUG TRAIN Batch 12/7000 loss 4.246386 loss_att 5.755805 loss_ctc 5.025713 loss_rnnt 3.554572 hw_loss 0.536285 lr 0.00048311 rank 7
2023-02-23 01:46:33,857 DEBUG TRAIN Batch 12/7000 loss 9.463515 loss_att 10.853127 loss_ctc 11.625957 loss_rnnt 8.659522 hw_loss 0.445773 lr 0.00048314 rank 6
2023-02-23 01:46:33,859 DEBUG TRAIN Batch 12/7000 loss 13.533862 loss_att 14.537768 loss_ctc 14.374270 loss_rnnt 12.988395 hw_loss 0.436186 lr 0.00048313 rank 5
2023-02-23 01:46:33,859 DEBUG TRAIN Batch 12/7000 loss 9.804262 loss_att 18.086983 loss_ctc 12.796016 loss_rnnt 7.525384 hw_loss 0.418937 lr 0.00048313 rank 3
2023-02-23 01:46:33,862 DEBUG TRAIN Batch 12/7000 loss 14.282748 loss_att 15.868425 loss_ctc 18.259466 loss_rnnt 13.195030 hw_loss 0.450660 lr 0.00048313 rank 4
2023-02-23 01:46:33,865 DEBUG TRAIN Batch 12/7000 loss 12.218979 loss_att 14.350645 loss_ctc 13.041747 loss_rnnt 11.414721 hw_loss 0.502915 lr 0.00048317 rank 1
2023-02-23 01:46:33,864 DEBUG TRAIN Batch 12/7000 loss 21.702116 loss_att 23.098320 loss_ctc 27.052549 loss_rnnt 20.431744 hw_loss 0.520766 lr 0.00048318 rank 0
2023-02-23 01:46:33,866 DEBUG TRAIN Batch 12/7000 loss 18.669981 loss_att 21.717192 loss_ctc 23.210880 loss_rnnt 17.258902 hw_loss 0.367847 lr 0.00048311 rank 2
2023-02-23 01:47:50,343 DEBUG TRAIN Batch 12/7100 loss 20.846987 loss_att 27.206055 loss_ctc 23.345909 loss_rnnt 18.988226 hw_loss 0.475796 lr 0.00048290 rank 5
2023-02-23 01:47:50,345 DEBUG TRAIN Batch 12/7100 loss 20.352980 loss_att 23.016830 loss_ctc 33.034309 loss_rnnt 17.890472 hw_loss 0.447928 lr 0.00048296 rank 0
2023-02-23 01:47:50,345 DEBUG TRAIN Batch 12/7100 loss 8.631207 loss_att 13.113027 loss_ctc 12.603305 loss_rnnt 7.000776 hw_loss 0.383350 lr 0.00048295 rank 1
2023-02-23 01:47:50,345 DEBUG TRAIN Batch 12/7100 loss 6.510140 loss_att 10.534349 loss_ctc 8.314823 loss_rnnt 5.279368 hw_loss 0.347449 lr 0.00048291 rank 6
2023-02-23 01:47:50,347 DEBUG TRAIN Batch 12/7100 loss 30.456255 loss_att 30.264521 loss_ctc 33.263458 loss_rnnt 29.907003 hw_loss 0.399949 lr 0.00048290 rank 4
2023-02-23 01:47:50,351 DEBUG TRAIN Batch 12/7100 loss 11.724466 loss_att 13.353431 loss_ctc 11.972060 loss_rnnt 11.147777 hw_loss 0.408534 lr 0.00048289 rank 2
2023-02-23 01:47:50,352 DEBUG TRAIN Batch 12/7100 loss 15.538507 loss_att 19.693340 loss_ctc 18.742645 loss_rnnt 14.074173 hw_loss 0.386527 lr 0.00048290 rank 3
2023-02-23 01:47:50,379 DEBUG TRAIN Batch 12/7100 loss 19.682266 loss_att 18.778179 loss_ctc 19.563438 loss_rnnt 19.690649 hw_loss 0.353021 lr 0.00048288 rank 7
2023-02-23 01:49:04,340 DEBUG TRAIN Batch 12/7200 loss 9.655751 loss_att 17.019558 loss_ctc 11.762379 loss_rnnt 7.688138 hw_loss 0.401190 lr 0.00048266 rank 7
2023-02-23 01:49:04,345 DEBUG TRAIN Batch 12/7200 loss 16.058020 loss_att 19.445755 loss_ctc 20.745808 loss_rnnt 14.555118 hw_loss 0.375594 lr 0.00048268 rank 3
2023-02-23 01:49:04,348 DEBUG TRAIN Batch 12/7200 loss 8.742541 loss_att 12.695900 loss_ctc 10.721735 loss_rnnt 7.485423 hw_loss 0.379789 lr 0.00048268 rank 4
2023-02-23 01:49:04,350 DEBUG TRAIN Batch 12/7200 loss 11.245487 loss_att 14.575949 loss_ctc 14.115988 loss_rnnt 9.971570 hw_loss 0.422045 lr 0.00048273 rank 0
2023-02-23 01:49:04,349 DEBUG TRAIN Batch 12/7200 loss 15.087610 loss_att 20.214386 loss_ctc 14.353011 loss_rnnt 13.957815 hw_loss 0.379476 lr 0.00048268 rank 5
2023-02-23 01:49:04,351 DEBUG TRAIN Batch 12/7200 loss 9.321293 loss_att 13.853095 loss_ctc 15.298470 loss_rnnt 7.453115 hw_loss 0.309114 lr 0.00048272 rank 1
2023-02-23 01:49:04,358 DEBUG TRAIN Batch 12/7200 loss 19.823023 loss_att 25.116402 loss_ctc 26.627579 loss_rnnt 17.631294 hw_loss 0.423338 lr 0.00048266 rank 2
2023-02-23 01:49:04,367 DEBUG TRAIN Batch 12/7200 loss 17.377075 loss_att 19.889664 loss_ctc 19.630445 loss_rnnt 16.371069 hw_loss 0.380703 lr 0.00048269 rank 6
2023-02-23 01:50:16,776 DEBUG TRAIN Batch 12/7300 loss 11.981921 loss_att 16.554005 loss_ctc 12.796348 loss_rnnt 10.755614 hw_loss 0.381189 lr 0.00048245 rank 3
2023-02-23 01:50:16,778 DEBUG TRAIN Batch 12/7300 loss 9.038293 loss_att 13.268013 loss_ctc 12.852743 loss_rnnt 7.452586 hw_loss 0.433443 lr 0.00048244 rank 2
2023-02-23 01:50:16,779 DEBUG TRAIN Batch 12/7300 loss 18.633631 loss_att 24.145334 loss_ctc 19.210209 loss_rnnt 17.250828 hw_loss 0.381722 lr 0.00048243 rank 7
2023-02-23 01:50:16,780 DEBUG TRAIN Batch 12/7300 loss 11.369175 loss_att 16.613857 loss_ctc 17.050932 loss_rnnt 9.358929 hw_loss 0.382016 lr 0.00048251 rank 0
2023-02-23 01:50:16,781 DEBUG TRAIN Batch 12/7300 loss 30.628502 loss_att 29.591499 loss_ctc 37.072266 loss_rnnt 29.730728 hw_loss 0.461265 lr 0.00048247 rank 6
2023-02-23 01:50:16,782 DEBUG TRAIN Batch 12/7300 loss 19.653896 loss_att 21.808842 loss_ctc 24.304352 loss_rnnt 18.400860 hw_loss 0.378726 lr 0.00048250 rank 1
2023-02-23 01:50:16,782 DEBUG TRAIN Batch 12/7300 loss 11.657549 loss_att 14.080117 loss_ctc 13.866421 loss_rnnt 10.642254 hw_loss 0.442995 lr 0.00048245 rank 5
2023-02-23 01:50:16,785 DEBUG TRAIN Batch 12/7300 loss 23.855112 loss_att 26.119572 loss_ctc 29.603289 loss_rnnt 22.393414 hw_loss 0.454466 lr 0.00048245 rank 4
2023-02-23 01:51:30,979 DEBUG TRAIN Batch 12/7400 loss 17.901735 loss_att 28.370148 loss_ctc 28.723234 loss_rnnt 14.126970 hw_loss 0.446651 lr 0.00048224 rank 6
2023-02-23 01:51:30,982 DEBUG TRAIN Batch 12/7400 loss 11.567492 loss_att 14.412905 loss_ctc 16.377586 loss_rnnt 10.054707 hw_loss 0.566920 lr 0.00048221 rank 2
2023-02-23 01:51:30,983 DEBUG TRAIN Batch 12/7400 loss 15.072742 loss_att 21.434948 loss_ctc 22.212404 loss_rnnt 12.642851 hw_loss 0.385302 lr 0.00048223 rank 3
2023-02-23 01:51:30,984 DEBUG TRAIN Batch 12/7400 loss 23.320290 loss_att 26.831808 loss_ctc 27.678497 loss_rnnt 21.830719 hw_loss 0.386579 lr 0.00048221 rank 7
2023-02-23 01:51:30,985 DEBUG TRAIN Batch 12/7400 loss 22.024265 loss_att 23.456528 loss_ctc 26.044971 loss_rnnt 20.982403 hw_loss 0.411216 lr 0.00048223 rank 4
2023-02-23 01:51:30,986 DEBUG TRAIN Batch 12/7400 loss 16.315056 loss_att 19.933210 loss_ctc 25.381004 loss_rnnt 14.173068 hw_loss 0.392933 lr 0.00048228 rank 0
2023-02-23 01:51:30,986 DEBUG TRAIN Batch 12/7400 loss 7.460302 loss_att 12.167744 loss_ctc 9.142422 loss_rnnt 6.060071 hw_loss 0.439612 lr 0.00048223 rank 5
2023-02-23 01:51:30,986 DEBUG TRAIN Batch 12/7400 loss 11.222047 loss_att 15.086124 loss_ctc 18.149519 loss_rnnt 9.324096 hw_loss 0.377761 lr 0.00048227 rank 1
2023-02-23 01:52:46,347 DEBUG TRAIN Batch 12/7500 loss 19.654570 loss_att 20.766365 loss_ctc 25.331837 loss_rnnt 18.432674 hw_loss 0.454811 lr 0.00048200 rank 3
2023-02-23 01:52:46,348 DEBUG TRAIN Batch 12/7500 loss 11.356563 loss_att 15.304955 loss_ctc 12.186414 loss_rnnt 10.253581 hw_loss 0.379980 lr 0.00048198 rank 7
2023-02-23 01:52:46,347 DEBUG TRAIN Batch 12/7500 loss 16.765385 loss_att 20.956306 loss_ctc 21.768915 loss_rnnt 15.101542 hw_loss 0.297226 lr 0.00048202 rank 6
2023-02-23 01:52:46,350 DEBUG TRAIN Batch 12/7500 loss 19.021099 loss_att 23.736057 loss_ctc 23.304085 loss_rnnt 17.270672 hw_loss 0.443190 lr 0.00048206 rank 0
2023-02-23 01:52:46,351 DEBUG TRAIN Batch 12/7500 loss 33.167000 loss_att 33.873894 loss_ctc 39.006527 loss_rnnt 32.000057 hw_loss 0.463056 lr 0.00048201 rank 4
2023-02-23 01:52:46,351 DEBUG TRAIN Batch 12/7500 loss 6.912325 loss_att 10.537262 loss_ctc 8.065012 loss_rnnt 5.843743 hw_loss 0.356066 lr 0.00048199 rank 2
2023-02-23 01:52:46,350 DEBUG TRAIN Batch 12/7500 loss 12.133101 loss_att 17.777199 loss_ctc 17.950985 loss_rnnt 9.993793 hw_loss 0.440191 lr 0.00048200 rank 5
2023-02-23 01:52:46,351 DEBUG TRAIN Batch 12/7500 loss 4.136362 loss_att 8.281090 loss_ctc 7.390137 loss_rnnt 2.634271 hw_loss 0.448705 lr 0.00048205 rank 1
2023-02-23 01:53:59,050 DEBUG TRAIN Batch 12/7600 loss 19.440702 loss_att 21.798492 loss_ctc 22.654844 loss_rnnt 18.327377 hw_loss 0.399779 lr 0.00048178 rank 5
2023-02-23 01:53:59,056 DEBUG TRAIN Batch 12/7600 loss 18.337704 loss_att 20.263002 loss_ctc 21.301851 loss_rnnt 17.327116 hw_loss 0.431828 lr 0.00048176 rank 7
2023-02-23 01:53:59,059 DEBUG TRAIN Batch 12/7600 loss 40.027477 loss_att 42.032986 loss_ctc 40.676250 loss_rnnt 39.312508 hw_loss 0.426311 lr 0.00048183 rank 0
2023-02-23 01:53:59,064 DEBUG TRAIN Batch 12/7600 loss 11.574135 loss_att 10.051508 loss_ctc 13.775714 loss_rnnt 11.215572 hw_loss 0.692895 lr 0.00048178 rank 4
2023-02-23 01:53:59,063 DEBUG TRAIN Batch 12/7600 loss 9.828356 loss_att 11.691678 loss_ctc 11.901978 loss_rnnt 8.915804 hw_loss 0.493884 lr 0.00048178 rank 3
2023-02-23 01:53:59,064 DEBUG TRAIN Batch 12/7600 loss 15.234208 loss_att 21.076019 loss_ctc 18.401670 loss_rnnt 13.397168 hw_loss 0.461905 lr 0.00048179 rank 6
2023-02-23 01:53:59,066 DEBUG TRAIN Batch 12/7600 loss 11.748968 loss_att 12.830761 loss_ctc 14.385986 loss_rnnt 10.934465 hw_loss 0.462264 lr 0.00048182 rank 1
2023-02-23 01:53:59,114 DEBUG TRAIN Batch 12/7600 loss 10.191838 loss_att 10.652328 loss_ctc 11.152706 loss_rnnt 9.662457 hw_loss 0.579689 lr 0.00048176 rank 2
2023-02-23 01:55:12,367 DEBUG TRAIN Batch 12/7700 loss 12.589991 loss_att 17.571350 loss_ctc 13.703629 loss_rnnt 11.243849 hw_loss 0.377597 lr 0.00048160 rank 1
2023-02-23 01:55:12,368 DEBUG TRAIN Batch 12/7700 loss 11.161335 loss_att 13.824918 loss_ctc 13.993838 loss_rnnt 10.037907 hw_loss 0.399459 lr 0.00048154 rank 7
2023-02-23 01:55:12,369 DEBUG TRAIN Batch 12/7700 loss 14.975441 loss_att 13.977608 loss_ctc 17.029312 loss_rnnt 14.566974 hw_loss 0.626596 lr 0.00048157 rank 6
2023-02-23 01:55:12,370 DEBUG TRAIN Batch 12/7700 loss 16.895624 loss_att 19.830433 loss_ctc 19.661552 loss_rnnt 15.740633 hw_loss 0.373575 lr 0.00048156 rank 3
2023-02-23 01:55:12,372 DEBUG TRAIN Batch 12/7700 loss 20.562180 loss_att 23.458944 loss_ctc 23.911263 loss_rnnt 19.346518 hw_loss 0.355806 lr 0.00048156 rank 4
2023-02-23 01:55:12,372 DEBUG TRAIN Batch 12/7700 loss 12.864683 loss_att 12.611582 loss_ctc 16.523748 loss_rnnt 12.128663 hw_loss 0.560184 lr 0.00048156 rank 5
2023-02-23 01:55:12,379 DEBUG TRAIN Batch 12/7700 loss 10.736332 loss_att 17.799555 loss_ctc 13.634342 loss_rnnt 8.703616 hw_loss 0.438132 lr 0.00048154 rank 2
2023-02-23 01:55:12,421 DEBUG TRAIN Batch 12/7700 loss 9.018538 loss_att 9.722836 loss_ctc 11.311118 loss_rnnt 8.207035 hw_loss 0.684311 lr 0.00048161 rank 0
2023-02-23 01:56:27,659 DEBUG TRAIN Batch 12/7800 loss 22.494598 loss_att 25.557425 loss_ctc 29.125790 loss_rnnt 20.795980 hw_loss 0.378551 lr 0.00048133 rank 4
2023-02-23 01:56:27,666 DEBUG TRAIN Batch 12/7800 loss 24.051615 loss_att 25.837843 loss_ctc 31.914696 loss_rnnt 22.457958 hw_loss 0.352502 lr 0.00048139 rank 0
2023-02-23 01:56:27,667 DEBUG TRAIN Batch 12/7800 loss 28.143665 loss_att 28.778862 loss_ctc 38.882553 loss_rnnt 26.328083 hw_loss 0.481293 lr 0.00048138 rank 1
2023-02-23 01:56:27,668 DEBUG TRAIN Batch 12/7800 loss 19.754669 loss_att 26.444336 loss_ctc 27.619659 loss_rnnt 17.153105 hw_loss 0.403062 lr 0.00048133 rank 5
2023-02-23 01:56:27,671 DEBUG TRAIN Batch 12/7800 loss 12.083077 loss_att 19.383919 loss_ctc 15.491439 loss_rnnt 9.967831 hw_loss 0.376182 lr 0.00048131 rank 7
2023-02-23 01:56:27,672 DEBUG TRAIN Batch 12/7800 loss 14.687853 loss_att 17.042351 loss_ctc 19.843761 loss_rnnt 13.321327 hw_loss 0.390323 lr 0.00048133 rank 3
2023-02-23 01:56:27,674 DEBUG TRAIN Batch 12/7800 loss 26.610514 loss_att 28.129181 loss_ctc 33.219315 loss_rnnt 25.262350 hw_loss 0.306105 lr 0.00048132 rank 2
2023-02-23 01:56:27,694 DEBUG TRAIN Batch 12/7800 loss 19.591274 loss_att 29.175169 loss_ctc 29.349167 loss_rnnt 16.186407 hw_loss 0.350694 lr 0.00048135 rank 6
2023-02-23 01:57:41,599 DEBUG TRAIN Batch 12/7900 loss 12.387579 loss_att 16.703283 loss_ctc 14.499079 loss_rnnt 11.011386 hw_loss 0.434099 lr 0.00048111 rank 3
2023-02-23 01:57:41,600 DEBUG TRAIN Batch 12/7900 loss 16.248106 loss_att 20.183441 loss_ctc 26.919903 loss_rnnt 13.763969 hw_loss 0.514054 lr 0.00048109 rank 7
2023-02-23 01:57:41,604 DEBUG TRAIN Batch 12/7900 loss 22.383232 loss_att 26.173805 loss_ctc 23.973639 loss_rnnt 21.226341 hw_loss 0.350105 lr 0.00048111 rank 4
2023-02-23 01:57:41,606 DEBUG TRAIN Batch 12/7900 loss 14.816210 loss_att 22.914637 loss_ctc 21.905968 loss_rnnt 12.055901 hw_loss 0.366231 lr 0.00048115 rank 1
2023-02-23 01:57:41,608 DEBUG TRAIN Batch 12/7900 loss 13.636721 loss_att 15.216707 loss_ctc 18.007061 loss_rnnt 12.543480 hw_loss 0.364748 lr 0.00048109 rank 2
2023-02-23 01:57:41,608 DEBUG TRAIN Batch 12/7900 loss 21.307743 loss_att 21.915859 loss_ctc 23.681084 loss_rnnt 20.674770 hw_loss 0.365449 lr 0.00048111 rank 5
2023-02-23 01:57:41,609 DEBUG TRAIN Batch 12/7900 loss 10.311028 loss_att 14.730210 loss_ctc 14.055677 loss_rnnt 8.746985 hw_loss 0.339226 lr 0.00048116 rank 0
2023-02-23 01:57:41,610 DEBUG TRAIN Batch 12/7900 loss 10.456361 loss_att 15.925083 loss_ctc 16.911592 loss_rnnt 8.281549 hw_loss 0.413191 lr 0.00048112 rank 6
2023-02-23 01:58:54,628 DEBUG TRAIN Batch 12/8000 loss 20.288927 loss_att 19.776691 loss_ctc 20.718964 loss_rnnt 20.150620 hw_loss 0.343910 lr 0.00048093 rank 1
2023-02-23 01:58:54,636 DEBUG TRAIN Batch 12/8000 loss 19.150934 loss_att 26.095158 loss_ctc 27.506382 loss_rnnt 16.417641 hw_loss 0.431982 lr 0.00048087 rank 7
2023-02-23 01:58:54,645 DEBUG TRAIN Batch 12/8000 loss 22.931856 loss_att 28.438408 loss_ctc 26.508791 loss_rnnt 21.136440 hw_loss 0.407213 lr 0.00048089 rank 3
2023-02-23 01:58:54,646 DEBUG TRAIN Batch 12/8000 loss 12.834835 loss_att 16.371801 loss_ctc 16.998699 loss_rnnt 11.357800 hw_loss 0.402113 lr 0.00048089 rank 5
2023-02-23 01:58:54,645 DEBUG TRAIN Batch 12/8000 loss 16.911938 loss_att 23.945454 loss_ctc 22.282402 loss_rnnt 14.622999 hw_loss 0.311577 lr 0.00048090 rank 6
2023-02-23 01:58:54,648 DEBUG TRAIN Batch 12/8000 loss 11.533785 loss_att 14.476895 loss_ctc 11.420853 loss_rnnt 10.752540 hw_loss 0.389403 lr 0.00048089 rank 4
2023-02-23 01:58:54,657 DEBUG TRAIN Batch 12/8000 loss 30.237684 loss_att 32.860218 loss_ctc 35.636482 loss_rnnt 28.820766 hw_loss 0.323573 lr 0.00048087 rank 2
2023-02-23 01:58:54,695 DEBUG TRAIN Batch 12/8000 loss 10.214417 loss_att 13.132641 loss_ctc 12.829237 loss_rnnt 9.071627 hw_loss 0.394694 lr 0.00048094 rank 0
2023-02-23 02:00:08,405 DEBUG TRAIN Batch 12/8100 loss 12.860373 loss_att 15.552815 loss_ctc 12.967734 loss_rnnt 12.115437 hw_loss 0.360251 lr 0.00048067 rank 4
2023-02-23 02:00:08,408 DEBUG TRAIN Batch 12/8100 loss 23.153696 loss_att 27.018154 loss_ctc 27.804443 loss_rnnt 21.603905 hw_loss 0.294001 lr 0.00048072 rank 0
2023-02-23 02:00:08,420 DEBUG TRAIN Batch 12/8100 loss 10.677698 loss_att 14.399849 loss_ctc 12.320029 loss_rnnt 9.451550 hw_loss 0.492640 lr 0.00048068 rank 6
2023-02-23 02:00:08,421 DEBUG TRAIN Batch 12/8100 loss 8.683995 loss_att 12.445794 loss_ctc 11.565208 loss_rnnt 7.340121 hw_loss 0.388786 lr 0.00048064 rank 7
2023-02-23 02:00:08,422 DEBUG TRAIN Batch 12/8100 loss 15.510275 loss_att 22.297367 loss_ctc 24.381966 loss_rnnt 12.763523 hw_loss 0.387074 lr 0.00048066 rank 3
2023-02-23 02:00:08,423 DEBUG TRAIN Batch 12/8100 loss 16.888212 loss_att 17.532665 loss_ctc 18.611547 loss_rnnt 16.288919 hw_loss 0.451169 lr 0.00048065 rank 2
2023-02-23 02:00:08,422 DEBUG TRAIN Batch 12/8100 loss 15.109411 loss_att 19.379930 loss_ctc 18.067358 loss_rnnt 13.650038 hw_loss 0.395395 lr 0.00048066 rank 5
2023-02-23 02:00:08,423 DEBUG TRAIN Batch 12/8100 loss 34.270912 loss_att 38.217152 loss_ctc 42.668556 loss_rnnt 32.145454 hw_loss 0.405984 lr 0.00048071 rank 1
2023-02-23 02:01:22,641 DEBUG TRAIN Batch 12/8200 loss 13.101683 loss_att 19.881514 loss_ctc 16.899155 loss_rnnt 10.967579 hw_loss 0.509640 lr 0.00048044 rank 5
2023-02-23 02:01:22,648 DEBUG TRAIN Batch 12/8200 loss 12.049014 loss_att 17.179642 loss_ctc 14.604929 loss_rnnt 10.462814 hw_loss 0.411160 lr 0.00048042 rank 7
2023-02-23 02:01:22,650 DEBUG TRAIN Batch 12/8200 loss 12.283110 loss_att 14.433891 loss_ctc 15.684921 loss_rnnt 11.158377 hw_loss 0.451881 lr 0.00048045 rank 4
2023-02-23 02:01:22,651 DEBUG TRAIN Batch 12/8200 loss 12.764772 loss_att 15.201227 loss_ctc 16.152220 loss_rnnt 11.554014 hw_loss 0.509640 lr 0.00048044 rank 3
2023-02-23 02:01:22,652 DEBUG TRAIN Batch 12/8200 loss 24.485506 loss_att 23.025246 loss_ctc 26.850813 loss_rnnt 24.217960 hw_loss 0.457916 lr 0.00048050 rank 0
2023-02-23 02:01:22,655 DEBUG TRAIN Batch 12/8200 loss 21.225885 loss_att 23.996492 loss_ctc 25.357903 loss_rnnt 19.853819 hw_loss 0.500645 lr 0.00048043 rank 2
2023-02-23 02:01:22,657 DEBUG TRAIN Batch 12/8200 loss 10.425808 loss_att 12.485588 loss_ctc 12.858785 loss_rnnt 9.494076 hw_loss 0.366337 lr 0.00048049 rank 1
2023-02-23 02:01:22,658 DEBUG TRAIN Batch 12/8200 loss 8.625092 loss_att 12.579567 loss_ctc 9.294641 loss_rnnt 7.509408 hw_loss 0.441593 lr 0.00048046 rank 6
2023-02-23 02:02:35,104 DEBUG TRAIN Batch 12/8300 loss 10.325753 loss_att 10.409064 loss_ctc 14.118409 loss_rnnt 9.489527 hw_loss 0.588519 lr 0.00048020 rank 7
2023-02-23 02:02:35,107 DEBUG TRAIN Batch 12/8300 loss 12.378179 loss_att 17.476673 loss_ctc 14.907057 loss_rnnt 10.836611 hw_loss 0.346283 lr 0.00048022 rank 5
2023-02-23 02:02:35,111 DEBUG TRAIN Batch 12/8300 loss 12.229673 loss_att 15.579015 loss_ctc 14.541473 loss_rnnt 10.990742 hw_loss 0.489045 lr 0.00048027 rank 0
2023-02-23 02:02:35,111 DEBUG TRAIN Batch 12/8300 loss 8.660152 loss_att 8.872537 loss_ctc 10.359962 loss_rnnt 8.161803 hw_loss 0.429809 lr 0.00048023 rank 6
2023-02-23 02:02:35,111 DEBUG TRAIN Batch 12/8300 loss 11.664930 loss_att 21.832165 loss_ctc 12.425926 loss_rnnt 9.312118 hw_loss 0.408560 lr 0.00048022 rank 3
2023-02-23 02:02:35,113 DEBUG TRAIN Batch 12/8300 loss 17.956753 loss_att 24.627323 loss_ctc 21.914175 loss_rnnt 15.880587 hw_loss 0.401988 lr 0.00048022 rank 4
2023-02-23 02:02:35,114 DEBUG TRAIN Batch 12/8300 loss 12.651278 loss_att 12.910294 loss_ctc 15.124907 loss_rnnt 12.063557 hw_loss 0.386438 lr 0.00048027 rank 1
2023-02-23 02:02:35,115 DEBUG TRAIN Batch 12/8300 loss 15.563985 loss_att 18.816259 loss_ctc 18.628115 loss_rnnt 14.230646 hw_loss 0.514373 lr 0.00048021 rank 2
2023-02-23 02:03:15,900 DEBUG CV Batch 12/0 loss 2.815511 loss_att 2.202672 loss_ctc 2.818821 loss_rnnt 2.464960 hw_loss 0.886271 history loss 2.711233 rank 4
2023-02-23 02:03:15,906 DEBUG CV Batch 12/0 loss 2.815511 loss_att 2.202672 loss_ctc 2.818821 loss_rnnt 2.464960 hw_loss 0.886271 history loss 2.711233 rank 2
2023-02-23 02:03:15,908 DEBUG CV Batch 12/0 loss 2.815511 loss_att 2.202672 loss_ctc 2.818821 loss_rnnt 2.464960 hw_loss 0.886271 history loss 2.711233 rank 1
2023-02-23 02:03:15,908 DEBUG CV Batch 12/0 loss 2.815511 loss_att 2.202672 loss_ctc 2.818821 loss_rnnt 2.464960 hw_loss 0.886271 history loss 2.711233 rank 5
2023-02-23 02:03:15,912 DEBUG CV Batch 12/0 loss 2.815511 loss_att 2.202672 loss_ctc 2.818821 loss_rnnt 2.464960 hw_loss 0.886271 history loss 2.711233 rank 6
2023-02-23 02:03:15,914 DEBUG CV Batch 12/0 loss 2.815511 loss_att 2.202672 loss_ctc 2.818821 loss_rnnt 2.464960 hw_loss 0.886271 history loss 2.711233 rank 3
2023-02-23 02:03:15,917 DEBUG CV Batch 12/0 loss 2.815511 loss_att 2.202672 loss_ctc 2.818821 loss_rnnt 2.464960 hw_loss 0.886271 history loss 2.711233 rank 0
2023-02-23 02:03:15,921 DEBUG CV Batch 12/0 loss 2.815511 loss_att 2.202672 loss_ctc 2.818821 loss_rnnt 2.464960 hw_loss 0.886271 history loss 2.711233 rank 7
2023-02-23 02:03:27,359 DEBUG CV Batch 12/100 loss 10.478541 loss_att 10.422475 loss_ctc 12.546339 loss_rnnt 9.971232 hw_loss 0.455279 history loss 4.633540 rank 4
2023-02-23 02:03:27,378 DEBUG CV Batch 12/100 loss 10.478541 loss_att 10.422475 loss_ctc 12.546339 loss_rnnt 9.971232 hw_loss 0.455279 history loss 4.633540 rank 6
2023-02-23 02:03:27,455 DEBUG CV Batch 12/100 loss 10.478541 loss_att 10.422475 loss_ctc 12.546339 loss_rnnt 9.971232 hw_loss 0.455279 history loss 4.633540 rank 0
2023-02-23 02:03:27,558 DEBUG CV Batch 12/100 loss 10.478541 loss_att 10.422475 loss_ctc 12.546339 loss_rnnt 9.971232 hw_loss 0.455279 history loss 4.633540 rank 5
2023-02-23 02:03:27,560 DEBUG CV Batch 12/100 loss 10.478541 loss_att 10.422475 loss_ctc 12.546339 loss_rnnt 9.971232 hw_loss 0.455279 history loss 4.633540 rank 2
2023-02-23 02:03:27,576 DEBUG CV Batch 12/100 loss 10.478541 loss_att 10.422475 loss_ctc 12.546339 loss_rnnt 9.971232 hw_loss 0.455279 history loss 4.633540 rank 7
2023-02-23 02:03:27,705 DEBUG CV Batch 12/100 loss 10.478541 loss_att 10.422475 loss_ctc 12.546339 loss_rnnt 9.971232 hw_loss 0.455279 history loss 4.633540 rank 3
2023-02-23 02:03:27,972 DEBUG CV Batch 12/100 loss 10.478541 loss_att 10.422475 loss_ctc 12.546339 loss_rnnt 9.971232 hw_loss 0.455279 history loss 4.633540 rank 1
2023-02-23 02:03:40,852 DEBUG CV Batch 12/200 loss 7.815283 loss_att 21.960413 loss_ctc 9.562880 loss_rnnt 4.602533 hw_loss 0.282583 history loss 5.388911 rank 6
2023-02-23 02:03:41,141 DEBUG CV Batch 12/200 loss 7.815283 loss_att 21.960413 loss_ctc 9.562880 loss_rnnt 4.602533 hw_loss 0.282583 history loss 5.388911 rank 2
2023-02-23 02:03:41,296 DEBUG CV Batch 12/200 loss 7.815283 loss_att 21.960413 loss_ctc 9.562880 loss_rnnt 4.602533 hw_loss 0.282583 history loss 5.388911 rank 4
2023-02-23 02:03:41,298 DEBUG CV Batch 12/200 loss 7.815283 loss_att 21.960413 loss_ctc 9.562880 loss_rnnt 4.602533 hw_loss 0.282583 history loss 5.388911 rank 0
2023-02-23 02:03:41,363 DEBUG CV Batch 12/200 loss 7.815283 loss_att 21.960413 loss_ctc 9.562880 loss_rnnt 4.602533 hw_loss 0.282583 history loss 5.388911 rank 7
2023-02-23 02:03:41,478 DEBUG CV Batch 12/200 loss 7.815283 loss_att 21.960413 loss_ctc 9.562880 loss_rnnt 4.602533 hw_loss 0.282583 history loss 5.388911 rank 1
2023-02-23 02:03:41,626 DEBUG CV Batch 12/200 loss 7.815283 loss_att 21.960413 loss_ctc 9.562880 loss_rnnt 4.602533 hw_loss 0.282583 history loss 5.388911 rank 3
2023-02-23 02:03:41,907 DEBUG CV Batch 12/200 loss 7.815283 loss_att 21.960413 loss_ctc 9.562880 loss_rnnt 4.602533 hw_loss 0.282583 history loss 5.388911 rank 5
2023-02-23 02:03:52,778 DEBUG CV Batch 12/300 loss 5.998830 loss_att 6.376645 loss_ctc 7.384686 loss_rnnt 5.463066 hw_loss 0.516414 history loss 5.549544 rank 6
2023-02-23 02:03:53,179 DEBUG CV Batch 12/300 loss 5.998830 loss_att 6.376645 loss_ctc 7.384686 loss_rnnt 5.463066 hw_loss 0.516414 history loss 5.549544 rank 4
2023-02-23 02:03:53,296 DEBUG CV Batch 12/300 loss 5.998830 loss_att 6.376645 loss_ctc 7.384686 loss_rnnt 5.463066 hw_loss 0.516414 history loss 5.549544 rank 2
2023-02-23 02:03:53,468 DEBUG CV Batch 12/300 loss 5.998830 loss_att 6.376645 loss_ctc 7.384686 loss_rnnt 5.463066 hw_loss 0.516414 history loss 5.549544 rank 0
2023-02-23 02:03:53,696 DEBUG CV Batch 12/300 loss 5.998830 loss_att 6.376645 loss_ctc 7.384686 loss_rnnt 5.463066 hw_loss 0.516414 history loss 5.549544 rank 1
2023-02-23 02:03:53,822 DEBUG CV Batch 12/300 loss 5.998830 loss_att 6.376645 loss_ctc 7.384686 loss_rnnt 5.463066 hw_loss 0.516414 history loss 5.549544 rank 7
2023-02-23 02:03:54,154 DEBUG CV Batch 12/300 loss 5.998830 loss_att 6.376645 loss_ctc 7.384686 loss_rnnt 5.463066 hw_loss 0.516414 history loss 5.549544 rank 3
2023-02-23 02:03:54,317 DEBUG CV Batch 12/300 loss 5.998830 loss_att 6.376645 loss_ctc 7.384686 loss_rnnt 5.463066 hw_loss 0.516414 history loss 5.549544 rank 5
2023-02-23 02:04:04,726 DEBUG CV Batch 12/400 loss 28.318756 loss_att 108.745972 loss_ctc 12.497469 loss_rnnt 14.211805 hw_loss 0.245648 history loss 6.779819 rank 6
2023-02-23 02:04:05,262 DEBUG CV Batch 12/400 loss 28.318756 loss_att 108.745972 loss_ctc 12.497469 loss_rnnt 14.211805 hw_loss 0.245648 history loss 6.779819 rank 4
2023-02-23 02:04:05,613 DEBUG CV Batch 12/400 loss 28.318756 loss_att 108.745972 loss_ctc 12.497469 loss_rnnt 14.211805 hw_loss 0.245648 history loss 6.779819 rank 2
2023-02-23 02:04:05,673 DEBUG CV Batch 12/400 loss 28.318756 loss_att 108.745972 loss_ctc 12.497469 loss_rnnt 14.211805 hw_loss 0.245648 history loss 6.779819 rank 0
2023-02-23 02:04:05,729 DEBUG CV Batch 12/400 loss 28.318756 loss_att 108.745972 loss_ctc 12.497469 loss_rnnt 14.211805 hw_loss 0.245648 history loss 6.779819 rank 1
2023-02-23 02:04:06,258 DEBUG CV Batch 12/400 loss 28.318756 loss_att 108.745972 loss_ctc 12.497469 loss_rnnt 14.211805 hw_loss 0.245648 history loss 6.779819 rank 7
2023-02-23 02:04:06,740 DEBUG CV Batch 12/400 loss 28.318756 loss_att 108.745972 loss_ctc 12.497469 loss_rnnt 14.211805 hw_loss 0.245648 history loss 6.779819 rank 3
2023-02-23 02:04:06,923 DEBUG CV Batch 12/400 loss 28.318756 loss_att 108.745972 loss_ctc 12.497469 loss_rnnt 14.211805 hw_loss 0.245648 history loss 6.779819 rank 5
2023-02-23 02:04:15,044 DEBUG CV Batch 12/500 loss 7.104841 loss_att 8.003201 loss_ctc 8.931319 loss_rnnt 6.445500 hw_loss 0.442758 history loss 7.769522 rank 6
2023-02-23 02:04:15,534 DEBUG CV Batch 12/500 loss 7.104841 loss_att 8.003201 loss_ctc 8.931319 loss_rnnt 6.445500 hw_loss 0.442758 history loss 7.769522 rank 4
2023-02-23 02:04:16,012 DEBUG CV Batch 12/500 loss 7.104841 loss_att 8.003201 loss_ctc 8.931319 loss_rnnt 6.445500 hw_loss 0.442758 history loss 7.769521 rank 1
2023-02-23 02:04:16,142 DEBUG CV Batch 12/500 loss 7.104841 loss_att 8.003201 loss_ctc 8.931319 loss_rnnt 6.445500 hw_loss 0.442758 history loss 7.769521 rank 2
2023-02-23 02:04:16,236 DEBUG CV Batch 12/500 loss 7.104841 loss_att 8.003201 loss_ctc 8.931319 loss_rnnt 6.445500 hw_loss 0.442758 history loss 7.769522 rank 0
2023-02-23 02:04:17,165 DEBUG CV Batch 12/500 loss 7.104841 loss_att 8.003201 loss_ctc 8.931319 loss_rnnt 6.445500 hw_loss 0.442758 history loss 7.769522 rank 7
2023-02-23 02:04:17,342 DEBUG CV Batch 12/500 loss 7.104841 loss_att 8.003201 loss_ctc 8.931319 loss_rnnt 6.445500 hw_loss 0.442758 history loss 7.769522 rank 5
2023-02-23 02:04:17,823 DEBUG CV Batch 12/500 loss 7.104841 loss_att 8.003201 loss_ctc 8.931319 loss_rnnt 6.445500 hw_loss 0.442758 history loss 7.769522 rank 3
2023-02-23 02:04:27,340 DEBUG CV Batch 12/600 loss 9.772493 loss_att 9.392133 loss_ctc 10.781696 loss_rnnt 9.311669 hw_loss 0.754378 history loss 8.857780 rank 6
2023-02-23 02:04:27,565 DEBUG CV Batch 12/600 loss 9.772493 loss_att 9.392133 loss_ctc 10.781696 loss_rnnt 9.311669 hw_loss 0.754378 history loss 8.857780 rank 4
2023-02-23 02:04:27,981 DEBUG CV Batch 12/600 loss 9.772493 loss_att 9.392133 loss_ctc 10.781696 loss_rnnt 9.311669 hw_loss 0.754378 history loss 8.857780 rank 1
2023-02-23 02:04:28,465 DEBUG CV Batch 12/600 loss 9.772493 loss_att 9.392133 loss_ctc 10.781696 loss_rnnt 9.311669 hw_loss 0.754378 history loss 8.857780 rank 2
2023-02-23 02:04:28,880 DEBUG CV Batch 12/600 loss 9.772493 loss_att 9.392133 loss_ctc 10.781696 loss_rnnt 9.311669 hw_loss 0.754378 history loss 8.857780 rank 0
2023-02-23 02:04:29,352 DEBUG CV Batch 12/600 loss 9.772493 loss_att 9.392133 loss_ctc 10.781696 loss_rnnt 9.311669 hw_loss 0.754378 history loss 8.857780 rank 5
2023-02-23 02:04:29,635 DEBUG CV Batch 12/600 loss 9.772493 loss_att 9.392133 loss_ctc 10.781696 loss_rnnt 9.311669 hw_loss 0.754378 history loss 8.857780 rank 7
2023-02-23 02:04:30,462 DEBUG CV Batch 12/600 loss 9.772493 loss_att 9.392133 loss_ctc 10.781696 loss_rnnt 9.311669 hw_loss 0.754378 history loss 8.857780 rank 3
2023-02-23 02:04:38,942 DEBUG CV Batch 12/700 loss 27.727764 loss_att 89.414536 loss_ctc 22.839691 loss_rnnt 15.870374 hw_loss 0.322083 history loss 9.762213 rank 6
2023-02-23 02:04:38,944 DEBUG CV Batch 12/700 loss 27.727764 loss_att 89.414536 loss_ctc 22.839691 loss_rnnt 15.870374 hw_loss 0.322083 history loss 9.762213 rank 4
2023-02-23 02:04:39,265 DEBUG CV Batch 12/700 loss 27.727764 loss_att 89.414536 loss_ctc 22.839691 loss_rnnt 15.870374 hw_loss 0.322083 history loss 9.762213 rank 1
2023-02-23 02:04:39,950 DEBUG CV Batch 12/700 loss 27.727764 loss_att 89.414536 loss_ctc 22.839691 loss_rnnt 15.870374 hw_loss 0.322083 history loss 9.762213 rank 2
2023-02-23 02:04:40,321 DEBUG CV Batch 12/700 loss 27.727764 loss_att 89.414536 loss_ctc 22.839691 loss_rnnt 15.870374 hw_loss 0.322083 history loss 9.762213 rank 0
2023-02-23 02:04:40,642 DEBUG CV Batch 12/700 loss 27.727764 loss_att 89.414536 loss_ctc 22.839691 loss_rnnt 15.870374 hw_loss 0.322083 history loss 9.762213 rank 5
2023-02-23 02:04:41,508 DEBUG CV Batch 12/700 loss 27.727764 loss_att 89.414536 loss_ctc 22.839691 loss_rnnt 15.870374 hw_loss 0.322083 history loss 9.762213 rank 7
2023-02-23 02:04:42,428 DEBUG CV Batch 12/700 loss 27.727764 loss_att 89.414536 loss_ctc 22.839691 loss_rnnt 15.870374 hw_loss 0.322083 history loss 9.762213 rank 3
2023-02-23 02:04:50,346 DEBUG CV Batch 12/800 loss 15.015248 loss_att 13.252925 loss_ctc 17.747154 loss_rnnt 14.736214 hw_loss 0.501085 history loss 9.075565 rank 6
2023-02-23 02:04:50,574 DEBUG CV Batch 12/800 loss 15.015248 loss_att 13.252925 loss_ctc 17.747154 loss_rnnt 14.736214 hw_loss 0.501085 history loss 9.075565 rank 4
2023-02-23 02:04:51,287 DEBUG CV Batch 12/800 loss 15.015248 loss_att 13.252925 loss_ctc 17.747154 loss_rnnt 14.736214 hw_loss 0.501085 history loss 9.075565 rank 1
2023-02-23 02:04:51,839 DEBUG CV Batch 12/800 loss 15.015248 loss_att 13.252925 loss_ctc 17.747154 loss_rnnt 14.736214 hw_loss 0.501085 history loss 9.075565 rank 2
2023-02-23 02:04:51,884 DEBUG CV Batch 12/800 loss 15.015248 loss_att 13.252925 loss_ctc 17.747154 loss_rnnt 14.736214 hw_loss 0.501085 history loss 9.075565 rank 0
2023-02-23 02:04:52,547 DEBUG CV Batch 12/800 loss 15.015248 loss_att 13.252925 loss_ctc 17.747154 loss_rnnt 14.736214 hw_loss 0.501085 history loss 9.075565 rank 5
2023-02-23 02:04:53,121 DEBUG CV Batch 12/800 loss 15.015248 loss_att 13.252925 loss_ctc 17.747154 loss_rnnt 14.736214 hw_loss 0.501085 history loss 9.075565 rank 7
2023-02-23 02:04:54,168 DEBUG CV Batch 12/800 loss 15.015248 loss_att 13.252925 loss_ctc 17.747154 loss_rnnt 14.736214 hw_loss 0.501085 history loss 9.075565 rank 3
2023-02-23 02:05:03,827 DEBUG CV Batch 12/900 loss 16.238119 loss_att 28.851601 loss_ctc 19.031826 loss_rnnt 13.198759 hw_loss 0.270318 history loss 8.825958 rank 6
2023-02-23 02:05:04,112 DEBUG CV Batch 12/900 loss 16.238119 loss_att 28.851601 loss_ctc 19.031826 loss_rnnt 13.198759 hw_loss 0.270318 history loss 8.825958 rank 4
2023-02-23 02:05:04,849 DEBUG CV Batch 12/900 loss 16.238119 loss_att 28.851601 loss_ctc 19.031826 loss_rnnt 13.198759 hw_loss 0.270318 history loss 8.825958 rank 1
2023-02-23 02:05:05,844 DEBUG CV Batch 12/900 loss 16.238119 loss_att 28.851601 loss_ctc 19.031826 loss_rnnt 13.198759 hw_loss 0.270318 history loss 8.825958 rank 2
2023-02-23 02:05:06,027 DEBUG CV Batch 12/900 loss 16.238119 loss_att 28.851601 loss_ctc 19.031826 loss_rnnt 13.198759 hw_loss 0.270318 history loss 8.825958 rank 0
2023-02-23 02:05:06,283 DEBUG CV Batch 12/900 loss 16.238119 loss_att 28.851601 loss_ctc 19.031826 loss_rnnt 13.198759 hw_loss 0.270318 history loss 8.825958 rank 5
2023-02-23 02:05:07,059 DEBUG CV Batch 12/900 loss 16.238119 loss_att 28.851601 loss_ctc 19.031826 loss_rnnt 13.198759 hw_loss 0.270318 history loss 8.825958 rank 7
2023-02-23 02:05:08,124 DEBUG CV Batch 12/900 loss 16.238119 loss_att 28.851601 loss_ctc 19.031826 loss_rnnt 13.198759 hw_loss 0.270318 history loss 8.825958 rank 3
2023-02-23 02:05:16,127 DEBUG CV Batch 12/1000 loss 5.514743 loss_att 6.090455 loss_ctc 5.184484 loss_rnnt 5.149661 hw_loss 0.551202 history loss 8.528342 rank 6
2023-02-23 02:05:16,193 DEBUG CV Batch 12/1000 loss 5.514743 loss_att 6.090455 loss_ctc 5.184484 loss_rnnt 5.149661 hw_loss 0.551202 history loss 8.528342 rank 4
2023-02-23 02:05:17,268 DEBUG CV Batch 12/1000 loss 5.514743 loss_att 6.090455 loss_ctc 5.184484 loss_rnnt 5.149661 hw_loss 0.551202 history loss 8.528342 rank 1
2023-02-23 02:05:18,483 DEBUG CV Batch 12/1000 loss 5.514743 loss_att 6.090455 loss_ctc 5.184484 loss_rnnt 5.149661 hw_loss 0.551202 history loss 8.528342 rank 0
2023-02-23 02:05:18,519 DEBUG CV Batch 12/1000 loss 5.514743 loss_att 6.090455 loss_ctc 5.184484 loss_rnnt 5.149661 hw_loss 0.551202 history loss 8.528342 rank 2
2023-02-23 02:05:18,787 DEBUG CV Batch 12/1000 loss 5.514743 loss_att 6.090455 loss_ctc 5.184484 loss_rnnt 5.149661 hw_loss 0.551202 history loss 8.528342 rank 5
2023-02-23 02:05:19,826 DEBUG CV Batch 12/1000 loss 5.514743 loss_att 6.090455 loss_ctc 5.184484 loss_rnnt 5.149661 hw_loss 0.551202 history loss 8.528342 rank 7
2023-02-23 02:05:21,028 DEBUG CV Batch 12/1000 loss 5.514743 loss_att 6.090455 loss_ctc 5.184484 loss_rnnt 5.149661 hw_loss 0.551202 history loss 8.528342 rank 3
2023-02-23 02:05:28,022 DEBUG CV Batch 12/1100 loss 9.151106 loss_att 7.604623 loss_ctc 10.713222 loss_rnnt 8.814543 hw_loss 0.820459 history loss 8.509497 rank 4
2023-02-23 02:05:28,199 DEBUG CV Batch 12/1100 loss 9.151106 loss_att 7.604623 loss_ctc 10.713222 loss_rnnt 8.814543 hw_loss 0.820459 history loss 8.509497 rank 6
2023-02-23 02:05:29,209 DEBUG CV Batch 12/1100 loss 9.151106 loss_att 7.604623 loss_ctc 10.713222 loss_rnnt 8.814543 hw_loss 0.820459 history loss 8.509497 rank 1
2023-02-23 02:05:30,733 DEBUG CV Batch 12/1100 loss 9.151106 loss_att 7.604623 loss_ctc 10.713222 loss_rnnt 8.814543 hw_loss 0.820459 history loss 8.509497 rank 0
2023-02-23 02:05:30,972 DEBUG CV Batch 12/1100 loss 9.151106 loss_att 7.604623 loss_ctc 10.713222 loss_rnnt 8.814543 hw_loss 0.820459 history loss 8.509497 rank 5
2023-02-23 02:05:31,064 DEBUG CV Batch 12/1100 loss 9.151106 loss_att 7.604623 loss_ctc 10.713222 loss_rnnt 8.814543 hw_loss 0.820459 history loss 8.509497 rank 2
2023-02-23 02:05:32,295 DEBUG CV Batch 12/1100 loss 9.151106 loss_att 7.604623 loss_ctc 10.713222 loss_rnnt 8.814543 hw_loss 0.820459 history loss 8.509497 rank 7
2023-02-23 02:05:33,520 DEBUG CV Batch 12/1100 loss 9.151106 loss_att 7.604623 loss_ctc 10.713222 loss_rnnt 8.814543 hw_loss 0.820459 history loss 8.509497 rank 3
2023-02-23 02:05:38,248 DEBUG CV Batch 12/1200 loss 8.312951 loss_att 9.163065 loss_ctc 10.872062 loss_rnnt 7.556672 hw_loss 0.459452 history loss 8.930730 rank 4
2023-02-23 02:05:38,743 DEBUG CV Batch 12/1200 loss 8.312951 loss_att 9.163065 loss_ctc 10.872062 loss_rnnt 7.556672 hw_loss 0.459452 history loss 8.930730 rank 6
2023-02-23 02:05:39,736 DEBUG CV Batch 12/1200 loss 8.312951 loss_att 9.163065 loss_ctc 10.872062 loss_rnnt 7.556672 hw_loss 0.459452 history loss 8.930730 rank 1
2023-02-23 02:05:41,310 DEBUG CV Batch 12/1200 loss 8.312951 loss_att 9.163065 loss_ctc 10.872062 loss_rnnt 7.556672 hw_loss 0.459452 history loss 8.930730 rank 0
2023-02-23 02:05:41,591 DEBUG CV Batch 12/1200 loss 8.312951 loss_att 9.163065 loss_ctc 10.872062 loss_rnnt 7.556672 hw_loss 0.459452 history loss 8.930730 rank 5
2023-02-23 02:05:42,251 DEBUG CV Batch 12/1200 loss 8.312951 loss_att 9.163065 loss_ctc 10.872062 loss_rnnt 7.556672 hw_loss 0.459452 history loss 8.930730 rank 2
2023-02-23 02:05:43,397 DEBUG CV Batch 12/1200 loss 8.312951 loss_att 9.163065 loss_ctc 10.872062 loss_rnnt 7.556672 hw_loss 0.459452 history loss 8.930730 rank 7
2023-02-23 02:05:44,746 DEBUG CV Batch 12/1200 loss 8.312951 loss_att 9.163065 loss_ctc 10.872062 loss_rnnt 7.556672 hw_loss 0.459452 history loss 8.930730 rank 3
2023-02-23 02:05:50,263 DEBUG CV Batch 12/1300 loss 8.339651 loss_att 7.403455 loss_ctc 8.979283 loss_rnnt 8.086866 hw_loss 0.665139 history loss 9.279156 rank 4
2023-02-23 02:05:50,758 DEBUG CV Batch 12/1300 loss 8.339651 loss_att 7.403455 loss_ctc 8.979283 loss_rnnt 8.086866 hw_loss 0.665139 history loss 9.279156 rank 6
2023-02-23 02:05:51,785 DEBUG CV Batch 12/1300 loss 8.339651 loss_att 7.403455 loss_ctc 8.979283 loss_rnnt 8.086866 hw_loss 0.665139 history loss 9.279156 rank 1
2023-02-23 02:05:53,457 DEBUG CV Batch 12/1300 loss 8.339651 loss_att 7.403455 loss_ctc 8.979283 loss_rnnt 8.086866 hw_loss 0.665139 history loss 9.279156 rank 0
2023-02-23 02:05:53,530 DEBUG CV Batch 12/1300 loss 8.339651 loss_att 7.403455 loss_ctc 8.979283 loss_rnnt 8.086866 hw_loss 0.665139 history loss 9.279156 rank 5
2023-02-23 02:05:54,526 DEBUG CV Batch 12/1300 loss 8.339651 loss_att 7.403455 loss_ctc 8.979283 loss_rnnt 8.086866 hw_loss 0.665139 history loss 9.279156 rank 2
2023-02-23 02:05:55,955 DEBUG CV Batch 12/1300 loss 8.339651 loss_att 7.403455 loss_ctc 8.979283 loss_rnnt 8.086866 hw_loss 0.665139 history loss 9.279156 rank 7
2023-02-23 02:05:57,297 DEBUG CV Batch 12/1300 loss 8.339651 loss_att 7.403455 loss_ctc 8.979283 loss_rnnt 8.086866 hw_loss 0.665139 history loss 9.279156 rank 3
2023-02-23 02:06:01,308 DEBUG CV Batch 12/1400 loss 13.374419 loss_att 38.577183 loss_ctc 11.473922 loss_rnnt 8.375364 hw_loss 0.397314 history loss 9.718226 rank 4
2023-02-23 02:06:01,872 DEBUG CV Batch 12/1400 loss 13.374419 loss_att 38.577183 loss_ctc 11.473922 loss_rnnt 8.375364 hw_loss 0.397314 history loss 9.718226 rank 6
2023-02-23 02:06:03,489 DEBUG CV Batch 12/1400 loss 13.374419 loss_att 38.577183 loss_ctc 11.473922 loss_rnnt 8.375364 hw_loss 0.397314 history loss 9.718226 rank 1
2023-02-23 02:06:04,776 DEBUG CV Batch 12/1400 loss 13.374419 loss_att 38.577183 loss_ctc 11.473922 loss_rnnt 8.375364 hw_loss 0.397314 history loss 9.718226 rank 0
2023-02-23 02:06:04,874 DEBUG CV Batch 12/1400 loss 13.374419 loss_att 38.577183 loss_ctc 11.473922 loss_rnnt 8.375364 hw_loss 0.397314 history loss 9.718226 rank 5
2023-02-23 02:06:06,373 DEBUG CV Batch 12/1400 loss 13.374419 loss_att 38.577183 loss_ctc 11.473922 loss_rnnt 8.375364 hw_loss 0.397314 history loss 9.718226 rank 2
2023-02-23 02:06:07,717 DEBUG CV Batch 12/1400 loss 13.374419 loss_att 38.577183 loss_ctc 11.473922 loss_rnnt 8.375364 hw_loss 0.397314 history loss 9.718226 rank 7
2023-02-23 02:06:09,195 DEBUG CV Batch 12/1400 loss 13.374419 loss_att 38.577183 loss_ctc 11.473922 loss_rnnt 8.375364 hw_loss 0.397314 history loss 9.718226 rank 3
2023-02-23 02:06:13,093 DEBUG CV Batch 12/1500 loss 11.183717 loss_att 10.444768 loss_ctc 9.880507 loss_rnnt 11.277629 hw_loss 0.426823 history loss 9.487993 rank 4
2023-02-23 02:06:13,409 DEBUG CV Batch 12/1500 loss 11.183717 loss_att 10.444768 loss_ctc 9.880507 loss_rnnt 11.277629 hw_loss 0.426823 history loss 9.487993 rank 6
2023-02-23 02:06:15,343 DEBUG CV Batch 12/1500 loss 11.183717 loss_att 10.444768 loss_ctc 9.880507 loss_rnnt 11.277629 hw_loss 0.426823 history loss 9.487993 rank 1
2023-02-23 02:06:16,256 DEBUG CV Batch 12/1500 loss 11.183717 loss_att 10.444768 loss_ctc 9.880507 loss_rnnt 11.277629 hw_loss 0.426823 history loss 9.487993 rank 0
2023-02-23 02:06:17,006 DEBUG CV Batch 12/1500 loss 11.183717 loss_att 10.444768 loss_ctc 9.880507 loss_rnnt 11.277629 hw_loss 0.426823 history loss 9.487993 rank 5
2023-02-23 02:06:18,053 DEBUG CV Batch 12/1500 loss 11.183717 loss_att 10.444768 loss_ctc 9.880507 loss_rnnt 11.277629 hw_loss 0.426823 history loss 9.487993 rank 2
2023-02-23 02:06:19,646 DEBUG CV Batch 12/1500 loss 11.183717 loss_att 10.444768 loss_ctc 9.880507 loss_rnnt 11.277629 hw_loss 0.426823 history loss 9.487993 rank 7
2023-02-23 02:06:21,201 DEBUG CV Batch 12/1500 loss 11.183717 loss_att 10.444768 loss_ctc 9.880507 loss_rnnt 11.277629 hw_loss 0.426823 history loss 9.487993 rank 3
2023-02-23 02:06:26,572 DEBUG CV Batch 12/1600 loss 14.174151 loss_att 16.524897 loss_ctc 15.250120 loss_rnnt 13.384501 hw_loss 0.330075 history loss 9.384509 rank 6
2023-02-23 02:06:26,603 DEBUG CV Batch 12/1600 loss 14.174151 loss_att 16.524897 loss_ctc 15.250120 loss_rnnt 13.384501 hw_loss 0.330075 history loss 9.384509 rank 4
2023-02-23 02:06:28,720 DEBUG CV Batch 12/1600 loss 14.174151 loss_att 16.524897 loss_ctc 15.250120 loss_rnnt 13.384501 hw_loss 0.330075 history loss 9.384509 rank 1
2023-02-23 02:06:29,588 DEBUG CV Batch 12/1600 loss 14.174151 loss_att 16.524897 loss_ctc 15.250120 loss_rnnt 13.384501 hw_loss 0.330075 history loss 9.384509 rank 0
2023-02-23 02:06:31,058 DEBUG CV Batch 12/1600 loss 14.174151 loss_att 16.524897 loss_ctc 15.250120 loss_rnnt 13.384501 hw_loss 0.330075 history loss 9.384509 rank 5
2023-02-23 02:06:31,780 DEBUG CV Batch 12/1600 loss 14.174151 loss_att 16.524897 loss_ctc 15.250120 loss_rnnt 13.384501 hw_loss 0.330075 history loss 9.384509 rank 2
2023-02-23 02:06:33,142 DEBUG CV Batch 12/1600 loss 14.174151 loss_att 16.524897 loss_ctc 15.250120 loss_rnnt 13.384501 hw_loss 0.330075 history loss 9.384509 rank 7
2023-02-23 02:06:34,819 DEBUG CV Batch 12/1600 loss 14.174151 loss_att 16.524897 loss_ctc 15.250120 loss_rnnt 13.384501 hw_loss 0.330075 history loss 9.384509 rank 3
2023-02-23 02:06:39,042 DEBUG CV Batch 12/1700 loss 14.758363 loss_att 12.774325 loss_ctc 18.475416 loss_rnnt 14.415554 hw_loss 0.457515 history loss 9.254087 rank 4
2023-02-23 02:06:39,162 DEBUG CV Batch 12/1700 loss 14.758363 loss_att 12.774325 loss_ctc 18.475416 loss_rnnt 14.415554 hw_loss 0.457515 history loss 9.254087 rank 6
2023-02-23 02:06:41,143 DEBUG CV Batch 12/1700 loss 14.758363 loss_att 12.774325 loss_ctc 18.475416 loss_rnnt 14.415554 hw_loss 0.457515 history loss 9.254087 rank 1
2023-02-23 02:06:42,285 DEBUG CV Batch 12/1700 loss 14.758363 loss_att 12.774325 loss_ctc 18.475416 loss_rnnt 14.415554 hw_loss 0.457515 history loss 9.254087 rank 0
2023-02-23 02:06:43,962 DEBUG CV Batch 12/1700 loss 14.758363 loss_att 12.774325 loss_ctc 18.475416 loss_rnnt 14.415554 hw_loss 0.457515 history loss 9.254087 rank 5
2023-02-23 02:06:44,894 DEBUG CV Batch 12/1700 loss 14.758363 loss_att 12.774325 loss_ctc 18.475416 loss_rnnt 14.415554 hw_loss 0.457515 history loss 9.254087 rank 2
2023-02-23 02:06:45,743 DEBUG CV Batch 12/1700 loss 14.758363 loss_att 12.774325 loss_ctc 18.475416 loss_rnnt 14.415554 hw_loss 0.457515 history loss 9.254087 rank 7
2023-02-23 02:06:47,525 DEBUG CV Batch 12/1700 loss 14.758363 loss_att 12.774325 loss_ctc 18.475416 loss_rnnt 14.415554 hw_loss 0.457515 history loss 9.254087 rank 3
2023-02-23 02:06:48,512 INFO Epoch 12 CV info cv_loss 9.198929079235556
2023-02-23 02:06:48,513 INFO Epoch 13 TRAIN info lr 0.00048011936098725944
2023-02-23 02:06:48,518 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:06:48,885 INFO Epoch 12 CV info cv_loss 9.19892908181133
2023-02-23 02:06:48,886 INFO Epoch 13 TRAIN info lr 0.0004801702793667319
2023-02-23 02:06:48,888 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:06:50,489 INFO Epoch 12 CV info cv_loss 9.19892907892543
2023-02-23 02:06:50,489 INFO Epoch 13 TRAIN info lr 0.0004802123545971549
2023-02-23 02:06:50,494 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:06:51,850 INFO Epoch 12 CV info cv_loss 9.198929079485382
2023-02-23 02:06:51,851 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/12.pt
2023-02-23 02:06:52,471 INFO Epoch 13 TRAIN info lr 0.000480214569389366
2023-02-23 02:06:52,475 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:06:53,402 INFO Epoch 12 CV info cv_loss 9.198929082130073
2023-02-23 02:06:53,402 INFO Epoch 13 TRAIN info lr 0.0004801569947493972
2023-02-23 02:06:53,405 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:06:54,312 INFO Epoch 12 CV info cv_loss 9.198929078641147
2023-02-23 02:06:54,312 INFO Epoch 13 TRAIN info lr 0.00048011936098725944
2023-02-23 02:06:54,317 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:06:54,957 INFO Epoch 12 CV info cv_loss 9.19892908231098
2023-02-23 02:06:54,958 INFO Epoch 13 TRAIN info lr 0.00048012600159639786
2023-02-23 02:06:54,960 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:06:56,832 INFO Epoch 12 CV info cv_loss 9.198929080484678
2023-02-23 02:06:56,833 INFO Epoch 13 TRAIN info lr 0.00048011714751210996
2023-02-23 02:06:56,838 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 02:08:00,583 DEBUG TRAIN Batch 13/0 loss 8.473500 loss_att 8.272839 loss_ctc 9.862064 loss_rnnt 7.915026 hw_loss 0.775246 lr 0.00048012 rank 4
2023-02-23 02:08:00,585 DEBUG TRAIN Batch 13/0 loss 12.595293 loss_att 11.752752 loss_ctc 15.456971 loss_rnnt 12.037546 hw_loss 0.646308 lr 0.00048012 rank 7
2023-02-23 02:08:00,586 DEBUG TRAIN Batch 13/0 loss 10.032701 loss_att 9.289926 loss_ctc 13.758275 loss_rnnt 9.307393 hw_loss 0.707100 lr 0.00048017 rank 6
2023-02-23 02:08:00,587 DEBUG TRAIN Batch 13/0 loss 11.489697 loss_att 11.722009 loss_ctc 13.730937 loss_rnnt 10.875090 hw_loss 0.504963 lr 0.00048021 rank 0
2023-02-23 02:08:00,588 DEBUG TRAIN Batch 13/0 loss 12.713213 loss_att 11.735517 loss_ctc 14.493656 loss_rnnt 12.225833 hw_loss 0.835364 lr 0.00048012 rank 2
2023-02-23 02:08:00,590 DEBUG TRAIN Batch 13/0 loss 10.334712 loss_att 9.837063 loss_ctc 11.502666 loss_rnnt 9.895888 hw_loss 0.717426 lr 0.00048015 rank 5
2023-02-23 02:08:00,621 DEBUG TRAIN Batch 13/0 loss 8.398489 loss_att 8.541347 loss_ctc 9.923469 loss_rnnt 7.778234 hw_loss 0.728161 lr 0.00048011 rank 3
2023-02-23 02:08:00,628 DEBUG TRAIN Batch 13/0 loss 16.771744 loss_att 15.963938 loss_ctc 18.433577 loss_rnnt 16.361328 hw_loss 0.656997 lr 0.00048021 rank 1
2023-02-23 02:09:13,520 DEBUG TRAIN Batch 13/100 loss 15.620889 loss_att 20.402929 loss_ctc 17.852386 loss_rnnt 14.158501 hw_loss 0.390838 lr 0.00047990 rank 7
2023-02-23 02:09:13,526 DEBUG TRAIN Batch 13/100 loss 24.273220 loss_att 27.704300 loss_ctc 32.001621 loss_rnnt 22.356787 hw_loss 0.374556 lr 0.00047989 rank 3
2023-02-23 02:09:13,527 DEBUG TRAIN Batch 13/100 loss 6.376791 loss_att 9.447882 loss_ctc 6.719558 loss_rnnt 5.499465 hw_loss 0.407635 lr 0.00047999 rank 0
2023-02-23 02:09:13,531 DEBUG TRAIN Batch 13/100 loss 9.671867 loss_att 16.261929 loss_ctc 14.545154 loss_rnnt 7.540478 hw_loss 0.306761 lr 0.00047995 rank 6
2023-02-23 02:09:13,533 DEBUG TRAIN Batch 13/100 loss 9.403613 loss_att 17.208269 loss_ctc 10.364885 loss_rnnt 7.531326 hw_loss 0.343475 lr 0.00047993 rank 5
2023-02-23 02:09:13,533 DEBUG TRAIN Batch 13/100 loss 12.846957 loss_att 20.800602 loss_ctc 15.761509 loss_rnnt 10.632905 hw_loss 0.440093 lr 0.00047999 rank 1
2023-02-23 02:09:13,536 DEBUG TRAIN Batch 13/100 loss 5.748085 loss_att 9.371845 loss_ctc 7.326097 loss_rnnt 4.607220 hw_loss 0.385707 lr 0.00047990 rank 2
2023-02-23 02:09:13,575 DEBUG TRAIN Batch 13/100 loss 15.580896 loss_att 18.258387 loss_ctc 18.452677 loss_rnnt 14.465691 hw_loss 0.369007 lr 0.00047990 rank 4
2023-02-23 02:10:26,353 DEBUG TRAIN Batch 13/200 loss 11.541293 loss_att 17.855019 loss_ctc 19.150959 loss_rnnt 9.056995 hw_loss 0.387993 lr 0.00047967 rank 3
2023-02-23 02:10:26,354 DEBUG TRAIN Batch 13/200 loss 8.172645 loss_att 11.598559 loss_ctc 11.691483 loss_rnnt 6.814826 hw_loss 0.381481 lr 0.00047968 rank 7
2023-02-23 02:10:26,358 DEBUG TRAIN Batch 13/200 loss 15.265414 loss_att 19.804308 loss_ctc 15.997025 loss_rnnt 14.060050 hw_loss 0.375069 lr 0.00047968 rank 4
2023-02-23 02:10:26,365 DEBUG TRAIN Batch 13/200 loss 23.874304 loss_att 28.378479 loss_ctc 30.099644 loss_rnnt 21.965734 hw_loss 0.333173 lr 0.00047977 rank 0
2023-02-23 02:10:26,366 DEBUG TRAIN Batch 13/200 loss 15.344663 loss_att 22.560690 loss_ctc 18.727289 loss_rnnt 13.232307 hw_loss 0.408997 lr 0.00047968 rank 2
2023-02-23 02:10:26,366 DEBUG TRAIN Batch 13/200 loss 7.046240 loss_att 11.056465 loss_ctc 11.631000 loss_rnnt 5.471408 hw_loss 0.302786 lr 0.00047971 rank 5
2023-02-23 02:10:26,366 DEBUG TRAIN Batch 13/200 loss 13.463142 loss_att 17.003616 loss_ctc 15.083759 loss_rnnt 12.353203 hw_loss 0.348303 lr 0.00047977 rank 1
2023-02-23 02:10:26,367 DEBUG TRAIN Batch 13/200 loss 18.163591 loss_att 24.105141 loss_ctc 27.779858 loss_rnnt 15.533247 hw_loss 0.299748 lr 0.00047973 rank 6
2023-02-23 02:11:39,504 DEBUG TRAIN Batch 13/300 loss 16.799318 loss_att 20.741692 loss_ctc 21.497442 loss_rnnt 15.141470 hw_loss 0.455549 lr 0.00047955 rank 0
2023-02-23 02:11:39,508 DEBUG TRAIN Batch 13/300 loss 10.262914 loss_att 15.099505 loss_ctc 12.806719 loss_rnnt 8.743419 hw_loss 0.399379 lr 0.00047945 rank 4
2023-02-23 02:11:39,512 DEBUG TRAIN Batch 13/300 loss 9.119879 loss_att 16.106701 loss_ctc 15.089002 loss_rnnt 6.767268 hw_loss 0.298806 lr 0.00047946 rank 7
2023-02-23 02:11:39,518 DEBUG TRAIN Batch 13/300 loss 19.521427 loss_att 24.500492 loss_ctc 26.115314 loss_rnnt 17.440769 hw_loss 0.385614 lr 0.00047945 rank 3
2023-02-23 02:11:39,521 DEBUG TRAIN Batch 13/300 loss 7.508995 loss_att 12.048468 loss_ctc 7.585498 loss_rnnt 6.373306 hw_loss 0.407989 lr 0.00047945 rank 2
2023-02-23 02:11:39,522 DEBUG TRAIN Batch 13/300 loss 21.832966 loss_att 26.693409 loss_ctc 24.334721 loss_rnnt 20.278107 hw_loss 0.467255 lr 0.00047951 rank 6
2023-02-23 02:11:39,526 DEBUG TRAIN Batch 13/300 loss 9.218736 loss_att 14.280842 loss_ctc 13.761839 loss_rnnt 7.382979 hw_loss 0.407978 lr 0.00047949 rank 5
2023-02-23 02:11:39,527 DEBUG TRAIN Batch 13/300 loss 17.067915 loss_att 20.015728 loss_ctc 25.578110 loss_rnnt 15.096403 hw_loss 0.463605 lr 0.00047955 rank 1
2023-02-23 02:12:53,389 DEBUG TRAIN Batch 13/400 loss 14.689284 loss_att 16.272202 loss_ctc 15.201629 loss_rnnt 13.996887 hw_loss 0.576566 lr 0.00047923 rank 3
2023-02-23 02:12:53,391 DEBUG TRAIN Batch 13/400 loss 12.995994 loss_att 13.836996 loss_ctc 14.207726 loss_rnnt 12.417191 hw_loss 0.466946 lr 0.00047924 rank 7
2023-02-23 02:12:53,393 DEBUG TRAIN Batch 13/400 loss 16.408655 loss_att 18.814720 loss_ctc 19.627239 loss_rnnt 15.238054 hw_loss 0.487956 lr 0.00047928 rank 6
2023-02-23 02:12:53,394 DEBUG TRAIN Batch 13/400 loss 7.392428 loss_att 11.037573 loss_ctc 7.428184 loss_rnnt 6.401785 hw_loss 0.481587 lr 0.00047933 rank 0
2023-02-23 02:12:53,394 DEBUG TRAIN Batch 13/400 loss 13.862498 loss_att 24.444216 loss_ctc 15.328907 loss_rnnt 11.327286 hw_loss 0.418777 lr 0.00047933 rank 1
2023-02-23 02:12:53,399 DEBUG TRAIN Batch 13/400 loss 9.301453 loss_att 13.234549 loss_ctc 10.887920 loss_rnnt 8.078743 hw_loss 0.421051 lr 0.00047923 rank 2
2023-02-23 02:12:53,418 DEBUG TRAIN Batch 13/400 loss 9.925722 loss_att 14.072302 loss_ctc 14.029852 loss_rnnt 8.299543 hw_loss 0.468085 lr 0.00047923 rank 4
2023-02-23 02:12:53,432 DEBUG TRAIN Batch 13/400 loss 8.878862 loss_att 13.412427 loss_ctc 10.564911 loss_rnnt 7.546997 hw_loss 0.375649 lr 0.00047927 rank 5
2023-02-23 02:14:06,862 DEBUG TRAIN Batch 13/500 loss 13.680528 loss_att 21.554237 loss_ctc 20.011932 loss_rnnt 11.038254 hw_loss 0.418771 lr 0.00047902 rank 7
2023-02-23 02:14:06,866 DEBUG TRAIN Batch 13/500 loss 19.263781 loss_att 21.972355 loss_ctc 22.034534 loss_rnnt 18.117283 hw_loss 0.441281 lr 0.00047905 rank 5
2023-02-23 02:14:06,868 DEBUG TRAIN Batch 13/500 loss 10.779679 loss_att 15.380135 loss_ctc 13.643039 loss_rnnt 9.227323 hw_loss 0.469658 lr 0.00047901 rank 3
2023-02-23 02:14:06,869 DEBUG TRAIN Batch 13/500 loss 18.628977 loss_att 22.688419 loss_ctc 26.054073 loss_rnnt 16.639853 hw_loss 0.351042 lr 0.00047911 rank 1
2023-02-23 02:14:06,871 DEBUG TRAIN Batch 13/500 loss 17.166376 loss_att 22.397999 loss_ctc 22.853893 loss_rnnt 15.149218 hw_loss 0.398436 lr 0.00047901 rank 4
2023-02-23 02:14:06,871 DEBUG TRAIN Batch 13/500 loss 15.965320 loss_att 18.970478 loss_ctc 20.677582 loss_rnnt 14.438248 hw_loss 0.558259 lr 0.00047911 rank 0
2023-02-23 02:14:06,877 DEBUG TRAIN Batch 13/500 loss 12.592324 loss_att 14.450600 loss_ctc 17.568283 loss_rnnt 11.352366 hw_loss 0.384078 lr 0.00047901 rank 2
2023-02-23 02:14:06,914 DEBUG TRAIN Batch 13/500 loss 10.497789 loss_att 13.253332 loss_ctc 14.390618 loss_rnnt 9.182964 hw_loss 0.458761 lr 0.00047906 rank 6
2023-02-23 02:15:20,835 DEBUG TRAIN Batch 13/600 loss 10.277189 loss_att 10.883040 loss_ctc 13.030710 loss_rnnt 9.484310 hw_loss 0.571074 lr 0.00047879 rank 3
2023-02-23 02:15:20,839 DEBUG TRAIN Batch 13/600 loss 13.775596 loss_att 13.377224 loss_ctc 18.958418 loss_rnnt 12.831138 hw_loss 0.624545 lr 0.00047880 rank 7
2023-02-23 02:15:20,843 DEBUG TRAIN Batch 13/600 loss 10.655381 loss_att 10.556116 loss_ctc 12.697341 loss_rnnt 9.968043 hw_loss 0.815492 lr 0.00047889 rank 1
2023-02-23 02:15:20,845 DEBUG TRAIN Batch 13/600 loss 13.418137 loss_att 13.397038 loss_ctc 17.174133 loss_rnnt 12.583215 hw_loss 0.634392 lr 0.00047885 rank 6
2023-02-23 02:15:20,846 DEBUG TRAIN Batch 13/600 loss 18.989479 loss_att 19.223238 loss_ctc 26.025928 loss_rnnt 17.706350 hw_loss 0.559095 lr 0.00047879 rank 4
2023-02-23 02:15:20,848 DEBUG TRAIN Batch 13/600 loss 15.432376 loss_att 16.345415 loss_ctc 20.225473 loss_rnnt 14.253654 hw_loss 0.669439 lr 0.00047889 rank 0
2023-02-23 02:15:20,849 DEBUG TRAIN Batch 13/600 loss 12.012571 loss_att 12.056694 loss_ctc 14.529669 loss_rnnt 11.275152 hw_loss 0.736843 lr 0.00047883 rank 5
2023-02-23 02:15:20,850 DEBUG TRAIN Batch 13/600 loss 13.110125 loss_att 11.619151 loss_ctc 15.524771 loss_rnnt 12.744427 hw_loss 0.641136 lr 0.00047879 rank 2
2023-02-23 02:16:36,150 DEBUG TRAIN Batch 13/700 loss 16.197800 loss_att 19.142609 loss_ctc 22.214445 loss_rnnt 14.622066 hw_loss 0.346034 lr 0.00047858 rank 7
2023-02-23 02:16:36,155 DEBUG TRAIN Batch 13/700 loss 7.659685 loss_att 10.403250 loss_ctc 6.413957 loss_rnnt 7.059183 hw_loss 0.408536 lr 0.00047857 rank 3
2023-02-23 02:16:36,160 DEBUG TRAIN Batch 13/700 loss 11.556958 loss_att 16.142527 loss_ctc 12.643618 loss_rnnt 10.244629 hw_loss 0.469365 lr 0.00047861 rank 5
2023-02-23 02:16:36,165 DEBUG TRAIN Batch 13/700 loss 15.843484 loss_att 21.154268 loss_ctc 18.457230 loss_rnnt 14.200744 hw_loss 0.435157 lr 0.00047867 rank 1
2023-02-23 02:16:36,185 DEBUG TRAIN Batch 13/700 loss 6.905807 loss_att 11.208759 loss_ctc 7.742398 loss_rnnt 5.706087 hw_loss 0.426719 lr 0.00047858 rank 4
2023-02-23 02:16:36,187 DEBUG TRAIN Batch 13/700 loss 15.427713 loss_att 19.687784 loss_ctc 18.902958 loss_rnnt 13.941406 hw_loss 0.320487 lr 0.00047858 rank 2
2023-02-23 02:16:36,187 DEBUG TRAIN Batch 13/700 loss 12.587402 loss_att 15.883310 loss_ctc 18.537189 loss_rnnt 10.917723 hw_loss 0.407239 lr 0.00047867 rank 0
2023-02-23 02:16:36,190 DEBUG TRAIN Batch 13/700 loss 10.200377 loss_att 12.952957 loss_ctc 13.956809 loss_rnnt 8.945274 hw_loss 0.381989 lr 0.00047863 rank 6
2023-02-23 02:17:49,221 DEBUG TRAIN Batch 13/800 loss 13.739131 loss_att 18.342419 loss_ctc 18.067738 loss_rnnt 12.010679 hw_loss 0.432462 lr 0.00047835 rank 3
2023-02-23 02:17:49,224 DEBUG TRAIN Batch 13/800 loss 8.235229 loss_att 14.692473 loss_ctc 10.987209 loss_rnnt 6.378287 hw_loss 0.372303 lr 0.00047841 rank 6
2023-02-23 02:17:49,223 DEBUG TRAIN Batch 13/800 loss 28.283497 loss_att 32.759781 loss_ctc 33.127419 loss_rnnt 26.554611 hw_loss 0.352074 lr 0.00047836 rank 7
2023-02-23 02:17:49,226 DEBUG TRAIN Batch 13/800 loss 12.200546 loss_att 16.484045 loss_ctc 12.265423 loss_rnnt 11.112482 hw_loss 0.417588 lr 0.00047845 rank 1
2023-02-23 02:17:49,231 DEBUG TRAIN Batch 13/800 loss 25.082642 loss_att 33.949909 loss_ctc 30.119293 loss_rnnt 22.430138 hw_loss 0.389061 lr 0.00047845 rank 0
2023-02-23 02:17:49,232 DEBUG TRAIN Batch 13/800 loss 18.748871 loss_att 22.642920 loss_ctc 20.191374 loss_rnnt 17.575586 hw_loss 0.379016 lr 0.00047836 rank 4
2023-02-23 02:17:49,234 DEBUG TRAIN Batch 13/800 loss 17.886497 loss_att 19.626072 loss_ctc 17.682564 loss_rnnt 17.381111 hw_loss 0.346240 lr 0.00047836 rank 2
2023-02-23 02:17:49,278 DEBUG TRAIN Batch 13/800 loss 19.386316 loss_att 21.469957 loss_ctc 19.638184 loss_rnnt 18.722282 hw_loss 0.400733 lr 0.00047839 rank 5
2023-02-23 02:19:01,973 DEBUG TRAIN Batch 13/900 loss 17.011189 loss_att 19.193336 loss_ctc 21.701967 loss_rnnt 15.726089 hw_loss 0.418565 lr 0.00047814 rank 7
2023-02-23 02:19:01,976 DEBUG TRAIN Batch 13/900 loss 12.480818 loss_att 13.324684 loss_ctc 15.322278 loss_rnnt 11.750654 hw_loss 0.342244 lr 0.00047823 rank 0
2023-02-23 02:19:01,976 DEBUG TRAIN Batch 13/900 loss 14.630416 loss_att 14.753639 loss_ctc 15.994330 loss_rnnt 14.178738 hw_loss 0.459710 lr 0.00047814 rank 3
2023-02-23 02:19:01,981 DEBUG TRAIN Batch 13/900 loss 9.465055 loss_att 13.952793 loss_ctc 11.258823 loss_rnnt 8.138291 hw_loss 0.356337 lr 0.00047823 rank 1
2023-02-23 02:19:01,981 DEBUG TRAIN Batch 13/900 loss 23.109304 loss_att 25.462637 loss_ctc 31.272404 loss_rnnt 21.308233 hw_loss 0.453734 lr 0.00047814 rank 4
2023-02-23 02:19:01,982 DEBUG TRAIN Batch 13/900 loss 11.314068 loss_att 12.857533 loss_ctc 13.332878 loss_rnnt 10.489193 hw_loss 0.463138 lr 0.00047817 rank 5
2023-02-23 02:19:01,986 DEBUG TRAIN Batch 13/900 loss 20.891359 loss_att 23.619364 loss_ctc 21.217695 loss_rnnt 20.093950 hw_loss 0.390557 lr 0.00047819 rank 6
2023-02-23 02:19:01,986 DEBUG TRAIN Batch 13/900 loss 6.668533 loss_att 11.289743 loss_ctc 9.083527 loss_rnnt 5.224540 hw_loss 0.370784 lr 0.00047814 rank 2
2023-02-23 02:20:15,345 DEBUG TRAIN Batch 13/1000 loss 13.813906 loss_att 19.776028 loss_ctc 17.202305 loss_rnnt 11.983363 hw_loss 0.349374 lr 0.00047796 rank 5
2023-02-23 02:20:15,346 DEBUG TRAIN Batch 13/1000 loss 10.262554 loss_att 13.627215 loss_ctc 14.199017 loss_rnnt 8.802430 hw_loss 0.491868 lr 0.00047793 rank 7
2023-02-23 02:20:15,345 DEBUG TRAIN Batch 13/1000 loss 16.224831 loss_att 18.265160 loss_ctc 16.930225 loss_rnnt 15.495749 hw_loss 0.425552 lr 0.00047792 rank 4
2023-02-23 02:20:15,347 DEBUG TRAIN Batch 13/1000 loss 11.921952 loss_att 17.292395 loss_ctc 15.787408 loss_rnnt 10.142872 hw_loss 0.355495 lr 0.00047792 rank 3
2023-02-23 02:20:15,351 DEBUG TRAIN Batch 13/1000 loss 15.794374 loss_att 18.484188 loss_ctc 16.860214 loss_rnnt 14.843342 hw_loss 0.508045 lr 0.00047797 rank 6
2023-02-23 02:20:15,358 DEBUG TRAIN Batch 13/1000 loss 21.223682 loss_att 22.530872 loss_ctc 29.260040 loss_rnnt 19.666029 hw_loss 0.421315 lr 0.00047801 rank 0
2023-02-23 02:20:15,375 DEBUG TRAIN Batch 13/1000 loss 9.373597 loss_att 13.639307 loss_ctc 10.682098 loss_rnnt 8.148576 hw_loss 0.370147 lr 0.00047801 rank 1
2023-02-23 02:20:15,380 DEBUG TRAIN Batch 13/1000 loss 17.526400 loss_att 21.041500 loss_ctc 22.387611 loss_rnnt 15.957434 hw_loss 0.408351 lr 0.00047792 rank 2
2023-02-23 02:21:30,197 DEBUG TRAIN Batch 13/1100 loss 15.731690 loss_att 20.694448 loss_ctc 19.112679 loss_rnnt 14.030649 hw_loss 0.483166 lr 0.00047771 rank 7
2023-02-23 02:21:30,197 DEBUG TRAIN Batch 13/1100 loss 12.121996 loss_att 13.355015 loss_ctc 11.028752 loss_rnnt 11.790209 hw_loss 0.433029 lr 0.00047770 rank 3
2023-02-23 02:21:30,198 DEBUG TRAIN Batch 13/1100 loss 11.951366 loss_att 16.966518 loss_ctc 16.107531 loss_rnnt 10.188505 hw_loss 0.385640 lr 0.00047775 rank 6
2023-02-23 02:21:30,199 DEBUG TRAIN Batch 13/1100 loss 13.122851 loss_att 15.597737 loss_ctc 14.572032 loss_rnnt 12.238523 hw_loss 0.367739 lr 0.00047779 rank 1
2023-02-23 02:21:30,200 DEBUG TRAIN Batch 13/1100 loss 6.305141 loss_att 8.338387 loss_ctc 7.371017 loss_rnnt 5.519189 hw_loss 0.444723 lr 0.00047779 rank 0
2023-02-23 02:21:30,201 DEBUG TRAIN Batch 13/1100 loss 14.274409 loss_att 20.644743 loss_ctc 19.014137 loss_rnnt 12.125438 hw_loss 0.455516 lr 0.00047770 rank 4
2023-02-23 02:21:30,213 DEBUG TRAIN Batch 13/1100 loss 15.309407 loss_att 14.793053 loss_ctc 16.280924 loss_rnnt 15.104201 hw_loss 0.335514 lr 0.00047770 rank 2
2023-02-23 02:21:30,219 DEBUG TRAIN Batch 13/1100 loss 15.994145 loss_att 17.672146 loss_ctc 18.903328 loss_rnnt 14.994853 hw_loss 0.517127 lr 0.00047774 rank 5
2023-02-23 02:22:43,910 DEBUG TRAIN Batch 13/1200 loss 10.824237 loss_att 13.807604 loss_ctc 14.394843 loss_rnnt 9.507987 hw_loss 0.456555 lr 0.00047749 rank 7
2023-02-23 02:22:43,913 DEBUG TRAIN Batch 13/1200 loss 14.422422 loss_att 14.943300 loss_ctc 17.885090 loss_rnnt 13.520316 hw_loss 0.630453 lr 0.00047757 rank 1
2023-02-23 02:22:43,913 DEBUG TRAIN Batch 13/1200 loss 20.400133 loss_att 21.135189 loss_ctc 22.206358 loss_rnnt 19.771984 hw_loss 0.450577 lr 0.00047752 rank 5
2023-02-23 02:22:43,914 DEBUG TRAIN Batch 13/1200 loss 12.976179 loss_att 15.534827 loss_ctc 17.350218 loss_rnnt 11.631754 hw_loss 0.467795 lr 0.00047753 rank 6
2023-02-23 02:22:43,915 DEBUG TRAIN Batch 13/1200 loss 12.293577 loss_att 13.862000 loss_ctc 13.286077 loss_rnnt 11.625992 hw_loss 0.415439 lr 0.00047748 rank 3
2023-02-23 02:22:43,919 DEBUG TRAIN Batch 13/1200 loss 9.309641 loss_att 11.519638 loss_ctc 12.786588 loss_rnnt 8.174074 hw_loss 0.431202 lr 0.00047748 rank 4
2023-02-23 02:22:43,920 DEBUG TRAIN Batch 13/1200 loss 20.511757 loss_att 23.866232 loss_ctc 27.224863 loss_rnnt 18.698282 hw_loss 0.464058 lr 0.00047758 rank 0
2023-02-23 02:22:43,920 DEBUG TRAIN Batch 13/1200 loss 9.690764 loss_att 10.021027 loss_ctc 10.481538 loss_rnnt 9.218084 hw_loss 0.564734 lr 0.00047748 rank 2
2023-02-23 02:23:56,926 DEBUG TRAIN Batch 13/1300 loss 10.594543 loss_att 9.422894 loss_ctc 12.517031 loss_rnnt 10.186604 hw_loss 0.723630 lr 0.00047726 rank 3
2023-02-23 02:23:56,927 DEBUG TRAIN Batch 13/1300 loss 11.794037 loss_att 16.778061 loss_ctc 14.925705 loss_rnnt 10.165507 hw_loss 0.401567 lr 0.00047727 rank 7
2023-02-23 02:23:56,927 DEBUG TRAIN Batch 13/1300 loss 17.787233 loss_att 23.666553 loss_ctc 26.319229 loss_rnnt 15.286451 hw_loss 0.351226 lr 0.00047736 rank 1
2023-02-23 02:23:56,927 DEBUG TRAIN Batch 13/1300 loss 19.144781 loss_att 23.939390 loss_ctc 26.087622 loss_rnnt 17.069466 hw_loss 0.357529 lr 0.00047727 rank 4
2023-02-23 02:23:56,929 DEBUG TRAIN Batch 13/1300 loss 17.298080 loss_att 21.964428 loss_ctc 19.738935 loss_rnnt 15.904410 hw_loss 0.253039 lr 0.00047732 rank 6
2023-02-23 02:23:56,929 DEBUG TRAIN Batch 13/1300 loss 7.153776 loss_att 11.612961 loss_ctc 7.725381 loss_rnnt 6.002018 hw_loss 0.344450 lr 0.00047736 rank 0
2023-02-23 02:23:56,934 DEBUG TRAIN Batch 13/1300 loss 15.306934 loss_att 23.696526 loss_ctc 19.204077 loss_rnnt 12.902652 hw_loss 0.387650 lr 0.00047727 rank 2
2023-02-23 02:23:56,935 DEBUG TRAIN Batch 13/1300 loss 9.578709 loss_att 13.025580 loss_ctc 12.623950 loss_rnnt 8.243702 hw_loss 0.449250 lr 0.00047730 rank 5
2023-02-23 02:25:12,017 DEBUG TRAIN Batch 13/1400 loss 15.027271 loss_att 18.746155 loss_ctc 20.090216 loss_rnnt 13.387861 hw_loss 0.413577 lr 0.00047708 rank 5
2023-02-23 02:25:12,029 DEBUG TRAIN Batch 13/1400 loss 7.421377 loss_att 9.887567 loss_ctc 13.188446 loss_rnnt 5.924643 hw_loss 0.439789 lr 0.00047705 rank 3
2023-02-23 02:25:12,029 DEBUG TRAIN Batch 13/1400 loss 12.989494 loss_att 16.005569 loss_ctc 15.778734 loss_rnnt 11.807139 hw_loss 0.388578 lr 0.00047705 rank 7
2023-02-23 02:25:12,032 DEBUG TRAIN Batch 13/1400 loss 21.180452 loss_att 23.345442 loss_ctc 23.754723 loss_rnnt 20.225456 hw_loss 0.335176 lr 0.00047705 rank 2
2023-02-23 02:25:12,040 DEBUG TRAIN Batch 13/1400 loss 7.942318 loss_att 10.684361 loss_ctc 9.417564 loss_rnnt 6.970357 hw_loss 0.425349 lr 0.00047705 rank 4
2023-02-23 02:25:12,040 DEBUG TRAIN Batch 13/1400 loss 18.227341 loss_att 19.332390 loss_ctc 26.036324 loss_rnnt 16.755707 hw_loss 0.392675 lr 0.00047714 rank 0
2023-02-23 02:25:12,047 DEBUG TRAIN Batch 13/1400 loss 4.429424 loss_att 7.273042 loss_ctc 4.852919 loss_rnnt 3.615210 hw_loss 0.354421 lr 0.00047714 rank 1
2023-02-23 02:25:12,055 DEBUG TRAIN Batch 13/1400 loss 14.784307 loss_att 19.878502 loss_ctc 17.866135 loss_rnnt 13.194106 hw_loss 0.300845 lr 0.00047710 rank 6
2023-02-23 02:26:25,833 DEBUG TRAIN Batch 13/1500 loss 25.414204 loss_att 29.938110 loss_ctc 29.166615 loss_rnnt 23.805929 hw_loss 0.380944 lr 0.00047683 rank 3
2023-02-23 02:26:25,837 DEBUG TRAIN Batch 13/1500 loss 13.473429 loss_att 14.370415 loss_ctc 17.046499 loss_rnnt 12.607027 hw_loss 0.394864 lr 0.00047683 rank 4
2023-02-23 02:26:25,837 DEBUG TRAIN Batch 13/1500 loss 11.397387 loss_att 12.235888 loss_ctc 14.937724 loss_rnnt 10.517977 hw_loss 0.449371 lr 0.00047688 rank 6
2023-02-23 02:26:25,839 DEBUG TRAIN Batch 13/1500 loss 11.440233 loss_att 13.884472 loss_ctc 13.019077 loss_rnnt 10.506947 hw_loss 0.438612 lr 0.00047684 rank 7
2023-02-23 02:26:25,839 DEBUG TRAIN Batch 13/1500 loss 19.526846 loss_att 19.777132 loss_ctc 19.469189 loss_rnnt 19.264297 hw_loss 0.412836 lr 0.00047687 rank 5
2023-02-23 02:26:25,839 DEBUG TRAIN Batch 13/1500 loss 11.444346 loss_att 14.494471 loss_ctc 16.162285 loss_rnnt 10.018440 hw_loss 0.350294 lr 0.00047692 rank 1
2023-02-23 02:26:25,846 DEBUG TRAIN Batch 13/1500 loss 14.524079 loss_att 18.045673 loss_ctc 15.922647 loss_rnnt 13.471977 hw_loss 0.302450 lr 0.00047683 rank 2
2023-02-23 02:26:25,887 DEBUG TRAIN Batch 13/1500 loss 10.084228 loss_att 12.480614 loss_ctc 13.218375 loss_rnnt 9.016475 hw_loss 0.319854 lr 0.00047692 rank 0
2023-02-23 02:27:39,159 DEBUG TRAIN Batch 13/1600 loss 17.077101 loss_att 17.908215 loss_ctc 17.270144 loss_rnnt 16.699631 hw_loss 0.347827 lr 0.00047662 rank 7
2023-02-23 02:27:39,166 DEBUG TRAIN Batch 13/1600 loss 13.920137 loss_att 16.686092 loss_ctc 18.546329 loss_rnnt 12.514221 hw_loss 0.442310 lr 0.00047665 rank 5
2023-02-23 02:27:39,167 DEBUG TRAIN Batch 13/1600 loss 10.249680 loss_att 15.346016 loss_ctc 16.071468 loss_rnnt 8.185751 hw_loss 0.503291 lr 0.00047661 rank 3
2023-02-23 02:27:39,172 DEBUG TRAIN Batch 13/1600 loss 27.803030 loss_att 33.179317 loss_ctc 33.870461 loss_rnnt 25.719345 hw_loss 0.373943 lr 0.00047671 rank 0
2023-02-23 02:27:39,173 DEBUG TRAIN Batch 13/1600 loss 12.767522 loss_att 18.059593 loss_ctc 14.914797 loss_rnnt 11.240400 hw_loss 0.342010 lr 0.00047671 rank 1
2023-02-23 02:27:39,173 DEBUG TRAIN Batch 13/1600 loss 16.904768 loss_att 19.607594 loss_ctc 24.795248 loss_rnnt 15.134501 hw_loss 0.333070 lr 0.00047666 rank 6
2023-02-23 02:27:39,174 DEBUG TRAIN Batch 13/1600 loss 10.251121 loss_att 12.249663 loss_ctc 13.784698 loss_rnnt 9.119216 hw_loss 0.489475 lr 0.00047661 rank 2
2023-02-23 02:27:39,178 DEBUG TRAIN Batch 13/1600 loss 17.396980 loss_att 16.269985 loss_ctc 16.019173 loss_rnnt 17.606346 hw_loss 0.374511 lr 0.00047661 rank 4
2023-02-23 02:28:53,255 DEBUG TRAIN Batch 13/1700 loss 20.919386 loss_att 23.117407 loss_ctc 24.555037 loss_rnnt 19.758141 hw_loss 0.444164 lr 0.00047645 rank 6
2023-02-23 02:28:53,258 DEBUG TRAIN Batch 13/1700 loss 12.551637 loss_att 18.145512 loss_ctc 16.291252 loss_rnnt 10.697844 hw_loss 0.443255 lr 0.00047640 rank 3
2023-02-23 02:28:53,259 DEBUG TRAIN Batch 13/1700 loss 10.423033 loss_att 14.855289 loss_ctc 15.654638 loss_rnnt 8.648535 hw_loss 0.357186 lr 0.00047640 rank 7
2023-02-23 02:28:53,265 DEBUG TRAIN Batch 13/1700 loss 11.306202 loss_att 15.038525 loss_ctc 11.868603 loss_rnnt 10.275496 hw_loss 0.392350 lr 0.00047649 rank 0
2023-02-23 02:28:53,267 DEBUG TRAIN Batch 13/1700 loss 10.657621 loss_att 11.799788 loss_ctc 10.893305 loss_rnnt 10.198177 hw_loss 0.374224 lr 0.00047643 rank 5
2023-02-23 02:28:53,267 DEBUG TRAIN Batch 13/1700 loss 25.460026 loss_att 26.401783 loss_ctc 27.899963 loss_rnnt 24.716709 hw_loss 0.430569 lr 0.00047640 rank 2
2023-02-23 02:28:53,270 DEBUG TRAIN Batch 13/1700 loss 13.476480 loss_att 13.208387 loss_ctc 13.024536 loss_rnnt 13.395860 hw_loss 0.364680 lr 0.00047640 rank 4
2023-02-23 02:28:53,269 DEBUG TRAIN Batch 13/1700 loss 12.135436 loss_att 15.734564 loss_ctc 14.987111 loss_rnnt 10.810785 hw_loss 0.421128 lr 0.00047649 rank 1
2023-02-23 02:30:09,260 DEBUG TRAIN Batch 13/1800 loss 17.086657 loss_att 17.376308 loss_ctc 18.199669 loss_rnnt 16.658329 hw_loss 0.416239 lr 0.00047623 rank 6
2023-02-23 02:30:09,262 DEBUG TRAIN Batch 13/1800 loss 13.124346 loss_att 15.012570 loss_ctc 18.855154 loss_rnnt 11.755569 hw_loss 0.425669 lr 0.00047619 rank 7
2023-02-23 02:30:09,262 DEBUG TRAIN Batch 13/1800 loss 11.534214 loss_att 15.313648 loss_ctc 14.579449 loss_rnnt 10.127097 hw_loss 0.459747 lr 0.00047618 rank 3
2023-02-23 02:30:09,262 DEBUG TRAIN Batch 13/1800 loss 9.303741 loss_att 11.090537 loss_ctc 13.442504 loss_rnnt 8.163697 hw_loss 0.432838 lr 0.00047627 rank 1
2023-02-23 02:30:09,263 DEBUG TRAIN Batch 13/1800 loss 17.575241 loss_att 19.039085 loss_ctc 19.767284 loss_rnnt 16.739401 hw_loss 0.470249 lr 0.00047627 rank 0
2023-02-23 02:30:09,267 DEBUG TRAIN Batch 13/1800 loss 19.422842 loss_att 20.981071 loss_ctc 23.698376 loss_rnnt 18.319525 hw_loss 0.415501 lr 0.00047622 rank 5
2023-02-23 02:30:09,266 DEBUG TRAIN Batch 13/1800 loss 10.772518 loss_att 13.051282 loss_ctc 13.589939 loss_rnnt 9.697651 hw_loss 0.456485 lr 0.00047618 rank 4
2023-02-23 02:30:09,316 DEBUG TRAIN Batch 13/1800 loss 10.230371 loss_att 14.338749 loss_ctc 10.914195 loss_rnnt 9.098069 hw_loss 0.411468 lr 0.00047618 rank 2
2023-02-23 02:31:23,534 DEBUG TRAIN Batch 13/1900 loss 22.107681 loss_att 26.636864 loss_ctc 28.901571 loss_rnnt 20.068033 hw_loss 0.427423 lr 0.00047597 rank 7
2023-02-23 02:31:23,535 DEBUG TRAIN Batch 13/1900 loss 9.525103 loss_att 9.731913 loss_ctc 11.566002 loss_rnnt 8.863730 hw_loss 0.652295 lr 0.00047596 rank 3
2023-02-23 02:31:23,539 DEBUG TRAIN Batch 13/1900 loss 7.823179 loss_att 12.785599 loss_ctc 14.931319 loss_rnnt 5.671392 hw_loss 0.396656 lr 0.00047606 rank 1
2023-02-23 02:31:23,541 DEBUG TRAIN Batch 13/1900 loss 12.668756 loss_att 13.090639 loss_ctc 17.880552 loss_rnnt 11.674571 hw_loss 0.402942 lr 0.00047600 rank 5
2023-02-23 02:31:23,542 DEBUG TRAIN Batch 13/1900 loss 14.011993 loss_att 17.318619 loss_ctc 18.402552 loss_rnnt 12.544107 hw_loss 0.414663 lr 0.00047597 rank 4
2023-02-23 02:31:23,544 DEBUG TRAIN Batch 13/1900 loss 11.323010 loss_att 11.604860 loss_ctc 13.858808 loss_rnnt 10.675350 hw_loss 0.474719 lr 0.00047597 rank 2
2023-02-23 02:31:23,546 DEBUG TRAIN Batch 13/1900 loss 10.526824 loss_att 9.683215 loss_ctc 11.569907 loss_rnnt 10.203476 hw_loss 0.661859 lr 0.00047606 rank 0
2023-02-23 02:31:23,584 DEBUG TRAIN Batch 13/1900 loss 6.412353 loss_att 8.149817 loss_ctc 7.771245 loss_rnnt 5.538411 hw_loss 0.647368 lr 0.00047602 rank 6
2023-02-23 02:32:36,978 DEBUG TRAIN Batch 13/2000 loss 16.296690 loss_att 21.270727 loss_ctc 21.560131 loss_rnnt 14.407806 hw_loss 0.360534 lr 0.00047576 rank 7
2023-02-23 02:32:36,985 DEBUG TRAIN Batch 13/2000 loss 18.543371 loss_att 23.164852 loss_ctc 24.506329 loss_rnnt 16.592262 hw_loss 0.434532 lr 0.00047575 rank 3
2023-02-23 02:32:36,986 DEBUG TRAIN Batch 13/2000 loss 24.327641 loss_att 28.517677 loss_ctc 28.955992 loss_rnnt 22.705952 hw_loss 0.312311 lr 0.00047575 rank 4
2023-02-23 02:32:36,986 DEBUG TRAIN Batch 13/2000 loss 18.992292 loss_att 27.783733 loss_ctc 20.727821 loss_rnnt 16.818577 hw_loss 0.345045 lr 0.00047580 rank 6
2023-02-23 02:32:36,990 DEBUG TRAIN Batch 13/2000 loss 13.593599 loss_att 17.890966 loss_ctc 20.820000 loss_rnnt 11.544929 hw_loss 0.423145 lr 0.00047575 rank 2
2023-02-23 02:32:36,990 DEBUG TRAIN Batch 13/2000 loss 5.101524 loss_att 7.500326 loss_ctc 6.155044 loss_rnnt 4.250501 hw_loss 0.432738 lr 0.00047584 rank 1
2023-02-23 02:32:36,994 DEBUG TRAIN Batch 13/2000 loss 6.368541 loss_att 10.239214 loss_ctc 5.355217 loss_rnnt 5.546482 hw_loss 0.343190 lr 0.00047584 rank 0
2023-02-23 02:32:36,995 DEBUG TRAIN Batch 13/2000 loss 13.033298 loss_att 15.809310 loss_ctc 16.504887 loss_rnnt 11.824007 hw_loss 0.358517 lr 0.00047579 rank 5
2023-02-23 02:33:52,263 DEBUG TRAIN Batch 13/2100 loss 20.491472 loss_att 21.552593 loss_ctc 27.640972 loss_rnnt 19.131557 hw_loss 0.364541 lr 0.00047557 rank 5
2023-02-23 02:33:52,264 DEBUG TRAIN Batch 13/2100 loss 11.409349 loss_att 18.112858 loss_ctc 12.830481 loss_rnnt 9.678579 hw_loss 0.376095 lr 0.00047563 rank 1
2023-02-23 02:33:52,267 DEBUG TRAIN Batch 13/2100 loss 10.939681 loss_att 16.143105 loss_ctc 10.514215 loss_rnnt 9.752957 hw_loss 0.380189 lr 0.00047554 rank 7
2023-02-23 02:33:52,273 DEBUG TRAIN Batch 13/2100 loss 26.373882 loss_att 28.543152 loss_ctc 31.463196 loss_rnnt 25.024933 hw_loss 0.443471 lr 0.00047554 rank 4
2023-02-23 02:33:52,275 DEBUG TRAIN Batch 13/2100 loss 20.008230 loss_att 26.312349 loss_ctc 27.184540 loss_rnnt 17.593662 hw_loss 0.369190 lr 0.00047553 rank 3
2023-02-23 02:33:52,276 DEBUG TRAIN Batch 13/2100 loss 16.465645 loss_att 26.884003 loss_ctc 19.562233 loss_rnnt 13.711637 hw_loss 0.482730 lr 0.00047558 rank 6
2023-02-23 02:33:52,279 DEBUG TRAIN Batch 13/2100 loss 17.134157 loss_att 18.180769 loss_ctc 19.268328 loss_rnnt 16.406837 hw_loss 0.437704 lr 0.00047563 rank 0
2023-02-23 02:33:52,291 DEBUG TRAIN Batch 13/2100 loss 6.771371 loss_att 14.126593 loss_ctc 9.489822 loss_rnnt 4.772688 hw_loss 0.309710 lr 0.00047554 rank 2
2023-02-23 02:35:06,420 DEBUG TRAIN Batch 13/2200 loss 10.936446 loss_att 13.831596 loss_ctc 14.750160 loss_rnnt 9.622181 hw_loss 0.425140 lr 0.00047533 rank 7
2023-02-23 02:35:06,423 DEBUG TRAIN Batch 13/2200 loss 20.807821 loss_att 24.802736 loss_ctc 29.182415 loss_rnnt 18.690865 hw_loss 0.377552 lr 0.00047541 rank 0
2023-02-23 02:35:06,424 DEBUG TRAIN Batch 13/2200 loss 24.184090 loss_att 26.606752 loss_ctc 34.256687 loss_rnnt 22.143682 hw_loss 0.399115 lr 0.00047541 rank 1
2023-02-23 02:35:06,428 DEBUG TRAIN Batch 13/2200 loss 9.314440 loss_att 14.512982 loss_ctc 13.158337 loss_rnnt 7.509691 hw_loss 0.473475 lr 0.00047532 rank 3
2023-02-23 02:35:06,428 DEBUG TRAIN Batch 13/2200 loss 9.502008 loss_att 13.004131 loss_ctc 12.218893 loss_rnnt 8.200665 hw_loss 0.447502 lr 0.00047532 rank 4
2023-02-23 02:35:06,429 DEBUG TRAIN Batch 13/2200 loss 7.283499 loss_att 11.041231 loss_ctc 11.130510 loss_rnnt 5.817610 hw_loss 0.377639 lr 0.00047532 rank 2
2023-02-23 02:35:06,430 DEBUG TRAIN Batch 13/2200 loss 8.466930 loss_att 11.696136 loss_ctc 12.377758 loss_rnnt 7.063387 hw_loss 0.442984 lr 0.00047536 rank 5
2023-02-23 02:35:06,476 DEBUG TRAIN Batch 13/2200 loss 16.079967 loss_att 20.584677 loss_ctc 18.637541 loss_rnnt 14.613768 hw_loss 0.420463 lr 0.00047537 rank 6
2023-02-23 02:36:18,861 DEBUG TRAIN Batch 13/2300 loss 11.280310 loss_att 18.443918 loss_ctc 13.396563 loss_rnnt 9.365920 hw_loss 0.374063 lr 0.00047511 rank 7
2023-02-23 02:36:18,861 DEBUG TRAIN Batch 13/2300 loss 15.224454 loss_att 18.775291 loss_ctc 17.742279 loss_rnnt 13.957807 hw_loss 0.413943 lr 0.00047510 rank 3
2023-02-23 02:36:18,865 DEBUG TRAIN Batch 13/2300 loss 12.575640 loss_att 15.990219 loss_ctc 15.249865 loss_rnnt 11.270360 hw_loss 0.498377 lr 0.00047520 rank 0
2023-02-23 02:36:18,866 DEBUG TRAIN Batch 13/2300 loss 16.292887 loss_att 22.043617 loss_ctc 19.049833 loss_rnnt 14.519968 hw_loss 0.478458 lr 0.00047514 rank 5
2023-02-23 02:36:18,867 DEBUG TRAIN Batch 13/2300 loss 15.565296 loss_att 15.143850 loss_ctc 16.212353 loss_rnnt 15.345711 hw_loss 0.408000 lr 0.00047520 rank 1
2023-02-23 02:36:18,871 DEBUG TRAIN Batch 13/2300 loss 14.040208 loss_att 15.772728 loss_ctc 15.002453 loss_rnnt 13.331373 hw_loss 0.438809 lr 0.00047511 rank 4
2023-02-23 02:36:18,870 DEBUG TRAIN Batch 13/2300 loss 21.270258 loss_att 25.139023 loss_ctc 30.106424 loss_rnnt 19.100136 hw_loss 0.409149 lr 0.00047511 rank 2
2023-02-23 02:36:18,912 DEBUG TRAIN Batch 13/2300 loss 12.784532 loss_att 15.694123 loss_ctc 17.070461 loss_rnnt 11.431433 hw_loss 0.374482 lr 0.00047516 rank 6
2023-02-23 02:37:32,716 DEBUG TRAIN Batch 13/2400 loss 6.898574 loss_att 9.440830 loss_ctc 10.507110 loss_rnnt 5.653244 hw_loss 0.479513 lr 0.00047498 rank 1
2023-02-23 02:37:32,722 DEBUG TRAIN Batch 13/2400 loss 8.880979 loss_att 13.483648 loss_ctc 10.560157 loss_rnnt 7.529746 hw_loss 0.387765 lr 0.00047493 rank 5
2023-02-23 02:37:32,725 DEBUG TRAIN Batch 13/2400 loss 20.660067 loss_att 18.066734 loss_ctc 18.924744 loss_rnnt 21.154694 hw_loss 0.478904 lr 0.00047489 rank 3
2023-02-23 02:37:32,728 DEBUG TRAIN Batch 13/2400 loss 12.624256 loss_att 13.423074 loss_ctc 13.090985 loss_rnnt 12.208117 hw_loss 0.364022 lr 0.00047490 rank 7
2023-02-23 02:37:32,731 DEBUG TRAIN Batch 13/2400 loss 25.472189 loss_att 27.900921 loss_ctc 30.937334 loss_rnnt 24.048397 hw_loss 0.392553 lr 0.00047489 rank 4
2023-02-23 02:37:32,733 DEBUG TRAIN Batch 13/2400 loss 11.440192 loss_att 16.029182 loss_ctc 15.734875 loss_rnnt 9.721851 hw_loss 0.427348 lr 0.00047498 rank 0
2023-02-23 02:37:32,735 DEBUG TRAIN Batch 13/2400 loss 8.012959 loss_att 11.226625 loss_ctc 8.946434 loss_rnnt 7.049440 hw_loss 0.368103 lr 0.00047494 rank 6
2023-02-23 02:37:32,782 DEBUG TRAIN Batch 13/2400 loss 13.104472 loss_att 14.113565 loss_ctc 14.605701 loss_rnnt 12.478270 hw_loss 0.420413 lr 0.00047489 rank 2
2023-02-23 02:38:50,149 DEBUG TRAIN Batch 13/2500 loss 12.767426 loss_att 13.594524 loss_ctc 17.469666 loss_rnnt 11.714805 hw_loss 0.487946 lr 0.00047468 rank 7
2023-02-23 02:38:50,149 DEBUG TRAIN Batch 13/2500 loss 10.716805 loss_att 12.166172 loss_ctc 12.311626 loss_rnnt 9.898979 hw_loss 0.591202 lr 0.00047477 rank 1
2023-02-23 02:38:50,151 DEBUG TRAIN Batch 13/2500 loss 12.841881 loss_att 12.186300 loss_ctc 15.276505 loss_rnnt 12.337398 hw_loss 0.583093 lr 0.00047477 rank 0
2023-02-23 02:38:50,153 DEBUG TRAIN Batch 13/2500 loss 10.087832 loss_att 13.465996 loss_ctc 15.105220 loss_rnnt 8.462150 hw_loss 0.526998 lr 0.00047468 rank 2
2023-02-23 02:38:50,154 DEBUG TRAIN Batch 13/2500 loss 19.215639 loss_att 20.728106 loss_ctc 24.115620 loss_rnnt 18.001476 hw_loss 0.484387 lr 0.00047468 rank 3
2023-02-23 02:38:50,156 DEBUG TRAIN Batch 13/2500 loss 17.705364 loss_att 19.552841 loss_ctc 18.956694 loss_rnnt 16.910686 hw_loss 0.484387 lr 0.00047468 rank 4
2023-02-23 02:38:50,205 DEBUG TRAIN Batch 13/2500 loss 18.745148 loss_att 21.955853 loss_ctc 20.821892 loss_rnnt 17.603331 hw_loss 0.417704 lr 0.00047473 rank 6
2023-02-23 02:38:50,588 DEBUG TRAIN Batch 13/2500 loss 13.025149 loss_att 11.736212 loss_ctc 15.369957 loss_rnnt 12.615650 hw_loss 0.664959 lr 0.00047471 rank 5
2023-02-23 02:40:04,192 DEBUG TRAIN Batch 13/2600 loss 9.666453 loss_att 15.482277 loss_ctc 16.408676 loss_rnnt 7.378969 hw_loss 0.422544 lr 0.00047446 rank 2
2023-02-23 02:40:04,203 DEBUG TRAIN Batch 13/2600 loss 2.935294 loss_att 5.250242 loss_ctc 5.338748 loss_rnnt 1.943677 hw_loss 0.390313 lr 0.00047456 rank 0
2023-02-23 02:40:04,215 DEBUG TRAIN Batch 13/2600 loss 22.302521 loss_att 28.655846 loss_ctc 23.012604 loss_rnnt 20.747952 hw_loss 0.354797 lr 0.00047446 rank 4
2023-02-23 02:40:04,216 DEBUG TRAIN Batch 13/2600 loss 8.925810 loss_att 16.120760 loss_ctc 13.589469 loss_rnnt 6.696735 hw_loss 0.315496 lr 0.00047447 rank 7
2023-02-23 02:40:04,221 DEBUG TRAIN Batch 13/2600 loss 6.702419 loss_att 11.163616 loss_ctc 9.097977 loss_rnnt 5.296128 hw_loss 0.364955 lr 0.00047451 rank 6
2023-02-23 02:40:04,220 DEBUG TRAIN Batch 13/2600 loss 11.519775 loss_att 21.413027 loss_ctc 13.945811 loss_rnnt 9.025166 hw_loss 0.360915 lr 0.00047446 rank 3
2023-02-23 02:40:04,248 DEBUG TRAIN Batch 13/2600 loss 14.833038 loss_att 16.423948 loss_ctc 18.132393 loss_rnnt 13.793581 hw_loss 0.527552 lr 0.00047450 rank 5
2023-02-23 02:40:04,291 DEBUG TRAIN Batch 13/2600 loss 22.015875 loss_att 23.614927 loss_ctc 26.331106 loss_rnnt 20.925240 hw_loss 0.366494 lr 0.00047455 rank 1
2023-02-23 02:41:17,437 DEBUG TRAIN Batch 13/2700 loss 8.421603 loss_att 15.215320 loss_ctc 12.282226 loss_rnnt 6.389234 hw_loss 0.297892 lr 0.00047425 rank 4
2023-02-23 02:41:17,438 DEBUG TRAIN Batch 13/2700 loss 5.785384 loss_att 10.858422 loss_ctc 8.269023 loss_rnnt 4.250872 hw_loss 0.353910 lr 0.00047426 rank 7
2023-02-23 02:41:17,439 DEBUG TRAIN Batch 13/2700 loss 14.653342 loss_att 16.245823 loss_ctc 14.712469 loss_rnnt 14.145644 hw_loss 0.339974 lr 0.00047430 rank 6
2023-02-23 02:41:17,439 DEBUG TRAIN Batch 13/2700 loss 15.835685 loss_att 20.498837 loss_ctc 22.985052 loss_rnnt 13.744296 hw_loss 0.385330 lr 0.00047425 rank 3
2023-02-23 02:41:17,440 DEBUG TRAIN Batch 13/2700 loss 7.497639 loss_att 11.387378 loss_ctc 12.175339 loss_rnnt 5.828336 hw_loss 0.501865 lr 0.00047429 rank 5
2023-02-23 02:41:17,443 DEBUG TRAIN Batch 13/2700 loss 9.620480 loss_att 16.068882 loss_ctc 12.624694 loss_rnnt 7.728436 hw_loss 0.378376 lr 0.00047434 rank 1
2023-02-23 02:41:17,447 DEBUG TRAIN Batch 13/2700 loss 13.950114 loss_att 18.813959 loss_ctc 17.477612 loss_rnnt 12.270251 hw_loss 0.443928 lr 0.00047434 rank 0
2023-02-23 02:41:17,498 DEBUG TRAIN Batch 13/2700 loss 8.205615 loss_att 12.248236 loss_ctc 11.696398 loss_rnnt 6.747200 hw_loss 0.345851 lr 0.00047425 rank 2
2023-02-23 02:42:31,877 DEBUG TRAIN Batch 13/2800 loss 15.113302 loss_att 16.908375 loss_ctc 15.065323 loss_rnnt 14.558696 hw_loss 0.378727 lr 0.00047404 rank 7
2023-02-23 02:42:31,882 DEBUG TRAIN Batch 13/2800 loss 18.461557 loss_att 25.128733 loss_ctc 27.043188 loss_rnnt 15.779519 hw_loss 0.383219 lr 0.00047413 rank 1
2023-02-23 02:42:31,897 DEBUG TRAIN Batch 13/2800 loss 11.023946 loss_att 15.626530 loss_ctc 17.236488 loss_rnnt 9.070671 hw_loss 0.383284 lr 0.00047409 rank 6
2023-02-23 02:42:31,898 DEBUG TRAIN Batch 13/2800 loss 9.744684 loss_att 12.925947 loss_ctc 15.026863 loss_rnnt 8.157400 hw_loss 0.462639 lr 0.00047407 rank 5
2023-02-23 02:42:31,900 DEBUG TRAIN Batch 13/2800 loss 8.186993 loss_att 12.585443 loss_ctc 13.702400 loss_rnnt 6.410801 hw_loss 0.302088 lr 0.00047404 rank 4
2023-02-23 02:42:31,902 DEBUG TRAIN Batch 13/2800 loss 5.778757 loss_att 10.579167 loss_ctc 7.339374 loss_rnnt 4.404653 hw_loss 0.386137 lr 0.00047403 rank 3
2023-02-23 02:42:31,901 DEBUG TRAIN Batch 13/2800 loss 9.771199 loss_att 14.127313 loss_ctc 14.062326 loss_rnnt 8.094936 hw_loss 0.436669 lr 0.00047413 rank 0
2023-02-23 02:42:31,907 DEBUG TRAIN Batch 13/2800 loss 8.042023 loss_att 12.885468 loss_ctc 12.265411 loss_rnnt 6.338679 hw_loss 0.321631 lr 0.00047404 rank 2
2023-02-23 02:43:46,413 DEBUG TRAIN Batch 13/2900 loss 22.856703 loss_att 26.363005 loss_ctc 29.870438 loss_rnnt 21.014174 hw_loss 0.386449 lr 0.00047382 rank 2
2023-02-23 02:43:46,423 DEBUG TRAIN Batch 13/2900 loss 12.788246 loss_att 15.017454 loss_ctc 12.561182 loss_rnnt 12.102333 hw_loss 0.506901 lr 0.00047382 rank 3
2023-02-23 02:43:46,425 DEBUG TRAIN Batch 13/2900 loss 14.628112 loss_att 19.219011 loss_ctc 16.526125 loss_rnnt 13.273466 hw_loss 0.343870 lr 0.00047383 rank 7
2023-02-23 02:43:46,429 DEBUG TRAIN Batch 13/2900 loss 10.375628 loss_att 13.961301 loss_ctc 13.525606 loss_rnnt 8.952073 hw_loss 0.537039 lr 0.00047392 rank 0
2023-02-23 02:43:46,432 DEBUG TRAIN Batch 13/2900 loss 18.349184 loss_att 21.082497 loss_ctc 23.400520 loss_rnnt 16.854645 hw_loss 0.514436 lr 0.00047382 rank 4
2023-02-23 02:43:46,437 DEBUG TRAIN Batch 13/2900 loss 18.757069 loss_att 23.297520 loss_ctc 25.301708 loss_rnnt 16.764603 hw_loss 0.397042 lr 0.00047386 rank 5
2023-02-23 02:43:46,440 DEBUG TRAIN Batch 13/2900 loss 19.117800 loss_att 21.141821 loss_ctc 19.265560 loss_rnnt 18.480774 hw_loss 0.398477 lr 0.00047391 rank 1
2023-02-23 02:43:46,483 DEBUG TRAIN Batch 13/2900 loss 15.751978 loss_att 18.989372 loss_ctc 19.715111 loss_rnnt 14.374498 hw_loss 0.377968 lr 0.00047387 rank 6
2023-02-23 02:44:59,475 DEBUG TRAIN Batch 13/3000 loss 6.506515 loss_att 10.524711 loss_ctc 7.416842 loss_rnnt 5.354502 hw_loss 0.425619 lr 0.00047361 rank 3
2023-02-23 02:44:59,476 DEBUG TRAIN Batch 13/3000 loss 13.925249 loss_att 16.353592 loss_ctc 17.310024 loss_rnnt 12.810183 hw_loss 0.333925 lr 0.00047362 rank 7
2023-02-23 02:44:59,482 DEBUG TRAIN Batch 13/3000 loss 10.387546 loss_att 14.708536 loss_ctc 14.717091 loss_rnnt 8.744804 hw_loss 0.377381 lr 0.00047366 rank 6
2023-02-23 02:44:59,482 DEBUG TRAIN Batch 13/3000 loss 8.291005 loss_att 10.802397 loss_ctc 10.488596 loss_rnnt 7.294927 hw_loss 0.376477 lr 0.00047361 rank 4
2023-02-23 02:44:59,483 DEBUG TRAIN Batch 13/3000 loss 11.580439 loss_att 15.886253 loss_ctc 18.242203 loss_rnnt 9.547773 hw_loss 0.531125 lr 0.00047370 rank 0
2023-02-23 02:44:59,484 DEBUG TRAIN Batch 13/3000 loss 18.749071 loss_att 22.271608 loss_ctc 22.735771 loss_rnnt 17.306198 hw_loss 0.387761 lr 0.00047370 rank 1
2023-02-23 02:44:59,484 DEBUG TRAIN Batch 13/3000 loss 8.857443 loss_att 10.621306 loss_ctc 13.438601 loss_rnnt 7.659956 hw_loss 0.438546 lr 0.00047365 rank 5
2023-02-23 02:44:59,486 DEBUG TRAIN Batch 13/3000 loss 12.613699 loss_att 17.410690 loss_ctc 18.747810 loss_rnnt 10.579361 hw_loss 0.481984 lr 0.00047361 rank 2
2023-02-23 02:46:13,767 DEBUG TRAIN Batch 13/3100 loss 11.945842 loss_att 11.681079 loss_ctc 14.294976 loss_rnnt 11.257813 hw_loss 0.802055 lr 0.00047344 rank 5
2023-02-23 02:46:13,774 DEBUG TRAIN Batch 13/3100 loss 16.222523 loss_att 17.457514 loss_ctc 20.955400 loss_rnnt 15.132839 hw_loss 0.396814 lr 0.00047341 rank 7
2023-02-23 02:46:13,777 DEBUG TRAIN Batch 13/3100 loss 8.502892 loss_att 11.661394 loss_ctc 10.329436 loss_rnnt 7.399898 hw_loss 0.427042 lr 0.00047340 rank 4
2023-02-23 02:46:13,778 DEBUG TRAIN Batch 13/3100 loss 14.897981 loss_att 14.526836 loss_ctc 20.099234 loss_rnnt 14.065523 hw_loss 0.399722 lr 0.00047340 rank 2
2023-02-23 02:46:13,779 DEBUG TRAIN Batch 13/3100 loss 18.755508 loss_att 20.357069 loss_ctc 22.190464 loss_rnnt 17.748756 hw_loss 0.428332 lr 0.00047340 rank 3
2023-02-23 02:46:13,779 DEBUG TRAIN Batch 13/3100 loss 9.639001 loss_att 9.038642 loss_ctc 11.970417 loss_rnnt 9.144465 hw_loss 0.569538 lr 0.00047349 rank 0
2023-02-23 02:46:13,779 DEBUG TRAIN Batch 13/3100 loss 10.845946 loss_att 12.906755 loss_ctc 15.241076 loss_rnnt 9.566598 hw_loss 0.527190 lr 0.00047349 rank 1
2023-02-23 02:46:13,827 DEBUG TRAIN Batch 13/3100 loss 17.218555 loss_att 19.677765 loss_ctc 25.793793 loss_rnnt 15.370703 hw_loss 0.398710 lr 0.00047345 rank 6
2023-02-23 02:47:29,879 DEBUG TRAIN Batch 13/3200 loss 8.446073 loss_att 8.570011 loss_ctc 10.076004 loss_rnnt 7.873899 hw_loss 0.618866 lr 0.00047319 rank 7
2023-02-23 02:47:29,895 DEBUG TRAIN Batch 13/3200 loss 14.093410 loss_att 14.398106 loss_ctc 16.764832 loss_rnnt 13.319834 hw_loss 0.668338 lr 0.00047318 rank 3
2023-02-23 02:47:29,898 DEBUG TRAIN Batch 13/3200 loss 12.415356 loss_att 12.799089 loss_ctc 13.385223 loss_rnnt 11.889516 hw_loss 0.599582 lr 0.00047319 rank 4
2023-02-23 02:47:29,900 DEBUG TRAIN Batch 13/3200 loss 10.398490 loss_att 14.707952 loss_ctc 6.541526 loss_rnnt 9.842438 hw_loss 0.390790 lr 0.00047322 rank 5
2023-02-23 02:47:29,900 DEBUG TRAIN Batch 13/3200 loss 25.045837 loss_att 25.838520 loss_ctc 25.223425 loss_rnnt 24.689419 hw_loss 0.326636 lr 0.00047328 rank 0
2023-02-23 02:47:29,901 DEBUG TRAIN Batch 13/3200 loss 13.521911 loss_att 11.665792 loss_ctc 13.709963 loss_rnnt 13.509559 hw_loss 0.672193 lr 0.00047324 rank 6
2023-02-23 02:47:29,904 DEBUG TRAIN Batch 13/3200 loss 17.077349 loss_att 20.295551 loss_ctc 22.577341 loss_rnnt 15.421579 hw_loss 0.522741 lr 0.00047319 rank 2
2023-02-23 02:47:29,914 DEBUG TRAIN Batch 13/3200 loss 10.980680 loss_att 17.759247 loss_ctc 17.347580 loss_rnnt 8.556429 hw_loss 0.411786 lr 0.00047328 rank 1
2023-02-23 02:48:43,487 DEBUG TRAIN Batch 13/3300 loss 19.571522 loss_att 20.581011 loss_ctc 22.869907 loss_rnnt 18.743137 hw_loss 0.350064 lr 0.00047302 rank 6
2023-02-23 02:48:43,488 DEBUG TRAIN Batch 13/3300 loss 7.420776 loss_att 10.522781 loss_ctc 7.540224 loss_rnnt 6.585924 hw_loss 0.372235 lr 0.00047298 rank 4
2023-02-23 02:48:43,489 DEBUG TRAIN Batch 13/3300 loss 9.787922 loss_att 14.463454 loss_ctc 11.755383 loss_rnnt 8.370506 hw_loss 0.412465 lr 0.00047298 rank 7
2023-02-23 02:48:43,490 DEBUG TRAIN Batch 13/3300 loss 7.089523 loss_att 11.236117 loss_ctc 7.654480 loss_rnnt 6.029203 hw_loss 0.291889 lr 0.00047297 rank 3
2023-02-23 02:48:43,492 DEBUG TRAIN Batch 13/3300 loss 9.315195 loss_att 13.498699 loss_ctc 12.660618 loss_rnnt 7.832684 hw_loss 0.374538 lr 0.00047306 rank 1
2023-02-23 02:48:43,495 DEBUG TRAIN Batch 13/3300 loss 13.500596 loss_att 18.065372 loss_ctc 15.925131 loss_rnnt 12.077670 hw_loss 0.350064 lr 0.00047301 rank 5
2023-02-23 02:48:43,498 DEBUG TRAIN Batch 13/3300 loss 12.322044 loss_att 16.682737 loss_ctc 19.385885 loss_rnnt 10.264583 hw_loss 0.456521 lr 0.00047298 rank 2
2023-02-23 02:48:43,499 DEBUG TRAIN Batch 13/3300 loss 14.186739 loss_att 21.320839 loss_ctc 22.729877 loss_rnnt 11.396973 hw_loss 0.419741 lr 0.00047307 rank 0
2023-02-23 02:49:56,080 DEBUG TRAIN Batch 13/3400 loss 14.271705 loss_att 17.881731 loss_ctc 18.554180 loss_rnnt 12.748951 hw_loss 0.430784 lr 0.00047277 rank 7
2023-02-23 02:49:56,084 DEBUG TRAIN Batch 13/3400 loss 15.327957 loss_att 22.010456 loss_ctc 20.970064 loss_rnnt 13.035801 hw_loss 0.381326 lr 0.00047285 rank 0
2023-02-23 02:49:56,086 DEBUG TRAIN Batch 13/3400 loss 5.158130 loss_att 9.930243 loss_ctc 5.330824 loss_rnnt 3.932324 hw_loss 0.465671 lr 0.00047281 rank 6
2023-02-23 02:49:56,094 DEBUG TRAIN Batch 13/3400 loss 22.308212 loss_att 25.489048 loss_ctc 29.284386 loss_rnnt 20.524914 hw_loss 0.406827 lr 0.00047276 rank 3
2023-02-23 02:49:56,094 DEBUG TRAIN Batch 13/3400 loss 24.004015 loss_att 33.794243 loss_ctc 36.223907 loss_rnnt 20.171089 hw_loss 0.460428 lr 0.00047280 rank 5
2023-02-23 02:49:56,102 DEBUG TRAIN Batch 13/3400 loss 9.286777 loss_att 15.647804 loss_ctc 12.052711 loss_rnnt 7.453057 hw_loss 0.361357 lr 0.00047285 rank 1
2023-02-23 02:49:56,106 DEBUG TRAIN Batch 13/3400 loss 6.851649 loss_att 12.140420 loss_ctc 9.963224 loss_rnnt 5.149672 hw_loss 0.430025 lr 0.00047276 rank 2
2023-02-23 02:49:56,140 DEBUG TRAIN Batch 13/3400 loss 16.827892 loss_att 19.185724 loss_ctc 22.752132 loss_rnnt 15.372793 hw_loss 0.363064 lr 0.00047276 rank 4
2023-02-23 02:51:10,993 DEBUG TRAIN Batch 13/3500 loss 5.552735 loss_att 9.420449 loss_ctc 6.359393 loss_rnnt 4.465582 hw_loss 0.386352 lr 0.00047264 rank 1
2023-02-23 02:51:10,998 DEBUG TRAIN Batch 13/3500 loss 9.869993 loss_att 12.998682 loss_ctc 13.789257 loss_rnnt 8.512923 hw_loss 0.391430 lr 0.00047255 rank 2
2023-02-23 02:51:11,005 DEBUG TRAIN Batch 13/3500 loss 18.108223 loss_att 21.696743 loss_ctc 24.284273 loss_rnnt 16.337025 hw_loss 0.431291 lr 0.00047255 rank 3
2023-02-23 02:51:11,009 DEBUG TRAIN Batch 13/3500 loss 13.222961 loss_att 15.147775 loss_ctc 18.197966 loss_rnnt 11.928398 hw_loss 0.461749 lr 0.00047256 rank 7
2023-02-23 02:51:11,011 DEBUG TRAIN Batch 13/3500 loss 13.983932 loss_att 16.463537 loss_ctc 17.497944 loss_rnnt 12.856399 hw_loss 0.305772 lr 0.00047259 rank 5
2023-02-23 02:51:11,012 DEBUG TRAIN Batch 13/3500 loss 22.832714 loss_att 28.171017 loss_ctc 29.856779 loss_rnnt 20.643200 hw_loss 0.347456 lr 0.00047255 rank 4
2023-02-23 02:51:11,015 DEBUG TRAIN Batch 13/3500 loss 15.947854 loss_att 18.834570 loss_ctc 19.520962 loss_rnnt 14.668349 hw_loss 0.423276 lr 0.00047264 rank 0
2023-02-23 02:51:11,062 DEBUG TRAIN Batch 13/3500 loss 9.567508 loss_att 12.885821 loss_ctc 11.838996 loss_rnnt 8.411210 hw_loss 0.355818 lr 0.00047260 rank 6
2023-02-23 02:52:25,705 DEBUG TRAIN Batch 13/3600 loss 13.637954 loss_att 19.205656 loss_ctc 23.959015 loss_rnnt 10.935911 hw_loss 0.398175 lr 0.00047235 rank 7
2023-02-23 02:52:25,713 DEBUG TRAIN Batch 13/3600 loss 13.520883 loss_att 17.690046 loss_ctc 19.342123 loss_rnnt 11.681517 hw_loss 0.430064 lr 0.00047243 rank 1
2023-02-23 02:52:25,715 DEBUG TRAIN Batch 13/3600 loss 24.002348 loss_att 28.341135 loss_ctc 28.300501 loss_rnnt 22.347204 hw_loss 0.401812 lr 0.00047238 rank 5
2023-02-23 02:52:25,716 DEBUG TRAIN Batch 13/3600 loss 14.241602 loss_att 16.432270 loss_ctc 19.996986 loss_rnnt 12.771618 hw_loss 0.495872 lr 0.00047234 rank 4
2023-02-23 02:52:25,716 DEBUG TRAIN Batch 13/3600 loss 15.309449 loss_att 17.048058 loss_ctc 18.330734 loss_rnnt 14.332483 hw_loss 0.424510 lr 0.00047234 rank 3
2023-02-23 02:52:25,718 DEBUG TRAIN Batch 13/3600 loss 12.076445 loss_att 15.748444 loss_ctc 18.200474 loss_rnnt 10.310346 hw_loss 0.403429 lr 0.00047243 rank 0
2023-02-23 02:52:25,722 DEBUG TRAIN Batch 13/3600 loss 19.358126 loss_att 21.521791 loss_ctc 23.775080 loss_rnnt 18.145203 hw_loss 0.358613 lr 0.00047234 rank 2
2023-02-23 02:52:25,766 DEBUG TRAIN Batch 13/3600 loss 7.800975 loss_att 8.574560 loss_ctc 9.188602 loss_rnnt 7.242127 hw_loss 0.410837 lr 0.00047239 rank 6
2023-02-23 02:53:39,009 DEBUG TRAIN Batch 13/3700 loss 14.500683 loss_att 15.545262 loss_ctc 21.956060 loss_rnnt 13.041455 hw_loss 0.480490 lr 0.00047222 rank 0
2023-02-23 02:53:39,020 DEBUG TRAIN Batch 13/3700 loss 11.583533 loss_att 15.754850 loss_ctc 14.501370 loss_rnnt 10.135571 hw_loss 0.421223 lr 0.00047214 rank 7
2023-02-23 02:53:39,025 DEBUG TRAIN Batch 13/3700 loss 18.628218 loss_att 19.055830 loss_ctc 25.087793 loss_rnnt 17.459898 hw_loss 0.415351 lr 0.00047213 rank 3
2023-02-23 02:53:39,028 DEBUG TRAIN Batch 13/3700 loss 11.923730 loss_att 14.215849 loss_ctc 13.477768 loss_rnnt 10.972766 hw_loss 0.535003 lr 0.00047217 rank 5
2023-02-23 02:53:39,028 DEBUG TRAIN Batch 13/3700 loss 29.861521 loss_att 30.337303 loss_ctc 34.638794 loss_rnnt 28.882711 hw_loss 0.462533 lr 0.00047222 rank 1
2023-02-23 02:53:39,033 DEBUG TRAIN Batch 13/3700 loss 10.793133 loss_att 14.468195 loss_ctc 14.770007 loss_rnnt 9.330650 hw_loss 0.369788 lr 0.00047213 rank 2
2023-02-23 02:53:39,033 DEBUG TRAIN Batch 13/3700 loss 21.529640 loss_att 23.234734 loss_ctc 24.187229 loss_rnnt 20.650639 hw_loss 0.344319 lr 0.00047213 rank 4
2023-02-23 02:53:39,037 DEBUG TRAIN Batch 13/3700 loss 14.847687 loss_att 15.637402 loss_ctc 18.697304 loss_rnnt 13.928637 hw_loss 0.464670 lr 0.00047218 rank 6
2023-02-23 02:54:52,042 DEBUG TRAIN Batch 13/3800 loss 22.247141 loss_att 22.597885 loss_ctc 28.193024 loss_rnnt 21.160347 hw_loss 0.419739 lr 0.00047193 rank 7
2023-02-23 02:54:52,048 DEBUG TRAIN Batch 13/3800 loss 11.501796 loss_att 12.043610 loss_ctc 12.967859 loss_rnnt 10.875278 hw_loss 0.605022 lr 0.00047197 rank 6
2023-02-23 02:54:52,049 DEBUG TRAIN Batch 13/3800 loss 9.890324 loss_att 10.448002 loss_ctc 11.684048 loss_rnnt 9.198248 hw_loss 0.640079 lr 0.00047201 rank 1
2023-02-23 02:54:52,052 DEBUG TRAIN Batch 13/3800 loss 23.726694 loss_att 22.166058 loss_ctc 33.724754 loss_rnnt 22.473127 hw_loss 0.436165 lr 0.00047192 rank 4
2023-02-23 02:54:52,054 DEBUG TRAIN Batch 13/3800 loss 16.491619 loss_att 22.397694 loss_ctc 20.492138 loss_rnnt 14.595409 hw_loss 0.340487 lr 0.00047201 rank 0
2023-02-23 02:54:52,055 DEBUG TRAIN Batch 13/3800 loss 13.378714 loss_att 17.718840 loss_ctc 19.200703 loss_rnnt 11.490976 hw_loss 0.456463 lr 0.00047192 rank 3
2023-02-23 02:54:52,058 DEBUG TRAIN Batch 13/3800 loss 12.999159 loss_att 12.971835 loss_ctc 16.581263 loss_rnnt 12.207226 hw_loss 0.599597 lr 0.00047192 rank 2
2023-02-23 02:54:52,060 DEBUG TRAIN Batch 13/3800 loss 8.276601 loss_att 13.256915 loss_ctc 7.729503 loss_rnnt 7.121450 hw_loss 0.435063 lr 0.00047196 rank 5
2023-02-23 02:56:07,387 DEBUG TRAIN Batch 13/3900 loss 6.720013 loss_att 8.186872 loss_ctc 9.273448 loss_rnnt 5.935219 hw_loss 0.283059 lr 0.00047172 rank 7
2023-02-23 02:56:07,388 DEBUG TRAIN Batch 13/3900 loss 14.855603 loss_att 18.862530 loss_ctc 21.159369 loss_rnnt 12.991492 hw_loss 0.416667 lr 0.00047171 rank 2
2023-02-23 02:56:07,391 DEBUG TRAIN Batch 13/3900 loss 18.877554 loss_att 23.832081 loss_ctc 22.639168 loss_rnnt 17.159939 hw_loss 0.422176 lr 0.00047171 rank 3
2023-02-23 02:56:07,393 DEBUG TRAIN Batch 13/3900 loss 11.894976 loss_att 16.727327 loss_ctc 15.567473 loss_rnnt 10.224860 hw_loss 0.401211 lr 0.00047171 rank 4
2023-02-23 02:56:07,393 DEBUG TRAIN Batch 13/3900 loss 18.066525 loss_att 23.460310 loss_ctc 23.361080 loss_rnnt 16.110512 hw_loss 0.321217 lr 0.00047176 rank 6
2023-02-23 02:56:07,398 DEBUG TRAIN Batch 13/3900 loss 7.743148 loss_att 8.552849 loss_ctc 8.903805 loss_rnnt 7.180755 hw_loss 0.460686 lr 0.00047175 rank 5
2023-02-23 02:56:07,399 DEBUG TRAIN Batch 13/3900 loss 20.006866 loss_att 23.848568 loss_ctc 24.150444 loss_rnnt 18.466314 hw_loss 0.412003 lr 0.00047180 rank 1
2023-02-23 02:56:07,439 DEBUG TRAIN Batch 13/3900 loss 15.872553 loss_att 19.789438 loss_ctc 16.550575 loss_rnnt 14.782596 hw_loss 0.405330 lr 0.00047180 rank 0
2023-02-23 02:57:21,920 DEBUG TRAIN Batch 13/4000 loss 36.365402 loss_att 37.947193 loss_ctc 41.692886 loss_rnnt 35.158894 hw_loss 0.337166 lr 0.00047150 rank 3
2023-02-23 02:57:21,941 DEBUG TRAIN Batch 13/4000 loss 11.675199 loss_att 13.338414 loss_ctc 15.163206 loss_rnnt 10.650988 hw_loss 0.424688 lr 0.00047150 rank 4
2023-02-23 02:57:21,945 DEBUG TRAIN Batch 13/4000 loss 14.304525 loss_att 20.102016 loss_ctc 17.811514 loss_rnnt 12.521467 hw_loss 0.292428 lr 0.00047151 rank 7
2023-02-23 02:57:21,948 DEBUG TRAIN Batch 13/4000 loss 18.185751 loss_att 21.352354 loss_ctc 24.723389 loss_rnnt 16.474131 hw_loss 0.387404 lr 0.00047159 rank 1
2023-02-23 02:57:21,950 DEBUG TRAIN Batch 13/4000 loss 13.621532 loss_att 16.234293 loss_ctc 16.631979 loss_rnnt 12.485805 hw_loss 0.397095 lr 0.00047150 rank 2
2023-02-23 02:57:21,951 DEBUG TRAIN Batch 13/4000 loss 33.914856 loss_att 35.829033 loss_ctc 37.099159 loss_rnnt 32.868362 hw_loss 0.448284 lr 0.00047154 rank 5
2023-02-23 02:57:21,952 DEBUG TRAIN Batch 13/4000 loss 15.634867 loss_att 18.100544 loss_ctc 17.263325 loss_rnnt 14.719961 hw_loss 0.383706 lr 0.00047155 rank 6
2023-02-23 02:57:21,955 DEBUG TRAIN Batch 13/4000 loss 5.212978 loss_att 8.311842 loss_ctc 6.312636 loss_rnnt 4.264603 hw_loss 0.341214 lr 0.00047159 rank 0
2023-02-23 02:58:35,140 DEBUG TRAIN Batch 13/4100 loss 7.218683 loss_att 10.736832 loss_ctc 9.327331 loss_rnnt 6.035851 hw_loss 0.371344 lr 0.00047130 rank 7
2023-02-23 02:58:35,143 DEBUG TRAIN Batch 13/4100 loss 11.686442 loss_att 15.390892 loss_ctc 16.925980 loss_rnnt 10.057357 hw_loss 0.355482 lr 0.00047129 rank 3
2023-02-23 02:58:35,146 DEBUG TRAIN Batch 13/4100 loss 21.917284 loss_att 21.680571 loss_ctc 29.344599 loss_rnnt 20.721722 hw_loss 0.473620 lr 0.00047138 rank 1
2023-02-23 02:58:35,148 DEBUG TRAIN Batch 13/4100 loss 12.337192 loss_att 16.780991 loss_ctc 13.427692 loss_rnnt 11.086660 hw_loss 0.405695 lr 0.00047138 rank 0
2023-02-23 02:58:35,149 DEBUG TRAIN Batch 13/4100 loss 7.671428 loss_att 9.321198 loss_ctc 13.351810 loss_rnnt 6.356323 hw_loss 0.427062 lr 0.00047129 rank 4
2023-02-23 02:58:35,153 DEBUG TRAIN Batch 13/4100 loss 9.718045 loss_att 11.960865 loss_ctc 11.466224 loss_rnnt 8.830943 hw_loss 0.385215 lr 0.00047134 rank 6
2023-02-23 02:58:35,154 DEBUG TRAIN Batch 13/4100 loss 7.895262 loss_att 14.279035 loss_ctc 10.599802 loss_rnnt 6.047397 hw_loss 0.394697 lr 0.00047129 rank 2
2023-02-23 02:58:35,195 DEBUG TRAIN Batch 13/4100 loss 20.051838 loss_att 24.597006 loss_ctc 23.154945 loss_rnnt 18.501347 hw_loss 0.426957 lr 0.00047133 rank 5
2023-02-23 02:59:48,226 DEBUG TRAIN Batch 13/4200 loss 12.541871 loss_att 16.850933 loss_ctc 15.523397 loss_rnnt 11.089191 hw_loss 0.362495 lr 0.00047108 rank 3
2023-02-23 02:59:48,228 DEBUG TRAIN Batch 13/4200 loss 13.260116 loss_att 19.342617 loss_ctc 18.262947 loss_rnnt 11.206679 hw_loss 0.318545 lr 0.00047113 rank 6
2023-02-23 02:59:48,228 DEBUG TRAIN Batch 13/4200 loss 19.050657 loss_att 21.010149 loss_ctc 20.344471 loss_rnnt 18.263094 hw_loss 0.418420 lr 0.00047109 rank 7
2023-02-23 02:59:48,228 DEBUG TRAIN Batch 13/4200 loss 14.434560 loss_att 16.609716 loss_ctc 16.442642 loss_rnnt 13.508233 hw_loss 0.419157 lr 0.00047117 rank 0
2023-02-23 02:59:48,231 DEBUG TRAIN Batch 13/4200 loss 11.711469 loss_att 15.852355 loss_ctc 17.175442 loss_rnnt 9.978063 hw_loss 0.331309 lr 0.00047108 rank 4
2023-02-23 02:59:48,232 DEBUG TRAIN Batch 13/4200 loss 11.559960 loss_att 13.401727 loss_ctc 14.389604 loss_rnnt 10.582970 hw_loss 0.433786 lr 0.00047112 rank 5
2023-02-23 02:59:48,235 DEBUG TRAIN Batch 13/4200 loss 15.714833 loss_att 18.638563 loss_ctc 17.112732 loss_rnnt 14.683129 hw_loss 0.488570 lr 0.00047117 rank 1
2023-02-23 02:59:48,252 DEBUG TRAIN Batch 13/4200 loss 28.706686 loss_att 33.379360 loss_ctc 37.440945 loss_rnnt 26.443226 hw_loss 0.308163 lr 0.00047108 rank 2
2023-02-23 03:01:03,913 DEBUG TRAIN Batch 13/4300 loss 13.584019 loss_att 18.272831 loss_ctc 19.365135 loss_rnnt 11.693881 hw_loss 0.340424 lr 0.00047091 rank 5
2023-02-23 03:01:03,928 DEBUG TRAIN Batch 13/4300 loss 14.117105 loss_att 19.490601 loss_ctc 19.209860 loss_rnnt 12.148729 hw_loss 0.402454 lr 0.00047088 rank 7
2023-02-23 03:01:03,929 DEBUG TRAIN Batch 13/4300 loss 17.747705 loss_att 21.189013 loss_ctc 20.927031 loss_rnnt 16.429626 hw_loss 0.386077 lr 0.00047087 rank 3
2023-02-23 03:01:03,932 DEBUG TRAIN Batch 13/4300 loss 10.088070 loss_att 9.687880 loss_ctc 13.407841 loss_rnnt 9.496322 hw_loss 0.429656 lr 0.00047087 rank 4
2023-02-23 03:01:03,933 DEBUG TRAIN Batch 13/4300 loss 14.722904 loss_att 13.848543 loss_ctc 15.116140 loss_rnnt 14.642173 hw_loss 0.380951 lr 0.00047096 rank 1
2023-02-23 03:01:03,934 DEBUG TRAIN Batch 13/4300 loss 10.965309 loss_att 15.681986 loss_ctc 16.289576 loss_rnnt 9.111046 hw_loss 0.376924 lr 0.00047087 rank 2
2023-02-23 03:01:03,933 DEBUG TRAIN Batch 13/4300 loss 12.600551 loss_att 14.225817 loss_ctc 15.380419 loss_rnnt 11.606281 hw_loss 0.559815 lr 0.00047096 rank 0
2023-02-23 03:01:03,981 DEBUG TRAIN Batch 13/4300 loss 15.067911 loss_att 18.213190 loss_ctc 21.462543 loss_rnnt 13.306477 hw_loss 0.524551 lr 0.00047092 rank 6
2023-02-23 03:02:17,856 DEBUG TRAIN Batch 13/4400 loss 13.612643 loss_att 16.842743 loss_ctc 20.224459 loss_rnnt 11.824445 hw_loss 0.488633 lr 0.00047066 rank 3
2023-02-23 03:02:17,857 DEBUG TRAIN Batch 13/4400 loss 15.901916 loss_att 19.447683 loss_ctc 22.982519 loss_rnnt 14.012301 hw_loss 0.443213 lr 0.00047067 rank 7
2023-02-23 03:02:17,858 DEBUG TRAIN Batch 13/4400 loss 15.048008 loss_att 17.271242 loss_ctc 16.555531 loss_rnnt 14.143317 hw_loss 0.485702 lr 0.00047071 rank 6
2023-02-23 03:02:17,859 DEBUG TRAIN Batch 13/4400 loss 8.728619 loss_att 10.473066 loss_ctc 11.270206 loss_rnnt 7.777228 hw_loss 0.494293 lr 0.00047075 rank 1
2023-02-23 03:02:17,861 DEBUG TRAIN Batch 13/4400 loss 17.866180 loss_att 17.784443 loss_ctc 25.459930 loss_rnnt 16.658216 hw_loss 0.397149 lr 0.00047066 rank 4
2023-02-23 03:02:17,867 DEBUG TRAIN Batch 13/4400 loss 17.831083 loss_att 19.905392 loss_ctc 20.845720 loss_rnnt 16.773647 hw_loss 0.451169 lr 0.00047075 rank 0
2023-02-23 03:02:17,869 DEBUG TRAIN Batch 13/4400 loss 11.285912 loss_att 14.217623 loss_ctc 13.320556 loss_rnnt 10.221212 hw_loss 0.388258 lr 0.00047066 rank 2
2023-02-23 03:02:17,911 DEBUG TRAIN Batch 13/4400 loss 19.199633 loss_att 20.717262 loss_ctc 26.422712 loss_rnnt 17.742851 hw_loss 0.356586 lr 0.00047070 rank 5
2023-02-23 03:03:31,454 DEBUG TRAIN Batch 13/4500 loss 18.134245 loss_att 17.494255 loss_ctc 26.676208 loss_rnnt 16.932312 hw_loss 0.358131 lr 0.00047055 rank 0
2023-02-23 03:03:31,469 DEBUG TRAIN Batch 13/4500 loss 18.559307 loss_att 22.322960 loss_ctc 26.156342 loss_rnnt 16.582127 hw_loss 0.396587 lr 0.00047046 rank 7
2023-02-23 03:03:31,470 DEBUG TRAIN Batch 13/4500 loss 12.555285 loss_att 13.030186 loss_ctc 16.157696 loss_rnnt 11.643583 hw_loss 0.630750 lr 0.00047045 rank 3
2023-02-23 03:03:31,473 DEBUG TRAIN Batch 13/4500 loss 5.579672 loss_att 12.431875 loss_ctc 8.647985 loss_rnnt 3.640904 hw_loss 0.298533 lr 0.00047049 rank 5
2023-02-23 03:03:31,477 DEBUG TRAIN Batch 13/4500 loss 6.994467 loss_att 11.567390 loss_ctc 6.487081 loss_rnnt 5.956573 hw_loss 0.358052 lr 0.00047050 rank 6
2023-02-23 03:03:31,482 DEBUG TRAIN Batch 13/4500 loss 8.954723 loss_att 14.162156 loss_ctc 13.900908 loss_rnnt 7.035593 hw_loss 0.409036 lr 0.00047046 rank 2
2023-02-23 03:03:31,482 DEBUG TRAIN Batch 13/4500 loss 19.964718 loss_att 21.296638 loss_ctc 21.069763 loss_rnnt 19.289694 hw_loss 0.489941 lr 0.00047054 rank 1
2023-02-23 03:03:31,525 DEBUG TRAIN Batch 13/4500 loss 7.211243 loss_att 10.409156 loss_ctc 10.515947 loss_rnnt 5.937519 hw_loss 0.362840 lr 0.00047046 rank 4
2023-02-23 03:04:46,909 DEBUG TRAIN Batch 13/4600 loss 6.977551 loss_att 9.390324 loss_ctc 6.493974 loss_rnnt 6.294096 hw_loss 0.497585 lr 0.00047025 rank 7
2023-02-23 03:04:46,911 DEBUG TRAIN Batch 13/4600 loss 26.728052 loss_att 36.626034 loss_ctc 37.123123 loss_rnnt 23.149944 hw_loss 0.398440 lr 0.00047034 rank 1
2023-02-23 03:04:46,914 DEBUG TRAIN Batch 13/4600 loss 8.711935 loss_att 12.083529 loss_ctc 11.867516 loss_rnnt 7.411779 hw_loss 0.384547 lr 0.00047030 rank 6
2023-02-23 03:04:46,919 DEBUG TRAIN Batch 13/4600 loss 9.529520 loss_att 12.057850 loss_ctc 10.765219 loss_rnnt 8.657223 hw_loss 0.378510 lr 0.00047025 rank 4
2023-02-23 03:04:46,918 DEBUG TRAIN Batch 13/4600 loss 25.314159 loss_att 28.448414 loss_ctc 27.300894 loss_rnnt 24.227610 hw_loss 0.365248 lr 0.00047034 rank 0
2023-02-23 03:04:46,921 DEBUG TRAIN Batch 13/4600 loss 7.149654 loss_att 9.477374 loss_ctc 9.069301 loss_rnnt 6.202388 hw_loss 0.423319 lr 0.00047028 rank 5
2023-02-23 03:04:46,924 DEBUG TRAIN Batch 13/4600 loss 7.367481 loss_att 9.847170 loss_ctc 10.493152 loss_rnnt 6.249464 hw_loss 0.384980 lr 0.00047025 rank 2
2023-02-23 03:04:46,937 DEBUG TRAIN Batch 13/4600 loss 6.229721 loss_att 12.453804 loss_ctc 7.128983 loss_rnnt 4.670280 hw_loss 0.365103 lr 0.00047025 rank 3
2023-02-23 03:06:01,034 DEBUG TRAIN Batch 13/4700 loss 20.204731 loss_att 20.885908 loss_ctc 23.610909 loss_rnnt 19.405174 hw_loss 0.392179 lr 0.00047013 rank 0
2023-02-23 03:06:01,036 DEBUG TRAIN Batch 13/4700 loss 18.365837 loss_att 25.049614 loss_ctc 22.224125 loss_rnnt 16.331055 hw_loss 0.344231 lr 0.00047004 rank 4
2023-02-23 03:06:01,035 DEBUG TRAIN Batch 13/4700 loss 12.840446 loss_att 17.696545 loss_ctc 15.603344 loss_rnnt 11.275118 hw_loss 0.423230 lr 0.00047009 rank 6
2023-02-23 03:06:01,037 DEBUG TRAIN Batch 13/4700 loss 12.721900 loss_att 15.972999 loss_ctc 14.869415 loss_rnnt 11.564400 hw_loss 0.414273 lr 0.00047004 rank 3
2023-02-23 03:06:01,038 DEBUG TRAIN Batch 13/4700 loss 7.647331 loss_att 9.088802 loss_ctc 8.089671 loss_rnnt 7.098632 hw_loss 0.377674 lr 0.00047005 rank 7
2023-02-23 03:06:01,041 DEBUG TRAIN Batch 13/4700 loss 7.464105 loss_att 10.621805 loss_ctc 9.297217 loss_rnnt 6.374600 hw_loss 0.400405 lr 0.00047008 rank 5
2023-02-23 03:06:01,046 DEBUG TRAIN Batch 13/4700 loss 13.246402 loss_att 18.850349 loss_ctc 16.026543 loss_rnnt 11.566905 hw_loss 0.352541 lr 0.00047004 rank 2
2023-02-23 03:06:01,085 DEBUG TRAIN Batch 13/4700 loss 12.423085 loss_att 20.841417 loss_ctc 15.624103 loss_rnnt 10.107634 hw_loss 0.384342 lr 0.00047013 rank 1
2023-02-23 03:07:14,125 DEBUG TRAIN Batch 13/4800 loss 10.770604 loss_att 13.808281 loss_ctc 14.217958 loss_rnnt 9.446962 hw_loss 0.480861 lr 0.00046992 rank 0
2023-02-23 03:07:14,125 DEBUG TRAIN Batch 13/4800 loss 10.172704 loss_att 17.169798 loss_ctc 15.171846 loss_rnnt 7.889641 hw_loss 0.407048 lr 0.00046988 rank 6
2023-02-23 03:07:14,126 DEBUG TRAIN Batch 13/4800 loss 10.625879 loss_att 11.117508 loss_ctc 11.354981 loss_rnnt 10.174739 hw_loss 0.479252 lr 0.00046984 rank 7
2023-02-23 03:07:14,127 DEBUG TRAIN Batch 13/4800 loss 10.987935 loss_att 14.321422 loss_ctc 12.844250 loss_rnnt 9.863323 hw_loss 0.394512 lr 0.00046983 rank 3
2023-02-23 03:07:14,129 DEBUG TRAIN Batch 13/4800 loss 15.131145 loss_att 18.475506 loss_ctc 23.317125 loss_rnnt 13.168949 hw_loss 0.378484 lr 0.00046992 rank 1
2023-02-23 03:07:14,130 DEBUG TRAIN Batch 13/4800 loss 7.020946 loss_att 10.523361 loss_ctc 10.495916 loss_rnnt 5.662336 hw_loss 0.365245 lr 0.00046987 rank 5
2023-02-23 03:07:14,129 DEBUG TRAIN Batch 13/4800 loss 22.870653 loss_att 27.488880 loss_ctc 32.789398 loss_rnnt 20.443129 hw_loss 0.340087 lr 0.00046983 rank 4
2023-02-23 03:07:14,185 DEBUG TRAIN Batch 13/4800 loss 11.516018 loss_att 15.424910 loss_ctc 11.709809 loss_rnnt 10.508726 hw_loss 0.374390 lr 0.00046983 rank 2
2023-02-23 03:08:28,372 DEBUG TRAIN Batch 13/4900 loss 13.298368 loss_att 14.192879 loss_ctc 18.540911 loss_rnnt 12.159422 hw_loss 0.489450 lr 0.00046963 rank 7
2023-02-23 03:08:28,379 DEBUG TRAIN Batch 13/4900 loss 9.085141 loss_att 12.470671 loss_ctc 12.437929 loss_rnnt 7.743857 hw_loss 0.407135 lr 0.00046962 rank 3
2023-02-23 03:08:28,379 DEBUG TRAIN Batch 13/4900 loss 11.443950 loss_att 15.426922 loss_ctc 14.419329 loss_rnnt 10.043211 hw_loss 0.388924 lr 0.00046963 rank 4
2023-02-23 03:08:28,381 DEBUG TRAIN Batch 13/4900 loss 8.681208 loss_att 13.329021 loss_ctc 13.817679 loss_rnnt 6.781747 hw_loss 0.534438 lr 0.00046971 rank 0
2023-02-23 03:08:28,383 DEBUG TRAIN Batch 13/4900 loss 12.804755 loss_att 20.588495 loss_ctc 18.140108 loss_rnnt 10.309784 hw_loss 0.425332 lr 0.00046967 rank 6
2023-02-23 03:08:28,385 DEBUG TRAIN Batch 13/4900 loss 14.524256 loss_att 16.820702 loss_ctc 20.191963 loss_rnnt 13.078625 hw_loss 0.432462 lr 0.00046963 rank 2
2023-02-23 03:08:28,386 DEBUG TRAIN Batch 13/4900 loss 12.851131 loss_att 15.408194 loss_ctc 18.000355 loss_rnnt 11.378390 hw_loss 0.515183 lr 0.00046971 rank 1
2023-02-23 03:08:28,402 DEBUG TRAIN Batch 13/4900 loss 20.285177 loss_att 24.302019 loss_ctc 24.614677 loss_rnnt 18.719358 hw_loss 0.347221 lr 0.00046966 rank 5
2023-02-23 03:09:44,456 DEBUG TRAIN Batch 13/5000 loss 11.782434 loss_att 14.901360 loss_ctc 15.535001 loss_rnnt 10.398153 hw_loss 0.487791 lr 0.00046942 rank 7
2023-02-23 03:09:44,459 DEBUG TRAIN Batch 13/5000 loss 8.773509 loss_att 9.962686 loss_ctc 11.023153 loss_rnnt 7.969598 hw_loss 0.498979 lr 0.00046945 rank 5
2023-02-23 03:09:44,462 DEBUG TRAIN Batch 13/5000 loss 13.229078 loss_att 12.384148 loss_ctc 15.701395 loss_rnnt 12.853370 hw_loss 0.403224 lr 0.00046942 rank 4
2023-02-23 03:09:44,463 DEBUG TRAIN Batch 13/5000 loss 8.996384 loss_att 13.082589 loss_ctc 12.429685 loss_rnnt 7.457715 hw_loss 0.494352 lr 0.00046951 rank 1
2023-02-23 03:09:44,464 DEBUG TRAIN Batch 13/5000 loss 12.876614 loss_att 13.481888 loss_ctc 14.217934 loss_rnnt 12.331667 hw_loss 0.459467 lr 0.00046947 rank 6
2023-02-23 03:09:44,465 DEBUG TRAIN Batch 13/5000 loss 15.463736 loss_att 14.988516 loss_ctc 16.314407 loss_rnnt 15.238381 hw_loss 0.388075 lr 0.00046942 rank 3
2023-02-23 03:09:44,491 DEBUG TRAIN Batch 13/5000 loss 6.982164 loss_att 12.173182 loss_ctc 7.038777 loss_rnnt 5.734447 hw_loss 0.378685 lr 0.00046951 rank 0
2023-02-23 03:09:44,508 DEBUG TRAIN Batch 13/5000 loss 12.100535 loss_att 15.173290 loss_ctc 19.503662 loss_rnnt 10.229086 hw_loss 0.505903 lr 0.00046942 rank 2
2023-02-23 03:10:57,869 DEBUG TRAIN Batch 13/5100 loss 15.985323 loss_att 21.470612 loss_ctc 21.796017 loss_rnnt 13.867737 hw_loss 0.460817 lr 0.00046925 rank 5
2023-02-23 03:10:57,870 DEBUG TRAIN Batch 13/5100 loss 9.389724 loss_att 14.983276 loss_ctc 14.415076 loss_rnnt 7.399314 hw_loss 0.378098 lr 0.00046922 rank 7
2023-02-23 03:10:57,873 DEBUG TRAIN Batch 13/5100 loss 6.998924 loss_att 10.016308 loss_ctc 8.833673 loss_rnnt 5.947544 hw_loss 0.381130 lr 0.00046921 rank 4
2023-02-23 03:10:57,873 DEBUG TRAIN Batch 13/5100 loss 18.354782 loss_att 17.662046 loss_ctc 21.012960 loss_rnnt 17.905815 hw_loss 0.437043 lr 0.00046921 rank 3
2023-02-23 03:10:57,876 DEBUG TRAIN Batch 13/5100 loss 20.824169 loss_att 20.904791 loss_ctc 28.198149 loss_rnnt 19.594610 hw_loss 0.431695 lr 0.00046930 rank 0
2023-02-23 03:10:57,877 DEBUG TRAIN Batch 13/5100 loss 17.232121 loss_att 20.253080 loss_ctc 21.744383 loss_rnnt 15.857685 hw_loss 0.316139 lr 0.00046930 rank 1
2023-02-23 03:10:57,880 DEBUG TRAIN Batch 13/5100 loss 20.685982 loss_att 22.814699 loss_ctc 26.316420 loss_rnnt 19.311066 hw_loss 0.372091 lr 0.00046921 rank 2
2023-02-23 03:10:57,885 DEBUG TRAIN Batch 13/5100 loss 11.176832 loss_att 10.962234 loss_ctc 12.732594 loss_rnnt 10.683250 hw_loss 0.617002 lr 0.00046926 rank 6
2023-02-23 03:12:11,187 DEBUG TRAIN Batch 13/5200 loss 19.888544 loss_att 19.653557 loss_ctc 26.174885 loss_rnnt 18.924072 hw_loss 0.324917 lr 0.00046909 rank 1
2023-02-23 03:12:11,201 DEBUG TRAIN Batch 13/5200 loss 3.829824 loss_att 8.486769 loss_ctc 3.151138 loss_rnnt 2.797648 hw_loss 0.358647 lr 0.00046901 rank 7
2023-02-23 03:12:11,208 DEBUG TRAIN Batch 13/5200 loss 35.624973 loss_att 40.283009 loss_ctc 52.711739 loss_rnnt 32.194359 hw_loss 0.413955 lr 0.00046909 rank 0
2023-02-23 03:12:11,208 DEBUG TRAIN Batch 13/5200 loss 11.167034 loss_att 19.446795 loss_ctc 13.356491 loss_rnnt 9.011692 hw_loss 0.388989 lr 0.00046900 rank 3
2023-02-23 03:12:11,211 DEBUG TRAIN Batch 13/5200 loss 16.437868 loss_att 18.046886 loss_ctc 23.197052 loss_rnnt 15.008540 hw_loss 0.386816 lr 0.00046901 rank 4
2023-02-23 03:12:11,215 DEBUG TRAIN Batch 13/5200 loss 12.961618 loss_att 17.755398 loss_ctc 15.763100 loss_rnnt 11.380032 hw_loss 0.467436 lr 0.00046901 rank 2
2023-02-23 03:12:11,221 DEBUG TRAIN Batch 13/5200 loss 11.900584 loss_att 12.548641 loss_ctc 13.918864 loss_rnnt 11.270709 hw_loss 0.433423 lr 0.00046904 rank 5
2023-02-23 03:12:11,256 DEBUG TRAIN Batch 13/5200 loss 11.337618 loss_att 13.866936 loss_ctc 13.508616 loss_rnnt 10.342247 hw_loss 0.375075 lr 0.00046905 rank 6
2023-02-23 03:13:26,032 DEBUG TRAIN Batch 13/5300 loss 13.563972 loss_att 17.868139 loss_ctc 17.427053 loss_rnnt 11.945715 hw_loss 0.454400 lr 0.00046880 rank 3
2023-02-23 03:13:26,038 DEBUG TRAIN Batch 13/5300 loss 23.102915 loss_att 27.978035 loss_ctc 31.505127 loss_rnnt 20.745159 hw_loss 0.492072 lr 0.00046883 rank 5
2023-02-23 03:13:26,042 DEBUG TRAIN Batch 13/5300 loss 21.383295 loss_att 24.303642 loss_ctc 29.306877 loss_rnnt 19.538858 hw_loss 0.382296 lr 0.00046881 rank 7
2023-02-23 03:13:26,046 DEBUG TRAIN Batch 13/5300 loss 19.947151 loss_att 25.304859 loss_ctc 25.171522 loss_rnnt 17.949480 hw_loss 0.430398 lr 0.00046889 rank 0
2023-02-23 03:13:26,046 DEBUG TRAIN Batch 13/5300 loss 9.100042 loss_att 10.776378 loss_ctc 13.704219 loss_rnnt 7.975165 hw_loss 0.329476 lr 0.00046880 rank 2
2023-02-23 03:13:26,046 DEBUG TRAIN Batch 13/5300 loss 25.541439 loss_att 31.142338 loss_ctc 32.351326 loss_rnnt 23.298773 hw_loss 0.402192 lr 0.00046880 rank 4
2023-02-23 03:13:26,050 DEBUG TRAIN Batch 13/5300 loss 18.689463 loss_att 20.798094 loss_ctc 24.351215 loss_rnnt 17.322071 hw_loss 0.357687 lr 0.00046889 rank 1
2023-02-23 03:13:26,090 DEBUG TRAIN Batch 13/5300 loss 11.191838 loss_att 12.519058 loss_ctc 12.664644 loss_rnnt 10.503933 hw_loss 0.423914 lr 0.00046885 rank 6
2023-02-23 03:14:40,394 DEBUG TRAIN Batch 13/5400 loss 20.638830 loss_att 21.265070 loss_ctc 28.656059 loss_rnnt 19.228260 hw_loss 0.405675 lr 0.00046860 rank 7
2023-02-23 03:14:40,397 DEBUG TRAIN Batch 13/5400 loss 10.925155 loss_att 19.366745 loss_ctc 12.795406 loss_rnnt 8.798156 hw_loss 0.354965 lr 0.00046859 rank 4
2023-02-23 03:14:40,398 DEBUG TRAIN Batch 13/5400 loss 14.421248 loss_att 15.974609 loss_ctc 16.333197 loss_rnnt 13.640746 hw_loss 0.402943 lr 0.00046859 rank 3
2023-02-23 03:14:40,403 DEBUG TRAIN Batch 13/5400 loss 22.354053 loss_att 24.246778 loss_ctc 29.820261 loss_rnnt 20.720634 hw_loss 0.486337 lr 0.00046868 rank 0
2023-02-23 03:14:40,404 DEBUG TRAIN Batch 13/5400 loss 7.182731 loss_att 10.733298 loss_ctc 8.748947 loss_rnnt 6.070840 hw_loss 0.361778 lr 0.00046863 rank 5
2023-02-23 03:14:40,433 DEBUG TRAIN Batch 13/5400 loss 12.639194 loss_att 16.629051 loss_ctc 13.742304 loss_rnnt 11.473004 hw_loss 0.414630 lr 0.00046864 rank 6
2023-02-23 03:14:40,437 DEBUG TRAIN Batch 13/5400 loss 5.729332 loss_att 7.480271 loss_ctc 6.779180 loss_rnnt 5.047363 hw_loss 0.359628 lr 0.00046859 rank 2
2023-02-23 03:14:40,451 DEBUG TRAIN Batch 13/5400 loss 7.294708 loss_att 9.796784 loss_ctc 7.954308 loss_rnnt 6.484203 hw_loss 0.416518 lr 0.00046868 rank 1
2023-02-23 03:15:53,670 DEBUG TRAIN Batch 13/5500 loss 11.301808 loss_att 16.339243 loss_ctc 16.255165 loss_rnnt 9.429132 hw_loss 0.383889 lr 0.00046839 rank 3
2023-02-23 03:15:53,671 DEBUG TRAIN Batch 13/5500 loss 14.656327 loss_att 15.670764 loss_ctc 16.120943 loss_rnnt 14.001608 hw_loss 0.481034 lr 0.00046839 rank 7
2023-02-23 03:15:53,674 DEBUG TRAIN Batch 13/5500 loss 10.208105 loss_att 11.210025 loss_ctc 11.389612 loss_rnnt 9.608138 hw_loss 0.453843 lr 0.00046839 rank 4
2023-02-23 03:15:53,675 DEBUG TRAIN Batch 13/5500 loss 10.186095 loss_att 14.654869 loss_ctc 13.369606 loss_rnnt 8.682348 hw_loss 0.347858 lr 0.00046842 rank 5
2023-02-23 03:15:53,676 DEBUG TRAIN Batch 13/5500 loss 13.209412 loss_att 16.398643 loss_ctc 15.714294 loss_rnnt 12.039234 hw_loss 0.371901 lr 0.00046847 rank 1
2023-02-23 03:15:53,678 DEBUG TRAIN Batch 13/5500 loss 15.473766 loss_att 17.899755 loss_ctc 18.686819 loss_rnnt 14.305550 hw_loss 0.477398 lr 0.00046848 rank 0
2023-02-23 03:15:53,682 DEBUG TRAIN Batch 13/5500 loss 15.827300 loss_att 20.141722 loss_ctc 22.152393 loss_rnnt 13.926802 hw_loss 0.364252 lr 0.00046843 rank 6
2023-02-23 03:15:53,729 DEBUG TRAIN Batch 13/5500 loss 5.799767 loss_att 9.681917 loss_ctc 8.200660 loss_rnnt 4.494780 hw_loss 0.390822 lr 0.00046839 rank 2
2023-02-23 03:17:08,056 DEBUG TRAIN Batch 13/5600 loss 16.676773 loss_att 22.119137 loss_ctc 26.190956 loss_rnnt 14.116218 hw_loss 0.381608 lr 0.00046819 rank 7
2023-02-23 03:17:08,058 DEBUG TRAIN Batch 13/5600 loss 19.172886 loss_att 24.558369 loss_ctc 21.669601 loss_rnnt 17.572662 hw_loss 0.356682 lr 0.00046818 rank 3
2023-02-23 03:17:08,061 DEBUG TRAIN Batch 13/5600 loss 17.461874 loss_att 19.609617 loss_ctc 21.489861 loss_rnnt 16.242634 hw_loss 0.473675 lr 0.00046818 rank 4
2023-02-23 03:17:08,060 DEBUG TRAIN Batch 13/5600 loss 12.854547 loss_att 15.748125 loss_ctc 18.957914 loss_rnnt 11.248108 hw_loss 0.401140 lr 0.00046827 rank 1
2023-02-23 03:17:08,062 DEBUG TRAIN Batch 13/5600 loss 6.744509 loss_att 9.881038 loss_ctc 8.992377 loss_rnnt 5.621112 hw_loss 0.368203 lr 0.00046823 rank 6
2023-02-23 03:17:08,063 DEBUG TRAIN Batch 13/5600 loss 11.932307 loss_att 14.359123 loss_ctc 12.920141 loss_rnnt 11.107914 hw_loss 0.388722 lr 0.00046822 rank 5
2023-02-23 03:17:08,064 DEBUG TRAIN Batch 13/5600 loss 15.660633 loss_att 16.423130 loss_ctc 18.429245 loss_rnnt 14.781847 hw_loss 0.669635 lr 0.00046827 rank 0
2023-02-23 03:17:08,069 DEBUG TRAIN Batch 13/5600 loss 14.865476 loss_att 18.738510 loss_ctc 17.414833 loss_rnnt 13.491196 hw_loss 0.487047 lr 0.00046818 rank 2
2023-02-23 03:18:24,569 DEBUG TRAIN Batch 13/5700 loss 16.908386 loss_att 19.589388 loss_ctc 23.341177 loss_rnnt 15.285822 hw_loss 0.428733 lr 0.00046798 rank 4
2023-02-23 03:18:24,574 DEBUG TRAIN Batch 13/5700 loss 17.614044 loss_att 20.897402 loss_ctc 18.302343 loss_rnnt 16.614983 hw_loss 0.469904 lr 0.00046797 rank 3
2023-02-23 03:18:24,575 DEBUG TRAIN Batch 13/5700 loss 10.596642 loss_att 10.233303 loss_ctc 12.710696 loss_rnnt 10.110754 hw_loss 0.518774 lr 0.00046798 rank 7
2023-02-23 03:18:24,576 DEBUG TRAIN Batch 13/5700 loss 8.357507 loss_att 10.779181 loss_ctc 12.084025 loss_rnnt 7.145310 hw_loss 0.433110 lr 0.00046806 rank 1
2023-02-23 03:18:24,577 DEBUG TRAIN Batch 13/5700 loss 17.153736 loss_att 22.755484 loss_ctc 25.184872 loss_rnnt 14.737232 hw_loss 0.422506 lr 0.00046802 rank 6
2023-02-23 03:18:24,583 DEBUG TRAIN Batch 13/5700 loss 8.706431 loss_att 9.261993 loss_ctc 11.542341 loss_rnnt 7.926309 hw_loss 0.545415 lr 0.00046801 rank 5
2023-02-23 03:18:24,586 DEBUG TRAIN Batch 13/5700 loss 4.435960 loss_att 8.918227 loss_ctc 4.595500 loss_rnnt 3.256733 hw_loss 0.490318 lr 0.00046798 rank 2
2023-02-23 03:18:24,654 DEBUG TRAIN Batch 13/5700 loss 12.490633 loss_att 15.401876 loss_ctc 19.234272 loss_rnnt 10.777679 hw_loss 0.434160 lr 0.00046807 rank 0
2023-02-23 03:19:38,013 DEBUG TRAIN Batch 13/5800 loss 11.943667 loss_att 13.750160 loss_ctc 17.171316 loss_rnnt 10.649114 hw_loss 0.442942 lr 0.00046778 rank 7
2023-02-23 03:19:38,017 DEBUG TRAIN Batch 13/5800 loss 11.963305 loss_att 18.718430 loss_ctc 14.178104 loss_rnnt 10.083818 hw_loss 0.437166 lr 0.00046777 rank 3
2023-02-23 03:19:38,019 DEBUG TRAIN Batch 13/5800 loss 12.516770 loss_att 11.378717 loss_ctc 9.640254 loss_rnnt 12.916002 hw_loss 0.397336 lr 0.00046786 rank 0
2023-02-23 03:19:38,020 DEBUG TRAIN Batch 13/5800 loss 8.001829 loss_att 10.347165 loss_ctc 9.460888 loss_rnnt 7.089700 hw_loss 0.465976 lr 0.00046777 rank 4
2023-02-23 03:19:38,024 DEBUG TRAIN Batch 13/5800 loss 9.696779 loss_att 10.726755 loss_ctc 11.683701 loss_rnnt 8.842795 hw_loss 0.718249 lr 0.00046782 rank 6
2023-02-23 03:19:38,027 DEBUG TRAIN Batch 13/5800 loss 15.944673 loss_att 21.308170 loss_ctc 15.262786 loss_rnnt 14.767185 hw_loss 0.366947 lr 0.00046786 rank 1
2023-02-23 03:19:38,028 DEBUG TRAIN Batch 13/5800 loss 11.808989 loss_att 12.569670 loss_ctc 15.216614 loss_rnnt 10.914493 hw_loss 0.540017 lr 0.00046777 rank 2
2023-02-23 03:19:38,069 DEBUG TRAIN Batch 13/5800 loss 20.628454 loss_att 22.081657 loss_ctc 27.335905 loss_rnnt 19.232738 hw_loss 0.395156 lr 0.00046781 rank 5
2023-02-23 03:20:51,534 DEBUG TRAIN Batch 13/5900 loss 11.509351 loss_att 16.510298 loss_ctc 15.299431 loss_rnnt 9.855877 hw_loss 0.277388 lr 0.00046757 rank 7
2023-02-23 03:20:51,536 DEBUG TRAIN Batch 13/5900 loss 12.346542 loss_att 15.369779 loss_ctc 12.621853 loss_rnnt 11.485568 hw_loss 0.411787 lr 0.00046757 rank 3
2023-02-23 03:20:51,539 DEBUG TRAIN Batch 13/5900 loss 8.703470 loss_att 11.498812 loss_ctc 7.976697 loss_rnnt 8.030693 hw_loss 0.394899 lr 0.00046761 rank 6
2023-02-23 03:20:51,541 DEBUG TRAIN Batch 13/5900 loss 12.099996 loss_att 18.267265 loss_ctc 17.454422 loss_rnnt 9.894430 hw_loss 0.484101 lr 0.00046765 rank 1
2023-02-23 03:20:51,546 DEBUG TRAIN Batch 13/5900 loss 19.163515 loss_att 21.344282 loss_ctc 18.798798 loss_rnnt 18.521702 hw_loss 0.476788 lr 0.00046757 rank 4
2023-02-23 03:20:51,546 DEBUG TRAIN Batch 13/5900 loss 15.620817 loss_att 20.009438 loss_ctc 25.947130 loss_rnnt 13.085497 hw_loss 0.526415 lr 0.00046766 rank 0
2023-02-23 03:20:51,548 DEBUG TRAIN Batch 13/5900 loss 22.387972 loss_att 25.722984 loss_ctc 30.040167 loss_rnnt 20.508617 hw_loss 0.360113 lr 0.00046757 rank 2
2023-02-23 03:20:51,549 DEBUG TRAIN Batch 13/5900 loss 19.805471 loss_att 23.328840 loss_ctc 27.843765 loss_rnnt 17.827454 hw_loss 0.377945 lr 0.00046760 rank 5
2023-02-23 03:22:06,592 DEBUG TRAIN Batch 13/6000 loss 16.542246 loss_att 21.034035 loss_ctc 25.491249 loss_rnnt 14.235221 hw_loss 0.404002 lr 0.00046745 rank 1
2023-02-23 03:22:06,598 DEBUG TRAIN Batch 13/6000 loss 11.245173 loss_att 10.903726 loss_ctc 12.605774 loss_rnnt 10.896257 hw_loss 0.442109 lr 0.00046736 rank 3
2023-02-23 03:22:06,599 DEBUG TRAIN Batch 13/6000 loss 13.214364 loss_att 17.434500 loss_ctc 14.664337 loss_rnnt 11.911487 hw_loss 0.497850 lr 0.00046737 rank 7
2023-02-23 03:22:06,602 DEBUG TRAIN Batch 13/6000 loss 19.527092 loss_att 23.987064 loss_ctc 29.520657 loss_rnnt 17.088640 hw_loss 0.401215 lr 0.00046736 rank 4
2023-02-23 03:22:06,605 DEBUG TRAIN Batch 13/6000 loss 9.696261 loss_att 11.968773 loss_ctc 11.200806 loss_rnnt 8.875116 hw_loss 0.311319 lr 0.00046740 rank 5
2023-02-23 03:22:06,606 DEBUG TRAIN Batch 13/6000 loss 22.212248 loss_att 22.940697 loss_ctc 29.583693 loss_rnnt 20.926558 hw_loss 0.294642 lr 0.00046736 rank 2
2023-02-23 03:22:06,608 DEBUG TRAIN Batch 13/6000 loss 10.215423 loss_att 12.479882 loss_ctc 13.694988 loss_rnnt 9.086133 hw_loss 0.398351 lr 0.00046745 rank 0
2023-02-23 03:22:06,616 DEBUG TRAIN Batch 13/6000 loss 19.533806 loss_att 22.536366 loss_ctc 23.540304 loss_rnnt 18.165737 hw_loss 0.437547 lr 0.00046741 rank 6
2023-02-23 03:23:20,346 DEBUG TRAIN Batch 13/6100 loss 8.819792 loss_att 11.591563 loss_ctc 10.328951 loss_rnnt 7.864373 hw_loss 0.374705 lr 0.00046716 rank 3
2023-02-23 03:23:20,347 DEBUG TRAIN Batch 13/6100 loss 7.744303 loss_att 9.831387 loss_ctc 8.701158 loss_rnnt 6.976597 hw_loss 0.417577 lr 0.00046717 rank 7
2023-02-23 03:23:20,348 DEBUG TRAIN Batch 13/6100 loss 16.300861 loss_att 20.355682 loss_ctc 26.144173 loss_rnnt 13.981702 hw_loss 0.367036 lr 0.00046724 rank 1
2023-02-23 03:23:20,350 DEBUG TRAIN Batch 13/6100 loss 14.673830 loss_att 17.115795 loss_ctc 19.186501 loss_rnnt 13.354662 hw_loss 0.429534 lr 0.00046716 rank 2
2023-02-23 03:23:20,351 DEBUG TRAIN Batch 13/6100 loss 8.992648 loss_att 10.171079 loss_ctc 9.936771 loss_rnnt 8.368697 hw_loss 0.491967 lr 0.00046721 rank 6
2023-02-23 03:23:20,353 DEBUG TRAIN Batch 13/6100 loss 7.788595 loss_att 12.025249 loss_ctc 10.426164 loss_rnnt 6.414278 hw_loss 0.328708 lr 0.00046725 rank 0
2023-02-23 03:23:20,357 DEBUG TRAIN Batch 13/6100 loss 14.228440 loss_att 17.023590 loss_ctc 18.227642 loss_rnnt 12.919734 hw_loss 0.405844 lr 0.00046716 rank 4
2023-02-23 03:23:20,397 DEBUG TRAIN Batch 13/6100 loss 8.490657 loss_att 13.625195 loss_ctc 13.849341 loss_rnnt 6.558139 hw_loss 0.358348 lr 0.00046719 rank 5
2023-02-23 03:24:33,385 DEBUG TRAIN Batch 13/6200 loss 25.834072 loss_att 25.702663 loss_ctc 27.269394 loss_rnnt 25.449100 hw_loss 0.412264 lr 0.00046696 rank 2
2023-02-23 03:24:33,386 DEBUG TRAIN Batch 13/6200 loss 7.051691 loss_att 9.773255 loss_ctc 10.565994 loss_rnnt 5.828600 hw_loss 0.394131 lr 0.00046700 rank 6
2023-02-23 03:24:33,387 DEBUG TRAIN Batch 13/6200 loss 17.275957 loss_att 21.114973 loss_ctc 20.240749 loss_rnnt 15.863586 hw_loss 0.467368 lr 0.00046696 rank 7
2023-02-23 03:24:33,390 DEBUG TRAIN Batch 13/6200 loss 12.739023 loss_att 15.055012 loss_ctc 14.947983 loss_rnnt 11.793941 hw_loss 0.351293 lr 0.00046695 rank 3
2023-02-23 03:24:33,393 DEBUG TRAIN Batch 13/6200 loss 18.349062 loss_att 19.672249 loss_ctc 22.639256 loss_rnnt 17.290415 hw_loss 0.416215 lr 0.00046699 rank 5
2023-02-23 03:24:33,394 DEBUG TRAIN Batch 13/6200 loss 13.428150 loss_att 15.986032 loss_ctc 17.953205 loss_rnnt 12.121031 hw_loss 0.360379 lr 0.00046696 rank 4
2023-02-23 03:24:33,396 DEBUG TRAIN Batch 13/6200 loss 23.300478 loss_att 23.928019 loss_ctc 29.598856 loss_rnnt 22.062174 hw_loss 0.511898 lr 0.00046704 rank 0
2023-02-23 03:24:33,397 DEBUG TRAIN Batch 13/6200 loss 6.945040 loss_att 11.253977 loss_ctc 9.237633 loss_rnnt 5.562956 hw_loss 0.402407 lr 0.00046704 rank 1
2023-02-23 03:25:46,098 DEBUG TRAIN Batch 13/6300 loss 20.972996 loss_att 22.022728 loss_ctc 28.642567 loss_rnnt 19.457825 hw_loss 0.529902 lr 0.00046676 rank 7
2023-02-23 03:25:46,100 DEBUG TRAIN Batch 13/6300 loss 16.518475 loss_att 17.623993 loss_ctc 19.725473 loss_rnnt 15.640953 hw_loss 0.429033 lr 0.00046675 rank 3
2023-02-23 03:25:46,102 DEBUG TRAIN Batch 13/6300 loss 12.505072 loss_att 12.711168 loss_ctc 14.769873 loss_rnnt 11.854842 hw_loss 0.575696 lr 0.00046675 rank 4
2023-02-23 03:25:46,102 DEBUG TRAIN Batch 13/6300 loss 15.011851 loss_att 17.824966 loss_ctc 20.154531 loss_rnnt 13.552274 hw_loss 0.396117 lr 0.00046675 rank 2
2023-02-23 03:25:46,105 DEBUG TRAIN Batch 13/6300 loss 12.363774 loss_att 12.279994 loss_ctc 13.268789 loss_rnnt 11.976378 hw_loss 0.531533 lr 0.00046679 rank 5
2023-02-23 03:25:46,108 DEBUG TRAIN Batch 13/6300 loss 9.679701 loss_att 11.903273 loss_ctc 11.617536 loss_rnnt 8.763031 hw_loss 0.400457 lr 0.00046680 rank 6
2023-02-23 03:25:46,109 DEBUG TRAIN Batch 13/6300 loss 19.404898 loss_att 19.919281 loss_ctc 22.923405 loss_rnnt 18.613913 hw_loss 0.410574 lr 0.00046684 rank 1
2023-02-23 03:25:46,122 DEBUG TRAIN Batch 13/6300 loss 14.540401 loss_att 18.069441 loss_ctc 21.264862 loss_rnnt 12.734642 hw_loss 0.381292 lr 0.00046684 rank 0
2023-02-23 03:27:03,865 DEBUG TRAIN Batch 13/6400 loss 15.050528 loss_att 13.957193 loss_ctc 15.758115 loss_rnnt 14.848004 hw_loss 0.612833 lr 0.00046655 rank 3
2023-02-23 03:27:03,869 DEBUG TRAIN Batch 13/6400 loss 11.238228 loss_att 15.014888 loss_ctc 17.604885 loss_rnnt 9.456549 hw_loss 0.332737 lr 0.00046655 rank 4
2023-02-23 03:27:03,870 DEBUG TRAIN Batch 13/6400 loss 10.386993 loss_att 12.470801 loss_ctc 14.386775 loss_rnnt 9.166591 hw_loss 0.506884 lr 0.00046655 rank 2
2023-02-23 03:27:03,870 DEBUG TRAIN Batch 13/6400 loss 14.224141 loss_att 15.060303 loss_ctc 17.762611 loss_rnnt 13.286803 hw_loss 0.559330 lr 0.00046663 rank 1
2023-02-23 03:27:03,870 DEBUG TRAIN Batch 13/6400 loss 20.604952 loss_att 24.172235 loss_ctc 25.859245 loss_rnnt 18.963139 hw_loss 0.427094 lr 0.00046655 rank 7
2023-02-23 03:27:03,873 DEBUG TRAIN Batch 13/6400 loss 22.173290 loss_att 25.602890 loss_ctc 24.557922 loss_rnnt 20.995592 hw_loss 0.325926 lr 0.00046658 rank 5
2023-02-23 03:27:03,873 DEBUG TRAIN Batch 13/6400 loss 15.383490 loss_att 22.494005 loss_ctc 23.075468 loss_rnnt 12.707582 hw_loss 0.427889 lr 0.00046664 rank 0
2023-02-23 03:27:03,873 DEBUG TRAIN Batch 13/6400 loss 11.593738 loss_att 15.097132 loss_ctc 15.425929 loss_rnnt 10.175769 hw_loss 0.386871 lr 0.00046660 rank 6
2023-02-23 03:28:16,568 DEBUG TRAIN Batch 13/6500 loss 12.372088 loss_att 15.824730 loss_ctc 13.263634 loss_rnnt 11.378353 hw_loss 0.345627 lr 0.00046643 rank 1
2023-02-23 03:28:16,569 DEBUG TRAIN Batch 13/6500 loss 7.481907 loss_att 11.344482 loss_ctc 7.922585 loss_rnnt 6.472339 hw_loss 0.334307 lr 0.00046639 rank 6
2023-02-23 03:28:16,571 DEBUG TRAIN Batch 13/6500 loss 6.577025 loss_att 8.272548 loss_ctc 7.009865 loss_rnnt 5.982482 hw_loss 0.370738 lr 0.00046635 rank 4
2023-02-23 03:28:16,571 DEBUG TRAIN Batch 13/6500 loss 10.077190 loss_att 11.704492 loss_ctc 12.441192 loss_rnnt 9.288516 hw_loss 0.277527 lr 0.00046635 rank 7
2023-02-23 03:28:16,572 DEBUG TRAIN Batch 13/6500 loss 10.470636 loss_att 12.875201 loss_ctc 9.446646 loss_rnnt 9.938343 hw_loss 0.352336 lr 0.00046634 rank 3
2023-02-23 03:28:16,575 DEBUG TRAIN Batch 13/6500 loss 10.875070 loss_att 20.029011 loss_ctc 11.326452 loss_rnnt 8.739421 hw_loss 0.458767 lr 0.00046635 rank 2
2023-02-23 03:28:16,585 DEBUG TRAIN Batch 13/6500 loss 16.007181 loss_att 19.613098 loss_ctc 20.622429 loss_rnnt 14.434025 hw_loss 0.443634 lr 0.00046638 rank 5
2023-02-23 03:28:16,584 DEBUG TRAIN Batch 13/6500 loss 8.169042 loss_att 12.146128 loss_ctc 14.312082 loss_rnnt 6.331148 hw_loss 0.418883 lr 0.00046643 rank 0
2023-02-23 03:29:29,244 DEBUG TRAIN Batch 13/6600 loss 17.499241 loss_att 21.559641 loss_ctc 24.173752 loss_rnnt 15.538763 hw_loss 0.484621 lr 0.00046614 rank 3
2023-02-23 03:29:29,246 DEBUG TRAIN Batch 13/6600 loss 9.099473 loss_att 14.296404 loss_ctc 13.056541 loss_rnnt 7.341760 hw_loss 0.357596 lr 0.00046615 rank 7
2023-02-23 03:29:29,248 DEBUG TRAIN Batch 13/6600 loss 12.854279 loss_att 14.775409 loss_ctc 21.444664 loss_rnnt 11.125774 hw_loss 0.372926 lr 0.00046614 rank 4
2023-02-23 03:29:29,249 DEBUG TRAIN Batch 13/6600 loss 5.127279 loss_att 11.322945 loss_ctc 7.194892 loss_rnnt 3.435284 hw_loss 0.332211 lr 0.00046614 rank 2
2023-02-23 03:29:29,250 DEBUG TRAIN Batch 13/6600 loss 4.511821 loss_att 8.035409 loss_ctc 5.111808 loss_rnnt 3.523972 hw_loss 0.380874 lr 0.00046619 rank 6
2023-02-23 03:29:29,252 DEBUG TRAIN Batch 13/6600 loss 19.264477 loss_att 19.621279 loss_ctc 26.571800 loss_rnnt 17.969563 hw_loss 0.467333 lr 0.00046618 rank 5
2023-02-23 03:29:29,252 DEBUG TRAIN Batch 13/6600 loss 20.571360 loss_att 23.966866 loss_ctc 26.346279 loss_rnnt 18.930801 hw_loss 0.359001 lr 0.00046623 rank 1
2023-02-23 03:29:29,306 DEBUG TRAIN Batch 13/6600 loss 12.410650 loss_att 16.417110 loss_ctc 18.808125 loss_rnnt 10.494439 hw_loss 0.491103 lr 0.00046623 rank 0
2023-02-23 03:30:42,983 DEBUG TRAIN Batch 13/6700 loss 16.537380 loss_att 19.983709 loss_ctc 19.606354 loss_rnnt 15.207060 hw_loss 0.434740 lr 0.00046599 rank 6
2023-02-23 03:30:42,990 DEBUG TRAIN Batch 13/6700 loss 17.135553 loss_att 18.432220 loss_ctc 22.604481 loss_rnnt 15.929941 hw_loss 0.407035 lr 0.00046594 rank 3
2023-02-23 03:30:42,996 DEBUG TRAIN Batch 13/6700 loss 10.829206 loss_att 13.463308 loss_ctc 15.676250 loss_rnnt 9.450870 hw_loss 0.384829 lr 0.00046594 rank 2
2023-02-23 03:30:42,996 DEBUG TRAIN Batch 13/6700 loss 8.364616 loss_att 11.144878 loss_ctc 13.843468 loss_rnnt 6.840659 hw_loss 0.445107 lr 0.00046595 rank 7
2023-02-23 03:30:43,004 DEBUG TRAIN Batch 13/6700 loss 19.094179 loss_att 20.421518 loss_ctc 22.727982 loss_rnnt 18.146929 hw_loss 0.369885 lr 0.00046603 rank 0
2023-02-23 03:30:43,005 DEBUG TRAIN Batch 13/6700 loss 5.042561 loss_att 8.664601 loss_ctc 8.079845 loss_rnnt 3.720599 hw_loss 0.361092 lr 0.00046594 rank 4
2023-02-23 03:30:43,005 DEBUG TRAIN Batch 13/6700 loss 15.475387 loss_att 18.765911 loss_ctc 18.296286 loss_rnnt 14.244375 hw_loss 0.368973 lr 0.00046603 rank 1
2023-02-23 03:30:43,023 DEBUG TRAIN Batch 13/6700 loss 11.984841 loss_att 15.053039 loss_ctc 14.369531 loss_rnnt 10.847425 hw_loss 0.385912 lr 0.00046597 rank 5
2023-02-23 03:32:00,437 DEBUG TRAIN Batch 13/6800 loss 17.619751 loss_att 21.329079 loss_ctc 23.060505 loss_rnnt 15.930009 hw_loss 0.417082 lr 0.00046577 rank 5
2023-02-23 03:32:00,438 DEBUG TRAIN Batch 13/6800 loss 27.285931 loss_att 18.954025 loss_ctc 19.433086 loss_rnnt 29.809868 hw_loss 0.355294 lr 0.00046574 rank 7
2023-02-23 03:32:00,438 DEBUG TRAIN Batch 13/6800 loss 20.171604 loss_att 22.307289 loss_ctc 29.944925 loss_rnnt 18.220089 hw_loss 0.414875 lr 0.00046574 rank 2
2023-02-23 03:32:00,439 DEBUG TRAIN Batch 13/6800 loss 15.934098 loss_att 21.755564 loss_ctc 16.720524 loss_rnnt 14.454508 hw_loss 0.394576 lr 0.00046582 rank 1
2023-02-23 03:32:00,440 DEBUG TRAIN Batch 13/6800 loss 12.351167 loss_att 14.212912 loss_ctc 16.129448 loss_rnnt 11.231705 hw_loss 0.456266 lr 0.00046574 rank 4
2023-02-23 03:32:00,441 DEBUG TRAIN Batch 13/6800 loss 9.122165 loss_att 12.308794 loss_ctc 11.074009 loss_rnnt 7.996494 hw_loss 0.427684 lr 0.00046574 rank 3
2023-02-23 03:32:00,443 DEBUG TRAIN Batch 13/6800 loss 19.732157 loss_att 22.020683 loss_ctc 23.022577 loss_rnnt 18.622429 hw_loss 0.399939 lr 0.00046578 rank 6
2023-02-23 03:32:00,446 DEBUG TRAIN Batch 13/6800 loss 10.521960 loss_att 14.660450 loss_ctc 15.031538 loss_rnnt 8.867432 hw_loss 0.422915 lr 0.00046583 rank 0
2023-02-23 03:33:13,628 DEBUG TRAIN Batch 13/6900 loss 17.967726 loss_att 17.806255 loss_ctc 24.057961 loss_rnnt 16.973057 hw_loss 0.402997 lr 0.00046554 rank 7
2023-02-23 03:33:13,631 DEBUG TRAIN Batch 13/6900 loss 16.842632 loss_att 20.442009 loss_ctc 24.099556 loss_rnnt 14.935528 hw_loss 0.411823 lr 0.00046554 rank 2
2023-02-23 03:33:13,631 DEBUG TRAIN Batch 13/6900 loss 10.488751 loss_att 13.109039 loss_ctc 11.214602 loss_rnnt 9.653362 hw_loss 0.402284 lr 0.00046553 rank 3
2023-02-23 03:33:13,633 DEBUG TRAIN Batch 13/6900 loss 11.509634 loss_att 14.337643 loss_ctc 16.116573 loss_rnnt 10.090710 hw_loss 0.448246 lr 0.00046554 rank 4
2023-02-23 03:33:13,635 DEBUG TRAIN Batch 13/6900 loss 32.278927 loss_att 40.003193 loss_ctc 41.270153 loss_rnnt 29.333130 hw_loss 0.378964 lr 0.00046562 rank 0
2023-02-23 03:33:13,636 DEBUG TRAIN Batch 13/6900 loss 6.407415 loss_att 10.011913 loss_ctc 7.990252 loss_rnnt 5.270201 hw_loss 0.384879 lr 0.00046558 rank 6
2023-02-23 03:33:13,636 DEBUG TRAIN Batch 13/6900 loss 15.241592 loss_att 16.892416 loss_ctc 20.835831 loss_rnnt 13.886983 hw_loss 0.522276 lr 0.00046557 rank 5
2023-02-23 03:33:13,637 DEBUG TRAIN Batch 13/6900 loss 13.636841 loss_att 16.694941 loss_ctc 15.365414 loss_rnnt 12.612591 hw_loss 0.341540 lr 0.00046562 rank 1
2023-02-23 03:34:26,318 DEBUG TRAIN Batch 13/7000 loss 5.029009 loss_att 6.907691 loss_ctc 6.884466 loss_rnnt 4.115068 hw_loss 0.545269 lr 0.00046533 rank 3
2023-02-23 03:34:26,319 DEBUG TRAIN Batch 13/7000 loss 9.050727 loss_att 9.359261 loss_ctc 9.478316 loss_rnnt 8.619074 hw_loss 0.586751 lr 0.00046534 rank 7
2023-02-23 03:34:26,320 DEBUG TRAIN Batch 13/7000 loss 11.890514 loss_att 15.122522 loss_ctc 16.454350 loss_rnnt 10.373422 hw_loss 0.491586 lr 0.00046533 rank 2
2023-02-23 03:34:26,321 DEBUG TRAIN Batch 13/7000 loss 13.975918 loss_att 13.471052 loss_ctc 12.468871 loss_rnnt 14.068779 hw_loss 0.391970 lr 0.00046533 rank 4
2023-02-23 03:34:26,321 DEBUG TRAIN Batch 13/7000 loss 10.766674 loss_att 20.607489 loss_ctc 13.945173 loss_rnnt 8.179066 hw_loss 0.366836 lr 0.00046537 rank 5
2023-02-23 03:34:26,326 DEBUG TRAIN Batch 13/7000 loss 17.566093 loss_att 18.234423 loss_ctc 27.367832 loss_rnnt 15.917232 hw_loss 0.390559 lr 0.00046542 rank 0
2023-02-23 03:34:26,330 DEBUG TRAIN Batch 13/7000 loss 12.615958 loss_att 12.602388 loss_ctc 16.407774 loss_rnnt 11.857723 hw_loss 0.478826 lr 0.00046538 rank 6
2023-02-23 03:34:26,346 DEBUG TRAIN Batch 13/7000 loss 8.347939 loss_att 10.811333 loss_ctc 10.783849 loss_rnnt 7.295107 hw_loss 0.441307 lr 0.00046542 rank 1
2023-02-23 03:35:41,322 DEBUG TRAIN Batch 13/7100 loss 29.750879 loss_att 35.350178 loss_ctc 38.617386 loss_rnnt 27.260914 hw_loss 0.352320 lr 0.00046514 rank 7
2023-02-23 03:35:41,324 DEBUG TRAIN Batch 13/7100 loss 8.093746 loss_att 11.937080 loss_ctc 10.851475 loss_rnnt 6.737857 hw_loss 0.411611 lr 0.00046513 rank 4
2023-02-23 03:35:41,326 DEBUG TRAIN Batch 13/7100 loss 14.520447 loss_att 15.212412 loss_ctc 17.449402 loss_rnnt 13.634945 hw_loss 0.668593 lr 0.00046513 rank 2
2023-02-23 03:35:41,327 DEBUG TRAIN Batch 13/7100 loss 8.032955 loss_att 15.790878 loss_ctc 10.961540 loss_rnnt 5.887294 hw_loss 0.381748 lr 0.00046522 rank 0
2023-02-23 03:35:41,328 DEBUG TRAIN Batch 13/7100 loss 17.122108 loss_att 21.807163 loss_ctc 18.318768 loss_rnnt 15.773524 hw_loss 0.472537 lr 0.00046513 rank 3
2023-02-23 03:35:41,328 DEBUG TRAIN Batch 13/7100 loss 9.425759 loss_att 11.892426 loss_ctc 16.443413 loss_rnnt 7.763946 hw_loss 0.436487 lr 0.00046517 rank 5
2023-02-23 03:35:41,331 DEBUG TRAIN Batch 13/7100 loss 12.493340 loss_att 14.290162 loss_ctc 16.213596 loss_rnnt 11.286229 hw_loss 0.659461 lr 0.00046518 rank 6
2023-02-23 03:35:41,370 DEBUG TRAIN Batch 13/7100 loss 7.769494 loss_att 14.922235 loss_ctc 8.919125 loss_rnnt 5.982852 hw_loss 0.380268 lr 0.00046522 rank 1
2023-02-23 03:36:53,908 DEBUG TRAIN Batch 13/7200 loss 14.650532 loss_att 17.977768 loss_ctc 13.964039 loss_rnnt 13.887037 hw_loss 0.355464 lr 0.00046493 rank 2
2023-02-23 03:36:53,911 DEBUG TRAIN Batch 13/7200 loss 12.171948 loss_att 15.974810 loss_ctc 14.961452 loss_rnnt 10.882746 hw_loss 0.293805 lr 0.00046494 rank 7
2023-02-23 03:36:53,913 DEBUG TRAIN Batch 13/7200 loss 14.953600 loss_att 20.758047 loss_ctc 15.402046 loss_rnnt 13.523367 hw_loss 0.392907 lr 0.00046493 rank 4
2023-02-23 03:36:53,914 DEBUG TRAIN Batch 13/7200 loss 26.465496 loss_att 30.175758 loss_ctc 35.837727 loss_rnnt 24.267385 hw_loss 0.387051 lr 0.00046497 rank 5
2023-02-23 03:36:53,914 DEBUG TRAIN Batch 13/7200 loss 10.775200 loss_att 13.460958 loss_ctc 15.399862 loss_rnnt 9.352415 hw_loss 0.504396 lr 0.00046493 rank 3
2023-02-23 03:36:53,918 DEBUG TRAIN Batch 13/7200 loss 6.621013 loss_att 10.887576 loss_ctc 8.833371 loss_rnnt 5.239570 hw_loss 0.437156 lr 0.00046498 rank 6
2023-02-23 03:36:53,921 DEBUG TRAIN Batch 13/7200 loss 18.469999 loss_att 23.341793 loss_ctc 24.737278 loss_rnnt 16.438025 hw_loss 0.416213 lr 0.00046502 rank 0
2023-02-23 03:36:53,963 DEBUG TRAIN Batch 13/7200 loss 8.667612 loss_att 9.886241 loss_ctc 9.705122 loss_rnnt 8.040564 hw_loss 0.459350 lr 0.00046502 rank 1
2023-02-23 03:38:06,030 DEBUG TRAIN Batch 13/7300 loss 18.024384 loss_att 22.461315 loss_ctc 23.317921 loss_rnnt 16.264290 hw_loss 0.312944 lr 0.00046482 rank 1
2023-02-23 03:38:06,044 DEBUG TRAIN Batch 13/7300 loss 18.695782 loss_att 23.006882 loss_ctc 20.891033 loss_rnnt 17.380911 hw_loss 0.299905 lr 0.00046474 rank 7
2023-02-23 03:38:06,044 DEBUG TRAIN Batch 13/7300 loss 9.614571 loss_att 15.574857 loss_ctc 17.008659 loss_rnnt 7.211050 hw_loss 0.422971 lr 0.00046473 rank 3
2023-02-23 03:38:06,047 DEBUG TRAIN Batch 13/7300 loss 8.163419 loss_att 9.762169 loss_ctc 11.345495 loss_rnnt 7.232555 hw_loss 0.350317 lr 0.00046477 rank 5
2023-02-23 03:38:06,049 DEBUG TRAIN Batch 13/7300 loss 13.309650 loss_att 18.361835 loss_ctc 20.616688 loss_rnnt 11.159521 hw_loss 0.310165 lr 0.00046473 rank 4
2023-02-23 03:38:06,049 DEBUG TRAIN Batch 13/7300 loss 14.315312 loss_att 17.764666 loss_ctc 19.564804 loss_rnnt 12.706768 hw_loss 0.410139 lr 0.00046473 rank 2
2023-02-23 03:38:06,049 DEBUG TRAIN Batch 13/7300 loss 14.090177 loss_att 16.346956 loss_ctc 15.703909 loss_rnnt 13.262729 hw_loss 0.301741 lr 0.00046478 rank 6
2023-02-23 03:38:06,094 DEBUG TRAIN Batch 13/7300 loss 11.524602 loss_att 14.411122 loss_ctc 14.558937 loss_rnnt 10.324648 hw_loss 0.408883 lr 0.00046482 rank 0
2023-02-23 03:39:19,666 DEBUG TRAIN Batch 13/7400 loss 11.492880 loss_att 14.909565 loss_ctc 13.928781 loss_rnnt 10.263536 hw_loss 0.414787 lr 0.00046454 rank 7
2023-02-23 03:39:19,670 DEBUG TRAIN Batch 13/7400 loss 33.399685 loss_att 35.943169 loss_ctc 40.920788 loss_rnnt 31.687792 hw_loss 0.375717 lr 0.00046453 rank 4
2023-02-23 03:39:19,676 DEBUG TRAIN Batch 13/7400 loss 21.853590 loss_att 26.730358 loss_ctc 28.938848 loss_rnnt 19.678555 hw_loss 0.478088 lr 0.00046453 rank 2
2023-02-23 03:39:19,679 DEBUG TRAIN Batch 13/7400 loss 21.826380 loss_att 24.027790 loss_ctc 29.376175 loss_rnnt 20.196918 hw_loss 0.342262 lr 0.00046453 rank 3
2023-02-23 03:39:19,680 DEBUG TRAIN Batch 13/7400 loss 17.170565 loss_att 17.016609 loss_ctc 23.180515 loss_rnnt 16.178820 hw_loss 0.414768 lr 0.00046458 rank 6
2023-02-23 03:39:19,683 DEBUG TRAIN Batch 13/7400 loss 11.653396 loss_att 14.926552 loss_ctc 13.632307 loss_rnnt 10.536453 hw_loss 0.372104 lr 0.00046461 rank 1
2023-02-23 03:39:19,689 DEBUG TRAIN Batch 13/7400 loss 9.224483 loss_att 10.950064 loss_ctc 12.015258 loss_rnnt 8.252196 hw_loss 0.478254 lr 0.00046462 rank 0
2023-02-23 03:39:19,730 DEBUG TRAIN Batch 13/7400 loss 16.002947 loss_att 16.909306 loss_ctc 19.833296 loss_rnnt 15.075820 hw_loss 0.440891 lr 0.00046456 rank 5
2023-02-23 03:40:34,194 DEBUG TRAIN Batch 13/7500 loss 15.696690 loss_att 16.406681 loss_ctc 20.582886 loss_rnnt 14.673738 hw_loss 0.430237 lr 0.00046434 rank 7
2023-02-23 03:40:34,198 DEBUG TRAIN Batch 13/7500 loss 11.183661 loss_att 12.474591 loss_ctc 13.922512 loss_rnnt 10.350231 hw_loss 0.393869 lr 0.00046433 rank 4
2023-02-23 03:40:34,198 DEBUG TRAIN Batch 13/7500 loss 7.985178 loss_att 13.269503 loss_ctc 8.908541 loss_rnnt 6.507521 hw_loss 0.558145 lr 0.00046433 rank 3
2023-02-23 03:40:34,199 DEBUG TRAIN Batch 13/7500 loss 22.397671 loss_att 24.647995 loss_ctc 29.046129 loss_rnnt 20.864853 hw_loss 0.368045 lr 0.00046441 rank 1
2023-02-23 03:40:34,202 DEBUG TRAIN Batch 13/7500 loss 24.582800 loss_att 28.240204 loss_ctc 29.790682 loss_rnnt 22.911766 hw_loss 0.459691 lr 0.00046436 rank 5
2023-02-23 03:40:34,203 DEBUG TRAIN Batch 13/7500 loss 7.578570 loss_att 13.370578 loss_ctc 11.161609 loss_rnnt 5.744283 hw_loss 0.371525 lr 0.00046433 rank 2
2023-02-23 03:40:34,208 DEBUG TRAIN Batch 13/7500 loss 9.248057 loss_att 10.030916 loss_ctc 11.906147 loss_rnnt 8.444643 hw_loss 0.548307 lr 0.00046442 rank 0
2023-02-23 03:40:34,210 DEBUG TRAIN Batch 13/7500 loss 9.651634 loss_att 13.094459 loss_ctc 10.646461 loss_rnnt 8.573769 hw_loss 0.481230 lr 0.00046438 rank 6
2023-02-23 03:41:46,374 DEBUG TRAIN Batch 13/7600 loss 15.185243 loss_att 16.116032 loss_ctc 18.183769 loss_rnnt 14.343341 hw_loss 0.479890 lr 0.00046414 rank 7
2023-02-23 03:41:46,377 DEBUG TRAIN Batch 13/7600 loss 16.558170 loss_att 16.973438 loss_ctc 21.347824 loss_rnnt 15.557877 hw_loss 0.522413 lr 0.00046413 rank 4
2023-02-23 03:41:46,377 DEBUG TRAIN Batch 13/7600 loss 7.124662 loss_att 10.336168 loss_ctc 9.152056 loss_rnnt 5.907218 hw_loss 0.571546 lr 0.00046413 rank 3
2023-02-23 03:41:46,381 DEBUG TRAIN Batch 13/7600 loss 7.166441 loss_att 9.715420 loss_ctc 10.269801 loss_rnnt 6.010159 hw_loss 0.436322 lr 0.00046421 rank 1
2023-02-23 03:41:46,386 DEBUG TRAIN Batch 13/7600 loss 12.557855 loss_att 13.206236 loss_ctc 16.810013 loss_rnnt 11.626225 hw_loss 0.440621 lr 0.00046418 rank 6
2023-02-23 03:41:46,387 DEBUG TRAIN Batch 13/7600 loss 16.770311 loss_att 18.435467 loss_ctc 20.954908 loss_rnnt 15.671145 hw_loss 0.390352 lr 0.00046422 rank 0
2023-02-23 03:41:46,390 DEBUG TRAIN Batch 13/7600 loss 24.692486 loss_att 29.122280 loss_ctc 31.798719 loss_rnnt 22.606607 hw_loss 0.473290 lr 0.00046413 rank 2
2023-02-23 03:41:46,429 DEBUG TRAIN Batch 13/7600 loss 14.110971 loss_att 14.087915 loss_ctc 17.390114 loss_rnnt 13.415020 hw_loss 0.493771 lr 0.00046416 rank 5
2023-02-23 03:42:58,644 DEBUG TRAIN Batch 13/7700 loss 19.146988 loss_att 22.822121 loss_ctc 21.501839 loss_rnnt 17.833395 hw_loss 0.496099 lr 0.00046393 rank 4
2023-02-23 03:42:58,657 DEBUG TRAIN Batch 13/7700 loss 14.081425 loss_att 14.246061 loss_ctc 15.433617 loss_rnnt 13.634568 hw_loss 0.438067 lr 0.00046393 rank 2
2023-02-23 03:42:58,657 DEBUG TRAIN Batch 13/7700 loss 10.029869 loss_att 10.839573 loss_ctc 12.676257 loss_rnnt 9.205012 hw_loss 0.581369 lr 0.00046401 rank 1
2023-02-23 03:42:58,660 DEBUG TRAIN Batch 13/7700 loss 13.182501 loss_att 15.890369 loss_ctc 15.530579 loss_rnnt 12.084178 hw_loss 0.456887 lr 0.00046394 rank 7
2023-02-23 03:42:58,660 DEBUG TRAIN Batch 13/7700 loss 6.485519 loss_att 12.975594 loss_ctc 10.749540 loss_rnnt 4.442998 hw_loss 0.329944 lr 0.00046396 rank 5
2023-02-23 03:42:58,663 DEBUG TRAIN Batch 13/7700 loss 7.604612 loss_att 12.193612 loss_ctc 9.544275 loss_rnnt 6.207909 hw_loss 0.413028 lr 0.00046393 rank 3
2023-02-23 03:42:58,665 DEBUG TRAIN Batch 13/7700 loss 8.318420 loss_att 12.946257 loss_ctc 7.369205 loss_rnnt 7.326462 hw_loss 0.361788 lr 0.00046402 rank 0
2023-02-23 03:42:58,709 DEBUG TRAIN Batch 13/7700 loss 12.311102 loss_att 14.066257 loss_ctc 14.441427 loss_rnnt 11.428312 hw_loss 0.464468 lr 0.00046398 rank 6
2023-02-23 03:44:12,818 DEBUG TRAIN Batch 13/7800 loss 14.725871 loss_att 20.212040 loss_ctc 17.256607 loss_rnnt 13.048514 hw_loss 0.455049 lr 0.00046374 rank 7
2023-02-23 03:44:12,817 DEBUG TRAIN Batch 13/7800 loss 11.999030 loss_att 13.120665 loss_ctc 13.849643 loss_rnnt 11.328289 hw_loss 0.374374 lr 0.00046373 rank 3
2023-02-23 03:44:12,818 DEBUG TRAIN Batch 13/7800 loss 3.888907 loss_att 7.860406 loss_ctc 4.732765 loss_rnnt 2.766205 hw_loss 0.404788 lr 0.00046378 rank 6
2023-02-23 03:44:12,820 DEBUG TRAIN Batch 13/7800 loss 17.071657 loss_att 20.645180 loss_ctc 23.712862 loss_rnnt 15.278961 hw_loss 0.360932 lr 0.00046382 rank 0
2023-02-23 03:44:12,824 DEBUG TRAIN Batch 13/7800 loss 7.468788 loss_att 14.381855 loss_ctc 10.725925 loss_rnnt 5.457565 hw_loss 0.364357 lr 0.00046373 rank 4
2023-02-23 03:44:12,823 DEBUG TRAIN Batch 13/7800 loss 7.268085 loss_att 10.753069 loss_ctc 10.480789 loss_rnnt 5.970424 hw_loss 0.323072 lr 0.00046373 rank 2
2023-02-23 03:44:12,823 DEBUG TRAIN Batch 13/7800 loss 15.995699 loss_att 17.409737 loss_ctc 20.487768 loss_rnnt 14.897525 hw_loss 0.405794 lr 0.00046381 rank 1
2023-02-23 03:44:12,826 DEBUG TRAIN Batch 13/7800 loss 15.201433 loss_att 20.115665 loss_ctc 23.467278 loss_rnnt 12.888842 hw_loss 0.426810 lr 0.00046376 rank 5
2023-02-23 03:45:25,485 DEBUG TRAIN Batch 13/7900 loss 11.392570 loss_att 16.121010 loss_ctc 11.865097 loss_rnnt 10.182577 hw_loss 0.377441 lr 0.00046357 rank 5
2023-02-23 03:45:25,486 DEBUG TRAIN Batch 13/7900 loss 5.448859 loss_att 9.279412 loss_ctc 8.481432 loss_rnnt 4.055910 hw_loss 0.417179 lr 0.00046354 rank 7
2023-02-23 03:45:25,487 DEBUG TRAIN Batch 13/7900 loss 13.849443 loss_att 16.797449 loss_ctc 18.565611 loss_rnnt 12.412227 hw_loss 0.410238 lr 0.00046358 rank 6
2023-02-23 03:45:25,488 DEBUG TRAIN Batch 13/7900 loss 18.908770 loss_att 19.722572 loss_ctc 21.141790 loss_rnnt 18.209652 hw_loss 0.447417 lr 0.00046353 rank 3
2023-02-23 03:45:25,489 DEBUG TRAIN Batch 13/7900 loss 9.681517 loss_att 12.836792 loss_ctc 16.193272 loss_rnnt 7.989082 hw_loss 0.362149 lr 0.00046353 rank 4
2023-02-23 03:45:25,488 DEBUG TRAIN Batch 13/7900 loss 20.006874 loss_att 22.111750 loss_ctc 22.899687 loss_rnnt 18.942923 hw_loss 0.482377 lr 0.00046353 rank 2
2023-02-23 03:45:25,492 DEBUG TRAIN Batch 13/7900 loss 12.628264 loss_att 16.051195 loss_ctc 16.052208 loss_rnnt 11.263203 hw_loss 0.419906 lr 0.00046362 rank 0
2023-02-23 03:45:25,495 DEBUG TRAIN Batch 13/7900 loss 11.335812 loss_att 16.757189 loss_ctc 13.811775 loss_rnnt 9.666080 hw_loss 0.478740 lr 0.00046362 rank 1
2023-02-23 03:46:36,608 DEBUG TRAIN Batch 13/8000 loss 20.089718 loss_att 24.960505 loss_ctc 26.672424 loss_rnnt 18.007526 hw_loss 0.431887 lr 0.00046333 rank 3
2023-02-23 03:46:36,617 DEBUG TRAIN Batch 13/8000 loss 12.784895 loss_att 15.163679 loss_ctc 12.845886 loss_rnnt 12.109284 hw_loss 0.359479 lr 0.00046334 rank 7
2023-02-23 03:46:36,621 DEBUG TRAIN Batch 13/8000 loss 9.440952 loss_att 12.858751 loss_ctc 12.835701 loss_rnnt 8.068113 hw_loss 0.443711 lr 0.00046333 rank 2
2023-02-23 03:46:36,623 DEBUG TRAIN Batch 13/8000 loss 14.184003 loss_att 15.047348 loss_ctc 17.674862 loss_rnnt 13.338112 hw_loss 0.389575 lr 0.00046337 rank 5
2023-02-23 03:46:36,624 DEBUG TRAIN Batch 13/8000 loss 12.154439 loss_att 16.840897 loss_ctc 16.563519 loss_rnnt 10.435715 hw_loss 0.362916 lr 0.00046333 rank 4
2023-02-23 03:46:36,626 DEBUG TRAIN Batch 13/8000 loss 4.551209 loss_att 6.372930 loss_ctc 5.399772 loss_rnnt 3.880442 hw_loss 0.362403 lr 0.00046342 rank 0
2023-02-23 03:46:36,627 DEBUG TRAIN Batch 13/8000 loss 11.800017 loss_att 13.304388 loss_ctc 14.112734 loss_rnnt 10.968533 hw_loss 0.416717 lr 0.00046338 rank 6
2023-02-23 03:46:36,628 DEBUG TRAIN Batch 13/8000 loss 20.288797 loss_att 23.507639 loss_ctc 25.062252 loss_rnnt 18.779104 hw_loss 0.430245 lr 0.00046342 rank 1
2023-02-23 03:47:49,201 DEBUG TRAIN Batch 13/8100 loss 12.260899 loss_att 14.399734 loss_ctc 17.629183 loss_rnnt 10.767097 hw_loss 0.656743 lr 0.00046322 rank 0
2023-02-23 03:47:49,208 DEBUG TRAIN Batch 13/8100 loss 16.549673 loss_att 18.694807 loss_ctc 23.148485 loss_rnnt 15.002737 hw_loss 0.446377 lr 0.00046318 rank 6
2023-02-23 03:47:49,210 DEBUG TRAIN Batch 13/8100 loss 4.321383 loss_att 6.863073 loss_ctc 6.440585 loss_rnnt 3.269942 hw_loss 0.488519 lr 0.00046317 rank 5
2023-02-23 03:47:49,210 DEBUG TRAIN Batch 13/8100 loss 10.210217 loss_att 11.931229 loss_ctc 13.673912 loss_rnnt 9.158709 hw_loss 0.460274 lr 0.00046314 rank 7
2023-02-23 03:47:49,211 DEBUG TRAIN Batch 13/8100 loss 15.825279 loss_att 18.293550 loss_ctc 19.178835 loss_rnnt 14.644064 hw_loss 0.450787 lr 0.00046313 rank 3
2023-02-23 03:47:49,211 DEBUG TRAIN Batch 13/8100 loss 13.433421 loss_att 14.723005 loss_ctc 15.304090 loss_rnnt 12.736664 hw_loss 0.355161 lr 0.00046313 rank 2
2023-02-23 03:47:49,214 DEBUG TRAIN Batch 13/8100 loss 11.831116 loss_att 16.958483 loss_ctc 14.691053 loss_rnnt 10.206925 hw_loss 0.407607 lr 0.00046322 rank 1
2023-02-23 03:47:49,253 DEBUG TRAIN Batch 13/8100 loss 10.064800 loss_att 14.982846 loss_ctc 12.416458 loss_rnnt 8.539773 hw_loss 0.427245 lr 0.00046313 rank 4
2023-02-23 03:49:01,663 DEBUG TRAIN Batch 13/8200 loss 10.191593 loss_att 12.845867 loss_ctc 12.248369 loss_rnnt 9.153264 hw_loss 0.437319 lr 0.00046294 rank 4
2023-02-23 03:49:01,664 DEBUG TRAIN Batch 13/8200 loss 12.431237 loss_att 13.417969 loss_ctc 13.974902 loss_rnnt 11.809939 hw_loss 0.408994 lr 0.00046294 rank 7
2023-02-23 03:49:01,668 DEBUG TRAIN Batch 13/8200 loss 10.455543 loss_att 11.574985 loss_ctc 13.869915 loss_rnnt 9.521918 hw_loss 0.477161 lr 0.00046297 rank 5
2023-02-23 03:49:01,673 DEBUG TRAIN Batch 13/8200 loss 14.448055 loss_att 18.676437 loss_ctc 16.755529 loss_rnnt 13.059539 hw_loss 0.440955 lr 0.00046302 rank 0
2023-02-23 03:49:01,677 DEBUG TRAIN Batch 13/8200 loss 7.336117 loss_att 8.708383 loss_ctc 10.260937 loss_rnnt 6.395267 hw_loss 0.518290 lr 0.00046298 rank 6
2023-02-23 03:49:01,680 DEBUG TRAIN Batch 13/8200 loss 17.952028 loss_att 21.458143 loss_ctc 21.902777 loss_rnnt 16.485006 hw_loss 0.448182 lr 0.00046294 rank 2
2023-02-23 03:49:01,681 DEBUG TRAIN Batch 13/8200 loss 12.763529 loss_att 16.905632 loss_ctc 18.099619 loss_rnnt 10.976306 hw_loss 0.463732 lr 0.00046302 rank 1
2023-02-23 03:49:01,686 DEBUG TRAIN Batch 13/8200 loss 7.892995 loss_att 10.321574 loss_ctc 10.943361 loss_rnnt 6.761372 hw_loss 0.448484 lr 0.00046293 rank 3
2023-02-23 03:50:13,474 DEBUG TRAIN Batch 13/8300 loss 9.555914 loss_att 15.008457 loss_ctc 14.531050 loss_rnnt 7.613611 hw_loss 0.353328 lr 0.00046274 rank 2
2023-02-23 03:50:13,474 DEBUG TRAIN Batch 13/8300 loss 8.105907 loss_att 14.206232 loss_ctc 10.458747 loss_rnnt 6.365625 hw_loss 0.387198 lr 0.00046273 rank 3
2023-02-23 03:50:13,476 DEBUG TRAIN Batch 13/8300 loss 13.562118 loss_att 13.557964 loss_ctc 14.412902 loss_rnnt 13.282495 hw_loss 0.313151 lr 0.00046274 rank 7
2023-02-23 03:50:13,477 DEBUG TRAIN Batch 13/8300 loss 12.665174 loss_att 14.671274 loss_ctc 13.886969 loss_rnnt 11.852354 hw_loss 0.466302 lr 0.00046274 rank 4
2023-02-23 03:50:13,478 DEBUG TRAIN Batch 13/8300 loss 7.290968 loss_att 14.239052 loss_ctc 9.161259 loss_rnnt 5.391525 hw_loss 0.488350 lr 0.00046277 rank 5
2023-02-23 03:50:13,480 DEBUG TRAIN Batch 13/8300 loss 7.064984 loss_att 11.098068 loss_ctc 11.145316 loss_rnnt 5.471637 hw_loss 0.455034 lr 0.00046278 rank 6
2023-02-23 03:50:13,481 DEBUG TRAIN Batch 13/8300 loss 9.302082 loss_att 12.533300 loss_ctc 11.462597 loss_rnnt 8.178823 hw_loss 0.354273 lr 0.00046282 rank 0
2023-02-23 03:50:13,491 DEBUG TRAIN Batch 13/8300 loss 27.466478 loss_att 31.849871 loss_ctc 28.762386 loss_rnnt 26.161713 hw_loss 0.478689 lr 0.00046282 rank 1
2023-02-23 03:51:11,796 DEBUG CV Batch 13/0 loss 2.902125 loss_att 2.592249 loss_ctc 3.581500 loss_rnnt 2.470015 hw_loss 0.756566 history loss 2.794639 rank 0
2023-02-23 03:51:11,801 DEBUG CV Batch 13/0 loss 2.902125 loss_att 2.592249 loss_ctc 3.581500 loss_rnnt 2.470015 hw_loss 0.756566 history loss 2.794639 rank 4
2023-02-23 03:51:11,811 DEBUG CV Batch 13/0 loss 2.902125 loss_att 2.592249 loss_ctc 3.581500 loss_rnnt 2.470015 hw_loss 0.756566 history loss 2.794639 rank 3
2023-02-23 03:51:11,812 DEBUG CV Batch 13/0 loss 2.902125 loss_att 2.592249 loss_ctc 3.581500 loss_rnnt 2.470015 hw_loss 0.756566 history loss 2.794639 rank 6
2023-02-23 03:51:11,813 DEBUG CV Batch 13/0 loss 2.902125 loss_att 2.592249 loss_ctc 3.581500 loss_rnnt 2.470015 hw_loss 0.756566 history loss 2.794639 rank 2
2023-02-23 03:51:11,814 DEBUG CV Batch 13/0 loss 2.902125 loss_att 2.592249 loss_ctc 3.581500 loss_rnnt 2.470015 hw_loss 0.756566 history loss 2.794639 rank 5
2023-02-23 03:51:11,818 DEBUG CV Batch 13/0 loss 2.902125 loss_att 2.592249 loss_ctc 3.581500 loss_rnnt 2.470015 hw_loss 0.756566 history loss 2.794639 rank 7
2023-02-23 03:51:11,819 DEBUG CV Batch 13/0 loss 2.902125 loss_att 2.592249 loss_ctc 3.581500 loss_rnnt 2.470015 hw_loss 0.756566 history loss 2.794639 rank 1
2023-02-23 03:51:23,331 DEBUG CV Batch 13/100 loss 9.186028 loss_att 10.280791 loss_ctc 11.370372 loss_rnnt 8.379316 hw_loss 0.555961 history loss 4.505388 rank 4
2023-02-23 03:51:23,337 DEBUG CV Batch 13/100 loss 9.186028 loss_att 10.280791 loss_ctc 11.370372 loss_rnnt 8.379316 hw_loss 0.555961 history loss 4.505388 rank 7
2023-02-23 03:51:23,366 DEBUG CV Batch 13/100 loss 9.186028 loss_att 10.280791 loss_ctc 11.370372 loss_rnnt 8.379316 hw_loss 0.555961 history loss 4.505388 rank 5
2023-02-23 03:51:23,407 DEBUG CV Batch 13/100 loss 9.186028 loss_att 10.280791 loss_ctc 11.370372 loss_rnnt 8.379316 hw_loss 0.555961 history loss 4.505388 rank 6
2023-02-23 03:51:23,449 DEBUG CV Batch 13/100 loss 9.186028 loss_att 10.280791 loss_ctc 11.370372 loss_rnnt 8.379316 hw_loss 0.555961 history loss 4.505388 rank 1
2023-02-23 03:51:23,554 DEBUG CV Batch 13/100 loss 9.186028 loss_att 10.280791 loss_ctc 11.370372 loss_rnnt 8.379316 hw_loss 0.555961 history loss 4.505388 rank 0
2023-02-23 03:51:23,573 DEBUG CV Batch 13/100 loss 9.186028 loss_att 10.280791 loss_ctc 11.370372 loss_rnnt 8.379316 hw_loss 0.555961 history loss 4.505388 rank 2
2023-02-23 03:51:23,751 DEBUG CV Batch 13/100 loss 9.186028 loss_att 10.280791 loss_ctc 11.370372 loss_rnnt 8.379316 hw_loss 0.555961 history loss 4.505388 rank 3
2023-02-23 03:51:37,049 DEBUG CV Batch 13/200 loss 3.921114 loss_att 13.174838 loss_ctc 4.259459 loss_rnnt 1.850030 hw_loss 0.328549 history loss 5.194078 rank 5
2023-02-23 03:51:37,078 DEBUG CV Batch 13/200 loss 3.921114 loss_att 13.174838 loss_ctc 4.259459 loss_rnnt 1.850030 hw_loss 0.328549 history loss 5.194078 rank 6
2023-02-23 03:51:37,098 DEBUG CV Batch 13/200 loss 3.921114 loss_att 13.174838 loss_ctc 4.259459 loss_rnnt 1.850030 hw_loss 0.328549 history loss 5.194078 rank 4
2023-02-23 03:51:37,175 DEBUG CV Batch 13/200 loss 3.921114 loss_att 13.174838 loss_ctc 4.259459 loss_rnnt 1.850030 hw_loss 0.328549 history loss 5.194078 rank 7
2023-02-23 03:51:37,253 DEBUG CV Batch 13/200 loss 3.921114 loss_att 13.174838 loss_ctc 4.259459 loss_rnnt 1.850030 hw_loss 0.328549 history loss 5.194078 rank 1
2023-02-23 03:51:37,518 DEBUG CV Batch 13/200 loss 3.921114 loss_att 13.174838 loss_ctc 4.259459 loss_rnnt 1.850030 hw_loss 0.328549 history loss 5.194078 rank 2
2023-02-23 03:51:37,595 DEBUG CV Batch 13/200 loss 3.921114 loss_att 13.174838 loss_ctc 4.259459 loss_rnnt 1.850030 hw_loss 0.328549 history loss 5.194078 rank 0
2023-02-23 03:51:37,708 DEBUG CV Batch 13/200 loss 3.921114 loss_att 13.174838 loss_ctc 4.259459 loss_rnnt 1.850030 hw_loss 0.328549 history loss 5.194078 rank 3
2023-02-23 03:51:49,094 DEBUG CV Batch 13/300 loss 6.314638 loss_att 7.115730 loss_ctc 8.027168 loss_rnnt 5.664332 hw_loss 0.490783 history loss 5.350945 rank 6
2023-02-23 03:51:49,387 DEBUG CV Batch 13/300 loss 6.314638 loss_att 7.115730 loss_ctc 8.027168 loss_rnnt 5.664332 hw_loss 0.490783 history loss 5.350945 rank 5
2023-02-23 03:51:49,426 DEBUG CV Batch 13/300 loss 6.314638 loss_att 7.115730 loss_ctc 8.027168 loss_rnnt 5.664332 hw_loss 0.490783 history loss 5.350945 rank 7
2023-02-23 03:51:49,499 DEBUG CV Batch 13/300 loss 6.314638 loss_att 7.115730 loss_ctc 8.027168 loss_rnnt 5.664332 hw_loss 0.490783 history loss 5.350945 rank 4
2023-02-23 03:51:49,581 DEBUG CV Batch 13/300 loss 6.314638 loss_att 7.115730 loss_ctc 8.027168 loss_rnnt 5.664332 hw_loss 0.490783 history loss 5.350945 rank 1
2023-02-23 03:51:49,960 DEBUG CV Batch 13/300 loss 6.314638 loss_att 7.115730 loss_ctc 8.027168 loss_rnnt 5.664332 hw_loss 0.490783 history loss 5.350945 rank 0
2023-02-23 03:51:50,023 DEBUG CV Batch 13/300 loss 6.314638 loss_att 7.115730 loss_ctc 8.027168 loss_rnnt 5.664332 hw_loss 0.490783 history loss 5.350945 rank 2
2023-02-23 03:51:50,165 DEBUG CV Batch 13/300 loss 6.314638 loss_att 7.115730 loss_ctc 8.027168 loss_rnnt 5.664332 hw_loss 0.490783 history loss 5.350945 rank 3
2023-02-23 03:52:01,098 DEBUG CV Batch 13/400 loss 28.835646 loss_att 129.436432 loss_ctc 13.565937 loss_rnnt 10.617318 hw_loss 0.251488 history loss 6.590928 rank 6
2023-02-23 03:52:01,479 DEBUG CV Batch 13/400 loss 28.835646 loss_att 129.436432 loss_ctc 13.565937 loss_rnnt 10.617318 hw_loss 0.251488 history loss 6.590928 rank 4
2023-02-23 03:52:01,688 DEBUG CV Batch 13/400 loss 28.835646 loss_att 129.436432 loss_ctc 13.565937 loss_rnnt 10.617318 hw_loss 0.251488 history loss 6.590928 rank 7
2023-02-23 03:52:01,890 DEBUG CV Batch 13/400 loss 28.835646 loss_att 129.436432 loss_ctc 13.565937 loss_rnnt 10.617318 hw_loss 0.251488 history loss 6.590928 rank 5
2023-02-23 03:52:01,918 DEBUG CV Batch 13/400 loss 28.835646 loss_att 129.436432 loss_ctc 13.565937 loss_rnnt 10.617318 hw_loss 0.251488 history loss 6.590928 rank 1
2023-02-23 03:52:02,140 DEBUG CV Batch 13/400 loss 28.835646 loss_att 129.436432 loss_ctc 13.565937 loss_rnnt 10.617318 hw_loss 0.251488 history loss 6.590928 rank 0
2023-02-23 03:52:02,489 DEBUG CV Batch 13/400 loss 28.835646 loss_att 129.436432 loss_ctc 13.565937 loss_rnnt 10.617318 hw_loss 0.251488 history loss 6.590928 rank 3
2023-02-23 03:52:02,642 DEBUG CV Batch 13/400 loss 28.835646 loss_att 129.436432 loss_ctc 13.565937 loss_rnnt 10.617318 hw_loss 0.251488 history loss 6.590928 rank 2
2023-02-23 03:52:11,446 DEBUG CV Batch 13/500 loss 6.248121 loss_att 7.513411 loss_ctc 8.693381 loss_rnnt 5.446622 hw_loss 0.417012 history loss 7.610581 rank 6
2023-02-23 03:52:11,903 DEBUG CV Batch 13/500 loss 6.248121 loss_att 7.513411 loss_ctc 8.693381 loss_rnnt 5.446622 hw_loss 0.417012 history loss 7.610581 rank 4
2023-02-23 03:52:12,256 DEBUG CV Batch 13/500 loss 6.248121 loss_att 7.513411 loss_ctc 8.693381 loss_rnnt 5.446622 hw_loss 0.417012 history loss 7.610581 rank 5
2023-02-23 03:52:12,377 DEBUG CV Batch 13/500 loss 6.248121 loss_att 7.513411 loss_ctc 8.693381 loss_rnnt 5.446622 hw_loss 0.417012 history loss 7.610581 rank 7
2023-02-23 03:52:12,737 DEBUG CV Batch 13/500 loss 6.248121 loss_att 7.513411 loss_ctc 8.693381 loss_rnnt 5.446622 hw_loss 0.417012 history loss 7.610581 rank 1
2023-02-23 03:52:13,060 DEBUG CV Batch 13/500 loss 6.248121 loss_att 7.513411 loss_ctc 8.693381 loss_rnnt 5.446622 hw_loss 0.417012 history loss 7.610581 rank 0
2023-02-23 03:52:13,403 DEBUG CV Batch 13/500 loss 6.248121 loss_att 7.513411 loss_ctc 8.693381 loss_rnnt 5.446622 hw_loss 0.417012 history loss 7.610581 rank 3
2023-02-23 03:52:13,608 DEBUG CV Batch 13/500 loss 6.248121 loss_att 7.513411 loss_ctc 8.693381 loss_rnnt 5.446622 hw_loss 0.417012 history loss 7.610581 rank 2
2023-02-23 03:52:23,468 DEBUG CV Batch 13/600 loss 9.633731 loss_att 8.251875 loss_ctc 10.978258 loss_rnnt 9.342840 hw_loss 0.727485 history loss 8.681495 rank 6
2023-02-23 03:52:24,069 DEBUG CV Batch 13/600 loss 9.633731 loss_att 8.251875 loss_ctc 10.978258 loss_rnnt 9.342840 hw_loss 0.727485 history loss 8.681495 rank 4
2023-02-23 03:52:24,429 DEBUG CV Batch 13/600 loss 9.633731 loss_att 8.251875 loss_ctc 10.978258 loss_rnnt 9.342840 hw_loss 0.727485 history loss 8.681495 rank 5
2023-02-23 03:52:24,683 DEBUG CV Batch 13/600 loss 9.633731 loss_att 8.251875 loss_ctc 10.978258 loss_rnnt 9.342840 hw_loss 0.727485 history loss 8.681495 rank 7
2023-02-23 03:52:24,980 DEBUG CV Batch 13/600 loss 9.633731 loss_att 8.251875 loss_ctc 10.978258 loss_rnnt 9.342840 hw_loss 0.727485 history loss 8.681495 rank 1
2023-02-23 03:52:25,688 DEBUG CV Batch 13/600 loss 9.633731 loss_att 8.251875 loss_ctc 10.978258 loss_rnnt 9.342840 hw_loss 0.727485 history loss 8.681495 rank 0
2023-02-23 03:52:25,817 DEBUG CV Batch 13/600 loss 9.633731 loss_att 8.251875 loss_ctc 10.978258 loss_rnnt 9.342840 hw_loss 0.727485 history loss 8.681495 rank 3
2023-02-23 03:52:26,228 DEBUG CV Batch 13/600 loss 9.633731 loss_att 8.251875 loss_ctc 10.978258 loss_rnnt 9.342840 hw_loss 0.727485 history loss 8.681495 rank 2
2023-02-23 03:52:35,104 DEBUG CV Batch 13/700 loss 22.960394 loss_att 73.997467 loss_ctc 21.151491 loss_rnnt 12.810574 hw_loss 0.344234 history loss 9.492999 rank 6
2023-02-23 03:52:35,515 DEBUG CV Batch 13/700 loss 22.960394 loss_att 73.997467 loss_ctc 21.151491 loss_rnnt 12.810574 hw_loss 0.344234 history loss 9.492999 rank 4
2023-02-23 03:52:35,820 DEBUG CV Batch 13/700 loss 22.960394 loss_att 73.997467 loss_ctc 21.151491 loss_rnnt 12.810574 hw_loss 0.344234 history loss 9.492999 rank 5
2023-02-23 03:52:36,285 DEBUG CV Batch 13/700 loss 22.960394 loss_att 73.997467 loss_ctc 21.151491 loss_rnnt 12.810574 hw_loss 0.344234 history loss 9.492999 rank 7
2023-02-23 03:52:36,590 DEBUG CV Batch 13/700 loss 22.960394 loss_att 73.997467 loss_ctc 21.151491 loss_rnnt 12.810574 hw_loss 0.344234 history loss 9.492999 rank 1
2023-02-23 03:52:37,206 DEBUG CV Batch 13/700 loss 22.960394 loss_att 73.997467 loss_ctc 21.151491 loss_rnnt 12.810574 hw_loss 0.344234 history loss 9.492999 rank 0
2023-02-23 03:52:37,705 DEBUG CV Batch 13/700 loss 22.960394 loss_att 73.997467 loss_ctc 21.151491 loss_rnnt 12.810574 hw_loss 0.344234 history loss 9.492999 rank 3
2023-02-23 03:52:38,143 DEBUG CV Batch 13/700 loss 22.960394 loss_att 73.997467 loss_ctc 21.151491 loss_rnnt 12.810574 hw_loss 0.344234 history loss 9.492999 rank 2
2023-02-23 03:52:46,778 DEBUG CV Batch 13/800 loss 14.411889 loss_att 13.610888 loss_ctc 18.463474 loss_rnnt 13.782367 hw_loss 0.467833 history loss 8.828020 rank 6
2023-02-23 03:52:47,297 DEBUG CV Batch 13/800 loss 14.411889 loss_att 13.610888 loss_ctc 18.463474 loss_rnnt 13.782367 hw_loss 0.467833 history loss 8.828020 rank 4
2023-02-23 03:52:47,665 DEBUG CV Batch 13/800 loss 14.411889 loss_att 13.610888 loss_ctc 18.463474 loss_rnnt 13.782367 hw_loss 0.467833 history loss 8.828020 rank 5
2023-02-23 03:52:47,734 DEBUG CV Batch 13/800 loss 14.411889 loss_att 13.610888 loss_ctc 18.463474 loss_rnnt 13.782367 hw_loss 0.467833 history loss 8.828020 rank 7
2023-02-23 03:52:48,005 DEBUG CV Batch 13/800 loss 14.411889 loss_att 13.610888 loss_ctc 18.463474 loss_rnnt 13.782367 hw_loss 0.467833 history loss 8.828020 rank 1
2023-02-23 03:52:49,091 DEBUG CV Batch 13/800 loss 14.411889 loss_att 13.610888 loss_ctc 18.463474 loss_rnnt 13.782367 hw_loss 0.467833 history loss 8.828020 rank 0
2023-02-23 03:52:49,854 DEBUG CV Batch 13/800 loss 14.411889 loss_att 13.610888 loss_ctc 18.463474 loss_rnnt 13.782367 hw_loss 0.467833 history loss 8.828020 rank 2
2023-02-23 03:52:50,232 DEBUG CV Batch 13/800 loss 14.411889 loss_att 13.610888 loss_ctc 18.463474 loss_rnnt 13.782367 hw_loss 0.467833 history loss 8.828020 rank 3
2023-02-23 03:53:00,365 DEBUG CV Batch 13/900 loss 18.259747 loss_att 26.700529 loss_ctc 24.806887 loss_rnnt 15.529957 hw_loss 0.316279 history loss 8.565693 rank 6
2023-02-23 03:53:01,376 DEBUG CV Batch 13/900 loss 18.259747 loss_att 26.700529 loss_ctc 24.806887 loss_rnnt 15.529957 hw_loss 0.316279 history loss 8.565693 rank 7
2023-02-23 03:53:01,379 DEBUG CV Batch 13/900 loss 18.259747 loss_att 26.700529 loss_ctc 24.806887 loss_rnnt 15.529957 hw_loss 0.316279 history loss 8.565693 rank 4
2023-02-23 03:53:01,593 DEBUG CV Batch 13/900 loss 18.259747 loss_att 26.700529 loss_ctc 24.806887 loss_rnnt 15.529957 hw_loss 0.316279 history loss 8.565693 rank 5
2023-02-23 03:53:01,722 DEBUG CV Batch 13/900 loss 18.259747 loss_att 26.700529 loss_ctc 24.806887 loss_rnnt 15.529957 hw_loss 0.316279 history loss 8.565693 rank 1
2023-02-23 03:53:02,925 DEBUG CV Batch 13/900 loss 18.259747 loss_att 26.700529 loss_ctc 24.806887 loss_rnnt 15.529957 hw_loss 0.316279 history loss 8.565693 rank 0
2023-02-23 03:53:03,803 DEBUG CV Batch 13/900 loss 18.259747 loss_att 26.700529 loss_ctc 24.806887 loss_rnnt 15.529957 hw_loss 0.316279 history loss 8.565693 rank 2
2023-02-23 03:53:04,203 DEBUG CV Batch 13/900 loss 18.259747 loss_att 26.700529 loss_ctc 24.806887 loss_rnnt 15.529957 hw_loss 0.316279 history loss 8.565693 rank 3
2023-02-23 03:53:12,531 DEBUG CV Batch 13/1000 loss 6.516886 loss_att 6.482104 loss_ctc 7.835275 loss_rnnt 6.068822 hw_loss 0.523564 history loss 8.276734 rank 6
2023-02-23 03:53:13,535 DEBUG CV Batch 13/1000 loss 6.516885 loss_att 6.482104 loss_ctc 7.835275 loss_rnnt 6.068822 hw_loss 0.523564 history loss 8.276734 rank 4
2023-02-23 03:53:13,893 DEBUG CV Batch 13/1000 loss 6.516886 loss_att 6.482104 loss_ctc 7.835275 loss_rnnt 6.068822 hw_loss 0.523564 history loss 8.276734 rank 7
2023-02-23 03:53:14,223 DEBUG CV Batch 13/1000 loss 6.516886 loss_att 6.482104 loss_ctc 7.835275 loss_rnnt 6.068822 hw_loss 0.523564 history loss 8.276734 rank 5
2023-02-23 03:53:14,334 DEBUG CV Batch 13/1000 loss 6.516885 loss_att 6.482104 loss_ctc 7.835275 loss_rnnt 6.068822 hw_loss 0.523564 history loss 8.276734 rank 1
2023-02-23 03:53:15,432 DEBUG CV Batch 13/1000 loss 6.516885 loss_att 6.482104 loss_ctc 7.835275 loss_rnnt 6.068822 hw_loss 0.523564 history loss 8.276734 rank 0
2023-02-23 03:53:16,751 DEBUG CV Batch 13/1000 loss 6.516886 loss_att 6.482104 loss_ctc 7.835275 loss_rnnt 6.068822 hw_loss 0.523564 history loss 8.276734 rank 3
2023-02-23 03:53:16,792 DEBUG CV Batch 13/1000 loss 6.516886 loss_att 6.482104 loss_ctc 7.835275 loss_rnnt 6.068822 hw_loss 0.523564 history loss 8.276734 rank 2
2023-02-23 03:53:24,527 DEBUG CV Batch 13/1100 loss 7.952466 loss_att 6.482296 loss_ctc 9.658305 loss_rnnt 7.656936 hw_loss 0.678973 history loss 8.265894 rank 6
2023-02-23 03:53:25,443 DEBUG CV Batch 13/1100 loss 7.952466 loss_att 6.482296 loss_ctc 9.658305 loss_rnnt 7.656936 hw_loss 0.678973 history loss 8.265894 rank 4
2023-02-23 03:53:26,182 DEBUG CV Batch 13/1100 loss 7.952466 loss_att 6.482296 loss_ctc 9.658305 loss_rnnt 7.656936 hw_loss 0.678973 history loss 8.265894 rank 7
2023-02-23 03:53:26,433 DEBUG CV Batch 13/1100 loss 7.952466 loss_att 6.482296 loss_ctc 9.658305 loss_rnnt 7.656936 hw_loss 0.678973 history loss 8.265894 rank 5
2023-02-23 03:53:26,672 DEBUG CV Batch 13/1100 loss 7.952466 loss_att 6.482296 loss_ctc 9.658305 loss_rnnt 7.656936 hw_loss 0.678973 history loss 8.265894 rank 1
2023-02-23 03:53:27,929 DEBUG CV Batch 13/1100 loss 7.952466 loss_att 6.482296 loss_ctc 9.658305 loss_rnnt 7.656936 hw_loss 0.678973 history loss 8.265894 rank 0
2023-02-23 03:53:29,221 DEBUG CV Batch 13/1100 loss 7.952466 loss_att 6.482296 loss_ctc 9.658305 loss_rnnt 7.656936 hw_loss 0.678973 history loss 8.265894 rank 3
2023-02-23 03:53:29,413 DEBUG CV Batch 13/1100 loss 7.952466 loss_att 6.482296 loss_ctc 9.658305 loss_rnnt 7.656936 hw_loss 0.678973 history loss 8.265894 rank 2
2023-02-23 03:53:35,093 DEBUG CV Batch 13/1200 loss 8.188255 loss_att 10.758170 loss_ctc 10.158454 loss_rnnt 7.133502 hw_loss 0.521395 history loss 8.707796 rank 6
2023-02-23 03:53:35,736 DEBUG CV Batch 13/1200 loss 8.188255 loss_att 10.758170 loss_ctc 10.158454 loss_rnnt 7.133502 hw_loss 0.521395 history loss 8.707796 rank 4
2023-02-23 03:53:36,902 DEBUG CV Batch 13/1200 loss 8.188255 loss_att 10.758170 loss_ctc 10.158454 loss_rnnt 7.133502 hw_loss 0.521395 history loss 8.707796 rank 5
2023-02-23 03:53:37,049 DEBUG CV Batch 13/1200 loss 8.188255 loss_att 10.758170 loss_ctc 10.158454 loss_rnnt 7.133502 hw_loss 0.521395 history loss 8.707796 rank 7
2023-02-23 03:53:37,669 DEBUG CV Batch 13/1200 loss 8.188255 loss_att 10.758170 loss_ctc 10.158454 loss_rnnt 7.133502 hw_loss 0.521395 history loss 8.707796 rank 1
2023-02-23 03:53:38,707 DEBUG CV Batch 13/1200 loss 8.188255 loss_att 10.758170 loss_ctc 10.158454 loss_rnnt 7.133502 hw_loss 0.521395 history loss 8.707796 rank 0
2023-02-23 03:53:39,954 DEBUG CV Batch 13/1200 loss 8.188255 loss_att 10.758170 loss_ctc 10.158454 loss_rnnt 7.133502 hw_loss 0.521395 history loss 8.707796 rank 3
2023-02-23 03:53:40,867 DEBUG CV Batch 13/1200 loss 8.188255 loss_att 10.758170 loss_ctc 10.158454 loss_rnnt 7.133502 hw_loss 0.521395 history loss 8.707796 rank 2
2023-02-23 03:53:47,179 DEBUG CV Batch 13/1300 loss 7.974279 loss_att 6.031592 loss_ctc 9.141939 loss_rnnt 7.868732 hw_loss 0.634492 history loss 9.052999 rank 6
2023-02-23 03:53:47,817 DEBUG CV Batch 13/1300 loss 7.974279 loss_att 6.031592 loss_ctc 9.141939 loss_rnnt 7.868732 hw_loss 0.634492 history loss 9.052999 rank 4
2023-02-23 03:53:49,036 DEBUG CV Batch 13/1300 loss 7.974279 loss_att 6.031592 loss_ctc 9.141939 loss_rnnt 7.868732 hw_loss 0.634492 history loss 9.052999 rank 5
2023-02-23 03:53:49,301 DEBUG CV Batch 13/1300 loss 7.974279 loss_att 6.031592 loss_ctc 9.141939 loss_rnnt 7.868732 hw_loss 0.634492 history loss 9.052999 rank 7
2023-02-23 03:53:49,804 DEBUG CV Batch 13/1300 loss 7.974279 loss_att 6.031592 loss_ctc 9.141939 loss_rnnt 7.868732 hw_loss 0.634492 history loss 9.052999 rank 1
2023-02-23 03:53:51,103 DEBUG CV Batch 13/1300 loss 7.974279 loss_att 6.031592 loss_ctc 9.141939 loss_rnnt 7.868732 hw_loss 0.634492 history loss 9.052999 rank 0
2023-02-23 03:53:52,283 DEBUG CV Batch 13/1300 loss 7.974279 loss_att 6.031592 loss_ctc 9.141939 loss_rnnt 7.868732 hw_loss 0.634492 history loss 9.052999 rank 3
2023-02-23 03:53:53,664 DEBUG CV Batch 13/1300 loss 7.974279 loss_att 6.031592 loss_ctc 9.141939 loss_rnnt 7.868732 hw_loss 0.634492 history loss 9.052999 rank 2
2023-02-23 03:53:58,341 DEBUG CV Batch 13/1400 loss 14.827256 loss_att 42.487293 loss_ctc 10.096560 loss_rnnt 9.779494 hw_loss 0.274712 history loss 9.449804 rank 6
2023-02-23 03:53:59,113 DEBUG CV Batch 13/1400 loss 14.827256 loss_att 42.487293 loss_ctc 10.096560 loss_rnnt 9.779494 hw_loss 0.274712 history loss 9.449804 rank 4
2023-02-23 03:54:00,813 DEBUG CV Batch 13/1400 loss 14.827256 loss_att 42.487293 loss_ctc 10.096560 loss_rnnt 9.779494 hw_loss 0.274712 history loss 9.449804 rank 5
2023-02-23 03:54:00,883 DEBUG CV Batch 13/1400 loss 14.827256 loss_att 42.487293 loss_ctc 10.096560 loss_rnnt 9.779494 hw_loss 0.274712 history loss 9.449804 rank 7
2023-02-23 03:54:01,393 DEBUG CV Batch 13/1400 loss 14.827256 loss_att 42.487293 loss_ctc 10.096560 loss_rnnt 9.779494 hw_loss 0.274712 history loss 9.449804 rank 1
2023-02-23 03:54:02,624 DEBUG CV Batch 13/1400 loss 14.827256 loss_att 42.487293 loss_ctc 10.096560 loss_rnnt 9.779494 hw_loss 0.274712 history loss 9.449804 rank 0
2023-02-23 03:54:03,928 DEBUG CV Batch 13/1400 loss 14.827256 loss_att 42.487293 loss_ctc 10.096560 loss_rnnt 9.779494 hw_loss 0.274712 history loss 9.449804 rank 3
2023-02-23 03:54:05,649 DEBUG CV Batch 13/1400 loss 14.827256 loss_att 42.487293 loss_ctc 10.096560 loss_rnnt 9.779494 hw_loss 0.274712 history loss 9.449804 rank 2
2023-02-23 03:54:10,060 DEBUG CV Batch 13/1500 loss 7.200340 loss_att 8.460436 loss_ctc 6.232358 loss_rnnt 6.832058 hw_loss 0.459988 history loss 9.216151 rank 6
2023-02-23 03:54:10,812 DEBUG CV Batch 13/1500 loss 7.200340 loss_att 8.460436 loss_ctc 6.232358 loss_rnnt 6.832058 hw_loss 0.459988 history loss 9.216151 rank 4
2023-02-23 03:54:12,638 DEBUG CV Batch 13/1500 loss 7.200340 loss_att 8.460436 loss_ctc 6.232358 loss_rnnt 6.832058 hw_loss 0.459988 history loss 9.216151 rank 7
2023-02-23 03:54:13,061 DEBUG CV Batch 13/1500 loss 7.200340 loss_att 8.460436 loss_ctc 6.232358 loss_rnnt 6.832058 hw_loss 0.459988 history loss 9.216151 rank 1
2023-02-23 03:54:13,511 DEBUG CV Batch 13/1500 loss 7.200340 loss_att 8.460436 loss_ctc 6.232358 loss_rnnt 6.832058 hw_loss 0.459988 history loss 9.216151 rank 5
2023-02-23 03:54:15,323 DEBUG CV Batch 13/1500 loss 7.200340 loss_att 8.460436 loss_ctc 6.232358 loss_rnnt 6.832058 hw_loss 0.459988 history loss 9.216151 rank 0
2023-02-23 03:54:16,090 DEBUG CV Batch 13/1500 loss 7.200340 loss_att 8.460436 loss_ctc 6.232358 loss_rnnt 6.832058 hw_loss 0.459988 history loss 9.216151 rank 3
2023-02-23 03:54:17,934 DEBUG CV Batch 13/1500 loss 7.200340 loss_att 8.460436 loss_ctc 6.232358 loss_rnnt 6.832058 hw_loss 0.459988 history loss 9.216151 rank 2
2023-02-23 03:54:23,524 DEBUG CV Batch 13/1600 loss 11.762218 loss_att 20.131172 loss_ctc 12.226176 loss_rnnt 9.770836 hw_loss 0.479492 history loss 9.111510 rank 6
2023-02-23 03:54:24,276 DEBUG CV Batch 13/1600 loss 11.762218 loss_att 20.131172 loss_ctc 12.226176 loss_rnnt 9.770836 hw_loss 0.479492 history loss 9.111510 rank 4
2023-02-23 03:54:26,126 DEBUG CV Batch 13/1600 loss 11.762218 loss_att 20.131172 loss_ctc 12.226176 loss_rnnt 9.770836 hw_loss 0.479492 history loss 9.111510 rank 7
2023-02-23 03:54:26,589 DEBUG CV Batch 13/1600 loss 11.762218 loss_att 20.131172 loss_ctc 12.226176 loss_rnnt 9.770836 hw_loss 0.479492 history loss 9.111510 rank 1
2023-02-23 03:54:27,256 DEBUG CV Batch 13/1600 loss 11.762218 loss_att 20.131172 loss_ctc 12.226176 loss_rnnt 9.770836 hw_loss 0.479492 history loss 9.111510 rank 5
2023-02-23 03:54:29,488 DEBUG CV Batch 13/1600 loss 11.762218 loss_att 20.131172 loss_ctc 12.226176 loss_rnnt 9.770836 hw_loss 0.479492 history loss 9.111510 rank 0
2023-02-23 03:54:30,223 DEBUG CV Batch 13/1600 loss 11.762218 loss_att 20.131172 loss_ctc 12.226176 loss_rnnt 9.770836 hw_loss 0.479492 history loss 9.111510 rank 3
2023-02-23 03:54:31,801 DEBUG CV Batch 13/1600 loss 11.762218 loss_att 20.131172 loss_ctc 12.226176 loss_rnnt 9.770836 hw_loss 0.479492 history loss 9.111510 rank 2
2023-02-23 03:54:36,147 DEBUG CV Batch 13/1700 loss 11.947385 loss_att 11.598328 loss_ctc 16.585426 loss_rnnt 11.131878 hw_loss 0.500461 history loss 8.978085 rank 6
2023-02-23 03:54:36,824 DEBUG CV Batch 13/1700 loss 11.947385 loss_att 11.598328 loss_ctc 16.585426 loss_rnnt 11.131878 hw_loss 0.500461 history loss 8.978085 rank 4
2023-02-23 03:54:38,944 DEBUG CV Batch 13/1700 loss 11.947385 loss_att 11.598328 loss_ctc 16.585426 loss_rnnt 11.131878 hw_loss 0.500461 history loss 8.978085 rank 7
2023-02-23 03:54:39,371 DEBUG CV Batch 13/1700 loss 11.947385 loss_att 11.598328 loss_ctc 16.585426 loss_rnnt 11.131878 hw_loss 0.500461 history loss 8.978085 rank 1
2023-02-23 03:54:39,827 DEBUG CV Batch 13/1700 loss 11.947385 loss_att 11.598328 loss_ctc 16.585426 loss_rnnt 11.131878 hw_loss 0.500461 history loss 8.978085 rank 5
2023-02-23 03:54:42,204 DEBUG CV Batch 13/1700 loss 11.947385 loss_att 11.598328 loss_ctc 16.585426 loss_rnnt 11.131878 hw_loss 0.500461 history loss 8.978085 rank 0
2023-02-23 03:54:42,738 DEBUG CV Batch 13/1700 loss 11.947385 loss_att 11.598328 loss_ctc 16.585426 loss_rnnt 11.131878 hw_loss 0.500461 history loss 8.978085 rank 3
2023-02-23 03:54:44,543 DEBUG CV Batch 13/1700 loss 11.947385 loss_att 11.598328 loss_ctc 16.585426 loss_rnnt 11.131878 hw_loss 0.500461 history loss 8.978085 rank 2
2023-02-23 03:54:45,666 INFO Epoch 13 CV info cv_loss 8.931582978904927
2023-02-23 03:54:45,667 INFO Epoch 14 TRAIN info lr 0.000462738868167815
2023-02-23 03:54:45,669 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 03:54:46,285 INFO Epoch 13 CV info cv_loss 8.931582976936484
2023-02-23 03:54:46,286 INFO Epoch 14 TRAIN info lr 0.0004626655626862625
2023-02-23 03:54:46,288 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 03:54:48,140 INFO Epoch 13 CV info cv_loss 8.93158298219141
2023-02-23 03:54:48,141 INFO Epoch 14 TRAIN info lr 0.0004626536785933618
2023-02-23 03:54:48,146 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 03:54:48,468 INFO Epoch 13 CV info cv_loss 8.931582981140425
2023-02-23 03:54:48,469 INFO Epoch 14 TRAIN info lr 0.00046277256074929093
2023-02-23 03:54:48,471 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 03:54:49,912 INFO Epoch 13 CV info cv_loss 8.931582978986766
2023-02-23 03:54:49,913 INFO Epoch 14 TRAIN info lr 0.00046268140956812377
2023-02-23 03:54:49,917 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 03:54:51,818 INFO Epoch 13 CV info cv_loss 8.93158297920644
2023-02-23 03:54:51,819 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/13.pt
2023-02-23 03:54:52,039 INFO Epoch 13 CV info cv_loss 8.931582982148337
2023-02-23 03:54:52,040 INFO Epoch 14 TRAIN info lr 0.0004626536785933618
2023-02-23 03:54:52,042 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 03:54:52,558 INFO Epoch 14 TRAIN info lr 0.00046267348592365573
2023-02-23 03:54:52,562 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 03:54:53,935 INFO Epoch 13 CV info cv_loss 8.931582981110274
2023-02-23 03:54:53,936 INFO Epoch 14 TRAIN info lr 0.00046268339054285037
2023-02-23 03:54:53,937 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 03:55:55,207 DEBUG TRAIN Batch 14/0 loss 11.786514 loss_att 10.990991 loss_ctc 13.427211 loss_rnnt 11.363594 hw_loss 0.681124 lr 0.00046265 rank 3
2023-02-23 03:55:55,211 DEBUG TRAIN Batch 14/0 loss 13.145200 loss_att 12.743719 loss_ctc 16.553144 loss_rnnt 12.409771 hw_loss 0.677498 lr 0.00046265 rank 7
2023-02-23 03:55:55,215 DEBUG TRAIN Batch 14/0 loss 9.972824 loss_att 8.805482 loss_ctc 11.479130 loss_rnnt 9.633773 hw_loss 0.696896 lr 0.00046277 rank 1
2023-02-23 03:55:55,216 DEBUG TRAIN Batch 14/0 loss 10.300625 loss_att 9.105940 loss_ctc 11.894194 loss_rnnt 9.899463 hw_loss 0.801793 lr 0.00046266 rank 4
2023-02-23 03:55:55,236 DEBUG TRAIN Batch 14/0 loss 11.571315 loss_att 10.453062 loss_ctc 14.435439 loss_rnnt 10.965472 hw_loss 0.839267 lr 0.00046268 rank 5
2023-02-23 03:55:55,237 DEBUG TRAIN Batch 14/0 loss 10.789304 loss_att 9.119435 loss_ctc 11.277145 loss_rnnt 10.684573 hw_loss 0.700611 lr 0.00046268 rank 2
2023-02-23 03:55:55,252 DEBUG TRAIN Batch 14/0 loss 11.287815 loss_att 10.161833 loss_ctc 12.876233 loss_rnnt 10.902524 hw_loss 0.747559 lr 0.00046267 rank 0
2023-02-23 03:55:55,262 DEBUG TRAIN Batch 14/0 loss 8.595131 loss_att 8.100714 loss_ctc 9.729193 loss_rnnt 8.189660 hw_loss 0.662147 lr 0.00046274 rank 6
2023-02-23 03:57:06,593 DEBUG TRAIN Batch 14/100 loss 9.661340 loss_att 15.656351 loss_ctc 17.388420 loss_rnnt 7.232132 hw_loss 0.374865 lr 0.00046245 rank 7
2023-02-23 03:57:06,596 DEBUG TRAIN Batch 14/100 loss 14.307855 loss_att 16.595921 loss_ctc 21.610083 loss_rnnt 12.677917 hw_loss 0.372552 lr 0.00046248 rank 2
2023-02-23 03:57:06,597 DEBUG TRAIN Batch 14/100 loss 11.046565 loss_att 12.339527 loss_ctc 12.258360 loss_rnnt 10.322986 hw_loss 0.568902 lr 0.00046247 rank 4
2023-02-23 03:57:06,597 DEBUG TRAIN Batch 14/100 loss 12.758470 loss_att 13.618274 loss_ctc 18.021061 loss_rnnt 11.657436 hw_loss 0.426363 lr 0.00046257 rank 1
2023-02-23 03:57:06,599 DEBUG TRAIN Batch 14/100 loss 6.747817 loss_att 9.073292 loss_ctc 8.209627 loss_rnnt 5.839245 hw_loss 0.466066 lr 0.00046245 rank 3
2023-02-23 03:57:06,604 DEBUG TRAIN Batch 14/100 loss 20.865637 loss_att 23.025848 loss_ctc 30.994312 loss_rnnt 18.893921 hw_loss 0.354716 lr 0.00046247 rank 0
2023-02-23 03:57:06,605 DEBUG TRAIN Batch 14/100 loss 8.792216 loss_att 12.331170 loss_ctc 8.858138 loss_rnnt 7.884134 hw_loss 0.359069 lr 0.00046254 rank 6
2023-02-23 03:57:06,644 DEBUG TRAIN Batch 14/100 loss 16.216871 loss_att 18.953499 loss_ctc 19.233191 loss_rnnt 15.044581 hw_loss 0.417731 lr 0.00046248 rank 5
2023-02-23 03:58:17,862 DEBUG TRAIN Batch 14/200 loss 21.806198 loss_att 26.949026 loss_ctc 28.374073 loss_rnnt 19.682425 hw_loss 0.411545 lr 0.00046227 rank 4
2023-02-23 03:58:17,863 DEBUG TRAIN Batch 14/200 loss 17.629473 loss_att 22.398441 loss_ctc 21.651327 loss_rnnt 15.937244 hw_loss 0.379098 lr 0.00046226 rank 3
2023-02-23 03:58:17,868 DEBUG TRAIN Batch 14/200 loss 12.509120 loss_att 13.555889 loss_ctc 11.958649 loss_rnnt 12.159194 hw_loss 0.401193 lr 0.00046226 rank 7
2023-02-23 03:58:17,868 DEBUG TRAIN Batch 14/200 loss 15.968100 loss_att 21.463728 loss_ctc 19.759521 loss_rnnt 14.146262 hw_loss 0.407230 lr 0.00046228 rank 0
2023-02-23 03:58:17,872 DEBUG TRAIN Batch 14/200 loss 19.989111 loss_att 22.103539 loss_ctc 29.277294 loss_rnnt 18.080936 hw_loss 0.462872 lr 0.00046228 rank 5
2023-02-23 03:58:17,873 DEBUG TRAIN Batch 14/200 loss 4.607553 loss_att 7.299772 loss_ctc 6.342290 loss_rnnt 3.625932 hw_loss 0.397273 lr 0.00046237 rank 1
2023-02-23 03:58:17,877 DEBUG TRAIN Batch 14/200 loss 12.799686 loss_att 15.583380 loss_ctc 14.171640 loss_rnnt 11.861523 hw_loss 0.372182 lr 0.00046234 rank 6
2023-02-23 03:58:17,879 DEBUG TRAIN Batch 14/200 loss 9.406172 loss_att 14.824180 loss_ctc 10.364489 loss_rnnt 7.988246 hw_loss 0.387277 lr 0.00046229 rank 2
2023-02-23 03:59:31,003 DEBUG TRAIN Batch 14/300 loss 8.855223 loss_att 13.478271 loss_ctc 14.755962 loss_rnnt 6.942658 hw_loss 0.377229 lr 0.00046206 rank 7
2023-02-23 03:59:31,005 DEBUG TRAIN Batch 14/300 loss 8.504297 loss_att 12.819861 loss_ctc 11.409924 loss_rnnt 7.057561 hw_loss 0.367887 lr 0.00046207 rank 4
2023-02-23 03:59:31,007 DEBUG TRAIN Batch 14/300 loss 7.209105 loss_att 10.828624 loss_ctc 11.831579 loss_rnnt 5.651268 hw_loss 0.408005 lr 0.00046214 rank 6
2023-02-23 03:59:31,007 DEBUG TRAIN Batch 14/300 loss 13.790864 loss_att 19.273247 loss_ctc 16.858366 loss_rnnt 12.102792 hw_loss 0.342364 lr 0.00046208 rank 0
2023-02-23 03:59:31,007 DEBUG TRAIN Batch 14/300 loss 12.059985 loss_att 20.327215 loss_ctc 12.890905 loss_rnnt 10.076420 hw_loss 0.411243 lr 0.00046209 rank 5
2023-02-23 03:59:31,010 DEBUG TRAIN Batch 14/300 loss 12.912937 loss_att 16.986105 loss_ctc 15.288929 loss_rnnt 11.584252 hw_loss 0.369848 lr 0.00046218 rank 1
2023-02-23 03:59:31,010 DEBUG TRAIN Batch 14/300 loss 25.203903 loss_att 31.083132 loss_ctc 33.016068 loss_rnnt 22.781693 hw_loss 0.383894 lr 0.00046209 rank 2
2023-02-23 03:59:31,017 DEBUG TRAIN Batch 14/300 loss 13.701800 loss_att 17.014717 loss_ctc 16.083168 loss_rnnt 12.479342 hw_loss 0.454423 lr 0.00046206 rank 3
2023-02-23 04:00:44,056 DEBUG TRAIN Batch 14/400 loss 13.985091 loss_att 15.078365 loss_ctc 17.526819 loss_rnnt 13.053886 hw_loss 0.450598 lr 0.00046186 rank 7
2023-02-23 04:00:44,056 DEBUG TRAIN Batch 14/400 loss 33.897953 loss_att 35.972210 loss_ctc 40.020035 loss_rnnt 32.466511 hw_loss 0.375588 lr 0.00046187 rank 4
2023-02-23 04:00:44,059 DEBUG TRAIN Batch 14/400 loss 19.753633 loss_att 19.607792 loss_ctc 23.829165 loss_rnnt 19.019150 hw_loss 0.412967 lr 0.00046189 rank 5
2023-02-23 04:00:44,059 DEBUG TRAIN Batch 14/400 loss 21.056843 loss_att 26.195915 loss_ctc 27.994072 loss_rnnt 18.913509 hw_loss 0.357290 lr 0.00046188 rank 0
2023-02-23 04:00:44,060 DEBUG TRAIN Batch 14/400 loss 19.568510 loss_att 24.613089 loss_ctc 20.325466 loss_rnnt 18.235231 hw_loss 0.418940 lr 0.00046198 rank 1
2023-02-23 04:00:44,061 DEBUG TRAIN Batch 14/400 loss 9.326485 loss_att 11.881025 loss_ctc 12.920198 loss_rnnt 8.148379 hw_loss 0.352567 lr 0.00046189 rank 2
2023-02-23 04:00:44,062 DEBUG TRAIN Batch 14/400 loss 10.563465 loss_att 12.190399 loss_ctc 13.911335 loss_rnnt 9.539803 hw_loss 0.472300 lr 0.00046186 rank 3
2023-02-23 04:00:44,062 DEBUG TRAIN Batch 14/400 loss 15.656199 loss_att 19.833052 loss_ctc 20.836182 loss_rnnt 13.877533 hw_loss 0.473683 lr 0.00046195 rank 6
2023-02-23 04:01:55,511 DEBUG TRAIN Batch 14/500 loss 16.961271 loss_att 22.990616 loss_ctc 22.320753 loss_rnnt 14.801346 hw_loss 0.448985 lr 0.00046169 rank 2
2023-02-23 04:01:55,511 DEBUG TRAIN Batch 14/500 loss 9.542842 loss_att 10.658269 loss_ctc 9.412499 loss_rnnt 9.120139 hw_loss 0.406867 lr 0.00046166 rank 3
2023-02-23 04:01:55,513 DEBUG TRAIN Batch 14/500 loss 9.201516 loss_att 11.740409 loss_ctc 12.082290 loss_rnnt 8.110660 hw_loss 0.373076 lr 0.00046168 rank 4
2023-02-23 04:01:55,513 DEBUG TRAIN Batch 14/500 loss 10.630131 loss_att 12.887844 loss_ctc 13.298679 loss_rnnt 9.584312 hw_loss 0.447129 lr 0.00046166 rank 7
2023-02-23 04:01:55,514 DEBUG TRAIN Batch 14/500 loss 9.309235 loss_att 10.668772 loss_ctc 9.123995 loss_rnnt 8.857367 hw_loss 0.383738 lr 0.00046178 rank 1
2023-02-23 04:01:55,517 DEBUG TRAIN Batch 14/500 loss 22.610109 loss_att 22.260048 loss_ctc 25.081526 loss_rnnt 22.126081 hw_loss 0.420978 lr 0.00046168 rank 0
2023-02-23 04:01:55,518 DEBUG TRAIN Batch 14/500 loss 18.206745 loss_att 22.262875 loss_ctc 21.309650 loss_rnnt 16.755648 hw_loss 0.424031 lr 0.00046175 rank 6
2023-02-23 04:01:55,519 DEBUG TRAIN Batch 14/500 loss 7.157447 loss_att 10.777961 loss_ctc 9.086321 loss_rnnt 5.890475 hw_loss 0.535660 lr 0.00046169 rank 5
2023-02-23 04:03:07,874 DEBUG TRAIN Batch 14/600 loss 10.298797 loss_att 10.617685 loss_ctc 12.410642 loss_rnnt 9.627269 hw_loss 0.611571 lr 0.00046147 rank 7
2023-02-23 04:03:07,877 DEBUG TRAIN Batch 14/600 loss 5.008468 loss_att 8.997141 loss_ctc 7.159516 loss_rnnt 3.681599 hw_loss 0.454366 lr 0.00046147 rank 3
2023-02-23 04:03:07,881 DEBUG TRAIN Batch 14/600 loss 19.004526 loss_att 18.656410 loss_ctc 23.378336 loss_rnnt 18.160696 hw_loss 0.619273 lr 0.00046149 rank 0
2023-02-23 04:03:07,883 DEBUG TRAIN Batch 14/600 loss 9.168063 loss_att 9.525990 loss_ctc 10.636950 loss_rnnt 8.604703 hw_loss 0.554855 lr 0.00046150 rank 2
2023-02-23 04:03:07,884 DEBUG TRAIN Batch 14/600 loss 10.519421 loss_att 11.678499 loss_ctc 13.835435 loss_rnnt 9.602590 hw_loss 0.455402 lr 0.00046148 rank 4
2023-02-23 04:03:07,886 DEBUG TRAIN Batch 14/600 loss 11.726481 loss_att 17.047899 loss_ctc 18.836075 loss_rnnt 9.527205 hw_loss 0.350713 lr 0.00046150 rank 5
2023-02-23 04:03:07,887 DEBUG TRAIN Batch 14/600 loss 7.527667 loss_att 8.981791 loss_ctc 9.580667 loss_rnnt 6.701639 hw_loss 0.490254 lr 0.00046159 rank 1
2023-02-23 04:03:07,898 DEBUG TRAIN Batch 14/600 loss 8.506907 loss_att 9.752420 loss_ctc 11.936874 loss_rnnt 7.562923 hw_loss 0.445409 lr 0.00046155 rank 6
2023-02-23 04:04:22,449 DEBUG TRAIN Batch 14/700 loss 15.610410 loss_att 18.428413 loss_ctc 17.252090 loss_rnnt 14.612605 hw_loss 0.403712 lr 0.00046127 rank 7
2023-02-23 04:04:22,450 DEBUG TRAIN Batch 14/700 loss 16.763144 loss_att 15.253695 loss_ctc 16.748035 loss_rnnt 16.883432 hw_loss 0.344274 lr 0.00046130 rank 2
2023-02-23 04:04:22,453 DEBUG TRAIN Batch 14/700 loss 7.267851 loss_att 7.104871 loss_ctc 7.692849 loss_rnnt 6.834923 hw_loss 0.766610 lr 0.00046127 rank 3
2023-02-23 04:04:22,455 DEBUG TRAIN Batch 14/700 loss 6.989716 loss_att 10.842700 loss_ctc 9.360232 loss_rnnt 5.715776 hw_loss 0.351137 lr 0.00046139 rank 1
2023-02-23 04:04:22,456 DEBUG TRAIN Batch 14/700 loss 16.683987 loss_att 20.243267 loss_ctc 23.544506 loss_rnnt 14.851916 hw_loss 0.385275 lr 0.00046129 rank 0
2023-02-23 04:04:22,457 DEBUG TRAIN Batch 14/700 loss 12.425966 loss_att 13.618151 loss_ctc 16.451246 loss_rnnt 11.411350 hw_loss 0.449017 lr 0.00046136 rank 6
2023-02-23 04:04:22,458 DEBUG TRAIN Batch 14/700 loss 13.017801 loss_att 16.494804 loss_ctc 13.974203 loss_rnnt 12.022029 hw_loss 0.324097 lr 0.00046130 rank 5
2023-02-23 04:04:22,495 DEBUG TRAIN Batch 14/700 loss 4.043649 loss_att 7.783816 loss_ctc 5.876034 loss_rnnt 2.867383 hw_loss 0.344841 lr 0.00046128 rank 4
2023-02-23 04:05:34,423 DEBUG TRAIN Batch 14/800 loss 16.724602 loss_att 19.482800 loss_ctc 20.077084 loss_rnnt 15.489517 hw_loss 0.443338 lr 0.00046108 rank 3
2023-02-23 04:05:34,424 DEBUG TRAIN Batch 14/800 loss 10.064770 loss_att 11.536419 loss_ctc 15.191669 loss_rnnt 8.876262 hw_loss 0.394860 lr 0.00046108 rank 7
2023-02-23 04:05:34,426 DEBUG TRAIN Batch 14/800 loss 7.411201 loss_att 11.868663 loss_ctc 10.088947 loss_rnnt 5.994507 hw_loss 0.315317 lr 0.00046119 rank 1
2023-02-23 04:05:34,429 DEBUG TRAIN Batch 14/800 loss 16.832203 loss_att 17.954130 loss_ctc 20.158279 loss_rnnt 15.987020 hw_loss 0.332475 lr 0.00046110 rank 2
2023-02-23 04:05:34,430 DEBUG TRAIN Batch 14/800 loss 18.426493 loss_att 21.905745 loss_ctc 25.772167 loss_rnnt 16.521511 hw_loss 0.430701 lr 0.00046116 rank 6
2023-02-23 04:05:34,431 DEBUG TRAIN Batch 14/800 loss 10.401105 loss_att 12.522326 loss_ctc 15.264057 loss_rnnt 9.106979 hw_loss 0.415290 lr 0.00046109 rank 0
2023-02-23 04:05:34,431 DEBUG TRAIN Batch 14/800 loss 8.334368 loss_att 11.352159 loss_ctc 12.087894 loss_rnnt 7.030782 hw_loss 0.374168 lr 0.00046109 rank 4
2023-02-23 04:05:34,433 DEBUG TRAIN Batch 14/800 loss 6.692667 loss_att 13.554065 loss_ctc 11.790581 loss_rnnt 4.411636 hw_loss 0.429430 lr 0.00046110 rank 5
2023-02-23 04:06:45,856 DEBUG TRAIN Batch 14/900 loss 7.834486 loss_att 15.383679 loss_ctc 7.637179 loss_rnnt 6.139625 hw_loss 0.396241 lr 0.00046089 rank 4
2023-02-23 04:06:45,869 DEBUG TRAIN Batch 14/900 loss 8.968298 loss_att 13.614251 loss_ctc 11.554797 loss_rnnt 7.493729 hw_loss 0.375960 lr 0.00046088 rank 3
2023-02-23 04:06:45,870 DEBUG TRAIN Batch 14/900 loss 13.163404 loss_att 17.532715 loss_ctc 18.184462 loss_rnnt 11.380667 hw_loss 0.448878 lr 0.00046096 rank 6
2023-02-23 04:06:45,871 DEBUG TRAIN Batch 14/900 loss 12.098665 loss_att 17.149780 loss_ctc 16.071514 loss_rnnt 10.337025 hw_loss 0.415697 lr 0.00046100 rank 1
2023-02-23 04:06:45,872 DEBUG TRAIN Batch 14/900 loss 11.907866 loss_att 15.897047 loss_ctc 15.873331 loss_rnnt 10.383864 hw_loss 0.370194 lr 0.00046088 rank 7
2023-02-23 04:06:45,873 DEBUG TRAIN Batch 14/900 loss 12.691197 loss_att 14.953419 loss_ctc 18.465321 loss_rnnt 11.272052 hw_loss 0.369036 lr 0.00046091 rank 2
2023-02-23 04:06:45,875 DEBUG TRAIN Batch 14/900 loss 11.354021 loss_att 15.363770 loss_ctc 15.007524 loss_rnnt 9.901451 hw_loss 0.306537 lr 0.00046090 rank 0
2023-02-23 04:06:45,876 DEBUG TRAIN Batch 14/900 loss 9.876097 loss_att 16.580832 loss_ctc 14.425488 loss_rnnt 7.684849 hw_loss 0.456967 lr 0.00046091 rank 5
2023-02-23 04:07:58,158 DEBUG TRAIN Batch 14/1000 loss 15.083541 loss_att 20.065235 loss_ctc 19.958345 loss_rnnt 13.237219 hw_loss 0.375017 lr 0.00046068 rank 7
2023-02-23 04:07:58,161 DEBUG TRAIN Batch 14/1000 loss 17.485518 loss_att 28.358990 loss_ctc 22.576771 loss_rnnt 14.412909 hw_loss 0.410776 lr 0.00046077 rank 6
2023-02-23 04:07:58,172 DEBUG TRAIN Batch 14/1000 loss 17.117186 loss_att 21.161404 loss_ctc 22.762341 loss_rnnt 15.332851 hw_loss 0.417752 lr 0.00046068 rank 3
2023-02-23 04:07:58,173 DEBUG TRAIN Batch 14/1000 loss 12.102695 loss_att 16.182638 loss_ctc 17.253941 loss_rnnt 10.386419 hw_loss 0.400228 lr 0.00046080 rank 1
2023-02-23 04:07:58,173 DEBUG TRAIN Batch 14/1000 loss 17.350351 loss_att 16.907253 loss_ctc 17.972097 loss_rnnt 17.112875 hw_loss 0.455995 lr 0.00046070 rank 0
2023-02-23 04:07:58,175 DEBUG TRAIN Batch 14/1000 loss 26.368057 loss_att 25.046965 loss_ctc 29.175797 loss_rnnt 26.048466 hw_loss 0.392704 lr 0.00046071 rank 2
2023-02-23 04:07:58,176 DEBUG TRAIN Batch 14/1000 loss 6.687125 loss_att 10.711219 loss_ctc 9.467609 loss_rnnt 5.303753 hw_loss 0.389666 lr 0.00046071 rank 5
2023-02-23 04:07:58,221 DEBUG TRAIN Batch 14/1000 loss 14.810087 loss_att 15.242231 loss_ctc 13.719772 loss_rnnt 14.700484 hw_loss 0.316032 lr 0.00046070 rank 4
2023-02-23 04:09:12,567 DEBUG TRAIN Batch 14/1100 loss 9.626921 loss_att 12.862967 loss_ctc 12.252495 loss_rnnt 8.423974 hw_loss 0.385614 lr 0.00046051 rank 0
2023-02-23 04:09:12,587 DEBUG TRAIN Batch 14/1100 loss 21.871618 loss_att 25.716177 loss_ctc 28.369331 loss_rnnt 20.004486 hw_loss 0.434736 lr 0.00046049 rank 7
2023-02-23 04:09:12,589 DEBUG TRAIN Batch 14/1100 loss 10.791327 loss_att 12.492235 loss_ctc 13.536700 loss_rnnt 9.851802 hw_loss 0.437427 lr 0.00046052 rank 5
2023-02-23 04:09:12,589 DEBUG TRAIN Batch 14/1100 loss 23.759071 loss_att 26.977043 loss_ctc 31.343004 loss_rnnt 21.877827 hw_loss 0.424609 lr 0.00046050 rank 4
2023-02-23 04:09:12,592 DEBUG TRAIN Batch 14/1100 loss 13.262421 loss_att 16.701082 loss_ctc 18.273434 loss_rnnt 11.672504 hw_loss 0.438843 lr 0.00046049 rank 3
2023-02-23 04:09:12,596 DEBUG TRAIN Batch 14/1100 loss 13.556926 loss_att 16.413599 loss_ctc 17.963598 loss_rnnt 12.179974 hw_loss 0.408864 lr 0.00046052 rank 2
2023-02-23 04:09:12,596 DEBUG TRAIN Batch 14/1100 loss 3.770353 loss_att 6.426411 loss_ctc 5.165736 loss_rnnt 2.826249 hw_loss 0.425327 lr 0.00046061 rank 1
2023-02-23 04:09:12,638 DEBUG TRAIN Batch 14/1100 loss 13.484807 loss_att 17.365215 loss_ctc 19.846970 loss_rnnt 11.590786 hw_loss 0.505596 lr 0.00046057 rank 6
2023-02-23 04:10:24,804 DEBUG TRAIN Batch 14/1200 loss 15.248860 loss_att 16.368103 loss_ctc 18.264273 loss_rnnt 14.395029 hw_loss 0.427363 lr 0.00046030 rank 4
2023-02-23 04:10:24,806 DEBUG TRAIN Batch 14/1200 loss 12.770362 loss_att 15.498022 loss_ctc 18.020454 loss_rnnt 11.268798 hw_loss 0.480037 lr 0.00046029 rank 7
2023-02-23 04:10:24,810 DEBUG TRAIN Batch 14/1200 loss 13.912891 loss_att 18.619919 loss_ctc 17.480839 loss_rnnt 12.259244 hw_loss 0.443466 lr 0.00046029 rank 3
2023-02-23 04:10:24,811 DEBUG TRAIN Batch 14/1200 loss 8.703147 loss_att 8.647736 loss_ctc 11.628006 loss_rnnt 8.002095 hw_loss 0.604037 lr 0.00046032 rank 2
2023-02-23 04:10:24,813 DEBUG TRAIN Batch 14/1200 loss 12.045110 loss_att 14.037239 loss_ctc 14.253322 loss_rnnt 11.184353 hw_loss 0.314817 lr 0.00046032 rank 5
2023-02-23 04:10:24,814 DEBUG TRAIN Batch 14/1200 loss 13.648350 loss_att 13.211685 loss_ctc 17.370859 loss_rnnt 12.933156 hw_loss 0.574106 lr 0.00046038 rank 6
2023-02-23 04:10:24,816 DEBUG TRAIN Batch 14/1200 loss 11.088570 loss_att 14.125983 loss_ctc 14.928507 loss_rnnt 9.710399 hw_loss 0.485058 lr 0.00046041 rank 1
2023-02-23 04:10:24,866 DEBUG TRAIN Batch 14/1200 loss 13.474760 loss_att 14.624464 loss_ctc 15.315578 loss_rnnt 12.750978 hw_loss 0.465746 lr 0.00046031 rank 0
2023-02-23 04:11:36,043 DEBUG TRAIN Batch 14/1300 loss 10.781858 loss_att 11.829156 loss_ctc 13.795739 loss_rnnt 9.917059 hw_loss 0.475292 lr 0.00046010 rank 3
2023-02-23 04:11:36,045 DEBUG TRAIN Batch 14/1300 loss 12.620061 loss_att 14.739202 loss_ctc 17.685049 loss_rnnt 11.333854 hw_loss 0.350714 lr 0.00046013 rank 2
2023-02-23 04:11:36,045 DEBUG TRAIN Batch 14/1300 loss 7.105580 loss_att 11.835188 loss_ctc 10.945414 loss_rnnt 5.495337 hw_loss 0.285644 lr 0.00046022 rank 1
2023-02-23 04:11:36,047 DEBUG TRAIN Batch 14/1300 loss 17.550875 loss_att 24.823589 loss_ctc 23.735695 loss_rnnt 15.069464 hw_loss 0.379173 lr 0.00046018 rank 6
2023-02-23 04:11:36,047 DEBUG TRAIN Batch 14/1300 loss 9.532331 loss_att 12.494739 loss_ctc 13.811331 loss_rnnt 8.088378 hw_loss 0.526759 lr 0.00046011 rank 4
2023-02-23 04:11:36,047 DEBUG TRAIN Batch 14/1300 loss 11.167887 loss_att 14.602247 loss_ctc 16.103228 loss_rnnt 9.653445 hw_loss 0.317857 lr 0.00046010 rank 7
2023-02-23 04:11:36,050 DEBUG TRAIN Batch 14/1300 loss 10.045890 loss_att 11.854514 loss_ctc 15.902536 loss_rnnt 8.683035 hw_loss 0.412958 lr 0.00046013 rank 5
2023-02-23 04:11:36,053 DEBUG TRAIN Batch 14/1300 loss 12.485762 loss_att 17.865213 loss_ctc 14.178454 loss_rnnt 11.038496 hw_loss 0.273156 lr 0.00046012 rank 0
2023-02-23 04:12:50,983 DEBUG TRAIN Batch 14/1400 loss 10.637848 loss_att 14.473768 loss_ctc 12.197223 loss_rnnt 9.475882 hw_loss 0.350372 lr 0.00046002 rank 1
2023-02-23 04:12:50,984 DEBUG TRAIN Batch 14/1400 loss 8.168309 loss_att 13.081612 loss_ctc 12.316956 loss_rnnt 6.392351 hw_loss 0.450271 lr 0.00045993 rank 5
2023-02-23 04:12:50,985 DEBUG TRAIN Batch 14/1400 loss 13.164149 loss_att 17.146393 loss_ctc 18.830992 loss_rnnt 11.355284 hw_loss 0.481570 lr 0.00045990 rank 3
2023-02-23 04:12:50,986 DEBUG TRAIN Batch 14/1400 loss 14.363176 loss_att 19.598217 loss_ctc 19.436583 loss_rnnt 12.391238 hw_loss 0.465893 lr 0.00045992 rank 0
2023-02-23 04:12:50,986 DEBUG TRAIN Batch 14/1400 loss 6.279544 loss_att 9.564391 loss_ctc 10.001139 loss_rnnt 4.870224 hw_loss 0.480260 lr 0.00045990 rank 7
2023-02-23 04:12:50,988 DEBUG TRAIN Batch 14/1400 loss 14.397592 loss_att 19.121433 loss_ctc 19.707962 loss_rnnt 12.554057 hw_loss 0.357591 lr 0.00045993 rank 2
2023-02-23 04:12:50,998 DEBUG TRAIN Batch 14/1400 loss 12.748179 loss_att 13.875866 loss_ctc 14.254137 loss_rnnt 12.140814 hw_loss 0.339438 lr 0.00045999 rank 6
2023-02-23 04:12:51,033 DEBUG TRAIN Batch 14/1400 loss 18.180044 loss_att 21.021868 loss_ctc 18.265064 loss_rnnt 17.373241 hw_loss 0.425815 lr 0.00045992 rank 4
2023-02-23 04:14:03,453 DEBUG TRAIN Batch 14/1500 loss 10.014030 loss_att 10.959105 loss_ctc 13.498840 loss_rnnt 9.145426 hw_loss 0.403029 lr 0.00045971 rank 7
2023-02-23 04:14:03,458 DEBUG TRAIN Batch 14/1500 loss 9.800566 loss_att 14.540683 loss_ctc 14.937325 loss_rnnt 7.991251 hw_loss 0.330733 lr 0.00045971 rank 3
2023-02-23 04:14:03,459 DEBUG TRAIN Batch 14/1500 loss 8.301640 loss_att 12.654852 loss_ctc 11.544351 loss_rnnt 6.795482 hw_loss 0.380913 lr 0.00045974 rank 2
2023-02-23 04:14:03,462 DEBUG TRAIN Batch 14/1500 loss 7.958118 loss_att 11.346754 loss_ctc 8.923904 loss_rnnt 6.893002 hw_loss 0.484909 lr 0.00045979 rank 6
2023-02-23 04:14:03,462 DEBUG TRAIN Batch 14/1500 loss 14.315577 loss_att 19.375212 loss_ctc 19.120228 loss_rnnt 12.449703 hw_loss 0.399985 lr 0.00045974 rank 5
2023-02-23 04:14:03,463 DEBUG TRAIN Batch 14/1500 loss 15.708677 loss_att 15.108021 loss_ctc 19.258141 loss_rnnt 15.095702 hw_loss 0.487210 lr 0.00045972 rank 4
2023-02-23 04:14:03,465 DEBUG TRAIN Batch 14/1500 loss 10.162123 loss_att 15.215282 loss_ctc 13.813768 loss_rnnt 8.385457 hw_loss 0.523401 lr 0.00045983 rank 1
2023-02-23 04:14:03,466 DEBUG TRAIN Batch 14/1500 loss 6.589296 loss_att 10.836073 loss_ctc 9.566596 loss_rnnt 5.116693 hw_loss 0.424265 lr 0.00045973 rank 0
2023-02-23 04:15:14,187 DEBUG TRAIN Batch 14/1600 loss 16.616470 loss_att 18.924603 loss_ctc 25.648897 loss_rnnt 14.718822 hw_loss 0.434431 lr 0.00045951 rank 7
2023-02-23 04:15:14,191 DEBUG TRAIN Batch 14/1600 loss 7.379339 loss_att 11.711138 loss_ctc 10.335188 loss_rnnt 5.911804 hw_loss 0.388243 lr 0.00045953 rank 4
2023-02-23 04:15:14,192 DEBUG TRAIN Batch 14/1600 loss 17.088379 loss_att 21.046452 loss_ctc 26.393751 loss_rnnt 14.823813 hw_loss 0.435443 lr 0.00045954 rank 2
2023-02-23 04:15:14,196 DEBUG TRAIN Batch 14/1600 loss 15.508095 loss_att 20.048706 loss_ctc 20.244247 loss_rnnt 13.757684 hw_loss 0.395252 lr 0.00045963 rank 1
2023-02-23 04:15:14,199 DEBUG TRAIN Batch 14/1600 loss 15.307186 loss_att 19.542606 loss_ctc 19.728649 loss_rnnt 13.660215 hw_loss 0.394422 lr 0.00045960 rank 6
2023-02-23 04:15:14,199 DEBUG TRAIN Batch 14/1600 loss 19.402943 loss_att 19.321281 loss_ctc 24.061237 loss_rnnt 18.564800 hw_loss 0.437564 lr 0.00045953 rank 0
2023-02-23 04:15:14,201 DEBUG TRAIN Batch 14/1600 loss 6.586870 loss_att 8.819290 loss_ctc 9.555270 loss_rnnt 5.545261 hw_loss 0.373758 lr 0.00045951 rank 3
2023-02-23 04:15:14,245 DEBUG TRAIN Batch 14/1600 loss 18.421564 loss_att 25.004946 loss_ctc 25.843140 loss_rnnt 15.879922 hw_loss 0.441415 lr 0.00045954 rank 5
2023-02-23 04:16:26,979 DEBUG TRAIN Batch 14/1700 loss 11.386096 loss_att 14.063379 loss_ctc 12.426498 loss_rnnt 10.519745 hw_loss 0.360325 lr 0.00045944 rank 1
2023-02-23 04:16:26,982 DEBUG TRAIN Batch 14/1700 loss 19.981787 loss_att 26.127674 loss_ctc 24.232635 loss_rnnt 17.935528 hw_loss 0.469316 lr 0.00045932 rank 7
2023-02-23 04:16:26,986 DEBUG TRAIN Batch 14/1700 loss 18.585224 loss_att 20.218519 loss_ctc 19.935154 loss_rnnt 17.892910 hw_loss 0.348119 lr 0.00045932 rank 3
2023-02-23 04:16:26,990 DEBUG TRAIN Batch 14/1700 loss 7.172904 loss_att 10.032328 loss_ctc 8.308557 loss_rnnt 6.191857 hw_loss 0.483266 lr 0.00045935 rank 5
2023-02-23 04:16:26,995 DEBUG TRAIN Batch 14/1700 loss 35.579239 loss_att 38.094769 loss_ctc 45.572498 loss_rnnt 33.552841 hw_loss 0.357857 lr 0.00045940 rank 6
2023-02-23 04:16:26,995 DEBUG TRAIN Batch 14/1700 loss 13.896017 loss_att 18.232492 loss_ctc 17.303904 loss_rnnt 12.328613 hw_loss 0.460732 lr 0.00045933 rank 4
2023-02-23 04:16:26,996 DEBUG TRAIN Batch 14/1700 loss 11.353421 loss_att 13.045611 loss_ctc 20.335293 loss_rnnt 9.608378 hw_loss 0.391917 lr 0.00045935 rank 2
2023-02-23 04:16:27,005 DEBUG TRAIN Batch 14/1700 loss 27.631899 loss_att 27.844692 loss_ctc 39.773865 loss_rnnt 25.782776 hw_loss 0.351814 lr 0.00045934 rank 0
2023-02-23 04:17:41,665 DEBUG TRAIN Batch 14/1800 loss 9.465064 loss_att 9.751859 loss_ctc 11.078778 loss_rnnt 8.900000 hw_loss 0.548519 lr 0.00045916 rank 2
2023-02-23 04:17:41,665 DEBUG TRAIN Batch 14/1800 loss 23.455826 loss_att 25.209843 loss_ctc 29.208826 loss_rnnt 22.089808 hw_loss 0.465274 lr 0.00045914 rank 4
2023-02-23 04:17:41,666 DEBUG TRAIN Batch 14/1800 loss 6.020177 loss_att 8.838936 loss_ctc 9.206594 loss_rnnt 4.785139 hw_loss 0.462059 lr 0.00045913 rank 7
2023-02-23 04:17:41,668 DEBUG TRAIN Batch 14/1800 loss 12.614479 loss_att 16.080849 loss_ctc 15.538755 loss_rnnt 11.315041 hw_loss 0.405488 lr 0.00045913 rank 3
2023-02-23 04:17:41,668 DEBUG TRAIN Batch 14/1800 loss 17.954185 loss_att 19.942537 loss_ctc 21.469101 loss_rnnt 16.819000 hw_loss 0.504111 lr 0.00045921 rank 6
2023-02-23 04:17:41,672 DEBUG TRAIN Batch 14/1800 loss 26.615807 loss_att 28.898775 loss_ctc 29.642506 loss_rnnt 25.532604 hw_loss 0.418218 lr 0.00045915 rank 5
2023-02-23 04:17:41,673 DEBUG TRAIN Batch 14/1800 loss 17.927691 loss_att 18.005114 loss_ctc 22.213648 loss_rnnt 17.117001 hw_loss 0.419521 lr 0.00045915 rank 0
2023-02-23 04:17:41,675 DEBUG TRAIN Batch 14/1800 loss 12.208031 loss_att 16.536051 loss_ctc 16.834131 loss_rnnt 10.500045 hw_loss 0.422940 lr 0.00045924 rank 1
2023-02-23 04:18:54,783 DEBUG TRAIN Batch 14/1900 loss 8.763558 loss_att 9.742273 loss_ctc 11.568180 loss_rnnt 7.875064 hw_loss 0.597753 lr 0.00045895 rank 0
2023-02-23 04:18:54,785 DEBUG TRAIN Batch 14/1900 loss 20.049032 loss_att 19.559769 loss_ctc 24.378479 loss_rnnt 19.354399 hw_loss 0.403554 lr 0.00045893 rank 3
2023-02-23 04:18:54,785 DEBUG TRAIN Batch 14/1900 loss 11.773381 loss_att 10.985461 loss_ctc 13.572706 loss_rnnt 11.303628 hw_loss 0.726426 lr 0.00045905 rank 1
2023-02-23 04:18:54,788 DEBUG TRAIN Batch 14/1900 loss 3.853280 loss_att 7.207150 loss_ctc 3.782792 loss_rnnt 2.977914 hw_loss 0.401231 lr 0.00045896 rank 2
2023-02-23 04:18:54,788 DEBUG TRAIN Batch 14/1900 loss 16.864246 loss_att 18.091988 loss_ctc 21.956753 loss_rnnt 15.736647 hw_loss 0.380723 lr 0.00045893 rank 7
2023-02-23 04:18:54,789 DEBUG TRAIN Batch 14/1900 loss 19.763159 loss_att 25.027653 loss_ctc 27.359411 loss_rnnt 17.458710 hw_loss 0.447591 lr 0.00045895 rank 4
2023-02-23 04:18:54,789 DEBUG TRAIN Batch 14/1900 loss 11.668291 loss_att 15.395491 loss_ctc 17.834627 loss_rnnt 9.932953 hw_loss 0.314475 lr 0.00045896 rank 5
2023-02-23 04:18:54,790 DEBUG TRAIN Batch 14/1900 loss 7.148344 loss_att 13.132103 loss_ctc 11.410731 loss_rnnt 5.217978 hw_loss 0.309928 lr 0.00045902 rank 6
2023-02-23 04:20:08,442 DEBUG TRAIN Batch 14/2000 loss 9.773724 loss_att 12.258764 loss_ctc 12.726330 loss_rnnt 8.662497 hw_loss 0.413508 lr 0.00045882 rank 6
2023-02-23 04:20:08,444 DEBUG TRAIN Batch 14/2000 loss 6.761717 loss_att 12.295033 loss_ctc 11.893485 loss_rnnt 4.773726 hw_loss 0.369546 lr 0.00045874 rank 3
2023-02-23 04:20:08,446 DEBUG TRAIN Batch 14/2000 loss 8.915941 loss_att 14.196686 loss_ctc 10.529178 loss_rnnt 7.472106 hw_loss 0.323602 lr 0.00045877 rank 2
2023-02-23 04:20:08,450 DEBUG TRAIN Batch 14/2000 loss 15.377990 loss_att 18.135973 loss_ctc 21.035542 loss_rnnt 13.879889 hw_loss 0.360306 lr 0.00045886 rank 1
2023-02-23 04:20:08,458 DEBUG TRAIN Batch 14/2000 loss 8.792443 loss_att 10.744027 loss_ctc 14.762469 loss_rnnt 7.321149 hw_loss 0.534326 lr 0.00045874 rank 7
2023-02-23 04:20:08,480 DEBUG TRAIN Batch 14/2000 loss 13.904441 loss_att 15.949051 loss_ctc 22.123276 loss_rnnt 12.171595 hw_loss 0.427646 lr 0.00045877 rank 5
2023-02-23 04:20:08,488 DEBUG TRAIN Batch 14/2000 loss 7.299830 loss_att 13.612579 loss_ctc 9.271912 loss_rnnt 5.572790 hw_loss 0.377900 lr 0.00045876 rank 0
2023-02-23 04:20:08,502 DEBUG TRAIN Batch 14/2000 loss 20.658270 loss_att 23.563475 loss_ctc 23.573593 loss_rnnt 19.502098 hw_loss 0.349542 lr 0.00045875 rank 4
2023-02-23 04:21:22,578 DEBUG TRAIN Batch 14/2100 loss 5.497910 loss_att 10.056809 loss_ctc 9.188028 loss_rnnt 3.854824 hw_loss 0.448670 lr 0.00045855 rank 3
2023-02-23 04:21:22,579 DEBUG TRAIN Batch 14/2100 loss 11.505169 loss_att 14.037598 loss_ctc 15.978146 loss_rnnt 10.221830 hw_loss 0.338355 lr 0.00045855 rank 7
2023-02-23 04:21:22,581 DEBUG TRAIN Batch 14/2100 loss 19.173111 loss_att 20.579643 loss_ctc 23.877502 loss_rnnt 18.053961 hw_loss 0.394858 lr 0.00045863 rank 6
2023-02-23 04:21:22,581 DEBUG TRAIN Batch 14/2100 loss 7.279031 loss_att 12.735203 loss_ctc 10.891536 loss_rnnt 5.513591 hw_loss 0.361009 lr 0.00045856 rank 4
2023-02-23 04:21:22,582 DEBUG TRAIN Batch 14/2100 loss 13.653713 loss_att 18.680326 loss_ctc 17.182014 loss_rnnt 11.962979 hw_loss 0.403071 lr 0.00045858 rank 2
2023-02-23 04:21:22,583 DEBUG TRAIN Batch 14/2100 loss 7.351003 loss_att 13.113251 loss_ctc 9.496942 loss_rnnt 5.567611 hw_loss 0.646532 lr 0.00045857 rank 0
2023-02-23 04:21:22,602 DEBUG TRAIN Batch 14/2100 loss 14.094949 loss_att 14.546926 loss_ctc 15.140751 loss_rnnt 13.615835 hw_loss 0.467395 lr 0.00045866 rank 1
2023-02-23 04:21:22,649 DEBUG TRAIN Batch 14/2100 loss 11.930018 loss_att 14.923633 loss_ctc 13.872417 loss_rnnt 10.835651 hw_loss 0.443732 lr 0.00045857 rank 5
2023-02-23 04:22:36,266 DEBUG TRAIN Batch 14/2200 loss 2.713850 loss_att 7.367635 loss_ctc 5.233437 loss_rnnt 1.187929 hw_loss 0.486034 lr 0.00045836 rank 3
2023-02-23 04:22:36,267 DEBUG TRAIN Batch 14/2200 loss 14.784177 loss_att 20.993803 loss_ctc 18.941591 loss_rnnt 12.801788 hw_loss 0.349015 lr 0.00045836 rank 7
2023-02-23 04:22:36,267 DEBUG TRAIN Batch 14/2200 loss 10.904349 loss_att 13.757295 loss_ctc 13.128874 loss_rnnt 9.879723 hw_loss 0.295189 lr 0.00045838 rank 2
2023-02-23 04:22:36,270 DEBUG TRAIN Batch 14/2200 loss 10.042063 loss_att 14.956762 loss_ctc 11.246876 loss_rnnt 8.653540 hw_loss 0.459266 lr 0.00045847 rank 1
2023-02-23 04:22:36,272 DEBUG TRAIN Batch 14/2200 loss 11.590069 loss_att 17.788807 loss_ctc 17.346516 loss_rnnt 9.337254 hw_loss 0.460392 lr 0.00045837 rank 4
2023-02-23 04:22:36,273 DEBUG TRAIN Batch 14/2200 loss 8.811664 loss_att 18.886631 loss_ctc 13.704201 loss_rnnt 5.947994 hw_loss 0.368132 lr 0.00045838 rank 5
2023-02-23 04:22:36,302 DEBUG TRAIN Batch 14/2200 loss 7.526860 loss_att 11.632129 loss_ctc 11.025530 loss_rnnt 6.038258 hw_loss 0.376984 lr 0.00045844 rank 6
2023-02-23 04:22:36,311 DEBUG TRAIN Batch 14/2200 loss 15.659557 loss_att 21.625668 loss_ctc 21.847485 loss_rnnt 13.442112 hw_loss 0.373435 lr 0.00045837 rank 0
2023-02-23 04:23:48,340 DEBUG TRAIN Batch 14/2300 loss 4.973950 loss_att 8.806487 loss_ctc 7.651581 loss_rnnt 3.656368 hw_loss 0.363858 lr 0.00045816 rank 3
2023-02-23 04:23:48,359 DEBUG TRAIN Batch 14/2300 loss 14.796118 loss_att 18.596802 loss_ctc 17.943047 loss_rnnt 13.393532 hw_loss 0.417859 lr 0.00045816 rank 7
2023-02-23 04:23:48,360 DEBUG TRAIN Batch 14/2300 loss 13.591741 loss_att 18.681135 loss_ctc 16.643049 loss_rnnt 11.980518 hw_loss 0.349691 lr 0.00045828 rank 1
2023-02-23 04:23:48,361 DEBUG TRAIN Batch 14/2300 loss 20.901949 loss_att 26.573931 loss_ctc 26.543324 loss_rnnt 18.779739 hw_loss 0.441805 lr 0.00045818 rank 0
2023-02-23 04:23:48,360 DEBUG TRAIN Batch 14/2300 loss 14.783737 loss_att 17.527082 loss_ctc 18.078632 loss_rnnt 13.590302 hw_loss 0.385213 lr 0.00045817 rank 4
2023-02-23 04:23:48,360 DEBUG TRAIN Batch 14/2300 loss 11.833159 loss_att 14.396894 loss_ctc 15.994638 loss_rnnt 10.544128 hw_loss 0.415162 lr 0.00045819 rank 2
2023-02-23 04:23:48,366 DEBUG TRAIN Batch 14/2300 loss 3.718045 loss_att 7.051579 loss_ctc 5.400200 loss_rnnt 2.593748 hw_loss 0.437442 lr 0.00045819 rank 5
2023-02-23 04:23:48,404 DEBUG TRAIN Batch 14/2300 loss 13.135365 loss_att 16.391691 loss_ctc 13.655791 loss_rnnt 12.187548 hw_loss 0.425926 lr 0.00045825 rank 6
2023-02-23 04:25:00,842 DEBUG TRAIN Batch 14/2400 loss 6.026805 loss_att 7.742106 loss_ctc 7.918771 loss_rnnt 5.241263 hw_loss 0.356662 lr 0.00045800 rank 2
2023-02-23 04:25:00,842 DEBUG TRAIN Batch 14/2400 loss 11.479383 loss_att 13.633580 loss_ctc 15.633480 loss_rnnt 10.267554 hw_loss 0.425831 lr 0.00045798 rank 4
2023-02-23 04:25:00,848 DEBUG TRAIN Batch 14/2400 loss 6.572578 loss_att 10.576290 loss_ctc 7.123246 loss_rnnt 5.482164 hw_loss 0.405468 lr 0.00045809 rank 1
2023-02-23 04:25:00,850 DEBUG TRAIN Batch 14/2400 loss 5.552166 loss_att 8.244308 loss_ctc 9.721847 loss_rnnt 4.231240 hw_loss 0.424762 lr 0.00045799 rank 0
2023-02-23 04:25:00,851 DEBUG TRAIN Batch 14/2400 loss 11.983573 loss_att 15.181550 loss_ctc 13.501447 loss_rnnt 10.909315 hw_loss 0.435527 lr 0.00045797 rank 3
2023-02-23 04:25:00,854 DEBUG TRAIN Batch 14/2400 loss 8.790362 loss_att 10.401429 loss_ctc 10.944425 loss_rnnt 7.975868 hw_loss 0.384512 lr 0.00045797 rank 7
2023-02-23 04:25:00,855 DEBUG TRAIN Batch 14/2400 loss 14.613749 loss_att 15.370665 loss_ctc 18.735441 loss_rnnt 13.716623 hw_loss 0.367842 lr 0.00045805 rank 6
2023-02-23 04:25:00,859 DEBUG TRAIN Batch 14/2400 loss 12.227801 loss_att 12.778300 loss_ctc 14.402637 loss_rnnt 11.596877 hw_loss 0.432837 lr 0.00045800 rank 5
2023-02-23 04:26:16,466 DEBUG TRAIN Batch 14/2500 loss 19.226288 loss_att 20.428196 loss_ctc 27.033714 loss_rnnt 17.687046 hw_loss 0.483503 lr 0.00045778 rank 7
2023-02-23 04:26:16,470 DEBUG TRAIN Batch 14/2500 loss 14.729943 loss_att 16.486506 loss_ctc 15.966132 loss_rnnt 14.003790 hw_loss 0.393781 lr 0.00045781 rank 2
2023-02-23 04:26:16,471 DEBUG TRAIN Batch 14/2500 loss 6.795594 loss_att 8.778963 loss_ctc 7.934461 loss_rnnt 5.956838 hw_loss 0.544188 lr 0.00045778 rank 3
2023-02-23 04:26:16,472 DEBUG TRAIN Batch 14/2500 loss 12.158868 loss_att 11.531724 loss_ctc 12.881991 loss_rnnt 11.906608 hw_loss 0.527387 lr 0.00045789 rank 1
2023-02-23 04:26:16,472 DEBUG TRAIN Batch 14/2500 loss 8.902475 loss_att 11.079768 loss_ctc 12.996387 loss_rnnt 7.561807 hw_loss 0.673789 lr 0.00045779 rank 4
2023-02-23 04:26:16,472 DEBUG TRAIN Batch 14/2500 loss 11.941293 loss_att 11.168502 loss_ctc 16.028503 loss_rnnt 11.184507 hw_loss 0.686966 lr 0.00045786 rank 6
2023-02-23 04:26:16,481 DEBUG TRAIN Batch 14/2500 loss 8.338568 loss_att 10.556073 loss_ctc 11.168006 loss_rnnt 7.185146 hw_loss 0.623740 lr 0.00045780 rank 0
2023-02-23 04:26:16,520 DEBUG TRAIN Batch 14/2500 loss 4.939707 loss_att 8.950790 loss_ctc 5.740893 loss_rnnt 3.880370 hw_loss 0.281803 lr 0.00045781 rank 5
2023-02-23 04:27:28,713 DEBUG TRAIN Batch 14/2600 loss 9.657304 loss_att 15.501366 loss_ctc 11.353183 loss_rnnt 8.064215 hw_loss 0.371550 lr 0.00045762 rank 2
2023-02-23 04:27:28,715 DEBUG TRAIN Batch 14/2600 loss 6.302685 loss_att 9.812377 loss_ctc 10.490032 loss_rnnt 4.807085 hw_loss 0.441278 lr 0.00045759 rank 7
2023-02-23 04:27:28,717 DEBUG TRAIN Batch 14/2600 loss 6.081446 loss_att 10.820986 loss_ctc 11.489874 loss_rnnt 4.180976 hw_loss 0.433945 lr 0.00045770 rank 1
2023-02-23 04:27:28,718 DEBUG TRAIN Batch 14/2600 loss 8.212308 loss_att 11.371900 loss_ctc 10.683555 loss_rnnt 7.046653 hw_loss 0.382945 lr 0.00045761 rank 0
2023-02-23 04:27:28,719 DEBUG TRAIN Batch 14/2600 loss 7.552269 loss_att 7.947539 loss_ctc 12.366056 loss_rnnt 6.510490 hw_loss 0.601664 lr 0.00045759 rank 3
2023-02-23 04:27:28,721 DEBUG TRAIN Batch 14/2600 loss 20.863377 loss_att 26.353956 loss_ctc 27.217529 loss_rnnt 18.744797 hw_loss 0.324832 lr 0.00045761 rank 5
2023-02-23 04:27:28,721 DEBUG TRAIN Batch 14/2600 loss 13.947946 loss_att 18.180260 loss_ctc 19.545841 loss_rnnt 12.128619 hw_loss 0.424645 lr 0.00045760 rank 4
2023-02-23 04:27:28,722 DEBUG TRAIN Batch 14/2600 loss 33.798809 loss_att 35.447067 loss_ctc 54.634884 loss_rnnt 30.476524 hw_loss 0.402177 lr 0.00045767 rank 6
2023-02-23 04:28:41,603 DEBUG TRAIN Batch 14/2700 loss 13.294533 loss_att 15.126262 loss_ctc 16.656572 loss_rnnt 12.270418 hw_loss 0.392807 lr 0.00045740 rank 3
2023-02-23 04:28:41,603 DEBUG TRAIN Batch 14/2700 loss 9.242339 loss_att 14.556971 loss_ctc 10.905677 loss_rnnt 7.734299 hw_loss 0.418757 lr 0.00045740 rank 7
2023-02-23 04:28:41,605 DEBUG TRAIN Batch 14/2700 loss 18.883266 loss_att 21.718979 loss_ctc 27.279602 loss_rnnt 16.998228 hw_loss 0.371970 lr 0.00045742 rank 2
2023-02-23 04:28:41,606 DEBUG TRAIN Batch 14/2700 loss 29.289110 loss_att 29.152363 loss_ctc 44.538879 loss_rnnt 27.057983 hw_loss 0.422201 lr 0.00045742 rank 5
2023-02-23 04:28:41,607 DEBUG TRAIN Batch 14/2700 loss 10.198982 loss_att 13.006362 loss_ctc 11.473761 loss_rnnt 9.274528 hw_loss 0.361891 lr 0.00045748 rank 6
2023-02-23 04:28:41,610 DEBUG TRAIN Batch 14/2700 loss 5.664304 loss_att 11.911062 loss_ctc 6.956540 loss_rnnt 4.040599 hw_loss 0.378854 lr 0.00045741 rank 0
2023-02-23 04:28:41,610 DEBUG TRAIN Batch 14/2700 loss 5.050585 loss_att 9.227164 loss_ctc 8.446982 loss_rnnt 3.520374 hw_loss 0.453831 lr 0.00045741 rank 4
2023-02-23 04:28:41,611 DEBUG TRAIN Batch 14/2700 loss 3.218063 loss_att 6.678473 loss_ctc 4.984818 loss_rnnt 2.099002 hw_loss 0.358897 lr 0.00045751 rank 1
2023-02-23 04:29:55,710 DEBUG TRAIN Batch 14/2800 loss 24.686857 loss_att 26.681618 loss_ctc 31.446648 loss_rnnt 23.153978 hw_loss 0.436163 lr 0.00045722 rank 4
2023-02-23 04:29:55,714 DEBUG TRAIN Batch 14/2800 loss 20.954323 loss_att 26.677429 loss_ctc 27.573952 loss_rnnt 18.753565 hw_loss 0.325344 lr 0.00045720 rank 7
2023-02-23 04:29:55,716 DEBUG TRAIN Batch 14/2800 loss 6.429727 loss_att 8.959764 loss_ctc 8.681435 loss_rnnt 5.396173 hw_loss 0.426221 lr 0.00045723 rank 5
2023-02-23 04:29:55,719 DEBUG TRAIN Batch 14/2800 loss 17.444277 loss_att 20.754398 loss_ctc 21.328907 loss_rnnt 16.071117 hw_loss 0.362218 lr 0.00045732 rank 1
2023-02-23 04:29:55,725 DEBUG TRAIN Batch 14/2800 loss 17.677719 loss_att 20.537710 loss_ctc 22.952759 loss_rnnt 16.166954 hw_loss 0.441428 lr 0.00045720 rank 3
2023-02-23 04:29:55,726 DEBUG TRAIN Batch 14/2800 loss 19.807594 loss_att 23.053949 loss_ctc 27.666294 loss_rnnt 17.891768 hw_loss 0.410120 lr 0.00045722 rank 0
2023-02-23 04:29:55,729 DEBUG TRAIN Batch 14/2800 loss 19.043720 loss_att 16.572296 loss_ctc 21.874763 loss_rnnt 18.943642 hw_loss 0.406668 lr 0.00045723 rank 2
2023-02-23 04:29:55,765 DEBUG TRAIN Batch 14/2800 loss 18.853107 loss_att 23.204973 loss_ctc 21.892990 loss_rnnt 17.389971 hw_loss 0.351460 lr 0.00045729 rank 6
2023-02-23 04:31:08,789 DEBUG TRAIN Batch 14/2900 loss 12.347792 loss_att 16.492380 loss_ctc 13.106148 loss_rnnt 11.216368 hw_loss 0.377612 lr 0.00045701 rank 3
2023-02-23 04:31:08,790 DEBUG TRAIN Batch 14/2900 loss 12.194166 loss_att 12.857713 loss_ctc 14.463137 loss_rnnt 11.529920 hw_loss 0.429389 lr 0.00045701 rank 7
2023-02-23 04:31:08,792 DEBUG TRAIN Batch 14/2900 loss 9.885013 loss_att 13.728651 loss_ctc 10.068520 loss_rnnt 8.849777 hw_loss 0.453824 lr 0.00045704 rank 2
2023-02-23 04:31:08,795 DEBUG TRAIN Batch 14/2900 loss 18.383310 loss_att 19.924206 loss_ctc 24.686161 loss_rnnt 17.007648 hw_loss 0.425814 lr 0.00045713 rank 1
2023-02-23 04:31:08,795 DEBUG TRAIN Batch 14/2900 loss 8.198760 loss_att 12.686930 loss_ctc 10.443412 loss_rnnt 6.778756 hw_loss 0.418280 lr 0.00045709 rank 6
2023-02-23 04:31:08,797 DEBUG TRAIN Batch 14/2900 loss 15.722696 loss_att 18.559422 loss_ctc 19.621977 loss_rnnt 14.439057 hw_loss 0.368229 lr 0.00045703 rank 0
2023-02-23 04:31:08,801 DEBUG TRAIN Batch 14/2900 loss 9.446532 loss_att 16.751816 loss_ctc 15.079132 loss_rnnt 7.035041 hw_loss 0.373915 lr 0.00045704 rank 5
2023-02-23 04:31:08,802 DEBUG TRAIN Batch 14/2900 loss 12.853950 loss_att 16.249893 loss_ctc 15.714510 loss_rnnt 11.558849 hw_loss 0.439694 lr 0.00045702 rank 4
2023-02-23 04:32:21,142 DEBUG TRAIN Batch 14/3000 loss 12.737277 loss_att 16.887474 loss_ctc 14.818623 loss_rnnt 11.403602 hw_loss 0.423982 lr 0.00045682 rank 3
2023-02-23 04:32:21,147 DEBUG TRAIN Batch 14/3000 loss 21.786486 loss_att 24.362293 loss_ctc 31.829901 loss_rnnt 19.698502 hw_loss 0.438185 lr 0.00045685 rank 5
2023-02-23 04:32:21,147 DEBUG TRAIN Batch 14/3000 loss 13.492848 loss_att 15.460648 loss_ctc 19.223707 loss_rnnt 12.149736 hw_loss 0.347694 lr 0.00045682 rank 7
2023-02-23 04:32:21,148 DEBUG TRAIN Batch 14/3000 loss 11.398955 loss_att 14.462421 loss_ctc 14.633831 loss_rnnt 10.109510 hw_loss 0.460191 lr 0.00045694 rank 1
2023-02-23 04:32:21,149 DEBUG TRAIN Batch 14/3000 loss 12.037058 loss_att 14.851384 loss_ctc 14.252261 loss_rnnt 10.940699 hw_loss 0.446498 lr 0.00045690 rank 6
2023-02-23 04:32:21,151 DEBUG TRAIN Batch 14/3000 loss 8.293791 loss_att 11.569723 loss_ctc 12.226034 loss_rnnt 6.918391 hw_loss 0.367340 lr 0.00045685 rank 2
2023-02-23 04:32:21,154 DEBUG TRAIN Batch 14/3000 loss 12.200430 loss_att 15.189974 loss_ctc 18.306704 loss_rnnt 10.560089 hw_loss 0.427989 lr 0.00045683 rank 4
2023-02-23 04:32:21,154 DEBUG TRAIN Batch 14/3000 loss 19.102837 loss_att 22.877535 loss_ctc 26.048870 loss_rnnt 17.221468 hw_loss 0.375544 lr 0.00045684 rank 0
2023-02-23 04:33:33,701 DEBUG TRAIN Batch 14/3100 loss 7.405114 loss_att 10.234927 loss_ctc 8.973934 loss_rnnt 6.395327 hw_loss 0.439965 lr 0.00045665 rank 0
2023-02-23 04:33:33,702 DEBUG TRAIN Batch 14/3100 loss 6.724133 loss_att 6.679191 loss_ctc 10.270274 loss_rnnt 5.994561 hw_loss 0.498265 lr 0.00045666 rank 5
2023-02-23 04:33:33,704 DEBUG TRAIN Batch 14/3100 loss 9.651562 loss_att 11.878589 loss_ctc 11.931446 loss_rnnt 8.675755 hw_loss 0.424533 lr 0.00045664 rank 4
2023-02-23 04:33:33,707 DEBUG TRAIN Batch 14/3100 loss 9.354561 loss_att 14.094652 loss_ctc 16.440407 loss_rnnt 7.195701 hw_loss 0.498867 lr 0.00045675 rank 1
2023-02-23 04:33:33,707 DEBUG TRAIN Batch 14/3100 loss 12.284536 loss_att 13.573303 loss_ctc 14.758858 loss_rnnt 11.397879 hw_loss 0.560617 lr 0.00045666 rank 2
2023-02-23 04:33:33,710 DEBUG TRAIN Batch 14/3100 loss 21.618227 loss_att 26.009344 loss_ctc 27.139618 loss_rnnt 19.763508 hw_loss 0.450581 lr 0.00045663 rank 3
2023-02-23 04:33:33,712 DEBUG TRAIN Batch 14/3100 loss 12.804155 loss_att 17.666460 loss_ctc 18.354074 loss_rnnt 10.873102 hw_loss 0.409881 lr 0.00045663 rank 7
2023-02-23 04:33:33,751 DEBUG TRAIN Batch 14/3100 loss 15.121475 loss_att 16.962368 loss_ctc 20.647816 loss_rnnt 13.702701 hw_loss 0.588285 lr 0.00045671 rank 6
2023-02-23 04:34:48,681 DEBUG TRAIN Batch 14/3200 loss 18.816837 loss_att 27.773857 loss_ctc 29.802486 loss_rnnt 15.363525 hw_loss 0.369662 lr 0.00045647 rank 2
2023-02-23 04:34:48,684 DEBUG TRAIN Batch 14/3200 loss 22.586433 loss_att 29.518501 loss_ctc 35.484291 loss_rnnt 19.286110 hw_loss 0.364113 lr 0.00045644 rank 7
2023-02-23 04:34:48,689 DEBUG TRAIN Batch 14/3200 loss 20.713181 loss_att 21.417131 loss_ctc 23.464376 loss_rnnt 19.985737 hw_loss 0.412176 lr 0.00045644 rank 3
2023-02-23 04:34:48,690 DEBUG TRAIN Batch 14/3200 loss 13.777041 loss_att 14.567533 loss_ctc 14.631068 loss_rnnt 13.344687 hw_loss 0.300725 lr 0.00045646 rank 0
2023-02-23 04:34:48,694 DEBUG TRAIN Batch 14/3200 loss 8.563188 loss_att 11.003841 loss_ctc 12.177344 loss_rnnt 7.344214 hw_loss 0.466791 lr 0.00045652 rank 6
2023-02-23 04:34:48,697 DEBUG TRAIN Batch 14/3200 loss 12.861399 loss_att 16.724407 loss_ctc 15.881377 loss_rnnt 11.472789 hw_loss 0.400020 lr 0.00045647 rank 5
2023-02-23 04:34:48,699 DEBUG TRAIN Batch 14/3200 loss 14.432825 loss_att 14.996916 loss_ctc 18.795910 loss_rnnt 13.511027 hw_loss 0.426065 lr 0.00045656 rank 1
2023-02-23 04:34:48,699 DEBUG TRAIN Batch 14/3200 loss 10.836897 loss_att 14.050777 loss_ctc 14.954139 loss_rnnt 9.455296 hw_loss 0.355988 lr 0.00045645 rank 4
2023-02-23 04:36:01,353 DEBUG TRAIN Batch 14/3300 loss 10.605926 loss_att 13.605091 loss_ctc 15.490374 loss_rnnt 9.159159 hw_loss 0.366887 lr 0.00045625 rank 7
2023-02-23 04:36:01,354 DEBUG TRAIN Batch 14/3300 loss 12.103279 loss_att 16.029680 loss_ctc 15.755599 loss_rnnt 10.618575 hw_loss 0.398340 lr 0.00045626 rank 4
2023-02-23 04:36:01,354 DEBUG TRAIN Batch 14/3300 loss 9.691419 loss_att 14.558928 loss_ctc 13.109096 loss_rnnt 8.064920 hw_loss 0.369946 lr 0.00045625 rank 3
2023-02-23 04:36:01,356 DEBUG TRAIN Batch 14/3300 loss 14.550242 loss_att 15.492329 loss_ctc 20.558598 loss_rnnt 13.356143 hw_loss 0.383566 lr 0.00045627 rank 0
2023-02-23 04:36:01,359 DEBUG TRAIN Batch 14/3300 loss 8.062849 loss_att 8.540434 loss_ctc 6.485418 loss_rnnt 7.946599 hw_loss 0.433233 lr 0.00045628 rank 5
2023-02-23 04:36:01,359 DEBUG TRAIN Batch 14/3300 loss 9.307920 loss_att 13.982706 loss_ctc 9.863915 loss_rnnt 8.097634 hw_loss 0.377240 lr 0.00045628 rank 2
2023-02-23 04:36:01,361 DEBUG TRAIN Batch 14/3300 loss 15.106287 loss_att 15.297098 loss_ctc 20.751261 loss_rnnt 14.100551 hw_loss 0.402955 lr 0.00045633 rank 6
2023-02-23 04:36:01,364 DEBUG TRAIN Batch 14/3300 loss 12.918121 loss_att 17.076149 loss_ctc 16.724102 loss_rnnt 11.391528 hw_loss 0.351608 lr 0.00045637 rank 1
2023-02-23 04:37:14,260 DEBUG TRAIN Batch 14/3400 loss 9.050287 loss_att 14.395082 loss_ctc 15.179550 loss_rnnt 6.985485 hw_loss 0.334891 lr 0.00045606 rank 7
2023-02-23 04:37:14,262 DEBUG TRAIN Batch 14/3400 loss 11.125073 loss_att 17.118076 loss_ctc 15.409747 loss_rnnt 9.176060 hw_loss 0.335855 lr 0.00045609 rank 2
2023-02-23 04:37:14,265 DEBUG TRAIN Batch 14/3400 loss 24.620609 loss_att 25.344151 loss_ctc 26.032194 loss_rnnt 24.083786 hw_loss 0.382320 lr 0.00045618 rank 1
2023-02-23 04:37:14,265 DEBUG TRAIN Batch 14/3400 loss 13.718152 loss_att 16.991264 loss_ctc 19.574295 loss_rnnt 12.072966 hw_loss 0.393270 lr 0.00045609 rank 5
2023-02-23 04:37:14,266 DEBUG TRAIN Batch 14/3400 loss 11.009687 loss_att 14.529987 loss_ctc 13.808441 loss_rnnt 9.737357 hw_loss 0.365819 lr 0.00045606 rank 3
2023-02-23 04:37:14,268 DEBUG TRAIN Batch 14/3400 loss 8.982333 loss_att 10.771837 loss_ctc 9.190722 loss_rnnt 8.369392 hw_loss 0.426101 lr 0.00045608 rank 0
2023-02-23 04:37:14,272 DEBUG TRAIN Batch 14/3400 loss 12.480533 loss_att 18.448692 loss_ctc 14.588613 loss_rnnt 10.811928 hw_loss 0.363552 lr 0.00045614 rank 6
2023-02-23 04:37:14,316 DEBUG TRAIN Batch 14/3400 loss 19.784536 loss_att 21.279177 loss_ctc 25.510437 loss_rnnt 18.517296 hw_loss 0.384108 lr 0.00045607 rank 4
2023-02-23 04:38:27,801 DEBUG TRAIN Batch 14/3500 loss 4.475794 loss_att 6.868021 loss_ctc 5.827354 loss_rnnt 3.605661 hw_loss 0.396525 lr 0.00045589 rank 0
2023-02-23 04:38:27,812 DEBUG TRAIN Batch 14/3500 loss 10.408473 loss_att 12.063026 loss_ctc 13.841771 loss_rnnt 9.447735 hw_loss 0.322602 lr 0.00045599 rank 1
2023-02-23 04:38:27,812 DEBUG TRAIN Batch 14/3500 loss 7.137428 loss_att 10.464228 loss_ctc 7.591971 loss_rnnt 6.207327 hw_loss 0.382751 lr 0.00045590 rank 2
2023-02-23 04:38:27,813 DEBUG TRAIN Batch 14/3500 loss 11.757695 loss_att 15.708284 loss_ctc 21.011814 loss_rnnt 9.537444 hw_loss 0.367969 lr 0.00045587 rank 3
2023-02-23 04:38:27,814 DEBUG TRAIN Batch 14/3500 loss 10.579843 loss_att 10.556709 loss_ctc 11.891670 loss_rnnt 10.188622 hw_loss 0.414257 lr 0.00045587 rank 7
2023-02-23 04:38:27,814 DEBUG TRAIN Batch 14/3500 loss 14.264391 loss_att 19.978706 loss_ctc 18.067480 loss_rnnt 12.418257 hw_loss 0.367861 lr 0.00045590 rank 5
2023-02-23 04:38:27,817 DEBUG TRAIN Batch 14/3500 loss 11.243147 loss_att 15.788176 loss_ctc 13.669144 loss_rnnt 9.813865 hw_loss 0.369021 lr 0.00045588 rank 4
2023-02-23 04:38:27,824 DEBUG TRAIN Batch 14/3500 loss 12.076949 loss_att 15.692303 loss_ctc 14.982942 loss_rnnt 10.728027 hw_loss 0.446972 lr 0.00045595 rank 6
2023-02-23 04:39:41,950 DEBUG TRAIN Batch 14/3600 loss 8.636859 loss_att 11.049467 loss_ctc 9.985404 loss_rnnt 7.732833 hw_loss 0.453183 lr 0.00045568 rank 7
2023-02-23 04:39:41,951 DEBUG TRAIN Batch 14/3600 loss 7.279066 loss_att 12.490353 loss_ctc 11.293636 loss_rnnt 5.486085 hw_loss 0.403964 lr 0.00045568 rank 3
2023-02-23 04:39:41,953 DEBUG TRAIN Batch 14/3600 loss 4.624298 loss_att 7.399605 loss_ctc 6.717911 loss_rnnt 3.539176 hw_loss 0.470459 lr 0.00045571 rank 2
2023-02-23 04:39:41,954 DEBUG TRAIN Batch 14/3600 loss 20.696020 loss_att 22.119219 loss_ctc 29.237747 loss_rnnt 19.057028 hw_loss 0.403980 lr 0.00045576 rank 6
2023-02-23 04:39:41,954 DEBUG TRAIN Batch 14/3600 loss 17.774387 loss_att 18.917765 loss_ctc 19.645681 loss_rnnt 17.069527 hw_loss 0.425025 lr 0.00045571 rank 5
2023-02-23 04:39:41,958 DEBUG TRAIN Batch 14/3600 loss 16.709599 loss_att 20.735853 loss_ctc 21.791998 loss_rnnt 14.974577 hw_loss 0.472722 lr 0.00045580 rank 1
2023-02-23 04:39:41,961 DEBUG TRAIN Batch 14/3600 loss 16.063591 loss_att 17.600445 loss_ctc 19.002621 loss_rnnt 15.162075 hw_loss 0.379262 lr 0.00045569 rank 4
2023-02-23 04:39:41,987 DEBUG TRAIN Batch 14/3600 loss 28.498686 loss_att 28.967617 loss_ctc 38.502739 loss_rnnt 26.895386 hw_loss 0.329327 lr 0.00045570 rank 0
2023-02-23 04:40:54,523 DEBUG TRAIN Batch 14/3700 loss 16.649939 loss_att 18.875034 loss_ctc 20.439177 loss_rnnt 15.466736 hw_loss 0.436783 lr 0.00045549 rank 7
2023-02-23 04:40:54,529 DEBUG TRAIN Batch 14/3700 loss 18.086603 loss_att 18.488222 loss_ctc 21.620741 loss_rnnt 17.308599 hw_loss 0.424620 lr 0.00045561 rank 1
2023-02-23 04:40:54,530 DEBUG TRAIN Batch 14/3700 loss 11.600163 loss_att 12.031246 loss_ctc 14.267372 loss_rnnt 10.876657 hw_loss 0.528113 lr 0.00045552 rank 5
2023-02-23 04:40:54,534 DEBUG TRAIN Batch 14/3700 loss 2.946050 loss_att 6.714583 loss_ctc 4.406596 loss_rnnt 1.733928 hw_loss 0.494392 lr 0.00045549 rank 3
2023-02-23 04:40:54,537 DEBUG TRAIN Batch 14/3700 loss 7.323982 loss_att 9.821238 loss_ctc 9.886914 loss_rnnt 6.249811 hw_loss 0.436868 lr 0.00045551 rank 0
2023-02-23 04:40:54,539 DEBUG TRAIN Batch 14/3700 loss 9.270748 loss_att 10.335105 loss_ctc 14.075789 loss_rnnt 8.173252 hw_loss 0.457411 lr 0.00045557 rank 6
2023-02-23 04:40:54,546 DEBUG TRAIN Batch 14/3700 loss 21.355312 loss_att 22.325111 loss_ctc 27.243179 loss_rnnt 20.116398 hw_loss 0.487323 lr 0.00045552 rank 2
2023-02-23 04:40:54,585 DEBUG TRAIN Batch 14/3700 loss 5.250234 loss_att 8.060551 loss_ctc 6.459036 loss_rnnt 4.280565 hw_loss 0.462061 lr 0.00045550 rank 4
2023-02-23 04:42:06,932 DEBUG TRAIN Batch 14/3800 loss 16.289261 loss_att 17.779907 loss_ctc 20.884584 loss_rnnt 15.169639 hw_loss 0.391470 lr 0.00045530 rank 3
2023-02-23 04:42:06,932 DEBUG TRAIN Batch 14/3800 loss 9.663346 loss_att 10.052541 loss_ctc 11.428491 loss_rnnt 9.105587 hw_loss 0.458564 lr 0.00045532 rank 4
2023-02-23 04:42:06,933 DEBUG TRAIN Batch 14/3800 loss 15.119135 loss_att 14.782636 loss_ctc 21.692015 loss_rnnt 14.033709 hw_loss 0.518139 lr 0.00045542 rank 1
2023-02-23 04:42:06,935 DEBUG TRAIN Batch 14/3800 loss 14.105511 loss_att 15.234138 loss_ctc 18.295486 loss_rnnt 13.034373 hw_loss 0.537652 lr 0.00045533 rank 2
2023-02-23 04:42:06,934 DEBUG TRAIN Batch 14/3800 loss 13.741714 loss_att 16.652987 loss_ctc 16.616608 loss_rnnt 12.635155 hw_loss 0.264346 lr 0.00045533 rank 5
2023-02-23 04:42:06,939 DEBUG TRAIN Batch 14/3800 loss 26.581301 loss_att 27.632952 loss_ctc 33.414917 loss_rnnt 25.270611 hw_loss 0.354769 lr 0.00045530 rank 7
2023-02-23 04:42:06,940 DEBUG TRAIN Batch 14/3800 loss 14.717494 loss_att 22.493942 loss_ctc 19.723322 loss_rnnt 12.313210 hw_loss 0.340410 lr 0.00045539 rank 6
2023-02-23 04:42:06,942 DEBUG TRAIN Batch 14/3800 loss 11.703005 loss_att 12.592207 loss_ctc 15.025139 loss_rnnt 10.726637 hw_loss 0.666706 lr 0.00045532 rank 0
2023-02-23 04:43:21,677 DEBUG TRAIN Batch 14/3900 loss 14.624276 loss_att 19.163776 loss_ctc 18.769245 loss_rnnt 12.931666 hw_loss 0.435088 lr 0.00045514 rank 2
2023-02-23 04:43:21,678 DEBUG TRAIN Batch 14/3900 loss 12.734838 loss_att 13.681437 loss_ctc 18.108618 loss_rnnt 11.605128 hw_loss 0.419786 lr 0.00045514 rank 5
2023-02-23 04:43:21,681 DEBUG TRAIN Batch 14/3900 loss 7.920315 loss_att 12.254629 loss_ctc 9.151844 loss_rnnt 6.661075 hw_loss 0.427827 lr 0.00045513 rank 4
2023-02-23 04:43:21,689 DEBUG TRAIN Batch 14/3900 loss 6.333439 loss_att 13.109816 loss_ctc 10.441917 loss_rnnt 4.222973 hw_loss 0.388862 lr 0.00045512 rank 7
2023-02-23 04:43:21,691 DEBUG TRAIN Batch 14/3900 loss 7.874898 loss_att 7.726808 loss_ctc 10.095080 loss_rnnt 7.305728 hw_loss 0.567683 lr 0.00045512 rank 3
2023-02-23 04:43:21,697 DEBUG TRAIN Batch 14/3900 loss 12.523301 loss_att 14.039905 loss_ctc 14.202251 loss_rnnt 11.784239 hw_loss 0.397280 lr 0.00045520 rank 6
2023-02-23 04:43:21,710 DEBUG TRAIN Batch 14/3900 loss 3.186146 loss_att 8.234642 loss_ctc 4.879615 loss_rnnt 1.754318 hw_loss 0.368125 lr 0.00045523 rank 1
2023-02-23 04:43:21,722 DEBUG TRAIN Batch 14/3900 loss 10.190969 loss_att 13.939379 loss_ctc 11.069263 loss_rnnt 9.082375 hw_loss 0.453387 lr 0.00045513 rank 0
2023-02-23 04:44:35,693 DEBUG TRAIN Batch 14/4000 loss 5.224714 loss_att 9.939678 loss_ctc 6.351739 loss_rnnt 3.965610 hw_loss 0.310952 lr 0.00045504 rank 1
2023-02-23 04:44:35,694 DEBUG TRAIN Batch 14/4000 loss 13.942862 loss_att 16.087906 loss_ctc 13.390375 loss_rnnt 13.396815 hw_loss 0.357565 lr 0.00045496 rank 2
2023-02-23 04:44:35,695 DEBUG TRAIN Batch 14/4000 loss 14.042149 loss_att 20.269819 loss_ctc 20.175987 loss_rnnt 11.771464 hw_loss 0.388695 lr 0.00045493 rank 3
2023-02-23 04:44:35,696 DEBUG TRAIN Batch 14/4000 loss 12.317542 loss_att 21.598890 loss_ctc 16.647408 loss_rnnt 9.701378 hw_loss 0.342336 lr 0.00045493 rank 7
2023-02-23 04:44:35,699 DEBUG TRAIN Batch 14/4000 loss 19.146172 loss_att 27.197824 loss_ctc 27.743759 loss_rnnt 16.188950 hw_loss 0.376026 lr 0.00045501 rank 6
2023-02-23 04:44:35,703 DEBUG TRAIN Batch 14/4000 loss 20.239492 loss_att 20.910681 loss_ctc 25.051218 loss_rnnt 19.220184 hw_loss 0.456576 lr 0.00045495 rank 5
2023-02-23 04:44:35,705 DEBUG TRAIN Batch 14/4000 loss 18.263969 loss_att 18.860258 loss_ctc 23.131100 loss_rnnt 17.265675 hw_loss 0.431411 lr 0.00045495 rank 0
2023-02-23 04:44:35,718 DEBUG TRAIN Batch 14/4000 loss 12.188880 loss_att 15.402317 loss_ctc 13.578264 loss_rnnt 11.171772 hw_loss 0.354693 lr 0.00045494 rank 4
2023-02-23 04:45:47,684 DEBUG TRAIN Batch 14/4100 loss 21.847643 loss_att 25.450691 loss_ctc 32.562935 loss_rnnt 19.505743 hw_loss 0.361093 lr 0.00045474 rank 7
2023-02-23 04:45:47,700 DEBUG TRAIN Batch 14/4100 loss 7.212163 loss_att 11.288366 loss_ctc 12.687344 loss_rnnt 5.485014 hw_loss 0.341034 lr 0.00045474 rank 3
2023-02-23 04:45:47,701 DEBUG TRAIN Batch 14/4100 loss 23.532961 loss_att 28.174736 loss_ctc 31.070065 loss_rnnt 21.414568 hw_loss 0.347046 lr 0.00045482 rank 6
2023-02-23 04:45:47,703 DEBUG TRAIN Batch 14/4100 loss 14.364595 loss_att 18.370596 loss_ctc 17.238596 loss_rnnt 12.932613 hw_loss 0.464215 lr 0.00045485 rank 1
2023-02-23 04:45:47,704 DEBUG TRAIN Batch 14/4100 loss 16.585236 loss_att 19.626825 loss_ctc 24.119843 loss_rnnt 14.748326 hw_loss 0.419955 lr 0.00045477 rank 5
2023-02-23 04:45:47,704 DEBUG TRAIN Batch 14/4100 loss 6.632269 loss_att 9.373337 loss_ctc 7.603426 loss_rnnt 5.760127 hw_loss 0.364578 lr 0.00045475 rank 4
2023-02-23 04:45:47,705 DEBUG TRAIN Batch 14/4100 loss 5.914895 loss_att 11.276129 loss_ctc 7.131558 loss_rnnt 4.501139 hw_loss 0.336164 lr 0.00045477 rank 2
2023-02-23 04:45:47,711 DEBUG TRAIN Batch 14/4100 loss 16.949089 loss_att 20.545946 loss_ctc 22.130323 loss_rnnt 15.343198 hw_loss 0.366916 lr 0.00045476 rank 0
2023-02-23 04:47:00,743 DEBUG TRAIN Batch 14/4200 loss 10.070003 loss_att 15.293615 loss_ctc 14.940084 loss_rnnt 8.166462 hw_loss 0.392764 lr 0.00045456 rank 4
2023-02-23 04:47:00,747 DEBUG TRAIN Batch 14/4200 loss 9.942006 loss_att 12.498968 loss_ctc 12.748514 loss_rnnt 8.839997 hw_loss 0.405780 lr 0.00045463 rank 6
2023-02-23 04:47:00,747 DEBUG TRAIN Batch 14/4200 loss 9.770620 loss_att 13.412839 loss_ctc 10.645847 loss_rnnt 8.703496 hw_loss 0.416218 lr 0.00045455 rank 3
2023-02-23 04:47:00,747 DEBUG TRAIN Batch 14/4200 loss 19.012142 loss_att 21.724815 loss_ctc 26.864557 loss_rnnt 17.213665 hw_loss 0.391790 lr 0.00045458 rank 5
2023-02-23 04:47:00,748 DEBUG TRAIN Batch 14/4200 loss 10.027270 loss_att 13.051861 loss_ctc 13.342512 loss_rnnt 8.760328 hw_loss 0.412483 lr 0.00045455 rank 7
2023-02-23 04:47:00,751 DEBUG TRAIN Batch 14/4200 loss 13.284401 loss_att 18.226345 loss_ctc 18.510605 loss_rnnt 11.394417 hw_loss 0.383941 lr 0.00045458 rank 2
2023-02-23 04:47:00,764 DEBUG TRAIN Batch 14/4200 loss 14.217897 loss_att 16.741001 loss_ctc 17.959946 loss_rnnt 13.025758 hw_loss 0.353585 lr 0.00045466 rank 1
2023-02-23 04:47:00,799 DEBUG TRAIN Batch 14/4200 loss 13.079477 loss_att 15.648566 loss_ctc 14.463929 loss_rnnt 12.168989 hw_loss 0.397644 lr 0.00045457 rank 0
2023-02-23 04:48:15,328 DEBUG TRAIN Batch 14/4300 loss 13.392322 loss_att 13.590231 loss_ctc 13.119651 loss_rnnt 13.159780 hw_loss 0.429966 lr 0.00045436 rank 7
2023-02-23 04:48:15,328 DEBUG TRAIN Batch 14/4300 loss 8.549602 loss_att 11.682972 loss_ctc 16.514631 loss_rnnt 6.680153 hw_loss 0.338946 lr 0.00045436 rank 3
2023-02-23 04:48:15,331 DEBUG TRAIN Batch 14/4300 loss 6.517902 loss_att 13.855167 loss_ctc 14.062160 loss_rnnt 3.780075 hw_loss 0.495886 lr 0.00045439 rank 2
2023-02-23 04:48:15,332 DEBUG TRAIN Batch 14/4300 loss 6.925565 loss_att 9.491655 loss_ctc 8.343513 loss_rnnt 6.022828 hw_loss 0.375859 lr 0.00045437 rank 4
2023-02-23 04:48:15,333 DEBUG TRAIN Batch 14/4300 loss 3.527086 loss_att 6.942369 loss_ctc 3.494669 loss_rnnt 2.580833 hw_loss 0.501598 lr 0.00045444 rank 6
2023-02-23 04:48:15,336 DEBUG TRAIN Batch 14/4300 loss 11.807261 loss_att 14.503467 loss_ctc 14.637951 loss_rnnt 10.653853 hw_loss 0.443889 lr 0.00045438 rank 0
2023-02-23 04:48:15,337 DEBUG TRAIN Batch 14/4300 loss 5.926326 loss_att 8.856728 loss_ctc 8.294607 loss_rnnt 4.690891 hw_loss 0.625469 lr 0.00045439 rank 5
2023-02-23 04:48:15,382 DEBUG TRAIN Batch 14/4300 loss 12.172398 loss_att 14.405469 loss_ctc 16.974510 loss_rnnt 10.838520 hw_loss 0.463089 lr 0.00045448 rank 1
2023-02-23 04:49:27,672 DEBUG TRAIN Batch 14/4400 loss 12.458379 loss_att 14.053265 loss_ctc 16.001936 loss_rnnt 11.390037 hw_loss 0.519169 lr 0.00045418 rank 3
2023-02-23 04:49:27,674 DEBUG TRAIN Batch 14/4400 loss 14.916474 loss_att 17.987368 loss_ctc 20.026947 loss_rnnt 13.353662 hw_loss 0.501068 lr 0.00045418 rank 7
2023-02-23 04:49:27,675 DEBUG TRAIN Batch 14/4400 loss 5.754377 loss_att 7.398473 loss_ctc 6.582836 loss_rnnt 5.096719 hw_loss 0.409457 lr 0.00045419 rank 4
2023-02-23 04:49:27,677 DEBUG TRAIN Batch 14/4400 loss 12.555003 loss_att 12.886364 loss_ctc 14.312919 loss_rnnt 12.010165 hw_loss 0.457833 lr 0.00045420 rank 2
2023-02-23 04:49:27,677 DEBUG TRAIN Batch 14/4400 loss 11.488980 loss_att 13.063730 loss_ctc 13.697192 loss_rnnt 10.528874 hw_loss 0.657613 lr 0.00045429 rank 1
2023-02-23 04:49:27,678 DEBUG TRAIN Batch 14/4400 loss 11.914455 loss_att 13.701818 loss_ctc 11.868545 loss_rnnt 11.335175 hw_loss 0.427369 lr 0.00045419 rank 0
2023-02-23 04:49:27,679 DEBUG TRAIN Batch 14/4400 loss 12.077351 loss_att 11.841101 loss_ctc 16.495646 loss_rnnt 11.213488 hw_loss 0.603762 lr 0.00045426 rank 6
2023-02-23 04:49:27,684 DEBUG TRAIN Batch 14/4400 loss 11.857623 loss_att 13.819482 loss_ctc 15.051132 loss_rnnt 10.836987 hw_loss 0.379618 lr 0.00045420 rank 5
2023-02-23 04:50:40,661 DEBUG TRAIN Batch 14/4500 loss 21.705135 loss_att 27.448456 loss_ctc 32.112038 loss_rnnt 18.974401 hw_loss 0.364658 lr 0.00045399 rank 7
2023-02-23 04:50:40,665 DEBUG TRAIN Batch 14/4500 loss 10.463462 loss_att 16.410246 loss_ctc 13.475920 loss_rnnt 8.688816 hw_loss 0.344304 lr 0.00045401 rank 0
2023-02-23 04:50:40,668 DEBUG TRAIN Batch 14/4500 loss 8.681298 loss_att 11.282529 loss_ctc 8.440985 loss_rnnt 7.953938 hw_loss 0.448418 lr 0.00045402 rank 2
2023-02-23 04:50:40,669 DEBUG TRAIN Batch 14/4500 loss 11.117913 loss_att 14.436860 loss_ctc 14.032921 loss_rnnt 9.871010 hw_loss 0.364588 lr 0.00045410 rank 1
2023-02-23 04:50:40,669 DEBUG TRAIN Batch 14/4500 loss 21.964659 loss_att 21.991976 loss_ctc 27.094212 loss_rnnt 21.045807 hw_loss 0.430214 lr 0.00045407 rank 6
2023-02-23 04:50:40,669 DEBUG TRAIN Batch 14/4500 loss 19.165401 loss_att 22.203724 loss_ctc 22.795065 loss_rnnt 17.819332 hw_loss 0.477096 lr 0.00045400 rank 4
2023-02-23 04:50:40,670 DEBUG TRAIN Batch 14/4500 loss 8.778439 loss_att 10.496792 loss_ctc 10.902834 loss_rnnt 7.922234 hw_loss 0.429902 lr 0.00045399 rank 3
2023-02-23 04:50:40,671 DEBUG TRAIN Batch 14/4500 loss 12.863685 loss_att 13.514442 loss_ctc 14.789658 loss_rnnt 12.261821 hw_loss 0.402968 lr 0.00045401 rank 5
2023-02-23 04:51:55,076 DEBUG TRAIN Batch 14/4600 loss 14.248027 loss_att 19.152590 loss_ctc 13.910553 loss_rnnt 13.084507 hw_loss 0.426758 lr 0.00045383 rank 5
2023-02-23 04:51:55,078 DEBUG TRAIN Batch 14/4600 loss 11.458226 loss_att 12.327331 loss_ctc 15.022858 loss_rnnt 10.645299 hw_loss 0.307168 lr 0.00045380 rank 7
2023-02-23 04:51:55,082 DEBUG TRAIN Batch 14/4600 loss 17.585068 loss_att 23.364017 loss_ctc 26.098259 loss_rnnt 15.146194 hw_loss 0.277482 lr 0.00045380 rank 3
2023-02-23 04:51:55,083 DEBUG TRAIN Batch 14/4600 loss 10.151167 loss_att 13.913164 loss_ctc 13.323441 loss_rnnt 8.790850 hw_loss 0.346777 lr 0.00045391 rank 1
2023-02-23 04:51:55,086 DEBUG TRAIN Batch 14/4600 loss 14.458122 loss_att 19.448639 loss_ctc 17.964212 loss_rnnt 12.743292 hw_loss 0.467343 lr 0.00045383 rank 2
2023-02-23 04:51:55,086 DEBUG TRAIN Batch 14/4600 loss 15.348589 loss_att 18.366606 loss_ctc 21.432602 loss_rnnt 13.711830 hw_loss 0.416165 lr 0.00045381 rank 4
2023-02-23 04:51:55,091 DEBUG TRAIN Batch 14/4600 loss 14.678894 loss_att 15.330826 loss_ctc 16.701433 loss_rnnt 14.000250 hw_loss 0.522351 lr 0.00045382 rank 0
2023-02-23 04:51:55,093 DEBUG TRAIN Batch 14/4600 loss 15.046764 loss_att 18.658550 loss_ctc 27.817663 loss_rnnt 12.429533 hw_loss 0.360165 lr 0.00045388 rank 6
2023-02-23 04:53:08,008 DEBUG TRAIN Batch 14/4700 loss 14.107230 loss_att 22.917990 loss_ctc 13.713987 loss_rnnt 12.220825 hw_loss 0.331285 lr 0.00045363 rank 0
2023-02-23 04:53:08,019 DEBUG TRAIN Batch 14/4700 loss 21.235737 loss_att 23.512617 loss_ctc 24.904089 loss_rnnt 20.031645 hw_loss 0.486756 lr 0.00045364 rank 2
2023-02-23 04:53:08,023 DEBUG TRAIN Batch 14/4700 loss 11.031532 loss_att 12.551064 loss_ctc 13.426865 loss_rnnt 10.145473 hw_loss 0.492700 lr 0.00045361 rank 7
2023-02-23 04:53:08,025 DEBUG TRAIN Batch 14/4700 loss 2.719595 loss_att 7.452094 loss_ctc 4.426325 loss_rnnt 1.329527 hw_loss 0.405009 lr 0.00045361 rank 3
2023-02-23 04:53:08,025 DEBUG TRAIN Batch 14/4700 loss 12.655858 loss_att 15.998447 loss_ctc 20.479816 loss_rnnt 10.756962 hw_loss 0.350968 lr 0.00045373 rank 1
2023-02-23 04:53:08,026 DEBUG TRAIN Batch 14/4700 loss 20.142124 loss_att 22.657644 loss_ctc 25.468941 loss_rnnt 18.727568 hw_loss 0.377271 lr 0.00045363 rank 4
2023-02-23 04:53:08,028 DEBUG TRAIN Batch 14/4700 loss 10.073053 loss_att 9.454723 loss_ctc 9.758373 loss_rnnt 9.977213 hw_loss 0.490246 lr 0.00045364 rank 5
2023-02-23 04:53:08,070 DEBUG TRAIN Batch 14/4700 loss 6.383996 loss_att 10.144663 loss_ctc 12.236943 loss_rnnt 4.658927 hw_loss 0.361016 lr 0.00045370 rank 6
2023-02-23 04:54:21,012 DEBUG TRAIN Batch 14/4800 loss 5.238467 loss_att 8.248372 loss_ctc 7.359225 loss_rnnt 4.114662 hw_loss 0.448231 lr 0.00045343 rank 7
2023-02-23 04:54:21,017 DEBUG TRAIN Batch 14/4800 loss 12.927930 loss_att 13.394033 loss_ctc 11.518188 loss_rnnt 12.822226 hw_loss 0.375842 lr 0.00045343 rank 3
2023-02-23 04:54:21,016 DEBUG TRAIN Batch 14/4800 loss 5.952982 loss_att 10.699242 loss_ctc 7.392142 loss_rnnt 4.571064 hw_loss 0.451460 lr 0.00045345 rank 0
2023-02-23 04:54:21,018 DEBUG TRAIN Batch 14/4800 loss 17.508493 loss_att 18.384098 loss_ctc 18.120943 loss_rnnt 17.034088 hw_loss 0.408046 lr 0.00045345 rank 5
2023-02-23 04:54:21,020 DEBUG TRAIN Batch 14/4800 loss 23.198711 loss_att 26.784616 loss_ctc 27.450119 loss_rnnt 21.707382 hw_loss 0.388676 lr 0.00045354 rank 1
2023-02-23 04:54:21,021 DEBUG TRAIN Batch 14/4800 loss 5.341139 loss_att 7.371804 loss_ctc 6.843960 loss_rnnt 4.509919 hw_loss 0.421333 lr 0.00045346 rank 2
2023-02-23 04:54:21,022 DEBUG TRAIN Batch 14/4800 loss 5.345304 loss_att 8.105190 loss_ctc 7.321600 loss_rnnt 4.305941 hw_loss 0.419775 lr 0.00045344 rank 4
2023-02-23 04:54:21,072 DEBUG TRAIN Batch 14/4800 loss 14.671799 loss_att 16.449718 loss_ctc 19.112133 loss_rnnt 13.531269 hw_loss 0.361692 lr 0.00045351 rank 6
2023-02-23 04:55:33,722 DEBUG TRAIN Batch 14/4900 loss 9.788435 loss_att 13.362864 loss_ctc 12.864780 loss_rnnt 8.477130 hw_loss 0.349199 lr 0.00045335 rank 1
2023-02-23 04:55:33,731 DEBUG TRAIN Batch 14/4900 loss 8.936508 loss_att 11.103498 loss_ctc 8.174100 loss_rnnt 8.404514 hw_loss 0.375468 lr 0.00045325 rank 4
2023-02-23 04:55:33,737 DEBUG TRAIN Batch 14/4900 loss 12.612161 loss_att 13.306063 loss_ctc 16.431383 loss_rnnt 11.754799 hw_loss 0.392534 lr 0.00045332 rank 6
2023-02-23 04:55:33,738 DEBUG TRAIN Batch 14/4900 loss 6.318261 loss_att 8.528845 loss_ctc 7.712781 loss_rnnt 5.474869 hw_loss 0.403761 lr 0.00045324 rank 3
2023-02-23 04:55:33,740 DEBUG TRAIN Batch 14/4900 loss 11.741565 loss_att 12.837900 loss_ctc 13.629657 loss_rnnt 11.008310 hw_loss 0.491703 lr 0.00045327 rank 5
2023-02-23 04:55:33,740 DEBUG TRAIN Batch 14/4900 loss 14.680780 loss_att 17.231281 loss_ctc 23.072203 loss_rnnt 12.837873 hw_loss 0.401158 lr 0.00045327 rank 2
2023-02-23 04:55:33,742 DEBUG TRAIN Batch 14/4900 loss 5.133070 loss_att 7.024633 loss_ctc 5.424629 loss_rnnt 4.501028 hw_loss 0.402854 lr 0.00045324 rank 7
2023-02-23 04:55:33,792 DEBUG TRAIN Batch 14/4900 loss 8.431588 loss_att 9.644489 loss_ctc 11.778284 loss_rnnt 7.515371 hw_loss 0.426395 lr 0.00045326 rank 0
2023-02-23 04:56:49,262 DEBUG TRAIN Batch 14/5000 loss 12.599219 loss_att 12.971999 loss_ctc 14.988458 loss_rnnt 11.881673 hw_loss 0.608296 lr 0.00045306 rank 7
2023-02-23 04:56:49,262 DEBUG TRAIN Batch 14/5000 loss 9.464296 loss_att 11.650393 loss_ctc 14.487268 loss_rnnt 8.095043 hw_loss 0.491819 lr 0.00045306 rank 3
2023-02-23 04:56:49,263 DEBUG TRAIN Batch 14/5000 loss 20.547163 loss_att 22.914566 loss_ctc 28.405491 loss_rnnt 18.767578 hw_loss 0.484363 lr 0.00045308 rank 2
2023-02-23 04:56:49,265 DEBUG TRAIN Batch 14/5000 loss 7.928835 loss_att 11.739941 loss_ctc 12.378669 loss_rnnt 6.334143 hw_loss 0.448425 lr 0.00045307 rank 0
2023-02-23 04:56:49,266 DEBUG TRAIN Batch 14/5000 loss 9.080636 loss_att 11.225161 loss_ctc 12.699068 loss_rnnt 7.869159 hw_loss 0.562715 lr 0.00045314 rank 6
2023-02-23 04:56:49,267 DEBUG TRAIN Batch 14/5000 loss 11.497843 loss_att 15.300898 loss_ctc 19.024139 loss_rnnt 9.525317 hw_loss 0.390765 lr 0.00045307 rank 4
2023-02-23 04:56:49,268 DEBUG TRAIN Batch 14/5000 loss 16.306179 loss_att 18.649782 loss_ctc 18.884590 loss_rnnt 15.177825 hw_loss 0.592208 lr 0.00045317 rank 1
2023-02-23 04:56:49,290 DEBUG TRAIN Batch 14/5000 loss 13.903154 loss_att 18.472422 loss_ctc 15.680323 loss_rnnt 12.539421 hw_loss 0.399232 lr 0.00045308 rank 5
2023-02-23 04:58:02,448 DEBUG TRAIN Batch 14/5100 loss 8.646132 loss_att 16.529692 loss_ctc 9.104192 loss_rnnt 6.786436 hw_loss 0.416079 lr 0.00045287 rank 7
2023-02-23 04:58:02,450 DEBUG TRAIN Batch 14/5100 loss 14.527031 loss_att 14.682027 loss_ctc 17.184805 loss_rnnt 13.842203 hw_loss 0.561485 lr 0.00045288 rank 4
2023-02-23 04:58:02,453 DEBUG TRAIN Batch 14/5100 loss 13.942926 loss_att 15.824603 loss_ctc 23.040661 loss_rnnt 12.142012 hw_loss 0.396654 lr 0.00045290 rank 2
2023-02-23 04:58:02,457 DEBUG TRAIN Batch 14/5100 loss 13.137007 loss_att 14.632399 loss_ctc 17.972198 loss_rnnt 11.959436 hw_loss 0.438371 lr 0.00045289 rank 0
2023-02-23 04:58:02,457 DEBUG TRAIN Batch 14/5100 loss 9.410535 loss_att 11.361641 loss_ctc 12.076003 loss_rnnt 8.472803 hw_loss 0.360216 lr 0.00045295 rank 6
2023-02-23 04:58:02,458 DEBUG TRAIN Batch 14/5100 loss 21.229685 loss_att 20.170523 loss_ctc 24.759995 loss_rnnt 20.732122 hw_loss 0.447535 lr 0.00045290 rank 5
2023-02-23 04:58:02,460 DEBUG TRAIN Batch 14/5100 loss 22.619316 loss_att 27.343523 loss_ctc 35.381615 loss_rnnt 19.778133 hw_loss 0.365068 lr 0.00045298 rank 1
2023-02-23 04:58:02,462 DEBUG TRAIN Batch 14/5100 loss 11.669649 loss_att 13.003538 loss_ctc 17.570824 loss_rnnt 10.327686 hw_loss 0.540678 lr 0.00045287 rank 3
2023-02-23 04:59:15,254 DEBUG TRAIN Batch 14/5200 loss 3.712667 loss_att 5.960918 loss_ctc 7.885602 loss_rnnt 2.532272 hw_loss 0.326912 lr 0.00045271 rank 2
2023-02-23 04:59:15,257 DEBUG TRAIN Batch 14/5200 loss 11.646163 loss_att 16.524017 loss_ctc 15.653811 loss_rnnt 9.879424 hw_loss 0.481530 lr 0.00045270 rank 4
2023-02-23 04:59:15,257 DEBUG TRAIN Batch 14/5200 loss 15.547262 loss_att 15.794870 loss_ctc 20.932571 loss_rnnt 14.583728 hw_loss 0.367446 lr 0.00045268 rank 3
2023-02-23 04:59:15,259 DEBUG TRAIN Batch 14/5200 loss 6.760162 loss_att 8.257814 loss_ctc 8.089315 loss_rnnt 6.075035 hw_loss 0.390705 lr 0.00045268 rank 7
2023-02-23 04:59:15,263 DEBUG TRAIN Batch 14/5200 loss 8.350283 loss_att 11.472692 loss_ctc 12.143951 loss_rnnt 7.016299 hw_loss 0.381898 lr 0.00045271 rank 5
2023-02-23 04:59:15,263 DEBUG TRAIN Batch 14/5200 loss 8.774930 loss_att 15.529596 loss_ctc 15.512951 loss_rnnt 6.349195 hw_loss 0.330747 lr 0.00045280 rank 1
2023-02-23 04:59:15,266 DEBUG TRAIN Batch 14/5200 loss 8.681951 loss_att 11.320131 loss_ctc 14.488120 loss_rnnt 7.213531 hw_loss 0.312426 lr 0.00045270 rank 0
2023-02-23 04:59:15,270 DEBUG TRAIN Batch 14/5200 loss 4.419744 loss_att 8.846858 loss_ctc 9.341658 loss_rnnt 2.701473 hw_loss 0.331113 lr 0.00045276 rank 6
2023-02-23 05:00:29,917 DEBUG TRAIN Batch 14/5300 loss 6.852108 loss_att 11.312013 loss_ctc 9.898039 loss_rnnt 5.342823 hw_loss 0.395962 lr 0.00045252 rank 5
2023-02-23 05:00:29,922 DEBUG TRAIN Batch 14/5300 loss 11.079811 loss_att 13.634562 loss_ctc 12.113297 loss_rnnt 10.194635 hw_loss 0.443302 lr 0.00045250 rank 7
2023-02-23 05:00:29,931 DEBUG TRAIN Batch 14/5300 loss 9.197984 loss_att 13.154797 loss_ctc 10.298710 loss_rnnt 8.084022 hw_loss 0.329693 lr 0.00045252 rank 0
2023-02-23 05:00:29,932 DEBUG TRAIN Batch 14/5300 loss 16.020144 loss_att 16.549364 loss_ctc 20.616226 loss_rnnt 15.103320 hw_loss 0.371565 lr 0.00045250 rank 3
2023-02-23 05:00:29,932 DEBUG TRAIN Batch 14/5300 loss 21.948269 loss_att 19.939104 loss_ctc 24.248524 loss_rnnt 21.776550 hw_loss 0.500347 lr 0.00045253 rank 2
2023-02-23 05:00:29,933 DEBUG TRAIN Batch 14/5300 loss 21.013147 loss_att 23.963306 loss_ctc 28.115515 loss_rnnt 19.247231 hw_loss 0.429194 lr 0.00045258 rank 6
2023-02-23 05:00:29,935 DEBUG TRAIN Batch 14/5300 loss 10.778789 loss_att 11.382680 loss_ctc 12.023898 loss_rnnt 10.265014 hw_loss 0.425592 lr 0.00045251 rank 4
2023-02-23 05:00:29,979 DEBUG TRAIN Batch 14/5300 loss 12.989849 loss_att 17.543295 loss_ctc 15.093234 loss_rnnt 11.612208 hw_loss 0.349688 lr 0.00045261 rank 1
2023-02-23 05:01:43,720 DEBUG TRAIN Batch 14/5400 loss 5.996956 loss_att 9.663245 loss_ctc 9.744091 loss_rnnt 4.607986 hw_loss 0.292678 lr 0.00045231 rank 7
2023-02-23 05:01:43,723 DEBUG TRAIN Batch 14/5400 loss 26.556208 loss_att 26.955368 loss_ctc 32.206421 loss_rnnt 25.524654 hw_loss 0.371931 lr 0.00045239 rank 6
2023-02-23 05:01:43,724 DEBUG TRAIN Batch 14/5400 loss 8.007258 loss_att 10.422970 loss_ctc 12.564678 loss_rnnt 6.677031 hw_loss 0.448930 lr 0.00045231 rank 3
2023-02-23 05:01:43,728 DEBUG TRAIN Batch 14/5400 loss 5.256157 loss_att 8.501930 loss_ctc 8.631034 loss_rnnt 3.965812 hw_loss 0.358512 lr 0.00045234 rank 2
2023-02-23 05:01:43,728 DEBUG TRAIN Batch 14/5400 loss 6.383572 loss_att 9.283402 loss_ctc 10.307886 loss_rnnt 5.089126 hw_loss 0.358572 lr 0.00045242 rank 1
2023-02-23 05:01:43,729 DEBUG TRAIN Batch 14/5400 loss 13.684955 loss_att 16.288492 loss_ctc 16.339052 loss_rnnt 12.592239 hw_loss 0.408989 lr 0.00045234 rank 5
2023-02-23 05:01:43,731 DEBUG TRAIN Batch 14/5400 loss 5.202923 loss_att 8.302095 loss_ctc 8.885834 loss_rnnt 3.894335 hw_loss 0.370683 lr 0.00045232 rank 4
2023-02-23 05:01:43,744 DEBUG TRAIN Batch 14/5400 loss 15.545148 loss_att 19.828087 loss_ctc 21.374205 loss_rnnt 13.675378 hw_loss 0.442450 lr 0.00045233 rank 0
2023-02-23 05:02:55,699 DEBUG TRAIN Batch 14/5500 loss 7.395650 loss_att 9.219299 loss_ctc 9.312020 loss_rnnt 6.586704 hw_loss 0.353812 lr 0.00045213 rank 7
2023-02-23 05:02:55,700 DEBUG TRAIN Batch 14/5500 loss 16.894342 loss_att 20.086727 loss_ctc 22.390076 loss_rnnt 15.256413 hw_loss 0.500042 lr 0.00045216 rank 2
2023-02-23 05:02:55,703 DEBUG TRAIN Batch 14/5500 loss 9.603713 loss_att 11.073971 loss_ctc 9.781074 loss_rnnt 9.061910 hw_loss 0.420193 lr 0.00045221 rank 6
2023-02-23 05:02:55,703 DEBUG TRAIN Batch 14/5500 loss 13.796515 loss_att 13.502638 loss_ctc 16.425440 loss_rnnt 13.295116 hw_loss 0.393094 lr 0.00045224 rank 1
2023-02-23 05:02:55,704 DEBUG TRAIN Batch 14/5500 loss 9.961037 loss_att 11.971300 loss_ctc 13.626701 loss_rnnt 8.829913 hw_loss 0.450592 lr 0.00045214 rank 4
2023-02-23 05:02:55,704 DEBUG TRAIN Batch 14/5500 loss 10.378760 loss_att 15.788535 loss_ctc 15.297855 loss_rnnt 8.477039 hw_loss 0.307286 lr 0.00045213 rank 3
2023-02-23 05:02:55,707 DEBUG TRAIN Batch 14/5500 loss 6.165556 loss_att 7.970911 loss_ctc 11.462699 loss_rnnt 4.841375 hw_loss 0.481545 lr 0.00045215 rank 5
2023-02-23 05:02:55,712 DEBUG TRAIN Batch 14/5500 loss 12.794035 loss_att 16.179157 loss_ctc 18.078129 loss_rnnt 11.219866 hw_loss 0.361122 lr 0.00045215 rank 0
2023-02-23 05:04:09,014 DEBUG TRAIN Batch 14/5600 loss 14.764176 loss_att 15.755939 loss_ctc 19.387611 loss_rnnt 13.693686 hw_loss 0.479403 lr 0.00045194 rank 7
2023-02-23 05:04:09,015 DEBUG TRAIN Batch 14/5600 loss 12.354365 loss_att 15.302357 loss_ctc 18.359816 loss_rnnt 10.729247 hw_loss 0.440237 lr 0.00045205 rank 1
2023-02-23 05:04:09,017 DEBUG TRAIN Batch 14/5600 loss 18.305527 loss_att 19.972450 loss_ctc 24.044952 loss_rnnt 17.014593 hw_loss 0.360548 lr 0.00045197 rank 2
2023-02-23 05:04:09,017 DEBUG TRAIN Batch 14/5600 loss 4.828035 loss_att 8.649892 loss_ctc 8.243542 loss_rnnt 3.420352 hw_loss 0.352331 lr 0.00045197 rank 5
2023-02-23 05:04:09,019 DEBUG TRAIN Batch 14/5600 loss 12.087832 loss_att 13.626947 loss_ctc 13.936647 loss_rnnt 11.207684 hw_loss 0.610908 lr 0.00045196 rank 4
2023-02-23 05:04:09,020 DEBUG TRAIN Batch 14/5600 loss 11.033530 loss_att 15.191654 loss_ctc 18.183355 loss_rnnt 9.042832 hw_loss 0.385803 lr 0.00045202 rank 6
2023-02-23 05:04:09,023 DEBUG TRAIN Batch 14/5600 loss 10.198077 loss_att 11.688441 loss_ctc 12.997346 loss_rnnt 9.290806 hw_loss 0.442429 lr 0.00045194 rank 3
2023-02-23 05:04:09,024 DEBUG TRAIN Batch 14/5600 loss 10.073040 loss_att 14.644484 loss_ctc 14.182636 loss_rnnt 8.391837 hw_loss 0.410567 lr 0.00045196 rank 0
2023-02-23 05:05:24,895 DEBUG TRAIN Batch 14/5700 loss 12.407582 loss_att 14.789944 loss_ctc 14.761116 loss_rnnt 11.425381 hw_loss 0.359859 lr 0.00045176 rank 7
2023-02-23 05:05:24,901 DEBUG TRAIN Batch 14/5700 loss 19.214827 loss_att 19.561312 loss_ctc 24.360624 loss_rnnt 18.208841 hw_loss 0.469841 lr 0.00045176 rank 3
2023-02-23 05:05:24,906 DEBUG TRAIN Batch 14/5700 loss 10.722342 loss_att 11.447387 loss_ctc 14.794451 loss_rnnt 9.758757 hw_loss 0.516804 lr 0.00045179 rank 2
2023-02-23 05:05:24,904 DEBUG TRAIN Batch 14/5700 loss 7.756569 loss_att 9.117208 loss_ctc 8.322337 loss_rnnt 7.156886 hw_loss 0.472724 lr 0.00045178 rank 0
2023-02-23 05:05:24,905 DEBUG TRAIN Batch 14/5700 loss 16.119936 loss_att 16.156076 loss_ctc 17.590054 loss_rnnt 15.623628 hw_loss 0.549495 lr 0.00045177 rank 4
2023-02-23 05:05:24,910 DEBUG TRAIN Batch 14/5700 loss 12.396221 loss_att 15.345493 loss_ctc 15.452650 loss_rnnt 11.103237 hw_loss 0.554262 lr 0.00045184 rank 6
2023-02-23 05:05:24,912 DEBUG TRAIN Batch 14/5700 loss 7.680799 loss_att 11.401503 loss_ctc 10.245714 loss_rnnt 6.413943 hw_loss 0.338864 lr 0.00045179 rank 5
2023-02-23 05:05:24,919 DEBUG TRAIN Batch 14/5700 loss 6.279039 loss_att 10.637928 loss_ctc 7.376792 loss_rnnt 5.054750 hw_loss 0.386519 lr 0.00045187 rank 1
2023-02-23 05:06:38,870 DEBUG TRAIN Batch 14/5800 loss 16.422035 loss_att 17.464622 loss_ctc 19.687778 loss_rnnt 15.569738 hw_loss 0.390654 lr 0.00045158 rank 7
2023-02-23 05:06:38,874 DEBUG TRAIN Batch 14/5800 loss 15.523978 loss_att 16.826275 loss_ctc 21.591988 loss_rnnt 14.279909 hw_loss 0.327265 lr 0.00045159 rank 4
2023-02-23 05:06:38,876 DEBUG TRAIN Batch 14/5800 loss 25.797321 loss_att 29.250000 loss_ctc 33.431221 loss_rnnt 23.840658 hw_loss 0.465513 lr 0.00045159 rank 0
2023-02-23 05:06:38,880 DEBUG TRAIN Batch 14/5800 loss 6.252828 loss_att 8.494776 loss_ctc 7.115077 loss_rnnt 5.437961 hw_loss 0.471583 lr 0.00045169 rank 1
2023-02-23 05:06:38,881 DEBUG TRAIN Batch 14/5800 loss 11.082109 loss_att 14.184802 loss_ctc 17.604855 loss_rnnt 9.389758 hw_loss 0.378962 lr 0.00045158 rank 3
2023-02-23 05:06:38,883 DEBUG TRAIN Batch 14/5800 loss 14.601184 loss_att 14.258406 loss_ctc 17.153542 loss_rnnt 14.055015 hw_loss 0.514519 lr 0.00045160 rank 2
2023-02-23 05:06:38,883 DEBUG TRAIN Batch 14/5800 loss 8.629500 loss_att 11.687856 loss_ctc 11.208557 loss_rnnt 7.452169 hw_loss 0.415850 lr 0.00045165 rank 6
2023-02-23 05:06:38,883 DEBUG TRAIN Batch 14/5800 loss 13.487380 loss_att 17.113543 loss_ctc 16.988773 loss_rnnt 12.095851 hw_loss 0.373958 lr 0.00045160 rank 5
2023-02-23 05:07:52,907 DEBUG TRAIN Batch 14/5900 loss 3.961402 loss_att 8.000795 loss_ctc 5.700649 loss_rnnt 2.693811 hw_loss 0.427149 lr 0.00045139 rank 7
2023-02-23 05:07:52,907 DEBUG TRAIN Batch 14/5900 loss 9.988789 loss_att 13.721539 loss_ctc 11.590458 loss_rnnt 8.835207 hw_loss 0.362769 lr 0.00045142 rank 2
2023-02-23 05:07:52,907 DEBUG TRAIN Batch 14/5900 loss 4.557004 loss_att 10.233475 loss_ctc 9.521290 loss_rnnt 2.492241 hw_loss 0.501683 lr 0.00045139 rank 3
2023-02-23 05:07:52,909 DEBUG TRAIN Batch 14/5900 loss 7.375547 loss_att 9.952067 loss_ctc 9.476043 loss_rnnt 6.396918 hw_loss 0.343612 lr 0.00045142 rank 5
2023-02-23 05:07:52,909 DEBUG TRAIN Batch 14/5900 loss 14.493860 loss_att 15.981655 loss_ctc 15.985223 loss_rnnt 13.801903 hw_loss 0.366657 lr 0.00045150 rank 1
2023-02-23 05:07:52,912 DEBUG TRAIN Batch 14/5900 loss 13.506289 loss_att 15.907277 loss_ctc 17.781364 loss_rnnt 12.273680 hw_loss 0.342005 lr 0.00045140 rank 4
2023-02-23 05:07:52,913 DEBUG TRAIN Batch 14/5900 loss 24.243839 loss_att 22.619129 loss_ctc 38.081398 loss_rnnt 22.494625 hw_loss 0.429652 lr 0.00045141 rank 0
2023-02-23 05:07:52,915 DEBUG TRAIN Batch 14/5900 loss 18.151667 loss_att 22.636164 loss_ctc 24.340208 loss_rnnt 16.165304 hw_loss 0.495611 lr 0.00045147 rank 6
2023-02-23 05:09:07,285 DEBUG TRAIN Batch 14/6000 loss 22.265045 loss_att 28.888042 loss_ctc 35.805984 loss_rnnt 18.933121 hw_loss 0.378500 lr 0.00045122 rank 4
2023-02-23 05:09:07,300 DEBUG TRAIN Batch 14/6000 loss 8.312006 loss_att 12.967692 loss_ctc 9.598196 loss_rnnt 7.013029 hw_loss 0.368152 lr 0.00045123 rank 2
2023-02-23 05:09:07,301 DEBUG TRAIN Batch 14/6000 loss 9.940462 loss_att 12.102036 loss_ctc 13.007013 loss_rnnt 8.908068 hw_loss 0.358509 lr 0.00045121 rank 7
2023-02-23 05:09:07,303 DEBUG TRAIN Batch 14/6000 loss 5.990876 loss_att 7.724476 loss_ctc 5.950108 loss_rnnt 5.439796 hw_loss 0.393365 lr 0.00045121 rank 3
2023-02-23 05:09:07,303 DEBUG TRAIN Batch 14/6000 loss 17.944332 loss_att 19.937645 loss_ctc 19.507402 loss_rnnt 17.088604 hw_loss 0.466233 lr 0.00045132 rank 1
2023-02-23 05:09:07,303 DEBUG TRAIN Batch 14/6000 loss 7.588180 loss_att 11.955687 loss_ctc 7.960614 loss_rnnt 6.417849 hw_loss 0.463448 lr 0.00045123 rank 0
2023-02-23 05:09:07,328 DEBUG TRAIN Batch 14/6000 loss 15.036493 loss_att 17.881783 loss_ctc 23.359720 loss_rnnt 13.135421 hw_loss 0.416723 lr 0.00045129 rank 6
2023-02-23 05:09:07,330 DEBUG TRAIN Batch 14/6000 loss 8.270278 loss_att 12.721745 loss_ctc 10.241205 loss_rnnt 6.905851 hw_loss 0.396266 lr 0.00045123 rank 5
2023-02-23 05:10:20,752 DEBUG TRAIN Batch 14/6100 loss 12.506199 loss_att 13.247516 loss_ctc 14.055056 loss_rnnt 11.918800 hw_loss 0.436165 lr 0.00045102 rank 7
2023-02-23 05:10:20,753 DEBUG TRAIN Batch 14/6100 loss 7.791375 loss_att 9.310892 loss_ctc 10.842979 loss_rnnt 6.845705 hw_loss 0.440412 lr 0.00045113 rank 1
2023-02-23 05:10:20,753 DEBUG TRAIN Batch 14/6100 loss 11.021467 loss_att 11.111218 loss_ctc 15.187365 loss_rnnt 10.215052 hw_loss 0.436898 lr 0.00045104 rank 0
2023-02-23 05:10:20,756 DEBUG TRAIN Batch 14/6100 loss 11.048626 loss_att 13.394072 loss_ctc 12.165047 loss_rnnt 10.252869 hw_loss 0.333395 lr 0.00045102 rank 3
2023-02-23 05:10:20,760 DEBUG TRAIN Batch 14/6100 loss 13.505919 loss_att 21.773251 loss_ctc 18.584450 loss_rnnt 10.930990 hw_loss 0.458109 lr 0.00045105 rank 5
2023-02-23 05:10:20,762 DEBUG TRAIN Batch 14/6100 loss 13.818707 loss_att 13.646006 loss_ctc 19.898273 loss_rnnt 12.824723 hw_loss 0.408591 lr 0.00045105 rank 2
2023-02-23 05:10:20,767 DEBUG TRAIN Batch 14/6100 loss 8.744017 loss_att 13.169122 loss_ctc 12.097870 loss_rnnt 7.171290 hw_loss 0.450987 lr 0.00045110 rank 6
2023-02-23 05:10:20,801 DEBUG TRAIN Batch 14/6100 loss 7.418653 loss_att 10.197017 loss_ctc 11.461657 loss_rnnt 6.089077 hw_loss 0.440316 lr 0.00045103 rank 4
2023-02-23 05:11:33,876 DEBUG TRAIN Batch 14/6200 loss 14.623062 loss_att 17.344553 loss_ctc 15.683684 loss_rnnt 13.733030 hw_loss 0.383096 lr 0.00045092 rank 6
2023-02-23 05:11:33,877 DEBUG TRAIN Batch 14/6200 loss 12.210004 loss_att 15.985642 loss_ctc 16.773306 loss_rnnt 10.647591 hw_loss 0.372834 lr 0.00045095 rank 1
2023-02-23 05:11:33,892 DEBUG TRAIN Batch 14/6200 loss 19.398241 loss_att 20.392124 loss_ctc 20.544401 loss_rnnt 18.768730 hw_loss 0.521087 lr 0.00045087 rank 5
2023-02-23 05:11:33,892 DEBUG TRAIN Batch 14/6200 loss 6.885947 loss_att 10.066553 loss_ctc 7.650328 loss_rnnt 5.886820 hw_loss 0.489539 lr 0.00045087 rank 2
2023-02-23 05:11:33,893 DEBUG TRAIN Batch 14/6200 loss 10.604420 loss_att 13.095679 loss_ctc 12.718713 loss_rnnt 9.613663 hw_loss 0.394873 lr 0.00045084 rank 7
2023-02-23 05:11:33,894 DEBUG TRAIN Batch 14/6200 loss 16.524376 loss_att 19.135532 loss_ctc 20.204464 loss_rnnt 15.246670 hw_loss 0.496491 lr 0.00045086 rank 0
2023-02-23 05:11:33,896 DEBUG TRAIN Batch 14/6200 loss 16.400120 loss_att 21.997623 loss_ctc 21.768330 loss_rnnt 14.345730 hw_loss 0.410862 lr 0.00045084 rank 3
2023-02-23 05:11:33,939 DEBUG TRAIN Batch 14/6200 loss 9.517015 loss_att 12.664062 loss_ctc 12.493795 loss_rnnt 8.308474 hw_loss 0.341675 lr 0.00045085 rank 4
2023-02-23 05:12:46,209 DEBUG TRAIN Batch 14/6300 loss 7.710392 loss_att 10.892258 loss_ctc 9.127201 loss_rnnt 6.655370 hw_loss 0.430765 lr 0.00045067 rank 4
2023-02-23 05:12:46,211 DEBUG TRAIN Batch 14/6300 loss 22.788486 loss_att 21.759983 loss_ctc 28.959898 loss_rnnt 22.012165 hw_loss 0.298433 lr 0.00045066 rank 7
2023-02-23 05:12:46,212 DEBUG TRAIN Batch 14/6300 loss 10.721558 loss_att 12.143202 loss_ctc 15.529687 loss_rnnt 9.524544 hw_loss 0.509251 lr 0.00045068 rank 2
2023-02-23 05:12:46,212 DEBUG TRAIN Batch 14/6300 loss 16.844885 loss_att 18.918104 loss_ctc 22.436543 loss_rnnt 15.460475 hw_loss 0.420397 lr 0.00045066 rank 3
2023-02-23 05:12:46,217 DEBUG TRAIN Batch 14/6300 loss 9.615849 loss_att 9.127472 loss_ctc 11.575574 loss_rnnt 9.032358 hw_loss 0.787256 lr 0.00045077 rank 1
2023-02-23 05:12:46,219 DEBUG TRAIN Batch 14/6300 loss 2.791896 loss_att 7.145133 loss_ctc 3.751477 loss_rnnt 1.608435 hw_loss 0.346632 lr 0.00045068 rank 5
2023-02-23 05:12:46,222 DEBUG TRAIN Batch 14/6300 loss 13.339494 loss_att 17.659233 loss_ctc 14.822455 loss_rnnt 11.980942 hw_loss 0.556643 lr 0.00045068 rank 0
2023-02-23 05:12:46,245 DEBUG TRAIN Batch 14/6300 loss 14.973279 loss_att 15.870253 loss_ctc 17.964714 loss_rnnt 14.150973 hw_loss 0.457599 lr 0.00045074 rank 6
2023-02-23 05:14:01,306 DEBUG TRAIN Batch 14/6400 loss 21.666685 loss_att 24.238060 loss_ctc 23.664669 loss_rnnt 20.682896 hw_loss 0.380843 lr 0.00045047 rank 7
2023-02-23 05:14:01,308 DEBUG TRAIN Batch 14/6400 loss 12.079337 loss_att 12.200216 loss_ctc 14.562664 loss_rnnt 11.343192 hw_loss 0.714112 lr 0.00045047 rank 3
2023-02-23 05:14:01,308 DEBUG TRAIN Batch 14/6400 loss 14.777575 loss_att 14.057865 loss_ctc 17.065662 loss_rnnt 14.317508 hw_loss 0.560495 lr 0.00045049 rank 0
2023-02-23 05:14:01,308 DEBUG TRAIN Batch 14/6400 loss 6.634810 loss_att 9.930902 loss_ctc 8.835114 loss_rnnt 5.474889 hw_loss 0.388741 lr 0.00045058 rank 1
2023-02-23 05:14:01,312 DEBUG TRAIN Batch 14/6400 loss 16.318129 loss_att 21.389961 loss_ctc 24.759605 loss_rnnt 13.941635 hw_loss 0.443618 lr 0.00045050 rank 2
2023-02-23 05:14:01,313 DEBUG TRAIN Batch 14/6400 loss 9.062396 loss_att 13.575318 loss_ctc 10.516931 loss_rnnt 7.780580 hw_loss 0.347425 lr 0.00045050 rank 5
2023-02-23 05:14:01,314 DEBUG TRAIN Batch 14/6400 loss 12.562859 loss_att 11.467494 loss_ctc 10.443445 loss_rnnt 12.908618 hw_loss 0.292316 lr 0.00045055 rank 6
2023-02-23 05:14:01,315 DEBUG TRAIN Batch 14/6400 loss 11.763134 loss_att 11.270620 loss_ctc 14.241368 loss_rnnt 11.224905 hw_loss 0.574314 lr 0.00045049 rank 4
2023-02-23 05:15:14,077 DEBUG TRAIN Batch 14/6500 loss 6.861922 loss_att 13.380334 loss_ctc 13.210154 loss_rnnt 4.513233 hw_loss 0.372328 lr 0.00045029 rank 7
2023-02-23 05:15:14,077 DEBUG TRAIN Batch 14/6500 loss 1.405518 loss_att 4.207512 loss_ctc 1.620248 loss_rnnt 0.629165 hw_loss 0.351230 lr 0.00045040 rank 1
2023-02-23 05:15:14,078 DEBUG TRAIN Batch 14/6500 loss 4.573798 loss_att 9.231709 loss_ctc 6.077190 loss_rnnt 3.224144 hw_loss 0.408038 lr 0.00045029 rank 3
2023-02-23 05:15:14,079 DEBUG TRAIN Batch 14/6500 loss 10.783940 loss_att 13.422743 loss_ctc 14.986270 loss_rnnt 9.498532 hw_loss 0.370008 lr 0.00045032 rank 2
2023-02-23 05:15:14,079 DEBUG TRAIN Batch 14/6500 loss 16.218376 loss_att 18.209631 loss_ctc 16.475674 loss_rnnt 15.606972 hw_loss 0.335337 lr 0.00045030 rank 4
2023-02-23 05:15:14,082 DEBUG TRAIN Batch 14/6500 loss 10.987403 loss_att 12.664455 loss_ctc 12.978693 loss_rnnt 10.181591 hw_loss 0.384178 lr 0.00045031 rank 0
2023-02-23 05:15:14,082 DEBUG TRAIN Batch 14/6500 loss 15.458533 loss_att 17.466488 loss_ctc 20.683815 loss_rnnt 14.109934 hw_loss 0.469319 lr 0.00045037 rank 6
2023-02-23 05:15:14,086 DEBUG TRAIN Batch 14/6500 loss 12.259328 loss_att 14.597324 loss_ctc 15.742704 loss_rnnt 11.163042 hw_loss 0.307940 lr 0.00045032 rank 5
2023-02-23 05:16:26,971 DEBUG TRAIN Batch 14/6600 loss 16.599068 loss_att 20.487623 loss_ctc 24.924713 loss_rnnt 14.487417 hw_loss 0.419729 lr 0.00045011 rank 3
2023-02-23 05:16:26,974 DEBUG TRAIN Batch 14/6600 loss 8.366336 loss_att 10.517389 loss_ctc 12.939204 loss_rnnt 7.132297 hw_loss 0.363963 lr 0.00045019 rank 6
2023-02-23 05:16:26,975 DEBUG TRAIN Batch 14/6600 loss 13.587869 loss_att 14.376918 loss_ctc 14.946057 loss_rnnt 13.047508 hw_loss 0.377733 lr 0.00045013 rank 5
2023-02-23 05:16:26,976 DEBUG TRAIN Batch 14/6600 loss 5.186548 loss_att 8.342445 loss_ctc 10.531847 loss_rnnt 3.648048 hw_loss 0.364901 lr 0.00045011 rank 7
2023-02-23 05:16:26,977 DEBUG TRAIN Batch 14/6600 loss 11.449198 loss_att 14.172898 loss_ctc 14.140480 loss_rnnt 10.376231 hw_loss 0.317604 lr 0.00045013 rank 0
2023-02-23 05:16:26,979 DEBUG TRAIN Batch 14/6600 loss 15.768601 loss_att 15.818381 loss_ctc 22.143333 loss_rnnt 14.664964 hw_loss 0.456970 lr 0.00045022 rank 1
2023-02-23 05:16:26,979 DEBUG TRAIN Batch 14/6600 loss 30.099098 loss_att 30.313641 loss_ctc 39.739090 loss_rnnt 28.540623 hw_loss 0.431686 lr 0.00045014 rank 2
2023-02-23 05:16:26,983 DEBUG TRAIN Batch 14/6600 loss 13.381880 loss_att 15.364845 loss_ctc 17.246933 loss_rnnt 12.280177 hw_loss 0.355819 lr 0.00045012 rank 4
2023-02-23 05:17:40,795 DEBUG TRAIN Batch 14/6700 loss 11.196056 loss_att 16.079420 loss_ctc 11.394503 loss_rnnt 9.895636 hw_loss 0.557414 lr 0.00044994 rank 0
2023-02-23 05:17:40,806 DEBUG TRAIN Batch 14/6700 loss 8.772871 loss_att 12.854860 loss_ctc 14.456404 loss_rnnt 6.987580 hw_loss 0.395792 lr 0.00045004 rank 1
2023-02-23 05:17:40,806 DEBUG TRAIN Batch 14/6700 loss 14.936091 loss_att 17.340792 loss_ctc 21.595034 loss_rnnt 13.291654 hw_loss 0.516820 lr 0.00044995 rank 2
2023-02-23 05:17:40,808 DEBUG TRAIN Batch 14/6700 loss 6.775176 loss_att 9.626358 loss_ctc 9.438106 loss_rnnt 5.611433 hw_loss 0.447091 lr 0.00044993 rank 7
2023-02-23 05:17:40,810 DEBUG TRAIN Batch 14/6700 loss 16.169399 loss_att 22.511864 loss_ctc 22.194208 loss_rnnt 13.888323 hw_loss 0.392391 lr 0.00044994 rank 4
2023-02-23 05:17:40,811 DEBUG TRAIN Batch 14/6700 loss 9.705617 loss_att 13.092420 loss_ctc 12.540535 loss_rnnt 8.332950 hw_loss 0.594971 lr 0.00044993 rank 3
2023-02-23 05:17:40,813 DEBUG TRAIN Batch 14/6700 loss 9.566754 loss_att 13.498313 loss_ctc 15.038996 loss_rnnt 7.847136 hw_loss 0.381890 lr 0.00044995 rank 5
2023-02-23 05:17:40,831 DEBUG TRAIN Batch 14/6700 loss 14.191632 loss_att 18.609604 loss_ctc 19.505424 loss_rnnt 12.368862 hw_loss 0.432505 lr 0.00045001 rank 6
2023-02-23 05:18:55,850 DEBUG TRAIN Batch 14/6800 loss 19.301365 loss_att 25.662638 loss_ctc 26.217781 loss_rnnt 16.841217 hw_loss 0.498193 lr 0.00044974 rank 3
2023-02-23 05:18:55,851 DEBUG TRAIN Batch 14/6800 loss 8.067842 loss_att 8.931100 loss_ctc 8.865478 loss_rnnt 7.550302 hw_loss 0.447256 lr 0.00044977 rank 5
2023-02-23 05:18:55,851 DEBUG TRAIN Batch 14/6800 loss 8.580929 loss_att 11.211681 loss_ctc 11.553016 loss_rnnt 7.416480 hw_loss 0.453786 lr 0.00044974 rank 7
2023-02-23 05:18:55,857 DEBUG TRAIN Batch 14/6800 loss 4.556941 loss_att 6.556584 loss_ctc 5.681488 loss_rnnt 3.809862 hw_loss 0.369768 lr 0.00044976 rank 4
2023-02-23 05:18:55,857 DEBUG TRAIN Batch 14/6800 loss 10.825661 loss_att 13.998905 loss_ctc 14.023388 loss_rnnt 9.498766 hw_loss 0.498529 lr 0.00044985 rank 1
2023-02-23 05:18:55,858 DEBUG TRAIN Batch 14/6800 loss 9.291644 loss_att 11.715006 loss_ctc 11.518254 loss_rnnt 8.233616 hw_loss 0.518390 lr 0.00044977 rank 2
2023-02-23 05:18:55,859 DEBUG TRAIN Batch 14/6800 loss 17.614166 loss_att 20.523338 loss_ctc 29.748154 loss_rnnt 15.176841 hw_loss 0.445546 lr 0.00044982 rank 6
2023-02-23 05:18:55,862 DEBUG TRAIN Batch 14/6800 loss 16.130537 loss_att 16.683617 loss_ctc 17.472973 loss_rnnt 15.635164 hw_loss 0.385809 lr 0.00044976 rank 0
2023-02-23 05:20:08,593 DEBUG TRAIN Batch 14/6900 loss 7.851520 loss_att 8.006338 loss_ctc 9.341657 loss_rnnt 7.295544 hw_loss 0.611864 lr 0.00044956 rank 7
2023-02-23 05:20:08,599 DEBUG TRAIN Batch 14/6900 loss 9.004643 loss_att 12.191448 loss_ctc 13.207596 loss_rnnt 7.608018 hw_loss 0.372882 lr 0.00044956 rank 3
2023-02-23 05:20:08,600 DEBUG TRAIN Batch 14/6900 loss 5.291296 loss_att 7.196501 loss_ctc 6.272756 loss_rnnt 4.539093 hw_loss 0.450566 lr 0.00044964 rank 6
2023-02-23 05:20:08,600 DEBUG TRAIN Batch 14/6900 loss 12.430863 loss_att 16.043407 loss_ctc 15.860694 loss_rnnt 10.955239 hw_loss 0.554632 lr 0.00044959 rank 2
2023-02-23 05:20:08,601 DEBUG TRAIN Batch 14/6900 loss 11.424889 loss_att 12.371901 loss_ctc 12.725048 loss_rnnt 10.825918 hw_loss 0.442901 lr 0.00044967 rank 1
2023-02-23 05:20:08,603 DEBUG TRAIN Batch 14/6900 loss 8.639678 loss_att 11.227243 loss_ctc 8.015134 loss_rnnt 7.955596 hw_loss 0.468455 lr 0.00044958 rank 0
2023-02-23 05:20:08,604 DEBUG TRAIN Batch 14/6900 loss 23.495651 loss_att 27.629124 loss_ctc 26.545097 loss_rnnt 22.072922 hw_loss 0.355199 lr 0.00044959 rank 5
2023-02-23 05:20:08,650 DEBUG TRAIN Batch 14/6900 loss 7.340211 loss_att 8.265287 loss_ctc 7.367576 loss_rnnt 6.928825 hw_loss 0.417603 lr 0.00044957 rank 4
2023-02-23 05:21:20,713 DEBUG TRAIN Batch 14/7000 loss 12.344468 loss_att 16.668285 loss_ctc 17.601620 loss_rnnt 10.542944 hw_loss 0.442138 lr 0.00044938 rank 7
2023-02-23 05:21:20,715 DEBUG TRAIN Batch 14/7000 loss 13.113266 loss_att 16.739489 loss_ctc 15.924910 loss_rnnt 11.809082 hw_loss 0.382600 lr 0.00044941 rank 5
2023-02-23 05:21:20,717 DEBUG TRAIN Batch 14/7000 loss 27.710703 loss_att 28.048805 loss_ctc 32.481781 loss_rnnt 26.763123 hw_loss 0.457153 lr 0.00044949 rank 1
2023-02-23 05:21:20,718 DEBUG TRAIN Batch 14/7000 loss 8.652781 loss_att 11.804106 loss_ctc 10.561140 loss_rnnt 7.461958 hw_loss 0.573954 lr 0.00044940 rank 0
2023-02-23 05:21:20,719 DEBUG TRAIN Batch 14/7000 loss 14.392323 loss_att 15.307070 loss_ctc 17.899242 loss_rnnt 13.514145 hw_loss 0.426821 lr 0.00044939 rank 4
2023-02-23 05:21:20,720 DEBUG TRAIN Batch 14/7000 loss 11.193578 loss_att 14.256123 loss_ctc 15.947632 loss_rnnt 9.718219 hw_loss 0.429331 lr 0.00044938 rank 3
2023-02-23 05:21:20,719 DEBUG TRAIN Batch 14/7000 loss 6.933309 loss_att 9.388490 loss_ctc 9.247644 loss_rnnt 5.944612 hw_loss 0.354529 lr 0.00044941 rank 2
2023-02-23 05:21:20,721 DEBUG TRAIN Batch 14/7000 loss 10.166943 loss_att 13.244063 loss_ctc 11.027000 loss_rnnt 9.224344 hw_loss 0.398439 lr 0.00044946 rank 6
2023-02-23 05:22:34,998 DEBUG TRAIN Batch 14/7100 loss 7.464559 loss_att 11.962835 loss_ctc 12.653255 loss_rnnt 5.709614 hw_loss 0.306493 lr 0.00044921 rank 4
2023-02-23 05:22:35,001 DEBUG TRAIN Batch 14/7100 loss 18.992905 loss_att 20.784538 loss_ctc 24.195494 loss_rnnt 17.715527 hw_loss 0.422574 lr 0.00044920 rank 3
2023-02-23 05:22:35,001 DEBUG TRAIN Batch 14/7100 loss 6.831675 loss_att 9.354382 loss_ctc 8.431792 loss_rnnt 5.935110 hw_loss 0.335015 lr 0.00044920 rank 7
2023-02-23 05:22:35,001 DEBUG TRAIN Batch 14/7100 loss 27.323399 loss_att 30.140503 loss_ctc 30.748318 loss_rnnt 26.120728 hw_loss 0.342363 lr 0.00044928 rank 6
2023-02-23 05:22:35,007 DEBUG TRAIN Batch 14/7100 loss 5.399654 loss_att 10.043482 loss_ctc 6.302822 loss_rnnt 4.163084 hw_loss 0.351342 lr 0.00044923 rank 2
2023-02-23 05:22:35,007 DEBUG TRAIN Batch 14/7100 loss 7.396058 loss_att 11.085520 loss_ctc 10.376088 loss_rnnt 6.069946 hw_loss 0.357906 lr 0.00044931 rank 1
2023-02-23 05:22:35,008 DEBUG TRAIN Batch 14/7100 loss 15.849670 loss_att 18.842043 loss_ctc 13.331508 loss_rnnt 15.353538 hw_loss 0.437651 lr 0.00044923 rank 5
2023-02-23 05:22:35,009 DEBUG TRAIN Batch 14/7100 loss 16.025156 loss_att 19.712818 loss_ctc 17.880194 loss_rnnt 14.831902 hw_loss 0.390719 lr 0.00044922 rank 0
2023-02-23 05:23:47,463 DEBUG TRAIN Batch 14/7200 loss 13.681374 loss_att 15.425380 loss_ctc 15.567779 loss_rnnt 12.846832 hw_loss 0.439163 lr 0.00044902 rank 7
2023-02-23 05:23:47,466 DEBUG TRAIN Batch 14/7200 loss 13.765661 loss_att 15.513429 loss_ctc 18.015251 loss_rnnt 12.643959 hw_loss 0.385381 lr 0.00044913 rank 1
2023-02-23 05:23:47,467 DEBUG TRAIN Batch 14/7200 loss 9.110471 loss_att 12.430252 loss_ctc 14.208594 loss_rnnt 7.548775 hw_loss 0.408730 lr 0.00044902 rank 3
2023-02-23 05:23:47,469 DEBUG TRAIN Batch 14/7200 loss 7.500447 loss_att 12.188990 loss_ctc 11.178215 loss_rnnt 5.857429 hw_loss 0.403012 lr 0.00044904 rank 5
2023-02-23 05:23:47,474 DEBUG TRAIN Batch 14/7200 loss 6.104121 loss_att 10.160728 loss_ctc 10.146441 loss_rnnt 4.527877 hw_loss 0.423649 lr 0.00044905 rank 2
2023-02-23 05:23:47,475 DEBUG TRAIN Batch 14/7200 loss 12.126279 loss_att 20.196711 loss_ctc 20.110786 loss_rnnt 9.211577 hw_loss 0.442526 lr 0.00044910 rank 6
2023-02-23 05:23:47,482 DEBUG TRAIN Batch 14/7200 loss 7.033543 loss_att 10.137825 loss_ctc 9.178066 loss_rnnt 5.878036 hw_loss 0.466338 lr 0.00044903 rank 4
2023-02-23 05:23:47,494 DEBUG TRAIN Batch 14/7200 loss 10.543209 loss_att 15.948938 loss_ctc 15.329763 loss_rnnt 8.576423 hw_loss 0.463939 lr 0.00044904 rank 0
2023-02-23 05:24:59,371 DEBUG TRAIN Batch 14/7300 loss 14.934127 loss_att 18.037182 loss_ctc 17.525612 loss_rnnt 13.782536 hw_loss 0.347717 lr 0.00044885 rank 4
2023-02-23 05:24:59,376 DEBUG TRAIN Batch 14/7300 loss 8.515390 loss_att 12.366989 loss_ctc 8.678649 loss_rnnt 7.452079 hw_loss 0.508544 lr 0.00044884 rank 7
2023-02-23 05:24:59,377 DEBUG TRAIN Batch 14/7300 loss 7.982222 loss_att 10.706609 loss_ctc 13.230568 loss_rnnt 6.527792 hw_loss 0.393324 lr 0.00044884 rank 3
2023-02-23 05:24:59,381 DEBUG TRAIN Batch 14/7300 loss 13.719889 loss_att 17.467354 loss_ctc 13.886673 loss_rnnt 12.702284 hw_loss 0.461015 lr 0.00044886 rank 2
2023-02-23 05:24:59,383 DEBUG TRAIN Batch 14/7300 loss 6.420057 loss_att 8.884818 loss_ctc 9.063044 loss_rnnt 5.343796 hw_loss 0.432957 lr 0.00044886 rank 5
2023-02-23 05:24:59,384 DEBUG TRAIN Batch 14/7300 loss 12.068560 loss_att 13.070592 loss_ctc 15.099148 loss_rnnt 11.236921 hw_loss 0.425915 lr 0.00044895 rank 1
2023-02-23 05:24:59,385 DEBUG TRAIN Batch 14/7300 loss 14.759158 loss_att 17.914072 loss_ctc 19.194191 loss_rnnt 13.292531 hw_loss 0.458075 lr 0.00044892 rank 6
2023-02-23 05:24:59,386 DEBUG TRAIN Batch 14/7300 loss 18.034370 loss_att 21.024845 loss_ctc 22.937492 loss_rnnt 16.566671 hw_loss 0.404728 lr 0.00044886 rank 0
2023-02-23 05:26:12,128 DEBUG TRAIN Batch 14/7400 loss 9.090932 loss_att 11.172515 loss_ctc 15.096029 loss_rnnt 7.688841 hw_loss 0.347051 lr 0.00044873 rank 6
2023-02-23 05:26:12,129 DEBUG TRAIN Batch 14/7400 loss 10.465764 loss_att 13.097434 loss_ctc 12.232822 loss_rnnt 9.454090 hw_loss 0.468248 lr 0.00044867 rank 4
2023-02-23 05:26:12,131 DEBUG TRAIN Batch 14/7400 loss 8.961091 loss_att 11.002169 loss_ctc 13.200418 loss_rnnt 7.779162 hw_loss 0.390880 lr 0.00044866 rank 7
2023-02-23 05:26:12,133 DEBUG TRAIN Batch 14/7400 loss 7.243721 loss_att 9.026306 loss_ctc 9.992142 loss_rnnt 6.288354 hw_loss 0.435738 lr 0.00044866 rank 3
2023-02-23 05:26:12,133 DEBUG TRAIN Batch 14/7400 loss 17.445910 loss_att 21.155664 loss_ctc 24.141066 loss_rnnt 15.586380 hw_loss 0.421670 lr 0.00044868 rank 5
2023-02-23 05:26:12,135 DEBUG TRAIN Batch 14/7400 loss 9.519064 loss_att 10.436352 loss_ctc 14.128791 loss_rnnt 8.504533 hw_loss 0.405830 lr 0.00044877 rank 1
2023-02-23 05:26:12,136 DEBUG TRAIN Batch 14/7400 loss 8.011541 loss_att 11.159282 loss_ctc 8.417136 loss_rnnt 7.127131 hw_loss 0.376469 lr 0.00044868 rank 2
2023-02-23 05:26:12,155 DEBUG TRAIN Batch 14/7400 loss 18.871105 loss_att 19.468191 loss_ctc 25.858946 loss_rnnt 17.546749 hw_loss 0.512300 lr 0.00044868 rank 0
2023-02-23 05:27:27,090 DEBUG TRAIN Batch 14/7500 loss 9.923802 loss_att 11.118933 loss_ctc 11.297093 loss_rnnt 9.229181 hw_loss 0.510916 lr 0.00044848 rank 7
2023-02-23 05:27:27,090 DEBUG TRAIN Batch 14/7500 loss 8.229341 loss_att 9.332365 loss_ctc 13.135142 loss_rnnt 7.135786 hw_loss 0.410331 lr 0.00044855 rank 6
2023-02-23 05:27:27,091 DEBUG TRAIN Batch 14/7500 loss 9.032635 loss_att 11.491138 loss_ctc 14.368704 loss_rnnt 7.604411 hw_loss 0.421963 lr 0.00044850 rank 2
2023-02-23 05:27:27,093 DEBUG TRAIN Batch 14/7500 loss 8.303301 loss_att 15.013975 loss_ctc 10.071920 loss_rnnt 6.526717 hw_loss 0.372436 lr 0.00044849 rank 4
2023-02-23 05:27:27,094 DEBUG TRAIN Batch 14/7500 loss 11.542763 loss_att 14.511450 loss_ctc 13.765383 loss_rnnt 10.425778 hw_loss 0.425431 lr 0.00044848 rank 3
2023-02-23 05:27:27,095 DEBUG TRAIN Batch 14/7500 loss 18.224447 loss_att 24.871494 loss_ctc 26.982548 loss_rnnt 15.536570 hw_loss 0.357604 lr 0.00044849 rank 0
2023-02-23 05:27:27,099 DEBUG TRAIN Batch 14/7500 loss 13.887443 loss_att 16.744789 loss_ctc 19.656439 loss_rnnt 12.277702 hw_loss 0.504508 lr 0.00044858 rank 1
2023-02-23 05:27:27,110 DEBUG TRAIN Batch 14/7500 loss 10.679759 loss_att 12.235686 loss_ctc 12.135497 loss_rnnt 10.002501 hw_loss 0.322452 lr 0.00044850 rank 5
2023-02-23 05:28:40,732 DEBUG TRAIN Batch 14/7600 loss 21.274170 loss_att 20.052984 loss_ctc 23.313559 loss_rnnt 20.956055 hw_loss 0.544561 lr 0.00044832 rank 2
2023-02-23 05:28:40,733 DEBUG TRAIN Batch 14/7600 loss 9.861543 loss_att 13.918669 loss_ctc 17.001373 loss_rnnt 7.911495 hw_loss 0.349957 lr 0.00044830 rank 7
2023-02-23 05:28:40,735 DEBUG TRAIN Batch 14/7600 loss 17.327120 loss_att 21.589371 loss_ctc 24.413044 loss_rnnt 15.303467 hw_loss 0.424524 lr 0.00044837 rank 6
2023-02-23 05:28:40,737 DEBUG TRAIN Batch 14/7600 loss 5.770442 loss_att 9.443976 loss_ctc 8.806082 loss_rnnt 4.372571 hw_loss 0.484523 lr 0.00044840 rank 1
2023-02-23 05:28:40,738 DEBUG TRAIN Batch 14/7600 loss 9.203335 loss_att 11.083839 loss_ctc 10.500190 loss_rnnt 8.368771 hw_loss 0.535405 lr 0.00044831 rank 0
2023-02-23 05:28:40,739 DEBUG TRAIN Batch 14/7600 loss 13.367643 loss_att 13.968312 loss_ctc 16.511091 loss_rnnt 12.614120 hw_loss 0.401745 lr 0.00044830 rank 3
2023-02-23 05:28:40,738 DEBUG TRAIN Batch 14/7600 loss 14.749308 loss_att 15.335022 loss_ctc 18.369242 loss_rnnt 13.920855 hw_loss 0.428721 lr 0.00044831 rank 4
2023-02-23 05:28:40,743 DEBUG TRAIN Batch 14/7600 loss 11.286314 loss_att 13.251436 loss_ctc 16.754276 loss_rnnt 9.932528 hw_loss 0.434437 lr 0.00044832 rank 5
2023-02-23 05:29:53,071 DEBUG TRAIN Batch 14/7700 loss 32.101234 loss_att 36.497356 loss_ctc 40.010735 loss_rnnt 29.944668 hw_loss 0.417639 lr 0.00044814 rank 2
2023-02-23 05:29:53,072 DEBUG TRAIN Batch 14/7700 loss 10.213432 loss_att 10.539564 loss_ctc 12.030933 loss_rnnt 9.605876 hw_loss 0.562493 lr 0.00044812 rank 3
2023-02-23 05:29:53,074 DEBUG TRAIN Batch 14/7700 loss 8.046699 loss_att 11.658073 loss_ctc 10.552151 loss_rnnt 6.725876 hw_loss 0.495914 lr 0.00044812 rank 7
2023-02-23 05:29:53,080 DEBUG TRAIN Batch 14/7700 loss 9.976926 loss_att 13.945446 loss_ctc 12.514910 loss_rnnt 8.664519 hw_loss 0.338070 lr 0.00044819 rank 6
2023-02-23 05:29:53,079 DEBUG TRAIN Batch 14/7700 loss 11.305683 loss_att 11.919020 loss_ctc 12.132895 loss_rnnt 10.818389 hw_loss 0.476873 lr 0.00044813 rank 4
2023-02-23 05:29:53,083 DEBUG TRAIN Batch 14/7700 loss 10.873794 loss_att 11.391069 loss_ctc 14.541385 loss_rnnt 9.990040 hw_loss 0.546162 lr 0.00044813 rank 0
2023-02-23 05:29:53,084 DEBUG TRAIN Batch 14/7700 loss 10.174583 loss_att 12.224189 loss_ctc 12.963623 loss_rnnt 9.166289 hw_loss 0.424691 lr 0.00044814 rank 5
2023-02-23 05:29:53,125 DEBUG TRAIN Batch 14/7700 loss 16.686531 loss_att 18.797226 loss_ctc 24.748556 loss_rnnt 15.001661 hw_loss 0.352118 lr 0.00044822 rank 1
2023-02-23 05:31:07,662 DEBUG TRAIN Batch 14/7800 loss 7.094285 loss_att 11.092977 loss_ctc 10.082781 loss_rnnt 5.697692 hw_loss 0.371979 lr 0.00044795 rank 0
2023-02-23 05:31:07,664 DEBUG TRAIN Batch 14/7800 loss 11.435835 loss_att 19.169348 loss_ctc 14.222378 loss_rnnt 9.309221 hw_loss 0.390696 lr 0.00044795 rank 4
2023-02-23 05:31:07,671 DEBUG TRAIN Batch 14/7800 loss 7.742503 loss_att 11.113602 loss_ctc 9.294514 loss_rnnt 6.646393 hw_loss 0.403041 lr 0.00044794 rank 7
2023-02-23 05:31:07,673 DEBUG TRAIN Batch 14/7800 loss 7.464240 loss_att 7.661906 loss_ctc 10.176374 loss_rnnt 6.860033 hw_loss 0.380728 lr 0.00044801 rank 6
2023-02-23 05:31:07,673 DEBUG TRAIN Batch 14/7800 loss 5.482666 loss_att 9.419332 loss_ctc 8.979428 loss_rnnt 4.025780 hw_loss 0.381223 lr 0.00044796 rank 2
2023-02-23 05:31:07,680 DEBUG TRAIN Batch 14/7800 loss 18.006128 loss_att 22.611975 loss_ctc 18.654160 loss_rnnt 16.781767 hw_loss 0.406479 lr 0.00044804 rank 1
2023-02-23 05:31:07,686 DEBUG TRAIN Batch 14/7800 loss 13.855205 loss_att 14.085012 loss_ctc 17.250092 loss_rnnt 13.135103 hw_loss 0.415290 lr 0.00044796 rank 5
2023-02-23 05:31:07,710 DEBUG TRAIN Batch 14/7800 loss 10.151512 loss_att 14.665045 loss_ctc 13.397304 loss_rnnt 8.587058 hw_loss 0.429331 lr 0.00044794 rank 3
2023-02-23 05:32:21,525 DEBUG TRAIN Batch 14/7900 loss 20.196001 loss_att 21.694763 loss_ctc 23.902510 loss_rnnt 19.202738 hw_loss 0.373700 lr 0.00044776 rank 3
2023-02-23 05:32:21,526 DEBUG TRAIN Batch 14/7900 loss 13.723769 loss_att 17.560463 loss_ctc 22.182961 loss_rnnt 11.611679 hw_loss 0.406611 lr 0.00044778 rank 2
2023-02-23 05:32:21,527 DEBUG TRAIN Batch 14/7900 loss 15.836057 loss_att 24.361399 loss_ctc 23.682476 loss_rnnt 12.869421 hw_loss 0.403832 lr 0.00044777 rank 4
2023-02-23 05:32:21,527 DEBUG TRAIN Batch 14/7900 loss 14.539110 loss_att 17.343714 loss_ctc 23.798674 loss_rnnt 12.458818 hw_loss 0.533928 lr 0.00044776 rank 7
2023-02-23 05:32:21,529 DEBUG TRAIN Batch 14/7900 loss 7.879135 loss_att 10.021476 loss_ctc 8.499543 loss_rnnt 7.186466 hw_loss 0.340274 lr 0.00044783 rank 6
2023-02-23 05:32:21,534 DEBUG TRAIN Batch 14/7900 loss 14.888583 loss_att 18.625353 loss_ctc 17.241215 loss_rnnt 13.661455 hw_loss 0.311418 lr 0.00044786 rank 1
2023-02-23 05:32:21,533 DEBUG TRAIN Batch 14/7900 loss 7.156627 loss_att 9.732019 loss_ctc 9.923893 loss_rnnt 6.033027 hw_loss 0.449163 lr 0.00044778 rank 5
2023-02-23 05:32:21,535 DEBUG TRAIN Batch 14/7900 loss 5.728719 loss_att 11.653684 loss_ctc 9.324251 loss_rnnt 3.847673 hw_loss 0.406215 lr 0.00044777 rank 0
2023-02-23 05:33:33,050 DEBUG TRAIN Batch 14/8000 loss 14.615832 loss_att 17.896631 loss_ctc 18.456316 loss_rnnt 13.234890 hw_loss 0.398845 lr 0.00044768 rank 1
2023-02-23 05:33:33,051 DEBUG TRAIN Batch 14/8000 loss 14.445638 loss_att 14.409391 loss_ctc 15.864088 loss_rnnt 14.050026 hw_loss 0.400752 lr 0.00044758 rank 7
2023-02-23 05:33:33,052 DEBUG TRAIN Batch 14/8000 loss 26.287773 loss_att 28.321438 loss_ctc 31.982630 loss_rnnt 24.863609 hw_loss 0.483970 lr 0.00044758 rank 3
2023-02-23 05:33:33,052 DEBUG TRAIN Batch 14/8000 loss 14.566835 loss_att 18.371298 loss_ctc 18.115429 loss_rnnt 13.090795 hw_loss 0.453753 lr 0.00044759 rank 4
2023-02-23 05:33:33,054 DEBUG TRAIN Batch 14/8000 loss 7.825092 loss_att 11.641081 loss_ctc 9.912043 loss_rnnt 6.569880 hw_loss 0.400789 lr 0.00044760 rank 2
2023-02-23 05:33:33,055 DEBUG TRAIN Batch 14/8000 loss 12.048769 loss_att 12.951422 loss_ctc 14.561203 loss_rnnt 11.296733 hw_loss 0.443462 lr 0.00044760 rank 5
2023-02-23 05:33:33,056 DEBUG TRAIN Batch 14/8000 loss 12.490104 loss_att 10.883001 loss_ctc 10.294107 loss_rnnt 12.887592 hw_loss 0.406372 lr 0.00044765 rank 6
2023-02-23 05:33:33,105 DEBUG TRAIN Batch 14/8000 loss 18.382135 loss_att 17.588985 loss_ctc 27.904144 loss_rnnt 17.066097 hw_loss 0.384499 lr 0.00044760 rank 0
2023-02-23 05:34:45,593 DEBUG TRAIN Batch 14/8100 loss 17.887827 loss_att 20.227497 loss_ctc 26.349756 loss_rnnt 16.067877 hw_loss 0.419550 lr 0.00044740 rank 7
2023-02-23 05:34:45,594 DEBUG TRAIN Batch 14/8100 loss 13.772120 loss_att 15.123302 loss_ctc 17.893663 loss_rnnt 12.687369 hw_loss 0.496829 lr 0.00044751 rank 1
2023-02-23 05:34:45,594 DEBUG TRAIN Batch 14/8100 loss 23.764029 loss_att 25.347488 loss_ctc 31.986031 loss_rnnt 22.150843 hw_loss 0.375422 lr 0.00044741 rank 4
2023-02-23 05:34:45,597 DEBUG TRAIN Batch 14/8100 loss 16.411133 loss_att 23.656706 loss_ctc 19.715893 loss_rnnt 14.325026 hw_loss 0.368169 lr 0.00044742 rank 0
2023-02-23 05:34:45,596 DEBUG TRAIN Batch 14/8100 loss 12.324017 loss_att 15.437893 loss_ctc 16.112360 loss_rnnt 10.991096 hw_loss 0.384436 lr 0.00044740 rank 3
2023-02-23 05:34:45,597 DEBUG TRAIN Batch 14/8100 loss 10.107118 loss_att 14.502397 loss_ctc 14.166053 loss_rnnt 8.496719 hw_loss 0.356535 lr 0.00044747 rank 6
2023-02-23 05:34:45,599 DEBUG TRAIN Batch 14/8100 loss 7.620021 loss_att 8.289022 loss_ctc 10.487126 loss_rnnt 6.849534 hw_loss 0.477010 lr 0.00044742 rank 2
2023-02-23 05:34:45,621 DEBUG TRAIN Batch 14/8100 loss 13.609224 loss_att 12.474939 loss_ctc 15.772509 loss_rnnt 13.252690 hw_loss 0.553038 lr 0.00044742 rank 5
2023-02-23 05:35:59,215 DEBUG TRAIN Batch 14/8200 loss 10.555503 loss_att 14.082091 loss_ctc 13.932889 loss_rnnt 9.178627 hw_loss 0.414826 lr 0.00044722 rank 3
2023-02-23 05:35:59,217 DEBUG TRAIN Batch 14/8200 loss 9.135885 loss_att 9.291422 loss_ctc 10.922298 loss_rnnt 8.520761 hw_loss 0.648426 lr 0.00044722 rank 7
2023-02-23 05:35:59,218 DEBUG TRAIN Batch 14/8200 loss 17.554068 loss_att 19.083647 loss_ctc 21.217590 loss_rnnt 16.507528 hw_loss 0.472790 lr 0.00044725 rank 2
2023-02-23 05:35:59,219 DEBUG TRAIN Batch 14/8200 loss 1.700912 loss_att 4.531081 loss_ctc 2.213141 loss_rnnt 0.867962 hw_loss 0.372412 lr 0.00044724 rank 5
2023-02-23 05:35:59,219 DEBUG TRAIN Batch 14/8200 loss 10.990889 loss_att 10.458677 loss_ctc 13.866239 loss_rnnt 10.407753 hw_loss 0.574120 lr 0.00044730 rank 6
2023-02-23 05:35:59,219 DEBUG TRAIN Batch 14/8200 loss 10.269896 loss_att 11.995501 loss_ctc 13.654160 loss_rnnt 9.221537 hw_loss 0.472505 lr 0.00044724 rank 0
2023-02-23 05:35:59,225 DEBUG TRAIN Batch 14/8200 loss 12.628613 loss_att 14.772081 loss_ctc 18.633823 loss_rnnt 11.188400 hw_loss 0.395294 lr 0.00044723 rank 4
2023-02-23 05:35:59,237 DEBUG TRAIN Batch 14/8200 loss 10.734705 loss_att 10.989594 loss_ctc 11.675989 loss_rnnt 10.296844 hw_loss 0.490083 lr 0.00044733 rank 1
2023-02-23 05:37:11,380 DEBUG TRAIN Batch 14/8300 loss 12.463284 loss_att 20.838076 loss_ctc 20.838655 loss_rnnt 9.469788 hw_loss 0.378415 lr 0.00044704 rank 7
2023-02-23 05:37:11,400 DEBUG TRAIN Batch 14/8300 loss 19.294275 loss_att 22.449165 loss_ctc 23.505705 loss_rnnt 17.860046 hw_loss 0.453237 lr 0.00044707 rank 2
2023-02-23 05:37:11,406 DEBUG TRAIN Batch 14/8300 loss 12.277391 loss_att 18.951176 loss_ctc 20.551107 loss_rnnt 9.572718 hw_loss 0.500166 lr 0.00044707 rank 5
2023-02-23 05:37:11,407 DEBUG TRAIN Batch 14/8300 loss 13.663502 loss_att 18.330280 loss_ctc 17.950050 loss_rnnt 11.958822 hw_loss 0.374596 lr 0.00044704 rank 3
2023-02-23 05:37:11,408 DEBUG TRAIN Batch 14/8300 loss 12.107638 loss_att 18.788353 loss_ctc 15.925023 loss_rnnt 10.068987 hw_loss 0.362855 lr 0.00044706 rank 0
2023-02-23 05:37:11,411 DEBUG TRAIN Batch 14/8300 loss 6.354872 loss_att 9.228230 loss_ctc 8.329574 loss_rnnt 5.301130 hw_loss 0.404582 lr 0.00044715 rank 1
2023-02-23 05:37:11,411 DEBUG TRAIN Batch 14/8300 loss 8.847548 loss_att 12.929324 loss_ctc 13.008043 loss_rnnt 7.250368 hw_loss 0.423921 lr 0.00044712 rank 6
2023-02-23 05:37:11,414 DEBUG TRAIN Batch 14/8300 loss 22.012083 loss_att 21.620155 loss_ctc 25.502781 loss_rnnt 21.444206 hw_loss 0.339068 lr 0.00044705 rank 4
2023-02-23 05:38:08,085 DEBUG CV Batch 14/0 loss 2.701655 loss_att 2.446015 loss_ctc 3.173940 loss_rnnt 2.236321 hw_loss 0.850295 history loss 2.601594 rank 7
2023-02-23 05:38:08,086 DEBUG CV Batch 14/0 loss 2.701655 loss_att 2.446015 loss_ctc 3.173940 loss_rnnt 2.236321 hw_loss 0.850295 history loss 2.601594 rank 0
2023-02-23 05:38:08,087 DEBUG CV Batch 14/0 loss 2.701655 loss_att 2.446015 loss_ctc 3.173940 loss_rnnt 2.236321 hw_loss 0.850295 history loss 2.601594 rank 2
2023-02-23 05:38:08,087 DEBUG CV Batch 14/0 loss 2.701655 loss_att 2.446015 loss_ctc 3.173940 loss_rnnt 2.236321 hw_loss 0.850295 history loss 2.601594 rank 4
2023-02-23 05:38:08,088 DEBUG CV Batch 14/0 loss 2.701655 loss_att 2.446015 loss_ctc 3.173940 loss_rnnt 2.236321 hw_loss 0.850295 history loss 2.601594 rank 5
2023-02-23 05:38:08,089 DEBUG CV Batch 14/0 loss 2.701655 loss_att 2.446015 loss_ctc 3.173940 loss_rnnt 2.236321 hw_loss 0.850295 history loss 2.601594 rank 3
2023-02-23 05:38:08,090 DEBUG CV Batch 14/0 loss 2.701655 loss_att 2.446015 loss_ctc 3.173940 loss_rnnt 2.236321 hw_loss 0.850295 history loss 2.601594 rank 6
2023-02-23 05:38:08,094 DEBUG CV Batch 14/0 loss 2.701655 loss_att 2.446015 loss_ctc 3.173940 loss_rnnt 2.236321 hw_loss 0.850295 history loss 2.601594 rank 1
2023-02-23 05:38:19,472 DEBUG CV Batch 14/100 loss 9.321184 loss_att 9.667192 loss_ctc 12.230968 loss_rnnt 8.511623 hw_loss 0.660728 history loss 4.291682 rank 7
2023-02-23 05:38:19,571 DEBUG CV Batch 14/100 loss 9.321184 loss_att 9.667192 loss_ctc 12.230968 loss_rnnt 8.511623 hw_loss 0.660728 history loss 4.291682 rank 2
2023-02-23 05:38:19,686 DEBUG CV Batch 14/100 loss 9.321184 loss_att 9.667192 loss_ctc 12.230968 loss_rnnt 8.511623 hw_loss 0.660728 history loss 4.291682 rank 4
2023-02-23 05:38:19,765 DEBUG CV Batch 14/100 loss 9.321184 loss_att 9.667192 loss_ctc 12.230968 loss_rnnt 8.511623 hw_loss 0.660728 history loss 4.291682 rank 1
2023-02-23 05:38:19,841 DEBUG CV Batch 14/100 loss 9.321184 loss_att 9.667192 loss_ctc 12.230968 loss_rnnt 8.511623 hw_loss 0.660728 history loss 4.291682 rank 6
2023-02-23 05:38:19,867 DEBUG CV Batch 14/100 loss 9.321184 loss_att 9.667192 loss_ctc 12.230968 loss_rnnt 8.511623 hw_loss 0.660728 history loss 4.291682 rank 0
2023-02-23 05:38:19,917 DEBUG CV Batch 14/100 loss 9.321184 loss_att 9.667192 loss_ctc 12.230968 loss_rnnt 8.511623 hw_loss 0.660728 history loss 4.291682 rank 3
2023-02-23 05:38:20,018 DEBUG CV Batch 14/100 loss 9.321184 loss_att 9.667192 loss_ctc 12.230968 loss_rnnt 8.511623 hw_loss 0.660728 history loss 4.291682 rank 5
2023-02-23 05:38:33,023 DEBUG CV Batch 14/200 loss 10.350317 loss_att 20.606533 loss_ctc 9.862780 loss_rnnt 8.189167 hw_loss 0.327959 history loss 4.947072 rank 7
2023-02-23 05:38:33,128 DEBUG CV Batch 14/200 loss 10.350317 loss_att 20.606533 loss_ctc 9.862780 loss_rnnt 8.189167 hw_loss 0.327959 history loss 4.947072 rank 2
2023-02-23 05:38:33,441 DEBUG CV Batch 14/200 loss 10.350317 loss_att 20.606533 loss_ctc 9.862780 loss_rnnt 8.189167 hw_loss 0.327959 history loss 4.947072 rank 1
2023-02-23 05:38:33,901 DEBUG CV Batch 14/200 loss 10.350317 loss_att 20.606533 loss_ctc 9.862780 loss_rnnt 8.189167 hw_loss 0.327959 history loss 4.947072 rank 4
2023-02-23 05:38:33,935 DEBUG CV Batch 14/200 loss 10.350317 loss_att 20.606533 loss_ctc 9.862780 loss_rnnt 8.189167 hw_loss 0.327959 history loss 4.947072 rank 6
2023-02-23 05:38:34,127 DEBUG CV Batch 14/200 loss 10.350317 loss_att 20.606533 loss_ctc 9.862780 loss_rnnt 8.189167 hw_loss 0.327959 history loss 4.947072 rank 5
2023-02-23 05:38:34,253 DEBUG CV Batch 14/200 loss 10.350317 loss_att 20.606533 loss_ctc 9.862780 loss_rnnt 8.189167 hw_loss 0.327959 history loss 4.947072 rank 0
2023-02-23 05:38:34,288 DEBUG CV Batch 14/200 loss 10.350317 loss_att 20.606533 loss_ctc 9.862780 loss_rnnt 8.189167 hw_loss 0.327959 history loss 4.947072 rank 3
2023-02-23 05:38:45,201 DEBUG CV Batch 14/300 loss 6.285309 loss_att 7.281870 loss_ctc 10.134035 loss_rnnt 5.284183 hw_loss 0.541221 history loss 5.094514 rank 7
2023-02-23 05:38:45,346 DEBUG CV Batch 14/300 loss 6.285309 loss_att 7.281870 loss_ctc 10.134035 loss_rnnt 5.284183 hw_loss 0.541221 history loss 5.094514 rank 2
2023-02-23 05:38:45,623 DEBUG CV Batch 14/300 loss 6.285309 loss_att 7.281870 loss_ctc 10.134035 loss_rnnt 5.284183 hw_loss 0.541221 history loss 5.094514 rank 1
2023-02-23 05:38:46,261 DEBUG CV Batch 14/300 loss 6.285309 loss_att 7.281870 loss_ctc 10.134035 loss_rnnt 5.284183 hw_loss 0.541221 history loss 5.094514 rank 4
2023-02-23 05:38:46,286 DEBUG CV Batch 14/300 loss 6.285309 loss_att 7.281870 loss_ctc 10.134035 loss_rnnt 5.284183 hw_loss 0.541221 history loss 5.094514 rank 6
2023-02-23 05:38:46,536 DEBUG CV Batch 14/300 loss 6.285309 loss_att 7.281870 loss_ctc 10.134035 loss_rnnt 5.284183 hw_loss 0.541221 history loss 5.094514 rank 3
2023-02-23 05:38:46,601 DEBUG CV Batch 14/300 loss 6.285309 loss_att 7.281870 loss_ctc 10.134035 loss_rnnt 5.284183 hw_loss 0.541221 history loss 5.094514 rank 0
2023-02-23 05:38:46,799 DEBUG CV Batch 14/300 loss 6.285309 loss_att 7.281870 loss_ctc 10.134035 loss_rnnt 5.284183 hw_loss 0.541221 history loss 5.094514 rank 5
2023-02-23 05:38:57,231 DEBUG CV Batch 14/400 loss 17.786575 loss_att 77.834015 loss_ctc 7.533093 loss_rnnt 6.983999 hw_loss 0.300410 history loss 6.147186 rank 7
2023-02-23 05:38:57,575 DEBUG CV Batch 14/400 loss 17.786575 loss_att 77.834015 loss_ctc 7.533093 loss_rnnt 6.983999 hw_loss 0.300410 history loss 6.147185 rank 2
2023-02-23 05:38:57,761 DEBUG CV Batch 14/400 loss 17.786575 loss_att 77.834015 loss_ctc 7.533093 loss_rnnt 6.983999 hw_loss 0.300410 history loss 6.147185 rank 1
2023-02-23 05:38:58,559 DEBUG CV Batch 14/400 loss 17.786575 loss_att 77.834015 loss_ctc 7.533093 loss_rnnt 6.983999 hw_loss 0.300410 history loss 6.147186 rank 4
2023-02-23 05:38:58,692 DEBUG CV Batch 14/400 loss 17.786575 loss_att 77.834015 loss_ctc 7.533093 loss_rnnt 6.983999 hw_loss 0.300410 history loss 6.147186 rank 6
2023-02-23 05:38:58,738 DEBUG CV Batch 14/400 loss 17.786575 loss_att 77.834015 loss_ctc 7.533093 loss_rnnt 6.983999 hw_loss 0.300410 history loss 6.147186 rank 3
2023-02-23 05:38:58,951 DEBUG CV Batch 14/400 loss 17.786575 loss_att 77.834015 loss_ctc 7.533093 loss_rnnt 6.983999 hw_loss 0.300410 history loss 6.147185 rank 0
2023-02-23 05:38:59,435 DEBUG CV Batch 14/400 loss 17.786575 loss_att 77.834015 loss_ctc 7.533093 loss_rnnt 6.983999 hw_loss 0.300410 history loss 6.147185 rank 5
2023-02-23 05:39:07,791 DEBUG CV Batch 14/500 loss 6.609498 loss_att 7.175351 loss_ctc 7.692779 loss_rnnt 6.142381 hw_loss 0.392828 history loss 7.043038 rank 7
2023-02-23 05:39:08,178 DEBUG CV Batch 14/500 loss 6.609498 loss_att 7.175351 loss_ctc 7.692779 loss_rnnt 6.142381 hw_loss 0.392828 history loss 7.043038 rank 2
2023-02-23 05:39:08,345 DEBUG CV Batch 14/500 loss 6.609498 loss_att 7.175351 loss_ctc 7.692779 loss_rnnt 6.142381 hw_loss 0.392828 history loss 7.043038 rank 1
2023-02-23 05:39:09,138 DEBUG CV Batch 14/500 loss 6.609498 loss_att 7.175351 loss_ctc 7.692779 loss_rnnt 6.142381 hw_loss 0.392828 history loss 7.043038 rank 4
2023-02-23 05:39:09,250 DEBUG CV Batch 14/500 loss 6.609498 loss_att 7.175351 loss_ctc 7.692779 loss_rnnt 6.142381 hw_loss 0.392828 history loss 7.043038 rank 6
2023-02-23 05:39:09,440 DEBUG CV Batch 14/500 loss 6.609498 loss_att 7.175351 loss_ctc 7.692779 loss_rnnt 6.142381 hw_loss 0.392828 history loss 7.043038 rank 3
2023-02-23 05:39:09,964 DEBUG CV Batch 14/500 loss 6.609498 loss_att 7.175351 loss_ctc 7.692779 loss_rnnt 6.142381 hw_loss 0.392828 history loss 7.043038 rank 0
2023-02-23 05:39:10,483 DEBUG CV Batch 14/500 loss 6.609498 loss_att 7.175351 loss_ctc 7.692779 loss_rnnt 6.142381 hw_loss 0.392828 history loss 7.043038 rank 5
2023-02-23 05:39:19,855 DEBUG CV Batch 14/600 loss 8.707931 loss_att 7.671396 loss_ctc 9.695766 loss_rnnt 8.379827 hw_loss 0.756935 history loss 8.093358 rank 7
2023-02-23 05:39:20,468 DEBUG CV Batch 14/600 loss 8.707931 loss_att 7.671396 loss_ctc 9.695766 loss_rnnt 8.379827 hw_loss 0.756935 history loss 8.093358 rank 2
2023-02-23 05:39:20,529 DEBUG CV Batch 14/600 loss 8.707931 loss_att 7.671396 loss_ctc 9.695766 loss_rnnt 8.379827 hw_loss 0.756935 history loss 8.093358 rank 1
2023-02-23 05:39:21,362 DEBUG CV Batch 14/600 loss 8.707931 loss_att 7.671396 loss_ctc 9.695766 loss_rnnt 8.379827 hw_loss 0.756935 history loss 8.093358 rank 4
2023-02-23 05:39:21,627 DEBUG CV Batch 14/600 loss 8.707931 loss_att 7.671396 loss_ctc 9.695766 loss_rnnt 8.379827 hw_loss 0.756935 history loss 8.093358 rank 6
2023-02-23 05:39:21,896 DEBUG CV Batch 14/600 loss 8.707931 loss_att 7.671396 loss_ctc 9.695766 loss_rnnt 8.379827 hw_loss 0.756935 history loss 8.093358 rank 3
2023-02-23 05:39:22,801 DEBUG CV Batch 14/600 loss 8.707931 loss_att 7.671396 loss_ctc 9.695766 loss_rnnt 8.379827 hw_loss 0.756935 history loss 8.093358 rank 0
2023-02-23 05:39:22,910 DEBUG CV Batch 14/600 loss 8.707931 loss_att 7.671396 loss_ctc 9.695766 loss_rnnt 8.379827 hw_loss 0.756935 history loss 8.093358 rank 5
2023-02-23 05:39:31,311 DEBUG CV Batch 14/700 loss 17.647161 loss_att 66.297226 loss_ctc 14.970399 loss_rnnt 8.049394 hw_loss 0.421234 history loss 8.902414 rank 7
2023-02-23 05:39:32,037 DEBUG CV Batch 14/700 loss 17.647161 loss_att 66.297226 loss_ctc 14.970399 loss_rnnt 8.049394 hw_loss 0.421234 history loss 8.902414 rank 2
2023-02-23 05:39:32,381 DEBUG CV Batch 14/700 loss 17.647161 loss_att 66.297226 loss_ctc 14.970399 loss_rnnt 8.049394 hw_loss 0.421234 history loss 8.902414 rank 1
2023-02-23 05:39:33,317 DEBUG CV Batch 14/700 loss 17.647161 loss_att 66.297226 loss_ctc 14.970399 loss_rnnt 8.049394 hw_loss 0.421234 history loss 8.902414 rank 4
2023-02-23 05:39:33,479 DEBUG CV Batch 14/700 loss 17.647161 loss_att 66.297226 loss_ctc 14.970399 loss_rnnt 8.049394 hw_loss 0.421234 history loss 8.902414 rank 6
2023-02-23 05:39:34,245 DEBUG CV Batch 14/700 loss 17.647161 loss_att 66.297226 loss_ctc 14.970399 loss_rnnt 8.049394 hw_loss 0.421234 history loss 8.902414 rank 3
2023-02-23 05:39:34,518 DEBUG CV Batch 14/700 loss 17.647161 loss_att 66.297226 loss_ctc 14.970399 loss_rnnt 8.049394 hw_loss 0.421234 history loss 8.902414 rank 0
2023-02-23 05:39:34,836 DEBUG CV Batch 14/700 loss 17.647161 loss_att 66.297226 loss_ctc 14.970399 loss_rnnt 8.049394 hw_loss 0.421234 history loss 8.902414 rank 5
2023-02-23 05:39:42,574 DEBUG CV Batch 14/800 loss 13.556767 loss_att 12.560499 loss_ctc 17.821836 loss_rnnt 12.890213 hw_loss 0.557120 history loss 8.281574 rank 7
2023-02-23 05:39:43,423 DEBUG CV Batch 14/800 loss 13.556767 loss_att 12.560499 loss_ctc 17.821836 loss_rnnt 12.890213 hw_loss 0.557120 history loss 8.281574 rank 2
2023-02-23 05:39:43,636 DEBUG CV Batch 14/800 loss 13.556767 loss_att 12.560499 loss_ctc 17.821836 loss_rnnt 12.890213 hw_loss 0.557120 history loss 8.281574 rank 1
2023-02-23 05:39:45,511 DEBUG CV Batch 14/800 loss 13.556767 loss_att 12.560499 loss_ctc 17.821836 loss_rnnt 12.890213 hw_loss 0.557120 history loss 8.281574 rank 4
2023-02-23 05:39:45,609 DEBUG CV Batch 14/800 loss 13.556767 loss_att 12.560499 loss_ctc 17.821836 loss_rnnt 12.890213 hw_loss 0.557120 history loss 8.281574 rank 6
2023-02-23 05:39:46,062 DEBUG CV Batch 14/800 loss 13.556767 loss_att 12.560499 loss_ctc 17.821836 loss_rnnt 12.890213 hw_loss 0.557120 history loss 8.281574 rank 0
2023-02-23 05:39:46,349 DEBUG CV Batch 14/800 loss 13.556767 loss_att 12.560499 loss_ctc 17.821836 loss_rnnt 12.890213 hw_loss 0.557120 history loss 8.281574 rank 5
2023-02-23 05:39:46,763 DEBUG CV Batch 14/800 loss 13.556767 loss_att 12.560499 loss_ctc 17.821836 loss_rnnt 12.890213 hw_loss 0.557120 history loss 8.281574 rank 3
2023-02-23 05:39:56,057 DEBUG CV Batch 14/900 loss 18.982908 loss_att 29.458015 loss_ctc 26.061121 loss_rnnt 15.786842 hw_loss 0.294906 history loss 8.044829 rank 7
2023-02-23 05:39:57,082 DEBUG CV Batch 14/900 loss 18.982908 loss_att 29.458015 loss_ctc 26.061121 loss_rnnt 15.786842 hw_loss 0.294906 history loss 8.044829 rank 2
2023-02-23 05:39:57,420 DEBUG CV Batch 14/900 loss 18.982908 loss_att 29.458015 loss_ctc 26.061121 loss_rnnt 15.786842 hw_loss 0.294906 history loss 8.044829 rank 1
2023-02-23 05:39:59,637 DEBUG CV Batch 14/900 loss 18.982908 loss_att 29.458015 loss_ctc 26.061121 loss_rnnt 15.786842 hw_loss 0.294906 history loss 8.044829 rank 6
2023-02-23 05:39:59,804 DEBUG CV Batch 14/900 loss 18.982908 loss_att 29.458015 loss_ctc 26.061121 loss_rnnt 15.786842 hw_loss 0.294906 history loss 8.044829 rank 4
2023-02-23 05:40:00,089 DEBUG CV Batch 14/900 loss 18.982908 loss_att 29.458015 loss_ctc 26.061121 loss_rnnt 15.786842 hw_loss 0.294906 history loss 8.044829 rank 0
2023-02-23 05:40:00,133 DEBUG CV Batch 14/900 loss 18.982908 loss_att 29.458015 loss_ctc 26.061121 loss_rnnt 15.786842 hw_loss 0.294906 history loss 8.044829 rank 5
2023-02-23 05:40:00,880 DEBUG CV Batch 14/900 loss 18.982908 loss_att 29.458015 loss_ctc 26.061121 loss_rnnt 15.786842 hw_loss 0.294906 history loss 8.044829 rank 3
2023-02-23 05:40:08,428 DEBUG CV Batch 14/1000 loss 5.457342 loss_att 5.999809 loss_ctc 6.505195 loss_rnnt 4.890264 hw_loss 0.597883 history loss 7.781466 rank 7
2023-02-23 05:40:09,749 DEBUG CV Batch 14/1000 loss 5.457342 loss_att 5.999809 loss_ctc 6.505195 loss_rnnt 4.890264 hw_loss 0.597883 history loss 7.781466 rank 2
2023-02-23 05:40:10,101 DEBUG CV Batch 14/1000 loss 5.457342 loss_att 5.999809 loss_ctc 6.505195 loss_rnnt 4.890264 hw_loss 0.597883 history loss 7.781466 rank 1
2023-02-23 05:40:12,111 DEBUG CV Batch 14/1000 loss 5.457342 loss_att 5.999809 loss_ctc 6.505195 loss_rnnt 4.890264 hw_loss 0.597883 history loss 7.781466 rank 4
2023-02-23 05:40:12,366 DEBUG CV Batch 14/1000 loss 5.457342 loss_att 5.999809 loss_ctc 6.505195 loss_rnnt 4.890264 hw_loss 0.597883 history loss 7.781466 rank 6
2023-02-23 05:40:12,985 DEBUG CV Batch 14/1000 loss 5.457342 loss_att 5.999809 loss_ctc 6.505195 loss_rnnt 4.890264 hw_loss 0.597883 history loss 7.781466 rank 0
2023-02-23 05:40:13,129 DEBUG CV Batch 14/1000 loss 5.457342 loss_att 5.999809 loss_ctc 6.505195 loss_rnnt 4.890264 hw_loss 0.597883 history loss 7.781466 rank 5
2023-02-23 05:40:13,266 DEBUG CV Batch 14/1000 loss 5.457342 loss_att 5.999809 loss_ctc 6.505195 loss_rnnt 4.890264 hw_loss 0.597883 history loss 7.781466 rank 3
2023-02-23 05:40:20,610 DEBUG CV Batch 14/1100 loss 6.214352 loss_att 5.629486 loss_ctc 7.566695 loss_rnnt 5.727086 hw_loss 0.794862 history loss 7.740323 rank 7
2023-02-23 05:40:22,008 DEBUG CV Batch 14/1100 loss 6.214352 loss_att 5.629486 loss_ctc 7.566695 loss_rnnt 5.727086 hw_loss 0.794862 history loss 7.740323 rank 2
2023-02-23 05:40:22,670 DEBUG CV Batch 14/1100 loss 6.214352 loss_att 5.629486 loss_ctc 7.566695 loss_rnnt 5.727086 hw_loss 0.794862 history loss 7.740323 rank 1
2023-02-23 05:40:24,232 DEBUG CV Batch 14/1100 loss 6.214352 loss_att 5.629486 loss_ctc 7.566695 loss_rnnt 5.727086 hw_loss 0.794862 history loss 7.740323 rank 4
2023-02-23 05:40:24,519 DEBUG CV Batch 14/1100 loss 6.214352 loss_att 5.629486 loss_ctc 7.566695 loss_rnnt 5.727086 hw_loss 0.794862 history loss 7.740323 rank 6
2023-02-23 05:40:25,348 DEBUG CV Batch 14/1100 loss 6.214352 loss_att 5.629486 loss_ctc 7.566695 loss_rnnt 5.727086 hw_loss 0.794862 history loss 7.740323 rank 0
2023-02-23 05:40:25,545 DEBUG CV Batch 14/1100 loss 6.214352 loss_att 5.629486 loss_ctc 7.566695 loss_rnnt 5.727086 hw_loss 0.794862 history loss 7.740323 rank 3
2023-02-23 05:40:25,652 DEBUG CV Batch 14/1100 loss 6.214352 loss_att 5.629486 loss_ctc 7.566695 loss_rnnt 5.727086 hw_loss 0.794862 history loss 7.740323 rank 5
2023-02-23 05:40:31,388 DEBUG CV Batch 14/1200 loss 7.705959 loss_att 9.587581 loss_ctc 9.536181 loss_rnnt 6.871782 hw_loss 0.400920 history loss 8.123146 rank 7
2023-02-23 05:40:33,123 DEBUG CV Batch 14/1200 loss 7.705959 loss_att 9.587581 loss_ctc 9.536181 loss_rnnt 6.871782 hw_loss 0.400920 history loss 8.123146 rank 2
2023-02-23 05:40:33,857 DEBUG CV Batch 14/1200 loss 7.705959 loss_att 9.587581 loss_ctc 9.536181 loss_rnnt 6.871782 hw_loss 0.400920 history loss 8.123146 rank 1
2023-02-23 05:40:34,927 DEBUG CV Batch 14/1200 loss 7.705959 loss_att 9.587581 loss_ctc 9.536181 loss_rnnt 6.871782 hw_loss 0.400920 history loss 8.123146 rank 4
2023-02-23 05:40:35,258 DEBUG CV Batch 14/1200 loss 7.705959 loss_att 9.587581 loss_ctc 9.536181 loss_rnnt 6.871782 hw_loss 0.400920 history loss 8.123146 rank 6
2023-02-23 05:40:36,297 DEBUG CV Batch 14/1200 loss 7.705959 loss_att 9.587581 loss_ctc 9.536181 loss_rnnt 6.871782 hw_loss 0.400920 history loss 8.123146 rank 3
2023-02-23 05:40:36,385 DEBUG CV Batch 14/1200 loss 7.705959 loss_att 9.587581 loss_ctc 9.536181 loss_rnnt 6.871782 hw_loss 0.400920 history loss 8.123146 rank 0
2023-02-23 05:40:37,199 DEBUG CV Batch 14/1200 loss 7.705959 loss_att 9.587581 loss_ctc 9.536181 loss_rnnt 6.871782 hw_loss 0.400920 history loss 8.123146 rank 5
2023-02-23 05:40:43,503 DEBUG CV Batch 14/1300 loss 7.257873 loss_att 5.758110 loss_ctc 8.638021 loss_rnnt 7.056537 hw_loss 0.594880 history loss 8.467931 rank 7
2023-02-23 05:40:45,411 DEBUG CV Batch 14/1300 loss 7.257873 loss_att 5.758110 loss_ctc 8.638021 loss_rnnt 7.056537 hw_loss 0.594880 history loss 8.467931 rank 2
2023-02-23 05:40:46,234 DEBUG CV Batch 14/1300 loss 7.257873 loss_att 5.758110 loss_ctc 8.638021 loss_rnnt 7.056537 hw_loss 0.594880 history loss 8.467931 rank 1
2023-02-23 05:40:47,140 DEBUG CV Batch 14/1300 loss 7.257873 loss_att 5.758110 loss_ctc 8.638021 loss_rnnt 7.056537 hw_loss 0.594880 history loss 8.467931 rank 4
2023-02-23 05:40:47,785 DEBUG CV Batch 14/1300 loss 7.257873 loss_att 5.758110 loss_ctc 8.638021 loss_rnnt 7.056537 hw_loss 0.594880 history loss 8.467931 rank 6
2023-02-23 05:40:48,641 DEBUG CV Batch 14/1300 loss 7.257873 loss_att 5.758110 loss_ctc 8.638021 loss_rnnt 7.056537 hw_loss 0.594880 history loss 8.467931 rank 3
2023-02-23 05:40:48,801 DEBUG CV Batch 14/1300 loss 7.257873 loss_att 5.758110 loss_ctc 8.638021 loss_rnnt 7.056537 hw_loss 0.594880 history loss 8.467931 rank 0
2023-02-23 05:40:49,742 DEBUG CV Batch 14/1300 loss 7.257873 loss_att 5.758110 loss_ctc 8.638021 loss_rnnt 7.056537 hw_loss 0.594880 history loss 8.467931 rank 5
2023-02-23 05:40:55,045 DEBUG CV Batch 14/1400 loss 11.836071 loss_att 37.635906 loss_ctc 8.486141 loss_rnnt 6.916848 hw_loss 0.386086 history loss 8.865709 rank 7
2023-02-23 05:40:56,980 DEBUG CV Batch 14/1400 loss 11.836071 loss_att 37.635906 loss_ctc 8.486141 loss_rnnt 6.916848 hw_loss 0.386086 history loss 8.865709 rank 2
2023-02-23 05:40:57,606 DEBUG CV Batch 14/1400 loss 11.836071 loss_att 37.635906 loss_ctc 8.486141 loss_rnnt 6.916848 hw_loss 0.386086 history loss 8.865709 rank 1
2023-02-23 05:40:59,065 DEBUG CV Batch 14/1400 loss 11.836071 loss_att 37.635906 loss_ctc 8.486141 loss_rnnt 6.916848 hw_loss 0.386086 history loss 8.865709 rank 4
2023-02-23 05:40:59,383 DEBUG CV Batch 14/1400 loss 11.836071 loss_att 37.635906 loss_ctc 8.486141 loss_rnnt 6.916848 hw_loss 0.386086 history loss 8.865709 rank 6
2023-02-23 05:41:00,078 DEBUG CV Batch 14/1400 loss 11.836071 loss_att 37.635906 loss_ctc 8.486141 loss_rnnt 6.916848 hw_loss 0.386086 history loss 8.865709 rank 3
2023-02-23 05:41:00,537 DEBUG CV Batch 14/1400 loss 11.836071 loss_att 37.635906 loss_ctc 8.486141 loss_rnnt 6.916848 hw_loss 0.386086 history loss 8.865709 rank 0
2023-02-23 05:41:01,437 DEBUG CV Batch 14/1400 loss 11.836071 loss_att 37.635906 loss_ctc 8.486141 loss_rnnt 6.916848 hw_loss 0.386086 history loss 8.865709 rank 5
2023-02-23 05:41:06,601 DEBUG CV Batch 14/1500 loss 7.977739 loss_att 9.748778 loss_ctc 9.602997 loss_rnnt 7.175991 hw_loss 0.432825 history loss 8.658273 rank 7
2023-02-23 05:41:08,884 DEBUG CV Batch 14/1500 loss 7.977739 loss_att 9.748778 loss_ctc 9.602997 loss_rnnt 7.175991 hw_loss 0.432825 history loss 8.658273 rank 2
2023-02-23 05:41:09,375 DEBUG CV Batch 14/1500 loss 7.977739 loss_att 9.748778 loss_ctc 9.602997 loss_rnnt 7.175991 hw_loss 0.432825 history loss 8.658273 rank 1
2023-02-23 05:41:11,996 DEBUG CV Batch 14/1500 loss 7.977739 loss_att 9.748778 loss_ctc 9.602997 loss_rnnt 7.175991 hw_loss 0.432825 history loss 8.658273 rank 4
2023-02-23 05:41:12,126 DEBUG CV Batch 14/1500 loss 7.977739 loss_att 9.748778 loss_ctc 9.602997 loss_rnnt 7.175991 hw_loss 0.432825 history loss 8.658273 rank 6
2023-02-23 05:41:12,524 DEBUG CV Batch 14/1500 loss 7.977739 loss_att 9.748778 loss_ctc 9.602997 loss_rnnt 7.175991 hw_loss 0.432825 history loss 8.658273 rank 0
2023-02-23 05:41:12,656 DEBUG CV Batch 14/1500 loss 7.977739 loss_att 9.748778 loss_ctc 9.602997 loss_rnnt 7.175991 hw_loss 0.432825 history loss 8.658273 rank 3
2023-02-23 05:41:13,859 DEBUG CV Batch 14/1500 loss 7.977739 loss_att 9.748778 loss_ctc 9.602997 loss_rnnt 7.175991 hw_loss 0.432825 history loss 8.658273 rank 5
2023-02-23 05:41:19,949 DEBUG CV Batch 14/1600 loss 9.049493 loss_att 14.346010 loss_ctc 11.173755 loss_rnnt 7.499470 hw_loss 0.389035 history loss 8.563749 rank 7
2023-02-23 05:41:22,264 DEBUG CV Batch 14/1600 loss 9.049493 loss_att 14.346010 loss_ctc 11.173755 loss_rnnt 7.499470 hw_loss 0.389035 history loss 8.563749 rank 2
2023-02-23 05:41:22,861 DEBUG CV Batch 14/1600 loss 9.049493 loss_att 14.346010 loss_ctc 11.173755 loss_rnnt 7.499470 hw_loss 0.389035 history loss 8.563749 rank 1
2023-02-23 05:41:25,705 DEBUG CV Batch 14/1600 loss 9.049493 loss_att 14.346010 loss_ctc 11.173755 loss_rnnt 7.499470 hw_loss 0.389035 history loss 8.563749 rank 4
2023-02-23 05:41:25,876 DEBUG CV Batch 14/1600 loss 9.049493 loss_att 14.346010 loss_ctc 11.173755 loss_rnnt 7.499470 hw_loss 0.389035 history loss 8.563749 rank 6
2023-02-23 05:41:26,103 DEBUG CV Batch 14/1600 loss 9.049493 loss_att 14.346010 loss_ctc 11.173755 loss_rnnt 7.499470 hw_loss 0.389035 history loss 8.563749 rank 0
2023-02-23 05:41:26,464 DEBUG CV Batch 14/1600 loss 9.049493 loss_att 14.346010 loss_ctc 11.173755 loss_rnnt 7.499470 hw_loss 0.389035 history loss 8.563749 rank 3
2023-02-23 05:41:27,686 DEBUG CV Batch 14/1600 loss 9.049493 loss_att 14.346010 loss_ctc 11.173755 loss_rnnt 7.499470 hw_loss 0.389035 history loss 8.563749 rank 5
2023-02-23 05:41:32,549 DEBUG CV Batch 14/1700 loss 12.494533 loss_att 10.935736 loss_ctc 16.860676 loss_rnnt 11.963772 hw_loss 0.488189 history loss 8.448966 rank 7
2023-02-23 05:41:34,987 DEBUG CV Batch 14/1700 loss 12.494533 loss_att 10.935736 loss_ctc 16.860676 loss_rnnt 11.963772 hw_loss 0.488189 history loss 8.448966 rank 2
2023-02-23 05:41:35,716 DEBUG CV Batch 14/1700 loss 12.494533 loss_att 10.935736 loss_ctc 16.860676 loss_rnnt 11.963772 hw_loss 0.488189 history loss 8.448966 rank 1
2023-02-23 05:41:38,064 DEBUG CV Batch 14/1700 loss 12.494534 loss_att 10.935736 loss_ctc 16.860676 loss_rnnt 11.963772 hw_loss 0.488189 history loss 8.448966 rank 4
2023-02-23 05:41:38,757 DEBUG CV Batch 14/1700 loss 12.494534 loss_att 10.935736 loss_ctc 16.860676 loss_rnnt 11.963772 hw_loss 0.488189 history loss 8.448966 rank 3
2023-02-23 05:41:39,084 DEBUG CV Batch 14/1700 loss 12.494533 loss_att 10.935736 loss_ctc 16.860676 loss_rnnt 11.963772 hw_loss 0.488189 history loss 8.448966 rank 0
2023-02-23 05:41:39,105 DEBUG CV Batch 14/1700 loss 12.494533 loss_att 10.935736 loss_ctc 16.860676 loss_rnnt 11.963772 hw_loss 0.488189 history loss 8.448966 rank 6
2023-02-23 05:41:40,479 DEBUG CV Batch 14/1700 loss 12.494533 loss_att 10.935736 loss_ctc 16.860676 loss_rnnt 11.963772 hw_loss 0.488189 history loss 8.448966 rank 5
2023-02-23 05:41:41,792 INFO Epoch 14 CV info cv_loss 8.392527459570225
2023-02-23 05:41:41,792 INFO Epoch 15 TRAIN info lr 0.0004469401517752627
2023-02-23 05:41:41,794 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 05:41:44,252 INFO Epoch 14 CV info cv_loss 8.392527460578137
2023-02-23 05:41:44,253 INFO Epoch 15 TRAIN info lr 0.0004469901562676742
2023-02-23 05:41:44,258 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 05:41:45,132 INFO Epoch 14 CV info cv_loss 8.392527458364176
2023-02-23 05:41:45,133 INFO Epoch 15 TRAIN info lr 0.0004470562593829398
2023-02-23 05:41:45,138 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 05:41:47,354 INFO Epoch 14 CV info cv_loss 8.392527459871737
2023-02-23 05:41:47,354 INFO Epoch 15 TRAIN info lr 0.0004470151648086213
2023-02-23 05:41:47,359 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 05:41:48,112 INFO Epoch 14 CV info cv_loss 8.392527462344136
2023-02-23 05:41:48,112 INFO Epoch 15 TRAIN info lr 0.00044699372865940134
2023-02-23 05:41:48,114 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 05:41:48,492 INFO Epoch 14 CV info cv_loss 8.392527460466146
2023-02-23 05:41:48,492 INFO Epoch 15 TRAIN info lr 0.0004470294572882159
2023-02-23 05:41:48,494 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 05:41:48,648 INFO Epoch 14 CV info cv_loss 8.392527458433092
2023-02-23 05:41:48,649 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/14.pt
2023-02-23 05:41:49,701 INFO Epoch 14 CV info cv_loss 8.392527458338332
2023-02-23 05:41:49,701 INFO Epoch 15 TRAIN info lr 0.0004469383662108998
2023-02-23 05:41:49,703 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 05:41:52,840 INFO Epoch 15 TRAIN info lr 0.0004470133783450605
2023-02-23 05:41:52,846 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 05:42:54,469 DEBUG TRAIN Batch 15/0 loss 8.495885 loss_att 7.726580 loss_ctc 9.281501 loss_rnnt 8.175568 hw_loss 0.692681 lr 0.00044699 rank 2
2023-02-23 05:42:54,469 DEBUG TRAIN Batch 15/0 loss 10.635447 loss_att 9.801767 loss_ctc 12.604193 loss_rnnt 10.207908 hw_loss 0.622080 lr 0.00044701 rank 4
2023-02-23 05:42:54,470 DEBUG TRAIN Batch 15/0 loss 9.933950 loss_att 9.707657 loss_ctc 11.840213 loss_rnnt 9.299066 hw_loss 0.798701 lr 0.00044699 rank 3
2023-02-23 05:42:54,470 DEBUG TRAIN Batch 15/0 loss 11.397029 loss_att 10.691942 loss_ctc 13.093376 loss_rnnt 10.931589 hw_loss 0.713018 lr 0.00044694 rank 7
2023-02-23 05:42:54,471 DEBUG TRAIN Batch 15/0 loss 15.942964 loss_att 14.885208 loss_ctc 18.639099 loss_rnnt 15.400850 hw_loss 0.739086 lr 0.00044703 rank 6
2023-02-23 05:42:54,480 DEBUG TRAIN Batch 15/0 loss 11.015598 loss_att 9.949060 loss_ctc 13.452994 loss_rnnt 10.457067 hw_loss 0.837847 lr 0.00044705 rank 1
2023-02-23 05:42:54,487 DEBUG TRAIN Batch 15/0 loss 11.250623 loss_att 10.591595 loss_ctc 12.020468 loss_rnnt 10.932197 hw_loss 0.651723 lr 0.00044701 rank 0
2023-02-23 05:42:54,530 DEBUG TRAIN Batch 15/0 loss 11.222751 loss_att 11.534417 loss_ctc 13.279212 loss_rnnt 10.446403 hw_loss 0.824663 lr 0.00044694 rank 5
2023-02-23 05:44:06,977 DEBUG TRAIN Batch 15/100 loss 16.461992 loss_att 15.359770 loss_ctc 21.798412 loss_rnnt 15.789221 hw_loss 0.340675 lr 0.00044688 rank 1
2023-02-23 05:44:06,977 DEBUG TRAIN Batch 15/100 loss 22.728321 loss_att 25.283478 loss_ctc 29.335098 loss_rnnt 21.184212 hw_loss 0.285327 lr 0.00044676 rank 7
2023-02-23 05:44:06,978 DEBUG TRAIN Batch 15/100 loss 20.123720 loss_att 20.759399 loss_ctc 20.412355 loss_rnnt 19.728065 hw_loss 0.431314 lr 0.00044681 rank 3
2023-02-23 05:44:06,978 DEBUG TRAIN Batch 15/100 loss 9.347616 loss_att 11.338087 loss_ctc 14.545673 loss_rnnt 7.996833 hw_loss 0.486777 lr 0.00044683 rank 4
2023-02-23 05:44:06,980 DEBUG TRAIN Batch 15/100 loss 15.588554 loss_att 16.966562 loss_ctc 22.270102 loss_rnnt 14.229522 hw_loss 0.361047 lr 0.00044681 rank 2
2023-02-23 05:44:06,985 DEBUG TRAIN Batch 15/100 loss 6.711576 loss_att 11.044828 loss_ctc 7.078836 loss_rnnt 5.587884 hw_loss 0.390136 lr 0.00044685 rank 6
2023-02-23 05:44:06,986 DEBUG TRAIN Batch 15/100 loss 12.923179 loss_att 15.120965 loss_ctc 15.682289 loss_rnnt 11.926231 hw_loss 0.355329 lr 0.00044683 rank 0
2023-02-23 05:44:07,033 DEBUG TRAIN Batch 15/100 loss 14.394364 loss_att 20.678223 loss_ctc 17.982367 loss_rnnt 12.459824 hw_loss 0.373817 lr 0.00044676 rank 5
2023-02-23 05:45:18,722 DEBUG TRAIN Batch 15/200 loss 13.701129 loss_att 13.742084 loss_ctc 20.863302 loss_rnnt 12.542847 hw_loss 0.365877 lr 0.00044658 rank 7
2023-02-23 05:45:18,728 DEBUG TRAIN Batch 15/200 loss 12.180237 loss_att 13.044192 loss_ctc 19.595207 loss_rnnt 10.801478 hw_loss 0.407448 lr 0.00044670 rank 1
2023-02-23 05:45:18,730 DEBUG TRAIN Batch 15/200 loss 19.060705 loss_att 22.075895 loss_ctc 27.638699 loss_rnnt 17.077909 hw_loss 0.442550 lr 0.00044665 rank 0
2023-02-23 05:45:18,730 DEBUG TRAIN Batch 15/200 loss 13.041245 loss_att 15.235327 loss_ctc 14.192335 loss_rnnt 12.224990 hw_loss 0.419927 lr 0.00044663 rank 2
2023-02-23 05:45:18,731 DEBUG TRAIN Batch 15/200 loss 14.389286 loss_att 14.960335 loss_ctc 22.044659 loss_rnnt 13.064967 hw_loss 0.355112 lr 0.00044664 rank 3
2023-02-23 05:45:18,731 DEBUG TRAIN Batch 15/200 loss 4.132377 loss_att 7.665330 loss_ctc 6.306170 loss_rnnt 2.905248 hw_loss 0.432560 lr 0.00044667 rank 6
2023-02-23 05:45:18,732 DEBUG TRAIN Batch 15/200 loss 9.156078 loss_att 10.931730 loss_ctc 15.015594 loss_rnnt 7.862801 hw_loss 0.294146 lr 0.00044666 rank 4
2023-02-23 05:45:18,737 DEBUG TRAIN Batch 15/200 loss 11.484596 loss_att 13.004010 loss_ctc 16.860134 loss_rnnt 10.261339 hw_loss 0.379941 lr 0.00044658 rank 5
2023-02-23 05:46:32,121 DEBUG TRAIN Batch 15/300 loss 8.760375 loss_att 9.696561 loss_ctc 12.765946 loss_rnnt 7.849882 hw_loss 0.354712 lr 0.00044652 rank 1
2023-02-23 05:46:32,122 DEBUG TRAIN Batch 15/300 loss 9.208631 loss_att 13.903925 loss_ctc 12.885546 loss_rnnt 7.533195 hw_loss 0.461477 lr 0.00044648 rank 4
2023-02-23 05:46:32,123 DEBUG TRAIN Batch 15/300 loss 15.659916 loss_att 19.324270 loss_ctc 19.728634 loss_rnnt 14.130085 hw_loss 0.477122 lr 0.00044640 rank 5
2023-02-23 05:46:32,124 DEBUG TRAIN Batch 15/300 loss 4.702731 loss_att 9.441467 loss_ctc 6.411505 loss_rnnt 3.260818 hw_loss 0.499368 lr 0.00044645 rank 2
2023-02-23 05:46:32,126 DEBUG TRAIN Batch 15/300 loss 7.761521 loss_att 8.566267 loss_ctc 10.377429 loss_rnnt 7.008164 hw_loss 0.456786 lr 0.00044648 rank 0
2023-02-23 05:46:32,126 DEBUG TRAIN Batch 15/300 loss 6.896906 loss_att 11.465270 loss_ctc 9.613847 loss_rnnt 5.439887 hw_loss 0.339540 lr 0.00044640 rank 7
2023-02-23 05:46:32,126 DEBUG TRAIN Batch 15/300 loss 9.383932 loss_att 11.832267 loss_ctc 11.589364 loss_rnnt 8.379525 hw_loss 0.413777 lr 0.00044646 rank 3
2023-02-23 05:46:32,154 DEBUG TRAIN Batch 15/300 loss 19.285713 loss_att 21.900640 loss_ctc 28.184860 loss_rnnt 17.384764 hw_loss 0.358896 lr 0.00044649 rank 6
2023-02-23 05:47:45,290 DEBUG TRAIN Batch 15/400 loss 9.290778 loss_att 12.974527 loss_ctc 11.366375 loss_rnnt 8.043221 hw_loss 0.438864 lr 0.00044630 rank 0
2023-02-23 05:47:45,305 DEBUG TRAIN Batch 15/400 loss 6.161390 loss_att 8.906785 loss_ctc 7.673476 loss_rnnt 5.183306 hw_loss 0.426363 lr 0.00044623 rank 7
2023-02-23 05:47:45,305 DEBUG TRAIN Batch 15/400 loss 6.997518 loss_att 13.434616 loss_ctc 13.036079 loss_rnnt 4.611875 hw_loss 0.549529 lr 0.00044631 rank 6
2023-02-23 05:47:45,306 DEBUG TRAIN Batch 15/400 loss 10.967079 loss_att 12.550941 loss_ctc 11.457431 loss_rnnt 10.395100 hw_loss 0.355926 lr 0.00044628 rank 3
2023-02-23 05:47:45,307 DEBUG TRAIN Batch 15/400 loss 8.668854 loss_att 10.542525 loss_ctc 12.194502 loss_rnnt 7.565836 hw_loss 0.484117 lr 0.00044630 rank 4
2023-02-23 05:47:45,308 DEBUG TRAIN Batch 15/400 loss 9.287987 loss_att 14.166406 loss_ctc 11.185774 loss_rnnt 7.846083 hw_loss 0.399717 lr 0.00044628 rank 2
2023-02-23 05:47:45,315 DEBUG TRAIN Batch 15/400 loss 12.882133 loss_att 18.809471 loss_ctc 18.176014 loss_rnnt 10.822101 hw_loss 0.316338 lr 0.00044622 rank 5
2023-02-23 05:47:45,316 DEBUG TRAIN Batch 15/400 loss 12.974494 loss_att 16.712713 loss_ctc 17.550095 loss_rnnt 11.418668 hw_loss 0.371440 lr 0.00044634 rank 1
2023-02-23 05:48:57,283 DEBUG TRAIN Batch 15/500 loss 10.100433 loss_att 10.786531 loss_ctc 12.388785 loss_rnnt 9.401126 hw_loss 0.481827 lr 0.00044605 rank 7
2023-02-23 05:48:57,287 DEBUG TRAIN Batch 15/500 loss 6.550337 loss_att 8.392576 loss_ctc 8.957453 loss_rnnt 5.579517 hw_loss 0.527669 lr 0.00044616 rank 1
2023-02-23 05:48:57,290 DEBUG TRAIN Batch 15/500 loss 14.150596 loss_att 18.538078 loss_ctc 17.580574 loss_rnnt 12.615993 hw_loss 0.374577 lr 0.00044610 rank 3
2023-02-23 05:48:57,292 DEBUG TRAIN Batch 15/500 loss 11.366963 loss_att 10.716997 loss_ctc 12.126184 loss_rnnt 11.186663 hw_loss 0.391993 lr 0.00044612 rank 4
2023-02-23 05:48:57,292 DEBUG TRAIN Batch 15/500 loss 9.789192 loss_att 11.585390 loss_ctc 13.320911 loss_rnnt 8.761049 hw_loss 0.371262 lr 0.00044610 rank 2
2023-02-23 05:48:57,292 DEBUG TRAIN Batch 15/500 loss 8.476459 loss_att 13.173197 loss_ctc 12.797295 loss_rnnt 6.743056 hw_loss 0.408644 lr 0.00044605 rank 5
2023-02-23 05:48:57,298 DEBUG TRAIN Batch 15/500 loss 8.383492 loss_att 9.590374 loss_ctc 12.091411 loss_rnnt 7.425904 hw_loss 0.415914 lr 0.00044612 rank 0
2023-02-23 05:48:57,346 DEBUG TRAIN Batch 15/500 loss 10.737819 loss_att 12.813256 loss_ctc 17.814602 loss_rnnt 9.152885 hw_loss 0.424266 lr 0.00044614 rank 6
2023-02-23 05:50:09,112 DEBUG TRAIN Batch 15/600 loss 19.641588 loss_att 21.021700 loss_ctc 21.527184 loss_rnnt 18.753647 hw_loss 0.675952 lr 0.00044592 rank 3
2023-02-23 05:50:09,114 DEBUG TRAIN Batch 15/600 loss 8.065437 loss_att 7.909796 loss_ctc 10.281087 loss_rnnt 7.479491 hw_loss 0.603104 lr 0.00044592 rank 2
2023-02-23 05:50:09,115 DEBUG TRAIN Batch 15/600 loss 6.962825 loss_att 6.936713 loss_ctc 7.774090 loss_rnnt 6.591249 hw_loss 0.503681 lr 0.00044599 rank 1
2023-02-23 05:50:09,115 DEBUG TRAIN Batch 15/600 loss 8.938069 loss_att 9.552363 loss_ctc 12.398782 loss_rnnt 8.071019 hw_loss 0.530181 lr 0.00044596 rank 6
2023-02-23 05:50:09,115 DEBUG TRAIN Batch 15/600 loss 12.911558 loss_att 13.260929 loss_ctc 15.931003 loss_rnnt 12.104057 hw_loss 0.628188 lr 0.00044587 rank 5
2023-02-23 05:50:09,115 DEBUG TRAIN Batch 15/600 loss 15.039632 loss_att 14.180015 loss_ctc 19.505716 loss_rnnt 14.281192 hw_loss 0.627911 lr 0.00044595 rank 4
2023-02-23 05:50:09,117 DEBUG TRAIN Batch 15/600 loss 10.484013 loss_att 10.930557 loss_ctc 13.331221 loss_rnnt 9.676613 hw_loss 0.634618 lr 0.00044587 rank 7
2023-02-23 05:50:09,121 DEBUG TRAIN Batch 15/600 loss 7.874050 loss_att 8.381246 loss_ctc 11.943076 loss_rnnt 6.908414 hw_loss 0.603112 lr 0.00044594 rank 0
2023-02-23 05:51:22,766 DEBUG TRAIN Batch 15/700 loss 10.369136 loss_att 15.313034 loss_ctc 14.829216 loss_rnnt 8.596273 hw_loss 0.355137 lr 0.00044569 rank 7
2023-02-23 05:51:22,766 DEBUG TRAIN Batch 15/700 loss 10.648951 loss_att 16.731876 loss_ctc 12.788702 loss_rnnt 8.950396 hw_loss 0.368756 lr 0.00044577 rank 4
2023-02-23 05:51:22,778 DEBUG TRAIN Batch 15/700 loss 11.734995 loss_att 17.764923 loss_ctc 15.886354 loss_rnnt 9.733114 hw_loss 0.454462 lr 0.00044569 rank 5
2023-02-23 05:51:22,779 DEBUG TRAIN Batch 15/700 loss 12.113640 loss_att 15.205414 loss_ctc 17.499733 loss_rnnt 10.597835 hw_loss 0.336197 lr 0.00044574 rank 2
2023-02-23 05:51:22,779 DEBUG TRAIN Batch 15/700 loss 17.001080 loss_att 21.329128 loss_ctc 19.640060 loss_rnnt 15.605297 hw_loss 0.334332 lr 0.00044577 rank 0
2023-02-23 05:51:22,781 DEBUG TRAIN Batch 15/700 loss 11.837917 loss_att 16.679688 loss_ctc 27.184601 loss_rnnt 8.608812 hw_loss 0.402235 lr 0.00044578 rank 6
2023-02-23 05:51:22,782 DEBUG TRAIN Batch 15/700 loss 5.978336 loss_att 9.137296 loss_ctc 9.926757 loss_rnnt 4.566320 hw_loss 0.475813 lr 0.00044581 rank 1
2023-02-23 05:51:22,812 DEBUG TRAIN Batch 15/700 loss 13.694735 loss_att 18.533455 loss_ctc 18.716751 loss_rnnt 11.817297 hw_loss 0.450172 lr 0.00044575 rank 3
2023-02-23 05:52:36,233 DEBUG TRAIN Batch 15/800 loss 6.445493 loss_att 11.109337 loss_ctc 9.291316 loss_rnnt 4.949896 hw_loss 0.343848 lr 0.00044559 rank 0
2023-02-23 05:52:36,255 DEBUG TRAIN Batch 15/800 loss 8.104730 loss_att 12.028615 loss_ctc 8.434577 loss_rnnt 7.083234 hw_loss 0.361386 lr 0.00044557 rank 2
2023-02-23 05:52:36,254 DEBUG TRAIN Batch 15/800 loss 10.640316 loss_att 15.126266 loss_ctc 13.387405 loss_rnnt 9.153391 hw_loss 0.418981 lr 0.00044551 rank 5
2023-02-23 05:52:36,255 DEBUG TRAIN Batch 15/800 loss 12.277192 loss_att 15.969822 loss_ctc 17.896635 loss_rnnt 10.543555 hw_loss 0.460973 lr 0.00044552 rank 7
2023-02-23 05:52:36,255 DEBUG TRAIN Batch 15/800 loss 10.163630 loss_att 13.409015 loss_ctc 13.691108 loss_rnnt 8.853710 hw_loss 0.357211 lr 0.00044559 rank 4
2023-02-23 05:52:36,257 DEBUG TRAIN Batch 15/800 loss 8.154087 loss_att 9.850178 loss_ctc 11.159456 loss_rnnt 7.225029 hw_loss 0.354607 lr 0.00044563 rank 1
2023-02-23 05:52:36,258 DEBUG TRAIN Batch 15/800 loss 14.751345 loss_att 16.441185 loss_ctc 17.329048 loss_rnnt 13.878382 hw_loss 0.358690 lr 0.00044557 rank 3
2023-02-23 05:52:36,263 DEBUG TRAIN Batch 15/800 loss 7.376695 loss_att 14.443996 loss_ctc 8.466839 loss_rnnt 5.613584 hw_loss 0.383058 lr 0.00044561 rank 6
2023-02-23 05:53:48,228 DEBUG TRAIN Batch 15/900 loss 12.426647 loss_att 15.443722 loss_ctc 13.984324 loss_rnnt 11.402096 hw_loss 0.400209 lr 0.00044541 rank 4
2023-02-23 05:53:48,239 DEBUG TRAIN Batch 15/900 loss 14.728743 loss_att 19.494406 loss_ctc 19.844213 loss_rnnt 12.925264 hw_loss 0.315527 lr 0.00044534 rank 7
2023-02-23 05:53:48,241 DEBUG TRAIN Batch 15/900 loss 7.012415 loss_att 10.800250 loss_ctc 8.237217 loss_rnnt 5.867546 hw_loss 0.419990 lr 0.00044539 rank 2
2023-02-23 05:53:48,242 DEBUG TRAIN Batch 15/900 loss 20.748184 loss_att 25.410137 loss_ctc 27.731205 loss_rnnt 18.676815 hw_loss 0.389829 lr 0.00044541 rank 0
2023-02-23 05:53:48,243 DEBUG TRAIN Batch 15/900 loss 7.904897 loss_att 10.600677 loss_ctc 9.707321 loss_rnnt 6.891747 hw_loss 0.438134 lr 0.00044539 rank 3
2023-02-23 05:53:48,244 DEBUG TRAIN Batch 15/900 loss 8.763269 loss_att 10.335278 loss_ctc 8.449306 loss_rnnt 8.292997 hw_loss 0.370747 lr 0.00044534 rank 5
2023-02-23 05:53:48,257 DEBUG TRAIN Batch 15/900 loss 9.269706 loss_att 11.866668 loss_ctc 9.442606 loss_rnnt 8.501001 hw_loss 0.424235 lr 0.00044543 rank 6
2023-02-23 05:53:48,266 DEBUG TRAIN Batch 15/900 loss 18.600613 loss_att 27.181622 loss_ctc 28.000607 loss_rnnt 15.373014 hw_loss 0.483872 lr 0.00044545 rank 1
2023-02-23 05:55:01,509 DEBUG TRAIN Batch 15/1000 loss 15.571938 loss_att 16.893486 loss_ctc 21.233431 loss_rnnt 14.358789 hw_loss 0.363696 lr 0.00044516 rank 7
2023-02-23 05:55:01,511 DEBUG TRAIN Batch 15/1000 loss 18.067875 loss_att 17.024090 loss_ctc 19.048357 loss_rnnt 17.941860 hw_loss 0.382574 lr 0.00044528 rank 1
2023-02-23 05:55:01,512 DEBUG TRAIN Batch 15/1000 loss 17.351397 loss_att 20.723160 loss_ctc 19.339357 loss_rnnt 16.156509 hw_loss 0.479014 lr 0.00044522 rank 3
2023-02-23 05:55:01,513 DEBUG TRAIN Batch 15/1000 loss 21.609114 loss_att 23.814037 loss_ctc 31.792604 loss_rnnt 19.581615 hw_loss 0.428841 lr 0.00044516 rank 5
2023-02-23 05:55:01,512 DEBUG TRAIN Batch 15/1000 loss 21.090023 loss_att 25.166336 loss_ctc 28.767826 loss_rnnt 19.020029 hw_loss 0.433169 lr 0.00044524 rank 4
2023-02-23 05:55:01,513 DEBUG TRAIN Batch 15/1000 loss 14.653683 loss_att 17.188034 loss_ctc 15.758863 loss_rnnt 13.824650 hw_loss 0.327758 lr 0.00044521 rank 2
2023-02-23 05:55:01,514 DEBUG TRAIN Batch 15/1000 loss 13.394507 loss_att 14.989598 loss_ctc 13.950180 loss_rnnt 12.740201 hw_loss 0.489748 lr 0.00044524 rank 0
2023-02-23 05:55:01,533 DEBUG TRAIN Batch 15/1000 loss 4.079126 loss_att 6.054175 loss_ctc 3.418854 loss_rnnt 3.545556 hw_loss 0.424869 lr 0.00044525 rank 6
2023-02-23 05:56:15,797 DEBUG TRAIN Batch 15/1100 loss 10.834246 loss_att 14.278713 loss_ctc 15.149027 loss_rnnt 9.334883 hw_loss 0.440936 lr 0.00044499 rank 5
2023-02-23 05:56:15,798 DEBUG TRAIN Batch 15/1100 loss 5.838632 loss_att 7.895574 loss_ctc 8.063928 loss_rnnt 4.849483 hw_loss 0.526977 lr 0.00044499 rank 7
2023-02-23 05:56:15,801 DEBUG TRAIN Batch 15/1100 loss 18.184084 loss_att 22.455128 loss_ctc 25.293156 loss_rnnt 16.159300 hw_loss 0.417562 lr 0.00044510 rank 1
2023-02-23 05:56:15,801 DEBUG TRAIN Batch 15/1100 loss 22.546173 loss_att 22.404493 loss_ctc 30.287546 loss_rnnt 21.343683 hw_loss 0.372454 lr 0.00044504 rank 2
2023-02-23 05:56:15,803 DEBUG TRAIN Batch 15/1100 loss 12.719691 loss_att 16.796124 loss_ctc 17.392723 loss_rnnt 11.082597 hw_loss 0.372631 lr 0.00044504 rank 3
2023-02-23 05:56:15,804 DEBUG TRAIN Batch 15/1100 loss 9.318714 loss_att 12.357759 loss_ctc 9.246750 loss_rnnt 8.502698 hw_loss 0.408379 lr 0.00044508 rank 6
2023-02-23 05:56:15,803 DEBUG TRAIN Batch 15/1100 loss 22.589779 loss_att 26.177450 loss_ctc 28.869678 loss_rnnt 20.798939 hw_loss 0.442475 lr 0.00044506 rank 0
2023-02-23 05:56:15,805 DEBUG TRAIN Batch 15/1100 loss 13.423003 loss_att 13.943873 loss_ctc 20.227890 loss_rnnt 12.146630 hw_loss 0.496649 lr 0.00044506 rank 4
2023-02-23 05:57:28,679 DEBUG TRAIN Batch 15/1200 loss 11.839772 loss_att 14.580417 loss_ctc 14.016536 loss_rnnt 10.721499 hw_loss 0.524832 lr 0.00044486 rank 2
2023-02-23 05:57:28,681 DEBUG TRAIN Batch 15/1200 loss 17.307819 loss_att 16.527149 loss_ctc 21.607714 loss_rnnt 16.646074 hw_loss 0.458544 lr 0.00044493 rank 1
2023-02-23 05:57:28,681 DEBUG TRAIN Batch 15/1200 loss 7.086864 loss_att 8.959228 loss_ctc 11.631113 loss_rnnt 5.823924 hw_loss 0.529813 lr 0.00044486 rank 3
2023-02-23 05:57:28,681 DEBUG TRAIN Batch 15/1200 loss 6.295470 loss_att 6.482807 loss_ctc 7.796912 loss_rnnt 5.814579 hw_loss 0.456058 lr 0.00044490 rank 6
2023-02-23 05:57:28,682 DEBUG TRAIN Batch 15/1200 loss 9.287330 loss_att 10.340008 loss_ctc 13.121089 loss_rnnt 8.234028 hw_loss 0.621747 lr 0.00044481 rank 7
2023-02-23 05:57:28,686 DEBUG TRAIN Batch 15/1200 loss 4.398104 loss_att 6.684537 loss_ctc 8.232694 loss_rnnt 3.179449 hw_loss 0.468917 lr 0.00044488 rank 4
2023-02-23 05:57:28,688 DEBUG TRAIN Batch 15/1200 loss 10.164788 loss_att 15.292768 loss_ctc 14.515381 loss_rnnt 8.341597 hw_loss 0.407843 lr 0.00044488 rank 0
2023-02-23 05:57:28,690 DEBUG TRAIN Batch 15/1200 loss 13.132613 loss_att 14.357004 loss_ctc 16.838211 loss_rnnt 12.094455 hw_loss 0.561002 lr 0.00044481 rank 5
2023-02-23 05:58:41,987 DEBUG TRAIN Batch 15/1300 loss 5.598012 loss_att 8.408974 loss_ctc 7.015288 loss_rnnt 4.637413 hw_loss 0.392695 lr 0.00044463 rank 5
2023-02-23 05:58:41,992 DEBUG TRAIN Batch 15/1300 loss 11.162498 loss_att 15.803192 loss_ctc 15.609962 loss_rnnt 9.432851 hw_loss 0.390962 lr 0.00044464 rank 7
2023-02-23 05:58:41,995 DEBUG TRAIN Batch 15/1300 loss 8.063897 loss_att 15.080042 loss_ctc 9.815230 loss_rnnt 6.227889 hw_loss 0.373628 lr 0.00044469 rank 3
2023-02-23 05:58:41,997 DEBUG TRAIN Batch 15/1300 loss 17.000736 loss_att 19.838390 loss_ctc 19.311920 loss_rnnt 15.907736 hw_loss 0.407459 lr 0.00044475 rank 1
2023-02-23 05:58:41,996 DEBUG TRAIN Batch 15/1300 loss 16.236336 loss_att 19.124138 loss_ctc 19.633240 loss_rnnt 15.039680 hw_loss 0.311576 lr 0.00044471 rank 0
2023-02-23 05:58:41,997 DEBUG TRAIN Batch 15/1300 loss 7.134801 loss_att 12.586452 loss_ctc 9.530283 loss_rnnt 5.570457 hw_loss 0.289905 lr 0.00044468 rank 2
2023-02-23 05:58:42,001 DEBUG TRAIN Batch 15/1300 loss 4.285935 loss_att 9.491755 loss_ctc 3.751180 loss_rnnt 3.097178 hw_loss 0.410425 lr 0.00044472 rank 6
2023-02-23 05:58:42,001 DEBUG TRAIN Batch 15/1300 loss 8.866043 loss_att 12.710010 loss_ctc 13.193306 loss_rnnt 7.294015 hw_loss 0.424249 lr 0.00044471 rank 4
2023-02-23 05:59:57,907 DEBUG TRAIN Batch 15/1400 loss 16.225752 loss_att 18.289709 loss_ctc 20.094995 loss_rnnt 15.116172 hw_loss 0.339168 lr 0.00044457 rank 1
2023-02-23 05:59:57,907 DEBUG TRAIN Batch 15/1400 loss 18.610615 loss_att 22.055908 loss_ctc 22.135054 loss_rnnt 17.262444 hw_loss 0.354726 lr 0.00044453 rank 4
2023-02-23 05:59:57,907 DEBUG TRAIN Batch 15/1400 loss 12.880056 loss_att 16.377113 loss_ctc 19.073193 loss_rnnt 11.158020 hw_loss 0.369138 lr 0.00044451 rank 3
2023-02-23 05:59:57,909 DEBUG TRAIN Batch 15/1400 loss 8.644643 loss_att 9.488095 loss_ctc 8.646575 loss_rnnt 8.301862 hw_loss 0.325937 lr 0.00044446 rank 7
2023-02-23 05:59:57,910 DEBUG TRAIN Batch 15/1400 loss 22.845726 loss_att 28.508297 loss_ctc 31.785141 loss_rnnt 20.290262 hw_loss 0.433173 lr 0.00044451 rank 2
2023-02-23 05:59:57,914 DEBUG TRAIN Batch 15/1400 loss 15.718778 loss_att 22.008944 loss_ctc 21.156055 loss_rnnt 13.561726 hw_loss 0.326340 lr 0.00044455 rank 6
2023-02-23 05:59:57,940 DEBUG TRAIN Batch 15/1400 loss 6.347203 loss_att 9.812676 loss_ctc 8.901611 loss_rnnt 5.024956 hw_loss 0.541058 lr 0.00044446 rank 5
2023-02-23 05:59:57,972 DEBUG TRAIN Batch 15/1400 loss 8.022559 loss_att 12.849734 loss_ctc 13.611534 loss_rnnt 6.085899 hw_loss 0.423804 lr 0.00044453 rank 0
2023-02-23 06:01:10,394 DEBUG TRAIN Batch 15/1500 loss 7.945580 loss_att 11.929123 loss_ctc 15.253314 loss_rnnt 5.935960 hw_loss 0.447275 lr 0.00044434 rank 3
2023-02-23 06:01:10,403 DEBUG TRAIN Batch 15/1500 loss 2.666191 loss_att 6.347540 loss_ctc 5.151844 loss_rnnt 1.372130 hw_loss 0.424445 lr 0.00044433 rank 2
2023-02-23 06:01:10,402 DEBUG TRAIN Batch 15/1500 loss 26.983444 loss_att 27.710726 loss_ctc 37.001564 loss_rnnt 25.314093 hw_loss 0.352772 lr 0.00044428 rank 7
2023-02-23 06:01:10,407 DEBUG TRAIN Batch 15/1500 loss 20.642868 loss_att 21.842596 loss_ctc 28.054296 loss_rnnt 19.176228 hw_loss 0.447193 lr 0.00044440 rank 1
2023-02-23 06:01:10,407 DEBUG TRAIN Batch 15/1500 loss 15.165089 loss_att 19.302313 loss_ctc 21.720072 loss_rnnt 13.201328 hw_loss 0.491842 lr 0.00044436 rank 0
2023-02-23 06:01:10,409 DEBUG TRAIN Batch 15/1500 loss 6.328558 loss_att 11.891609 loss_ctc 9.122475 loss_rnnt 4.614775 hw_loss 0.428719 lr 0.00044437 rank 6
2023-02-23 06:01:10,416 DEBUG TRAIN Batch 15/1500 loss 6.978371 loss_att 11.836756 loss_ctc 6.850246 loss_rnnt 5.802577 hw_loss 0.414752 lr 0.00044428 rank 5
2023-02-23 06:01:10,456 DEBUG TRAIN Batch 15/1500 loss 4.828427 loss_att 7.677342 loss_ctc 6.809991 loss_rnnt 3.739605 hw_loss 0.477807 lr 0.00044436 rank 4
2023-02-23 06:02:22,323 DEBUG TRAIN Batch 15/1600 loss 9.081314 loss_att 10.994383 loss_ctc 10.594292 loss_rnnt 8.257793 hw_loss 0.448456 lr 0.00044411 rank 7
2023-02-23 06:02:22,329 DEBUG TRAIN Batch 15/1600 loss 5.301164 loss_att 8.741537 loss_ctc 8.817805 loss_rnnt 3.931042 hw_loss 0.399677 lr 0.00044416 rank 2
2023-02-23 06:02:22,329 DEBUG TRAIN Batch 15/1600 loss 17.402117 loss_att 23.618860 loss_ctc 23.880440 loss_rnnt 15.052502 hw_loss 0.454671 lr 0.00044418 rank 0
2023-02-23 06:02:22,331 DEBUG TRAIN Batch 15/1600 loss 7.024156 loss_att 11.252575 loss_ctc 9.450146 loss_rnnt 5.640808 hw_loss 0.401622 lr 0.00044422 rank 1
2023-02-23 06:02:22,333 DEBUG TRAIN Batch 15/1600 loss 3.738204 loss_att 6.138737 loss_ctc 4.305380 loss_rnnt 2.960064 hw_loss 0.417017 lr 0.00044420 rank 6
2023-02-23 06:02:22,332 DEBUG TRAIN Batch 15/1600 loss 12.566046 loss_att 15.821323 loss_ctc 12.632679 loss_rnnt 11.689783 hw_loss 0.405606 lr 0.00044418 rank 4
2023-02-23 06:02:22,335 DEBUG TRAIN Batch 15/1600 loss 15.530034 loss_att 14.990131 loss_ctc 16.334324 loss_rnnt 15.333300 hw_loss 0.370266 lr 0.00044411 rank 5
2023-02-23 06:02:22,336 DEBUG TRAIN Batch 15/1600 loss 16.716293 loss_att 24.660366 loss_ctc 23.967838 loss_rnnt 13.915432 hw_loss 0.459697 lr 0.00044416 rank 3
2023-02-23 06:03:34,864 DEBUG TRAIN Batch 15/1700 loss 10.778076 loss_att 12.708599 loss_ctc 12.665640 loss_rnnt 9.909212 hw_loss 0.433283 lr 0.00044405 rank 1
2023-02-23 06:03:34,874 DEBUG TRAIN Batch 15/1700 loss 14.082426 loss_att 15.978388 loss_ctc 21.653139 loss_rnnt 12.491866 hw_loss 0.378638 lr 0.00044399 rank 3
2023-02-23 06:03:34,878 DEBUG TRAIN Batch 15/1700 loss 14.981864 loss_att 19.303932 loss_ctc 20.066185 loss_rnnt 13.237711 hw_loss 0.378429 lr 0.00044393 rank 7
2023-02-23 06:03:34,879 DEBUG TRAIN Batch 15/1700 loss 16.620125 loss_att 17.882732 loss_ctc 20.082844 loss_rnnt 15.702046 hw_loss 0.382237 lr 0.00044401 rank 0
2023-02-23 06:03:34,885 DEBUG TRAIN Batch 15/1700 loss 13.879598 loss_att 16.229660 loss_ctc 18.096018 loss_rnnt 12.588600 hw_loss 0.485241 lr 0.00044393 rank 5
2023-02-23 06:03:34,886 DEBUG TRAIN Batch 15/1700 loss 11.754517 loss_att 17.666260 loss_ctc 20.236725 loss_rnnt 9.254560 hw_loss 0.349960 lr 0.00044398 rank 2
2023-02-23 06:03:34,901 DEBUG TRAIN Batch 15/1700 loss 6.286826 loss_att 9.750305 loss_ctc 8.440410 loss_rnnt 5.119833 hw_loss 0.350912 lr 0.00044401 rank 4
2023-02-23 06:03:34,901 DEBUG TRAIN Batch 15/1700 loss 15.310775 loss_att 16.591131 loss_ctc 18.841366 loss_rnnt 14.360732 hw_loss 0.418550 lr 0.00044402 rank 6
2023-02-23 06:04:49,477 DEBUG TRAIN Batch 15/1800 loss 11.522886 loss_att 11.946834 loss_ctc 14.067713 loss_rnnt 10.850437 hw_loss 0.465655 lr 0.00044381 rank 2
2023-02-23 06:04:49,477 DEBUG TRAIN Batch 15/1800 loss 18.204435 loss_att 21.253363 loss_ctc 23.368010 loss_rnnt 16.680225 hw_loss 0.423654 lr 0.00044376 rank 7
2023-02-23 06:04:49,479 DEBUG TRAIN Batch 15/1800 loss 9.784545 loss_att 11.387269 loss_ctc 12.726519 loss_rnnt 8.812305 hw_loss 0.486434 lr 0.00044376 rank 5
2023-02-23 06:04:49,480 DEBUG TRAIN Batch 15/1800 loss 8.082635 loss_att 10.001979 loss_ctc 10.683415 loss_rnnt 7.112924 hw_loss 0.448258 lr 0.00044381 rank 3
2023-02-23 06:04:49,481 DEBUG TRAIN Batch 15/1800 loss 7.637866 loss_att 10.468142 loss_ctc 9.395912 loss_rnnt 6.617985 hw_loss 0.411412 lr 0.00044385 rank 6
2023-02-23 06:04:49,482 DEBUG TRAIN Batch 15/1800 loss 11.183606 loss_att 13.587322 loss_ctc 16.504431 loss_rnnt 9.720859 hw_loss 0.511054 lr 0.00044383 rank 0
2023-02-23 06:04:49,483 DEBUG TRAIN Batch 15/1800 loss 15.798470 loss_att 18.877333 loss_ctc 25.044010 loss_rnnt 13.701014 hw_loss 0.466770 lr 0.00044383 rank 4
2023-02-23 06:04:49,532 DEBUG TRAIN Batch 15/1800 loss 10.397996 loss_att 15.116116 loss_ctc 13.810593 loss_rnnt 8.805922 hw_loss 0.362695 lr 0.00044387 rank 1
2023-02-23 06:06:02,897 DEBUG TRAIN Batch 15/1900 loss 11.765696 loss_att 11.119976 loss_ctc 15.421509 loss_rnnt 11.060601 hw_loss 0.650245 lr 0.00044364 rank 3
2023-02-23 06:06:02,902 DEBUG TRAIN Batch 15/1900 loss 10.863017 loss_att 9.516833 loss_ctc 13.072215 loss_rnnt 10.448042 hw_loss 0.730598 lr 0.00044363 rank 2
2023-02-23 06:06:02,903 DEBUG TRAIN Batch 15/1900 loss 15.306102 loss_att 14.591543 loss_ctc 18.575361 loss_rnnt 14.721181 hw_loss 0.547369 lr 0.00044358 rank 7
2023-02-23 06:06:02,904 DEBUG TRAIN Batch 15/1900 loss 9.900768 loss_att 13.804507 loss_ctc 12.243235 loss_rnnt 8.564972 hw_loss 0.455100 lr 0.00044367 rank 6
2023-02-23 06:06:02,905 DEBUG TRAIN Batch 15/1900 loss 7.536269 loss_att 7.258839 loss_ctc 8.382976 loss_rnnt 7.126674 hw_loss 0.660350 lr 0.00044366 rank 4
2023-02-23 06:06:02,906 DEBUG TRAIN Batch 15/1900 loss 30.337355 loss_att 31.507660 loss_ctc 36.296513 loss_rnnt 29.114584 hw_loss 0.364039 lr 0.00044358 rank 5
2023-02-23 06:06:02,912 DEBUG TRAIN Batch 15/1900 loss 13.387922 loss_att 16.464390 loss_ctc 16.982327 loss_rnnt 12.142657 hw_loss 0.282595 lr 0.00044370 rank 1
2023-02-23 06:06:02,961 DEBUG TRAIN Batch 15/1900 loss 14.248943 loss_att 17.619970 loss_ctc 16.767328 loss_rnnt 13.012442 hw_loss 0.424710 lr 0.00044366 rank 0
2023-02-23 06:07:14,339 DEBUG TRAIN Batch 15/2000 loss 20.289185 loss_att 28.420111 loss_ctc 31.691673 loss_rnnt 16.986572 hw_loss 0.292679 lr 0.00044348 rank 4
2023-02-23 06:07:14,343 DEBUG TRAIN Batch 15/2000 loss 12.312928 loss_att 17.127804 loss_ctc 21.268026 loss_rnnt 9.983290 hw_loss 0.323719 lr 0.00044341 rank 7
2023-02-23 06:07:14,345 DEBUG TRAIN Batch 15/2000 loss 2.876515 loss_att 5.005483 loss_ctc 4.411371 loss_rnnt 2.042641 hw_loss 0.381439 lr 0.00044346 rank 2
2023-02-23 06:07:14,347 DEBUG TRAIN Batch 15/2000 loss 11.903913 loss_att 16.186850 loss_ctc 13.893725 loss_rnnt 10.517578 hw_loss 0.495824 lr 0.00044350 rank 6
2023-02-23 06:07:14,348 DEBUG TRAIN Batch 15/2000 loss 2.959156 loss_att 7.203968 loss_ctc 4.553810 loss_rnnt 1.687038 hw_loss 0.394752 lr 0.00044346 rank 3
2023-02-23 06:07:14,349 DEBUG TRAIN Batch 15/2000 loss 9.684752 loss_att 11.295755 loss_ctc 12.373228 loss_rnnt 8.827586 hw_loss 0.330941 lr 0.00044348 rank 0
2023-02-23 06:07:14,352 DEBUG TRAIN Batch 15/2000 loss 19.349825 loss_att 23.111074 loss_ctc 28.379925 loss_rnnt 17.192894 hw_loss 0.376255 lr 0.00044352 rank 1
2023-02-23 06:07:14,356 DEBUG TRAIN Batch 15/2000 loss 14.832385 loss_att 19.070560 loss_ctc 22.586422 loss_rnnt 12.706309 hw_loss 0.458566 lr 0.00044341 rank 5
2023-02-23 06:08:28,283 DEBUG TRAIN Batch 15/2100 loss 18.303457 loss_att 21.805031 loss_ctc 26.843109 loss_rnnt 16.267874 hw_loss 0.368715 lr 0.00044324 rank 7
2023-02-23 06:08:28,283 DEBUG TRAIN Batch 15/2100 loss 7.482034 loss_att 10.430624 loss_ctc 11.026590 loss_rnnt 6.193725 hw_loss 0.423719 lr 0.00044329 rank 3
2023-02-23 06:08:28,286 DEBUG TRAIN Batch 15/2100 loss 10.948920 loss_att 16.525892 loss_ctc 13.878788 loss_rnnt 9.192945 hw_loss 0.468620 lr 0.00044328 rank 2
2023-02-23 06:08:28,286 DEBUG TRAIN Batch 15/2100 loss 12.757372 loss_att 15.107170 loss_ctc 17.332211 loss_rnnt 11.481409 hw_loss 0.367547 lr 0.00044331 rank 4
2023-02-23 06:08:28,291 DEBUG TRAIN Batch 15/2100 loss 13.347483 loss_att 16.507034 loss_ctc 17.404133 loss_rnnt 11.913446 hw_loss 0.489822 lr 0.00044335 rank 1
2023-02-23 06:08:28,292 DEBUG TRAIN Batch 15/2100 loss 8.464408 loss_att 9.380597 loss_ctc 7.328585 loss_rnnt 8.218629 hw_loss 0.401221 lr 0.00044323 rank 5
2023-02-23 06:08:28,294 DEBUG TRAIN Batch 15/2100 loss 9.176750 loss_att 11.508799 loss_ctc 10.472967 loss_rnnt 8.353442 hw_loss 0.345131 lr 0.00044332 rank 6
2023-02-23 06:08:28,294 DEBUG TRAIN Batch 15/2100 loss 10.350155 loss_att 12.527710 loss_ctc 15.248795 loss_rnnt 9.056467 hw_loss 0.384421 lr 0.00044331 rank 0
2023-02-23 06:09:41,111 DEBUG TRAIN Batch 15/2200 loss 9.619554 loss_att 13.031361 loss_ctc 10.251051 loss_rnnt 8.672087 hw_loss 0.339197 lr 0.00044313 rank 4
2023-02-23 06:09:41,121 DEBUG TRAIN Batch 15/2200 loss 14.987844 loss_att 20.347914 loss_ctc 21.649948 loss_rnnt 12.838999 hw_loss 0.353530 lr 0.00044311 rank 3
2023-02-23 06:09:41,125 DEBUG TRAIN Batch 15/2200 loss 12.197849 loss_att 14.433619 loss_ctc 14.507974 loss_rnnt 11.147242 hw_loss 0.553945 lr 0.00044306 rank 7
2023-02-23 06:09:41,126 DEBUG TRAIN Batch 15/2200 loss 8.802836 loss_att 15.498913 loss_ctc 13.085222 loss_rnnt 6.658855 hw_loss 0.438339 lr 0.00044306 rank 5
2023-02-23 06:09:41,128 DEBUG TRAIN Batch 15/2200 loss 8.807425 loss_att 14.884529 loss_ctc 12.219980 loss_rnnt 6.889335 hw_loss 0.464367 lr 0.00044315 rank 6
2023-02-23 06:09:41,129 DEBUG TRAIN Batch 15/2200 loss 15.201988 loss_att 19.135622 loss_ctc 18.870216 loss_rnnt 13.676458 hw_loss 0.468199 lr 0.00044317 rank 1
2023-02-23 06:09:41,130 DEBUG TRAIN Batch 15/2200 loss 4.523098 loss_att 8.877111 loss_ctc 7.047488 loss_rnnt 3.111132 hw_loss 0.383583 lr 0.00044311 rank 2
2023-02-23 06:09:41,187 DEBUG TRAIN Batch 15/2200 loss 7.723413 loss_att 9.415571 loss_ctc 8.999594 loss_rnnt 6.974094 hw_loss 0.451367 lr 0.00044313 rank 0
2023-02-23 06:10:53,447 DEBUG TRAIN Batch 15/2300 loss 15.917638 loss_att 19.442493 loss_ctc 22.454082 loss_rnnt 14.130217 hw_loss 0.395483 lr 0.00044294 rank 3
2023-02-23 06:10:53,457 DEBUG TRAIN Batch 15/2300 loss 5.829065 loss_att 8.718328 loss_ctc 10.417400 loss_rnnt 4.458960 hw_loss 0.338391 lr 0.00044294 rank 2
2023-02-23 06:10:53,462 DEBUG TRAIN Batch 15/2300 loss 13.198608 loss_att 18.775726 loss_ctc 18.850475 loss_rnnt 11.137762 hw_loss 0.359703 lr 0.00044300 rank 1
2023-02-23 06:10:53,462 DEBUG TRAIN Batch 15/2300 loss 22.613159 loss_att 24.160297 loss_ctc 23.628643 loss_rnnt 21.969795 hw_loss 0.372264 lr 0.00044297 rank 6
2023-02-23 06:10:53,464 DEBUG TRAIN Batch 15/2300 loss 11.444016 loss_att 16.269169 loss_ctc 21.746662 loss_rnnt 8.838593 hw_loss 0.500073 lr 0.00044289 rank 7
2023-02-23 06:10:53,464 DEBUG TRAIN Batch 15/2300 loss 11.665163 loss_att 16.391603 loss_ctc 13.993162 loss_rnnt 10.226543 hw_loss 0.342997 lr 0.00044289 rank 5
2023-02-23 06:10:53,467 DEBUG TRAIN Batch 15/2300 loss 14.891938 loss_att 16.136696 loss_ctc 19.927362 loss_rnnt 13.773034 hw_loss 0.372305 lr 0.00044296 rank 4
2023-02-23 06:10:53,513 DEBUG TRAIN Batch 15/2300 loss 8.102134 loss_att 13.479868 loss_ctc 13.807121 loss_rnnt 5.992941 hw_loss 0.511839 lr 0.00044296 rank 0
2023-02-23 06:12:06,498 DEBUG TRAIN Batch 15/2400 loss 10.166291 loss_att 11.787317 loss_ctc 13.146814 loss_rnnt 9.224142 hw_loss 0.413514 lr 0.00044271 rank 7
2023-02-23 06:12:06,498 DEBUG TRAIN Batch 15/2400 loss 15.745738 loss_att 15.599296 loss_ctc 22.224308 loss_rnnt 14.597282 hw_loss 0.588627 lr 0.00044280 rank 6
2023-02-23 06:12:06,500 DEBUG TRAIN Batch 15/2400 loss 16.532408 loss_att 18.249588 loss_ctc 27.530390 loss_rnnt 14.524258 hw_loss 0.371841 lr 0.00044276 rank 2
2023-02-23 06:12:06,500 DEBUG TRAIN Batch 15/2400 loss 9.387898 loss_att 14.354048 loss_ctc 13.104801 loss_rnnt 7.633163 hw_loss 0.498597 lr 0.00044279 rank 4
2023-02-23 06:12:06,501 DEBUG TRAIN Batch 15/2400 loss 21.126644 loss_att 26.277966 loss_ctc 26.088860 loss_rnnt 19.170940 hw_loss 0.494647 lr 0.00044277 rank 3
2023-02-23 06:12:06,501 DEBUG TRAIN Batch 15/2400 loss 22.099644 loss_att 27.862318 loss_ctc 25.841944 loss_rnnt 20.216801 hw_loss 0.433749 lr 0.00044283 rank 1
2023-02-23 06:12:06,510 DEBUG TRAIN Batch 15/2400 loss 8.101684 loss_att 9.960751 loss_ctc 12.503670 loss_rnnt 6.937537 hw_loss 0.385129 lr 0.00044278 rank 0
2023-02-23 06:12:06,553 DEBUG TRAIN Batch 15/2400 loss 6.824389 loss_att 8.587612 loss_ctc 11.998346 loss_rnnt 5.506199 hw_loss 0.516906 lr 0.00044271 rank 5
2023-02-23 06:13:22,046 DEBUG TRAIN Batch 15/2500 loss 12.147915 loss_att 12.872274 loss_ctc 16.942595 loss_rnnt 11.086267 hw_loss 0.520283 lr 0.00044254 rank 7
2023-02-23 06:13:22,054 DEBUG TRAIN Batch 15/2500 loss 11.308980 loss_att 11.441257 loss_ctc 13.751178 loss_rnnt 10.640640 hw_loss 0.592983 lr 0.00044259 rank 3
2023-02-23 06:13:22,055 DEBUG TRAIN Batch 15/2500 loss 10.782469 loss_att 11.004939 loss_ctc 12.367894 loss_rnnt 10.236234 hw_loss 0.544407 lr 0.00044261 rank 0
2023-02-23 06:13:22,057 DEBUG TRAIN Batch 15/2500 loss 10.332046 loss_att 10.137842 loss_ctc 11.480378 loss_rnnt 9.861796 hw_loss 0.667460 lr 0.00044254 rank 5
2023-02-23 06:13:22,057 DEBUG TRAIN Batch 15/2500 loss 12.888116 loss_att 14.937658 loss_ctc 15.998751 loss_rnnt 11.803728 hw_loss 0.486987 lr 0.00044259 rank 2
2023-02-23 06:13:22,059 DEBUG TRAIN Batch 15/2500 loss 12.041330 loss_att 12.244799 loss_ctc 15.453783 loss_rnnt 11.222034 hw_loss 0.606765 lr 0.00044261 rank 4
2023-02-23 06:13:22,061 DEBUG TRAIN Batch 15/2500 loss 3.068665 loss_att 6.814995 loss_ctc 4.976557 loss_rnnt 1.861563 hw_loss 0.381470 lr 0.00044263 rank 6
2023-02-23 06:13:22,062 DEBUG TRAIN Batch 15/2500 loss 19.529327 loss_att 17.373066 loss_ctc 26.531178 loss_rnnt 18.717701 hw_loss 0.579941 lr 0.00044265 rank 1
2023-02-23 06:14:34,500 DEBUG TRAIN Batch 15/2600 loss 9.688390 loss_att 11.266443 loss_ctc 9.880091 loss_rnnt 9.139152 hw_loss 0.390127 lr 0.00044244 rank 4
2023-02-23 06:14:34,512 DEBUG TRAIN Batch 15/2600 loss 7.682077 loss_att 10.704706 loss_ctc 8.608888 loss_rnnt 6.767008 hw_loss 0.350564 lr 0.00044237 rank 7
2023-02-23 06:14:34,513 DEBUG TRAIN Batch 15/2600 loss 4.559814 loss_att 9.155313 loss_ctc 5.821235 loss_rnnt 3.146007 hw_loss 0.612222 lr 0.00044242 rank 2
2023-02-23 06:14:34,518 DEBUG TRAIN Batch 15/2600 loss 21.136320 loss_att 25.845028 loss_ctc 32.109905 loss_rnnt 18.513189 hw_loss 0.409205 lr 0.00044237 rank 5
2023-02-23 06:14:34,518 DEBUG TRAIN Batch 15/2600 loss 13.270646 loss_att 15.458151 loss_ctc 12.495234 loss_rnnt 12.736855 hw_loss 0.374398 lr 0.00044248 rank 1
2023-02-23 06:14:34,520 DEBUG TRAIN Batch 15/2600 loss 18.307789 loss_att 20.143286 loss_ctc 24.064938 loss_rnnt 16.959019 hw_loss 0.401345 lr 0.00044245 rank 6
2023-02-23 06:14:34,520 DEBUG TRAIN Batch 15/2600 loss 19.260538 loss_att 23.086340 loss_ctc 25.034000 loss_rnnt 17.487661 hw_loss 0.446105 lr 0.00044242 rank 3
2023-02-23 06:14:34,521 DEBUG TRAIN Batch 15/2600 loss 2.708432 loss_att 5.710453 loss_ctc 2.492195 loss_rnnt 1.947550 hw_loss 0.354955 lr 0.00044244 rank 0
2023-02-23 06:15:47,314 DEBUG TRAIN Batch 15/2700 loss 6.378466 loss_att 10.796209 loss_ctc 9.843047 loss_rnnt 4.843442 hw_loss 0.355368 lr 0.00044219 rank 7
2023-02-23 06:15:47,316 DEBUG TRAIN Batch 15/2700 loss 5.302331 loss_att 7.571825 loss_ctc 7.036088 loss_rnnt 4.429428 hw_loss 0.352195 lr 0.00044224 rank 2
2023-02-23 06:15:47,323 DEBUG TRAIN Batch 15/2700 loss 10.578873 loss_att 13.380953 loss_ctc 13.041808 loss_rnnt 9.491244 hw_loss 0.372789 lr 0.00044225 rank 3
2023-02-23 06:15:47,323 DEBUG TRAIN Batch 15/2700 loss 4.648452 loss_att 8.575488 loss_ctc 5.259644 loss_rnnt 3.547777 hw_loss 0.438328 lr 0.00044231 rank 1
2023-02-23 06:15:47,324 DEBUG TRAIN Batch 15/2700 loss 11.947424 loss_att 16.444809 loss_ctc 19.299759 loss_rnnt 9.876604 hw_loss 0.358182 lr 0.00044219 rank 5
2023-02-23 06:15:47,325 DEBUG TRAIN Batch 15/2700 loss 2.134255 loss_att 5.342358 loss_ctc 2.544837 loss_rnnt 1.201626 hw_loss 0.442995 lr 0.00044228 rank 6
2023-02-23 06:15:47,328 DEBUG TRAIN Batch 15/2700 loss 8.508945 loss_att 10.043889 loss_ctc 11.928465 loss_rnnt 7.536508 hw_loss 0.392837 lr 0.00044227 rank 4
2023-02-23 06:15:47,329 DEBUG TRAIN Batch 15/2700 loss 12.162956 loss_att 18.712589 loss_ctc 17.855085 loss_rnnt 9.843551 hw_loss 0.469742 lr 0.00044226 rank 0
2023-02-23 06:17:00,703 DEBUG TRAIN Batch 15/2800 loss 9.066700 loss_att 11.139299 loss_ctc 11.209705 loss_rnnt 8.153480 hw_loss 0.399313 lr 0.00044202 rank 7
2023-02-23 06:17:00,703 DEBUG TRAIN Batch 15/2800 loss 6.001480 loss_att 11.046942 loss_ctc 10.625199 loss_rnnt 4.137486 hw_loss 0.447009 lr 0.00044202 rank 5
2023-02-23 06:17:00,706 DEBUG TRAIN Batch 15/2800 loss 10.510349 loss_att 12.348455 loss_ctc 13.441893 loss_rnnt 9.548452 hw_loss 0.381381 lr 0.00044213 rank 1
2023-02-23 06:17:00,707 DEBUG TRAIN Batch 15/2800 loss 12.071430 loss_att 16.666582 loss_ctc 15.161856 loss_rnnt 10.520292 hw_loss 0.412596 lr 0.00044209 rank 0
2023-02-23 06:17:00,711 DEBUG TRAIN Batch 15/2800 loss 5.811809 loss_att 8.526103 loss_ctc 8.236711 loss_rnnt 4.704464 hw_loss 0.452185 lr 0.00044207 rank 3
2023-02-23 06:17:00,713 DEBUG TRAIN Batch 15/2800 loss 13.757138 loss_att 17.010927 loss_ctc 19.732414 loss_rnnt 12.035004 hw_loss 0.515009 lr 0.00044207 rank 2
2023-02-23 06:17:00,718 DEBUG TRAIN Batch 15/2800 loss 8.154529 loss_att 10.694390 loss_ctc 9.603232 loss_rnnt 7.239935 hw_loss 0.400237 lr 0.00044211 rank 6
2023-02-23 06:17:00,757 DEBUG TRAIN Batch 15/2800 loss 9.829069 loss_att 12.593493 loss_ctc 15.622705 loss_rnnt 8.270166 hw_loss 0.437876 lr 0.00044209 rank 4
2023-02-23 06:18:14,606 DEBUG TRAIN Batch 15/2900 loss 2.710648 loss_att 5.635032 loss_ctc 4.657556 loss_rnnt 1.632495 hw_loss 0.438167 lr 0.00044190 rank 2
2023-02-23 06:18:14,609 DEBUG TRAIN Batch 15/2900 loss 7.247581 loss_att 10.074733 loss_ctc 10.497370 loss_rnnt 6.004155 hw_loss 0.458795 lr 0.00044185 rank 7
2023-02-23 06:18:14,609 DEBUG TRAIN Batch 15/2900 loss 9.992293 loss_att 13.462631 loss_ctc 16.020365 loss_rnnt 8.264872 hw_loss 0.430519 lr 0.00044192 rank 0
2023-02-23 06:18:14,609 DEBUG TRAIN Batch 15/2900 loss 11.994269 loss_att 14.006828 loss_ctc 16.113726 loss_rnnt 10.837542 hw_loss 0.384287 lr 0.00044190 rank 3
2023-02-23 06:18:14,613 DEBUG TRAIN Batch 15/2900 loss 12.525522 loss_att 17.405628 loss_ctc 15.422705 loss_rnnt 10.978661 hw_loss 0.346028 lr 0.00044185 rank 5
2023-02-23 06:18:14,615 DEBUG TRAIN Batch 15/2900 loss 14.593553 loss_att 17.888412 loss_ctc 15.829012 loss_rnnt 13.571051 hw_loss 0.372756 lr 0.00044196 rank 1
2023-02-23 06:18:14,615 DEBUG TRAIN Batch 15/2900 loss 13.880762 loss_att 12.780287 loss_ctc 16.318878 loss_rnnt 13.549347 hw_loss 0.424555 lr 0.00044193 rank 6
2023-02-23 06:18:14,663 DEBUG TRAIN Batch 15/2900 loss 3.890503 loss_att 6.368509 loss_ctc 4.328027 loss_rnnt 3.091204 hw_loss 0.460054 lr 0.00044192 rank 4
2023-02-23 06:19:26,989 DEBUG TRAIN Batch 15/3000 loss 7.253582 loss_att 10.565034 loss_ctc 8.961568 loss_rnnt 6.177544 hw_loss 0.348780 lr 0.00044168 rank 7
2023-02-23 06:19:26,990 DEBUG TRAIN Batch 15/3000 loss 10.234731 loss_att 14.016590 loss_ctc 16.348423 loss_rnnt 8.475092 hw_loss 0.352701 lr 0.00044172 rank 2
2023-02-23 06:19:26,990 DEBUG TRAIN Batch 15/3000 loss 12.790240 loss_att 14.149174 loss_ctc 17.437660 loss_rnnt 11.648438 hw_loss 0.469424 lr 0.00044179 rank 1
2023-02-23 06:19:26,992 DEBUG TRAIN Batch 15/3000 loss 14.017231 loss_att 17.887297 loss_ctc 20.845592 loss_rnnt 12.106726 hw_loss 0.423833 lr 0.00044175 rank 4
2023-02-23 06:19:26,993 DEBUG TRAIN Batch 15/3000 loss 8.227005 loss_att 10.297070 loss_ctc 11.789245 loss_rnnt 7.137146 hw_loss 0.376650 lr 0.00044167 rank 5
2023-02-23 06:19:26,995 DEBUG TRAIN Batch 15/3000 loss 10.842909 loss_att 14.084175 loss_ctc 16.898987 loss_rnnt 9.158490 hw_loss 0.428792 lr 0.00044176 rank 6
2023-02-23 06:19:26,996 DEBUG TRAIN Batch 15/3000 loss 16.294758 loss_att 20.828632 loss_ctc 22.375778 loss_rnnt 14.356184 hw_loss 0.414372 lr 0.00044173 rank 3
2023-02-23 06:19:27,046 DEBUG TRAIN Batch 15/3000 loss 9.962201 loss_att 12.763493 loss_ctc 14.794306 loss_rnnt 8.472785 hw_loss 0.534145 lr 0.00044175 rank 0
2023-02-23 06:20:40,444 DEBUG TRAIN Batch 15/3100 loss 9.084857 loss_att 9.589536 loss_ctc 10.309835 loss_rnnt 8.502026 hw_loss 0.597309 lr 0.00044150 rank 7
2023-02-23 06:20:40,444 DEBUG TRAIN Batch 15/3100 loss 10.804640 loss_att 14.433143 loss_ctc 18.325043 loss_rnnt 8.863345 hw_loss 0.399138 lr 0.00044155 rank 2
2023-02-23 06:20:40,449 DEBUG TRAIN Batch 15/3100 loss 13.853372 loss_att 15.124626 loss_ctc 17.123543 loss_rnnt 12.915660 hw_loss 0.463945 lr 0.00044157 rank 0
2023-02-23 06:20:40,451 DEBUG TRAIN Batch 15/3100 loss 10.680861 loss_att 13.254265 loss_ctc 14.607651 loss_rnnt 9.417734 hw_loss 0.421638 lr 0.00044156 rank 3
2023-02-23 06:20:40,453 DEBUG TRAIN Batch 15/3100 loss 12.274184 loss_att 15.195145 loss_ctc 18.315042 loss_rnnt 10.699963 hw_loss 0.346090 lr 0.00044162 rank 1
2023-02-23 06:20:40,455 DEBUG TRAIN Batch 15/3100 loss 13.504488 loss_att 13.114711 loss_ctc 14.913022 loss_rnnt 13.076617 hw_loss 0.596291 lr 0.00044150 rank 5
2023-02-23 06:20:40,455 DEBUG TRAIN Batch 15/3100 loss 12.428864 loss_att 13.540340 loss_ctc 16.922049 loss_rnnt 11.293853 hw_loss 0.588048 lr 0.00044159 rank 6
2023-02-23 06:20:40,498 DEBUG TRAIN Batch 15/3100 loss 12.277801 loss_att 11.589175 loss_ctc 13.415201 loss_rnnt 11.978419 hw_loss 0.535224 lr 0.00044158 rank 4
2023-02-23 06:21:56,169 DEBUG TRAIN Batch 15/3200 loss 5.903098 loss_att 6.306228 loss_ctc 6.759481 loss_rnnt 5.429171 hw_loss 0.523342 lr 0.00044138 rank 2
2023-02-23 06:21:56,175 DEBUG TRAIN Batch 15/3200 loss 8.758753 loss_att 8.657612 loss_ctc 10.531145 loss_rnnt 8.213018 hw_loss 0.618082 lr 0.00044144 rank 1
2023-02-23 06:21:56,180 DEBUG TRAIN Batch 15/3200 loss 9.669117 loss_att 14.794173 loss_ctc 14.092558 loss_rnnt 7.796888 hw_loss 0.482674 lr 0.00044133 rank 7
2023-02-23 06:21:56,198 DEBUG TRAIN Batch 15/3200 loss 19.162170 loss_att 23.601696 loss_ctc 27.654873 loss_rnnt 16.947350 hw_loss 0.364792 lr 0.00044133 rank 5
2023-02-23 06:21:56,201 DEBUG TRAIN Batch 15/3200 loss 11.036604 loss_att 13.291609 loss_ctc 12.763802 loss_rnnt 10.167921 hw_loss 0.351355 lr 0.00044138 rank 3
2023-02-23 06:21:56,216 DEBUG TRAIN Batch 15/3200 loss 6.216140 loss_att 9.394522 loss_ctc 9.456464 loss_rnnt 4.909450 hw_loss 0.448069 lr 0.00044140 rank 4
2023-02-23 06:21:56,225 DEBUG TRAIN Batch 15/3200 loss 16.841238 loss_att 23.190971 loss_ctc 24.111477 loss_rnnt 14.359530 hw_loss 0.454492 lr 0.00044140 rank 0
2023-02-23 06:21:56,225 DEBUG TRAIN Batch 15/3200 loss 2.907304 loss_att 5.875481 loss_ctc 4.115339 loss_rnnt 1.953639 hw_loss 0.373048 lr 0.00044142 rank 6
2023-02-23 06:23:08,544 DEBUG TRAIN Batch 15/3300 loss 3.530073 loss_att 6.203305 loss_ctc 4.073750 loss_rnnt 2.696200 hw_loss 0.425131 lr 0.00044116 rank 7
2023-02-23 06:23:08,550 DEBUG TRAIN Batch 15/3300 loss 8.117236 loss_att 11.244615 loss_ctc 12.670469 loss_rnnt 6.662149 hw_loss 0.417214 lr 0.00044121 rank 3
2023-02-23 06:23:08,553 DEBUG TRAIN Batch 15/3300 loss 7.325811 loss_att 12.485865 loss_ctc 10.598390 loss_rnnt 5.628702 hw_loss 0.428915 lr 0.00044123 rank 0
2023-02-23 06:23:08,554 DEBUG TRAIN Batch 15/3300 loss 6.083588 loss_att 8.437622 loss_ctc 6.727886 loss_rnnt 5.328382 hw_loss 0.372175 lr 0.00044121 rank 2
2023-02-23 06:23:08,554 DEBUG TRAIN Batch 15/3300 loss 8.608847 loss_att 12.958324 loss_ctc 13.344269 loss_rnnt 6.910005 hw_loss 0.370420 lr 0.00044123 rank 4
2023-02-23 06:23:08,557 DEBUG TRAIN Batch 15/3300 loss 21.268301 loss_att 26.323116 loss_ctc 33.142189 loss_rnnt 18.496861 hw_loss 0.332421 lr 0.00044127 rank 1
2023-02-23 06:23:08,560 DEBUG TRAIN Batch 15/3300 loss 15.425204 loss_att 17.811974 loss_ctc 18.202093 loss_rnnt 14.360886 hw_loss 0.406337 lr 0.00044125 rank 6
2023-02-23 06:23:08,601 DEBUG TRAIN Batch 15/3300 loss 10.980322 loss_att 12.654402 loss_ctc 11.581775 loss_rnnt 10.341869 hw_loss 0.418956 lr 0.00044116 rank 5
2023-02-23 06:24:21,390 DEBUG TRAIN Batch 15/3400 loss 5.467320 loss_att 10.172703 loss_ctc 8.495963 loss_rnnt 3.904385 hw_loss 0.408824 lr 0.00044104 rank 3
2023-02-23 06:24:21,391 DEBUG TRAIN Batch 15/3400 loss 11.766171 loss_att 15.617386 loss_ctc 19.233213 loss_rnnt 9.805020 hw_loss 0.366190 lr 0.00044099 rank 5
2023-02-23 06:24:21,393 DEBUG TRAIN Batch 15/3400 loss 6.387827 loss_att 9.002383 loss_ctc 6.559566 loss_rnnt 5.636642 hw_loss 0.385078 lr 0.00044106 rank 4
2023-02-23 06:24:21,393 DEBUG TRAIN Batch 15/3400 loss 7.505437 loss_att 8.108555 loss_ctc 9.368595 loss_rnnt 6.937928 hw_loss 0.372119 lr 0.00044110 rank 1
2023-02-23 06:24:21,395 DEBUG TRAIN Batch 15/3400 loss 11.816297 loss_att 13.798553 loss_ctc 15.901525 loss_rnnt 10.678102 hw_loss 0.369461 lr 0.00044099 rank 7
2023-02-23 06:24:21,396 DEBUG TRAIN Batch 15/3400 loss 3.730796 loss_att 7.331243 loss_ctc 4.092416 loss_rnnt 2.772842 hw_loss 0.355590 lr 0.00044106 rank 0
2023-02-23 06:24:21,400 DEBUG TRAIN Batch 15/3400 loss 6.716892 loss_att 10.164798 loss_ctc 8.796818 loss_rnnt 5.571681 hw_loss 0.334323 lr 0.00044104 rank 2
2023-02-23 06:24:21,401 DEBUG TRAIN Batch 15/3400 loss 20.594273 loss_att 21.419857 loss_ctc 22.751183 loss_rnnt 19.963398 hw_loss 0.334067 lr 0.00044107 rank 6
2023-02-23 06:25:35,452 DEBUG TRAIN Batch 15/3500 loss 10.997636 loss_att 13.783640 loss_ctc 13.476555 loss_rnnt 9.907671 hw_loss 0.379203 lr 0.00044087 rank 3
2023-02-23 06:25:35,457 DEBUG TRAIN Batch 15/3500 loss 8.395370 loss_att 12.490705 loss_ctc 10.274849 loss_rnnt 7.072703 hw_loss 0.474381 lr 0.00044093 rank 1
2023-02-23 06:25:35,465 DEBUG TRAIN Batch 15/3500 loss 8.830639 loss_att 10.182623 loss_ctc 12.800947 loss_rnnt 7.782994 hw_loss 0.464762 lr 0.00044089 rank 4
2023-02-23 06:25:35,468 DEBUG TRAIN Batch 15/3500 loss 11.443265 loss_att 13.132011 loss_ctc 14.849633 loss_rnnt 10.430538 hw_loss 0.413990 lr 0.00044090 rank 6
2023-02-23 06:25:35,473 DEBUG TRAIN Batch 15/3500 loss 7.372601 loss_att 13.065868 loss_ctc 10.231097 loss_rnnt 5.643704 hw_loss 0.392080 lr 0.00044087 rank 2
2023-02-23 06:25:35,491 DEBUG TRAIN Batch 15/3500 loss 7.980363 loss_att 11.701153 loss_ctc 10.398392 loss_rnnt 6.691313 hw_loss 0.417163 lr 0.00044082 rank 7
2023-02-23 06:25:35,505 DEBUG TRAIN Batch 15/3500 loss 20.481550 loss_att 24.192280 loss_ctc 27.460762 loss_rnnt 18.577614 hw_loss 0.433554 lr 0.00044082 rank 5
2023-02-23 06:25:35,514 DEBUG TRAIN Batch 15/3500 loss 13.257005 loss_att 14.105608 loss_ctc 15.540144 loss_rnnt 12.540783 hw_loss 0.453904 lr 0.00044089 rank 0
2023-02-23 06:26:49,366 DEBUG TRAIN Batch 15/3600 loss 21.035713 loss_att 24.152225 loss_ctc 27.189850 loss_rnnt 19.373253 hw_loss 0.409890 lr 0.00044065 rank 7
2023-02-23 06:26:49,368 DEBUG TRAIN Batch 15/3600 loss 9.748700 loss_att 10.722021 loss_ctc 9.192505 loss_rnnt 9.411590 hw_loss 0.406135 lr 0.00044070 rank 3
2023-02-23 06:26:49,367 DEBUG TRAIN Batch 15/3600 loss 7.820335 loss_att 11.747948 loss_ctc 11.703753 loss_rnnt 6.325136 hw_loss 0.359787 lr 0.00044072 rank 4
2023-02-23 06:26:49,369 DEBUG TRAIN Batch 15/3600 loss 9.372360 loss_att 14.114914 loss_ctc 13.479502 loss_rnnt 7.647965 hw_loss 0.427998 lr 0.00044069 rank 2
2023-02-23 06:26:49,371 DEBUG TRAIN Batch 15/3600 loss 13.040736 loss_att 15.201367 loss_ctc 18.425825 loss_rnnt 11.687531 hw_loss 0.380752 lr 0.00044072 rank 0
2023-02-23 06:26:49,372 DEBUG TRAIN Batch 15/3600 loss 10.638161 loss_att 13.068571 loss_ctc 13.494382 loss_rnnt 9.506149 hw_loss 0.497063 lr 0.00044073 rank 6
2023-02-23 06:26:49,373 DEBUG TRAIN Batch 15/3600 loss 11.878468 loss_att 14.358759 loss_ctc 14.793380 loss_rnnt 10.772345 hw_loss 0.415145 lr 0.00044064 rank 5
2023-02-23 06:26:49,420 DEBUG TRAIN Batch 15/3600 loss 12.548747 loss_att 13.977556 loss_ctc 14.273079 loss_rnnt 11.830267 hw_loss 0.380266 lr 0.00044076 rank 1
2023-02-23 06:28:01,633 DEBUG TRAIN Batch 15/3700 loss 9.902784 loss_att 13.898646 loss_ctc 15.761421 loss_rnnt 8.117883 hw_loss 0.383582 lr 0.00044059 rank 1
2023-02-23 06:28:01,635 DEBUG TRAIN Batch 15/3700 loss 12.226332 loss_att 14.943290 loss_ctc 16.488022 loss_rnnt 10.833724 hw_loss 0.526860 lr 0.00044053 rank 3
2023-02-23 06:28:01,635 DEBUG TRAIN Batch 15/3700 loss 4.753178 loss_att 7.519251 loss_ctc 6.642021 loss_rnnt 3.728362 hw_loss 0.412042 lr 0.00044047 rank 7
2023-02-23 06:28:01,636 DEBUG TRAIN Batch 15/3700 loss 15.752745 loss_att 15.292201 loss_ctc 18.234407 loss_rnnt 15.284247 hw_loss 0.430721 lr 0.00044056 rank 6
2023-02-23 06:28:01,637 DEBUG TRAIN Batch 15/3700 loss 8.391468 loss_att 13.376328 loss_ctc 10.533659 loss_rnnt 6.871526 hw_loss 0.445022 lr 0.00044047 rank 5
2023-02-23 06:28:01,638 DEBUG TRAIN Batch 15/3700 loss 11.003728 loss_att 12.842625 loss_ctc 15.688475 loss_rnnt 9.774928 hw_loss 0.443228 lr 0.00044052 rank 2
2023-02-23 06:28:01,639 DEBUG TRAIN Batch 15/3700 loss 11.608891 loss_att 13.543234 loss_ctc 17.633165 loss_rnnt 10.165271 hw_loss 0.475340 lr 0.00044055 rank 4
2023-02-23 06:28:01,640 DEBUG TRAIN Batch 15/3700 loss 10.713503 loss_att 12.643414 loss_ctc 13.089485 loss_rnnt 9.801478 hw_loss 0.392335 lr 0.00044054 rank 0
2023-02-23 06:29:13,932 DEBUG TRAIN Batch 15/3800 loss 6.365965 loss_att 8.039669 loss_ctc 9.619627 loss_rnnt 5.335646 hw_loss 0.490794 lr 0.00044036 rank 3
2023-02-23 06:29:13,935 DEBUG TRAIN Batch 15/3800 loss 13.200274 loss_att 13.364357 loss_ctc 17.706442 loss_rnnt 12.286000 hw_loss 0.526190 lr 0.00044030 rank 7
2023-02-23 06:29:13,936 DEBUG TRAIN Batch 15/3800 loss 12.214518 loss_att 15.117768 loss_ctc 12.627578 loss_rnnt 11.381927 hw_loss 0.369119 lr 0.00044035 rank 2
2023-02-23 06:29:13,936 DEBUG TRAIN Batch 15/3800 loss 18.697943 loss_att 19.404566 loss_ctc 22.789381 loss_rnnt 17.754128 hw_loss 0.481808 lr 0.00044041 rank 1
2023-02-23 06:29:13,938 DEBUG TRAIN Batch 15/3800 loss 12.431184 loss_att 11.312789 loss_ctc 14.976185 loss_rnnt 11.941492 hw_loss 0.701317 lr 0.00044038 rank 4
2023-02-23 06:29:13,943 DEBUG TRAIN Batch 15/3800 loss 12.835139 loss_att 19.169308 loss_ctc 22.583279 loss_rnnt 10.073305 hw_loss 0.366088 lr 0.00044039 rank 6
2023-02-23 06:29:13,947 DEBUG TRAIN Batch 15/3800 loss 12.079242 loss_att 12.813053 loss_ctc 13.430568 loss_rnnt 11.487594 hw_loss 0.496330 lr 0.00044037 rank 0
2023-02-23 06:29:13,987 DEBUG TRAIN Batch 15/3800 loss 21.034206 loss_att 21.590633 loss_ctc 27.420483 loss_rnnt 19.872227 hw_loss 0.373483 lr 0.00044030 rank 5
2023-02-23 06:30:28,958 DEBUG TRAIN Batch 15/3900 loss 11.833334 loss_att 13.557948 loss_ctc 13.529003 loss_rnnt 11.105628 hw_loss 0.293800 lr 0.00044024 rank 1
2023-02-23 06:30:28,959 DEBUG TRAIN Batch 15/3900 loss 17.194427 loss_att 18.018166 loss_ctc 17.620853 loss_rnnt 16.777695 hw_loss 0.365866 lr 0.00044013 rank 5
2023-02-23 06:30:28,970 DEBUG TRAIN Batch 15/3900 loss 10.260426 loss_att 15.585629 loss_ctc 14.863582 loss_rnnt 8.365859 hw_loss 0.404573 lr 0.00044018 rank 2
2023-02-23 06:30:28,971 DEBUG TRAIN Batch 15/3900 loss 8.243747 loss_att 11.439085 loss_ctc 11.204121 loss_rnnt 6.973602 hw_loss 0.443175 lr 0.00044022 rank 6
2023-02-23 06:30:28,971 DEBUG TRAIN Batch 15/3900 loss 11.780214 loss_att 16.654770 loss_ctc 16.876198 loss_rnnt 9.944049 hw_loss 0.340854 lr 0.00044020 rank 4
2023-02-23 06:30:28,973 DEBUG TRAIN Batch 15/3900 loss 16.706501 loss_att 23.239338 loss_ctc 26.304718 loss_rnnt 13.899840 hw_loss 0.413122 lr 0.00044020 rank 0
2023-02-23 06:30:28,978 DEBUG TRAIN Batch 15/3900 loss 5.579935 loss_att 7.905770 loss_ctc 6.082066 loss_rnnt 4.779909 hw_loss 0.502328 lr 0.00044018 rank 3
2023-02-23 06:30:28,990 DEBUG TRAIN Batch 15/3900 loss 11.671385 loss_att 16.937796 loss_ctc 15.316568 loss_rnnt 9.946418 hw_loss 0.348112 lr 0.00044013 rank 7
2023-02-23 06:31:41,748 DEBUG TRAIN Batch 15/4000 loss 5.908020 loss_att 9.494932 loss_ctc 5.883342 loss_rnnt 5.006380 hw_loss 0.351654 lr 0.00044001 rank 3
2023-02-23 06:31:41,768 DEBUG TRAIN Batch 15/4000 loss 7.468319 loss_att 11.435597 loss_ctc 12.538898 loss_rnnt 5.780790 hw_loss 0.408742 lr 0.00044007 rank 1
2023-02-23 06:31:41,770 DEBUG TRAIN Batch 15/4000 loss 18.025694 loss_att 23.114798 loss_ctc 23.050604 loss_rnnt 16.114326 hw_loss 0.419175 lr 0.00043996 rank 7
2023-02-23 06:31:41,770 DEBUG TRAIN Batch 15/4000 loss 19.888704 loss_att 20.511208 loss_ctc 23.436346 loss_rnnt 19.064972 hw_loss 0.424147 lr 0.00044003 rank 4
2023-02-23 06:31:41,771 DEBUG TRAIN Batch 15/4000 loss 19.367525 loss_att 24.852341 loss_ctc 30.489965 loss_rnnt 16.590620 hw_loss 0.369282 lr 0.00044005 rank 6
2023-02-23 06:31:41,773 DEBUG TRAIN Batch 15/4000 loss 21.252544 loss_att 24.917437 loss_ctc 27.388733 loss_rnnt 19.486013 hw_loss 0.403865 lr 0.00044001 rank 2
2023-02-23 06:31:41,774 DEBUG TRAIN Batch 15/4000 loss 14.181503 loss_att 16.635925 loss_ctc 20.362640 loss_rnnt 12.646091 hw_loss 0.413202 lr 0.00044003 rank 0
2023-02-23 06:31:41,775 DEBUG TRAIN Batch 15/4000 loss 8.422573 loss_att 9.134868 loss_ctc 12.310138 loss_rnnt 7.510955 hw_loss 0.470280 lr 0.00043996 rank 5
2023-02-23 06:32:53,933 DEBUG TRAIN Batch 15/4100 loss 8.611504 loss_att 14.201158 loss_ctc 11.512736 loss_rnnt 6.914194 hw_loss 0.361029 lr 0.00043986 rank 4
2023-02-23 06:32:53,934 DEBUG TRAIN Batch 15/4100 loss 17.760929 loss_att 21.719055 loss_ctc 21.008459 loss_rnnt 16.316542 hw_loss 0.412044 lr 0.00043990 rank 1
2023-02-23 06:32:53,936 DEBUG TRAIN Batch 15/4100 loss 17.916468 loss_att 18.391829 loss_ctc 20.259687 loss_rnnt 17.309948 hw_loss 0.373157 lr 0.00043986 rank 0
2023-02-23 06:32:53,937 DEBUG TRAIN Batch 15/4100 loss 9.021544 loss_att 12.667542 loss_ctc 12.801937 loss_rnnt 7.601465 hw_loss 0.350301 lr 0.00043979 rank 5
2023-02-23 06:32:53,938 DEBUG TRAIN Batch 15/4100 loss 8.412524 loss_att 12.785373 loss_ctc 10.590454 loss_rnnt 7.005430 hw_loss 0.454001 lr 0.00043984 rank 3
2023-02-23 06:32:53,941 DEBUG TRAIN Batch 15/4100 loss 13.040876 loss_att 17.774433 loss_ctc 15.195738 loss_rnnt 11.582829 hw_loss 0.420038 lr 0.00043988 rank 6
2023-02-23 06:32:53,941 DEBUG TRAIN Batch 15/4100 loss 11.629618 loss_att 15.406248 loss_ctc 17.911791 loss_rnnt 9.857388 hw_loss 0.336152 lr 0.00043984 rank 2
2023-02-23 06:32:53,950 DEBUG TRAIN Batch 15/4100 loss 7.028141 loss_att 9.321402 loss_ctc 11.513162 loss_rnnt 5.767964 hw_loss 0.381602 lr 0.00043979 rank 7
2023-02-23 06:34:06,912 DEBUG TRAIN Batch 15/4200 loss 9.625561 loss_att 11.597881 loss_ctc 10.615481 loss_rnnt 8.871856 hw_loss 0.426096 lr 0.00043969 rank 0
2023-02-23 06:34:06,912 DEBUG TRAIN Batch 15/4200 loss 17.489586 loss_att 21.104073 loss_ctc 24.410576 loss_rnnt 15.650073 hw_loss 0.363405 lr 0.00043973 rank 1
2023-02-23 06:34:06,912 DEBUG TRAIN Batch 15/4200 loss 8.331485 loss_att 9.608570 loss_ctc 11.248936 loss_rnnt 7.447499 hw_loss 0.449202 lr 0.00043962 rank 7
2023-02-23 06:34:06,914 DEBUG TRAIN Batch 15/4200 loss 13.065142 loss_att 15.781242 loss_ctc 15.133382 loss_rnnt 12.021576 hw_loss 0.421087 lr 0.00043962 rank 5
2023-02-23 06:34:06,917 DEBUG TRAIN Batch 15/4200 loss 15.920260 loss_att 18.316111 loss_ctc 20.558369 loss_rnnt 14.600316 hw_loss 0.416925 lr 0.00043967 rank 2
2023-02-23 06:34:06,918 DEBUG TRAIN Batch 15/4200 loss 9.665122 loss_att 14.046535 loss_ctc 14.159485 loss_rnnt 7.979673 hw_loss 0.393596 lr 0.00043967 rank 3
2023-02-23 06:34:06,919 DEBUG TRAIN Batch 15/4200 loss 6.637204 loss_att 7.484168 loss_ctc 7.827535 loss_rnnt 6.106647 hw_loss 0.379601 lr 0.00043971 rank 6
2023-02-23 06:34:06,920 DEBUG TRAIN Batch 15/4200 loss 8.182219 loss_att 9.793320 loss_ctc 8.896741 loss_rnnt 7.548775 hw_loss 0.404911 lr 0.00043969 rank 4
2023-02-23 06:35:21,289 DEBUG TRAIN Batch 15/4300 loss 22.850164 loss_att 24.149883 loss_ctc 26.318382 loss_rnnt 21.898981 hw_loss 0.429018 lr 0.00043945 rank 7
2023-02-23 06:35:21,291 DEBUG TRAIN Batch 15/4300 loss 9.493620 loss_att 11.576565 loss_ctc 13.133217 loss_rnnt 8.375229 hw_loss 0.405980 lr 0.00043952 rank 0
2023-02-23 06:35:21,291 DEBUG TRAIN Batch 15/4300 loss 5.278949 loss_att 9.378875 loss_ctc 9.955895 loss_rnnt 3.620685 hw_loss 0.402536 lr 0.00043950 rank 3
2023-02-23 06:35:21,292 DEBUG TRAIN Batch 15/4300 loss 17.730761 loss_att 17.045208 loss_ctc 21.288042 loss_rnnt 17.154245 hw_loss 0.448727 lr 0.00043945 rank 5
2023-02-23 06:35:21,291 DEBUG TRAIN Batch 15/4300 loss 15.158195 loss_att 19.201927 loss_ctc 22.267181 loss_rnnt 13.208076 hw_loss 0.362825 lr 0.00043952 rank 4
2023-02-23 06:35:21,293 DEBUG TRAIN Batch 15/4300 loss 11.253261 loss_att 13.494696 loss_ctc 13.997732 loss_rnnt 10.182451 hw_loss 0.481113 lr 0.00043950 rank 2
2023-02-23 06:35:21,293 DEBUG TRAIN Batch 15/4300 loss 12.698609 loss_att 15.420069 loss_ctc 15.846124 loss_rnnt 11.516846 hw_loss 0.408382 lr 0.00043956 rank 1
2023-02-23 06:35:21,336 DEBUG TRAIN Batch 15/4300 loss 10.417930 loss_att 12.991293 loss_ctc 14.967974 loss_rnnt 9.118774 hw_loss 0.333393 lr 0.00043954 rank 6
2023-02-23 06:36:33,362 DEBUG TRAIN Batch 15/4400 loss 8.924862 loss_att 10.347283 loss_ctc 9.806915 loss_rnnt 8.246225 hw_loss 0.518523 lr 0.00043928 rank 7
2023-02-23 06:36:33,363 DEBUG TRAIN Batch 15/4400 loss 12.852677 loss_att 16.229776 loss_ctc 16.954937 loss_rnnt 11.408768 hw_loss 0.415355 lr 0.00043933 rank 2
2023-02-23 06:36:33,365 DEBUG TRAIN Batch 15/4400 loss 7.957690 loss_att 9.032346 loss_ctc 10.771131 loss_rnnt 7.119012 hw_loss 0.466166 lr 0.00043935 rank 0
2023-02-23 06:36:33,366 DEBUG TRAIN Batch 15/4400 loss 14.334327 loss_att 13.951838 loss_ctc 17.153749 loss_rnnt 13.721764 hw_loss 0.587132 lr 0.00043937 rank 6
2023-02-23 06:36:33,366 DEBUG TRAIN Batch 15/4400 loss 20.655466 loss_att 20.514210 loss_ctc 19.375336 loss_rnnt 20.614092 hw_loss 0.450580 lr 0.00043933 rank 3
2023-02-23 06:36:33,370 DEBUG TRAIN Batch 15/4400 loss 7.089821 loss_att 9.480657 loss_ctc 12.530443 loss_rnnt 5.652338 hw_loss 0.438564 lr 0.00043939 rank 1
2023-02-23 06:36:33,371 DEBUG TRAIN Batch 15/4400 loss 13.049777 loss_att 16.320635 loss_ctc 17.210609 loss_rnnt 11.606712 hw_loss 0.438967 lr 0.00043935 rank 4
2023-02-23 06:36:33,374 DEBUG TRAIN Batch 15/4400 loss 10.925877 loss_att 13.259243 loss_ctc 16.895023 loss_rnnt 9.409895 hw_loss 0.475166 lr 0.00043928 rank 5
2023-02-23 06:37:45,805 DEBUG TRAIN Batch 15/4500 loss 13.930412 loss_att 15.448653 loss_ctc 16.752239 loss_rnnt 12.957198 hw_loss 0.549979 lr 0.00043916 rank 2
2023-02-23 06:37:45,806 DEBUG TRAIN Batch 15/4500 loss 20.388849 loss_att 20.808144 loss_ctc 19.476488 loss_rnnt 20.220261 hw_loss 0.386956 lr 0.00043918 rank 0
2023-02-23 06:37:45,806 DEBUG TRAIN Batch 15/4500 loss 10.821232 loss_att 11.595798 loss_ctc 13.055410 loss_rnnt 10.192907 hw_loss 0.329100 lr 0.00043918 rank 4
2023-02-23 06:37:45,807 DEBUG TRAIN Batch 15/4500 loss 6.872628 loss_att 10.411097 loss_ctc 14.118499 loss_rnnt 4.984815 hw_loss 0.401256 lr 0.00043911 rank 7
2023-02-23 06:37:45,807 DEBUG TRAIN Batch 15/4500 loss 11.997892 loss_att 10.858881 loss_ctc 14.062754 loss_rnnt 11.544983 hw_loss 0.760119 lr 0.00043922 rank 1
2023-02-23 06:37:45,810 DEBUG TRAIN Batch 15/4500 loss 7.395574 loss_att 11.620207 loss_ctc 10.575715 loss_rnnt 5.920855 hw_loss 0.385826 lr 0.00043920 rank 6
2023-02-23 06:37:45,812 DEBUG TRAIN Batch 15/4500 loss 11.919362 loss_att 15.051307 loss_ctc 18.052568 loss_rnnt 10.245996 hw_loss 0.429781 lr 0.00043911 rank 5
2023-02-23 06:37:45,811 DEBUG TRAIN Batch 15/4500 loss 4.078358 loss_att 8.485682 loss_ctc 5.289339 loss_rnnt 2.863556 hw_loss 0.322262 lr 0.00043916 rank 3
2023-02-23 06:38:59,998 DEBUG TRAIN Batch 15/4600 loss 13.774977 loss_att 16.428961 loss_ctc 19.257818 loss_rnnt 12.301250 hw_loss 0.397285 lr 0.00043894 rank 5
2023-02-23 06:39:00,003 DEBUG TRAIN Batch 15/4600 loss 5.991960 loss_att 8.614943 loss_ctc 10.412814 loss_rnnt 4.668447 hw_loss 0.392752 lr 0.00043901 rank 0
2023-02-23 06:39:00,010 DEBUG TRAIN Batch 15/4600 loss 15.092823 loss_att 17.946121 loss_ctc 18.861195 loss_rnnt 13.864608 hw_loss 0.290824 lr 0.00043903 rank 6
2023-02-23 06:39:00,012 DEBUG TRAIN Batch 15/4600 loss 26.170952 loss_att 29.787384 loss_ctc 38.138542 loss_rnnt 23.630650 hw_loss 0.415008 lr 0.00043902 rank 4
2023-02-23 06:39:00,018 DEBUG TRAIN Batch 15/4600 loss 5.025199 loss_att 11.645355 loss_ctc 5.830303 loss_rnnt 3.406401 hw_loss 0.351413 lr 0.00043894 rank 7
2023-02-23 06:39:00,020 DEBUG TRAIN Batch 15/4600 loss 5.536251 loss_att 6.780818 loss_ctc 5.985591 loss_rnnt 4.951974 hw_loss 0.516470 lr 0.00043899 rank 2
2023-02-23 06:39:00,021 DEBUG TRAIN Batch 15/4600 loss 7.976957 loss_att 10.458603 loss_ctc 8.371395 loss_rnnt 7.197555 hw_loss 0.432154 lr 0.00043900 rank 3
2023-02-23 06:39:00,057 DEBUG TRAIN Batch 15/4600 loss 10.455606 loss_att 13.314869 loss_ctc 13.024928 loss_rnnt 9.353676 hw_loss 0.351563 lr 0.00043905 rank 1
2023-02-23 06:40:13,590 DEBUG TRAIN Batch 15/4700 loss 8.153200 loss_att 10.461502 loss_ctc 10.517635 loss_rnnt 7.186826 hw_loss 0.355230 lr 0.00043878 rank 7
2023-02-23 06:40:13,592 DEBUG TRAIN Batch 15/4700 loss 14.521701 loss_att 17.976635 loss_ctc 17.347355 loss_rnnt 13.232896 hw_loss 0.414495 lr 0.00043886 rank 6
2023-02-23 06:40:13,593 DEBUG TRAIN Batch 15/4700 loss 10.488173 loss_att 12.346959 loss_ctc 13.554562 loss_rnnt 9.506032 hw_loss 0.377869 lr 0.00043885 rank 4
2023-02-23 06:40:13,597 DEBUG TRAIN Batch 15/4700 loss 5.950228 loss_att 11.614118 loss_ctc 8.754285 loss_rnnt 4.255998 hw_loss 0.351708 lr 0.00043889 rank 1
2023-02-23 06:40:13,599 DEBUG TRAIN Batch 15/4700 loss 8.779750 loss_att 13.226370 loss_ctc 13.433543 loss_rnnt 7.036786 hw_loss 0.437127 lr 0.00043884 rank 0
2023-02-23 06:40:13,599 DEBUG TRAIN Batch 15/4700 loss 9.794113 loss_att 14.133814 loss_ctc 11.468406 loss_rnnt 8.529931 hw_loss 0.324381 lr 0.00043883 rank 3
2023-02-23 06:40:13,599 DEBUG TRAIN Batch 15/4700 loss 5.809126 loss_att 9.884485 loss_ctc 7.985332 loss_rnnt 4.462108 hw_loss 0.453350 lr 0.00043882 rank 2
2023-02-23 06:40:13,603 DEBUG TRAIN Batch 15/4700 loss 8.924044 loss_att 14.492158 loss_ctc 10.981089 loss_rnnt 7.322055 hw_loss 0.401426 lr 0.00043877 rank 5
2023-02-23 06:41:26,062 DEBUG TRAIN Batch 15/4800 loss 10.303002 loss_att 15.176638 loss_ctc 12.557947 loss_rnnt 8.776575 hw_loss 0.470701 lr 0.00043868 rank 4
2023-02-23 06:41:26,064 DEBUG TRAIN Batch 15/4800 loss 5.439937 loss_att 9.057773 loss_ctc 6.478354 loss_rnnt 4.356641 hw_loss 0.414887 lr 0.00043865 rank 2
2023-02-23 06:41:26,067 DEBUG TRAIN Batch 15/4800 loss 7.831648 loss_att 12.601917 loss_ctc 8.070812 loss_rnnt 6.659947 hw_loss 0.348297 lr 0.00043861 rank 7
2023-02-23 06:41:26,068 DEBUG TRAIN Batch 15/4800 loss 18.881861 loss_att 23.502602 loss_ctc 25.533449 loss_rnnt 16.815956 hw_loss 0.477896 lr 0.00043872 rank 1
2023-02-23 06:41:26,068 DEBUG TRAIN Batch 15/4800 loss 6.445404 loss_att 11.241470 loss_ctc 10.206201 loss_rnnt 4.705732 hw_loss 0.523159 lr 0.00043866 rank 3
2023-02-23 06:41:26,078 DEBUG TRAIN Batch 15/4800 loss 9.536713 loss_att 14.577675 loss_ctc 12.801867 loss_rnnt 7.886247 hw_loss 0.387973 lr 0.00043860 rank 5
2023-02-23 06:41:26,079 DEBUG TRAIN Batch 15/4800 loss 10.196570 loss_att 13.724990 loss_ctc 12.619484 loss_rnnt 8.976278 hw_loss 0.359160 lr 0.00043869 rank 6
2023-02-23 06:41:26,083 DEBUG TRAIN Batch 15/4800 loss 11.684396 loss_att 15.910009 loss_ctc 14.341965 loss_rnnt 10.245173 hw_loss 0.449545 lr 0.00043868 rank 0
2023-02-23 06:42:38,889 DEBUG TRAIN Batch 15/4900 loss 14.244029 loss_att 17.731037 loss_ctc 15.941849 loss_rnnt 13.156716 hw_loss 0.306627 lr 0.00043851 rank 0
2023-02-23 06:42:38,890 DEBUG TRAIN Batch 15/4900 loss 14.291999 loss_att 19.124933 loss_ctc 16.890167 loss_rnnt 12.748554 hw_loss 0.432067 lr 0.00043844 rank 7
2023-02-23 06:42:38,890 DEBUG TRAIN Batch 15/4900 loss 22.537323 loss_att 29.098055 loss_ctc 32.213203 loss_rnnt 19.727127 hw_loss 0.389873 lr 0.00043852 rank 6
2023-02-23 06:42:38,891 DEBUG TRAIN Batch 15/4900 loss 13.129363 loss_att 14.323124 loss_ctc 15.255495 loss_rnnt 12.390733 hw_loss 0.405736 lr 0.00043851 rank 4
2023-02-23 06:42:38,891 DEBUG TRAIN Batch 15/4900 loss 11.823364 loss_att 14.200239 loss_ctc 13.016915 loss_rnnt 10.930683 hw_loss 0.484061 lr 0.00043844 rank 5
2023-02-23 06:42:38,893 DEBUG TRAIN Batch 15/4900 loss 5.591872 loss_att 8.585009 loss_ctc 7.386856 loss_rnnt 4.569234 hw_loss 0.346274 lr 0.00043855 rank 1
2023-02-23 06:42:38,894 DEBUG TRAIN Batch 15/4900 loss 10.616306 loss_att 13.054935 loss_ctc 15.123798 loss_rnnt 9.278153 hw_loss 0.467677 lr 0.00043849 rank 3
2023-02-23 06:42:38,894 DEBUG TRAIN Batch 15/4900 loss 10.182945 loss_att 14.548719 loss_ctc 13.425444 loss_rnnt 8.660313 hw_loss 0.407145 lr 0.00043849 rank 2
2023-02-23 06:43:53,694 DEBUG TRAIN Batch 15/5000 loss 10.710664 loss_att 12.402855 loss_ctc 13.671515 loss_rnnt 9.725965 hw_loss 0.471522 lr 0.00043827 rank 7
2023-02-23 06:43:53,700 DEBUG TRAIN Batch 15/5000 loss 4.178152 loss_att 6.885649 loss_ctc 5.574788 loss_rnnt 3.230998 hw_loss 0.411442 lr 0.00043832 rank 2
2023-02-23 06:43:53,701 DEBUG TRAIN Batch 15/5000 loss 17.281538 loss_att 20.507868 loss_ctc 20.086823 loss_rnnt 16.048065 hw_loss 0.401569 lr 0.00043832 rank 3
2023-02-23 06:43:53,701 DEBUG TRAIN Batch 15/5000 loss 16.467970 loss_att 17.473799 loss_ctc 18.464025 loss_rnnt 15.759406 hw_loss 0.452361 lr 0.00043834 rank 4
2023-02-23 06:43:53,704 DEBUG TRAIN Batch 15/5000 loss 8.699112 loss_att 10.271622 loss_ctc 11.590858 loss_rnnt 7.771204 hw_loss 0.427202 lr 0.00043827 rank 5
2023-02-23 06:43:53,708 DEBUG TRAIN Batch 15/5000 loss 7.920360 loss_att 10.888439 loss_ctc 11.056433 loss_rnnt 6.637432 hw_loss 0.508443 lr 0.00043834 rank 0
2023-02-23 06:43:53,709 DEBUG TRAIN Batch 15/5000 loss 13.806579 loss_att 13.607043 loss_ctc 15.726605 loss_rnnt 13.318618 hw_loss 0.509747 lr 0.00043835 rank 6
2023-02-23 06:43:53,711 DEBUG TRAIN Batch 15/5000 loss 10.804501 loss_att 13.146365 loss_ctc 11.760498 loss_rnnt 9.965103 hw_loss 0.456671 lr 0.00043838 rank 1
2023-02-23 06:45:06,810 DEBUG TRAIN Batch 15/5100 loss 13.919079 loss_att 17.850584 loss_ctc 15.542595 loss_rnnt 12.732432 hw_loss 0.344771 lr 0.00043810 rank 5
2023-02-23 06:45:06,811 DEBUG TRAIN Batch 15/5100 loss 14.939823 loss_att 14.839972 loss_ctc 17.482712 loss_rnnt 14.372113 hw_loss 0.466177 lr 0.00043810 rank 7
2023-02-23 06:45:06,814 DEBUG TRAIN Batch 15/5100 loss 13.152966 loss_att 14.693539 loss_ctc 15.831107 loss_rnnt 12.259105 hw_loss 0.428740 lr 0.00043815 rank 2
2023-02-23 06:45:06,815 DEBUG TRAIN Batch 15/5100 loss 11.723775 loss_att 10.862511 loss_ctc 15.144266 loss_rnnt 11.098184 hw_loss 0.640835 lr 0.00043819 rank 6
2023-02-23 06:45:06,820 DEBUG TRAIN Batch 15/5100 loss 6.683792 loss_att 7.445522 loss_ctc 7.870370 loss_rnnt 6.121230 hw_loss 0.472510 lr 0.00043817 rank 0
2023-02-23 06:45:06,820 DEBUG TRAIN Batch 15/5100 loss 7.455787 loss_att 12.098000 loss_ctc 7.437527 loss_rnnt 6.334018 hw_loss 0.367054 lr 0.00043817 rank 4
2023-02-23 06:45:06,822 DEBUG TRAIN Batch 15/5100 loss 16.981567 loss_att 23.204821 loss_ctc 20.259367 loss_rnnt 15.129095 hw_loss 0.320213 lr 0.00043815 rank 3
2023-02-23 06:45:06,861 DEBUG TRAIN Batch 15/5100 loss 6.911863 loss_att 8.917391 loss_ctc 9.460150 loss_rnnt 5.861471 hw_loss 0.580339 lr 0.00043821 rank 1
2023-02-23 06:46:20,996 DEBUG TRAIN Batch 15/5200 loss 10.595448 loss_att 14.823786 loss_ctc 12.118305 loss_rnnt 9.361986 hw_loss 0.346399 lr 0.00043798 rank 3
2023-02-23 06:46:20,998 DEBUG TRAIN Batch 15/5200 loss 9.733174 loss_att 11.917125 loss_ctc 10.736454 loss_rnnt 8.963067 hw_loss 0.374147 lr 0.00043800 rank 4
2023-02-23 06:46:21,000 DEBUG TRAIN Batch 15/5200 loss 21.471498 loss_att 25.348335 loss_ctc 29.424412 loss_rnnt 19.422436 hw_loss 0.399951 lr 0.00043804 rank 1
2023-02-23 06:46:21,003 DEBUG TRAIN Batch 15/5200 loss 10.188880 loss_att 11.292287 loss_ctc 13.162750 loss_rnnt 9.332802 hw_loss 0.447903 lr 0.00043800 rank 0
2023-02-23 06:46:21,005 DEBUG TRAIN Batch 15/5200 loss 9.787148 loss_att 18.372478 loss_ctc 13.527496 loss_rnnt 7.362792 hw_loss 0.391078 lr 0.00043793 rank 5
2023-02-23 06:46:21,011 DEBUG TRAIN Batch 15/5200 loss 17.752218 loss_att 21.304852 loss_ctc 23.168020 loss_rnnt 15.999251 hw_loss 0.600624 lr 0.00043793 rank 7
2023-02-23 06:46:21,027 DEBUG TRAIN Batch 15/5200 loss 24.298117 loss_att 33.038448 loss_ctc 33.558170 loss_rnnt 21.116360 hw_loss 0.373161 lr 0.00043798 rank 2
2023-02-23 06:46:21,046 DEBUG TRAIN Batch 15/5200 loss 9.761204 loss_att 17.086102 loss_ctc 14.053453 loss_rnnt 7.540793 hw_loss 0.343371 lr 0.00043802 rank 6
2023-02-23 06:47:36,160 DEBUG TRAIN Batch 15/5300 loss 5.976935 loss_att 9.162938 loss_ctc 7.651769 loss_rnnt 4.925558 hw_loss 0.357873 lr 0.00043777 rank 7
2023-02-23 06:47:36,163 DEBUG TRAIN Batch 15/5300 loss 13.740821 loss_att 18.005255 loss_ctc 18.530144 loss_rnnt 12.026529 hw_loss 0.417802 lr 0.00043787 rank 1
2023-02-23 06:47:36,164 DEBUG TRAIN Batch 15/5300 loss 9.338851 loss_att 10.351195 loss_ctc 10.556881 loss_rnnt 8.689901 hw_loss 0.532643 lr 0.00043781 rank 2
2023-02-23 06:47:36,164 DEBUG TRAIN Batch 15/5300 loss 9.854986 loss_att 13.276642 loss_ctc 15.192928 loss_rnnt 8.258236 hw_loss 0.376300 lr 0.00043784 rank 4
2023-02-23 06:47:36,164 DEBUG TRAIN Batch 15/5300 loss 21.236681 loss_att 23.709511 loss_ctc 23.200054 loss_rnnt 20.269733 hw_loss 0.394875 lr 0.00043785 rank 6
2023-02-23 06:47:36,166 DEBUG TRAIN Batch 15/5300 loss 7.882383 loss_att 12.849131 loss_ctc 12.849617 loss_rnnt 5.981679 hw_loss 0.459481 lr 0.00043783 rank 0
2023-02-23 06:47:36,169 DEBUG TRAIN Batch 15/5300 loss 16.339231 loss_att 23.638897 loss_ctc 23.531744 loss_rnnt 13.720858 hw_loss 0.373944 lr 0.00043776 rank 5
2023-02-23 06:47:36,171 DEBUG TRAIN Batch 15/5300 loss 15.567307 loss_att 22.102657 loss_ctc 24.876499 loss_rnnt 12.816002 hw_loss 0.380641 lr 0.00043782 rank 3
2023-02-23 06:48:49,452 DEBUG TRAIN Batch 15/5400 loss 7.710429 loss_att 9.330507 loss_ctc 9.084600 loss_rnnt 6.956633 hw_loss 0.462294 lr 0.00043765 rank 3
2023-02-23 06:48:49,455 DEBUG TRAIN Batch 15/5400 loss 27.293787 loss_att 26.942305 loss_ctc 48.441357 loss_rnnt 24.378712 hw_loss 0.310679 lr 0.00043760 rank 7
2023-02-23 06:48:49,458 DEBUG TRAIN Batch 15/5400 loss 12.069510 loss_att 14.690363 loss_ctc 16.164383 loss_rnnt 10.762363 hw_loss 0.444362 lr 0.00043767 rank 4
2023-02-23 06:48:49,458 DEBUG TRAIN Batch 15/5400 loss 7.642721 loss_att 8.975286 loss_ctc 7.790129 loss_rnnt 7.168844 hw_loss 0.351955 lr 0.00043764 rank 2
2023-02-23 06:48:49,458 DEBUG TRAIN Batch 15/5400 loss 4.651680 loss_att 7.963147 loss_ctc 5.321673 loss_rnnt 3.657553 hw_loss 0.454690 lr 0.00043768 rank 6
2023-02-23 06:48:49,461 DEBUG TRAIN Batch 15/5400 loss 8.003113 loss_att 10.205964 loss_ctc 12.582386 loss_rnnt 6.731281 hw_loss 0.413796 lr 0.00043760 rank 5
2023-02-23 06:48:49,462 DEBUG TRAIN Batch 15/5400 loss 14.210868 loss_att 19.407431 loss_ctc 16.331692 loss_rnnt 12.649028 hw_loss 0.449534 lr 0.00043771 rank 1
2023-02-23 06:48:49,462 DEBUG TRAIN Batch 15/5400 loss 5.060897 loss_att 9.120046 loss_ctc 5.247905 loss_rnnt 4.014106 hw_loss 0.393801 lr 0.00043767 rank 0
2023-02-23 06:50:01,610 DEBUG TRAIN Batch 15/5500 loss 4.556723 loss_att 8.660316 loss_ctc 7.668265 loss_rnnt 3.069936 hw_loss 0.470991 lr 0.00043748 rank 2
2023-02-23 06:50:01,613 DEBUG TRAIN Batch 15/5500 loss 18.213072 loss_att 20.772820 loss_ctc 23.429161 loss_rnnt 16.789850 hw_loss 0.404613 lr 0.00043748 rank 3
2023-02-23 06:50:01,616 DEBUG TRAIN Batch 15/5500 loss 14.027581 loss_att 16.495455 loss_ctc 21.230043 loss_rnnt 12.363894 hw_loss 0.393343 lr 0.00043750 rank 4
2023-02-23 06:50:01,617 DEBUG TRAIN Batch 15/5500 loss 10.452842 loss_att 13.731047 loss_ctc 14.617104 loss_rnnt 9.011320 hw_loss 0.432460 lr 0.00043743 rank 7
2023-02-23 06:50:01,619 DEBUG TRAIN Batch 15/5500 loss 12.715658 loss_att 12.479618 loss_ctc 14.530097 loss_rnnt 12.291330 hw_loss 0.430519 lr 0.00043751 rank 6
2023-02-23 06:50:01,618 DEBUG TRAIN Batch 15/5500 loss 11.975304 loss_att 16.859486 loss_ctc 16.171886 loss_rnnt 10.250852 hw_loss 0.352632 lr 0.00043750 rank 0
2023-02-23 06:50:01,621 DEBUG TRAIN Batch 15/5500 loss 13.706628 loss_att 21.278419 loss_ctc 21.617924 loss_rnnt 10.950071 hw_loss 0.351297 lr 0.00043754 rank 1
2023-02-23 06:50:01,625 DEBUG TRAIN Batch 15/5500 loss 5.728932 loss_att 8.990225 loss_ctc 7.957891 loss_rnnt 4.561681 hw_loss 0.408372 lr 0.00043743 rank 5
2023-02-23 06:51:14,266 DEBUG TRAIN Batch 15/5600 loss 7.128954 loss_att 12.368632 loss_ctc 9.238688 loss_rnnt 5.586346 hw_loss 0.400079 lr 0.00043733 rank 0
2023-02-23 06:51:14,278 DEBUG TRAIN Batch 15/5600 loss 11.301595 loss_att 15.310410 loss_ctc 14.817472 loss_rnnt 9.821356 hw_loss 0.393173 lr 0.00043731 rank 2
2023-02-23 06:51:14,279 DEBUG TRAIN Batch 15/5600 loss 11.797420 loss_att 13.356413 loss_ctc 15.254952 loss_rnnt 10.784067 hw_loss 0.451028 lr 0.00043733 rank 4
2023-02-23 06:51:14,283 DEBUG TRAIN Batch 15/5600 loss 15.699741 loss_att 17.677073 loss_ctc 21.707130 loss_rnnt 14.276113 hw_loss 0.425955 lr 0.00043726 rank 7
2023-02-23 06:51:14,283 DEBUG TRAIN Batch 15/5600 loss 15.961916 loss_att 17.263802 loss_ctc 23.390131 loss_rnnt 14.477767 hw_loss 0.437519 lr 0.00043731 rank 3
2023-02-23 06:51:14,286 DEBUG TRAIN Batch 15/5600 loss 5.461921 loss_att 8.935411 loss_ctc 6.554445 loss_rnnt 4.382392 hw_loss 0.448426 lr 0.00043735 rank 6
2023-02-23 06:51:14,289 DEBUG TRAIN Batch 15/5600 loss 10.412491 loss_att 11.735213 loss_ctc 14.041700 loss_rnnt 9.444849 hw_loss 0.411005 lr 0.00043737 rank 1
2023-02-23 06:51:14,289 DEBUG TRAIN Batch 15/5600 loss 9.693455 loss_att 11.927902 loss_ctc 12.688926 loss_rnnt 8.642759 hw_loss 0.383268 lr 0.00043726 rank 5
2023-02-23 06:52:29,974 DEBUG TRAIN Batch 15/5700 loss 13.173855 loss_att 16.249317 loss_ctc 17.663923 loss_rnnt 11.752619 hw_loss 0.388998 lr 0.00043714 rank 2
2023-02-23 06:52:29,975 DEBUG TRAIN Batch 15/5700 loss 7.033984 loss_att 7.961109 loss_ctc 11.025736 loss_rnnt 6.049223 hw_loss 0.500816 lr 0.00043717 rank 4
2023-02-23 06:52:29,976 DEBUG TRAIN Batch 15/5700 loss 14.622782 loss_att 20.055109 loss_ctc 18.652670 loss_rnnt 12.738017 hw_loss 0.489339 lr 0.00043710 rank 7
2023-02-23 06:52:29,978 DEBUG TRAIN Batch 15/5700 loss 8.982224 loss_att 9.084788 loss_ctc 10.429645 loss_rnnt 8.526575 hw_loss 0.454022 lr 0.00043716 rank 0
2023-02-23 06:52:29,980 DEBUG TRAIN Batch 15/5700 loss 12.534035 loss_att 13.376418 loss_ctc 20.232597 loss_rnnt 11.196918 hw_loss 0.266559 lr 0.00043715 rank 3
2023-02-23 06:52:29,981 DEBUG TRAIN Batch 15/5700 loss 22.317497 loss_att 24.614483 loss_ctc 28.948311 loss_rnnt 20.729498 hw_loss 0.458425 lr 0.00043718 rank 6
2023-02-23 06:52:29,983 DEBUG TRAIN Batch 15/5700 loss 8.065962 loss_att 9.590981 loss_ctc 10.689616 loss_rnnt 7.087026 hw_loss 0.607708 lr 0.00043720 rank 1
2023-02-23 06:52:30,033 DEBUG TRAIN Batch 15/5700 loss 12.158882 loss_att 11.391835 loss_ctc 15.301825 loss_rnnt 11.603836 hw_loss 0.542616 lr 0.00043709 rank 5
2023-02-23 06:53:41,840 DEBUG TRAIN Batch 15/5800 loss 7.422718 loss_att 9.959654 loss_ctc 11.102233 loss_rnnt 6.179383 hw_loss 0.460021 lr 0.00043704 rank 1
2023-02-23 06:53:41,855 DEBUG TRAIN Batch 15/5800 loss 8.155897 loss_att 7.996423 loss_ctc 11.053630 loss_rnnt 7.530279 hw_loss 0.508403 lr 0.00043693 rank 7
2023-02-23 06:53:41,857 DEBUG TRAIN Batch 15/5800 loss 6.247658 loss_att 7.586499 loss_ctc 8.162770 loss_rnnt 5.366542 hw_loss 0.671248 lr 0.00043698 rank 2
2023-02-23 06:53:41,857 DEBUG TRAIN Batch 15/5800 loss 15.013315 loss_att 19.178358 loss_ctc 22.081741 loss_rnnt 13.015150 hw_loss 0.417564 lr 0.00043698 rank 3
2023-02-23 06:53:41,861 DEBUG TRAIN Batch 15/5800 loss 29.593477 loss_att 28.737156 loss_ctc 37.556183 loss_rnnt 28.475876 hw_loss 0.425946 lr 0.00043700 rank 0
2023-02-23 06:53:41,861 DEBUG TRAIN Batch 15/5800 loss 2.562100 loss_att 5.975720 loss_ctc 3.317506 loss_rnnt 1.565347 hw_loss 0.399951 lr 0.00043701 rank 6
2023-02-23 06:53:41,869 DEBUG TRAIN Batch 15/5800 loss 7.084913 loss_att 12.172716 loss_ctc 14.026754 loss_rnnt 4.937666 hw_loss 0.382700 lr 0.00043693 rank 5
2023-02-23 06:53:41,909 DEBUG TRAIN Batch 15/5800 loss 3.623140 loss_att 6.678683 loss_ctc 4.160885 loss_rnnt 2.764628 hw_loss 0.329445 lr 0.00043700 rank 4
2023-02-23 06:54:54,711 DEBUG TRAIN Batch 15/5900 loss 2.213138 loss_att 5.614205 loss_ctc 4.018150 loss_rnnt 1.085703 hw_loss 0.387288 lr 0.00043676 rank 7
2023-02-23 06:54:54,715 DEBUG TRAIN Batch 15/5900 loss 7.704210 loss_att 12.269013 loss_ctc 11.373752 loss_rnnt 6.075496 hw_loss 0.424653 lr 0.00043683 rank 0
2023-02-23 06:54:54,717 DEBUG TRAIN Batch 15/5900 loss 18.021353 loss_att 24.060219 loss_ctc 33.105927 loss_rnnt 14.607759 hw_loss 0.364768 lr 0.00043687 rank 1
2023-02-23 06:54:54,718 DEBUG TRAIN Batch 15/5900 loss 11.768208 loss_att 13.568909 loss_ctc 15.653491 loss_rnnt 10.718351 hw_loss 0.321896 lr 0.00043681 rank 2
2023-02-23 06:54:54,719 DEBUG TRAIN Batch 15/5900 loss 8.858795 loss_att 10.358479 loss_ctc 11.447467 loss_rnnt 8.018241 hw_loss 0.366490 lr 0.00043676 rank 5
2023-02-23 06:54:54,720 DEBUG TRAIN Batch 15/5900 loss 9.733347 loss_att 11.320068 loss_ctc 12.853817 loss_rnnt 8.830545 hw_loss 0.317613 lr 0.00043681 rank 3
2023-02-23 06:54:54,721 DEBUG TRAIN Batch 15/5900 loss 11.484197 loss_att 13.346727 loss_ctc 12.166735 loss_rnnt 10.857889 hw_loss 0.305239 lr 0.00043685 rank 6
2023-02-23 06:54:54,768 DEBUG TRAIN Batch 15/5900 loss 6.331222 loss_att 10.646523 loss_ctc 6.647678 loss_rnnt 5.218565 hw_loss 0.388880 lr 0.00043683 rank 4
2023-02-23 06:56:08,698 DEBUG TRAIN Batch 15/6000 loss 13.821130 loss_att 16.465168 loss_ctc 14.018580 loss_rnnt 13.027739 hw_loss 0.446731 lr 0.00043666 rank 0
2023-02-23 06:56:08,714 DEBUG TRAIN Batch 15/6000 loss 11.650487 loss_att 14.462053 loss_ctc 14.702081 loss_rnnt 10.485049 hw_loss 0.367960 lr 0.00043667 rank 4
2023-02-23 06:56:08,714 DEBUG TRAIN Batch 15/6000 loss 13.694645 loss_att 17.878920 loss_ctc 18.343491 loss_rnnt 12.081347 hw_loss 0.293619 lr 0.00043668 rank 6
2023-02-23 06:56:08,714 DEBUG TRAIN Batch 15/6000 loss 9.132240 loss_att 12.008107 loss_ctc 11.009396 loss_rnnt 8.048842 hw_loss 0.483633 lr 0.00043664 rank 2
2023-02-23 06:56:08,716 DEBUG TRAIN Batch 15/6000 loss 9.011436 loss_att 12.351226 loss_ctc 12.316132 loss_rnnt 7.687431 hw_loss 0.403915 lr 0.00043670 rank 1
2023-02-23 06:56:08,717 DEBUG TRAIN Batch 15/6000 loss 5.363002 loss_att 8.369682 loss_ctc 7.320299 loss_rnnt 4.261737 hw_loss 0.448043 lr 0.00043665 rank 3
2023-02-23 06:56:08,717 DEBUG TRAIN Batch 15/6000 loss 11.268729 loss_att 12.591562 loss_ctc 13.611856 loss_rnnt 10.502659 hw_loss 0.354536 lr 0.00043659 rank 5
2023-02-23 06:56:08,722 DEBUG TRAIN Batch 15/6000 loss 10.003742 loss_att 11.352180 loss_ctc 12.411306 loss_rnnt 9.202309 hw_loss 0.395131 lr 0.00043660 rank 7
2023-02-23 06:57:22,666 DEBUG TRAIN Batch 15/6100 loss 17.556734 loss_att 22.498940 loss_ctc 22.899433 loss_rnnt 15.620385 hw_loss 0.441653 lr 0.00043643 rank 7
2023-02-23 06:57:22,666 DEBUG TRAIN Batch 15/6100 loss 17.230553 loss_att 20.246801 loss_ctc 20.379808 loss_rnnt 15.973006 hw_loss 0.439490 lr 0.00043650 rank 0
2023-02-23 06:57:22,668 DEBUG TRAIN Batch 15/6100 loss 11.159490 loss_att 13.779165 loss_ctc 15.071838 loss_rnnt 9.902487 hw_loss 0.396416 lr 0.00043654 rank 1
2023-02-23 06:57:22,667 DEBUG TRAIN Batch 15/6100 loss 16.268492 loss_att 16.779970 loss_ctc 16.705822 loss_rnnt 15.927782 hw_loss 0.337691 lr 0.00043648 rank 3
2023-02-23 06:57:22,668 DEBUG TRAIN Batch 15/6100 loss 18.934063 loss_att 24.799225 loss_ctc 22.714357 loss_rnnt 17.053881 hw_loss 0.380833 lr 0.00043648 rank 2
2023-02-23 06:57:22,668 DEBUG TRAIN Batch 15/6100 loss 11.257850 loss_att 12.102424 loss_ctc 13.057139 loss_rnnt 10.671545 hw_loss 0.332784 lr 0.00043650 rank 4
2023-02-23 06:57:22,673 DEBUG TRAIN Batch 15/6100 loss 10.150215 loss_att 12.192958 loss_ctc 13.580638 loss_rnnt 9.076481 hw_loss 0.389619 lr 0.00043643 rank 5
2023-02-23 06:57:22,718 DEBUG TRAIN Batch 15/6100 loss 7.784167 loss_att 11.779037 loss_ctc 10.101303 loss_rnnt 6.492471 hw_loss 0.344568 lr 0.00043651 rank 6
2023-02-23 06:58:35,080 DEBUG TRAIN Batch 15/6200 loss 9.236375 loss_att 15.676370 loss_ctc 11.375595 loss_rnnt 7.454175 hw_loss 0.391821 lr 0.00043626 rank 7
2023-02-23 06:58:35,088 DEBUG TRAIN Batch 15/6200 loss 21.839859 loss_att 23.879715 loss_ctc 30.983326 loss_rnnt 20.015852 hw_loss 0.369201 lr 0.00043633 rank 4
2023-02-23 06:58:35,090 DEBUG TRAIN Batch 15/6200 loss 14.348039 loss_att 14.672560 loss_ctc 19.429564 loss_rnnt 13.390009 hw_loss 0.404228 lr 0.00043631 rank 3
2023-02-23 06:58:35,091 DEBUG TRAIN Batch 15/6200 loss 10.102098 loss_att 13.337518 loss_ctc 12.185025 loss_rnnt 8.955679 hw_loss 0.415523 lr 0.00043631 rank 2
2023-02-23 06:58:35,093 DEBUG TRAIN Batch 15/6200 loss 4.708228 loss_att 8.400772 loss_ctc 8.772358 loss_rnnt 3.219702 hw_loss 0.390247 lr 0.00043637 rank 1
2023-02-23 06:58:35,094 DEBUG TRAIN Batch 15/6200 loss 9.347363 loss_att 12.102445 loss_ctc 12.639145 loss_rnnt 8.139452 hw_loss 0.408731 lr 0.00043626 rank 5
2023-02-23 06:58:35,094 DEBUG TRAIN Batch 15/6200 loss 3.967291 loss_att 4.277653 loss_ctc 3.371216 loss_rnnt 3.723529 hw_loss 0.489687 lr 0.00043633 rank 0
2023-02-23 06:58:35,095 DEBUG TRAIN Batch 15/6200 loss 20.751728 loss_att 23.291798 loss_ctc 26.943447 loss_rnnt 19.179230 hw_loss 0.447974 lr 0.00043635 rank 6
2023-02-23 06:59:47,503 DEBUG TRAIN Batch 15/6300 loss 8.903625 loss_att 12.385537 loss_ctc 11.987537 loss_rnnt 7.538639 hw_loss 0.482653 lr 0.00043610 rank 7
2023-02-23 06:59:47,511 DEBUG TRAIN Batch 15/6300 loss 7.317372 loss_att 10.016047 loss_ctc 12.886617 loss_rnnt 5.794768 hw_loss 0.450567 lr 0.00043617 rank 0
2023-02-23 06:59:47,510 DEBUG TRAIN Batch 15/6300 loss 10.098480 loss_att 9.418851 loss_ctc 11.822768 loss_rnnt 9.656205 hw_loss 0.653054 lr 0.00043615 rank 3
2023-02-23 06:59:47,512 DEBUG TRAIN Batch 15/6300 loss 12.102564 loss_att 12.652387 loss_ctc 18.809031 loss_rnnt 10.860716 hw_loss 0.445666 lr 0.00043620 rank 1
2023-02-23 06:59:47,512 DEBUG TRAIN Batch 15/6300 loss 11.632891 loss_att 12.027663 loss_ctc 13.453408 loss_rnnt 11.058946 hw_loss 0.472978 lr 0.00043610 rank 5
2023-02-23 06:59:47,515 DEBUG TRAIN Batch 15/6300 loss 10.520403 loss_att 11.887424 loss_ctc 14.203731 loss_rnnt 9.490028 hw_loss 0.498487 lr 0.00043617 rank 4
2023-02-23 06:59:47,519 DEBUG TRAIN Batch 15/6300 loss 15.018712 loss_att 16.566582 loss_ctc 17.979868 loss_rnnt 14.100586 hw_loss 0.400746 lr 0.00043614 rank 2
2023-02-23 06:59:47,538 DEBUG TRAIN Batch 15/6300 loss 11.422779 loss_att 13.789400 loss_ctc 14.916651 loss_rnnt 10.219731 hw_loss 0.494764 lr 0.00043618 rank 6
2023-02-23 07:01:03,319 DEBUG TRAIN Batch 15/6400 loss 11.727106 loss_att 11.996177 loss_ctc 14.500917 loss_rnnt 11.018524 hw_loss 0.534238 lr 0.00043598 rank 2
2023-02-23 07:01:03,320 DEBUG TRAIN Batch 15/6400 loss 12.655939 loss_att 16.021452 loss_ctc 16.616827 loss_rnnt 11.230201 hw_loss 0.420970 lr 0.00043601 rank 6
2023-02-23 07:01:03,321 DEBUG TRAIN Batch 15/6400 loss 20.724735 loss_att 30.157320 loss_ctc 30.532820 loss_rnnt 17.315979 hw_loss 0.402178 lr 0.00043598 rank 3
2023-02-23 07:01:03,322 DEBUG TRAIN Batch 15/6400 loss 9.960065 loss_att 9.717909 loss_ctc 12.091886 loss_rnnt 9.340231 hw_loss 0.720040 lr 0.00043593 rank 7
2023-02-23 07:01:03,324 DEBUG TRAIN Batch 15/6400 loss 16.219202 loss_att 18.664488 loss_ctc 20.050159 loss_rnnt 14.997749 hw_loss 0.415503 lr 0.00043593 rank 5
2023-02-23 07:01:03,330 DEBUG TRAIN Batch 15/6400 loss 12.806968 loss_att 15.051658 loss_ctc 17.375074 loss_rnnt 11.457778 hw_loss 0.545945 lr 0.00043600 rank 0
2023-02-23 07:01:03,334 DEBUG TRAIN Batch 15/6400 loss 14.010441 loss_att 17.628431 loss_ctc 21.602642 loss_rnnt 12.093727 hw_loss 0.339040 lr 0.00043604 rank 1
2023-02-23 07:01:03,391 DEBUG TRAIN Batch 15/6400 loss 5.288033 loss_att 8.572988 loss_ctc 5.426505 loss_rnnt 4.364851 hw_loss 0.464488 lr 0.00043600 rank 4
2023-02-23 07:02:16,094 DEBUG TRAIN Batch 15/6500 loss 12.859323 loss_att 14.333628 loss_ctc 19.241077 loss_rnnt 11.407963 hw_loss 0.572995 lr 0.00043582 rank 3
2023-02-23 07:02:16,098 DEBUG TRAIN Batch 15/6500 loss 10.674504 loss_att 16.096451 loss_ctc 16.139980 loss_rnnt 8.618769 hw_loss 0.454906 lr 0.00043577 rank 7
2023-02-23 07:02:16,115 DEBUG TRAIN Batch 15/6500 loss 10.124730 loss_att 12.441493 loss_ctc 12.184050 loss_rnnt 9.184639 hw_loss 0.379055 lr 0.00043584 rank 4
2023-02-23 07:02:16,116 DEBUG TRAIN Batch 15/6500 loss 10.178171 loss_att 12.653437 loss_ctc 12.280428 loss_rnnt 9.220697 hw_loss 0.341473 lr 0.00043576 rank 5
2023-02-23 07:02:16,117 DEBUG TRAIN Batch 15/6500 loss 10.059036 loss_att 13.482506 loss_ctc 11.779074 loss_rnnt 8.963151 hw_loss 0.340971 lr 0.00043587 rank 1
2023-02-23 07:02:16,123 DEBUG TRAIN Batch 15/6500 loss 2.038056 loss_att 5.470429 loss_ctc 2.641773 loss_rnnt 1.032489 hw_loss 0.447369 lr 0.00043583 rank 0
2023-02-23 07:02:16,123 DEBUG TRAIN Batch 15/6500 loss 15.983788 loss_att 18.646309 loss_ctc 22.838242 loss_rnnt 14.371910 hw_loss 0.310214 lr 0.00043581 rank 2
2023-02-23 07:02:16,161 DEBUG TRAIN Batch 15/6500 loss 9.555492 loss_att 13.946673 loss_ctc 16.423645 loss_rnnt 7.550910 hw_loss 0.394861 lr 0.00043585 rank 6
2023-02-23 07:03:28,519 DEBUG TRAIN Batch 15/6600 loss 21.451660 loss_att 23.719227 loss_ctc 27.292721 loss_rnnt 20.038132 hw_loss 0.339759 lr 0.00043565 rank 3
2023-02-23 07:03:28,524 DEBUG TRAIN Batch 15/6600 loss 10.965417 loss_att 16.175449 loss_ctc 17.125296 loss_rnnt 8.794462 hw_loss 0.576809 lr 0.00043560 rank 7
2023-02-23 07:03:28,527 DEBUG TRAIN Batch 15/6600 loss 4.009355 loss_att 7.148142 loss_ctc 4.620404 loss_rnnt 3.057853 hw_loss 0.454259 lr 0.00043571 rank 1
2023-02-23 07:03:28,532 DEBUG TRAIN Batch 15/6600 loss 13.850457 loss_att 18.546101 loss_ctc 18.272163 loss_rnnt 12.148781 hw_loss 0.324351 lr 0.00043567 rank 0
2023-02-23 07:03:28,533 DEBUG TRAIN Batch 15/6600 loss 17.454351 loss_att 21.044121 loss_ctc 24.872869 loss_rnnt 15.536674 hw_loss 0.394853 lr 0.00043567 rank 4
2023-02-23 07:03:28,536 DEBUG TRAIN Batch 15/6600 loss 13.731279 loss_att 20.100861 loss_ctc 18.379364 loss_rnnt 11.666433 hw_loss 0.320974 lr 0.00043565 rank 2
2023-02-23 07:03:28,536 DEBUG TRAIN Batch 15/6600 loss 11.685028 loss_att 11.814398 loss_ctc 13.061619 loss_rnnt 11.222156 hw_loss 0.475225 lr 0.00043568 rank 6
2023-02-23 07:03:28,536 DEBUG TRAIN Batch 15/6600 loss 12.317018 loss_att 15.060209 loss_ctc 15.145048 loss_rnnt 11.184878 hw_loss 0.387058 lr 0.00043560 rank 5
2023-02-23 07:04:42,065 DEBUG TRAIN Batch 15/6700 loss 11.158287 loss_att 12.888639 loss_ctc 14.897422 loss_rnnt 10.080789 hw_loss 0.436642 lr 0.00043544 rank 7
2023-02-23 07:04:42,066 DEBUG TRAIN Batch 15/6700 loss 11.742874 loss_att 15.699432 loss_ctc 12.610050 loss_rnnt 10.624441 hw_loss 0.396559 lr 0.00043554 rank 1
2023-02-23 07:04:42,068 DEBUG TRAIN Batch 15/6700 loss 9.861368 loss_att 11.891370 loss_ctc 10.260241 loss_rnnt 9.200747 hw_loss 0.377696 lr 0.00043548 rank 2
2023-02-23 07:04:42,068 DEBUG TRAIN Batch 15/6700 loss 8.790211 loss_att 10.058366 loss_ctc 9.997988 loss_rnnt 8.090195 hw_loss 0.535027 lr 0.00043550 rank 0
2023-02-23 07:04:42,068 DEBUG TRAIN Batch 15/6700 loss 12.039149 loss_att 16.515125 loss_ctc 16.873316 loss_rnnt 10.287872 hw_loss 0.396612 lr 0.00043552 rank 6
2023-02-23 07:04:42,072 DEBUG TRAIN Batch 15/6700 loss 7.825342 loss_att 13.225438 loss_ctc 12.440578 loss_rnnt 5.947459 hw_loss 0.342185 lr 0.00043550 rank 4
2023-02-23 07:04:42,073 DEBUG TRAIN Batch 15/6700 loss 11.507679 loss_att 14.514129 loss_ctc 17.424459 loss_rnnt 9.898733 hw_loss 0.410158 lr 0.00043543 rank 5
2023-02-23 07:04:42,076 DEBUG TRAIN Batch 15/6700 loss 17.291227 loss_att 21.566759 loss_ctc 23.514191 loss_rnnt 15.415581 hw_loss 0.357771 lr 0.00043548 rank 3
2023-02-23 07:05:56,379 DEBUG TRAIN Batch 15/6800 loss 10.964069 loss_att 12.937107 loss_ctc 16.045042 loss_rnnt 9.703426 hw_loss 0.353573 lr 0.00043527 rank 7
2023-02-23 07:05:56,386 DEBUG TRAIN Batch 15/6800 loss 10.658391 loss_att 10.857059 loss_ctc 17.113842 loss_rnnt 9.490294 hw_loss 0.501818 lr 0.00043527 rank 5
2023-02-23 07:05:56,385 DEBUG TRAIN Batch 15/6800 loss 9.401136 loss_att 10.596649 loss_ctc 8.841835 loss_rnnt 9.015895 hw_loss 0.413837 lr 0.00043535 rank 6
2023-02-23 07:05:56,387 DEBUG TRAIN Batch 15/6800 loss 11.533407 loss_att 13.933922 loss_ctc 12.099672 loss_rnnt 10.744102 hw_loss 0.438185 lr 0.00043538 rank 1
2023-02-23 07:05:56,388 DEBUG TRAIN Batch 15/6800 loss 6.652678 loss_att 10.448593 loss_ctc 11.292570 loss_rnnt 5.066489 hw_loss 0.390665 lr 0.00043532 rank 3
2023-02-23 07:05:56,390 DEBUG TRAIN Batch 15/6800 loss 8.544433 loss_att 11.242495 loss_ctc 15.498701 loss_rnnt 6.865086 hw_loss 0.398435 lr 0.00043534 rank 4
2023-02-23 07:05:56,390 DEBUG TRAIN Batch 15/6800 loss 16.403843 loss_att 17.509304 loss_ctc 22.350546 loss_rnnt 15.146955 hw_loss 0.455442 lr 0.00043534 rank 0
2023-02-23 07:05:56,392 DEBUG TRAIN Batch 15/6800 loss 4.487574 loss_att 7.600835 loss_ctc 6.030050 loss_rnnt 3.459752 hw_loss 0.374074 lr 0.00043532 rank 2
2023-02-23 07:07:09,001 DEBUG TRAIN Batch 15/6900 loss 5.004619 loss_att 7.507793 loss_ctc 6.702464 loss_rnnt 4.064449 hw_loss 0.399667 lr 0.00043511 rank 7
2023-02-23 07:07:09,006 DEBUG TRAIN Batch 15/6900 loss 17.164780 loss_att 21.162952 loss_ctc 21.358976 loss_rnnt 15.552617 hw_loss 0.474942 lr 0.00043515 rank 2
2023-02-23 07:07:09,006 DEBUG TRAIN Batch 15/6900 loss 7.982481 loss_att 11.614187 loss_ctc 16.310257 loss_rnnt 5.929652 hw_loss 0.405220 lr 0.00043521 rank 1
2023-02-23 07:07:09,007 DEBUG TRAIN Batch 15/6900 loss 9.881906 loss_att 12.524980 loss_ctc 13.559234 loss_rnnt 8.629540 hw_loss 0.437698 lr 0.00043519 rank 6
2023-02-23 07:07:09,007 DEBUG TRAIN Batch 15/6900 loss 8.892794 loss_att 9.783823 loss_ctc 11.688700 loss_rnnt 8.061925 hw_loss 0.524767 lr 0.00043515 rank 3
2023-02-23 07:07:09,009 DEBUG TRAIN Batch 15/6900 loss 6.114126 loss_att 8.962519 loss_ctc 9.579134 loss_rnnt 4.847363 hw_loss 0.440781 lr 0.00043517 rank 0
2023-02-23 07:07:09,009 DEBUG TRAIN Batch 15/6900 loss 17.124500 loss_att 18.888622 loss_ctc 17.136974 loss_rnnt 16.580320 hw_loss 0.355674 lr 0.00043517 rank 4
2023-02-23 07:07:09,013 DEBUG TRAIN Batch 15/6900 loss 9.774651 loss_att 13.057283 loss_ctc 12.723533 loss_rnnt 8.510137 hw_loss 0.402755 lr 0.00043510 rank 5
2023-02-23 07:08:20,988 DEBUG TRAIN Batch 15/7000 loss 12.366317 loss_att 10.995983 loss_ctc 16.652515 loss_rnnt 11.885338 hw_loss 0.344160 lr 0.00043499 rank 3
2023-02-23 07:08:21,005 DEBUG TRAIN Batch 15/7000 loss 14.054701 loss_att 14.619547 loss_ctc 16.781925 loss_rnnt 13.233493 hw_loss 0.646144 lr 0.00043494 rank 7
2023-02-23 07:08:21,007 DEBUG TRAIN Batch 15/7000 loss 8.905143 loss_att 11.027100 loss_ctc 13.944379 loss_rnnt 7.551558 hw_loss 0.482428 lr 0.00043505 rank 1
2023-02-23 07:08:21,006 DEBUG TRAIN Batch 15/7000 loss 12.947552 loss_att 14.339329 loss_ctc 17.391819 loss_rnnt 11.848625 hw_loss 0.427504 lr 0.00043499 rank 2
2023-02-23 07:08:21,007 DEBUG TRAIN Batch 15/7000 loss 16.252934 loss_att 19.607950 loss_ctc 22.213522 loss_rnnt 14.504683 hw_loss 0.529691 lr 0.00043501 rank 0
2023-02-23 07:08:21,007 DEBUG TRAIN Batch 15/7000 loss 16.744413 loss_att 18.640253 loss_ctc 20.770813 loss_rnnt 15.634483 hw_loss 0.363580 lr 0.00043502 rank 6
2023-02-23 07:08:21,012 DEBUG TRAIN Batch 15/7000 loss 12.207346 loss_att 11.899256 loss_ctc 12.781560 loss_rnnt 11.913403 hw_loss 0.523123 lr 0.00043501 rank 4
2023-02-23 07:08:21,020 DEBUG TRAIN Batch 15/7000 loss 10.623238 loss_att 10.790461 loss_ctc 12.685686 loss_rnnt 10.016193 hw_loss 0.559885 lr 0.00043494 rank 5
2023-02-23 07:09:35,899 DEBUG TRAIN Batch 15/7100 loss 7.468787 loss_att 9.677835 loss_ctc 8.639479 loss_rnnt 6.634665 hw_loss 0.442912 lr 0.00043486 rank 6
2023-02-23 07:09:35,901 DEBUG TRAIN Batch 15/7100 loss 15.738563 loss_att 22.235531 loss_ctc 23.084538 loss_rnnt 13.231859 hw_loss 0.427211 lr 0.00043478 rank 7
2023-02-23 07:09:35,903 DEBUG TRAIN Batch 15/7100 loss 10.321506 loss_att 15.521007 loss_ctc 18.351429 loss_rnnt 8.019686 hw_loss 0.358617 lr 0.00043488 rank 1
2023-02-23 07:09:35,906 DEBUG TRAIN Batch 15/7100 loss 11.651968 loss_att 12.658335 loss_ctc 18.205887 loss_rnnt 10.369526 hw_loss 0.388709 lr 0.00043485 rank 4
2023-02-23 07:09:35,908 DEBUG TRAIN Batch 15/7100 loss 9.229329 loss_att 10.061687 loss_ctc 16.367563 loss_rnnt 7.925021 hw_loss 0.348885 lr 0.00043483 rank 3
2023-02-23 07:09:35,910 DEBUG TRAIN Batch 15/7100 loss 11.013618 loss_att 11.390671 loss_ctc 12.055093 loss_rnnt 10.459832 hw_loss 0.636584 lr 0.00043482 rank 2
2023-02-23 07:09:35,935 DEBUG TRAIN Batch 15/7100 loss 12.129547 loss_att 16.035091 loss_ctc 11.952589 loss_rnnt 11.158415 hw_loss 0.400534 lr 0.00043484 rank 0
2023-02-23 07:09:35,952 DEBUG TRAIN Batch 15/7100 loss 3.305906 loss_att 7.044379 loss_ctc 6.571685 loss_rnnt 1.940841 hw_loss 0.341124 lr 0.00043477 rank 5
2023-02-23 07:10:48,125 DEBUG TRAIN Batch 15/7200 loss 4.707973 loss_att 7.828074 loss_ctc 6.026916 loss_rnnt 3.668609 hw_loss 0.449034 lr 0.00043461 rank 7
2023-02-23 07:10:48,129 DEBUG TRAIN Batch 15/7200 loss 13.358524 loss_att 17.550215 loss_ctc 21.164717 loss_rnnt 11.263030 hw_loss 0.405621 lr 0.00043468 rank 0
2023-02-23 07:10:48,132 DEBUG TRAIN Batch 15/7200 loss 7.813949 loss_att 9.201702 loss_ctc 9.079433 loss_rnnt 7.106450 hw_loss 0.489782 lr 0.00043469 rank 6
2023-02-23 07:10:48,132 DEBUG TRAIN Batch 15/7200 loss 10.803115 loss_att 13.605846 loss_ctc 17.509893 loss_rnnt 9.100401 hw_loss 0.464869 lr 0.00043472 rank 1
2023-02-23 07:10:48,133 DEBUG TRAIN Batch 15/7200 loss 28.885872 loss_att 26.597872 loss_ctc 42.771233 loss_rnnt 27.285849 hw_loss 0.386706 lr 0.00043466 rank 2
2023-02-23 07:10:48,134 DEBUG TRAIN Batch 15/7200 loss 11.715547 loss_att 13.852318 loss_ctc 13.843966 loss_rnnt 10.807269 hw_loss 0.369628 lr 0.00043461 rank 5
2023-02-23 07:10:48,136 DEBUG TRAIN Batch 15/7200 loss 8.684457 loss_att 14.361570 loss_ctc 13.756588 loss_rnnt 6.692647 hw_loss 0.337690 lr 0.00043466 rank 3
2023-02-23 07:10:48,180 DEBUG TRAIN Batch 15/7200 loss 7.765670 loss_att 10.423880 loss_ctc 10.654040 loss_rnnt 6.653537 hw_loss 0.366330 lr 0.00043468 rank 4
2023-02-23 07:12:01,041 DEBUG TRAIN Batch 15/7300 loss 7.761066 loss_att 11.852155 loss_ctc 8.042174 loss_rnnt 6.698239 hw_loss 0.388365 lr 0.00043445 rank 5
2023-02-23 07:12:01,041 DEBUG TRAIN Batch 15/7300 loss 13.464027 loss_att 18.440826 loss_ctc 17.455273 loss_rnnt 11.761304 hw_loss 0.328495 lr 0.00043445 rank 7
2023-02-23 07:12:01,042 DEBUG TRAIN Batch 15/7300 loss 12.152219 loss_att 15.576715 loss_ctc 14.677085 loss_rnnt 10.914009 hw_loss 0.406240 lr 0.00043449 rank 2
2023-02-23 07:12:01,044 DEBUG TRAIN Batch 15/7300 loss 13.617920 loss_att 20.261183 loss_ctc 18.150507 loss_rnnt 11.515604 hw_loss 0.317470 lr 0.00043452 rank 4
2023-02-23 07:12:01,047 DEBUG TRAIN Batch 15/7300 loss 10.693048 loss_att 13.645561 loss_ctc 12.017302 loss_rnnt 9.677557 hw_loss 0.465789 lr 0.00043450 rank 3
2023-02-23 07:12:01,047 DEBUG TRAIN Batch 15/7300 loss 7.085058 loss_att 9.315470 loss_ctc 12.156491 loss_rnnt 5.732450 hw_loss 0.431879 lr 0.00043455 rank 1
2023-02-23 07:12:01,049 DEBUG TRAIN Batch 15/7300 loss 13.285734 loss_att 15.854434 loss_ctc 19.797569 loss_rnnt 11.717110 hw_loss 0.349949 lr 0.00043451 rank 0
2023-02-23 07:12:01,091 DEBUG TRAIN Batch 15/7300 loss 11.938783 loss_att 15.821021 loss_ctc 15.124604 loss_rnnt 10.504090 hw_loss 0.437754 lr 0.00043453 rank 6
2023-02-23 07:13:14,039 DEBUG TRAIN Batch 15/7400 loss 13.338933 loss_att 14.173054 loss_ctc 15.536240 loss_rnnt 12.649791 hw_loss 0.430020 lr 0.00043428 rank 5
2023-02-23 07:13:14,045 DEBUG TRAIN Batch 15/7400 loss 18.345615 loss_att 21.221764 loss_ctc 21.578157 loss_rnnt 17.143187 hw_loss 0.367861 lr 0.00043433 rank 2
2023-02-23 07:13:14,051 DEBUG TRAIN Batch 15/7400 loss 13.241398 loss_att 14.762587 loss_ctc 18.719666 loss_rnnt 12.018883 hw_loss 0.352204 lr 0.00043428 rank 7
2023-02-23 07:13:14,052 DEBUG TRAIN Batch 15/7400 loss 14.421847 loss_att 17.566101 loss_ctc 16.388063 loss_rnnt 13.291676 hw_loss 0.448422 lr 0.00043437 rank 6
2023-02-23 07:13:14,056 DEBUG TRAIN Batch 15/7400 loss 12.023882 loss_att 14.649055 loss_ctc 15.344456 loss_rnnt 10.819017 hw_loss 0.444537 lr 0.00043439 rank 1
2023-02-23 07:13:14,057 DEBUG TRAIN Batch 15/7400 loss 13.733409 loss_att 16.402382 loss_ctc 14.013383 loss_rnnt 12.917410 hw_loss 0.459139 lr 0.00043435 rank 4
2023-02-23 07:13:14,058 DEBUG TRAIN Batch 15/7400 loss 8.811234 loss_att 12.335844 loss_ctc 10.400373 loss_rnnt 7.674358 hw_loss 0.412628 lr 0.00043433 rank 3
2023-02-23 07:13:14,105 DEBUG TRAIN Batch 15/7400 loss 20.049971 loss_att 24.276209 loss_ctc 28.834332 loss_rnnt 17.784899 hw_loss 0.466079 lr 0.00043435 rank 0
2023-02-23 07:14:28,983 DEBUG TRAIN Batch 15/7500 loss 17.436592 loss_att 22.357410 loss_ctc 21.077431 loss_rnnt 15.752598 hw_loss 0.401972 lr 0.00043419 rank 4
2023-02-23 07:14:28,994 DEBUG TRAIN Batch 15/7500 loss 10.537249 loss_att 15.117441 loss_ctc 14.879048 loss_rnnt 8.804562 hw_loss 0.445766 lr 0.00043423 rank 1
2023-02-23 07:14:29,000 DEBUG TRAIN Batch 15/7500 loss 15.386395 loss_att 15.930269 loss_ctc 19.529322 loss_rnnt 14.491137 hw_loss 0.438921 lr 0.00043417 rank 3
2023-02-23 07:14:29,003 DEBUG TRAIN Batch 15/7500 loss 21.851851 loss_att 18.670891 loss_ctc 26.129383 loss_rnnt 21.730604 hw_loss 0.350815 lr 0.00043412 rank 7
2023-02-23 07:14:29,005 DEBUG TRAIN Batch 15/7500 loss 10.971689 loss_att 14.361497 loss_ctc 14.261669 loss_rnnt 9.659760 hw_loss 0.366196 lr 0.00043417 rank 2
2023-02-23 07:14:29,007 DEBUG TRAIN Batch 15/7500 loss 17.489878 loss_att 19.068399 loss_ctc 27.284939 loss_rnnt 15.627845 hw_loss 0.450604 lr 0.00043420 rank 6
2023-02-23 07:14:29,011 DEBUG TRAIN Batch 15/7500 loss 19.016375 loss_att 25.717087 loss_ctc 25.862904 loss_rnnt 16.525562 hw_loss 0.445870 lr 0.00043419 rank 0
2023-02-23 07:14:29,056 DEBUG TRAIN Batch 15/7500 loss 10.265993 loss_att 12.106311 loss_ctc 10.528765 loss_rnnt 9.663271 hw_loss 0.374294 lr 0.00043412 rank 5
2023-02-23 07:15:42,091 DEBUG TRAIN Batch 15/7600 loss 17.060652 loss_att 18.517139 loss_ctc 21.748158 loss_rnnt 15.876451 hw_loss 0.502315 lr 0.00043400 rank 2
2023-02-23 07:15:42,094 DEBUG TRAIN Batch 15/7600 loss 7.412062 loss_att 9.341737 loss_ctc 10.047271 loss_rnnt 6.401965 hw_loss 0.511502 lr 0.00043403 rank 4
2023-02-23 07:15:42,094 DEBUG TRAIN Batch 15/7600 loss 12.858438 loss_att 13.972866 loss_ctc 13.118863 loss_rnnt 12.321975 hw_loss 0.522850 lr 0.00043396 rank 7
2023-02-23 07:15:42,097 DEBUG TRAIN Batch 15/7600 loss 12.549965 loss_att 16.627342 loss_ctc 14.535968 loss_rnnt 11.227835 hw_loss 0.453477 lr 0.00043402 rank 0
2023-02-23 07:15:42,097 DEBUG TRAIN Batch 15/7600 loss 15.114471 loss_att 15.699281 loss_ctc 17.689224 loss_rnnt 14.327785 hw_loss 0.612044 lr 0.00043404 rank 6
2023-02-23 07:15:42,101 DEBUG TRAIN Batch 15/7600 loss 7.303453 loss_att 14.375268 loss_ctc 8.751269 loss_rnnt 5.475285 hw_loss 0.413933 lr 0.00043401 rank 3
2023-02-23 07:15:42,102 DEBUG TRAIN Batch 15/7600 loss 7.054287 loss_att 9.650678 loss_ctc 8.424913 loss_rnnt 6.060760 hw_loss 0.546563 lr 0.00043395 rank 5
2023-02-23 07:15:42,106 DEBUG TRAIN Batch 15/7600 loss 22.605093 loss_att 23.542992 loss_ctc 26.983234 loss_rnnt 21.607367 hw_loss 0.424490 lr 0.00043406 rank 1
2023-02-23 07:16:54,797 DEBUG TRAIN Batch 15/7700 loss 18.302204 loss_att 25.502544 loss_ctc 19.792028 loss_rnnt 16.439217 hw_loss 0.420518 lr 0.00043384 rank 3
2023-02-23 07:16:54,797 DEBUG TRAIN Batch 15/7700 loss 5.976797 loss_att 11.559331 loss_ctc 9.455675 loss_rnnt 4.218535 hw_loss 0.333570 lr 0.00043379 rank 7
2023-02-23 07:16:54,799 DEBUG TRAIN Batch 15/7700 loss 9.411029 loss_att 12.790429 loss_ctc 18.058401 loss_rnnt 7.309648 hw_loss 0.510971 lr 0.00043379 rank 5
2023-02-23 07:16:54,799 DEBUG TRAIN Batch 15/7700 loss 5.797627 loss_att 9.821587 loss_ctc 8.105984 loss_rnnt 4.463153 hw_loss 0.416064 lr 0.00043387 rank 6
2023-02-23 07:16:54,801 DEBUG TRAIN Batch 15/7700 loss 15.401711 loss_att 15.540314 loss_ctc 19.754608 loss_rnnt 14.510498 hw_loss 0.530824 lr 0.00043384 rank 2
2023-02-23 07:16:54,804 DEBUG TRAIN Batch 15/7700 loss 8.006374 loss_att 13.816607 loss_ctc 14.232450 loss_rnnt 5.799718 hw_loss 0.402123 lr 0.00043390 rank 1
2023-02-23 07:16:54,806 DEBUG TRAIN Batch 15/7700 loss 11.357008 loss_att 15.257635 loss_ctc 13.831767 loss_rnnt 10.045812 hw_loss 0.377068 lr 0.00043386 rank 4
2023-02-23 07:16:54,852 DEBUG TRAIN Batch 15/7700 loss 15.409546 loss_att 17.249294 loss_ctc 21.860653 loss_rnnt 13.954547 hw_loss 0.425438 lr 0.00043386 rank 0
2023-02-23 07:18:09,834 DEBUG TRAIN Batch 15/7800 loss 4.923982 loss_att 8.943792 loss_ctc 6.611006 loss_rnnt 3.703672 hw_loss 0.358896 lr 0.00043371 rank 6
2023-02-23 07:18:09,845 DEBUG TRAIN Batch 15/7800 loss 5.807796 loss_att 10.463725 loss_ctc 8.848822 loss_rnnt 4.302857 hw_loss 0.315529 lr 0.00043363 rank 7
2023-02-23 07:18:09,845 DEBUG TRAIN Batch 15/7800 loss 14.958562 loss_att 16.311178 loss_ctc 21.271267 loss_rnnt 13.671013 hw_loss 0.328748 lr 0.00043370 rank 4
2023-02-23 07:18:09,846 DEBUG TRAIN Batch 15/7800 loss 18.075195 loss_att 19.999144 loss_ctc 21.687904 loss_rnnt 17.041767 hw_loss 0.313020 lr 0.00043368 rank 3
2023-02-23 07:18:09,850 DEBUG TRAIN Batch 15/7800 loss 11.134759 loss_att 14.679304 loss_ctc 15.536613 loss_rnnt 9.634631 hw_loss 0.383071 lr 0.00043370 rank 0
2023-02-23 07:18:09,851 DEBUG TRAIN Batch 15/7800 loss 14.668228 loss_att 17.497353 loss_ctc 21.865383 loss_rnnt 12.908607 hw_loss 0.439078 lr 0.00043374 rank 1
2023-02-23 07:18:09,855 DEBUG TRAIN Batch 15/7800 loss 6.322580 loss_att 8.367643 loss_ctc 6.602794 loss_rnnt 5.596082 hw_loss 0.525233 lr 0.00043368 rank 2
2023-02-23 07:18:09,879 DEBUG TRAIN Batch 15/7800 loss 9.735188 loss_att 14.049937 loss_ctc 16.501915 loss_rnnt 7.834264 hw_loss 0.254519 lr 0.00043363 rank 5
2023-02-23 07:19:23,414 DEBUG TRAIN Batch 15/7900 loss 2.195338 loss_att 5.137498 loss_ctc 3.896748 loss_rnnt 1.173888 hw_loss 0.386556 lr 0.00043347 rank 7
2023-02-23 07:19:23,418 DEBUG TRAIN Batch 15/7900 loss 8.577795 loss_att 12.378432 loss_ctc 10.944658 loss_rnnt 7.291317 hw_loss 0.395191 lr 0.00043352 rank 3
2023-02-23 07:19:23,418 DEBUG TRAIN Batch 15/7900 loss 9.153385 loss_att 13.720673 loss_ctc 15.028458 loss_rnnt 7.237855 hw_loss 0.410117 lr 0.00043351 rank 2
2023-02-23 07:19:23,419 DEBUG TRAIN Batch 15/7900 loss 17.325443 loss_att 21.590252 loss_ctc 20.195223 loss_rnnt 15.856743 hw_loss 0.437065 lr 0.00043357 rank 1
2023-02-23 07:19:23,420 DEBUG TRAIN Batch 15/7900 loss 11.538272 loss_att 14.052348 loss_ctc 14.850121 loss_rnnt 10.369951 hw_loss 0.419859 lr 0.00043354 rank 4
2023-02-23 07:19:23,421 DEBUG TRAIN Batch 15/7900 loss 10.225177 loss_att 12.396736 loss_ctc 11.131066 loss_rnnt 9.448802 hw_loss 0.414895 lr 0.00043355 rank 6
2023-02-23 07:19:23,424 DEBUG TRAIN Batch 15/7900 loss 17.540543 loss_att 19.186993 loss_ctc 21.013294 loss_rnnt 16.583002 hw_loss 0.309782 lr 0.00043353 rank 0
2023-02-23 07:19:23,427 DEBUG TRAIN Batch 15/7900 loss 7.324629 loss_att 10.351750 loss_ctc 9.921309 loss_rnnt 6.161569 hw_loss 0.396397 lr 0.00043347 rank 5
2023-02-23 07:20:35,869 DEBUG TRAIN Batch 15/8000 loss 17.530474 loss_att 19.797436 loss_ctc 26.144156 loss_rnnt 15.712845 hw_loss 0.404519 lr 0.00043335 rank 2
2023-02-23 07:20:35,872 DEBUG TRAIN Batch 15/8000 loss 14.143410 loss_att 17.392433 loss_ctc 17.714970 loss_rnnt 12.770905 hw_loss 0.462176 lr 0.00043335 rank 3
2023-02-23 07:20:35,877 DEBUG TRAIN Batch 15/8000 loss 7.521667 loss_att 12.329692 loss_ctc 11.046829 loss_rnnt 5.867326 hw_loss 0.417590 lr 0.00043339 rank 6
2023-02-23 07:20:35,877 DEBUG TRAIN Batch 15/8000 loss 12.053172 loss_att 14.053482 loss_ctc 12.835772 loss_rnnt 11.335222 hw_loss 0.400389 lr 0.00043330 rank 7
2023-02-23 07:20:35,878 DEBUG TRAIN Batch 15/8000 loss 11.162879 loss_att 12.315150 loss_ctc 16.423763 loss_rnnt 10.018116 hw_loss 0.399107 lr 0.00043330 rank 5
2023-02-23 07:20:35,878 DEBUG TRAIN Batch 15/8000 loss 18.223227 loss_att 19.879562 loss_ctc 25.771656 loss_rnnt 16.657898 hw_loss 0.426756 lr 0.00043337 rank 4
2023-02-23 07:20:35,881 DEBUG TRAIN Batch 15/8000 loss 43.295197 loss_att 47.686150 loss_ctc 61.138515 loss_rnnt 39.860184 hw_loss 0.333213 lr 0.00043337 rank 0
2023-02-23 07:20:35,922 DEBUG TRAIN Batch 15/8000 loss 8.807543 loss_att 10.312820 loss_ctc 8.505587 loss_rnnt 8.330774 hw_loss 0.404949 lr 0.00043341 rank 1
2023-02-23 07:21:48,924 DEBUG TRAIN Batch 15/8100 loss 10.867660 loss_att 12.564798 loss_ctc 13.743591 loss_rnnt 9.836518 hw_loss 0.577980 lr 0.00043319 rank 3
2023-02-23 07:21:48,930 DEBUG TRAIN Batch 15/8100 loss 8.625595 loss_att 9.375740 loss_ctc 9.941220 loss_rnnt 8.043465 hw_loss 0.481284 lr 0.00043321 rank 4
2023-02-23 07:21:48,940 DEBUG TRAIN Batch 15/8100 loss 7.632697 loss_att 9.798223 loss_ctc 11.838021 loss_rnnt 6.447428 hw_loss 0.358975 lr 0.00043325 rank 1
2023-02-23 07:21:48,941 DEBUG TRAIN Batch 15/8100 loss 8.504361 loss_att 12.837793 loss_ctc 11.877251 loss_rnnt 6.901369 hw_loss 0.537351 lr 0.00043322 rank 6
2023-02-23 07:21:48,946 DEBUG TRAIN Batch 15/8100 loss 9.769191 loss_att 14.633705 loss_ctc 14.608459 loss_rnnt 7.940095 hw_loss 0.395543 lr 0.00043321 rank 0
2023-02-23 07:21:48,946 DEBUG TRAIN Batch 15/8100 loss 4.939863 loss_att 8.011293 loss_ctc 7.013951 loss_rnnt 3.855288 hw_loss 0.363270 lr 0.00043314 rank 7
2023-02-23 07:21:48,946 DEBUG TRAIN Batch 15/8100 loss 7.968562 loss_att 11.254471 loss_ctc 11.185409 loss_rnnt 6.675314 hw_loss 0.388411 lr 0.00043319 rank 2
2023-02-23 07:21:48,965 DEBUG TRAIN Batch 15/8100 loss 6.219811 loss_att 7.963855 loss_ctc 6.446608 loss_rnnt 5.615396 hw_loss 0.422562 lr 0.00043314 rank 5
2023-02-23 07:23:02,027 DEBUG TRAIN Batch 15/8200 loss 8.303894 loss_att 11.204428 loss_ctc 13.154007 loss_rnnt 6.830121 hw_loss 0.463094 lr 0.00043298 rank 7
2023-02-23 07:23:02,029 DEBUG TRAIN Batch 15/8200 loss 10.710292 loss_att 9.920014 loss_ctc 12.694987 loss_rnnt 10.361432 hw_loss 0.454292 lr 0.00043305 rank 4
2023-02-23 07:23:02,030 DEBUG TRAIN Batch 15/8200 loss 18.711634 loss_att 20.349497 loss_ctc 24.529633 loss_rnnt 17.401062 hw_loss 0.388624 lr 0.00043298 rank 5
2023-02-23 07:23:02,032 DEBUG TRAIN Batch 15/8200 loss 5.787237 loss_att 7.345794 loss_ctc 8.978572 loss_rnnt 4.805671 hw_loss 0.458144 lr 0.00043306 rank 6
2023-02-23 07:23:02,034 DEBUG TRAIN Batch 15/8200 loss 13.821149 loss_att 14.042182 loss_ctc 17.753809 loss_rnnt 13.038677 hw_loss 0.401083 lr 0.00043302 rank 2
2023-02-23 07:23:02,034 DEBUG TRAIN Batch 15/8200 loss 13.270793 loss_att 15.075123 loss_ctc 14.756561 loss_rnnt 12.536257 hw_loss 0.329187 lr 0.00043303 rank 3
2023-02-23 07:23:02,036 DEBUG TRAIN Batch 15/8200 loss 9.565350 loss_att 9.751997 loss_ctc 11.020448 loss_rnnt 9.079627 hw_loss 0.476962 lr 0.00043308 rank 1
2023-02-23 07:23:02,084 DEBUG TRAIN Batch 15/8200 loss 3.237621 loss_att 6.405697 loss_ctc 4.509785 loss_rnnt 2.217205 hw_loss 0.407212 lr 0.00043305 rank 0
2023-02-23 07:24:14,800 DEBUG TRAIN Batch 15/8300 loss 10.575606 loss_att 15.009747 loss_ctc 12.200857 loss_rnnt 9.323556 hw_loss 0.278478 lr 0.00043287 rank 3
2023-02-23 07:24:14,805 DEBUG TRAIN Batch 15/8300 loss 19.334642 loss_att 24.569822 loss_ctc 23.472025 loss_rnnt 17.537830 hw_loss 0.371484 lr 0.00043286 rank 2
2023-02-23 07:24:14,805 DEBUG TRAIN Batch 15/8300 loss 15.856329 loss_att 18.669926 loss_ctc 20.341225 loss_rnnt 14.477561 hw_loss 0.408868 lr 0.00043290 rank 6
2023-02-23 07:24:14,805 DEBUG TRAIN Batch 15/8300 loss 19.010073 loss_att 20.331005 loss_ctc 26.542067 loss_rnnt 17.526184 hw_loss 0.403941 lr 0.00043288 rank 0
2023-02-23 07:24:14,805 DEBUG TRAIN Batch 15/8300 loss 7.276157 loss_att 9.570992 loss_ctc 8.222430 loss_rnnt 6.449143 hw_loss 0.453518 lr 0.00043282 rank 7
2023-02-23 07:24:14,806 DEBUG TRAIN Batch 15/8300 loss 11.950531 loss_att 14.948045 loss_ctc 18.830597 loss_rnnt 10.222869 hw_loss 0.395285 lr 0.00043289 rank 4
2023-02-23 07:24:14,809 DEBUG TRAIN Batch 15/8300 loss 12.520061 loss_att 15.076053 loss_ctc 18.534966 loss_rnnt 10.995161 hw_loss 0.396967 lr 0.00043292 rank 1
2023-02-23 07:24:14,857 DEBUG TRAIN Batch 15/8300 loss 6.793803 loss_att 11.879194 loss_ctc 10.944115 loss_rnnt 5.047616 hw_loss 0.329500 lr 0.00043282 rank 5
2023-02-23 07:25:02,985 DEBUG CV Batch 15/0 loss 2.711703 loss_att 2.339483 loss_ctc 2.896896 loss_rnnt 2.319291 hw_loss 0.829055 history loss 2.611269 rank 5
2023-02-23 07:25:02,985 DEBUG CV Batch 15/0 loss 2.711703 loss_att 2.339483 loss_ctc 2.896896 loss_rnnt 2.319291 hw_loss 0.829055 history loss 2.611269 rank 0
2023-02-23 07:25:02,987 DEBUG CV Batch 15/0 loss 2.711703 loss_att 2.339483 loss_ctc 2.896896 loss_rnnt 2.319291 hw_loss 0.829055 history loss 2.611269 rank 4
2023-02-23 07:25:02,989 DEBUG CV Batch 15/0 loss 2.711703 loss_att 2.339483 loss_ctc 2.896896 loss_rnnt 2.319291 hw_loss 0.829055 history loss 2.611269 rank 2
2023-02-23 07:25:02,990 DEBUG CV Batch 15/0 loss 2.711703 loss_att 2.339483 loss_ctc 2.896896 loss_rnnt 2.319291 hw_loss 0.829055 history loss 2.611269 rank 6
2023-02-23 07:25:03,002 DEBUG CV Batch 15/0 loss 2.711703 loss_att 2.339483 loss_ctc 2.896896 loss_rnnt 2.319291 hw_loss 0.829055 history loss 2.611269 rank 7
2023-02-23 07:25:03,003 DEBUG CV Batch 15/0 loss 2.711703 loss_att 2.339483 loss_ctc 2.896896 loss_rnnt 2.319291 hw_loss 0.829055 history loss 2.611269 rank 1
2023-02-23 07:25:03,008 DEBUG CV Batch 15/0 loss 2.711703 loss_att 2.339483 loss_ctc 2.896896 loss_rnnt 2.319291 hw_loss 0.829055 history loss 2.611269 rank 3
2023-02-23 07:25:14,554 DEBUG CV Batch 15/100 loss 8.046527 loss_att 8.434338 loss_ctc 9.860474 loss_rnnt 7.418185 hw_loss 0.579225 history loss 4.042462 rank 1
2023-02-23 07:25:14,599 DEBUG CV Batch 15/100 loss 8.046527 loss_att 8.434338 loss_ctc 9.860474 loss_rnnt 7.418185 hw_loss 0.579225 history loss 4.042462 rank 4
2023-02-23 07:25:14,604 DEBUG CV Batch 15/100 loss 8.046527 loss_att 8.434338 loss_ctc 9.860474 loss_rnnt 7.418185 hw_loss 0.579225 history loss 4.042462 rank 7
2023-02-23 07:25:14,610 DEBUG CV Batch 15/100 loss 8.046527 loss_att 8.434338 loss_ctc 9.860474 loss_rnnt 7.418185 hw_loss 0.579225 history loss 4.042462 rank 2
2023-02-23 07:25:14,635 DEBUG CV Batch 15/100 loss 8.046527 loss_att 8.434338 loss_ctc 9.860474 loss_rnnt 7.418185 hw_loss 0.579225 history loss 4.042462 rank 6
2023-02-23 07:25:14,667 DEBUG CV Batch 15/100 loss 8.046527 loss_att 8.434338 loss_ctc 9.860474 loss_rnnt 7.418185 hw_loss 0.579225 history loss 4.042462 rank 5
2023-02-23 07:25:14,698 DEBUG CV Batch 15/100 loss 8.046527 loss_att 8.434338 loss_ctc 9.860474 loss_rnnt 7.418185 hw_loss 0.579225 history loss 4.042462 rank 0
2023-02-23 07:25:14,802 DEBUG CV Batch 15/100 loss 8.046527 loss_att 8.434338 loss_ctc 9.860474 loss_rnnt 7.418185 hw_loss 0.579225 history loss 4.042462 rank 3
2023-02-23 07:25:28,132 DEBUG CV Batch 15/200 loss 9.954921 loss_att 16.420784 loss_ctc 10.014339 loss_rnnt 8.517932 hw_loss 0.254801 history loss 4.622133 rank 1
2023-02-23 07:25:28,192 DEBUG CV Batch 15/200 loss 9.954921 loss_att 16.420784 loss_ctc 10.014339 loss_rnnt 8.517932 hw_loss 0.254801 history loss 4.622133 rank 4
2023-02-23 07:25:28,297 DEBUG CV Batch 15/200 loss 9.954921 loss_att 16.420784 loss_ctc 10.014339 loss_rnnt 8.517932 hw_loss 0.254801 history loss 4.622133 rank 5
2023-02-23 07:25:28,367 DEBUG CV Batch 15/200 loss 9.954921 loss_att 16.420784 loss_ctc 10.014339 loss_rnnt 8.517932 hw_loss 0.254801 history loss 4.622133 rank 2
2023-02-23 07:25:28,384 DEBUG CV Batch 15/200 loss 9.954921 loss_att 16.420784 loss_ctc 10.014339 loss_rnnt 8.517932 hw_loss 0.254801 history loss 4.622133 rank 7
2023-02-23 07:25:28,662 DEBUG CV Batch 15/200 loss 9.954921 loss_att 16.420784 loss_ctc 10.014339 loss_rnnt 8.517932 hw_loss 0.254801 history loss 4.622133 rank 0
2023-02-23 07:25:28,704 DEBUG CV Batch 15/200 loss 9.954921 loss_att 16.420784 loss_ctc 10.014339 loss_rnnt 8.517932 hw_loss 0.254801 history loss 4.622133 rank 6
2023-02-23 07:25:29,102 DEBUG CV Batch 15/200 loss 9.954921 loss_att 16.420784 loss_ctc 10.014339 loss_rnnt 8.517932 hw_loss 0.254801 history loss 4.622133 rank 3
2023-02-23 07:25:40,192 DEBUG CV Batch 15/300 loss 5.792558 loss_att 6.072282 loss_ctc 8.182674 loss_rnnt 5.087623 hw_loss 0.619328 history loss 4.735225 rank 4
2023-02-23 07:25:40,199 DEBUG CV Batch 15/300 loss 5.792558 loss_att 6.072282 loss_ctc 8.182674 loss_rnnt 5.087623 hw_loss 0.619328 history loss 4.735225 rank 1
2023-02-23 07:25:40,687 DEBUG CV Batch 15/300 loss 5.792558 loss_att 6.072282 loss_ctc 8.182674 loss_rnnt 5.087623 hw_loss 0.619328 history loss 4.735225 rank 7
2023-02-23 07:25:40,698 DEBUG CV Batch 15/300 loss 5.792558 loss_att 6.072282 loss_ctc 8.182674 loss_rnnt 5.087623 hw_loss 0.619328 history loss 4.735225 rank 5
2023-02-23 07:25:40,707 DEBUG CV Batch 15/300 loss 5.792558 loss_att 6.072282 loss_ctc 8.182674 loss_rnnt 5.087623 hw_loss 0.619328 history loss 4.735225 rank 6
2023-02-23 07:25:40,748 DEBUG CV Batch 15/300 loss 5.792558 loss_att 6.072282 loss_ctc 8.182674 loss_rnnt 5.087623 hw_loss 0.619328 history loss 4.735225 rank 2
2023-02-23 07:25:40,958 DEBUG CV Batch 15/300 loss 5.792558 loss_att 6.072282 loss_ctc 8.182674 loss_rnnt 5.087623 hw_loss 0.619328 history loss 4.735225 rank 0
2023-02-23 07:25:41,587 DEBUG CV Batch 15/300 loss 5.792558 loss_att 6.072282 loss_ctc 8.182674 loss_rnnt 5.087623 hw_loss 0.619328 history loss 4.735225 rank 3
2023-02-23 07:25:52,263 DEBUG CV Batch 15/400 loss 15.476539 loss_att 57.738602 loss_ctc 11.162171 loss_rnnt 7.456932 hw_loss 0.267081 history loss 5.747441 rank 4
2023-02-23 07:25:52,630 DEBUG CV Batch 15/400 loss 15.476539 loss_att 57.738602 loss_ctc 11.162171 loss_rnnt 7.456932 hw_loss 0.267081 history loss 5.747441 rank 1
2023-02-23 07:25:52,696 DEBUG CV Batch 15/400 loss 15.476539 loss_att 57.738602 loss_ctc 11.162171 loss_rnnt 7.456932 hw_loss 0.267081 history loss 5.747441 rank 6
2023-02-23 07:25:52,903 DEBUG CV Batch 15/400 loss 15.476539 loss_att 57.738602 loss_ctc 11.162171 loss_rnnt 7.456932 hw_loss 0.267081 history loss 5.747441 rank 7
2023-02-23 07:25:52,973 DEBUG CV Batch 15/400 loss 15.476539 loss_att 57.738602 loss_ctc 11.162171 loss_rnnt 7.456932 hw_loss 0.267081 history loss 5.747441 rank 5
2023-02-23 07:25:53,183 DEBUG CV Batch 15/400 loss 15.476539 loss_att 57.738602 loss_ctc 11.162171 loss_rnnt 7.456932 hw_loss 0.267081 history loss 5.747441 rank 2
2023-02-23 07:25:53,347 DEBUG CV Batch 15/400 loss 15.476539 loss_att 57.738602 loss_ctc 11.162171 loss_rnnt 7.456932 hw_loss 0.267081 history loss 5.747441 rank 0
2023-02-23 07:25:54,011 DEBUG CV Batch 15/400 loss 15.476539 loss_att 57.738602 loss_ctc 11.162171 loss_rnnt 7.456932 hw_loss 0.267081 history loss 5.747441 rank 3
2023-02-23 07:26:02,621 DEBUG CV Batch 15/500 loss 6.369311 loss_att 7.216037 loss_ctc 7.289580 loss_rnnt 5.850500 hw_loss 0.425182 history loss 6.586399 rank 4
2023-02-23 07:26:03,069 DEBUG CV Batch 15/500 loss 6.369311 loss_att 7.216037 loss_ctc 7.289580 loss_rnnt 5.850500 hw_loss 0.425182 history loss 6.586399 rank 1
2023-02-23 07:26:03,138 DEBUG CV Batch 15/500 loss 6.369311 loss_att 7.216037 loss_ctc 7.289580 loss_rnnt 5.850500 hw_loss 0.425182 history loss 6.586399 rank 6
2023-02-23 07:26:03,458 DEBUG CV Batch 15/500 loss 6.369311 loss_att 7.216037 loss_ctc 7.289580 loss_rnnt 5.850500 hw_loss 0.425182 history loss 6.586399 rank 5
2023-02-23 07:26:03,542 DEBUG CV Batch 15/500 loss 6.369311 loss_att 7.216037 loss_ctc 7.289580 loss_rnnt 5.850500 hw_loss 0.425182 history loss 6.586399 rank 7
2023-02-23 07:26:04,086 DEBUG CV Batch 15/500 loss 6.369311 loss_att 7.216037 loss_ctc 7.289580 loss_rnnt 5.850500 hw_loss 0.425182 history loss 6.586399 rank 2
2023-02-23 07:26:04,731 DEBUG CV Batch 15/500 loss 6.369311 loss_att 7.216037 loss_ctc 7.289580 loss_rnnt 5.850500 hw_loss 0.425182 history loss 6.586399 rank 3
2023-02-23 07:26:04,743 DEBUG CV Batch 15/500 loss 6.369311 loss_att 7.216037 loss_ctc 7.289580 loss_rnnt 5.850500 hw_loss 0.425182 history loss 6.586399 rank 0
2023-02-23 07:26:15,101 DEBUG CV Batch 15/600 loss 7.982099 loss_att 7.429104 loss_ctc 9.660474 loss_rnnt 7.440160 hw_loss 0.803914 history loss 7.621611 rank 4
2023-02-23 07:26:15,103 DEBUG CV Batch 15/600 loss 7.982099 loss_att 7.429104 loss_ctc 9.660474 loss_rnnt 7.440160 hw_loss 0.803913 history loss 7.621611 rank 1
2023-02-23 07:26:15,493 DEBUG CV Batch 15/600 loss 7.982099 loss_att 7.429104 loss_ctc 9.660474 loss_rnnt 7.440160 hw_loss 0.803914 history loss 7.621611 rank 6
2023-02-23 07:26:15,627 DEBUG CV Batch 15/600 loss 7.982099 loss_att 7.429104 loss_ctc 9.660474 loss_rnnt 7.440160 hw_loss 0.803913 history loss 7.621611 rank 5
2023-02-23 07:26:15,789 DEBUG CV Batch 15/600 loss 7.982099 loss_att 7.429104 loss_ctc 9.660474 loss_rnnt 7.440160 hw_loss 0.803914 history loss 7.621611 rank 7
2023-02-23 07:26:16,615 DEBUG CV Batch 15/600 loss 7.982099 loss_att 7.429104 loss_ctc 9.660474 loss_rnnt 7.440160 hw_loss 0.803914 history loss 7.621611 rank 2
2023-02-23 07:26:17,316 DEBUG CV Batch 15/600 loss 7.982099 loss_att 7.429104 loss_ctc 9.660474 loss_rnnt 7.440160 hw_loss 0.803914 history loss 7.621611 rank 3
2023-02-23 07:26:17,317 DEBUG CV Batch 15/600 loss 7.982099 loss_att 7.429104 loss_ctc 9.660474 loss_rnnt 7.440160 hw_loss 0.803914 history loss 7.621611 rank 0
2023-02-23 07:26:26,310 DEBUG CV Batch 15/700 loss 15.256599 loss_att 48.706726 loss_ctc 16.213223 loss_rnnt 8.245341 hw_loss 0.363154 history loss 8.348892 rank 4
2023-02-23 07:26:26,416 DEBUG CV Batch 15/700 loss 15.256599 loss_att 48.706726 loss_ctc 16.213223 loss_rnnt 8.245341 hw_loss 0.363154 history loss 8.348892 rank 1
2023-02-23 07:26:27,052 DEBUG CV Batch 15/700 loss 15.256599 loss_att 48.706726 loss_ctc 16.213223 loss_rnnt 8.245341 hw_loss 0.363154 history loss 8.348892 rank 5
2023-02-23 07:26:27,293 DEBUG CV Batch 15/700 loss 15.256599 loss_att 48.706726 loss_ctc 16.213223 loss_rnnt 8.245341 hw_loss 0.363154 history loss 8.348892 rank 7
2023-02-23 07:26:27,333 DEBUG CV Batch 15/700 loss 15.256599 loss_att 48.706726 loss_ctc 16.213223 loss_rnnt 8.245341 hw_loss 0.363154 history loss 8.348892 rank 6
2023-02-23 07:26:28,416 DEBUG CV Batch 15/700 loss 15.256599 loss_att 48.706726 loss_ctc 16.213223 loss_rnnt 8.245341 hw_loss 0.363154 history loss 8.348892 rank 2
2023-02-23 07:26:28,971 DEBUG CV Batch 15/700 loss 15.256599 loss_att 48.706726 loss_ctc 16.213223 loss_rnnt 8.245341 hw_loss 0.363154 history loss 8.348892 rank 3
2023-02-23 07:26:29,293 DEBUG CV Batch 15/700 loss 15.256599 loss_att 48.706726 loss_ctc 16.213223 loss_rnnt 8.245341 hw_loss 0.363154 history loss 8.348892 rank 0
2023-02-23 07:26:38,095 DEBUG CV Batch 15/800 loss 12.896971 loss_att 11.552536 loss_ctc 16.197660 loss_rnnt 12.412901 hw_loss 0.586622 history loss 7.776596 rank 1
2023-02-23 07:26:38,268 DEBUG CV Batch 15/800 loss 12.896971 loss_att 11.552536 loss_ctc 16.197660 loss_rnnt 12.412901 hw_loss 0.586622 history loss 7.776596 rank 5
2023-02-23 07:26:38,454 DEBUG CV Batch 15/800 loss 12.896971 loss_att 11.552536 loss_ctc 16.197660 loss_rnnt 12.412901 hw_loss 0.586622 history loss 7.776596 rank 4
2023-02-23 07:26:38,690 DEBUG CV Batch 15/800 loss 12.896971 loss_att 11.552536 loss_ctc 16.197660 loss_rnnt 12.412901 hw_loss 0.586622 history loss 7.776596 rank 7
2023-02-23 07:26:39,649 DEBUG CV Batch 15/800 loss 12.896971 loss_att 11.552536 loss_ctc 16.197660 loss_rnnt 12.412901 hw_loss 0.586622 history loss 7.776596 rank 6
2023-02-23 07:26:40,044 DEBUG CV Batch 15/800 loss 12.896971 loss_att 11.552536 loss_ctc 16.197660 loss_rnnt 12.412901 hw_loss 0.586622 history loss 7.776596 rank 2
2023-02-23 07:26:40,620 DEBUG CV Batch 15/800 loss 12.896971 loss_att 11.552536 loss_ctc 16.197660 loss_rnnt 12.412901 hw_loss 0.586622 history loss 7.776596 rank 0
2023-02-23 07:26:41,471 DEBUG CV Batch 15/800 loss 12.896971 loss_att 11.552536 loss_ctc 16.197660 loss_rnnt 12.412901 hw_loss 0.586622 history loss 7.776596 rank 3
2023-02-23 07:26:51,528 DEBUG CV Batch 15/900 loss 13.298714 loss_att 21.725834 loss_ctc 24.942635 loss_rnnt 9.892425 hw_loss 0.315642 history loss 7.556113 rank 1
2023-02-23 07:26:52,089 DEBUG CV Batch 15/900 loss 13.298714 loss_att 21.725834 loss_ctc 24.942635 loss_rnnt 9.892425 hw_loss 0.315642 history loss 7.556113 rank 5
2023-02-23 07:26:52,269 DEBUG CV Batch 15/900 loss 13.298714 loss_att 21.725834 loss_ctc 24.942635 loss_rnnt 9.892425 hw_loss 0.315642 history loss 7.556113 rank 7
2023-02-23 07:26:52,457 DEBUG CV Batch 15/900 loss 13.298714 loss_att 21.725834 loss_ctc 24.942635 loss_rnnt 9.892425 hw_loss 0.315642 history loss 7.556113 rank 4
2023-02-23 07:26:53,930 DEBUG CV Batch 15/900 loss 13.298714 loss_att 21.725834 loss_ctc 24.942635 loss_rnnt 9.892425 hw_loss 0.315642 history loss 7.556113 rank 2
2023-02-23 07:26:53,944 DEBUG CV Batch 15/900 loss 13.298714 loss_att 21.725834 loss_ctc 24.942635 loss_rnnt 9.892425 hw_loss 0.315642 history loss 7.556113 rank 6
2023-02-23 07:26:54,563 DEBUG CV Batch 15/900 loss 13.298714 loss_att 21.725834 loss_ctc 24.942635 loss_rnnt 9.892425 hw_loss 0.315642 history loss 7.556113 rank 0
2023-02-23 07:26:55,875 DEBUG CV Batch 15/900 loss 13.298714 loss_att 21.725834 loss_ctc 24.942635 loss_rnnt 9.892425 hw_loss 0.315642 history loss 7.556113 rank 3
2023-02-23 07:27:03,619 DEBUG CV Batch 15/1000 loss 4.224770 loss_att 4.660397 loss_ctc 4.759469 loss_rnnt 3.755364 hw_loss 0.583103 history loss 7.287963 rank 1
2023-02-23 07:27:04,278 DEBUG CV Batch 15/1000 loss 4.224770 loss_att 4.660397 loss_ctc 4.759469 loss_rnnt 3.755364 hw_loss 0.583103 history loss 7.287963 rank 5
2023-02-23 07:27:04,604 DEBUG CV Batch 15/1000 loss 4.224770 loss_att 4.660397 loss_ctc 4.759469 loss_rnnt 3.755364 hw_loss 0.583102 history loss 7.287963 rank 4
2023-02-23 07:27:04,872 DEBUG CV Batch 15/1000 loss 4.224770 loss_att 4.660397 loss_ctc 4.759469 loss_rnnt 3.755364 hw_loss 0.583103 history loss 7.287963 rank 7
2023-02-23 07:27:06,602 DEBUG CV Batch 15/1000 loss 4.224770 loss_att 4.660397 loss_ctc 4.759469 loss_rnnt 3.755364 hw_loss 0.583103 history loss 7.287963 rank 6
2023-02-23 07:27:06,717 DEBUG CV Batch 15/1000 loss 4.224770 loss_att 4.660397 loss_ctc 4.759469 loss_rnnt 3.755364 hw_loss 0.583103 history loss 7.287963 rank 2
2023-02-23 07:27:07,524 DEBUG CV Batch 15/1000 loss 4.224770 loss_att 4.660397 loss_ctc 4.759469 loss_rnnt 3.755364 hw_loss 0.583103 history loss 7.287963 rank 0
2023-02-23 07:27:08,492 DEBUG CV Batch 15/1000 loss 4.224770 loss_att 4.660397 loss_ctc 4.759469 loss_rnnt 3.755364 hw_loss 0.583103 history loss 7.287963 rank 3
2023-02-23 07:27:15,914 DEBUG CV Batch 15/1100 loss 6.565517 loss_att 5.755530 loss_ctc 8.434614 loss_rnnt 6.051121 hw_loss 0.800965 history loss 7.269867 rank 1
2023-02-23 07:27:16,454 DEBUG CV Batch 15/1100 loss 6.565517 loss_att 5.755530 loss_ctc 8.434614 loss_rnnt 6.051121 hw_loss 0.800965 history loss 7.269867 rank 4
2023-02-23 07:27:16,519 DEBUG CV Batch 15/1100 loss 6.565517 loss_att 5.755530 loss_ctc 8.434614 loss_rnnt 6.051121 hw_loss 0.800965 history loss 7.269867 rank 5
2023-02-23 07:27:17,172 DEBUG CV Batch 15/1100 loss 6.565517 loss_att 5.755530 loss_ctc 8.434614 loss_rnnt 6.051121 hw_loss 0.800965 history loss 7.269867 rank 7
2023-02-23 07:27:18,531 DEBUG CV Batch 15/1100 loss 6.565517 loss_att 5.755530 loss_ctc 8.434614 loss_rnnt 6.051121 hw_loss 0.800965 history loss 7.269867 rank 6
2023-02-23 07:27:19,169 DEBUG CV Batch 15/1100 loss 6.565517 loss_att 5.755530 loss_ctc 8.434614 loss_rnnt 6.051121 hw_loss 0.800965 history loss 7.269867 rank 2
2023-02-23 07:27:19,891 DEBUG CV Batch 15/1100 loss 6.565517 loss_att 5.755530 loss_ctc 8.434614 loss_rnnt 6.051121 hw_loss 0.800965 history loss 7.269867 rank 0
2023-02-23 07:27:20,903 DEBUG CV Batch 15/1100 loss 6.565517 loss_att 5.755530 loss_ctc 8.434614 loss_rnnt 6.051121 hw_loss 0.800965 history loss 7.269867 rank 3
2023-02-23 07:27:26,563 DEBUG CV Batch 15/1200 loss 9.610950 loss_att 9.613843 loss_ctc 9.959930 loss_rnnt 9.306887 hw_loss 0.481789 history loss 7.617137 rank 1
2023-02-23 07:27:27,032 DEBUG CV Batch 15/1200 loss 9.610950 loss_att 9.613843 loss_ctc 9.959930 loss_rnnt 9.306887 hw_loss 0.481789 history loss 7.617137 rank 5
2023-02-23 07:27:27,163 DEBUG CV Batch 15/1200 loss 9.610950 loss_att 9.613843 loss_ctc 9.959930 loss_rnnt 9.306887 hw_loss 0.481789 history loss 7.617137 rank 4
2023-02-23 07:27:28,073 DEBUG CV Batch 15/1200 loss 9.610950 loss_att 9.613843 loss_ctc 9.959930 loss_rnnt 9.306887 hw_loss 0.481789 history loss 7.617137 rank 7
2023-02-23 07:27:29,102 DEBUG CV Batch 15/1200 loss 9.610950 loss_att 9.613843 loss_ctc 9.959930 loss_rnnt 9.306887 hw_loss 0.481789 history loss 7.617137 rank 6
2023-02-23 07:27:30,224 DEBUG CV Batch 15/1200 loss 9.610950 loss_att 9.613843 loss_ctc 9.959930 loss_rnnt 9.306887 hw_loss 0.481789 history loss 7.617137 rank 2
2023-02-23 07:27:31,584 DEBUG CV Batch 15/1200 loss 9.610950 loss_att 9.613843 loss_ctc 9.959930 loss_rnnt 9.306887 hw_loss 0.481789 history loss 7.617137 rank 0
2023-02-23 07:27:31,800 DEBUG CV Batch 15/1200 loss 9.610950 loss_att 9.613843 loss_ctc 9.959930 loss_rnnt 9.306887 hw_loss 0.481789 history loss 7.617137 rank 3
2023-02-23 07:27:38,602 DEBUG CV Batch 15/1300 loss 6.747568 loss_att 5.640846 loss_ctc 7.983140 loss_rnnt 6.484200 hw_loss 0.599944 history loss 7.953961 rank 1
2023-02-23 07:27:39,082 DEBUG CV Batch 15/1300 loss 6.747568 loss_att 5.640846 loss_ctc 7.983140 loss_rnnt 6.484200 hw_loss 0.599944 history loss 7.953961 rank 5
2023-02-23 07:27:39,173 DEBUG CV Batch 15/1300 loss 6.747568 loss_att 5.640846 loss_ctc 7.983140 loss_rnnt 6.484200 hw_loss 0.599944 history loss 7.953961 rank 4
2023-02-23 07:27:40,283 DEBUG CV Batch 15/1300 loss 6.747568 loss_att 5.640846 loss_ctc 7.983140 loss_rnnt 6.484200 hw_loss 0.599944 history loss 7.953961 rank 7
2023-02-23 07:27:41,278 DEBUG CV Batch 15/1300 loss 6.747568 loss_att 5.640846 loss_ctc 7.983140 loss_rnnt 6.484200 hw_loss 0.599944 history loss 7.953961 rank 6
2023-02-23 07:27:42,712 DEBUG CV Batch 15/1300 loss 6.747568 loss_att 5.640846 loss_ctc 7.983140 loss_rnnt 6.484200 hw_loss 0.599944 history loss 7.953961 rank 2
2023-02-23 07:27:44,107 DEBUG CV Batch 15/1300 loss 6.747568 loss_att 5.640846 loss_ctc 7.983140 loss_rnnt 6.484200 hw_loss 0.599944 history loss 7.953961 rank 0
2023-02-23 07:27:44,425 DEBUG CV Batch 15/1300 loss 6.747568 loss_att 5.640846 loss_ctc 7.983140 loss_rnnt 6.484200 hw_loss 0.599944 history loss 7.953961 rank 3
2023-02-23 07:27:49,672 DEBUG CV Batch 15/1400 loss 5.970692 loss_att 26.261454 loss_ctc 3.916986 loss_rnnt 1.968469 hw_loss 0.408559 history loss 8.317216 rank 1
2023-02-23 07:27:51,229 DEBUG CV Batch 15/1400 loss 5.970692 loss_att 26.261454 loss_ctc 3.916986 loss_rnnt 1.968469 hw_loss 0.408559 history loss 8.317216 rank 5
2023-02-23 07:27:51,633 DEBUG CV Batch 15/1400 loss 5.970692 loss_att 26.261454 loss_ctc 3.916986 loss_rnnt 1.968469 hw_loss 0.408559 history loss 8.317216 rank 4
2023-02-23 07:27:51,891 DEBUG CV Batch 15/1400 loss 5.970692 loss_att 26.261454 loss_ctc 3.916986 loss_rnnt 1.968469 hw_loss 0.408559 history loss 8.317216 rank 7
2023-02-23 07:27:53,520 DEBUG CV Batch 15/1400 loss 5.970692 loss_att 26.261454 loss_ctc 3.916986 loss_rnnt 1.968469 hw_loss 0.408559 history loss 8.317216 rank 6
2023-02-23 07:27:54,495 DEBUG CV Batch 15/1400 loss 5.970692 loss_att 26.261454 loss_ctc 3.916986 loss_rnnt 1.968469 hw_loss 0.408559 history loss 8.317216 rank 2
2023-02-23 07:27:55,805 DEBUG CV Batch 15/1400 loss 5.970692 loss_att 26.261454 loss_ctc 3.916986 loss_rnnt 1.968469 hw_loss 0.408559 history loss 8.317216 rank 0
2023-02-23 07:27:56,549 DEBUG CV Batch 15/1400 loss 5.970692 loss_att 26.261454 loss_ctc 3.916986 loss_rnnt 1.968469 hw_loss 0.408559 history loss 8.317216 rank 3
2023-02-23 07:28:01,145 DEBUG CV Batch 15/1500 loss 8.535622 loss_att 9.581730 loss_ctc 9.663294 loss_rnnt 7.912129 hw_loss 0.494838 history loss 8.128208 rank 1
2023-02-23 07:28:02,553 DEBUG CV Batch 15/1500 loss 8.535622 loss_att 9.581730 loss_ctc 9.663294 loss_rnnt 7.912129 hw_loss 0.494838 history loss 8.128208 rank 5
2023-02-23 07:28:03,563 DEBUG CV Batch 15/1500 loss 8.535622 loss_att 9.581730 loss_ctc 9.663294 loss_rnnt 7.912129 hw_loss 0.494838 history loss 8.128208 rank 7
2023-02-23 07:28:04,571 DEBUG CV Batch 15/1500 loss 8.535622 loss_att 9.581730 loss_ctc 9.663294 loss_rnnt 7.912129 hw_loss 0.494838 history loss 8.128208 rank 4
2023-02-23 07:28:06,068 DEBUG CV Batch 15/1500 loss 8.535622 loss_att 9.581730 loss_ctc 9.663294 loss_rnnt 7.912129 hw_loss 0.494838 history loss 8.128208 rank 6
2023-02-23 07:28:06,455 DEBUG CV Batch 15/1500 loss 8.535622 loss_att 9.581730 loss_ctc 9.663294 loss_rnnt 7.912129 hw_loss 0.494838 history loss 8.128208 rank 2
2023-02-23 07:28:07,712 DEBUG CV Batch 15/1500 loss 8.535622 loss_att 9.581730 loss_ctc 9.663294 loss_rnnt 7.912129 hw_loss 0.494838 history loss 8.128208 rank 0
2023-02-23 07:28:09,280 DEBUG CV Batch 15/1500 loss 8.535622 loss_att 9.581730 loss_ctc 9.663294 loss_rnnt 7.912129 hw_loss 0.494838 history loss 8.128208 rank 3
2023-02-23 07:28:14,358 DEBUG CV Batch 15/1600 loss 12.166447 loss_att 16.457174 loss_ctc 14.301432 loss_rnnt 10.864667 hw_loss 0.298067 history loss 8.043048 rank 1
2023-02-23 07:28:15,800 DEBUG CV Batch 15/1600 loss 12.166447 loss_att 16.457174 loss_ctc 14.301432 loss_rnnt 10.864667 hw_loss 0.298067 history loss 8.043048 rank 5
2023-02-23 07:28:17,058 DEBUG CV Batch 15/1600 loss 12.166447 loss_att 16.457174 loss_ctc 14.301432 loss_rnnt 10.864667 hw_loss 0.298067 history loss 8.043048 rank 7
2023-02-23 07:28:18,878 DEBUG CV Batch 15/1600 loss 12.166447 loss_att 16.457174 loss_ctc 14.301432 loss_rnnt 10.864667 hw_loss 0.298067 history loss 8.043048 rank 4
2023-02-23 07:28:19,944 DEBUG CV Batch 15/1600 loss 12.166447 loss_att 16.457174 loss_ctc 14.301432 loss_rnnt 10.864667 hw_loss 0.298067 history loss 8.043048 rank 6
2023-02-23 07:28:20,063 DEBUG CV Batch 15/1600 loss 12.166447 loss_att 16.457174 loss_ctc 14.301432 loss_rnnt 10.864667 hw_loss 0.298067 history loss 8.043048 rank 2
2023-02-23 07:28:21,652 DEBUG CV Batch 15/1600 loss 12.166447 loss_att 16.457174 loss_ctc 14.301432 loss_rnnt 10.864667 hw_loss 0.298067 history loss 8.043048 rank 0
2023-02-23 07:28:23,446 DEBUG CV Batch 15/1600 loss 12.166447 loss_att 16.457174 loss_ctc 14.301432 loss_rnnt 10.864667 hw_loss 0.298067 history loss 8.043048 rank 3
2023-02-23 07:28:27,341 DEBUG CV Batch 15/1700 loss 8.522755 loss_att 8.207840 loss_ctc 11.796310 loss_rnnt 7.833467 hw_loss 0.592116 history loss 7.927510 rank 1
2023-02-23 07:28:28,742 DEBUG CV Batch 15/1700 loss 8.522755 loss_att 8.207840 loss_ctc 11.796310 loss_rnnt 7.833467 hw_loss 0.592116 history loss 7.927510 rank 5
2023-02-23 07:28:29,814 DEBUG CV Batch 15/1700 loss 8.522755 loss_att 8.207840 loss_ctc 11.796310 loss_rnnt 7.833467 hw_loss 0.592116 history loss 7.927510 rank 7
2023-02-23 07:28:31,464 DEBUG CV Batch 15/1700 loss 8.522755 loss_att 8.207840 loss_ctc 11.796310 loss_rnnt 7.833467 hw_loss 0.592116 history loss 7.927510 rank 4
2023-02-23 07:28:32,511 DEBUG CV Batch 15/1700 loss 8.522755 loss_att 8.207840 loss_ctc 11.796310 loss_rnnt 7.833467 hw_loss 0.592116 history loss 7.927510 rank 6
2023-02-23 07:28:32,801 DEBUG CV Batch 15/1700 loss 8.522755 loss_att 8.207840 loss_ctc 11.796310 loss_rnnt 7.833467 hw_loss 0.592116 history loss 7.927510 rank 2
2023-02-23 07:28:34,871 DEBUG CV Batch 15/1700 loss 8.522755 loss_att 8.207840 loss_ctc 11.796310 loss_rnnt 7.833467 hw_loss 0.592116 history loss 7.927510 rank 0
2023-02-23 07:28:35,781 DEBUG CV Batch 15/1700 loss 8.522755 loss_att 8.207840 loss_ctc 11.796310 loss_rnnt 7.833467 hw_loss 0.592116 history loss 7.927510 rank 3
2023-02-23 07:28:36,658 INFO Epoch 15 CV info cv_loss 7.882051262888782
2023-02-23 07:28:36,658 INFO Epoch 16 TRAIN info lr 0.00043285906419028504
2023-02-23 07:28:36,660 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 07:28:38,241 INFO Epoch 15 CV info cv_loss 7.88205126294047
2023-02-23 07:28:38,242 INFO Epoch 16 TRAIN info lr 0.0004327504264243662
2023-02-23 07:28:38,247 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 07:28:38,964 INFO Epoch 15 CV info cv_loss 7.882051262742333
2023-02-23 07:28:38,965 INFO Epoch 16 TRAIN info lr 0.0004327650148013916
2023-02-23 07:28:38,970 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 07:28:40,813 INFO Epoch 15 CV info cv_loss 7.882051263280748
2023-02-23 07:28:40,814 INFO Epoch 16 TRAIN info lr 0.0004328250047444059
2023-02-23 07:28:40,816 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 07:28:42,108 INFO Epoch 15 CV info cv_loss 7.88205126183349
2023-02-23 07:28:42,108 INFO Epoch 16 TRAIN info lr 0.0004328347351942514
2023-02-23 07:28:42,110 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 07:28:42,167 INFO Epoch 15 CV info cv_loss 7.882051259468773
2023-02-23 07:28:42,167 INFO Epoch 16 TRAIN info lr 0.00043282824815477143
2023-02-23 07:28:42,172 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 07:28:44,330 INFO Epoch 15 CV info cv_loss 7.882051262759563
2023-02-23 07:28:44,331 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/15.pt
2023-02-23 07:28:44,944 INFO Epoch 16 TRAIN info lr 0.0004328460882152176
2023-02-23 07:28:44,949 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 07:28:45,237 INFO Epoch 15 CV info cv_loss 7.882051260347466
2023-02-23 07:28:45,238 INFO Epoch 16 TRAIN info lr 0.00043276987792160087
2023-02-23 07:28:45,243 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 07:29:46,677 DEBUG TRAIN Batch 16/0 loss 13.030676 loss_att 11.325433 loss_ctc 16.720938 loss_rnnt 12.519223 hw_loss 0.675873 lr 0.00043283 rank 2
2023-02-23 07:29:46,682 DEBUG TRAIN Batch 16/0 loss 9.790545 loss_att 9.439211 loss_ctc 11.326737 loss_rnnt 9.187696 hw_loss 0.878042 lr 0.00043275 rank 5
2023-02-23 07:29:46,683 DEBUG TRAIN Batch 16/0 loss 9.184808 loss_att 8.592682 loss_ctc 11.395024 loss_rnnt 8.646112 hw_loss 0.679548 lr 0.00043276 rank 7
2023-02-23 07:29:46,688 DEBUG TRAIN Batch 16/0 loss 9.165875 loss_att 8.776749 loss_ctc 12.399176 loss_rnnt 8.447055 hw_loss 0.685386 lr 0.00043283 rank 6
2023-02-23 07:29:46,703 DEBUG TRAIN Batch 16/0 loss 10.193128 loss_att 9.576982 loss_ctc 13.133829 loss_rnnt 9.558723 hw_loss 0.685386 lr 0.00043284 rank 0
2023-02-23 07:29:46,706 DEBUG TRAIN Batch 16/0 loss 7.490809 loss_att 7.148643 loss_ctc 8.624031 loss_rnnt 7.025225 hw_loss 0.717979 lr 0.00043282 rank 4
2023-02-23 07:29:46,707 DEBUG TRAIN Batch 16/0 loss 6.742043 loss_att 6.319437 loss_ctc 8.086097 loss_rnnt 6.331723 hw_loss 0.591812 lr 0.00043277 rank 3
2023-02-23 07:29:46,711 DEBUG TRAIN Batch 16/0 loss 7.549477 loss_att 7.509201 loss_ctc 9.301013 loss_rnnt 6.930127 hw_loss 0.738500 lr 0.00043286 rank 1
2023-02-23 07:30:58,374 DEBUG TRAIN Batch 16/100 loss 18.691471 loss_att 25.946381 loss_ctc 30.030762 loss_rnnt 15.534938 hw_loss 0.363085 lr 0.00043266 rank 4
2023-02-23 07:30:58,374 DEBUG TRAIN Batch 16/100 loss 9.301968 loss_att 11.636627 loss_ctc 10.204676 loss_rnnt 8.454868 hw_loss 0.487138 lr 0.00043270 rank 1
2023-02-23 07:30:58,377 DEBUG TRAIN Batch 16/100 loss 8.700757 loss_att 12.346834 loss_ctc 13.711074 loss_rnnt 7.062217 hw_loss 0.452404 lr 0.00043260 rank 7
2023-02-23 07:30:58,378 DEBUG TRAIN Batch 16/100 loss 8.306305 loss_att 12.311334 loss_ctc 11.272303 loss_rnnt 6.879030 hw_loss 0.432757 lr 0.00043261 rank 3
2023-02-23 07:30:58,379 DEBUG TRAIN Batch 16/100 loss 10.524649 loss_att 13.417614 loss_ctc 15.264595 loss_rnnt 9.115387 hw_loss 0.372517 lr 0.00043266 rank 6
2023-02-23 07:30:58,383 DEBUG TRAIN Batch 16/100 loss 9.319223 loss_att 11.981708 loss_ctc 15.983875 loss_rnnt 7.721286 hw_loss 0.331538 lr 0.00043259 rank 5
2023-02-23 07:30:58,385 DEBUG TRAIN Batch 16/100 loss 23.884130 loss_att 26.639269 loss_ctc 28.013548 loss_rnnt 22.604843 hw_loss 0.333133 lr 0.00043268 rank 0
2023-02-23 07:30:58,385 DEBUG TRAIN Batch 16/100 loss 3.962847 loss_att 8.846933 loss_ctc 5.310676 loss_rnnt 2.625771 hw_loss 0.338529 lr 0.00043267 rank 2
2023-02-23 07:32:10,687 DEBUG TRAIN Batch 16/200 loss 6.422304 loss_att 12.034695 loss_ctc 9.210333 loss_rnnt 4.694746 hw_loss 0.437515 lr 0.00043250 rank 6
2023-02-23 07:32:10,702 DEBUG TRAIN Batch 16/200 loss 15.110299 loss_att 16.299547 loss_ctc 15.663862 loss_rnnt 14.553154 hw_loss 0.460289 lr 0.00043244 rank 3
2023-02-23 07:32:10,703 DEBUG TRAIN Batch 16/200 loss 7.166820 loss_att 10.481329 loss_ctc 11.049271 loss_rnnt 5.691137 hw_loss 0.553352 lr 0.00043243 rank 5
2023-02-23 07:32:10,703 DEBUG TRAIN Batch 16/200 loss 8.144431 loss_att 10.168084 loss_ctc 11.161662 loss_rnnt 7.136110 hw_loss 0.377425 lr 0.00043251 rank 2
2023-02-23 07:32:10,704 DEBUG TRAIN Batch 16/200 loss 8.586433 loss_att 11.166763 loss_ctc 12.114159 loss_rnnt 7.377169 hw_loss 0.417816 lr 0.00043244 rank 7
2023-02-23 07:32:10,706 DEBUG TRAIN Batch 16/200 loss 8.146905 loss_att 14.394374 loss_ctc 10.335953 loss_rnnt 6.374683 hw_loss 0.432854 lr 0.00043252 rank 0
2023-02-23 07:32:10,707 DEBUG TRAIN Batch 16/200 loss 10.911101 loss_att 15.523947 loss_ctc 17.124073 loss_rnnt 8.927478 hw_loss 0.436235 lr 0.00043250 rank 4
2023-02-23 07:32:10,716 DEBUG TRAIN Batch 16/200 loss 8.064469 loss_att 10.849408 loss_ctc 12.259420 loss_rnnt 6.733745 hw_loss 0.402020 lr 0.00043253 rank 1
2023-02-23 07:33:23,678 DEBUG TRAIN Batch 16/300 loss 7.187376 loss_att 9.555613 loss_ctc 8.318813 loss_rnnt 6.365499 hw_loss 0.370070 lr 0.00043236 rank 0
2023-02-23 07:33:23,678 DEBUG TRAIN Batch 16/300 loss 8.642499 loss_att 9.496135 loss_ctc 12.342162 loss_rnnt 7.812486 hw_loss 0.311246 lr 0.00043234 rank 6
2023-02-23 07:33:23,684 DEBUG TRAIN Batch 16/300 loss 11.179224 loss_att 16.719543 loss_ctc 11.224980 loss_rnnt 9.847971 hw_loss 0.407041 lr 0.00043235 rank 2
2023-02-23 07:33:23,686 DEBUG TRAIN Batch 16/300 loss 15.340778 loss_att 16.279953 loss_ctc 17.131527 loss_rnnt 14.751625 hw_loss 0.304784 lr 0.00043228 rank 7
2023-02-23 07:33:23,686 DEBUG TRAIN Batch 16/300 loss 12.666368 loss_att 16.142792 loss_ctc 17.917933 loss_rnnt 11.028767 hw_loss 0.453953 lr 0.00043228 rank 3
2023-02-23 07:33:23,689 DEBUG TRAIN Batch 16/300 loss 6.808594 loss_att 12.073868 loss_ctc 7.661625 loss_rnnt 5.449450 hw_loss 0.360662 lr 0.00043237 rank 1
2023-02-23 07:33:23,693 DEBUG TRAIN Batch 16/300 loss 7.312634 loss_att 10.258756 loss_ctc 11.315484 loss_rnnt 5.952065 hw_loss 0.445559 lr 0.00043234 rank 4
2023-02-23 07:33:23,696 DEBUG TRAIN Batch 16/300 loss 11.061656 loss_att 18.073368 loss_ctc 19.796721 loss_rnnt 8.295443 hw_loss 0.373493 lr 0.00043226 rank 5
2023-02-23 07:34:37,161 DEBUG TRAIN Batch 16/400 loss 9.894063 loss_att 14.284760 loss_ctc 13.285963 loss_rnnt 8.328530 hw_loss 0.440887 lr 0.00043219 rank 2
2023-02-23 07:34:37,165 DEBUG TRAIN Batch 16/400 loss 7.138148 loss_att 11.617307 loss_ctc 11.931290 loss_rnnt 5.389015 hw_loss 0.401656 lr 0.00043218 rank 6
2023-02-23 07:34:37,165 DEBUG TRAIN Batch 16/400 loss 15.497233 loss_att 17.031738 loss_ctc 19.777241 loss_rnnt 14.393026 hw_loss 0.424949 lr 0.00043221 rank 1
2023-02-23 07:34:37,166 DEBUG TRAIN Batch 16/400 loss 11.689263 loss_att 15.350712 loss_ctc 14.784179 loss_rnnt 10.331419 hw_loss 0.399187 lr 0.00043212 rank 3
2023-02-23 07:34:37,167 DEBUG TRAIN Batch 16/400 loss 11.066345 loss_att 12.695434 loss_ctc 12.855556 loss_rnnt 10.291264 hw_loss 0.395067 lr 0.00043210 rank 5
2023-02-23 07:34:37,170 DEBUG TRAIN Batch 16/400 loss 6.829404 loss_att 7.513647 loss_ctc 8.128233 loss_rnnt 6.320341 hw_loss 0.373195 lr 0.00043212 rank 7
2023-02-23 07:34:37,180 DEBUG TRAIN Batch 16/400 loss 16.094130 loss_att 16.459961 loss_ctc 19.473745 loss_rnnt 15.324343 hw_loss 0.461260 lr 0.00043220 rank 0
2023-02-23 07:34:37,214 DEBUG TRAIN Batch 16/400 loss 10.580544 loss_att 15.186452 loss_ctc 13.097130 loss_rnnt 9.140345 hw_loss 0.344009 lr 0.00043218 rank 4
2023-02-23 07:35:49,528 DEBUG TRAIN Batch 16/500 loss 11.929265 loss_att 12.137149 loss_ctc 14.876482 loss_rnnt 11.280963 hw_loss 0.400808 lr 0.00043196 rank 7
2023-02-23 07:35:49,531 DEBUG TRAIN Batch 16/500 loss 10.528183 loss_att 11.924955 loss_ctc 14.722087 loss_rnnt 9.427810 hw_loss 0.490935 lr 0.00043201 rank 4
2023-02-23 07:35:49,532 DEBUG TRAIN Batch 16/500 loss 10.693132 loss_att 12.958973 loss_ctc 12.674253 loss_rnnt 9.786599 hw_loss 0.354779 lr 0.00043202 rank 2
2023-02-23 07:35:49,534 DEBUG TRAIN Batch 16/500 loss 13.463585 loss_att 14.676603 loss_ctc 17.805069 loss_rnnt 12.387506 hw_loss 0.477393 lr 0.00043196 rank 3
2023-02-23 07:35:49,535 DEBUG TRAIN Batch 16/500 loss 9.186123 loss_att 11.904617 loss_ctc 12.512230 loss_rnnt 7.963856 hw_loss 0.440787 lr 0.00043194 rank 5
2023-02-23 07:35:49,536 DEBUG TRAIN Batch 16/500 loss 4.157675 loss_att 7.404718 loss_ctc 7.167590 loss_rnnt 2.871517 hw_loss 0.441428 lr 0.00043205 rank 1
2023-02-23 07:35:49,540 DEBUG TRAIN Batch 16/500 loss 18.168573 loss_att 19.780563 loss_ctc 24.725269 loss_rnnt 16.740501 hw_loss 0.433963 lr 0.00043202 rank 6
2023-02-23 07:35:49,586 DEBUG TRAIN Batch 16/500 loss 5.695933 loss_att 7.754221 loss_ctc 8.027820 loss_rnnt 4.761192 hw_loss 0.397811 lr 0.00043204 rank 0
2023-02-23 07:37:02,396 DEBUG TRAIN Batch 16/600 loss 5.825240 loss_att 6.432495 loss_ctc 8.053073 loss_rnnt 5.089688 hw_loss 0.594481 lr 0.00043179 rank 7
2023-02-23 07:37:02,397 DEBUG TRAIN Batch 16/600 loss 7.700998 loss_att 9.948826 loss_ctc 10.347294 loss_rnnt 6.665828 hw_loss 0.436434 lr 0.00043189 rank 1
2023-02-23 07:37:02,400 DEBUG TRAIN Batch 16/600 loss 8.505672 loss_att 8.830533 loss_ctc 11.247038 loss_rnnt 7.794086 hw_loss 0.527062 lr 0.00043186 rank 2
2023-02-23 07:37:02,401 DEBUG TRAIN Batch 16/600 loss 17.108076 loss_att 20.944599 loss_ctc 23.213972 loss_rnnt 15.331596 hw_loss 0.365729 lr 0.00043185 rank 4
2023-02-23 07:37:02,403 DEBUG TRAIN Batch 16/600 loss 10.868486 loss_att 12.519723 loss_ctc 14.381863 loss_rnnt 9.848110 hw_loss 0.415647 lr 0.00043180 rank 3
2023-02-23 07:37:02,405 DEBUG TRAIN Batch 16/600 loss 9.669479 loss_att 11.133677 loss_ctc 11.534803 loss_rnnt 8.908165 hw_loss 0.412059 lr 0.00043187 rank 0
2023-02-23 07:37:02,404 DEBUG TRAIN Batch 16/600 loss 9.590028 loss_att 9.825382 loss_ctc 11.930440 loss_rnnt 8.852324 hw_loss 0.709833 lr 0.00043186 rank 6
2023-02-23 07:37:02,409 DEBUG TRAIN Batch 16/600 loss 16.079649 loss_att 16.824406 loss_ctc 22.701488 loss_rnnt 14.735006 hw_loss 0.586464 lr 0.00043178 rank 5
2023-02-23 07:38:18,557 DEBUG TRAIN Batch 16/700 loss 5.198963 loss_att 10.313330 loss_ctc 7.823940 loss_rnnt 3.614368 hw_loss 0.396983 lr 0.00043170 rank 2
2023-02-23 07:38:18,559 DEBUG TRAIN Batch 16/700 loss 7.396632 loss_att 12.035858 loss_ctc 11.990619 loss_rnnt 5.617293 hw_loss 0.448053 lr 0.00043162 rank 5
2023-02-23 07:38:18,563 DEBUG TRAIN Batch 16/700 loss 9.227673 loss_att 16.651638 loss_ctc 12.784275 loss_rnnt 7.060108 hw_loss 0.391046 lr 0.00043170 rank 6
2023-02-23 07:38:18,563 DEBUG TRAIN Batch 16/700 loss 9.737860 loss_att 14.395294 loss_ctc 12.015422 loss_rnnt 8.291938 hw_loss 0.395174 lr 0.00043164 rank 3
2023-02-23 07:38:18,585 DEBUG TRAIN Batch 16/700 loss 9.734797 loss_att 15.109334 loss_ctc 14.725698 loss_rnnt 7.793741 hw_loss 0.376306 lr 0.00043163 rank 7
2023-02-23 07:38:18,590 DEBUG TRAIN Batch 16/700 loss 8.889354 loss_att 16.698692 loss_ctc 11.556740 loss_rnnt 6.696088 hw_loss 0.517026 lr 0.00043171 rank 0
2023-02-23 07:38:18,603 DEBUG TRAIN Batch 16/700 loss 3.593080 loss_att 6.451622 loss_ctc 4.904933 loss_rnnt 2.632066 hw_loss 0.401986 lr 0.00043169 rank 4
2023-02-23 07:38:18,940 DEBUG TRAIN Batch 16/700 loss 13.863008 loss_att 16.799839 loss_ctc 20.108902 loss_rnnt 12.237918 hw_loss 0.384262 lr 0.00043173 rank 1
2023-02-23 07:39:32,925 DEBUG TRAIN Batch 16/800 loss 7.006753 loss_att 8.905819 loss_ctc 8.600408 loss_rnnt 6.206236 hw_loss 0.390407 lr 0.00043153 rank 4
2023-02-23 07:39:32,927 DEBUG TRAIN Batch 16/800 loss 4.860063 loss_att 8.847019 loss_ctc 7.416673 loss_rnnt 3.534204 hw_loss 0.351722 lr 0.00043147 rank 7
2023-02-23 07:39:32,929 DEBUG TRAIN Batch 16/800 loss 14.971466 loss_att 20.138739 loss_ctc 17.876698 loss_rnnt 13.349914 hw_loss 0.376375 lr 0.00043154 rank 2
2023-02-23 07:39:32,932 DEBUG TRAIN Batch 16/800 loss 26.713900 loss_att 25.642262 loss_ctc 40.274902 loss_rnnt 24.922520 hw_loss 0.370444 lr 0.00043154 rank 6
2023-02-23 07:39:32,933 DEBUG TRAIN Batch 16/800 loss 12.028737 loss_att 16.569889 loss_ctc 18.073917 loss_rnnt 10.134706 hw_loss 0.337081 lr 0.00043148 rank 3
2023-02-23 07:39:32,934 DEBUG TRAIN Batch 16/800 loss 17.058893 loss_att 20.217968 loss_ctc 22.524572 loss_rnnt 15.472418 hw_loss 0.423568 lr 0.00043155 rank 0
2023-02-23 07:39:32,935 DEBUG TRAIN Batch 16/800 loss 9.842680 loss_att 13.249289 loss_ctc 13.784945 loss_rnnt 8.459025 hw_loss 0.331306 lr 0.00043146 rank 5
2023-02-23 07:39:32,935 DEBUG TRAIN Batch 16/800 loss 6.644281 loss_att 8.702087 loss_ctc 10.518076 loss_rnnt 5.509620 hw_loss 0.387366 lr 0.00043157 rank 1
2023-02-23 07:40:45,736 DEBUG TRAIN Batch 16/900 loss 6.032351 loss_att 11.620418 loss_ctc 7.576041 loss_rnnt 4.526455 hw_loss 0.342106 lr 0.00043131 rank 7
2023-02-23 07:40:45,746 DEBUG TRAIN Batch 16/900 loss 16.266584 loss_att 17.671211 loss_ctc 18.947456 loss_rnnt 15.391012 hw_loss 0.444744 lr 0.00043138 rank 2
2023-02-23 07:40:45,750 DEBUG TRAIN Batch 16/900 loss 9.044260 loss_att 13.384620 loss_ctc 12.592253 loss_rnnt 7.530130 hw_loss 0.324362 lr 0.00043132 rank 3
2023-02-23 07:40:45,751 DEBUG TRAIN Batch 16/900 loss 12.844999 loss_att 16.971416 loss_ctc 17.052387 loss_rnnt 11.225185 hw_loss 0.437899 lr 0.00043139 rank 0
2023-02-23 07:40:45,753 DEBUG TRAIN Batch 16/900 loss 8.534908 loss_att 10.651193 loss_ctc 9.931484 loss_rnnt 7.694551 hw_loss 0.432918 lr 0.00043137 rank 6
2023-02-23 07:40:45,753 DEBUG TRAIN Batch 16/900 loss 8.403572 loss_att 12.431193 loss_ctc 13.510169 loss_rnnt 6.689819 hw_loss 0.426280 lr 0.00043140 rank 1
2023-02-23 07:40:45,756 DEBUG TRAIN Batch 16/900 loss 19.446209 loss_att 23.086193 loss_ctc 27.735554 loss_rnnt 17.403473 hw_loss 0.392801 lr 0.00043130 rank 5
2023-02-23 07:40:45,795 DEBUG TRAIN Batch 16/900 loss 10.477599 loss_att 13.730270 loss_ctc 15.589229 loss_rnnt 8.909717 hw_loss 0.442121 lr 0.00043137 rank 4
2023-02-23 07:41:58,832 DEBUG TRAIN Batch 16/1000 loss 10.450575 loss_att 13.516653 loss_ctc 13.656828 loss_rnnt 9.175512 hw_loss 0.439398 lr 0.00043123 rank 0
2023-02-23 07:41:58,834 DEBUG TRAIN Batch 16/1000 loss 4.528517 loss_att 10.033972 loss_ctc 8.209637 loss_rnnt 2.738539 hw_loss 0.371384 lr 0.00043115 rank 7
2023-02-23 07:41:58,836 DEBUG TRAIN Batch 16/1000 loss 4.083253 loss_att 7.996497 loss_ctc 6.967668 loss_rnnt 2.710236 hw_loss 0.385837 lr 0.00043122 rank 2
2023-02-23 07:41:58,837 DEBUG TRAIN Batch 16/1000 loss 22.205347 loss_att 27.762163 loss_ctc 28.305717 loss_rnnt 20.093750 hw_loss 0.350345 lr 0.00043116 rank 3
2023-02-23 07:41:58,838 DEBUG TRAIN Batch 16/1000 loss 5.193593 loss_att 6.881919 loss_ctc 8.836126 loss_rnnt 4.156108 hw_loss 0.401529 lr 0.00043124 rank 1
2023-02-23 07:41:58,851 DEBUG TRAIN Batch 16/1000 loss 6.615425 loss_att 9.202063 loss_ctc 6.148961 loss_rnnt 5.902696 hw_loss 0.482993 lr 0.00043121 rank 4
2023-02-23 07:41:58,854 DEBUG TRAIN Batch 16/1000 loss 5.798554 loss_att 9.255709 loss_ctc 6.876556 loss_rnnt 4.737677 hw_loss 0.423213 lr 0.00043121 rank 6
2023-02-23 07:41:58,861 DEBUG TRAIN Batch 16/1000 loss 17.361525 loss_att 17.084099 loss_ctc 26.064411 loss_rnnt 16.065521 hw_loss 0.358318 lr 0.00043114 rank 5
2023-02-23 07:43:13,069 DEBUG TRAIN Batch 16/1100 loss 6.899368 loss_att 10.832771 loss_ctc 8.054354 loss_rnnt 5.745624 hw_loss 0.399499 lr 0.00043105 rank 4
2023-02-23 07:43:13,072 DEBUG TRAIN Batch 16/1100 loss 12.916650 loss_att 12.371161 loss_ctc 13.652203 loss_rnnt 12.716209 hw_loss 0.396498 lr 0.00043100 rank 3
2023-02-23 07:43:13,073 DEBUG TRAIN Batch 16/1100 loss 7.642126 loss_att 10.344609 loss_ctc 9.420187 loss_rnnt 6.650545 hw_loss 0.401267 lr 0.00043107 rank 0
2023-02-23 07:43:13,074 DEBUG TRAIN Batch 16/1100 loss 15.634860 loss_att 16.069134 loss_ctc 21.663986 loss_rnnt 14.496800 hw_loss 0.463727 lr 0.00043098 rank 5
2023-02-23 07:43:13,073 DEBUG TRAIN Batch 16/1100 loss 13.021645 loss_att 15.274002 loss_ctc 15.045353 loss_rnnt 12.096992 hw_loss 0.383162 lr 0.00043105 rank 6
2023-02-23 07:43:13,074 DEBUG TRAIN Batch 16/1100 loss 11.571019 loss_att 14.748089 loss_ctc 17.758415 loss_rnnt 9.864031 hw_loss 0.462354 lr 0.00043106 rank 2
2023-02-23 07:43:13,077 DEBUG TRAIN Batch 16/1100 loss 7.762638 loss_att 10.444877 loss_ctc 11.552895 loss_rnnt 6.479002 hw_loss 0.453413 lr 0.00043099 rank 7
2023-02-23 07:43:13,077 DEBUG TRAIN Batch 16/1100 loss 10.287207 loss_att 15.711025 loss_ctc 10.818975 loss_rnnt 8.846684 hw_loss 0.534105 lr 0.00043108 rank 1
2023-02-23 07:44:25,486 DEBUG TRAIN Batch 16/1200 loss 6.696220 loss_att 8.625820 loss_ctc 7.274476 loss_rnnt 5.912782 hw_loss 0.600783 lr 0.00043083 rank 7
2023-02-23 07:44:25,490 DEBUG TRAIN Batch 16/1200 loss 8.140352 loss_att 9.999542 loss_ctc 10.297144 loss_rnnt 7.200945 hw_loss 0.524994 lr 0.00043089 rank 6
2023-02-23 07:44:25,490 DEBUG TRAIN Batch 16/1200 loss 12.586211 loss_att 14.145967 loss_ctc 17.492790 loss_rnnt 11.327271 hw_loss 0.548961 lr 0.00043092 rank 1
2023-02-23 07:44:25,490 DEBUG TRAIN Batch 16/1200 loss 18.279259 loss_att 18.836000 loss_ctc 23.716772 loss_rnnt 17.198469 hw_loss 0.458325 lr 0.00043084 rank 3
2023-02-23 07:44:25,493 DEBUG TRAIN Batch 16/1200 loss 14.977010 loss_att 16.854622 loss_ctc 19.857441 loss_rnnt 13.739276 hw_loss 0.396537 lr 0.00043090 rank 2
2023-02-23 07:44:25,493 DEBUG TRAIN Batch 16/1200 loss 9.928923 loss_att 13.051388 loss_ctc 12.724276 loss_rnnt 8.695024 hw_loss 0.443797 lr 0.00043082 rank 5
2023-02-23 07:44:25,494 DEBUG TRAIN Batch 16/1200 loss 14.831477 loss_att 16.944546 loss_ctc 21.633595 loss_rnnt 13.248695 hw_loss 0.474785 lr 0.00043091 rank 0
2023-02-23 07:44:25,497 DEBUG TRAIN Batch 16/1200 loss 11.650146 loss_att 16.369631 loss_ctc 18.318027 loss_rnnt 9.590558 hw_loss 0.424953 lr 0.00043089 rank 4
2023-02-23 07:45:37,774 DEBUG TRAIN Batch 16/1300 loss 15.953422 loss_att 19.300400 loss_ctc 20.493942 loss_rnnt 14.463268 hw_loss 0.403788 lr 0.00043068 rank 3
2023-02-23 07:45:37,779 DEBUG TRAIN Batch 16/1300 loss 22.388870 loss_att 29.441158 loss_ctc 32.203918 loss_rnnt 19.384899 hw_loss 0.534077 lr 0.00043076 rank 1
2023-02-23 07:45:37,780 DEBUG TRAIN Batch 16/1300 loss 11.302735 loss_att 13.449501 loss_ctc 15.283712 loss_rnnt 10.138676 hw_loss 0.382330 lr 0.00043073 rank 6
2023-02-23 07:45:37,780 DEBUG TRAIN Batch 16/1300 loss 6.114283 loss_att 10.270001 loss_ctc 6.879992 loss_rnnt 4.877550 hw_loss 0.569053 lr 0.00043067 rank 7
2023-02-23 07:45:37,780 DEBUG TRAIN Batch 16/1300 loss 10.441120 loss_att 10.977299 loss_ctc 13.241061 loss_rnnt 9.642628 hw_loss 0.596120 lr 0.00043073 rank 4
2023-02-23 07:45:37,782 DEBUG TRAIN Batch 16/1300 loss 10.302690 loss_att 12.179153 loss_ctc 17.909611 loss_rnnt 8.693475 hw_loss 0.411873 lr 0.00043074 rank 2
2023-02-23 07:45:37,783 DEBUG TRAIN Batch 16/1300 loss 7.300063 loss_att 8.999605 loss_ctc 9.216218 loss_rnnt 6.496383 hw_loss 0.390533 lr 0.00043066 rank 5
2023-02-23 07:45:37,787 DEBUG TRAIN Batch 16/1300 loss 10.396020 loss_att 15.712513 loss_ctc 12.213043 loss_rnnt 8.867305 hw_loss 0.418402 lr 0.00043075 rank 0
2023-02-23 07:46:52,836 DEBUG TRAIN Batch 16/1400 loss 11.238439 loss_att 12.257895 loss_ctc 14.065166 loss_rnnt 10.449358 hw_loss 0.390549 lr 0.00043052 rank 3
2023-02-23 07:46:52,838 DEBUG TRAIN Batch 16/1400 loss 7.205089 loss_att 9.957773 loss_ctc 9.208416 loss_rnnt 6.156281 hw_loss 0.433423 lr 0.00043060 rank 1
2023-02-23 07:46:52,848 DEBUG TRAIN Batch 16/1400 loss 5.790623 loss_att 9.687536 loss_ctc 11.978488 loss_rnnt 3.995242 hw_loss 0.358031 lr 0.00043051 rank 7
2023-02-23 07:46:52,849 DEBUG TRAIN Batch 16/1400 loss 18.225473 loss_att 21.574856 loss_ctc 30.592321 loss_rnnt 15.744921 hw_loss 0.303302 lr 0.00043059 rank 0
2023-02-23 07:46:52,850 DEBUG TRAIN Batch 16/1400 loss 5.224714 loss_att 9.287582 loss_ctc 7.365692 loss_rnnt 3.934706 hw_loss 0.359944 lr 0.00043058 rank 2
2023-02-23 07:46:52,858 DEBUG TRAIN Batch 16/1400 loss 15.419604 loss_att 17.007488 loss_ctc 18.139868 loss_rnnt 14.531904 hw_loss 0.388915 lr 0.00043050 rank 5
2023-02-23 07:46:52,869 DEBUG TRAIN Batch 16/1400 loss 9.686179 loss_att 13.875957 loss_ctc 14.795542 loss_rnnt 7.974361 hw_loss 0.361150 lr 0.00043057 rank 4
2023-02-23 07:46:52,896 DEBUG TRAIN Batch 16/1400 loss 10.582059 loss_att 15.905909 loss_ctc 11.889040 loss_rnnt 9.124146 hw_loss 0.410399 lr 0.00043057 rank 6
2023-02-23 07:48:06,172 DEBUG TRAIN Batch 16/1500 loss 8.182668 loss_att 10.771358 loss_ctc 12.887903 loss_rnnt 6.774842 hw_loss 0.492603 lr 0.00043035 rank 7
2023-02-23 07:48:06,173 DEBUG TRAIN Batch 16/1500 loss 8.072861 loss_att 12.388968 loss_ctc 14.707835 loss_rnnt 6.092178 hw_loss 0.436495 lr 0.00043036 rank 3
2023-02-23 07:48:06,174 DEBUG TRAIN Batch 16/1500 loss 10.245584 loss_att 12.865824 loss_ctc 13.596458 loss_rnnt 9.103108 hw_loss 0.321832 lr 0.00043041 rank 4
2023-02-23 07:48:06,177 DEBUG TRAIN Batch 16/1500 loss 10.046342 loss_att 13.100336 loss_ctc 13.245646 loss_rnnt 8.810526 hw_loss 0.372080 lr 0.00043043 rank 0
2023-02-23 07:48:06,178 DEBUG TRAIN Batch 16/1500 loss 15.410479 loss_att 18.361221 loss_ctc 20.723320 loss_rnnt 13.864418 hw_loss 0.464124 lr 0.00043042 rank 2
2023-02-23 07:48:06,177 DEBUG TRAIN Batch 16/1500 loss 10.309669 loss_att 13.915511 loss_ctc 16.369837 loss_rnnt 8.538887 hw_loss 0.452983 lr 0.00043044 rank 1
2023-02-23 07:48:06,181 DEBUG TRAIN Batch 16/1500 loss 7.267425 loss_att 11.896910 loss_ctc 11.147703 loss_rnnt 5.633551 hw_loss 0.357389 lr 0.00043041 rank 6
2023-02-23 07:48:06,182 DEBUG TRAIN Batch 16/1500 loss 12.383878 loss_att 14.793244 loss_ctc 20.575525 loss_rnnt 10.587513 hw_loss 0.416761 lr 0.00043034 rank 5
2023-02-23 07:49:18,473 DEBUG TRAIN Batch 16/1600 loss 5.635366 loss_att 9.737314 loss_ctc 9.657389 loss_rnnt 4.126073 hw_loss 0.286188 lr 0.00043019 rank 7
2023-02-23 07:49:18,478 DEBUG TRAIN Batch 16/1600 loss 8.373102 loss_att 12.927180 loss_ctc 9.483187 loss_rnnt 7.107827 hw_loss 0.387089 lr 0.00043025 rank 6
2023-02-23 07:49:18,480 DEBUG TRAIN Batch 16/1600 loss 8.389478 loss_att 12.321011 loss_ctc 11.443922 loss_rnnt 7.031556 hw_loss 0.308168 lr 0.00043018 rank 5
2023-02-23 07:49:18,481 DEBUG TRAIN Batch 16/1600 loss 4.943224 loss_att 7.600736 loss_ctc 7.498309 loss_rnnt 3.870080 hw_loss 0.376807 lr 0.00043020 rank 3
2023-02-23 07:49:18,481 DEBUG TRAIN Batch 16/1600 loss 5.130416 loss_att 9.094833 loss_ctc 9.499385 loss_rnnt 3.510227 hw_loss 0.458957 lr 0.00043027 rank 0
2023-02-23 07:49:18,483 DEBUG TRAIN Batch 16/1600 loss 8.992004 loss_att 12.281721 loss_ctc 15.714070 loss_rnnt 7.263336 hw_loss 0.327092 lr 0.00043025 rank 4
2023-02-23 07:49:18,484 DEBUG TRAIN Batch 16/1600 loss 12.102406 loss_att 14.518264 loss_ctc 17.870468 loss_rnnt 10.667688 hw_loss 0.342131 lr 0.00043026 rank 2
2023-02-23 07:49:18,484 DEBUG TRAIN Batch 16/1600 loss 6.283875 loss_att 9.282278 loss_ctc 9.584991 loss_rnnt 5.045278 hw_loss 0.372689 lr 0.00043029 rank 1
2023-02-23 07:50:31,210 DEBUG TRAIN Batch 16/1700 loss 6.433435 loss_att 8.350690 loss_ctc 7.324413 loss_rnnt 5.707206 hw_loss 0.419966 lr 0.00043013 rank 1
2023-02-23 07:50:31,211 DEBUG TRAIN Batch 16/1700 loss 12.632301 loss_att 13.753220 loss_ctc 15.376863 loss_rnnt 11.816740 hw_loss 0.422692 lr 0.00043003 rank 7
2023-02-23 07:50:31,213 DEBUG TRAIN Batch 16/1700 loss 16.612152 loss_att 18.154922 loss_ctc 24.353424 loss_rnnt 14.992905 hw_loss 0.522233 lr 0.00043002 rank 5
2023-02-23 07:50:31,212 DEBUG TRAIN Batch 16/1700 loss 14.294181 loss_att 15.753098 loss_ctc 17.144329 loss_rnnt 13.447991 hw_loss 0.326975 lr 0.00043009 rank 4
2023-02-23 07:50:31,214 DEBUG TRAIN Batch 16/1700 loss 18.337585 loss_att 19.748791 loss_ctc 24.419899 loss_rnnt 17.023020 hw_loss 0.415030 lr 0.00043004 rank 3
2023-02-23 07:50:31,216 DEBUG TRAIN Batch 16/1700 loss 14.207747 loss_att 17.332439 loss_ctc 14.969310 loss_rnnt 13.296764 hw_loss 0.345944 lr 0.00043010 rank 2
2023-02-23 07:50:31,217 DEBUG TRAIN Batch 16/1700 loss 18.737534 loss_att 20.679907 loss_ctc 25.105366 loss_rnnt 17.268089 hw_loss 0.434859 lr 0.00043011 rank 0
2023-02-23 07:50:31,262 DEBUG TRAIN Batch 16/1700 loss 13.378383 loss_att 19.132181 loss_ctc 14.833653 loss_rnnt 11.811722 hw_loss 0.415998 lr 0.00043010 rank 6
2023-02-23 07:51:46,546 DEBUG TRAIN Batch 16/1800 loss 11.674279 loss_att 14.874987 loss_ctc 14.827684 loss_rnnt 10.344677 hw_loss 0.504388 lr 0.00042987 rank 7
2023-02-23 07:51:46,554 DEBUG TRAIN Batch 16/1800 loss 8.214630 loss_att 11.704907 loss_ctc 12.018042 loss_rnnt 6.773368 hw_loss 0.442661 lr 0.00042993 rank 4
2023-02-23 07:51:46,556 DEBUG TRAIN Batch 16/1800 loss 11.106771 loss_att 13.073198 loss_ctc 15.949836 loss_rnnt 9.838671 hw_loss 0.429512 lr 0.00042988 rank 3
2023-02-23 07:51:46,556 DEBUG TRAIN Batch 16/1800 loss 16.430489 loss_att 16.920992 loss_ctc 24.529295 loss_rnnt 14.990485 hw_loss 0.491366 lr 0.00042995 rank 0
2023-02-23 07:51:46,557 DEBUG TRAIN Batch 16/1800 loss 9.425798 loss_att 11.269226 loss_ctc 13.195382 loss_rnnt 8.371165 hw_loss 0.343756 lr 0.00042994 rank 2
2023-02-23 07:51:46,557 DEBUG TRAIN Batch 16/1800 loss 8.695664 loss_att 8.913998 loss_ctc 8.049934 loss_rnnt 8.446020 hw_loss 0.547639 lr 0.00042986 rank 5
2023-02-23 07:51:46,563 DEBUG TRAIN Batch 16/1800 loss 10.837914 loss_att 12.166401 loss_ctc 13.764412 loss_rnnt 9.913846 hw_loss 0.502822 lr 0.00042994 rank 6
2023-02-23 07:51:46,606 DEBUG TRAIN Batch 16/1800 loss 13.920262 loss_att 14.724031 loss_ctc 20.596033 loss_rnnt 12.592375 hw_loss 0.519432 lr 0.00042997 rank 1
2023-02-23 07:52:59,979 DEBUG TRAIN Batch 16/1900 loss 9.158592 loss_att 13.475651 loss_ctc 16.383444 loss_rnnt 7.145025 hw_loss 0.350328 lr 0.00042970 rank 5
2023-02-23 07:52:59,995 DEBUG TRAIN Batch 16/1900 loss 9.464290 loss_att 9.941390 loss_ctc 11.862675 loss_rnnt 8.750845 hw_loss 0.559198 lr 0.00042978 rank 2
2023-02-23 07:52:59,995 DEBUG TRAIN Batch 16/1900 loss 7.459741 loss_att 9.350704 loss_ctc 9.337331 loss_rnnt 6.590365 hw_loss 0.451571 lr 0.00042977 rank 4
2023-02-23 07:52:59,996 DEBUG TRAIN Batch 16/1900 loss 12.688104 loss_att 13.242863 loss_ctc 16.619164 loss_rnnt 11.822948 hw_loss 0.431368 lr 0.00042972 rank 3
2023-02-23 07:52:59,996 DEBUG TRAIN Batch 16/1900 loss 4.328475 loss_att 8.912189 loss_ctc 7.129691 loss_rnnt 2.739133 hw_loss 0.560819 lr 0.00042972 rank 7
2023-02-23 07:52:59,997 DEBUG TRAIN Batch 16/1900 loss 5.156473 loss_att 8.214472 loss_ctc 7.843957 loss_rnnt 3.960986 hw_loss 0.422918 lr 0.00042981 rank 1
2023-02-23 07:52:59,999 DEBUG TRAIN Batch 16/1900 loss 25.505369 loss_att 27.580057 loss_ctc 34.067524 loss_rnnt 23.795620 hw_loss 0.287238 lr 0.00042978 rank 6
2023-02-23 07:53:00,045 DEBUG TRAIN Batch 16/1900 loss 15.176715 loss_att 14.763763 loss_ctc 21.791512 loss_rnnt 14.082436 hw_loss 0.552930 lr 0.00042980 rank 0
2023-02-23 07:54:12,116 DEBUG TRAIN Batch 16/2000 loss 8.215334 loss_att 11.033002 loss_ctc 9.702009 loss_rnnt 7.249550 hw_loss 0.382550 lr 0.00042956 rank 7
2023-02-23 07:54:12,119 DEBUG TRAIN Batch 16/2000 loss 7.102310 loss_att 10.013737 loss_ctc 11.092044 loss_rnnt 5.743103 hw_loss 0.459297 lr 0.00042963 rank 2
2023-02-23 07:54:12,121 DEBUG TRAIN Batch 16/2000 loss 8.932662 loss_att 10.782349 loss_ctc 13.411575 loss_rnnt 7.762807 hw_loss 0.380118 lr 0.00042962 rank 4
2023-02-23 07:54:12,122 DEBUG TRAIN Batch 16/2000 loss 8.740360 loss_att 10.890997 loss_ctc 9.885340 loss_rnnt 7.954070 hw_loss 0.381560 lr 0.00042964 rank 0
2023-02-23 07:54:12,125 DEBUG TRAIN Batch 16/2000 loss 21.155622 loss_att 26.312801 loss_ctc 30.308788 loss_rnnt 18.678558 hw_loss 0.422262 lr 0.00042954 rank 5
2023-02-23 07:54:12,124 DEBUG TRAIN Batch 16/2000 loss 14.391968 loss_att 20.927608 loss_ctc 19.529072 loss_rnnt 12.147078 hw_loss 0.474029 lr 0.00042956 rank 3
2023-02-23 07:54:12,126 DEBUG TRAIN Batch 16/2000 loss 9.455566 loss_att 10.830342 loss_ctc 11.275827 loss_rnnt 8.687313 hw_loss 0.469868 lr 0.00042962 rank 6
2023-02-23 07:54:12,128 DEBUG TRAIN Batch 16/2000 loss 25.368456 loss_att 24.719059 loss_ctc 32.872780 loss_rnnt 24.334778 hw_loss 0.305589 lr 0.00042965 rank 1
2023-02-23 07:55:26,126 DEBUG TRAIN Batch 16/2100 loss 21.909140 loss_att 22.893154 loss_ctc 35.348595 loss_rnnt 19.721222 hw_loss 0.373476 lr 0.00042947 rank 2
2023-02-23 07:55:26,130 DEBUG TRAIN Batch 16/2100 loss 7.934857 loss_att 10.389416 loss_ctc 10.003597 loss_rnnt 6.938245 hw_loss 0.431003 lr 0.00042949 rank 1
2023-02-23 07:55:26,140 DEBUG TRAIN Batch 16/2100 loss 9.203430 loss_att 12.271269 loss_ctc 12.273468 loss_rnnt 7.978366 hw_loss 0.379046 lr 0.00042940 rank 3
2023-02-23 07:55:26,140 DEBUG TRAIN Batch 16/2100 loss 10.150723 loss_att 12.889983 loss_ctc 11.808025 loss_rnnt 9.190396 hw_loss 0.359066 lr 0.00042940 rank 7
2023-02-23 07:55:26,144 DEBUG TRAIN Batch 16/2100 loss 6.220991 loss_att 8.114181 loss_ctc 7.285087 loss_rnnt 5.512465 hw_loss 0.352515 lr 0.00042946 rank 4
2023-02-23 07:55:26,146 DEBUG TRAIN Batch 16/2100 loss 10.081197 loss_att 10.430055 loss_ctc 11.291252 loss_rnnt 9.637210 hw_loss 0.399140 lr 0.00042938 rank 5
2023-02-23 07:55:26,146 DEBUG TRAIN Batch 16/2100 loss 12.256263 loss_att 16.170471 loss_ctc 18.246166 loss_rnnt 10.483820 hw_loss 0.358027 lr 0.00042948 rank 0
2023-02-23 07:55:26,193 DEBUG TRAIN Batch 16/2100 loss 8.679855 loss_att 9.573190 loss_ctc 12.991366 loss_rnnt 7.693675 hw_loss 0.436211 lr 0.00042946 rank 6
2023-02-23 07:56:39,669 DEBUG TRAIN Batch 16/2200 loss 7.106348 loss_att 9.719879 loss_ctc 9.869503 loss_rnnt 5.926369 hw_loss 0.541598 lr 0.00042931 rank 2
2023-02-23 07:56:39,671 DEBUG TRAIN Batch 16/2200 loss 11.803814 loss_att 14.312263 loss_ctc 17.889040 loss_rnnt 10.270738 hw_loss 0.412543 lr 0.00042924 rank 7
2023-02-23 07:56:39,671 DEBUG TRAIN Batch 16/2200 loss 14.265766 loss_att 16.559116 loss_ctc 19.440685 loss_rnnt 12.896906 hw_loss 0.412873 lr 0.00042925 rank 3
2023-02-23 07:56:39,673 DEBUG TRAIN Batch 16/2200 loss 5.060315 loss_att 8.359037 loss_ctc 6.627254 loss_rnnt 4.023860 hw_loss 0.314596 lr 0.00042930 rank 4
2023-02-23 07:56:39,673 DEBUG TRAIN Batch 16/2200 loss 11.426601 loss_att 18.265583 loss_ctc 20.507553 loss_rnnt 8.662313 hw_loss 0.348183 lr 0.00042933 rank 1
2023-02-23 07:56:39,677 DEBUG TRAIN Batch 16/2200 loss 12.657538 loss_att 16.357763 loss_ctc 16.099010 loss_rnnt 11.210184 hw_loss 0.465835 lr 0.00042932 rank 0
2023-02-23 07:56:39,677 DEBUG TRAIN Batch 16/2200 loss 22.912582 loss_att 24.099644 loss_ctc 30.445431 loss_rnnt 21.454756 hw_loss 0.405067 lr 0.00042923 rank 5
2023-02-23 07:56:39,680 DEBUG TRAIN Batch 16/2200 loss 11.522357 loss_att 14.167320 loss_ctc 18.532175 loss_rnnt 9.856106 hw_loss 0.379905 lr 0.00042930 rank 6
2023-02-23 07:57:51,535 DEBUG TRAIN Batch 16/2300 loss 13.148706 loss_att 15.429386 loss_ctc 15.780176 loss_rnnt 12.101340 hw_loss 0.450690 lr 0.00042908 rank 7
2023-02-23 07:57:51,535 DEBUG TRAIN Batch 16/2300 loss 13.122882 loss_att 17.087368 loss_ctc 16.472431 loss_rnnt 11.681870 hw_loss 0.377826 lr 0.00042916 rank 0
2023-02-23 07:57:51,539 DEBUG TRAIN Batch 16/2300 loss 12.303313 loss_att 15.174896 loss_ctc 15.110058 loss_rnnt 11.126615 hw_loss 0.427781 lr 0.00042914 rank 6
2023-02-23 07:57:51,538 DEBUG TRAIN Batch 16/2300 loss 9.688436 loss_att 13.504721 loss_ctc 12.989347 loss_rnnt 8.250926 hw_loss 0.438995 lr 0.00042915 rank 2
2023-02-23 07:57:51,538 DEBUG TRAIN Batch 16/2300 loss 12.728619 loss_att 14.571307 loss_ctc 17.611847 loss_rnnt 11.490122 hw_loss 0.410365 lr 0.00042914 rank 4
2023-02-23 07:57:51,539 DEBUG TRAIN Batch 16/2300 loss 9.291730 loss_att 15.119363 loss_ctc 10.819279 loss_rnnt 7.699199 hw_loss 0.418747 lr 0.00042907 rank 5
2023-02-23 07:57:51,541 DEBUG TRAIN Batch 16/2300 loss 10.557528 loss_att 11.453826 loss_ctc 11.878002 loss_rnnt 10.023195 hw_loss 0.335644 lr 0.00042909 rank 3
2023-02-23 07:57:51,586 DEBUG TRAIN Batch 16/2300 loss 14.458372 loss_att 15.340925 loss_ctc 19.209827 loss_rnnt 13.422659 hw_loss 0.423143 lr 0.00042917 rank 1
2023-02-23 07:59:04,338 DEBUG TRAIN Batch 16/2400 loss 12.611016 loss_att 14.934857 loss_ctc 14.389817 loss_rnnt 11.665751 hw_loss 0.456230 lr 0.00042893 rank 3
2023-02-23 07:59:04,341 DEBUG TRAIN Batch 16/2400 loss 9.756691 loss_att 11.677550 loss_ctc 14.311361 loss_rnnt 8.581148 hw_loss 0.345153 lr 0.00042902 rank 1
2023-02-23 07:59:04,343 DEBUG TRAIN Batch 16/2400 loss 12.516203 loss_att 15.336540 loss_ctc 16.831434 loss_rnnt 11.185518 hw_loss 0.358599 lr 0.00042892 rank 7
2023-02-23 07:59:04,344 DEBUG TRAIN Batch 16/2400 loss 14.930510 loss_att 15.659690 loss_ctc 20.987198 loss_rnnt 13.781713 hw_loss 0.366380 lr 0.00042900 rank 0
2023-02-23 07:59:04,346 DEBUG TRAIN Batch 16/2400 loss 16.091965 loss_att 18.619904 loss_ctc 20.840305 loss_rnnt 14.728617 hw_loss 0.421214 lr 0.00042899 rank 6
2023-02-23 07:59:04,348 DEBUG TRAIN Batch 16/2400 loss 10.697459 loss_att 12.675655 loss_ctc 15.299441 loss_rnnt 9.477152 hw_loss 0.395758 lr 0.00042891 rank 5
2023-02-23 07:59:04,348 DEBUG TRAIN Batch 16/2400 loss 14.227283 loss_att 17.964588 loss_ctc 18.076605 loss_rnnt 12.774999 hw_loss 0.359217 lr 0.00042899 rank 2
2023-02-23 07:59:04,366 DEBUG TRAIN Batch 16/2400 loss 11.905557 loss_att 14.209879 loss_ctc 14.297636 loss_rnnt 10.918641 hw_loss 0.388325 lr 0.00042898 rank 4
2023-02-23 08:00:20,047 DEBUG TRAIN Batch 16/2500 loss 7.237010 loss_att 8.660517 loss_ctc 8.957056 loss_rnnt 6.451888 hw_loss 0.508278 lr 0.00042877 rank 7
2023-02-23 08:00:20,048 DEBUG TRAIN Batch 16/2500 loss 16.289932 loss_att 18.463190 loss_ctc 20.197805 loss_rnnt 15.074385 hw_loss 0.487212 lr 0.00042883 rank 2
2023-02-23 08:00:20,048 DEBUG TRAIN Batch 16/2500 loss 4.655257 loss_att 6.448708 loss_ctc 5.837325 loss_rnnt 3.862636 hw_loss 0.518104 lr 0.00042877 rank 3
2023-02-23 08:00:20,050 DEBUG TRAIN Batch 16/2500 loss 6.394627 loss_att 8.966438 loss_ctc 10.245922 loss_rnnt 5.090819 hw_loss 0.517385 lr 0.00042885 rank 0
2023-02-23 08:00:20,054 DEBUG TRAIN Batch 16/2500 loss 7.027043 loss_att 8.335348 loss_ctc 8.862979 loss_rnnt 6.162831 hw_loss 0.670799 lr 0.00042883 rank 6
2023-02-23 08:00:20,055 DEBUG TRAIN Batch 16/2500 loss 9.324492 loss_att 10.276003 loss_ctc 13.082916 loss_rnnt 8.385352 hw_loss 0.464462 lr 0.00042883 rank 4
2023-02-23 08:00:20,055 DEBUG TRAIN Batch 16/2500 loss 6.619375 loss_att 9.245883 loss_ctc 7.808064 loss_rnnt 5.655077 hw_loss 0.525947 lr 0.00042886 rank 1
2023-02-23 08:00:20,057 DEBUG TRAIN Batch 16/2500 loss 7.474391 loss_att 10.361113 loss_ctc 10.393008 loss_rnnt 6.306431 hw_loss 0.377749 lr 0.00042875 rank 5
2023-02-23 08:01:31,908 DEBUG TRAIN Batch 16/2600 loss 5.575169 loss_att 9.796108 loss_ctc 5.945269 loss_rnnt 4.439188 hw_loss 0.454588 lr 0.00042861 rank 3
2023-02-23 08:01:31,911 DEBUG TRAIN Batch 16/2600 loss 2.624476 loss_att 7.577410 loss_ctc 5.032757 loss_rnnt 1.093158 hw_loss 0.411801 lr 0.00042861 rank 7
2023-02-23 08:01:31,915 DEBUG TRAIN Batch 16/2600 loss 6.113234 loss_att 7.993951 loss_ctc 6.295752 loss_rnnt 5.434548 hw_loss 0.521635 lr 0.00042860 rank 5
2023-02-23 08:01:31,918 DEBUG TRAIN Batch 16/2600 loss 9.714664 loss_att 12.360142 loss_ctc 11.750919 loss_rnnt 8.668629 hw_loss 0.460200 lr 0.00042868 rank 2
2023-02-23 08:01:31,919 DEBUG TRAIN Batch 16/2600 loss 12.573711 loss_att 15.829949 loss_ctc 17.686979 loss_rnnt 11.020217 hw_loss 0.413397 lr 0.00042870 rank 1
2023-02-23 08:01:31,920 DEBUG TRAIN Batch 16/2600 loss 5.848783 loss_att 8.374516 loss_ctc 7.065005 loss_rnnt 4.928874 hw_loss 0.473623 lr 0.00042867 rank 6
2023-02-23 08:01:31,920 DEBUG TRAIN Batch 16/2600 loss 11.485799 loss_att 10.204349 loss_ctc 13.878688 loss_rnnt 11.002894 hw_loss 0.787767 lr 0.00042869 rank 0
2023-02-23 08:01:31,921 DEBUG TRAIN Batch 16/2600 loss 7.595472 loss_att 8.882673 loss_ctc 7.696047 loss_rnnt 7.122037 hw_loss 0.379846 lr 0.00042867 rank 4
2023-02-23 08:02:43,802 DEBUG TRAIN Batch 16/2700 loss 6.795196 loss_att 9.010948 loss_ctc 8.596195 loss_rnnt 5.922114 hw_loss 0.355870 lr 0.00042845 rank 7
2023-02-23 08:02:43,803 DEBUG TRAIN Batch 16/2700 loss 6.706320 loss_att 11.641153 loss_ctc 8.196478 loss_rnnt 5.347742 hw_loss 0.324234 lr 0.00042851 rank 4
2023-02-23 08:02:43,805 DEBUG TRAIN Batch 16/2700 loss 3.520133 loss_att 6.093161 loss_ctc 4.137074 loss_rnnt 2.646190 hw_loss 0.519520 lr 0.00042846 rank 3
2023-02-23 08:02:43,810 DEBUG TRAIN Batch 16/2700 loss 8.866177 loss_att 10.278090 loss_ctc 11.190501 loss_rnnt 8.084069 hw_loss 0.355901 lr 0.00042852 rank 2
2023-02-23 08:02:43,811 DEBUG TRAIN Batch 16/2700 loss 4.301866 loss_att 8.401423 loss_ctc 4.226199 loss_rnnt 3.269399 hw_loss 0.417458 lr 0.00042853 rank 0
2023-02-23 08:02:43,813 DEBUG TRAIN Batch 16/2700 loss 9.222555 loss_att 13.142218 loss_ctc 15.012501 loss_rnnt 7.436465 hw_loss 0.431558 lr 0.00042851 rank 6
2023-02-23 08:02:43,815 DEBUG TRAIN Batch 16/2700 loss 17.303871 loss_att 19.445858 loss_ctc 22.123899 loss_rnnt 15.960326 hw_loss 0.510893 lr 0.00042844 rank 5
2023-02-23 08:02:43,816 DEBUG TRAIN Batch 16/2700 loss 7.779564 loss_att 10.837205 loss_ctc 9.269988 loss_rnnt 6.769079 hw_loss 0.375439 lr 0.00042854 rank 1
2023-02-23 08:03:57,769 DEBUG TRAIN Batch 16/2800 loss 7.470190 loss_att 10.337936 loss_ctc 9.248026 loss_rnnt 6.446764 hw_loss 0.399059 lr 0.00042828 rank 5
2023-02-23 08:03:57,771 DEBUG TRAIN Batch 16/2800 loss 8.808863 loss_att 13.472239 loss_ctc 13.681610 loss_rnnt 7.004580 hw_loss 0.416077 lr 0.00042829 rank 7
2023-02-23 08:03:57,771 DEBUG TRAIN Batch 16/2800 loss 10.049562 loss_att 14.222179 loss_ctc 14.312796 loss_rnnt 8.461535 hw_loss 0.347011 lr 0.00042835 rank 4
2023-02-23 08:03:57,772 DEBUG TRAIN Batch 16/2800 loss 9.418656 loss_att 11.346779 loss_ctc 10.663328 loss_rnnt 8.675374 hw_loss 0.359439 lr 0.00042836 rank 2
2023-02-23 08:03:57,776 DEBUG TRAIN Batch 16/2800 loss 16.267046 loss_att 18.552673 loss_ctc 21.674976 loss_rnnt 14.864458 hw_loss 0.420758 lr 0.00042837 rank 0
2023-02-23 08:03:57,777 DEBUG TRAIN Batch 16/2800 loss 8.832388 loss_att 12.692600 loss_ctc 10.581861 loss_rnnt 7.616125 hw_loss 0.395545 lr 0.00042830 rank 3
2023-02-23 08:03:57,779 DEBUG TRAIN Batch 16/2800 loss 7.499849 loss_att 10.016859 loss_ctc 11.552542 loss_rnnt 6.256677 hw_loss 0.373894 lr 0.00042836 rank 6
2023-02-23 08:03:57,782 DEBUG TRAIN Batch 16/2800 loss 24.488684 loss_att 28.115475 loss_ctc 30.122641 loss_rnnt 22.768080 hw_loss 0.457597 lr 0.00042839 rank 1
2023-02-23 08:05:10,847 DEBUG TRAIN Batch 16/2900 loss 10.917425 loss_att 12.805705 loss_ctc 12.759253 loss_rnnt 10.056306 hw_loss 0.446035 lr 0.00042821 rank 2
2023-02-23 08:05:10,849 DEBUG TRAIN Batch 16/2900 loss 7.547202 loss_att 9.389757 loss_ctc 8.782714 loss_rnnt 6.780494 hw_loss 0.437741 lr 0.00042814 rank 7
2023-02-23 08:05:10,848 DEBUG TRAIN Batch 16/2900 loss 15.052202 loss_att 17.540445 loss_ctc 21.340889 loss_rnnt 13.497686 hw_loss 0.409454 lr 0.00042814 rank 3
2023-02-23 08:05:10,856 DEBUG TRAIN Batch 16/2900 loss 23.598848 loss_att 24.526810 loss_ctc 33.768143 loss_rnnt 21.874960 hw_loss 0.341983 lr 0.00042820 rank 6
2023-02-23 08:05:10,858 DEBUG TRAIN Batch 16/2900 loss 7.211030 loss_att 10.203400 loss_ctc 8.546009 loss_rnnt 6.252904 hw_loss 0.340601 lr 0.00042822 rank 0
2023-02-23 08:05:10,859 DEBUG TRAIN Batch 16/2900 loss 15.457748 loss_att 19.330704 loss_ctc 24.225731 loss_rnnt 13.319845 hw_loss 0.364215 lr 0.00042820 rank 4
2023-02-23 08:05:10,861 DEBUG TRAIN Batch 16/2900 loss 15.077799 loss_att 15.877704 loss_ctc 19.380409 loss_rnnt 14.113943 hw_loss 0.431613 lr 0.00042812 rank 5
2023-02-23 08:05:10,902 DEBUG TRAIN Batch 16/2900 loss 5.081542 loss_att 7.615073 loss_ctc 5.646533 loss_rnnt 4.278223 hw_loss 0.414902 lr 0.00042823 rank 1
2023-02-23 08:06:23,386 DEBUG TRAIN Batch 16/3000 loss 7.852009 loss_att 9.525839 loss_ctc 7.892569 loss_rnnt 7.296798 hw_loss 0.403193 lr 0.00042804 rank 6
2023-02-23 08:06:23,391 DEBUG TRAIN Batch 16/3000 loss 4.189384 loss_att 7.809966 loss_ctc 6.922119 loss_rnnt 2.918213 hw_loss 0.342542 lr 0.00042806 rank 0
2023-02-23 08:06:23,397 DEBUG TRAIN Batch 16/3000 loss 5.817074 loss_att 7.435069 loss_ctc 9.074176 loss_rnnt 4.857181 hw_loss 0.378776 lr 0.00042805 rank 2
2023-02-23 08:06:23,397 DEBUG TRAIN Batch 16/3000 loss 11.682148 loss_att 15.106498 loss_ctc 14.600511 loss_rnnt 10.357263 hw_loss 0.470435 lr 0.00042798 rank 7
2023-02-23 08:06:23,400 DEBUG TRAIN Batch 16/3000 loss 16.941027 loss_att 20.431807 loss_ctc 28.432865 loss_rnnt 14.511314 hw_loss 0.373706 lr 0.00042804 rank 4
2023-02-23 08:06:23,400 DEBUG TRAIN Batch 16/3000 loss 10.885069 loss_att 14.164660 loss_ctc 16.985102 loss_rnnt 9.208320 hw_loss 0.389051 lr 0.00042799 rank 3
2023-02-23 08:06:23,400 DEBUG TRAIN Batch 16/3000 loss 19.495386 loss_att 20.763243 loss_ctc 26.725866 loss_rnnt 18.028803 hw_loss 0.466775 lr 0.00042797 rank 5
2023-02-23 08:06:23,402 DEBUG TRAIN Batch 16/3000 loss 6.589628 loss_att 9.461637 loss_ctc 9.510070 loss_rnnt 5.388173 hw_loss 0.445614 lr 0.00042807 rank 1
2023-02-23 08:07:35,603 DEBUG TRAIN Batch 16/3100 loss 12.973816 loss_att 15.246675 loss_ctc 18.468685 loss_rnnt 11.500129 hw_loss 0.537123 lr 0.00042792 rank 1
2023-02-23 08:07:35,604 DEBUG TRAIN Batch 16/3100 loss 17.445644 loss_att 18.861355 loss_ctc 27.393488 loss_rnnt 15.534485 hw_loss 0.565570 lr 0.00042789 rank 6
2023-02-23 08:07:35,605 DEBUG TRAIN Batch 16/3100 loss 13.796900 loss_att 15.831570 loss_ctc 19.153221 loss_rnnt 12.403254 hw_loss 0.511002 lr 0.00042789 rank 2
2023-02-23 08:07:35,606 DEBUG TRAIN Batch 16/3100 loss 6.784403 loss_att 10.463198 loss_ctc 11.561287 loss_rnnt 5.139393 hw_loss 0.510624 lr 0.00042782 rank 7
2023-02-23 08:07:35,607 DEBUG TRAIN Batch 16/3100 loss 9.316430 loss_att 11.713781 loss_ctc 13.004504 loss_rnnt 8.060367 hw_loss 0.534093 lr 0.00042790 rank 0
2023-02-23 08:07:35,608 DEBUG TRAIN Batch 16/3100 loss 14.944898 loss_att 16.590757 loss_ctc 21.461521 loss_rnnt 13.401340 hw_loss 0.647814 lr 0.00042781 rank 5
2023-02-23 08:07:35,609 DEBUG TRAIN Batch 16/3100 loss 7.694510 loss_att 9.346720 loss_ctc 10.719874 loss_rnnt 6.666397 hw_loss 0.551793 lr 0.00042783 rank 3
2023-02-23 08:07:35,639 DEBUG TRAIN Batch 16/3100 loss 10.160993 loss_att 14.979422 loss_ctc 14.768188 loss_rnnt 8.351017 hw_loss 0.434992 lr 0.00042788 rank 4
2023-02-23 08:08:51,014 DEBUG TRAIN Batch 16/3200 loss 10.782195 loss_att 11.367253 loss_ctc 14.313128 loss_rnnt 9.826456 hw_loss 0.689881 lr 0.00042767 rank 3
2023-02-23 08:08:51,015 DEBUG TRAIN Batch 16/3200 loss 18.874559 loss_att 20.718052 loss_ctc 24.074631 loss_rnnt 17.553717 hw_loss 0.485251 lr 0.00042767 rank 7
2023-02-23 08:08:51,016 DEBUG TRAIN Batch 16/3200 loss 8.226570 loss_att 7.941684 loss_ctc 9.418178 loss_rnnt 7.740992 hw_loss 0.719389 lr 0.00042773 rank 6
2023-02-23 08:08:51,016 DEBUG TRAIN Batch 16/3200 loss 6.449723 loss_att 11.647792 loss_ctc 10.249989 loss_rnnt 4.727725 hw_loss 0.329403 lr 0.00042773 rank 2
2023-02-23 08:08:51,017 DEBUG TRAIN Batch 16/3200 loss 6.670033 loss_att 8.100666 loss_ctc 9.724049 loss_rnnt 5.666681 hw_loss 0.581296 lr 0.00042773 rank 4
2023-02-23 08:08:51,020 DEBUG TRAIN Batch 16/3200 loss 12.858988 loss_att 14.478504 loss_ctc 17.231533 loss_rnnt 11.672129 hw_loss 0.524904 lr 0.00042775 rank 0
2023-02-23 08:08:51,022 DEBUG TRAIN Batch 16/3200 loss 15.718598 loss_att 19.709490 loss_ctc 18.717159 loss_rnnt 14.298950 hw_loss 0.415617 lr 0.00042765 rank 5
2023-02-23 08:08:51,100 DEBUG TRAIN Batch 16/3200 loss 13.892599 loss_att 20.587906 loss_ctc 18.307243 loss_rnnt 11.816195 hw_loss 0.278855 lr 0.00042776 rank 1
2023-02-23 08:10:03,306 DEBUG TRAIN Batch 16/3300 loss 20.791428 loss_att 24.473488 loss_ctc 24.955748 loss_rnnt 19.317211 hw_loss 0.342300 lr 0.00042757 rank 4
2023-02-23 08:10:03,307 DEBUG TRAIN Batch 16/3300 loss 11.108284 loss_att 15.452724 loss_ctc 18.283276 loss_rnnt 9.122458 hw_loss 0.300511 lr 0.00042759 rank 0
2023-02-23 08:10:03,309 DEBUG TRAIN Batch 16/3300 loss 16.574152 loss_att 19.736412 loss_ctc 25.862377 loss_rnnt 14.511541 hw_loss 0.359491 lr 0.00042752 rank 3
2023-02-23 08:10:03,311 DEBUG TRAIN Batch 16/3300 loss 8.748152 loss_att 14.254646 loss_ctc 13.156128 loss_rnnt 6.880642 hw_loss 0.334652 lr 0.00042751 rank 7
2023-02-23 08:10:03,314 DEBUG TRAIN Batch 16/3300 loss 8.707543 loss_att 10.508631 loss_ctc 10.784107 loss_rnnt 7.888240 hw_loss 0.341647 lr 0.00042758 rank 2
2023-02-23 08:10:03,315 DEBUG TRAIN Batch 16/3300 loss 18.399593 loss_att 18.469120 loss_ctc 21.867851 loss_rnnt 17.709946 hw_loss 0.399951 lr 0.00042757 rank 6
2023-02-23 08:10:03,317 DEBUG TRAIN Batch 16/3300 loss 6.789664 loss_att 8.887451 loss_ctc 6.429014 loss_rnnt 6.205229 hw_loss 0.399308 lr 0.00042750 rank 5
2023-02-23 08:10:03,318 DEBUG TRAIN Batch 16/3300 loss 8.762877 loss_att 15.266649 loss_ctc 9.804831 loss_rnnt 7.137293 hw_loss 0.348565 lr 0.00042760 rank 1
2023-02-23 08:11:15,595 DEBUG TRAIN Batch 16/3400 loss 32.398605 loss_att 35.042740 loss_ctc 31.627890 loss_rnnt 31.763985 hw_loss 0.391047 lr 0.00042736 rank 7
2023-02-23 08:11:15,601 DEBUG TRAIN Batch 16/3400 loss 8.579292 loss_att 11.747644 loss_ctc 13.355717 loss_rnnt 7.113326 hw_loss 0.366450 lr 0.00042745 rank 1
2023-02-23 08:11:15,604 DEBUG TRAIN Batch 16/3400 loss 13.189679 loss_att 16.507252 loss_ctc 18.764208 loss_rnnt 11.558463 hw_loss 0.420808 lr 0.00042736 rank 3
2023-02-23 08:11:15,605 DEBUG TRAIN Batch 16/3400 loss 7.086695 loss_att 11.518243 loss_ctc 11.602997 loss_rnnt 5.399681 hw_loss 0.372244 lr 0.00042742 rank 2
2023-02-23 08:11:15,606 DEBUG TRAIN Batch 16/3400 loss 4.623545 loss_att 8.009179 loss_ctc 10.473330 loss_rnnt 2.921719 hw_loss 0.458863 lr 0.00042741 rank 4
2023-02-23 08:11:15,608 DEBUG TRAIN Batch 16/3400 loss 14.002234 loss_att 18.442516 loss_ctc 19.547527 loss_rnnt 12.088120 hw_loss 0.537537 lr 0.00042734 rank 5
2023-02-23 08:11:15,610 DEBUG TRAIN Batch 16/3400 loss 7.748813 loss_att 9.997416 loss_ctc 9.397124 loss_rnnt 6.850464 hw_loss 0.429099 lr 0.00042743 rank 0
2023-02-23 08:11:15,612 DEBUG TRAIN Batch 16/3400 loss 10.748245 loss_att 13.048268 loss_ctc 12.380018 loss_rnnt 9.873213 hw_loss 0.370232 lr 0.00042742 rank 6
2023-02-23 08:12:28,638 DEBUG TRAIN Batch 16/3500 loss 16.712265 loss_att 20.901791 loss_ctc 22.612640 loss_rnnt 14.874100 hw_loss 0.400395 lr 0.00042726 rank 4
2023-02-23 08:12:28,638 DEBUG TRAIN Batch 16/3500 loss 5.941403 loss_att 8.200868 loss_ctc 8.233870 loss_rnnt 5.005519 hw_loss 0.334368 lr 0.00042727 rank 2
2023-02-23 08:12:28,643 DEBUG TRAIN Batch 16/3500 loss 14.133319 loss_att 15.782665 loss_ctc 18.712677 loss_rnnt 13.022778 hw_loss 0.318921 lr 0.00042719 rank 5
2023-02-23 08:12:28,653 DEBUG TRAIN Batch 16/3500 loss 7.920915 loss_att 11.844267 loss_ctc 11.197317 loss_rnnt 6.466249 hw_loss 0.437141 lr 0.00042720 rank 7
2023-02-23 08:12:28,655 DEBUG TRAIN Batch 16/3500 loss 8.253629 loss_att 13.624526 loss_ctc 15.700239 loss_rnnt 5.988200 hw_loss 0.371940 lr 0.00042729 rank 1
2023-02-23 08:12:28,659 DEBUG TRAIN Batch 16/3500 loss 12.672309 loss_att 19.232903 loss_ctc 19.137447 loss_rnnt 10.295710 hw_loss 0.379618 lr 0.00042720 rank 3
2023-02-23 08:12:28,661 DEBUG TRAIN Batch 16/3500 loss 24.301483 loss_att 27.822655 loss_ctc 26.687490 loss_rnnt 23.050102 hw_loss 0.429396 lr 0.00042728 rank 0
2023-02-23 08:12:28,678 DEBUG TRAIN Batch 16/3500 loss 6.506584 loss_att 10.447309 loss_ctc 10.546047 loss_rnnt 4.968570 hw_loss 0.396139 lr 0.00042726 rank 6
2023-02-23 08:13:42,429 DEBUG TRAIN Batch 16/3600 loss 9.839666 loss_att 11.504929 loss_ctc 12.939766 loss_rnnt 8.911263 hw_loss 0.341259 lr 0.00042713 rank 1
2023-02-23 08:13:42,443 DEBUG TRAIN Batch 16/3600 loss 11.848365 loss_att 14.412940 loss_ctc 16.644335 loss_rnnt 10.484198 hw_loss 0.397102 lr 0.00042704 rank 7
2023-02-23 08:13:42,448 DEBUG TRAIN Batch 16/3600 loss 13.443433 loss_att 16.424568 loss_ctc 23.131676 loss_rnnt 11.331928 hw_loss 0.419084 lr 0.00042711 rank 2
2023-02-23 08:13:42,448 DEBUG TRAIN Batch 16/3600 loss 14.470235 loss_att 17.820183 loss_ctc 19.640066 loss_rnnt 12.879614 hw_loss 0.433730 lr 0.00042703 rank 5
2023-02-23 08:13:42,449 DEBUG TRAIN Batch 16/3600 loss 10.651603 loss_att 15.252756 loss_ctc 12.397792 loss_rnnt 9.304203 hw_loss 0.364393 lr 0.00042712 rank 0
2023-02-23 08:13:42,451 DEBUG TRAIN Batch 16/3600 loss 7.164518 loss_att 10.878916 loss_ctc 11.762901 loss_rnnt 5.577196 hw_loss 0.433734 lr 0.00042710 rank 4
2023-02-23 08:13:42,457 DEBUG TRAIN Batch 16/3600 loss 7.882965 loss_att 10.064363 loss_ctc 9.796507 loss_rnnt 6.987069 hw_loss 0.383395 lr 0.00042705 rank 3
2023-02-23 08:13:42,496 DEBUG TRAIN Batch 16/3600 loss 22.781307 loss_att 25.353025 loss_ctc 27.876537 loss_rnnt 21.385187 hw_loss 0.379525 lr 0.00042710 rank 6
2023-02-23 08:14:55,110 DEBUG TRAIN Batch 16/3700 loss 12.158248 loss_att 15.030729 loss_ctc 15.453986 loss_rnnt 10.926380 hw_loss 0.408637 lr 0.00042695 rank 4
2023-02-23 08:14:55,112 DEBUG TRAIN Batch 16/3700 loss 7.464209 loss_att 9.040560 loss_ctc 10.165026 loss_rnnt 6.582791 hw_loss 0.386322 lr 0.00042689 rank 7
2023-02-23 08:14:55,115 DEBUG TRAIN Batch 16/3700 loss 8.134931 loss_att 12.262039 loss_ctc 13.790253 loss_rnnt 6.331844 hw_loss 0.419291 lr 0.00042689 rank 3
2023-02-23 08:14:55,115 DEBUG TRAIN Batch 16/3700 loss 5.241258 loss_att 6.363625 loss_ctc 7.492778 loss_rnnt 4.517150 hw_loss 0.373935 lr 0.00042698 rank 1
2023-02-23 08:14:55,115 DEBUG TRAIN Batch 16/3700 loss 7.329205 loss_att 8.337875 loss_ctc 9.096210 loss_rnnt 6.631187 hw_loss 0.488779 lr 0.00042695 rank 2
2023-02-23 08:14:55,119 DEBUG TRAIN Batch 16/3700 loss 6.995287 loss_att 9.656842 loss_ctc 9.639923 loss_rnnt 5.856374 hw_loss 0.476220 lr 0.00042697 rank 0
2023-02-23 08:14:55,129 DEBUG TRAIN Batch 16/3700 loss 13.730145 loss_att 14.799193 loss_ctc 16.086287 loss_rnnt 12.952365 hw_loss 0.468411 lr 0.00042687 rank 5
2023-02-23 08:14:55,128 DEBUG TRAIN Batch 16/3700 loss 14.156929 loss_att 17.437763 loss_ctc 22.763140 loss_rnnt 12.115601 hw_loss 0.445626 lr 0.00042695 rank 6
2023-02-23 08:16:08,095 DEBUG TRAIN Batch 16/3800 loss 6.429876 loss_att 13.357525 loss_ctc 10.596010 loss_rnnt 4.291771 hw_loss 0.369544 lr 0.00042672 rank 5
2023-02-23 08:16:08,109 DEBUG TRAIN Batch 16/3800 loss 9.304650 loss_att 8.501313 loss_ctc 10.632300 loss_rnnt 8.957628 hw_loss 0.620004 lr 0.00042673 rank 7
2023-02-23 08:16:08,115 DEBUG TRAIN Batch 16/3800 loss 15.916739 loss_att 16.412701 loss_ctc 22.263474 loss_rnnt 14.749435 hw_loss 0.416027 lr 0.00042679 rank 4
2023-02-23 08:16:08,118 DEBUG TRAIN Batch 16/3800 loss 10.630627 loss_att 9.897959 loss_ctc 11.771297 loss_rnnt 10.226948 hw_loss 0.746481 lr 0.00042680 rank 2
2023-02-23 08:16:08,120 DEBUG TRAIN Batch 16/3800 loss 6.550556 loss_att 8.501031 loss_ctc 8.412801 loss_rnnt 5.656698 hw_loss 0.478994 lr 0.00042681 rank 0
2023-02-23 08:16:08,124 DEBUG TRAIN Batch 16/3800 loss 15.790068 loss_att 15.984944 loss_ctc 22.022701 loss_rnnt 14.641820 hw_loss 0.521728 lr 0.00042674 rank 3
2023-02-23 08:16:08,125 DEBUG TRAIN Batch 16/3800 loss 9.492300 loss_att 10.317246 loss_ctc 12.229877 loss_rnnt 8.721386 hw_loss 0.451715 lr 0.00042682 rank 1
2023-02-23 08:16:08,163 DEBUG TRAIN Batch 16/3800 loss 7.270074 loss_att 7.377880 loss_ctc 8.260471 loss_rnnt 6.779970 hw_loss 0.630920 lr 0.00042679 rank 6
2023-02-23 08:17:22,834 DEBUG TRAIN Batch 16/3900 loss 17.525858 loss_att 15.915718 loss_ctc 26.662014 loss_rnnt 16.387184 hw_loss 0.454774 lr 0.00042656 rank 5
2023-02-23 08:17:22,841 DEBUG TRAIN Batch 16/3900 loss 7.170548 loss_att 11.221538 loss_ctc 8.750853 loss_rnnt 5.978754 hw_loss 0.320417 lr 0.00042658 rank 3
2023-02-23 08:17:22,846 DEBUG TRAIN Batch 16/3900 loss 20.479679 loss_att 22.358881 loss_ctc 21.765701 loss_rnnt 19.733065 hw_loss 0.373692 lr 0.00042664 rank 2
2023-02-23 08:17:22,845 DEBUG TRAIN Batch 16/3900 loss 8.870841 loss_att 11.766195 loss_ctc 13.992598 loss_rnnt 7.470671 hw_loss 0.259121 lr 0.00042664 rank 6
2023-02-23 08:17:22,846 DEBUG TRAIN Batch 16/3900 loss 16.046446 loss_att 20.654560 loss_ctc 21.406906 loss_rnnt 14.227336 hw_loss 0.342673 lr 0.00042663 rank 4
2023-02-23 08:17:22,849 DEBUG TRAIN Batch 16/3900 loss 11.087069 loss_att 15.128582 loss_ctc 17.812870 loss_rnnt 9.175940 hw_loss 0.386350 lr 0.00042667 rank 1
2023-02-23 08:17:22,868 DEBUG TRAIN Batch 16/3900 loss 5.433436 loss_att 9.109581 loss_ctc 8.884646 loss_rnnt 4.032578 hw_loss 0.385251 lr 0.00042658 rank 7
2023-02-23 08:17:22,891 DEBUG TRAIN Batch 16/3900 loss 17.471819 loss_att 16.218771 loss_ctc 20.217010 loss_rnnt 16.989363 hw_loss 0.688199 lr 0.00042665 rank 0
2023-02-23 08:18:36,302 DEBUG TRAIN Batch 16/4000 loss 5.610460 loss_att 8.475872 loss_ctc 7.780787 loss_rnnt 4.493261 hw_loss 0.477637 lr 0.00042648 rank 4
2023-02-23 08:18:36,313 DEBUG TRAIN Batch 16/4000 loss 11.901067 loss_att 13.066303 loss_ctc 12.840506 loss_rnnt 11.342316 hw_loss 0.375834 lr 0.00042642 rank 7
2023-02-23 08:18:36,319 DEBUG TRAIN Batch 16/4000 loss 4.058397 loss_att 6.439729 loss_ctc 4.836904 loss_rnnt 3.291000 hw_loss 0.351243 lr 0.00042651 rank 1
2023-02-23 08:18:36,322 DEBUG TRAIN Batch 16/4000 loss 4.973193 loss_att 7.614580 loss_ctc 6.587564 loss_rnnt 3.925725 hw_loss 0.569888 lr 0.00042650 rank 0
2023-02-23 08:18:36,323 DEBUG TRAIN Batch 16/4000 loss 11.361643 loss_att 14.450907 loss_ctc 14.513008 loss_rnnt 10.081183 hw_loss 0.454548 lr 0.00042643 rank 3
2023-02-23 08:18:36,324 DEBUG TRAIN Batch 16/4000 loss 7.422003 loss_att 11.138104 loss_ctc 11.262198 loss_rnnt 5.955083 hw_loss 0.396887 lr 0.00042641 rank 5
2023-02-23 08:18:36,326 DEBUG TRAIN Batch 16/4000 loss 14.042903 loss_att 18.370295 loss_ctc 17.851158 loss_rnnt 12.451587 hw_loss 0.408881 lr 0.00042649 rank 2
2023-02-23 08:18:36,327 DEBUG TRAIN Batch 16/4000 loss 7.463413 loss_att 11.615904 loss_ctc 10.304180 loss_rnnt 6.021789 hw_loss 0.435670 lr 0.00042648 rank 6
2023-02-23 08:19:48,553 DEBUG TRAIN Batch 16/4100 loss 9.557416 loss_att 14.142380 loss_ctc 14.921011 loss_rnnt 7.713338 hw_loss 0.397385 lr 0.00042634 rank 0
2023-02-23 08:19:48,553 DEBUG TRAIN Batch 16/4100 loss 7.155357 loss_att 9.843122 loss_ctc 12.430731 loss_rnnt 5.670001 hw_loss 0.458287 lr 0.00042636 rank 1
2023-02-23 08:19:48,554 DEBUG TRAIN Batch 16/4100 loss 22.065577 loss_att 27.908503 loss_ctc 32.413902 loss_rnnt 19.363285 hw_loss 0.288615 lr 0.00042627 rank 7
2023-02-23 08:19:48,555 DEBUG TRAIN Batch 16/4100 loss 11.904487 loss_att 14.644428 loss_ctc 19.286770 loss_rnnt 10.124557 hw_loss 0.464319 lr 0.00042627 rank 3
2023-02-23 08:19:48,557 DEBUG TRAIN Batch 16/4100 loss 10.421879 loss_att 11.221412 loss_ctc 13.083248 loss_rnnt 9.649228 hw_loss 0.483550 lr 0.00042625 rank 5
2023-02-23 08:19:48,558 DEBUG TRAIN Batch 16/4100 loss 12.452853 loss_att 14.277163 loss_ctc 18.342991 loss_rnnt 11.054292 hw_loss 0.465652 lr 0.00042633 rank 2
2023-02-23 08:19:48,561 DEBUG TRAIN Batch 16/4100 loss 14.537429 loss_att 20.532467 loss_ctc 22.214859 loss_rnnt 12.046621 hw_loss 0.502768 lr 0.00042633 rank 6
2023-02-23 08:19:48,602 DEBUG TRAIN Batch 16/4100 loss 5.502707 loss_att 7.526366 loss_ctc 9.214775 loss_rnnt 4.449670 hw_loss 0.287555 lr 0.00042632 rank 4
2023-02-23 08:21:02,496 DEBUG TRAIN Batch 16/4200 loss 11.440871 loss_att 13.110996 loss_ctc 17.981346 loss_rnnt 10.035536 hw_loss 0.373590 lr 0.00042612 rank 3
2023-02-23 08:21:02,499 DEBUG TRAIN Batch 16/4200 loss 12.533742 loss_att 15.088935 loss_ctc 22.928852 loss_rnnt 10.417551 hw_loss 0.410885 lr 0.00042619 rank 0
2023-02-23 08:21:02,500 DEBUG TRAIN Batch 16/4200 loss 6.779100 loss_att 11.510190 loss_ctc 9.006678 loss_rnnt 5.306857 hw_loss 0.429402 lr 0.00042617 rank 6
2023-02-23 08:21:02,501 DEBUG TRAIN Batch 16/4200 loss 13.228300 loss_att 14.959206 loss_ctc 15.478104 loss_rnnt 12.369592 hw_loss 0.398536 lr 0.00042611 rank 7
2023-02-23 08:21:02,506 DEBUG TRAIN Batch 16/4200 loss 10.891107 loss_att 13.852909 loss_ctc 16.895279 loss_rnnt 9.244759 hw_loss 0.475185 lr 0.00042618 rank 2
2023-02-23 08:21:02,507 DEBUG TRAIN Batch 16/4200 loss 12.518313 loss_att 15.559538 loss_ctc 21.245239 loss_rnnt 10.557746 hw_loss 0.353872 lr 0.00042610 rank 5
2023-02-23 08:21:02,507 DEBUG TRAIN Batch 16/4200 loss 13.357275 loss_att 16.966099 loss_ctc 20.387381 loss_rnnt 11.459048 hw_loss 0.448339 lr 0.00042617 rank 4
2023-02-23 08:21:02,507 DEBUG TRAIN Batch 16/4200 loss 18.771135 loss_att 23.722219 loss_ctc 27.050591 loss_rnnt 16.413921 hw_loss 0.493253 lr 0.00042620 rank 1
2023-02-23 08:22:17,712 DEBUG TRAIN Batch 16/4300 loss 11.728850 loss_att 13.636391 loss_ctc 13.170856 loss_rnnt 10.953733 hw_loss 0.377517 lr 0.00042596 rank 3
2023-02-23 08:22:17,713 DEBUG TRAIN Batch 16/4300 loss 16.176060 loss_att 18.742867 loss_ctc 22.668184 loss_rnnt 14.581613 hw_loss 0.404007 lr 0.00042596 rank 7
2023-02-23 08:22:17,713 DEBUG TRAIN Batch 16/4300 loss 8.390507 loss_att 9.854732 loss_ctc 9.155861 loss_rnnt 7.769371 hw_loss 0.424206 lr 0.00042602 rank 2
2023-02-23 08:22:17,716 DEBUG TRAIN Batch 16/4300 loss 9.843416 loss_att 12.928356 loss_ctc 15.010640 loss_rnnt 8.284595 hw_loss 0.474129 lr 0.00042603 rank 0
2023-02-23 08:22:17,717 DEBUG TRAIN Batch 16/4300 loss 19.656952 loss_att 18.571289 loss_ctc 25.553581 loss_rnnt 18.844011 hw_loss 0.457226 lr 0.00042605 rank 1
2023-02-23 08:22:17,720 DEBUG TRAIN Batch 16/4300 loss 13.577670 loss_att 16.028389 loss_ctc 18.787361 loss_rnnt 12.197601 hw_loss 0.366186 lr 0.00042602 rank 6
2023-02-23 08:22:17,726 DEBUG TRAIN Batch 16/4300 loss 11.423759 loss_att 14.347918 loss_ctc 17.625116 loss_rnnt 9.782673 hw_loss 0.430137 lr 0.00042601 rank 4
2023-02-23 08:22:17,768 DEBUG TRAIN Batch 16/4300 loss 6.623608 loss_att 9.224950 loss_ctc 8.975909 loss_rnnt 5.553614 hw_loss 0.442660 lr 0.00042594 rank 5
2023-02-23 08:23:30,806 DEBUG TRAIN Batch 16/4400 loss 7.061585 loss_att 8.556041 loss_ctc 8.462654 loss_rnnt 6.335345 hw_loss 0.451013 lr 0.00042580 rank 7
2023-02-23 08:23:30,808 DEBUG TRAIN Batch 16/4400 loss 10.458395 loss_att 11.327916 loss_ctc 11.714307 loss_rnnt 9.825736 hw_loss 0.546189 lr 0.00042581 rank 3
2023-02-23 08:23:30,810 DEBUG TRAIN Batch 16/4400 loss 7.490417 loss_att 10.491175 loss_ctc 12.271000 loss_rnnt 6.043823 hw_loss 0.391936 lr 0.00042588 rank 0
2023-02-23 08:23:30,813 DEBUG TRAIN Batch 16/4400 loss 8.885992 loss_att 9.654103 loss_ctc 12.067188 loss_rnnt 7.985695 hw_loss 0.604715 lr 0.00042587 rank 2
2023-02-23 08:23:30,814 DEBUG TRAIN Batch 16/4400 loss 8.394203 loss_att 8.377651 loss_ctc 12.511870 loss_rnnt 7.595451 hw_loss 0.474449 lr 0.00042579 rank 5
2023-02-23 08:23:30,817 DEBUG TRAIN Batch 16/4400 loss 14.439287 loss_att 14.354082 loss_ctc 16.475365 loss_rnnt 13.928391 hw_loss 0.480865 lr 0.00042586 rank 6
2023-02-23 08:23:30,819 DEBUG TRAIN Batch 16/4400 loss 12.666260 loss_att 14.451017 loss_ctc 16.729988 loss_rnnt 11.480681 hw_loss 0.537743 lr 0.00042589 rank 1
2023-02-23 08:23:30,863 DEBUG TRAIN Batch 16/4400 loss 5.922998 loss_att 11.533928 loss_ctc 10.459687 loss_rnnt 3.976837 hw_loss 0.410782 lr 0.00042586 rank 4
2023-02-23 08:24:44,171 DEBUG TRAIN Batch 16/4500 loss 5.464091 loss_att 8.985191 loss_ctc 6.479150 loss_rnnt 4.398623 hw_loss 0.423574 lr 0.00042574 rank 1
2023-02-23 08:24:44,174 DEBUG TRAIN Batch 16/4500 loss 8.471616 loss_att 10.483398 loss_ctc 10.596297 loss_rnnt 7.516598 hw_loss 0.505069 lr 0.00042571 rank 4
2023-02-23 08:24:44,174 DEBUG TRAIN Batch 16/4500 loss 12.524374 loss_att 17.930544 loss_ctc 19.441225 loss_rnnt 10.322868 hw_loss 0.371297 lr 0.00042565 rank 3
2023-02-23 08:24:44,175 DEBUG TRAIN Batch 16/4500 loss 9.126361 loss_att 10.936335 loss_ctc 11.026983 loss_rnnt 8.274514 hw_loss 0.443315 lr 0.00042565 rank 7
2023-02-23 08:24:44,175 DEBUG TRAIN Batch 16/4500 loss 12.787527 loss_att 14.245698 loss_ctc 14.266546 loss_rnnt 12.095400 hw_loss 0.381168 lr 0.00042563 rank 5
2023-02-23 08:24:44,176 DEBUG TRAIN Batch 16/4500 loss 7.424266 loss_att 9.131422 loss_ctc 12.492655 loss_rnnt 6.183956 hw_loss 0.418302 lr 0.00042571 rank 2
2023-02-23 08:24:44,179 DEBUG TRAIN Batch 16/4500 loss 6.076535 loss_att 8.888684 loss_ctc 8.598025 loss_rnnt 4.966823 hw_loss 0.395782 lr 0.00042573 rank 0
2023-02-23 08:24:44,222 DEBUG TRAIN Batch 16/4500 loss 14.299862 loss_att 16.253674 loss_ctc 26.295767 loss_rnnt 12.055025 hw_loss 0.477412 lr 0.00042571 rank 6
2023-02-23 08:25:59,744 DEBUG TRAIN Batch 16/4600 loss 5.576913 loss_att 9.359453 loss_ctc 9.568995 loss_rnnt 4.087896 hw_loss 0.375434 lr 0.00042558 rank 1
2023-02-23 08:25:59,745 DEBUG TRAIN Batch 16/4600 loss 10.709861 loss_att 16.933300 loss_ctc 12.902454 loss_rnnt 8.893724 hw_loss 0.523316 lr 0.00042557 rank 0
2023-02-23 08:25:59,752 DEBUG TRAIN Batch 16/4600 loss 12.366184 loss_att 13.624575 loss_ctc 14.051582 loss_rnnt 11.635172 hw_loss 0.477399 lr 0.00042549 rank 7
2023-02-23 08:25:59,753 DEBUG TRAIN Batch 16/4600 loss 6.130622 loss_att 8.327036 loss_ctc 9.236488 loss_rnnt 5.090100 hw_loss 0.350857 lr 0.00042548 rank 5
2023-02-23 08:25:59,753 DEBUG TRAIN Batch 16/4600 loss 17.730568 loss_att 22.339039 loss_ctc 22.936214 loss_rnnt 15.923203 hw_loss 0.359222 lr 0.00042555 rank 4
2023-02-23 08:25:59,754 DEBUG TRAIN Batch 16/4600 loss 8.173864 loss_att 9.900143 loss_ctc 8.975084 loss_rnnt 7.502080 hw_loss 0.411935 lr 0.00042550 rank 3
2023-02-23 08:25:59,754 DEBUG TRAIN Batch 16/4600 loss 4.627765 loss_att 8.494301 loss_ctc 7.028697 loss_rnnt 3.380603 hw_loss 0.288247 lr 0.00042555 rank 6
2023-02-23 08:25:59,757 DEBUG TRAIN Batch 16/4600 loss 18.058973 loss_att 22.116585 loss_ctc 21.747631 loss_rnnt 16.524408 hw_loss 0.433541 lr 0.00042556 rank 2
2023-02-23 08:27:13,398 DEBUG TRAIN Batch 16/4700 loss 20.279350 loss_att 22.783379 loss_ctc 26.677036 loss_rnnt 18.753372 hw_loss 0.322774 lr 0.00042534 rank 7
2023-02-23 08:27:13,400 DEBUG TRAIN Batch 16/4700 loss 7.305567 loss_att 11.528775 loss_ctc 9.931149 loss_rnnt 5.897867 hw_loss 0.399338 lr 0.00042540 rank 6
2023-02-23 08:27:13,401 DEBUG TRAIN Batch 16/4700 loss 22.231089 loss_att 28.622662 loss_ctc 30.490261 loss_rnnt 19.692810 hw_loss 0.297639 lr 0.00042541 rank 2
2023-02-23 08:27:13,403 DEBUG TRAIN Batch 16/4700 loss 18.075247 loss_att 20.658724 loss_ctc 21.785702 loss_rnnt 16.806557 hw_loss 0.482376 lr 0.00042543 rank 1
2023-02-23 08:27:13,403 DEBUG TRAIN Batch 16/4700 loss 7.397407 loss_att 13.009789 loss_ctc 11.476375 loss_rnnt 5.506233 hw_loss 0.421565 lr 0.00042542 rank 0
2023-02-23 08:27:13,402 DEBUG TRAIN Batch 16/4700 loss 11.829726 loss_att 13.962162 loss_ctc 14.515145 loss_rnnt 10.852604 hw_loss 0.361087 lr 0.00042534 rank 3
2023-02-23 08:27:13,406 DEBUG TRAIN Batch 16/4700 loss 16.922150 loss_att 20.562363 loss_ctc 22.066204 loss_rnnt 15.328844 hw_loss 0.336349 lr 0.00042533 rank 5
2023-02-23 08:27:13,464 DEBUG TRAIN Batch 16/4700 loss 11.075902 loss_att 13.235514 loss_ctc 21.221893 loss_rnnt 9.110036 hw_loss 0.339647 lr 0.00042540 rank 4
2023-02-23 08:28:25,091 DEBUG TRAIN Batch 16/4800 loss 13.552693 loss_att 15.198857 loss_ctc 15.920918 loss_rnnt 12.691196 hw_loss 0.405939 lr 0.00042519 rank 3
2023-02-23 08:28:25,110 DEBUG TRAIN Batch 16/4800 loss 14.141621 loss_att 16.826797 loss_ctc 21.995783 loss_rnnt 12.332730 hw_loss 0.421185 lr 0.00042519 rank 7
2023-02-23 08:28:25,113 DEBUG TRAIN Batch 16/4800 loss 9.158626 loss_att 14.496761 loss_ctc 17.458933 loss_rnnt 6.786379 hw_loss 0.371084 lr 0.00042524 rank 4
2023-02-23 08:28:25,115 DEBUG TRAIN Batch 16/4800 loss 9.726372 loss_att 14.400065 loss_ctc 13.856380 loss_rnnt 8.031640 hw_loss 0.392484 lr 0.00042517 rank 5
2023-02-23 08:28:25,115 DEBUG TRAIN Batch 16/4800 loss 8.595531 loss_att 11.700912 loss_ctc 10.992549 loss_rnnt 7.468312 hw_loss 0.349761 lr 0.00042526 rank 0
2023-02-23 08:28:25,117 DEBUG TRAIN Batch 16/4800 loss 13.658528 loss_att 18.386902 loss_ctc 17.129553 loss_rnnt 11.982428 hw_loss 0.501792 lr 0.00042528 rank 1
2023-02-23 08:28:25,118 DEBUG TRAIN Batch 16/4800 loss 6.681310 loss_att 10.629461 loss_ctc 8.351040 loss_rnnt 5.447157 hw_loss 0.416047 lr 0.00042525 rank 2
2023-02-23 08:28:25,119 DEBUG TRAIN Batch 16/4800 loss 11.958950 loss_att 15.798458 loss_ctc 16.671028 loss_rnnt 10.365722 hw_loss 0.369468 lr 0.00042525 rank 6
2023-02-23 08:29:37,962 DEBUG TRAIN Batch 16/4900 loss 13.946159 loss_att 17.604134 loss_ctc 17.067596 loss_rnnt 12.582574 hw_loss 0.404622 lr 0.00042511 rank 0
2023-02-23 08:29:37,962 DEBUG TRAIN Batch 16/4900 loss 8.169703 loss_att 10.620475 loss_ctc 10.793236 loss_rnnt 7.068378 hw_loss 0.490060 lr 0.00042504 rank 3
2023-02-23 08:29:37,973 DEBUG TRAIN Batch 16/4900 loss 15.183724 loss_att 16.590340 loss_ctc 18.505196 loss_rnnt 14.230301 hw_loss 0.429822 lr 0.00042503 rank 7
2023-02-23 08:29:37,975 DEBUG TRAIN Batch 16/4900 loss 8.165640 loss_att 10.322329 loss_ctc 11.185015 loss_rnnt 7.141642 hw_loss 0.356394 lr 0.00042509 rank 6
2023-02-23 08:29:37,978 DEBUG TRAIN Batch 16/4900 loss 12.949631 loss_att 12.899940 loss_ctc 16.014339 loss_rnnt 12.348595 hw_loss 0.379399 lr 0.00042510 rank 2
2023-02-23 08:29:37,980 DEBUG TRAIN Batch 16/4900 loss 12.473844 loss_att 14.414464 loss_ctc 19.969652 loss_rnnt 10.864293 hw_loss 0.416221 lr 0.00042512 rank 1
2023-02-23 08:29:37,980 DEBUG TRAIN Batch 16/4900 loss 8.021054 loss_att 10.624244 loss_ctc 9.991322 loss_rnnt 7.053306 hw_loss 0.345766 lr 0.00042502 rank 5
2023-02-23 08:29:37,983 DEBUG TRAIN Batch 16/4900 loss 9.574330 loss_att 12.076846 loss_ctc 13.652914 loss_rnnt 8.340094 hw_loss 0.356104 lr 0.00042509 rank 4
2023-02-23 08:30:52,699 DEBUG TRAIN Batch 16/5000 loss 7.163620 loss_att 9.180299 loss_ctc 12.879324 loss_rnnt 5.800070 hw_loss 0.371476 lr 0.00042494 rank 4
2023-02-23 08:30:52,712 DEBUG TRAIN Batch 16/5000 loss 11.809585 loss_att 11.294447 loss_ctc 11.785224 loss_rnnt 11.676804 hw_loss 0.448231 lr 0.00042497 rank 1
2023-02-23 08:30:52,716 DEBUG TRAIN Batch 16/5000 loss 12.300931 loss_att 14.472895 loss_ctc 18.383860 loss_rnnt 10.849373 hw_loss 0.386454 lr 0.00042496 rank 0
2023-02-23 08:30:52,716 DEBUG TRAIN Batch 16/5000 loss 7.984698 loss_att 10.721282 loss_ctc 8.139473 loss_rnnt 7.216870 hw_loss 0.374763 lr 0.00042494 rank 6
2023-02-23 08:30:52,716 DEBUG TRAIN Batch 16/5000 loss 8.436767 loss_att 10.911541 loss_ctc 12.125655 loss_rnnt 7.192781 hw_loss 0.482212 lr 0.00042488 rank 7
2023-02-23 08:30:52,718 DEBUG TRAIN Batch 16/5000 loss 7.038919 loss_att 8.897943 loss_ctc 9.600471 loss_rnnt 6.125669 hw_loss 0.374822 lr 0.00042495 rank 2
2023-02-23 08:30:52,721 DEBUG TRAIN Batch 16/5000 loss 16.510487 loss_att 18.617922 loss_ctc 23.593178 loss_rnnt 14.893396 hw_loss 0.471082 lr 0.00042488 rank 3
2023-02-23 08:30:52,762 DEBUG TRAIN Batch 16/5000 loss 22.938301 loss_att 22.713531 loss_ctc 29.892048 loss_rnnt 21.839931 hw_loss 0.405298 lr 0.00042487 rank 5
2023-02-23 08:32:04,921 DEBUG TRAIN Batch 16/5100 loss 11.392636 loss_att 16.463785 loss_ctc 18.533772 loss_rnnt 9.164025 hw_loss 0.491679 lr 0.00042473 rank 7
2023-02-23 08:32:04,925 DEBUG TRAIN Batch 16/5100 loss 7.698327 loss_att 9.158299 loss_ctc 8.895569 loss_rnnt 6.971814 hw_loss 0.515410 lr 0.00042479 rank 6
2023-02-23 08:32:04,926 DEBUG TRAIN Batch 16/5100 loss 13.311126 loss_att 14.576563 loss_ctc 15.829883 loss_rnnt 12.402761 hw_loss 0.598955 lr 0.00042480 rank 0
2023-02-23 08:32:04,926 DEBUG TRAIN Batch 16/5100 loss 12.927010 loss_att 13.988493 loss_ctc 12.724930 loss_rnnt 12.522038 hw_loss 0.411785 lr 0.00042473 rank 3
2023-02-23 08:32:04,925 DEBUG TRAIN Batch 16/5100 loss 8.215835 loss_att 9.879564 loss_ctc 10.498335 loss_rnnt 7.287262 hw_loss 0.546550 lr 0.00042481 rank 1
2023-02-23 08:32:04,931 DEBUG TRAIN Batch 16/5100 loss 14.025239 loss_att 16.802252 loss_ctc 16.521820 loss_rnnt 12.869145 hw_loss 0.502149 lr 0.00042478 rank 4
2023-02-23 08:32:04,934 DEBUG TRAIN Batch 16/5100 loss 10.036927 loss_att 17.927593 loss_ctc 13.718885 loss_rnnt 7.796246 hw_loss 0.321788 lr 0.00042471 rank 5
2023-02-23 08:32:04,935 DEBUG TRAIN Batch 16/5100 loss 11.320794 loss_att 15.419280 loss_ctc 18.367516 loss_rnnt 9.361309 hw_loss 0.375420 lr 0.00042479 rank 2
2023-02-23 08:33:17,073 DEBUG TRAIN Batch 16/5200 loss 10.456221 loss_att 12.038090 loss_ctc 15.705648 loss_rnnt 9.237698 hw_loss 0.379171 lr 0.00042458 rank 3
2023-02-23 08:33:17,077 DEBUG TRAIN Batch 16/5200 loss 10.429637 loss_att 9.707411 loss_ctc 12.104406 loss_rnnt 9.977972 hw_loss 0.699012 lr 0.00042465 rank 0
2023-02-23 08:33:17,078 DEBUG TRAIN Batch 16/5200 loss 12.005943 loss_att 12.605859 loss_ctc 18.476156 loss_rnnt 10.831446 hw_loss 0.359663 lr 0.00042466 rank 1
2023-02-23 08:33:17,079 DEBUG TRAIN Batch 16/5200 loss 5.366624 loss_att 7.408279 loss_ctc 5.612674 loss_rnnt 4.689809 hw_loss 0.441897 lr 0.00042457 rank 7
2023-02-23 08:33:17,082 DEBUG TRAIN Batch 16/5200 loss 9.777525 loss_att 12.549233 loss_ctc 15.446930 loss_rnnt 8.292608 hw_loss 0.327478 lr 0.00042456 rank 5
2023-02-23 08:33:17,082 DEBUG TRAIN Batch 16/5200 loss 12.239524 loss_att 16.959543 loss_ctc 20.495663 loss_rnnt 10.001236 hw_loss 0.362747 lr 0.00042464 rank 2
2023-02-23 08:33:17,082 DEBUG TRAIN Batch 16/5200 loss 16.251850 loss_att 22.611502 loss_ctc 28.538458 loss_rnnt 13.145475 hw_loss 0.367934 lr 0.00042463 rank 4
2023-02-23 08:33:17,086 DEBUG TRAIN Batch 16/5200 loss 7.933073 loss_att 12.437183 loss_ctc 10.700610 loss_rnnt 6.441257 hw_loss 0.416229 lr 0.00042463 rank 6
2023-02-23 08:34:31,237 DEBUG TRAIN Batch 16/5300 loss 7.616642 loss_att 11.935974 loss_ctc 10.368484 loss_rnnt 6.191401 hw_loss 0.364617 lr 0.00042448 rank 6
2023-02-23 08:34:31,244 DEBUG TRAIN Batch 16/5300 loss 4.925359 loss_att 8.428353 loss_ctc 9.106987 loss_rnnt 3.499987 hw_loss 0.313544 lr 0.00042442 rank 7
2023-02-23 08:34:31,245 DEBUG TRAIN Batch 16/5300 loss 9.924547 loss_att 12.115992 loss_ctc 10.964684 loss_rnnt 9.086844 hw_loss 0.488869 lr 0.00042442 rank 3
2023-02-23 08:34:31,248 DEBUG TRAIN Batch 16/5300 loss 8.325274 loss_att 10.665097 loss_ctc 12.526619 loss_rnnt 7.105497 hw_loss 0.359310 lr 0.00042449 rank 2
2023-02-23 08:34:31,250 DEBUG TRAIN Batch 16/5300 loss 2.837360 loss_att 5.036192 loss_ctc 3.705749 loss_rnnt 2.032775 hw_loss 0.466935 lr 0.00042450 rank 0
2023-02-23 08:34:31,267 DEBUG TRAIN Batch 16/5300 loss 17.988651 loss_att 22.308615 loss_ctc 24.456671 loss_rnnt 16.003906 hw_loss 0.484403 lr 0.00042441 rank 5
2023-02-23 08:34:31,280 DEBUG TRAIN Batch 16/5300 loss 3.504597 loss_att 5.019618 loss_ctc 3.713126 loss_rnnt 2.973425 hw_loss 0.375683 lr 0.00042451 rank 1
2023-02-23 08:34:31,281 DEBUG TRAIN Batch 16/5300 loss 7.405042 loss_att 11.947439 loss_ctc 13.374270 loss_rnnt 5.514692 hw_loss 0.348700 lr 0.00042448 rank 4
2023-02-23 08:35:45,035 DEBUG TRAIN Batch 16/5400 loss 5.555419 loss_att 8.783000 loss_ctc 8.114468 loss_rnnt 4.348660 hw_loss 0.412570 lr 0.00042427 rank 7
2023-02-23 08:35:45,038 DEBUG TRAIN Batch 16/5400 loss 11.840781 loss_att 15.516722 loss_ctc 14.859013 loss_rnnt 10.468558 hw_loss 0.439884 lr 0.00042436 rank 1
2023-02-23 08:35:45,041 DEBUG TRAIN Batch 16/5400 loss 9.904188 loss_att 14.286551 loss_ctc 13.424590 loss_rnnt 8.385660 hw_loss 0.323752 lr 0.00042425 rank 5
2023-02-23 08:35:45,042 DEBUG TRAIN Batch 16/5400 loss 14.966701 loss_att 16.419472 loss_ctc 17.720417 loss_rnnt 14.120260 hw_loss 0.353859 lr 0.00042427 rank 3
2023-02-23 08:35:45,042 DEBUG TRAIN Batch 16/5400 loss 7.609863 loss_att 9.352703 loss_ctc 9.716417 loss_rnnt 6.782180 hw_loss 0.371702 lr 0.00042433 rank 2
2023-02-23 08:35:45,046 DEBUG TRAIN Batch 16/5400 loss 15.363230 loss_att 18.098291 loss_ctc 18.698936 loss_rnnt 14.122116 hw_loss 0.467512 lr 0.00042432 rank 4
2023-02-23 08:35:45,073 DEBUG TRAIN Batch 16/5400 loss 12.598600 loss_att 13.186432 loss_ctc 17.665203 loss_rnnt 11.579990 hw_loss 0.422807 lr 0.00042433 rank 6
2023-02-23 08:35:45,083 DEBUG TRAIN Batch 16/5400 loss 7.497028 loss_att 14.601011 loss_ctc 12.364571 loss_rnnt 5.183360 hw_loss 0.457247 lr 0.00042434 rank 0
2023-02-23 08:36:57,290 DEBUG TRAIN Batch 16/5500 loss 13.842135 loss_att 16.704361 loss_ctc 21.705853 loss_rnnt 12.019365 hw_loss 0.378428 lr 0.00042410 rank 5
2023-02-23 08:36:57,291 DEBUG TRAIN Batch 16/5500 loss 6.946150 loss_att 11.315184 loss_ctc 13.060253 loss_rnnt 5.034050 hw_loss 0.418275 lr 0.00042418 rank 2
2023-02-23 08:36:57,292 DEBUG TRAIN Batch 16/5500 loss 10.554559 loss_att 13.044448 loss_ctc 13.776069 loss_rnnt 9.418178 hw_loss 0.391630 lr 0.00042411 rank 7
2023-02-23 08:36:57,294 DEBUG TRAIN Batch 16/5500 loss 13.428708 loss_att 16.530228 loss_ctc 23.110712 loss_rnnt 11.312433 hw_loss 0.384443 lr 0.00042417 rank 6
2023-02-23 08:36:57,294 DEBUG TRAIN Batch 16/5500 loss 10.034949 loss_att 13.585717 loss_ctc 17.692364 loss_rnnt 8.079176 hw_loss 0.421183 lr 0.00042419 rank 0
2023-02-23 08:36:57,295 DEBUG TRAIN Batch 16/5500 loss 12.371703 loss_att 16.147858 loss_ctc 19.023670 loss_rnnt 10.503063 hw_loss 0.424650 lr 0.00042412 rank 3
2023-02-23 08:36:57,297 DEBUG TRAIN Batch 16/5500 loss 19.374655 loss_att 22.746531 loss_ctc 26.144876 loss_rnnt 17.583445 hw_loss 0.401511 lr 0.00042420 rank 1
2023-02-23 08:36:57,296 DEBUG TRAIN Batch 16/5500 loss 12.100445 loss_att 13.696734 loss_ctc 16.055262 loss_rnnt 11.052185 hw_loss 0.378175 lr 0.00042417 rank 4
2023-02-23 08:38:10,632 DEBUG TRAIN Batch 16/5600 loss 15.892813 loss_att 21.399700 loss_ctc 24.159510 loss_rnnt 13.464029 hw_loss 0.422210 lr 0.00042396 rank 7
2023-02-23 08:38:10,633 DEBUG TRAIN Batch 16/5600 loss 9.259414 loss_att 12.202001 loss_ctc 13.549833 loss_rnnt 7.909278 hw_loss 0.355430 lr 0.00042404 rank 0
2023-02-23 08:38:10,638 DEBUG TRAIN Batch 16/5600 loss 5.483099 loss_att 8.699430 loss_ctc 5.916693 loss_rnnt 4.577037 hw_loss 0.384343 lr 0.00042395 rank 5
2023-02-23 08:38:10,639 DEBUG TRAIN Batch 16/5600 loss 10.691544 loss_att 12.199017 loss_ctc 15.433397 loss_rnnt 9.517482 hw_loss 0.450602 lr 0.00042397 rank 3
2023-02-23 08:38:10,639 DEBUG TRAIN Batch 16/5600 loss 14.523204 loss_att 14.928741 loss_ctc 16.409327 loss_rnnt 13.963046 hw_loss 0.426689 lr 0.00042403 rank 2
2023-02-23 08:38:10,639 DEBUG TRAIN Batch 16/5600 loss 7.005389 loss_att 8.908894 loss_ctc 12.608372 loss_rnnt 5.615593 hw_loss 0.491307 lr 0.00042402 rank 6
2023-02-23 08:38:10,641 DEBUG TRAIN Batch 16/5600 loss 8.920414 loss_att 10.021340 loss_ctc 12.600828 loss_rnnt 7.993753 hw_loss 0.404538 lr 0.00042405 rank 1
2023-02-23 08:38:10,679 DEBUG TRAIN Batch 16/5600 loss 9.450875 loss_att 11.113564 loss_ctc 11.943017 loss_rnnt 8.612378 hw_loss 0.325640 lr 0.00042402 rank 4
2023-02-23 08:39:26,090 DEBUG TRAIN Batch 16/5700 loss 4.749384 loss_att 11.437147 loss_ctc 7.980966 loss_rnnt 2.748044 hw_loss 0.436707 lr 0.00042381 rank 7
2023-02-23 08:39:26,091 DEBUG TRAIN Batch 16/5700 loss 10.614835 loss_att 10.409423 loss_ctc 13.101975 loss_rnnt 9.983493 hw_loss 0.639008 lr 0.00042381 rank 3
2023-02-23 08:39:26,093 DEBUG TRAIN Batch 16/5700 loss 7.103906 loss_att 8.514098 loss_ctc 9.469066 loss_rnnt 6.273647 hw_loss 0.436621 lr 0.00042390 rank 1
2023-02-23 08:39:26,097 DEBUG TRAIN Batch 16/5700 loss 11.484217 loss_att 12.486671 loss_ctc 15.077655 loss_rnnt 10.567374 hw_loss 0.444798 lr 0.00042380 rank 5
2023-02-23 08:39:26,100 DEBUG TRAIN Batch 16/5700 loss 19.848186 loss_att 20.668564 loss_ctc 23.418552 loss_rnnt 18.973640 hw_loss 0.439539 lr 0.00042389 rank 0
2023-02-23 08:39:26,105 DEBUG TRAIN Batch 16/5700 loss 8.160582 loss_att 9.926003 loss_ctc 10.182982 loss_rnnt 7.258545 hw_loss 0.523688 lr 0.00042387 rank 6
2023-02-23 08:39:26,115 DEBUG TRAIN Batch 16/5700 loss 12.228591 loss_att 15.249325 loss_ctc 14.621277 loss_rnnt 11.092954 hw_loss 0.398372 lr 0.00042387 rank 4
2023-02-23 08:39:26,117 DEBUG TRAIN Batch 16/5700 loss 8.135292 loss_att 15.123550 loss_ctc 9.079239 loss_rnnt 6.421241 hw_loss 0.357261 lr 0.00042387 rank 2
2023-02-23 08:40:38,103 DEBUG TRAIN Batch 16/5800 loss 3.903259 loss_att 7.597996 loss_ctc 6.854465 loss_rnnt 2.567171 hw_loss 0.381838 lr 0.00042366 rank 7
2023-02-23 08:40:38,108 DEBUG TRAIN Batch 16/5800 loss 5.615085 loss_att 8.243361 loss_ctc 8.975900 loss_rnnt 4.402399 hw_loss 0.447980 lr 0.00042372 rank 2
2023-02-23 08:40:38,109 DEBUG TRAIN Batch 16/5800 loss 9.207006 loss_att 12.192936 loss_ctc 13.086712 loss_rnnt 7.853211 hw_loss 0.448713 lr 0.00042366 rank 3
2023-02-23 08:40:38,109 DEBUG TRAIN Batch 16/5800 loss 7.674272 loss_att 12.045689 loss_ctc 7.607026 loss_rnnt 6.574819 hw_loss 0.439004 lr 0.00042375 rank 1
2023-02-23 08:40:38,109 DEBUG TRAIN Batch 16/5800 loss 6.194860 loss_att 10.030780 loss_ctc 13.203921 loss_rnnt 4.283604 hw_loss 0.392867 lr 0.00042364 rank 5
2023-02-23 08:40:38,109 DEBUG TRAIN Batch 16/5800 loss 6.756821 loss_att 8.454831 loss_ctc 8.018755 loss_rnnt 5.992172 hw_loss 0.481479 lr 0.00042371 rank 4
2023-02-23 08:40:38,114 DEBUG TRAIN Batch 16/5800 loss 2.951697 loss_att 3.716418 loss_ctc 3.753658 loss_rnnt 2.470170 hw_loss 0.415603 lr 0.00042373 rank 0
2023-02-23 08:40:38,117 DEBUG TRAIN Batch 16/5800 loss 10.409883 loss_att 14.240915 loss_ctc 14.693649 loss_rnnt 8.825207 hw_loss 0.463689 lr 0.00042372 rank 6
2023-02-23 08:41:50,414 DEBUG TRAIN Batch 16/5900 loss 14.168022 loss_att 16.641594 loss_ctc 17.239162 loss_rnnt 13.063174 hw_loss 0.376215 lr 0.00042356 rank 4
2023-02-23 08:41:50,414 DEBUG TRAIN Batch 16/5900 loss 5.209438 loss_att 10.571171 loss_ctc 10.873327 loss_rnnt 3.208723 hw_loss 0.324719 lr 0.00042351 rank 3
2023-02-23 08:41:50,414 DEBUG TRAIN Batch 16/5900 loss 17.084032 loss_att 22.621880 loss_ctc 25.346609 loss_rnnt 14.666430 hw_loss 0.390664 lr 0.00042349 rank 5
2023-02-23 08:41:50,415 DEBUG TRAIN Batch 16/5900 loss 8.430023 loss_att 13.653851 loss_ctc 10.750038 loss_rnnt 6.898070 hw_loss 0.333475 lr 0.00042359 rank 1
2023-02-23 08:41:50,416 DEBUG TRAIN Batch 16/5900 loss 8.713951 loss_att 11.106348 loss_ctc 12.794178 loss_rnnt 7.493357 hw_loss 0.371409 lr 0.00042351 rank 7
2023-02-23 08:41:50,417 DEBUG TRAIN Batch 16/5900 loss 9.111973 loss_att 13.116045 loss_ctc 9.653177 loss_rnnt 8.008211 hw_loss 0.432724 lr 0.00042356 rank 6
2023-02-23 08:41:50,421 DEBUG TRAIN Batch 16/5900 loss 9.377931 loss_att 9.621568 loss_ctc 11.541743 loss_rnnt 8.812304 hw_loss 0.428234 lr 0.00042357 rank 2
2023-02-23 08:41:50,464 DEBUG TRAIN Batch 16/5900 loss 11.077763 loss_att 12.812373 loss_ctc 12.222618 loss_rnnt 10.361579 hw_loss 0.406154 lr 0.00042358 rank 0
2023-02-23 08:43:04,939 DEBUG TRAIN Batch 16/6000 loss 6.855647 loss_att 8.516948 loss_ctc 6.971696 loss_rnnt 6.227000 hw_loss 0.526712 lr 0.00042335 rank 7
2023-02-23 08:43:04,945 DEBUG TRAIN Batch 16/6000 loss 8.605725 loss_att 12.072576 loss_ctc 9.507913 loss_rnnt 7.560104 hw_loss 0.434923 lr 0.00042341 rank 6
2023-02-23 08:43:04,951 DEBUG TRAIN Batch 16/6000 loss 11.741199 loss_att 14.053148 loss_ctc 16.526106 loss_rnnt 10.418816 hw_loss 0.416259 lr 0.00042341 rank 4
2023-02-23 08:43:04,956 DEBUG TRAIN Batch 16/6000 loss 12.422359 loss_att 14.173462 loss_ctc 17.430296 loss_rnnt 11.181614 hw_loss 0.417750 lr 0.00042342 rank 2
2023-02-23 08:43:04,957 DEBUG TRAIN Batch 16/6000 loss 6.730584 loss_att 8.944763 loss_ctc 10.213501 loss_rnnt 5.615539 hw_loss 0.389664 lr 0.00042336 rank 3
2023-02-23 08:43:04,958 DEBUG TRAIN Batch 16/6000 loss 12.049171 loss_att 14.491123 loss_ctc 14.934866 loss_rnnt 10.901735 hw_loss 0.514286 lr 0.00042334 rank 5
2023-02-23 08:43:04,961 DEBUG TRAIN Batch 16/6000 loss 11.564379 loss_att 12.752701 loss_ctc 11.057508 loss_rnnt 11.188773 hw_loss 0.385357 lr 0.00042344 rank 1
2023-02-23 08:43:05,002 DEBUG TRAIN Batch 16/6000 loss 6.228827 loss_att 9.028111 loss_ctc 8.091314 loss_rnnt 5.130383 hw_loss 0.544229 lr 0.00042343 rank 0
2023-02-23 08:44:18,436 DEBUG TRAIN Batch 16/6100 loss 12.956186 loss_att 16.286718 loss_ctc 21.132593 loss_rnnt 10.991756 hw_loss 0.390254 lr 0.00042320 rank 7
2023-02-23 08:44:18,437 DEBUG TRAIN Batch 16/6100 loss 16.456684 loss_att 21.226982 loss_ctc 22.203547 loss_rnnt 14.546659 hw_loss 0.355722 lr 0.00042329 rank 1
2023-02-23 08:44:18,438 DEBUG TRAIN Batch 16/6100 loss 17.532560 loss_att 24.582722 loss_ctc 32.007050 loss_rnnt 14.009496 hw_loss 0.343313 lr 0.00042319 rank 5
2023-02-23 08:44:18,439 DEBUG TRAIN Batch 16/6100 loss 12.210192 loss_att 13.887958 loss_ctc 14.425399 loss_rnnt 11.386518 hw_loss 0.361423 lr 0.00042327 rank 2
2023-02-23 08:44:18,441 DEBUG TRAIN Batch 16/6100 loss 17.531328 loss_att 19.200102 loss_ctc 24.332020 loss_rnnt 16.063972 hw_loss 0.425331 lr 0.00042328 rank 0
2023-02-23 08:44:18,441 DEBUG TRAIN Batch 16/6100 loss 12.963499 loss_att 16.651022 loss_ctc 16.501419 loss_rnnt 11.522175 hw_loss 0.435181 lr 0.00042321 rank 3
2023-02-23 08:44:18,446 DEBUG TRAIN Batch 16/6100 loss 7.977630 loss_att 10.009626 loss_ctc 10.302715 loss_rnnt 7.014813 hw_loss 0.462011 lr 0.00042326 rank 4
2023-02-23 08:44:18,489 DEBUG TRAIN Batch 16/6100 loss 11.538095 loss_att 13.561974 loss_ctc 18.852726 loss_rnnt 9.937403 hw_loss 0.413685 lr 0.00042326 rank 6
2023-02-23 08:45:30,686 DEBUG TRAIN Batch 16/6200 loss 6.881976 loss_att 10.892689 loss_ctc 10.299044 loss_rnnt 5.423984 hw_loss 0.375452 lr 0.00042305 rank 7
2023-02-23 08:45:30,689 DEBUG TRAIN Batch 16/6200 loss 14.663962 loss_att 17.355934 loss_ctc 23.380451 loss_rnnt 12.741994 hw_loss 0.415079 lr 0.00042311 rank 6
2023-02-23 08:45:30,693 DEBUG TRAIN Batch 16/6200 loss 15.196072 loss_att 20.652855 loss_ctc 22.695032 loss_rnnt 12.919185 hw_loss 0.348130 lr 0.00042304 rank 5
2023-02-23 08:45:30,694 DEBUG TRAIN Batch 16/6200 loss 7.277543 loss_att 8.690522 loss_ctc 9.311431 loss_rnnt 6.459026 hw_loss 0.496378 lr 0.00042312 rank 2
2023-02-23 08:45:30,694 DEBUG TRAIN Batch 16/6200 loss 8.583758 loss_att 11.508898 loss_ctc 11.286158 loss_rnnt 7.406117 hw_loss 0.435547 lr 0.00042314 rank 1
2023-02-23 08:45:30,696 DEBUG TRAIN Batch 16/6200 loss 6.415473 loss_att 10.394206 loss_ctc 10.534182 loss_rnnt 4.853304 hw_loss 0.407362 lr 0.00042311 rank 4
2023-02-23 08:45:30,697 DEBUG TRAIN Batch 16/6200 loss 13.177402 loss_att 15.313663 loss_ctc 14.935587 loss_rnnt 12.256463 hw_loss 0.486117 lr 0.00042305 rank 3
2023-02-23 08:45:30,742 DEBUG TRAIN Batch 16/6200 loss 19.348503 loss_att 22.979931 loss_ctc 22.572178 loss_rnnt 17.944839 hw_loss 0.464169 lr 0.00042313 rank 0
2023-02-23 08:46:43,622 DEBUG TRAIN Batch 16/6300 loss 6.972241 loss_att 9.209054 loss_ctc 10.705873 loss_rnnt 5.786759 hw_loss 0.450564 lr 0.00042297 rank 0
2023-02-23 08:46:43,633 DEBUG TRAIN Batch 16/6300 loss 1.936508 loss_att 5.043409 loss_ctc 3.195472 loss_rnnt 0.929592 hw_loss 0.408140 lr 0.00042296 rank 2
2023-02-23 08:46:43,632 DEBUG TRAIN Batch 16/6300 loss 10.061775 loss_att 10.894131 loss_ctc 12.996596 loss_rnnt 9.296223 hw_loss 0.389572 lr 0.00042290 rank 7
2023-02-23 08:46:43,634 DEBUG TRAIN Batch 16/6300 loss 13.161207 loss_att 17.999121 loss_ctc 21.064344 loss_rnnt 10.904686 hw_loss 0.440975 lr 0.00042299 rank 1
2023-02-23 08:46:43,635 DEBUG TRAIN Batch 16/6300 loss 10.945423 loss_att 11.647423 loss_ctc 12.570674 loss_rnnt 10.314345 hw_loss 0.513709 lr 0.00042290 rank 3
2023-02-23 08:46:43,636 DEBUG TRAIN Batch 16/6300 loss 18.961475 loss_att 20.436852 loss_ctc 23.952341 loss_rnnt 17.807203 hw_loss 0.363276 lr 0.00042296 rank 6
2023-02-23 08:46:43,640 DEBUG TRAIN Batch 16/6300 loss 11.762602 loss_att 13.503668 loss_ctc 14.353361 loss_rnnt 10.814625 hw_loss 0.476866 lr 0.00042295 rank 4
2023-02-23 08:46:43,689 DEBUG TRAIN Batch 16/6300 loss 13.603859 loss_att 14.856901 loss_ctc 20.419249 loss_rnnt 12.214086 hw_loss 0.432085 lr 0.00042289 rank 5
2023-02-23 08:47:58,942 DEBUG TRAIN Batch 16/6400 loss 3.733099 loss_att 8.402924 loss_ctc 3.735497 loss_rnnt 2.583391 hw_loss 0.403919 lr 0.00042275 rank 7
2023-02-23 08:47:58,947 DEBUG TRAIN Batch 16/6400 loss 14.809206 loss_att 14.752429 loss_ctc 12.982810 loss_rnnt 14.858951 hw_loss 0.384623 lr 0.00042281 rank 2
2023-02-23 08:47:58,948 DEBUG TRAIN Batch 16/6400 loss 5.976016 loss_att 7.151948 loss_ctc 6.960668 loss_rnnt 5.349221 hw_loss 0.488101 lr 0.00042282 rank 0
2023-02-23 08:47:58,950 DEBUG TRAIN Batch 16/6400 loss 7.971361 loss_att 8.824369 loss_ctc 10.540763 loss_rnnt 7.117825 hw_loss 0.638151 lr 0.00042281 rank 6
2023-02-23 08:47:58,951 DEBUG TRAIN Batch 16/6400 loss 6.842421 loss_att 10.673212 loss_ctc 9.765746 loss_rnnt 5.468440 hw_loss 0.408836 lr 0.00042275 rank 3
2023-02-23 08:47:58,954 DEBUG TRAIN Batch 16/6400 loss 6.838758 loss_att 8.415261 loss_ctc 9.081466 loss_rnnt 5.961396 hw_loss 0.493186 lr 0.00042284 rank 1
2023-02-23 08:47:58,982 DEBUG TRAIN Batch 16/6400 loss 20.116833 loss_att 23.801773 loss_ctc 31.649189 loss_rnnt 17.626465 hw_loss 0.404499 lr 0.00042280 rank 4
2023-02-23 08:47:59,012 DEBUG TRAIN Batch 16/6400 loss 7.084712 loss_att 10.185674 loss_ctc 13.986742 loss_rnnt 5.300593 hw_loss 0.456853 lr 0.00042273 rank 5
2023-02-23 08:49:11,821 DEBUG TRAIN Batch 16/6500 loss 12.521217 loss_att 15.296413 loss_ctc 17.607294 loss_rnnt 11.054190 hw_loss 0.438459 lr 0.00042260 rank 7
2023-02-23 08:49:11,821 DEBUG TRAIN Batch 16/6500 loss 10.195805 loss_att 11.574327 loss_ctc 16.043930 loss_rnnt 8.911376 hw_loss 0.429328 lr 0.00042266 rank 2
2023-02-23 08:49:11,823 DEBUG TRAIN Batch 16/6500 loss 4.583505 loss_att 7.602114 loss_ctc 8.004612 loss_rnnt 3.267771 hw_loss 0.479745 lr 0.00042260 rank 3
2023-02-23 08:49:11,827 DEBUG TRAIN Batch 16/6500 loss 19.419462 loss_att 21.306110 loss_ctc 23.211607 loss_rnnt 18.332481 hw_loss 0.382555 lr 0.00042266 rank 6
2023-02-23 08:49:11,830 DEBUG TRAIN Batch 16/6500 loss 16.019054 loss_att 19.749523 loss_ctc 22.615757 loss_rnnt 14.202497 hw_loss 0.357943 lr 0.00042268 rank 1
2023-02-23 08:49:11,830 DEBUG TRAIN Batch 16/6500 loss 10.266275 loss_att 10.244305 loss_ctc 13.976977 loss_rnnt 9.474098 hw_loss 0.565895 lr 0.00042267 rank 0
2023-02-23 08:49:11,840 DEBUG TRAIN Batch 16/6500 loss 13.108574 loss_att 15.933814 loss_ctc 18.175457 loss_rnnt 11.640659 hw_loss 0.426153 lr 0.00042258 rank 5
2023-02-23 08:49:11,873 DEBUG TRAIN Batch 16/6500 loss 4.301798 loss_att 7.233785 loss_ctc 4.083649 loss_rnnt 3.528703 hw_loss 0.404596 lr 0.00042265 rank 4
2023-02-23 08:50:24,953 DEBUG TRAIN Batch 16/6600 loss 15.080864 loss_att 17.881483 loss_ctc 20.162252 loss_rnnt 13.626898 hw_loss 0.405606 lr 0.00042243 rank 5
2023-02-23 08:50:24,966 DEBUG TRAIN Batch 16/6600 loss 4.509977 loss_att 9.453467 loss_ctc 7.046109 loss_rnnt 3.023510 hw_loss 0.299284 lr 0.00042245 rank 7
2023-02-23 08:50:24,968 DEBUG TRAIN Batch 16/6600 loss 6.791468 loss_att 13.067324 loss_ctc 12.747604 loss_rnnt 4.451465 hw_loss 0.545024 lr 0.00042250 rank 6
2023-02-23 08:50:24,968 DEBUG TRAIN Batch 16/6600 loss 10.082274 loss_att 14.466681 loss_ctc 14.627403 loss_rnnt 8.414001 hw_loss 0.347577 lr 0.00042251 rank 2
2023-02-23 08:50:24,969 DEBUG TRAIN Batch 16/6600 loss 7.294667 loss_att 9.879087 loss_ctc 9.393325 loss_rnnt 6.317512 hw_loss 0.338344 lr 0.00042250 rank 4
2023-02-23 08:50:24,970 DEBUG TRAIN Batch 16/6600 loss 13.458225 loss_att 19.923332 loss_ctc 21.771801 loss_rnnt 10.871668 hw_loss 0.346984 lr 0.00042245 rank 3
2023-02-23 08:50:24,973 DEBUG TRAIN Batch 16/6600 loss 8.229090 loss_att 10.901949 loss_ctc 11.637106 loss_rnnt 7.059367 hw_loss 0.338904 lr 0.00042253 rank 1
2023-02-23 08:50:24,974 DEBUG TRAIN Batch 16/6600 loss 25.897720 loss_att 29.566015 loss_ctc 35.529770 loss_rnnt 23.680603 hw_loss 0.373473 lr 0.00042252 rank 0
2023-02-23 08:51:39,322 DEBUG TRAIN Batch 16/6700 loss 12.786291 loss_att 15.235934 loss_ctc 16.411743 loss_rnnt 11.585177 hw_loss 0.427110 lr 0.00042230 rank 7
2023-02-23 08:51:39,328 DEBUG TRAIN Batch 16/6700 loss 19.742430 loss_att 25.313210 loss_ctc 26.633772 loss_rnnt 17.500404 hw_loss 0.391916 lr 0.00042237 rank 0
2023-02-23 08:51:39,332 DEBUG TRAIN Batch 16/6700 loss 15.006705 loss_att 16.779884 loss_ctc 20.645256 loss_rnnt 13.615778 hw_loss 0.533408 lr 0.00042238 rank 1
2023-02-23 08:51:39,344 DEBUG TRAIN Batch 16/6700 loss 4.431418 loss_att 6.330544 loss_ctc 4.223857 loss_rnnt 3.857664 hw_loss 0.415507 lr 0.00042228 rank 5
2023-02-23 08:51:39,347 DEBUG TRAIN Batch 16/6700 loss 13.410967 loss_att 15.074385 loss_ctc 17.911659 loss_rnnt 12.221713 hw_loss 0.480895 lr 0.00042235 rank 4
2023-02-23 08:51:39,348 DEBUG TRAIN Batch 16/6700 loss 7.735815 loss_att 12.626066 loss_ctc 9.977203 loss_rnnt 6.264103 hw_loss 0.365268 lr 0.00042230 rank 3
2023-02-23 08:51:39,349 DEBUG TRAIN Batch 16/6700 loss 6.817884 loss_att 9.541754 loss_ctc 9.420308 loss_rnnt 5.650683 hw_loss 0.516446 lr 0.00042236 rank 2
2023-02-23 08:51:39,352 DEBUG TRAIN Batch 16/6700 loss 13.465464 loss_att 16.018570 loss_ctc 19.531101 loss_rnnt 11.949932 hw_loss 0.367798 lr 0.00042235 rank 6
2023-02-23 08:52:53,766 DEBUG TRAIN Batch 16/6800 loss 18.199862 loss_att 17.065271 loss_ctc 25.099548 loss_rnnt 17.288872 hw_loss 0.408652 lr 0.00042221 rank 2
2023-02-23 08:52:53,770 DEBUG TRAIN Batch 16/6800 loss 11.829466 loss_att 14.941217 loss_ctc 23.790260 loss_rnnt 9.364713 hw_loss 0.464306 lr 0.00042214 rank 7
2023-02-23 08:52:53,772 DEBUG TRAIN Batch 16/6800 loss 7.653247 loss_att 11.605927 loss_ctc 9.837204 loss_rnnt 6.345754 hw_loss 0.423306 lr 0.00042213 rank 5
2023-02-23 08:52:53,772 DEBUG TRAIN Batch 16/6800 loss 8.772195 loss_att 13.103711 loss_ctc 13.139412 loss_rnnt 7.128172 hw_loss 0.366418 lr 0.00042223 rank 1
2023-02-23 08:52:53,774 DEBUG TRAIN Batch 16/6800 loss 5.561639 loss_att 8.786411 loss_ctc 5.691014 loss_rnnt 4.682079 hw_loss 0.407541 lr 0.00042220 rank 6
2023-02-23 08:52:53,775 DEBUG TRAIN Batch 16/6800 loss 10.042668 loss_att 13.738902 loss_ctc 11.995231 loss_rnnt 8.821250 hw_loss 0.415934 lr 0.00042215 rank 3
2023-02-23 08:52:53,777 DEBUG TRAIN Batch 16/6800 loss 5.275941 loss_att 10.446348 loss_ctc 9.856554 loss_rnnt 3.431794 hw_loss 0.373721 lr 0.00042220 rank 4
2023-02-23 08:52:53,778 DEBUG TRAIN Batch 16/6800 loss 5.007933 loss_att 7.151722 loss_ctc 6.461730 loss_rnnt 4.179997 hw_loss 0.385008 lr 0.00042222 rank 0
2023-02-23 08:54:06,709 DEBUG TRAIN Batch 16/6900 loss 16.117571 loss_att 16.173353 loss_ctc 20.686380 loss_rnnt 15.289533 hw_loss 0.389453 lr 0.00042199 rank 7
2023-02-23 08:54:06,716 DEBUG TRAIN Batch 16/6900 loss 7.401022 loss_att 7.875268 loss_ctc 9.407789 loss_rnnt 6.740578 hw_loss 0.558800 lr 0.00042200 rank 3
2023-02-23 08:54:06,720 DEBUG TRAIN Batch 16/6900 loss 6.659477 loss_att 6.775851 loss_ctc 8.152023 loss_rnnt 6.026047 hw_loss 0.770904 lr 0.00042206 rank 2
2023-02-23 08:54:06,720 DEBUG TRAIN Batch 16/6900 loss 11.952628 loss_att 14.187502 loss_ctc 18.345238 loss_rnnt 10.399308 hw_loss 0.476243 lr 0.00042198 rank 5
2023-02-23 08:54:06,720 DEBUG TRAIN Batch 16/6900 loss 10.918562 loss_att 10.611848 loss_ctc 12.745594 loss_rnnt 10.498619 hw_loss 0.445655 lr 0.00042205 rank 4
2023-02-23 08:54:06,722 DEBUG TRAIN Batch 16/6900 loss 14.987993 loss_att 17.684990 loss_ctc 24.919041 loss_rnnt 12.931820 hw_loss 0.361189 lr 0.00042205 rank 6
2023-02-23 08:54:06,724 DEBUG TRAIN Batch 16/6900 loss 10.804347 loss_att 14.032975 loss_ctc 14.710772 loss_rnnt 9.441247 hw_loss 0.368472 lr 0.00042208 rank 1
2023-02-23 08:54:06,726 DEBUG TRAIN Batch 16/6900 loss 6.828309 loss_att 7.774970 loss_ctc 7.694155 loss_rnnt 6.322048 hw_loss 0.377780 lr 0.00042207 rank 0
2023-02-23 08:55:18,942 DEBUG TRAIN Batch 16/7000 loss 4.177377 loss_att 6.876605 loss_ctc 6.826463 loss_rnnt 3.101060 hw_loss 0.343613 lr 0.00042184 rank 7
2023-02-23 08:55:18,942 DEBUG TRAIN Batch 16/7000 loss 6.149332 loss_att 9.815306 loss_ctc 6.795070 loss_rnnt 5.110868 hw_loss 0.410947 lr 0.00042185 rank 3
2023-02-23 08:55:18,949 DEBUG TRAIN Batch 16/7000 loss 14.640130 loss_att 23.043701 loss_ctc 21.040836 loss_rnnt 11.915062 hw_loss 0.357986 lr 0.00042191 rank 2
2023-02-23 08:55:18,949 DEBUG TRAIN Batch 16/7000 loss 13.242604 loss_att 14.759311 loss_ctc 17.405277 loss_rnnt 12.131189 hw_loss 0.474467 lr 0.00042183 rank 5
2023-02-23 08:55:18,950 DEBUG TRAIN Batch 16/7000 loss 16.391243 loss_att 17.619213 loss_ctc 19.256184 loss_rnnt 15.501029 hw_loss 0.492427 lr 0.00042192 rank 0
2023-02-23 08:55:18,951 DEBUG TRAIN Batch 16/7000 loss 10.731910 loss_att 12.502935 loss_ctc 14.257957 loss_rnnt 9.694227 hw_loss 0.400007 lr 0.00042190 rank 6
2023-02-23 08:55:18,956 DEBUG TRAIN Batch 16/7000 loss 9.355536 loss_att 12.364735 loss_ctc 13.689165 loss_rnnt 7.929416 hw_loss 0.462116 lr 0.00042190 rank 4
2023-02-23 08:55:19,000 DEBUG TRAIN Batch 16/7000 loss 2.477984 loss_att 5.280010 loss_ctc 3.967735 loss_rnnt 1.502402 hw_loss 0.406019 lr 0.00042193 rank 1
2023-02-23 08:56:34,020 DEBUG TRAIN Batch 16/7100 loss 10.269035 loss_att 9.754319 loss_ctc 13.329816 loss_rnnt 9.541091 hw_loss 0.792719 lr 0.00042175 rank 4
2023-02-23 08:56:34,022 DEBUG TRAIN Batch 16/7100 loss 8.892801 loss_att 10.883068 loss_ctc 9.961685 loss_rnnt 8.131025 hw_loss 0.414759 lr 0.00042170 rank 3
2023-02-23 08:56:34,024 DEBUG TRAIN Batch 16/7100 loss 8.560965 loss_att 10.365604 loss_ctc 11.693046 loss_rnnt 7.504527 hw_loss 0.521060 lr 0.00042178 rank 1
2023-02-23 08:56:34,026 DEBUG TRAIN Batch 16/7100 loss 9.258210 loss_att 11.577521 loss_ctc 10.813022 loss_rnnt 8.353359 hw_loss 0.438151 lr 0.00042169 rank 7
2023-02-23 08:56:34,028 DEBUG TRAIN Batch 16/7100 loss 12.790133 loss_att 17.241516 loss_ctc 18.758791 loss_rnnt 10.933811 hw_loss 0.319167 lr 0.00042176 rank 2
2023-02-23 08:56:34,029 DEBUG TRAIN Batch 16/7100 loss 12.293335 loss_att 11.710228 loss_ctc 15.645187 loss_rnnt 11.664188 hw_loss 0.560351 lr 0.00042177 rank 0
2023-02-23 08:56:34,031 DEBUG TRAIN Batch 16/7100 loss 28.437077 loss_att 31.874462 loss_ctc 40.385342 loss_rnnt 25.935204 hw_loss 0.414926 lr 0.00042168 rank 5
2023-02-23 08:56:34,070 DEBUG TRAIN Batch 16/7100 loss 9.334929 loss_att 15.319704 loss_ctc 15.497970 loss_rnnt 7.126174 hw_loss 0.356364 lr 0.00042175 rank 6
2023-02-23 08:57:46,932 DEBUG TRAIN Batch 16/7200 loss 11.029727 loss_att 12.207401 loss_ctc 15.019595 loss_rnnt 10.064436 hw_loss 0.370827 lr 0.00042154 rank 7
2023-02-23 08:57:46,939 DEBUG TRAIN Batch 16/7200 loss 7.514319 loss_att 10.080982 loss_ctc 7.433362 loss_rnnt 6.810003 hw_loss 0.378332 lr 0.00042160 rank 4
2023-02-23 08:57:46,941 DEBUG TRAIN Batch 16/7200 loss 7.876553 loss_att 14.143623 loss_ctc 10.167234 loss_rnnt 6.151249 hw_loss 0.312122 lr 0.00042155 rank 3
2023-02-23 08:57:46,943 DEBUG TRAIN Batch 16/7200 loss 10.281408 loss_att 14.230881 loss_ctc 15.015201 loss_rnnt 8.601189 hw_loss 0.485912 lr 0.00042163 rank 1
2023-02-23 08:57:46,944 DEBUG TRAIN Batch 16/7200 loss 11.451324 loss_att 15.898270 loss_ctc 16.452135 loss_rnnt 9.705639 hw_loss 0.355353 lr 0.00042161 rank 2
2023-02-23 08:57:46,945 DEBUG TRAIN Batch 16/7200 loss 3.939082 loss_att 6.625151 loss_ctc 4.306575 loss_rnnt 3.132936 hw_loss 0.412375 lr 0.00042162 rank 0
2023-02-23 08:57:46,945 DEBUG TRAIN Batch 16/7200 loss 16.097885 loss_att 19.054977 loss_ctc 18.409981 loss_rnnt 14.976555 hw_loss 0.415561 lr 0.00042153 rank 5
2023-02-23 08:57:46,993 DEBUG TRAIN Batch 16/7200 loss 23.198223 loss_att 30.467089 loss_ctc 24.887760 loss_rnnt 21.263664 hw_loss 0.479087 lr 0.00042160 rank 6
2023-02-23 08:59:00,138 DEBUG TRAIN Batch 16/7300 loss 7.049658 loss_att 9.517736 loss_ctc 9.174852 loss_rnnt 6.024774 hw_loss 0.464830 lr 0.00042145 rank 6
2023-02-23 08:59:00,142 DEBUG TRAIN Batch 16/7300 loss 10.198921 loss_att 11.392160 loss_ctc 11.746805 loss_rnnt 9.526768 hw_loss 0.425852 lr 0.00042140 rank 3
2023-02-23 08:59:00,144 DEBUG TRAIN Batch 16/7300 loss 11.706233 loss_att 11.714314 loss_ctc 14.026996 loss_rnnt 11.158604 hw_loss 0.443584 lr 0.00042139 rank 7
2023-02-23 08:59:00,146 DEBUG TRAIN Batch 16/7300 loss 3.324826 loss_att 6.765708 loss_ctc 4.721519 loss_rnnt 2.280381 hw_loss 0.318829 lr 0.00042147 rank 0
2023-02-23 08:59:00,147 DEBUG TRAIN Batch 16/7300 loss 11.994745 loss_att 18.351570 loss_ctc 18.329210 loss_rnnt 9.637244 hw_loss 0.452890 lr 0.00042145 rank 4
2023-02-23 08:59:00,150 DEBUG TRAIN Batch 16/7300 loss 9.440241 loss_att 11.598140 loss_ctc 10.078373 loss_rnnt 8.736929 hw_loss 0.349964 lr 0.00042138 rank 5
2023-02-23 08:59:00,151 DEBUG TRAIN Batch 16/7300 loss 3.625071 loss_att 6.768960 loss_ctc 5.266563 loss_rnnt 2.589828 hw_loss 0.351749 lr 0.00042146 rank 2
2023-02-23 08:59:00,196 DEBUG TRAIN Batch 16/7300 loss 4.909872 loss_att 7.918215 loss_ctc 6.394855 loss_rnnt 3.873952 hw_loss 0.442976 lr 0.00042148 rank 1
2023-02-23 09:00:13,986 DEBUG TRAIN Batch 16/7400 loss 25.689499 loss_att 27.270716 loss_ctc 30.197624 loss_rnnt 24.606909 hw_loss 0.309870 lr 0.00042132 rank 0
2023-02-23 09:00:13,997 DEBUG TRAIN Batch 16/7400 loss 7.041378 loss_att 10.585364 loss_ctc 10.621885 loss_rnnt 5.629723 hw_loss 0.422731 lr 0.00042124 rank 7
2023-02-23 09:00:13,998 DEBUG TRAIN Batch 16/7400 loss 10.097246 loss_att 12.196550 loss_ctc 12.759821 loss_rnnt 9.077686 hw_loss 0.458790 lr 0.00042130 rank 6
2023-02-23 09:00:13,998 DEBUG TRAIN Batch 16/7400 loss 9.777503 loss_att 12.335081 loss_ctc 12.575642 loss_rnnt 8.687957 hw_loss 0.384273 lr 0.00042130 rank 4
2023-02-23 09:00:13,999 DEBUG TRAIN Batch 16/7400 loss 12.957637 loss_att 17.577208 loss_ctc 16.160587 loss_rnnt 11.410559 hw_loss 0.367696 lr 0.00042133 rank 1
2023-02-23 09:00:14,000 DEBUG TRAIN Batch 16/7400 loss 7.206966 loss_att 10.631783 loss_ctc 9.662565 loss_rnnt 5.983153 hw_loss 0.396442 lr 0.00042125 rank 3
2023-02-23 09:00:14,007 DEBUG TRAIN Batch 16/7400 loss 26.502319 loss_att 30.660677 loss_ctc 32.092422 loss_rnnt 24.675945 hw_loss 0.467543 lr 0.00042131 rank 2
2023-02-23 09:00:14,047 DEBUG TRAIN Batch 16/7400 loss 13.327408 loss_att 19.186310 loss_ctc 18.048645 loss_rnnt 11.310829 hw_loss 0.403685 lr 0.00042123 rank 5
2023-02-23 09:01:28,867 DEBUG TRAIN Batch 16/7500 loss 12.035295 loss_att 13.947754 loss_ctc 15.122072 loss_rnnt 10.960911 hw_loss 0.525606 lr 0.00042110 rank 7
2023-02-23 09:01:28,871 DEBUG TRAIN Batch 16/7500 loss 9.364728 loss_att 9.640206 loss_ctc 13.346844 loss_rnnt 8.478027 hw_loss 0.563731 lr 0.00042110 rank 3
2023-02-23 09:01:28,874 DEBUG TRAIN Batch 16/7500 loss 4.135679 loss_att 8.070648 loss_ctc 7.121695 loss_rnnt 2.723294 hw_loss 0.426104 lr 0.00042115 rank 4
2023-02-23 09:01:28,875 DEBUG TRAIN Batch 16/7500 loss 12.090697 loss_att 14.597917 loss_ctc 15.459703 loss_rnnt 10.926113 hw_loss 0.401136 lr 0.00042117 rank 0
2023-02-23 09:01:28,876 DEBUG TRAIN Batch 16/7500 loss 14.118922 loss_att 17.581896 loss_ctc 18.248121 loss_rnnt 12.650784 hw_loss 0.421847 lr 0.00042115 rank 6
2023-02-23 09:01:28,877 DEBUG TRAIN Batch 16/7500 loss 9.236073 loss_att 9.568967 loss_ctc 11.570918 loss_rnnt 8.635933 hw_loss 0.416716 lr 0.00042116 rank 2
2023-02-23 09:01:28,884 DEBUG TRAIN Batch 16/7500 loss 10.975388 loss_att 16.059410 loss_ctc 18.659872 loss_rnnt 8.715810 hw_loss 0.409079 lr 0.00042118 rank 1
2023-02-23 09:01:28,928 DEBUG TRAIN Batch 16/7500 loss 8.145089 loss_att 11.606976 loss_ctc 10.682268 loss_rnnt 6.861095 hw_loss 0.474985 lr 0.00042108 rank 5
2023-02-23 09:02:42,071 DEBUG TRAIN Batch 16/7600 loss 13.908033 loss_att 15.031393 loss_ctc 22.951797 loss_rnnt 12.259295 hw_loss 0.409184 lr 0.00042095 rank 3
2023-02-23 09:02:42,072 DEBUG TRAIN Batch 16/7600 loss 5.536132 loss_att 7.499605 loss_ctc 8.335292 loss_rnnt 4.564456 hw_loss 0.385800 lr 0.00042093 rank 5
2023-02-23 09:02:42,076 DEBUG TRAIN Batch 16/7600 loss 12.886344 loss_att 16.937128 loss_ctc 20.491837 loss_rnnt 10.792563 hw_loss 0.505422 lr 0.00042100 rank 6
2023-02-23 09:02:42,077 DEBUG TRAIN Batch 16/7600 loss 7.838334 loss_att 9.049388 loss_ctc 12.069852 loss_rnnt 6.780292 hw_loss 0.471803 lr 0.00042095 rank 7
2023-02-23 09:02:42,078 DEBUG TRAIN Batch 16/7600 loss 6.215672 loss_att 9.260067 loss_ctc 6.249043 loss_rnnt 5.346973 hw_loss 0.478820 lr 0.00042101 rank 2
2023-02-23 09:02:42,079 DEBUG TRAIN Batch 16/7600 loss 8.388506 loss_att 11.778164 loss_ctc 9.891165 loss_rnnt 7.308790 hw_loss 0.377681 lr 0.00042100 rank 4
2023-02-23 09:02:42,081 DEBUG TRAIN Batch 16/7600 loss 6.322278 loss_att 10.534907 loss_ctc 8.146544 loss_rnnt 4.919014 hw_loss 0.595317 lr 0.00042103 rank 1
2023-02-23 09:02:42,128 DEBUG TRAIN Batch 16/7600 loss 7.099072 loss_att 8.524596 loss_ctc 10.189537 loss_rnnt 6.202548 hw_loss 0.373796 lr 0.00042102 rank 0
2023-02-23 09:03:55,388 DEBUG TRAIN Batch 16/7700 loss 12.536534 loss_att 17.839642 loss_ctc 15.975189 loss_rnnt 10.861987 hw_loss 0.291448 lr 0.00042080 rank 7
2023-02-23 09:03:55,394 DEBUG TRAIN Batch 16/7700 loss 9.935133 loss_att 12.139764 loss_ctc 14.694440 loss_rnnt 8.587166 hw_loss 0.510874 lr 0.00042088 rank 1
2023-02-23 09:03:55,398 DEBUG TRAIN Batch 16/7700 loss 12.459718 loss_att 15.582798 loss_ctc 16.796904 loss_rnnt 11.007286 hw_loss 0.467859 lr 0.00042087 rank 0
2023-02-23 09:03:55,398 DEBUG TRAIN Batch 16/7700 loss 14.513621 loss_att 18.570183 loss_ctc 15.943576 loss_rnnt 13.263510 hw_loss 0.465261 lr 0.00042080 rank 3
2023-02-23 09:03:55,398 DEBUG TRAIN Batch 16/7700 loss 8.290417 loss_att 9.312682 loss_ctc 11.981486 loss_rnnt 7.274554 hw_loss 0.598626 lr 0.00042085 rank 4
2023-02-23 09:03:55,399 DEBUG TRAIN Batch 16/7700 loss 3.144414 loss_att 7.948554 loss_ctc 4.034312 loss_rnnt 1.894012 hw_loss 0.320476 lr 0.00042086 rank 6
2023-02-23 09:03:55,401 DEBUG TRAIN Batch 16/7700 loss 9.854190 loss_att 12.583094 loss_ctc 14.598967 loss_rnnt 8.455974 hw_loss 0.412124 lr 0.00042086 rank 2
2023-02-23 09:03:55,409 DEBUG TRAIN Batch 16/7700 loss 12.522830 loss_att 16.378159 loss_ctc 14.282978 loss_rnnt 11.342308 hw_loss 0.327693 lr 0.00042078 rank 5
2023-02-23 09:05:10,815 DEBUG TRAIN Batch 16/7800 loss 16.255236 loss_att 19.966156 loss_ctc 16.661835 loss_rnnt 15.262294 hw_loss 0.368523 lr 0.00042070 rank 4
2023-02-23 09:05:10,824 DEBUG TRAIN Batch 16/7800 loss 18.043945 loss_att 23.033169 loss_ctc 25.690861 loss_rnnt 15.748065 hw_loss 0.522088 lr 0.00042071 rank 2
2023-02-23 09:05:10,829 DEBUG TRAIN Batch 16/7800 loss 17.378935 loss_att 20.199997 loss_ctc 21.808907 loss_rnnt 16.038036 hw_loss 0.348792 lr 0.00042063 rank 5
2023-02-23 09:05:10,831 DEBUG TRAIN Batch 16/7800 loss 12.243751 loss_att 19.574951 loss_ctc 19.101864 loss_rnnt 9.673779 hw_loss 0.354968 lr 0.00042072 rank 0
2023-02-23 09:05:10,832 DEBUG TRAIN Batch 16/7800 loss 10.285111 loss_att 11.939872 loss_ctc 14.895958 loss_rnnt 9.159381 hw_loss 0.337499 lr 0.00042065 rank 7
2023-02-23 09:05:10,835 DEBUG TRAIN Batch 16/7800 loss 20.915157 loss_att 25.619072 loss_ctc 26.023069 loss_rnnt 19.072220 hw_loss 0.414561 lr 0.00042073 rank 1
2023-02-23 09:05:10,835 DEBUG TRAIN Batch 16/7800 loss 11.471942 loss_att 12.217336 loss_ctc 15.119110 loss_rnnt 10.617767 hw_loss 0.410260 lr 0.00042065 rank 3
2023-02-23 09:05:10,847 DEBUG TRAIN Batch 16/7800 loss 6.975463 loss_att 9.619911 loss_ctc 8.730579 loss_rnnt 5.973845 hw_loss 0.447586 lr 0.00042071 rank 6
2023-02-23 09:06:24,399 DEBUG TRAIN Batch 16/7900 loss 19.827942 loss_att 25.704960 loss_ctc 27.647621 loss_rnnt 17.402309 hw_loss 0.389262 lr 0.00042050 rank 7
2023-02-23 09:06:24,401 DEBUG TRAIN Batch 16/7900 loss 9.448867 loss_att 14.517378 loss_ctc 17.224943 loss_rnnt 7.165376 hw_loss 0.436836 lr 0.00042055 rank 4
2023-02-23 09:06:24,404 DEBUG TRAIN Batch 16/7900 loss 6.936271 loss_att 10.941109 loss_ctc 12.148784 loss_rnnt 5.204467 hw_loss 0.442191 lr 0.00042059 rank 1
2023-02-23 09:06:24,405 DEBUG TRAIN Batch 16/7900 loss 7.748261 loss_att 10.586501 loss_ctc 8.480017 loss_rnnt 6.885848 hw_loss 0.369746 lr 0.00042050 rank 3
2023-02-23 09:06:24,406 DEBUG TRAIN Batch 16/7900 loss 11.266533 loss_att 15.219231 loss_ctc 17.182583 loss_rnnt 9.469600 hw_loss 0.407978 lr 0.00042056 rank 2
2023-02-23 09:06:24,408 DEBUG TRAIN Batch 16/7900 loss 11.522814 loss_att 17.319637 loss_ctc 17.263281 loss_rnnt 9.423976 hw_loss 0.326396 lr 0.00042056 rank 6
2023-02-23 09:06:24,411 DEBUG TRAIN Batch 16/7900 loss 4.318885 loss_att 8.435928 loss_ctc 7.428773 loss_rnnt 2.821759 hw_loss 0.485748 lr 0.00042049 rank 5
2023-02-23 09:06:24,463 DEBUG TRAIN Batch 16/7900 loss 5.381344 loss_att 7.543826 loss_ctc 5.694191 loss_rnnt 4.667891 hw_loss 0.448582 lr 0.00042057 rank 0
2023-02-23 09:07:37,237 DEBUG TRAIN Batch 16/8000 loss 13.059064 loss_att 14.008533 loss_ctc 16.072727 loss_rnnt 12.246109 hw_loss 0.414823 lr 0.00042035 rank 7
2023-02-23 09:07:37,242 DEBUG TRAIN Batch 16/8000 loss 10.488931 loss_att 12.646227 loss_ctc 11.942972 loss_rnnt 9.637192 hw_loss 0.424514 lr 0.00042041 rank 2
2023-02-23 09:07:37,247 DEBUG TRAIN Batch 16/8000 loss 8.572884 loss_att 11.056198 loss_ctc 8.827772 loss_rnnt 7.808924 hw_loss 0.437460 lr 0.00042042 rank 0
2023-02-23 09:07:37,249 DEBUG TRAIN Batch 16/8000 loss 16.176073 loss_att 15.360781 loss_ctc 21.246521 loss_rnnt 15.460661 hw_loss 0.379520 lr 0.00042036 rank 3
2023-02-23 09:07:37,250 DEBUG TRAIN Batch 16/8000 loss 18.952827 loss_att 24.849033 loss_ctc 25.241583 loss_rnnt 16.767548 hw_loss 0.314131 lr 0.00042041 rank 4
2023-02-23 09:07:37,253 DEBUG TRAIN Batch 16/8000 loss 11.675612 loss_att 16.170582 loss_ctc 16.385963 loss_rnnt 9.915436 hw_loss 0.437130 lr 0.00042041 rank 6
2023-02-23 09:07:37,256 DEBUG TRAIN Batch 16/8000 loss 29.005329 loss_att 30.393879 loss_ctc 32.786747 loss_rnnt 28.037081 hw_loss 0.349404 lr 0.00042034 rank 5
2023-02-23 09:07:37,303 DEBUG TRAIN Batch 16/8000 loss 12.875264 loss_att 13.758341 loss_ctc 19.736351 loss_rnnt 11.588351 hw_loss 0.366538 lr 0.00042044 rank 1
2023-02-23 09:08:50,683 DEBUG TRAIN Batch 16/8100 loss 15.468527 loss_att 17.061691 loss_ctc 19.701244 loss_rnnt 14.368269 hw_loss 0.407368 lr 0.00042026 rank 4
2023-02-23 09:08:50,688 DEBUG TRAIN Batch 16/8100 loss 17.642569 loss_att 20.718822 loss_ctc 27.099874 loss_rnnt 15.580550 hw_loss 0.348363 lr 0.00042019 rank 5
2023-02-23 09:08:50,687 DEBUG TRAIN Batch 16/8100 loss 8.427914 loss_att 9.572282 loss_ctc 12.198512 loss_rnnt 7.426148 hw_loss 0.506519 lr 0.00042029 rank 1
2023-02-23 09:08:50,688 DEBUG TRAIN Batch 16/8100 loss 13.127083 loss_att 16.325699 loss_ctc 18.136587 loss_rnnt 11.614275 hw_loss 0.384655 lr 0.00042020 rank 7
2023-02-23 09:08:50,691 DEBUG TRAIN Batch 16/8100 loss 6.397745 loss_att 8.582406 loss_ctc 10.382495 loss_rnnt 5.183588 hw_loss 0.461109 lr 0.00042027 rank 2
2023-02-23 09:08:50,693 DEBUG TRAIN Batch 16/8100 loss 7.718550 loss_att 10.341283 loss_ctc 11.926490 loss_rnnt 6.413110 hw_loss 0.412191 lr 0.00042021 rank 3
2023-02-23 09:08:50,694 DEBUG TRAIN Batch 16/8100 loss 4.408353 loss_att 7.509181 loss_ctc 5.433348 loss_rnnt 3.452456 hw_loss 0.373247 lr 0.00042028 rank 0
2023-02-23 09:08:50,696 DEBUG TRAIN Batch 16/8100 loss 14.159591 loss_att 14.697802 loss_ctc 23.944050 loss_rnnt 12.445248 hw_loss 0.566447 lr 0.00042026 rank 6
2023-02-23 09:10:04,228 DEBUG TRAIN Batch 16/8200 loss 13.636596 loss_att 19.363493 loss_ctc 22.683334 loss_rnnt 11.140888 hw_loss 0.270180 lr 0.00042006 rank 3
2023-02-23 09:10:04,241 DEBUG TRAIN Batch 16/8200 loss 10.662273 loss_att 11.506327 loss_ctc 12.358835 loss_rnnt 10.025031 hw_loss 0.454169 lr 0.00042005 rank 7
2023-02-23 09:10:04,244 DEBUG TRAIN Batch 16/8200 loss 5.168053 loss_att 8.124159 loss_ctc 7.532431 loss_rnnt 4.073824 hw_loss 0.352044 lr 0.00042004 rank 5
2023-02-23 09:10:04,245 DEBUG TRAIN Batch 16/8200 loss 14.516330 loss_att 17.808489 loss_ctc 18.326847 loss_rnnt 13.128542 hw_loss 0.414913 lr 0.00042013 rank 0
2023-02-23 09:10:04,246 DEBUG TRAIN Batch 16/8200 loss 7.281748 loss_att 9.819757 loss_ctc 8.542774 loss_rnnt 6.401427 hw_loss 0.383591 lr 0.00042012 rank 2
2023-02-23 09:10:04,248 DEBUG TRAIN Batch 16/8200 loss 10.290081 loss_att 10.860970 loss_ctc 18.217615 loss_rnnt 8.889303 hw_loss 0.430490 lr 0.00042011 rank 4
2023-02-23 09:10:04,249 DEBUG TRAIN Batch 16/8200 loss 10.284956 loss_att 12.789332 loss_ctc 11.278157 loss_rnnt 9.433022 hw_loss 0.409936 lr 0.00042011 rank 6
2023-02-23 09:10:04,296 DEBUG TRAIN Batch 16/8200 loss 10.953281 loss_att 12.230929 loss_ctc 11.436428 loss_rnnt 10.396820 hw_loss 0.443462 lr 0.00042014 rank 1
2023-02-23 09:11:16,226 DEBUG TRAIN Batch 16/8300 loss 8.788138 loss_att 12.788399 loss_ctc 17.356798 loss_rnnt 6.644832 hw_loss 0.376439 lr 0.00041997 rank 2
2023-02-23 09:11:16,241 DEBUG TRAIN Batch 16/8300 loss 6.621443 loss_att 12.359178 loss_ctc 9.596432 loss_rnnt 4.894321 hw_loss 0.342956 lr 0.00041991 rank 7
2023-02-23 09:11:16,248 DEBUG TRAIN Batch 16/8300 loss 19.446939 loss_att 20.687990 loss_ctc 24.260090 loss_rnnt 18.292873 hw_loss 0.495190 lr 0.00041989 rank 5
2023-02-23 09:11:16,248 DEBUG TRAIN Batch 16/8300 loss 10.296565 loss_att 11.839973 loss_ctc 12.077454 loss_rnnt 9.488619 hw_loss 0.490898 lr 0.00041999 rank 1
2023-02-23 09:11:16,249 DEBUG TRAIN Batch 16/8300 loss 5.286717 loss_att 8.554962 loss_ctc 6.144004 loss_rnnt 4.291433 hw_loss 0.426244 lr 0.00041996 rank 6
2023-02-23 09:11:16,251 DEBUG TRAIN Batch 16/8300 loss 10.028903 loss_att 13.072281 loss_ctc 12.474718 loss_rnnt 8.872560 hw_loss 0.415422 lr 0.00041998 rank 0
2023-02-23 09:11:16,251 DEBUG TRAIN Batch 16/8300 loss 17.226833 loss_att 16.772219 loss_ctc 20.568436 loss_rnnt 16.694767 hw_loss 0.332705 lr 0.00041991 rank 3
2023-02-23 09:11:16,253 DEBUG TRAIN Batch 16/8300 loss 7.344692 loss_att 8.563234 loss_ctc 8.172981 loss_rnnt 6.741472 hw_loss 0.467010 lr 0.00041996 rank 4
2023-02-23 09:12:08,547 DEBUG CV Batch 16/0 loss 1.978174 loss_att 1.823467 loss_ctc 2.491303 loss_rnnt 1.499692 hw_loss 0.826887 history loss 1.904909 rank 7
2023-02-23 09:12:08,551 DEBUG CV Batch 16/0 loss 1.978174 loss_att 1.823467 loss_ctc 2.491303 loss_rnnt 1.499692 hw_loss 0.826887 history loss 1.904909 rank 6
2023-02-23 09:12:08,553 DEBUG CV Batch 16/0 loss 1.978174 loss_att 1.823467 loss_ctc 2.491303 loss_rnnt 1.499692 hw_loss 0.826887 history loss 1.904909 rank 0
2023-02-23 09:12:08,559 DEBUG CV Batch 16/0 loss 1.978174 loss_att 1.823467 loss_ctc 2.491303 loss_rnnt 1.499692 hw_loss 0.826887 history loss 1.904909 rank 3
2023-02-23 09:12:08,560 DEBUG CV Batch 16/0 loss 1.978174 loss_att 1.823467 loss_ctc 2.491303 loss_rnnt 1.499692 hw_loss 0.826887 history loss 1.904909 rank 1
2023-02-23 09:12:08,566 DEBUG CV Batch 16/0 loss 1.978174 loss_att 1.823467 loss_ctc 2.491303 loss_rnnt 1.499692 hw_loss 0.826887 history loss 1.904909 rank 5
2023-02-23 09:12:08,568 DEBUG CV Batch 16/0 loss 1.978174 loss_att 1.823467 loss_ctc 2.491303 loss_rnnt 1.499692 hw_loss 0.826887 history loss 1.904909 rank 2
2023-02-23 09:12:08,572 DEBUG CV Batch 16/0 loss 1.978174 loss_att 1.823467 loss_ctc 2.491303 loss_rnnt 1.499692 hw_loss 0.826887 history loss 1.904909 rank 4
2023-02-23 09:12:20,004 DEBUG CV Batch 16/100 loss 5.811551 loss_att 6.706707 loss_ctc 7.472271 loss_rnnt 5.165887 hw_loss 0.459756 history loss 3.957068 rank 4
2023-02-23 09:12:20,055 DEBUG CV Batch 16/100 loss 5.811551 loss_att 6.706707 loss_ctc 7.472271 loss_rnnt 5.165887 hw_loss 0.459756 history loss 3.957068 rank 3
2023-02-23 09:12:20,154 DEBUG CV Batch 16/100 loss 5.811551 loss_att 6.706707 loss_ctc 7.472271 loss_rnnt 5.165887 hw_loss 0.459756 history loss 3.957068 rank 2
2023-02-23 09:12:20,280 DEBUG CV Batch 16/100 loss 5.811551 loss_att 6.706707 loss_ctc 7.472271 loss_rnnt 5.165887 hw_loss 0.459756 history loss 3.957068 rank 6
2023-02-23 09:12:20,378 DEBUG CV Batch 16/100 loss 5.811551 loss_att 6.706707 loss_ctc 7.472271 loss_rnnt 5.165887 hw_loss 0.459756 history loss 3.957068 rank 7
2023-02-23 09:12:20,413 DEBUG CV Batch 16/100 loss 5.811551 loss_att 6.706707 loss_ctc 7.472271 loss_rnnt 5.165887 hw_loss 0.459756 history loss 3.957068 rank 1
2023-02-23 09:12:20,417 DEBUG CV Batch 16/100 loss 5.811551 loss_att 6.706707 loss_ctc 7.472271 loss_rnnt 5.165887 hw_loss 0.459756 history loss 3.957068 rank 0
2023-02-23 09:12:20,522 DEBUG CV Batch 16/100 loss 5.811551 loss_att 6.706707 loss_ctc 7.472271 loss_rnnt 5.165887 hw_loss 0.459756 history loss 3.957068 rank 5
2023-02-23 09:12:33,563 DEBUG CV Batch 16/200 loss 6.742878 loss_att 20.882362 loss_ctc 8.061606 loss_rnnt 3.563299 hw_loss 0.329722 history loss 4.553308 rank 3
2023-02-23 09:12:33,737 DEBUG CV Batch 16/200 loss 6.742878 loss_att 20.882362 loss_ctc 8.061606 loss_rnnt 3.563299 hw_loss 0.329722 history loss 4.553308 rank 2
2023-02-23 09:12:33,999 DEBUG CV Batch 16/200 loss 6.742878 loss_att 20.882362 loss_ctc 8.061606 loss_rnnt 3.563299 hw_loss 0.329722 history loss 4.553308 rank 1
2023-02-23 09:12:34,500 DEBUG CV Batch 16/200 loss 6.742878 loss_att 20.882362 loss_ctc 8.061606 loss_rnnt 3.563299 hw_loss 0.329722 history loss 4.553308 rank 4
2023-02-23 09:12:34,593 DEBUG CV Batch 16/200 loss 6.742878 loss_att 20.882362 loss_ctc 8.061606 loss_rnnt 3.563299 hw_loss 0.329722 history loss 4.553308 rank 6
2023-02-23 09:12:34,654 DEBUG CV Batch 16/200 loss 6.742878 loss_att 20.882362 loss_ctc 8.061606 loss_rnnt 3.563299 hw_loss 0.329722 history loss 4.553308 rank 5
2023-02-23 09:12:34,749 DEBUG CV Batch 16/200 loss 6.742878 loss_att 20.882362 loss_ctc 8.061606 loss_rnnt 3.563299 hw_loss 0.329722 history loss 4.553308 rank 7
2023-02-23 09:12:34,937 DEBUG CV Batch 16/200 loss 6.742878 loss_att 20.882362 loss_ctc 8.061606 loss_rnnt 3.563299 hw_loss 0.329722 history loss 4.553308 rank 0
2023-02-23 09:12:45,776 DEBUG CV Batch 16/300 loss 5.232153 loss_att 6.034167 loss_ctc 8.347349 loss_rnnt 4.350750 hw_loss 0.573075 history loss 4.707114 rank 3
2023-02-23 09:12:45,937 DEBUG CV Batch 16/300 loss 5.232153 loss_att 6.034167 loss_ctc 8.347349 loss_rnnt 4.350750 hw_loss 0.573075 history loss 4.707114 rank 2
2023-02-23 09:12:46,119 DEBUG CV Batch 16/300 loss 5.232153 loss_att 6.034167 loss_ctc 8.347349 loss_rnnt 4.350750 hw_loss 0.573075 history loss 4.707114 rank 1
2023-02-23 09:12:46,632 DEBUG CV Batch 16/300 loss 5.232153 loss_att 6.034167 loss_ctc 8.347349 loss_rnnt 4.350750 hw_loss 0.573075 history loss 4.707114 rank 4
2023-02-23 09:12:46,841 DEBUG CV Batch 16/300 loss 5.232153 loss_att 6.034167 loss_ctc 8.347349 loss_rnnt 4.350750 hw_loss 0.573075 history loss 4.707114 rank 6
2023-02-23 09:12:46,856 DEBUG CV Batch 16/300 loss 5.232153 loss_att 6.034167 loss_ctc 8.347349 loss_rnnt 4.350750 hw_loss 0.573075 history loss 4.707114 rank 7
2023-02-23 09:12:47,303 DEBUG CV Batch 16/300 loss 5.232153 loss_att 6.034167 loss_ctc 8.347349 loss_rnnt 4.350750 hw_loss 0.573075 history loss 4.707114 rank 0
2023-02-23 09:12:47,551 DEBUG CV Batch 16/300 loss 5.232153 loss_att 6.034167 loss_ctc 8.347349 loss_rnnt 4.350750 hw_loss 0.573075 history loss 4.707114 rank 5
2023-02-23 09:12:57,943 DEBUG CV Batch 16/400 loss 18.185169 loss_att 77.630394 loss_ctc 7.998311 loss_rnnt 7.458042 hw_loss 0.368119 history loss 5.700024 rank 3
2023-02-23 09:12:58,190 DEBUG CV Batch 16/400 loss 18.185169 loss_att 77.630394 loss_ctc 7.998311 loss_rnnt 7.458042 hw_loss 0.368119 history loss 5.700024 rank 2
2023-02-23 09:12:58,436 DEBUG CV Batch 16/400 loss 18.185169 loss_att 77.630394 loss_ctc 7.998311 loss_rnnt 7.458042 hw_loss 0.368119 history loss 5.700024 rank 1
2023-02-23 09:12:58,752 DEBUG CV Batch 16/400 loss 18.185169 loss_att 77.630394 loss_ctc 7.998311 loss_rnnt 7.458042 hw_loss 0.368119 history loss 5.700024 rank 4
2023-02-23 09:12:58,846 DEBUG CV Batch 16/400 loss 18.185169 loss_att 77.630394 loss_ctc 7.998311 loss_rnnt 7.458042 hw_loss 0.368119 history loss 5.700024 rank 7
2023-02-23 09:12:59,093 DEBUG CV Batch 16/400 loss 18.185169 loss_att 77.630394 loss_ctc 7.998311 loss_rnnt 7.458042 hw_loss 0.368119 history loss 5.700024 rank 6
2023-02-23 09:12:59,682 DEBUG CV Batch 16/400 loss 18.185169 loss_att 77.630394 loss_ctc 7.998311 loss_rnnt 7.458042 hw_loss 0.368119 history loss 5.700024 rank 0
2023-02-23 09:13:00,496 DEBUG CV Batch 16/400 loss 18.185169 loss_att 77.630394 loss_ctc 7.998311 loss_rnnt 7.458042 hw_loss 0.368119 history loss 5.700024 rank 5
2023-02-23 09:13:08,506 DEBUG CV Batch 16/500 loss 5.391365 loss_att 6.523535 loss_ctc 7.275382 loss_rnnt 4.653393 hw_loss 0.488128 history loss 6.649361 rank 3
2023-02-23 09:13:08,844 DEBUG CV Batch 16/500 loss 5.391365 loss_att 6.523535 loss_ctc 7.275382 loss_rnnt 4.653393 hw_loss 0.488128 history loss 6.649361 rank 2
2023-02-23 09:13:08,992 DEBUG CV Batch 16/500 loss 5.391365 loss_att 6.523535 loss_ctc 7.275382 loss_rnnt 4.653393 hw_loss 0.488128 history loss 6.649361 rank 1
2023-02-23 09:13:09,537 DEBUG CV Batch 16/500 loss 5.391365 loss_att 6.523535 loss_ctc 7.275382 loss_rnnt 4.653393 hw_loss 0.488128 history loss 6.649361 rank 7
2023-02-23 09:13:09,743 DEBUG CV Batch 16/500 loss 5.391365 loss_att 6.523535 loss_ctc 7.275382 loss_rnnt 4.653393 hw_loss 0.488128 history loss 6.649361 rank 4
2023-02-23 09:13:10,059 DEBUG CV Batch 16/500 loss 5.391365 loss_att 6.523535 loss_ctc 7.275382 loss_rnnt 4.653393 hw_loss 0.488128 history loss 6.649361 rank 6
2023-02-23 09:13:10,453 DEBUG CV Batch 16/500 loss 5.391365 loss_att 6.523535 loss_ctc 7.275382 loss_rnnt 4.653393 hw_loss 0.488128 history loss 6.649361 rank 0
2023-02-23 09:13:11,782 DEBUG CV Batch 16/500 loss 5.391365 loss_att 6.523535 loss_ctc 7.275382 loss_rnnt 4.653393 hw_loss 0.488128 history loss 6.649361 rank 5
2023-02-23 09:13:20,762 DEBUG CV Batch 16/600 loss 8.944041 loss_att 7.836896 loss_ctc 10.556626 loss_rnnt 8.575466 hw_loss 0.703112 history loss 7.667280 rank 3
2023-02-23 09:13:21,173 DEBUG CV Batch 16/600 loss 8.944041 loss_att 7.836896 loss_ctc 10.556626 loss_rnnt 8.575466 hw_loss 0.703112 history loss 7.667280 rank 2
2023-02-23 09:13:21,174 DEBUG CV Batch 16/600 loss 8.944041 loss_att 7.836896 loss_ctc 10.556626 loss_rnnt 8.575466 hw_loss 0.703112 history loss 7.667280 rank 1
2023-02-23 09:13:22,090 DEBUG CV Batch 16/600 loss 8.944041 loss_att 7.836896 loss_ctc 10.556626 loss_rnnt 8.575466 hw_loss 0.703112 history loss 7.667280 rank 4
2023-02-23 09:13:22,301 DEBUG CV Batch 16/600 loss 8.944041 loss_att 7.836896 loss_ctc 10.556626 loss_rnnt 8.575466 hw_loss 0.703112 history loss 7.667280 rank 7
2023-02-23 09:13:22,646 DEBUG CV Batch 16/600 loss 8.944041 loss_att 7.836896 loss_ctc 10.556626 loss_rnnt 8.575466 hw_loss 0.703112 history loss 7.667280 rank 6
2023-02-23 09:13:23,145 DEBUG CV Batch 16/600 loss 8.944041 loss_att 7.836896 loss_ctc 10.556626 loss_rnnt 8.575466 hw_loss 0.703112 history loss 7.667280 rank 0
2023-02-23 09:13:24,606 DEBUG CV Batch 16/600 loss 8.944041 loss_att 7.836896 loss_ctc 10.556626 loss_rnnt 8.575466 hw_loss 0.703112 history loss 7.667280 rank 5
2023-02-23 09:13:32,268 DEBUG CV Batch 16/700 loss 23.084785 loss_att 67.926796 loss_ctc 19.910610 loss_rnnt 14.355508 hw_loss 0.345181 history loss 8.390964 rank 3
2023-02-23 09:13:32,565 DEBUG CV Batch 16/700 loss 23.084785 loss_att 67.926796 loss_ctc 19.910610 loss_rnnt 14.355508 hw_loss 0.345181 history loss 8.390964 rank 1
2023-02-23 09:13:32,852 DEBUG CV Batch 16/700 loss 23.084785 loss_att 67.926796 loss_ctc 19.910610 loss_rnnt 14.355508 hw_loss 0.345181 history loss 8.390964 rank 2
2023-02-23 09:13:34,142 DEBUG CV Batch 16/700 loss 23.084785 loss_att 67.926796 loss_ctc 19.910610 loss_rnnt 14.355508 hw_loss 0.345181 history loss 8.390964 rank 4
2023-02-23 09:13:34,200 DEBUG CV Batch 16/700 loss 23.084785 loss_att 67.926796 loss_ctc 19.910610 loss_rnnt 14.355508 hw_loss 0.345181 history loss 8.390964 rank 6
2023-02-23 09:13:35,181 DEBUG CV Batch 16/700 loss 23.084785 loss_att 67.926796 loss_ctc 19.910610 loss_rnnt 14.355508 hw_loss 0.345181 history loss 8.390964 rank 0
2023-02-23 09:13:35,189 DEBUG CV Batch 16/700 loss 23.084785 loss_att 67.926796 loss_ctc 19.910610 loss_rnnt 14.355508 hw_loss 0.345181 history loss 8.390964 rank 7
2023-02-23 09:13:36,665 DEBUG CV Batch 16/700 loss 23.084785 loss_att 67.926796 loss_ctc 19.910610 loss_rnnt 14.355508 hw_loss 0.345181 history loss 8.390964 rank 5
2023-02-23 09:13:43,474 DEBUG CV Batch 16/800 loss 10.146524 loss_att 10.234447 loss_ctc 14.777557 loss_rnnt 9.227662 hw_loss 0.532135 history loss 7.792951 rank 3
2023-02-23 09:13:43,729 DEBUG CV Batch 16/800 loss 10.146524 loss_att 10.234447 loss_ctc 14.777557 loss_rnnt 9.227662 hw_loss 0.532135 history loss 7.792951 rank 1
2023-02-23 09:13:44,264 DEBUG CV Batch 16/800 loss 10.146524 loss_att 10.234447 loss_ctc 14.777557 loss_rnnt 9.227662 hw_loss 0.532135 history loss 7.792951 rank 2
2023-02-23 09:13:45,704 DEBUG CV Batch 16/800 loss 10.146524 loss_att 10.234447 loss_ctc 14.777557 loss_rnnt 9.227662 hw_loss 0.532135 history loss 7.792951 rank 6
2023-02-23 09:13:46,203 DEBUG CV Batch 16/800 loss 10.146524 loss_att 10.234447 loss_ctc 14.777557 loss_rnnt 9.227662 hw_loss 0.532135 history loss 7.792951 rank 4
2023-02-23 09:13:47,212 DEBUG CV Batch 16/800 loss 10.146524 loss_att 10.234447 loss_ctc 14.777557 loss_rnnt 9.227662 hw_loss 0.532135 history loss 7.792951 rank 0
2023-02-23 09:13:47,911 DEBUG CV Batch 16/800 loss 10.146524 loss_att 10.234447 loss_ctc 14.777557 loss_rnnt 9.227662 hw_loss 0.532135 history loss 7.792951 rank 7
2023-02-23 09:13:48,804 DEBUG CV Batch 16/800 loss 10.146524 loss_att 10.234447 loss_ctc 14.777557 loss_rnnt 9.227662 hw_loss 0.532135 history loss 7.792951 rank 5
2023-02-23 09:13:56,948 DEBUG CV Batch 16/900 loss 12.640604 loss_att 24.209988 loss_ctc 22.156364 loss_rnnt 8.915535 hw_loss 0.267045 history loss 7.560965 rank 3
2023-02-23 09:13:57,137 DEBUG CV Batch 16/900 loss 12.640604 loss_att 24.209988 loss_ctc 22.156364 loss_rnnt 8.915535 hw_loss 0.267045 history loss 7.560965 rank 1
2023-02-23 09:13:57,977 DEBUG CV Batch 16/900 loss 12.640604 loss_att 24.209988 loss_ctc 22.156364 loss_rnnt 8.915535 hw_loss 0.267045 history loss 7.560965 rank 2
2023-02-23 09:13:59,721 DEBUG CV Batch 16/900 loss 12.640604 loss_att 24.209988 loss_ctc 22.156364 loss_rnnt 8.915535 hw_loss 0.267045 history loss 7.560965 rank 6
2023-02-23 09:14:00,246 DEBUG CV Batch 16/900 loss 12.640604 loss_att 24.209988 loss_ctc 22.156364 loss_rnnt 8.915535 hw_loss 0.267045 history loss 7.560965 rank 4
2023-02-23 09:14:01,404 DEBUG CV Batch 16/900 loss 12.640604 loss_att 24.209988 loss_ctc 22.156364 loss_rnnt 8.915535 hw_loss 0.267045 history loss 7.560965 rank 0
2023-02-23 09:14:02,142 DEBUG CV Batch 16/900 loss 12.640604 loss_att 24.209988 loss_ctc 22.156364 loss_rnnt 8.915535 hw_loss 0.267045 history loss 7.560965 rank 7
2023-02-23 09:14:02,763 DEBUG CV Batch 16/900 loss 12.640604 loss_att 24.209988 loss_ctc 22.156364 loss_rnnt 8.915535 hw_loss 0.267045 history loss 7.560965 rank 5
2023-02-23 09:14:09,432 DEBUG CV Batch 16/1000 loss 4.578544 loss_att 5.051184 loss_ctc 4.699063 loss_rnnt 4.158957 hw_loss 0.579357 history loss 7.300712 rank 1
2023-02-23 09:14:09,468 DEBUG CV Batch 16/1000 loss 4.578544 loss_att 5.051184 loss_ctc 4.699063 loss_rnnt 4.158957 hw_loss 0.579357 history loss 7.300712 rank 3
2023-02-23 09:14:10,714 DEBUG CV Batch 16/1000 loss 4.578544 loss_att 5.051184 loss_ctc 4.699063 loss_rnnt 4.158957 hw_loss 0.579357 history loss 7.300712 rank 2
2023-02-23 09:14:11,910 DEBUG CV Batch 16/1000 loss 4.578544 loss_att 5.051184 loss_ctc 4.699063 loss_rnnt 4.158957 hw_loss 0.579357 history loss 7.300712 rank 6
2023-02-23 09:14:13,158 DEBUG CV Batch 16/1000 loss 4.578544 loss_att 5.051184 loss_ctc 4.699063 loss_rnnt 4.158957 hw_loss 0.579357 history loss 7.300712 rank 4
2023-02-23 09:14:14,085 DEBUG CV Batch 16/1000 loss 4.578544 loss_att 5.051184 loss_ctc 4.699063 loss_rnnt 4.158957 hw_loss 0.579357 history loss 7.300712 rank 0
2023-02-23 09:14:14,603 DEBUG CV Batch 16/1000 loss 4.578544 loss_att 5.051184 loss_ctc 4.699063 loss_rnnt 4.158957 hw_loss 0.579357 history loss 7.300712 rank 7
2023-02-23 09:14:15,657 DEBUG CV Batch 16/1000 loss 4.578544 loss_att 5.051184 loss_ctc 4.699063 loss_rnnt 4.158957 hw_loss 0.579357 history loss 7.300712 rank 5
2023-02-23 09:14:21,659 DEBUG CV Batch 16/1100 loss 7.246244 loss_att 5.834683 loss_ctc 8.430616 loss_rnnt 6.947946 hw_loss 0.792551 history loss 7.264049 rank 3
2023-02-23 09:14:21,806 DEBUG CV Batch 16/1100 loss 7.246244 loss_att 5.834683 loss_ctc 8.430616 loss_rnnt 6.947946 hw_loss 0.792551 history loss 7.264049 rank 1
2023-02-23 09:14:23,027 DEBUG CV Batch 16/1100 loss 7.246244 loss_att 5.834683 loss_ctc 8.430616 loss_rnnt 6.947946 hw_loss 0.792551 history loss 7.264049 rank 2
2023-02-23 09:14:24,048 DEBUG CV Batch 16/1100 loss 7.246244 loss_att 5.834683 loss_ctc 8.430616 loss_rnnt 6.947946 hw_loss 0.792551 history loss 7.264049 rank 6
2023-02-23 09:14:25,323 DEBUG CV Batch 16/1100 loss 7.246244 loss_att 5.834683 loss_ctc 8.430616 loss_rnnt 6.947946 hw_loss 0.792551 history loss 7.264049 rank 4
2023-02-23 09:14:26,668 DEBUG CV Batch 16/1100 loss 7.246244 loss_att 5.834683 loss_ctc 8.430616 loss_rnnt 6.947946 hw_loss 0.792551 history loss 7.264049 rank 0
2023-02-23 09:14:26,737 DEBUG CV Batch 16/1100 loss 7.246244 loss_att 5.834683 loss_ctc 8.430616 loss_rnnt 6.947946 hw_loss 0.792551 history loss 7.264049 rank 7
2023-02-23 09:14:28,098 DEBUG CV Batch 16/1100 loss 7.246244 loss_att 5.834683 loss_ctc 8.430616 loss_rnnt 6.947946 hw_loss 0.792551 history loss 7.264049 rank 5
2023-02-23 09:14:32,443 DEBUG CV Batch 16/1200 loss 7.505445 loss_att 8.641762 loss_ctc 8.451537 loss_rnnt 6.912803 hw_loss 0.448561 history loss 7.661768 rank 3
2023-02-23 09:14:32,885 DEBUG CV Batch 16/1200 loss 7.505445 loss_att 8.641762 loss_ctc 8.451537 loss_rnnt 6.912803 hw_loss 0.448561 history loss 7.661768 rank 1
2023-02-23 09:14:34,196 DEBUG CV Batch 16/1200 loss 7.505445 loss_att 8.641762 loss_ctc 8.451537 loss_rnnt 6.912803 hw_loss 0.448561 history loss 7.661768 rank 2
2023-02-23 09:14:34,925 DEBUG CV Batch 16/1200 loss 7.505445 loss_att 8.641762 loss_ctc 8.451537 loss_rnnt 6.912803 hw_loss 0.448561 history loss 7.661768 rank 6
2023-02-23 09:14:35,923 DEBUG CV Batch 16/1200 loss 7.505445 loss_att 8.641762 loss_ctc 8.451537 loss_rnnt 6.912803 hw_loss 0.448561 history loss 7.661768 rank 4
2023-02-23 09:14:37,522 DEBUG CV Batch 16/1200 loss 7.505445 loss_att 8.641762 loss_ctc 8.451537 loss_rnnt 6.912803 hw_loss 0.448561 history loss 7.661768 rank 0
2023-02-23 09:14:37,682 DEBUG CV Batch 16/1200 loss 7.505445 loss_att 8.641762 loss_ctc 8.451537 loss_rnnt 6.912803 hw_loss 0.448561 history loss 7.661768 rank 7
2023-02-23 09:14:39,303 DEBUG CV Batch 16/1200 loss 7.505445 loss_att 8.641762 loss_ctc 8.451537 loss_rnnt 6.912803 hw_loss 0.448561 history loss 7.661768 rank 5
2023-02-23 09:14:44,633 DEBUG CV Batch 16/1300 loss 5.516027 loss_att 5.234581 loss_ctc 7.563779 loss_rnnt 4.946431 hw_loss 0.661596 history loss 7.992520 rank 3
2023-02-23 09:14:45,020 DEBUG CV Batch 16/1300 loss 5.516027 loss_att 5.234581 loss_ctc 7.563779 loss_rnnt 4.946431 hw_loss 0.661596 history loss 7.992520 rank 1
2023-02-23 09:14:46,553 DEBUG CV Batch 16/1300 loss 5.516027 loss_att 5.234581 loss_ctc 7.563779 loss_rnnt 4.946431 hw_loss 0.661596 history loss 7.992520 rank 2
2023-02-23 09:14:47,377 DEBUG CV Batch 16/1300 loss 5.516027 loss_att 5.234581 loss_ctc 7.563779 loss_rnnt 4.946431 hw_loss 0.661596 history loss 7.992520 rank 6
2023-02-23 09:14:48,053 DEBUG CV Batch 16/1300 loss 5.516027 loss_att 5.234581 loss_ctc 7.563779 loss_rnnt 4.946431 hw_loss 0.661596 history loss 7.992520 rank 4
2023-02-23 09:14:49,895 DEBUG CV Batch 16/1300 loss 5.516027 loss_att 5.234581 loss_ctc 7.563779 loss_rnnt 4.946431 hw_loss 0.661596 history loss 7.992520 rank 0
2023-02-23 09:14:49,969 DEBUG CV Batch 16/1300 loss 5.516027 loss_att 5.234581 loss_ctc 7.563779 loss_rnnt 4.946431 hw_loss 0.661596 history loss 7.992520 rank 7
2023-02-23 09:14:52,079 DEBUG CV Batch 16/1300 loss 5.516027 loss_att 5.234581 loss_ctc 7.563779 loss_rnnt 4.946431 hw_loss 0.661596 history loss 7.992520 rank 5
2023-02-23 09:14:56,061 DEBUG CV Batch 16/1400 loss 6.559020 loss_att 23.160622 loss_ctc 3.858205 loss_rnnt 3.404360 hw_loss 0.364590 history loss 8.351218 rank 3
2023-02-23 09:14:56,616 DEBUG CV Batch 16/1400 loss 6.559020 loss_att 23.160622 loss_ctc 3.858205 loss_rnnt 3.404360 hw_loss 0.364590 history loss 8.351218 rank 1
2023-02-23 09:14:58,226 DEBUG CV Batch 16/1400 loss 6.559020 loss_att 23.160622 loss_ctc 3.858205 loss_rnnt 3.404360 hw_loss 0.364590 history loss 8.351218 rank 2
2023-02-23 09:14:59,543 DEBUG CV Batch 16/1400 loss 6.559020 loss_att 23.160622 loss_ctc 3.858205 loss_rnnt 3.404360 hw_loss 0.364590 history loss 8.351218 rank 4
2023-02-23 09:15:00,563 DEBUG CV Batch 16/1400 loss 6.559020 loss_att 23.160622 loss_ctc 3.858205 loss_rnnt 3.404360 hw_loss 0.364590 history loss 8.351218 rank 6
2023-02-23 09:15:01,464 DEBUG CV Batch 16/1400 loss 6.559020 loss_att 23.160622 loss_ctc 3.858205 loss_rnnt 3.404360 hw_loss 0.364590 history loss 8.351218 rank 0
2023-02-23 09:15:03,029 DEBUG CV Batch 16/1400 loss 6.559020 loss_att 23.160622 loss_ctc 3.858205 loss_rnnt 3.404360 hw_loss 0.364590 history loss 8.351218 rank 7
2023-02-23 09:15:03,806 DEBUG CV Batch 16/1400 loss 6.559020 loss_att 23.160622 loss_ctc 3.858205 loss_rnnt 3.404360 hw_loss 0.364590 history loss 8.351218 rank 5
2023-02-23 09:15:07,613 DEBUG CV Batch 16/1500 loss 8.235224 loss_att 8.738774 loss_ctc 7.583477 loss_rnnt 8.016042 hw_loss 0.385073 history loss 8.160691 rank 3
2023-02-23 09:15:08,063 DEBUG CV Batch 16/1500 loss 8.235224 loss_att 8.738774 loss_ctc 7.583477 loss_rnnt 8.016042 hw_loss 0.385073 history loss 8.160691 rank 1
2023-02-23 09:15:10,067 DEBUG CV Batch 16/1500 loss 8.235224 loss_att 8.738774 loss_ctc 7.583477 loss_rnnt 8.016042 hw_loss 0.385073 history loss 8.160691 rank 2
2023-02-23 09:15:12,402 DEBUG CV Batch 16/1500 loss 8.235224 loss_att 8.738774 loss_ctc 7.583477 loss_rnnt 8.016042 hw_loss 0.385073 history loss 8.160691 rank 4
2023-02-23 09:15:12,997 DEBUG CV Batch 16/1500 loss 8.235224 loss_att 8.738774 loss_ctc 7.583477 loss_rnnt 8.016042 hw_loss 0.385073 history loss 8.160691 rank 6
2023-02-23 09:15:13,910 DEBUG CV Batch 16/1500 loss 8.235224 loss_att 8.738774 loss_ctc 7.583477 loss_rnnt 8.016042 hw_loss 0.385073 history loss 8.160691 rank 0
2023-02-23 09:15:15,971 DEBUG CV Batch 16/1500 loss 8.235224 loss_att 8.738774 loss_ctc 7.583477 loss_rnnt 8.016042 hw_loss 0.385073 history loss 8.160691 rank 5
2023-02-23 09:15:16,119 DEBUG CV Batch 16/1500 loss 8.235224 loss_att 8.738774 loss_ctc 7.583477 loss_rnnt 8.016042 hw_loss 0.385073 history loss 8.160691 rank 7
2023-02-23 09:15:20,966 DEBUG CV Batch 16/1600 loss 9.954327 loss_att 12.288026 loss_ctc 11.140679 loss_rnnt 9.121696 hw_loss 0.389457 history loss 8.076922 rank 3
2023-02-23 09:15:21,889 DEBUG CV Batch 16/1600 loss 9.954327 loss_att 12.288026 loss_ctc 11.140679 loss_rnnt 9.121696 hw_loss 0.389457 history loss 8.076922 rank 1
2023-02-23 09:15:23,482 DEBUG CV Batch 16/1600 loss 9.954327 loss_att 12.288026 loss_ctc 11.140679 loss_rnnt 9.121696 hw_loss 0.389457 history loss 8.076922 rank 2
2023-02-23 09:15:26,443 DEBUG CV Batch 16/1600 loss 9.954327 loss_att 12.288026 loss_ctc 11.140679 loss_rnnt 9.121696 hw_loss 0.389457 history loss 8.076922 rank 4
2023-02-23 09:15:26,815 DEBUG CV Batch 16/1600 loss 9.954327 loss_att 12.288026 loss_ctc 11.140679 loss_rnnt 9.121696 hw_loss 0.389457 history loss 8.076922 rank 6
2023-02-23 09:15:28,056 DEBUG CV Batch 16/1600 loss 9.954327 loss_att 12.288026 loss_ctc 11.140679 loss_rnnt 9.121696 hw_loss 0.389457 history loss 8.076922 rank 0
2023-02-23 09:15:29,785 DEBUG CV Batch 16/1600 loss 9.954327 loss_att 12.288026 loss_ctc 11.140679 loss_rnnt 9.121696 hw_loss 0.389457 history loss 8.076922 rank 5
2023-02-23 09:15:30,195 DEBUG CV Batch 16/1600 loss 9.954327 loss_att 12.288026 loss_ctc 11.140679 loss_rnnt 9.121696 hw_loss 0.389457 history loss 8.076922 rank 7
2023-02-23 09:15:33,524 DEBUG CV Batch 16/1700 loss 7.672121 loss_att 7.637141 loss_ctc 11.735704 loss_rnnt 6.843960 hw_loss 0.550023 history loss 7.969475 rank 3
2023-02-23 09:15:34,740 DEBUG CV Batch 16/1700 loss 7.672121 loss_att 7.637141 loss_ctc 11.735704 loss_rnnt 6.843960 hw_loss 0.550023 history loss 7.969475 rank 1
2023-02-23 09:15:36,209 DEBUG CV Batch 16/1700 loss 7.672121 loss_att 7.637141 loss_ctc 11.735704 loss_rnnt 6.843960 hw_loss 0.550023 history loss 7.969475 rank 2
2023-02-23 09:15:39,005 DEBUG CV Batch 16/1700 loss 7.672121 loss_att 7.637141 loss_ctc 11.735704 loss_rnnt 6.843960 hw_loss 0.550023 history loss 7.969475 rank 4
2023-02-23 09:15:40,013 DEBUG CV Batch 16/1700 loss 7.672121 loss_att 7.637141 loss_ctc 11.735704 loss_rnnt 6.843960 hw_loss 0.550023 history loss 7.969475 rank 6
2023-02-23 09:15:40,887 DEBUG CV Batch 16/1700 loss 7.672121 loss_att 7.637141 loss_ctc 11.735704 loss_rnnt 6.843960 hw_loss 0.550023 history loss 7.969475 rank 0
2023-02-23 09:15:42,345 DEBUG CV Batch 16/1700 loss 7.672121 loss_att 7.637141 loss_ctc 11.735704 loss_rnnt 6.843960 hw_loss 0.550023 history loss 7.969475 rank 7
2023-02-23 09:15:42,562 DEBUG CV Batch 16/1700 loss 7.672121 loss_att 7.637141 loss_ctc 11.735704 loss_rnnt 6.843960 hw_loss 0.550023 history loss 7.969475 rank 5
2023-02-23 09:15:42,723 INFO Epoch 16 CV info cv_loss 7.926890696386396
2023-02-23 09:15:42,723 INFO Epoch 17 TRAIN info lr 0.0004198228284893166
2023-02-23 09:15:42,728 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:15:44,634 INFO Epoch 16 CV info cv_loss 7.926890695934127
2023-02-23 09:15:44,635 INFO Epoch 17 TRAIN info lr 0.00041997385828087934
2023-02-23 09:15:44,637 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:15:45,523 INFO Epoch 16 CV info cv_loss 7.926890695249265
2023-02-23 09:15:45,524 INFO Epoch 17 TRAIN info lr 0.0004198761145144706
2023-02-23 09:15:45,529 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:15:48,356 INFO Epoch 16 CV info cv_loss 7.926890697579522
2023-02-23 09:15:48,357 INFO Epoch 17 TRAIN info lr 0.00041992645883987554
2023-02-23 09:15:48,359 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:15:49,423 INFO Epoch 16 CV info cv_loss 7.926890696050425
2023-02-23 09:15:49,423 INFO Epoch 17 TRAIN info lr 0.0004199175732312518
2023-02-23 09:15:49,428 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:15:50,431 INFO Epoch 16 CV info cv_loss 7.926890696774054
2023-02-23 09:15:50,431 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/16.pt
2023-02-23 09:15:51,689 INFO Epoch 16 CV info cv_loss 7.9268906965113075
2023-02-23 09:15:51,689 INFO Epoch 17 TRAIN info lr 0.0004198509491409837
2023-02-23 09:15:51,693 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:15:53,792 INFO Epoch 16 CV info cv_loss 7.926890691213309
2023-02-23 09:15:53,792 INFO Epoch 17 TRAIN info lr 0.0004198494689658057
2023-02-23 09:15:53,797 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:15:53,849 INFO Epoch 17 TRAIN info lr 0.000419950156554151
2023-02-23 09:15:53,853 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 09:16:55,430 DEBUG TRAIN Batch 17/0 loss 12.636059 loss_att 11.656157 loss_ctc 14.625032 loss_rnnt 12.176540 hw_loss 0.731817 lr 0.00041987 rank 2
2023-02-23 09:16:55,432 DEBUG TRAIN Batch 17/0 loss 8.276943 loss_att 8.184721 loss_ctc 10.914265 loss_rnnt 7.552788 hw_loss 0.733045 lr 0.00041992 rank 4
2023-02-23 09:16:55,433 DEBUG TRAIN Batch 17/0 loss 7.400043 loss_att 6.570335 loss_ctc 8.151793 loss_rnnt 7.163821 hw_loss 0.566120 lr 0.00041985 rank 7
2023-02-23 09:16:55,433 DEBUG TRAIN Batch 17/0 loss 11.791142 loss_att 10.623201 loss_ctc 13.980810 loss_rnnt 11.340609 hw_loss 0.735309 lr 0.00041997 rank 1
2023-02-23 09:16:55,436 DEBUG TRAIN Batch 17/0 loss 10.444888 loss_att 9.721701 loss_ctc 11.996198 loss_rnnt 10.079793 hw_loss 0.567923 lr 0.00041992 rank 6
2023-02-23 09:16:55,440 DEBUG TRAIN Batch 17/0 loss 12.616753 loss_att 11.394980 loss_ctc 14.092053 loss_rnnt 12.287869 hw_loss 0.705997 lr 0.00041982 rank 3
2023-02-23 09:16:55,445 DEBUG TRAIN Batch 17/0 loss 10.082511 loss_att 9.784575 loss_ctc 12.901567 loss_rnnt 9.422884 hw_loss 0.643761 lr 0.00041985 rank 5
2023-02-23 09:16:55,463 DEBUG TRAIN Batch 17/0 loss 8.958123 loss_att 8.701749 loss_ctc 10.361620 loss_rnnt 8.450914 hw_loss 0.696282 lr 0.00041995 rank 0
2023-02-23 09:18:08,561 DEBUG TRAIN Batch 17/100 loss 16.451555 loss_att 20.638535 loss_ctc 28.428076 loss_rnnt 13.789639 hw_loss 0.426846 lr 0.00041967 rank 3
2023-02-23 09:18:08,564 DEBUG TRAIN Batch 17/100 loss 15.465322 loss_att 20.951942 loss_ctc 20.869081 loss_rnnt 13.433006 hw_loss 0.402167 lr 0.00041970 rank 7
2023-02-23 09:18:08,565 DEBUG TRAIN Batch 17/100 loss 5.410660 loss_att 8.247442 loss_ctc 8.355668 loss_rnnt 4.236495 hw_loss 0.401514 lr 0.00041982 rank 1
2023-02-23 09:18:08,568 DEBUG TRAIN Batch 17/100 loss 13.144685 loss_att 15.045848 loss_ctc 17.998537 loss_rnnt 11.896864 hw_loss 0.413268 lr 0.00041970 rank 5
2023-02-23 09:18:08,569 DEBUG TRAIN Batch 17/100 loss 8.246799 loss_att 12.268156 loss_ctc 10.930369 loss_rnnt 6.856880 hw_loss 0.427194 lr 0.00041980 rank 0
2023-02-23 09:18:08,572 DEBUG TRAIN Batch 17/100 loss 7.950064 loss_att 10.307965 loss_ctc 12.666080 loss_rnnt 6.645194 hw_loss 0.383413 lr 0.00041973 rank 2
2023-02-23 09:18:08,572 DEBUG TRAIN Batch 17/100 loss 6.670718 loss_att 9.459285 loss_ctc 10.240997 loss_rnnt 5.425471 hw_loss 0.396555 lr 0.00041977 rank 6
2023-02-23 09:18:08,616 DEBUG TRAIN Batch 17/100 loss 6.955110 loss_att 10.808205 loss_ctc 10.475005 loss_rnnt 5.486012 hw_loss 0.429674 lr 0.00041978 rank 4
2023-02-23 09:19:22,047 DEBUG TRAIN Batch 17/200 loss 5.524723 loss_att 9.521759 loss_ctc 7.847080 loss_rnnt 4.233500 hw_loss 0.341564 lr 0.00041953 rank 3
2023-02-23 09:19:22,048 DEBUG TRAIN Batch 17/200 loss 8.641040 loss_att 12.817935 loss_ctc 10.577271 loss_rnnt 7.361083 hw_loss 0.349527 lr 0.00041955 rank 7
2023-02-23 09:19:22,051 DEBUG TRAIN Batch 17/200 loss 12.143475 loss_att 15.885964 loss_ctc 14.961349 loss_rnnt 10.833320 hw_loss 0.348641 lr 0.00041963 rank 4
2023-02-23 09:19:22,053 DEBUG TRAIN Batch 17/200 loss 11.516166 loss_att 14.390980 loss_ctc 14.820473 loss_rnnt 10.296476 hw_loss 0.382784 lr 0.00041968 rank 1
2023-02-23 09:19:22,054 DEBUG TRAIN Batch 17/200 loss 16.388630 loss_att 19.159094 loss_ctc 20.272039 loss_rnnt 15.099762 hw_loss 0.406849 lr 0.00041955 rank 5
2023-02-23 09:19:22,055 DEBUG TRAIN Batch 17/200 loss 11.492183 loss_att 14.659807 loss_ctc 13.521845 loss_rnnt 10.413561 hw_loss 0.327142 lr 0.00041962 rank 6
2023-02-23 09:19:22,059 DEBUG TRAIN Batch 17/200 loss 7.007170 loss_att 9.415730 loss_ctc 10.666392 loss_rnnt 5.823593 hw_loss 0.401190 lr 0.00041965 rank 0
2023-02-23 09:19:22,058 DEBUG TRAIN Batch 17/200 loss 5.627320 loss_att 6.988981 loss_ctc 8.894745 loss_rnnt 4.714732 hw_loss 0.383625 lr 0.00041958 rank 2
2023-02-23 09:20:35,240 DEBUG TRAIN Batch 17/300 loss 16.902439 loss_att 23.260614 loss_ctc 22.408695 loss_rnnt 14.643232 hw_loss 0.475133 lr 0.00041947 rank 6
2023-02-23 09:20:35,253 DEBUG TRAIN Batch 17/300 loss 10.728761 loss_att 12.441254 loss_ctc 14.271268 loss_rnnt 9.699495 hw_loss 0.402062 lr 0.00041940 rank 7
2023-02-23 09:20:35,254 DEBUG TRAIN Batch 17/300 loss 13.723103 loss_att 19.081268 loss_ctc 18.260777 loss_rnnt 11.830459 hw_loss 0.404977 lr 0.00041938 rank 3
2023-02-23 09:20:35,254 DEBUG TRAIN Batch 17/300 loss 11.109691 loss_att 15.396120 loss_ctc 18.015709 loss_rnnt 9.112253 hw_loss 0.411279 lr 0.00041943 rank 2
2023-02-23 09:20:35,255 DEBUG TRAIN Batch 17/300 loss 9.003023 loss_att 11.780364 loss_ctc 13.407436 loss_rnnt 7.677971 hw_loss 0.341867 lr 0.00041948 rank 4
2023-02-23 09:20:35,258 DEBUG TRAIN Batch 17/300 loss 17.538050 loss_att 17.589558 loss_ctc 25.091991 loss_rnnt 16.298653 hw_loss 0.416071 lr 0.00041941 rank 5
2023-02-23 09:20:35,260 DEBUG TRAIN Batch 17/300 loss 8.907211 loss_att 13.830477 loss_ctc 10.578524 loss_rnnt 7.522507 hw_loss 0.332270 lr 0.00041951 rank 0
2023-02-23 09:20:35,309 DEBUG TRAIN Batch 17/300 loss 5.045777 loss_att 8.268721 loss_ctc 6.589511 loss_rnnt 4.028386 hw_loss 0.313071 lr 0.00041953 rank 1
2023-02-23 09:21:49,159 DEBUG TRAIN Batch 17/400 loss 5.658975 loss_att 9.236143 loss_ctc 8.035877 loss_rnnt 4.411173 hw_loss 0.403965 lr 0.00041926 rank 5
2023-02-23 09:21:49,159 DEBUG TRAIN Batch 17/400 loss 8.456441 loss_att 11.620907 loss_ctc 10.223183 loss_rnnt 7.380754 hw_loss 0.388551 lr 0.00041926 rank 7
2023-02-23 09:21:49,162 DEBUG TRAIN Batch 17/400 loss 11.523608 loss_att 13.221864 loss_ctc 11.587182 loss_rnnt 10.937905 hw_loss 0.445454 lr 0.00041923 rank 3
2023-02-23 09:21:49,162 DEBUG TRAIN Batch 17/400 loss 11.379672 loss_att 14.648068 loss_ctc 17.193058 loss_rnnt 9.721062 hw_loss 0.430899 lr 0.00041933 rank 4
2023-02-23 09:21:49,163 DEBUG TRAIN Batch 17/400 loss 9.718558 loss_att 12.726925 loss_ctc 13.515764 loss_rnnt 8.409793 hw_loss 0.376496 lr 0.00041936 rank 0
2023-02-23 09:21:49,163 DEBUG TRAIN Batch 17/400 loss 8.514103 loss_att 11.301380 loss_ctc 13.289996 loss_rnnt 7.080761 hw_loss 0.448313 lr 0.00041928 rank 2
2023-02-23 09:21:49,163 DEBUG TRAIN Batch 17/400 loss 10.765577 loss_att 13.662250 loss_ctc 14.259859 loss_rnnt 9.514365 hw_loss 0.386199 lr 0.00041932 rank 6
2023-02-23 09:21:49,167 DEBUG TRAIN Batch 17/400 loss 9.852532 loss_att 10.810551 loss_ctc 11.275330 loss_rnnt 9.256731 hw_loss 0.402169 lr 0.00041938 rank 1
2023-02-23 09:23:01,347 DEBUG TRAIN Batch 17/500 loss 11.530684 loss_att 13.605320 loss_ctc 15.638984 loss_rnnt 10.331743 hw_loss 0.442949 lr 0.00041921 rank 0
2023-02-23 09:23:01,350 DEBUG TRAIN Batch 17/500 loss 10.696383 loss_att 14.126845 loss_ctc 18.120575 loss_rnnt 8.784522 hw_loss 0.442269 lr 0.00041911 rank 7
2023-02-23 09:23:01,352 DEBUG TRAIN Batch 17/500 loss 10.279265 loss_att 15.365135 loss_ctc 14.651407 loss_rnnt 8.475276 hw_loss 0.382245 lr 0.00041914 rank 2
2023-02-23 09:23:01,351 DEBUG TRAIN Batch 17/500 loss 13.589540 loss_att 20.469206 loss_ctc 17.473278 loss_rnnt 11.482271 hw_loss 0.400316 lr 0.00041923 rank 1
2023-02-23 09:23:01,352 DEBUG TRAIN Batch 17/500 loss 9.457756 loss_att 12.149031 loss_ctc 12.817370 loss_rnnt 8.225726 hw_loss 0.460926 lr 0.00041908 rank 3
2023-02-23 09:23:01,354 DEBUG TRAIN Batch 17/500 loss 13.890361 loss_att 15.661974 loss_ctc 17.080204 loss_rnnt 12.916256 hw_loss 0.364629 lr 0.00041911 rank 5
2023-02-23 09:23:01,357 DEBUG TRAIN Batch 17/500 loss 11.831573 loss_att 12.869200 loss_ctc 16.611794 loss_rnnt 10.736917 hw_loss 0.468311 lr 0.00041918 rank 6
2023-02-23 09:23:01,359 DEBUG TRAIN Batch 17/500 loss 10.190600 loss_att 15.297129 loss_ctc 16.211567 loss_rnnt 8.190126 hw_loss 0.330698 lr 0.00041919 rank 4
2023-02-23 09:24:13,957 DEBUG TRAIN Batch 17/600 loss 12.695418 loss_att 13.792071 loss_ctc 17.814280 loss_rnnt 11.491798 hw_loss 0.565826 lr 0.00041896 rank 7
2023-02-23 09:24:13,964 DEBUG TRAIN Batch 17/600 loss 22.532824 loss_att 22.981287 loss_ctc 30.357166 loss_rnnt 21.124046 hw_loss 0.517199 lr 0.00041899 rank 2
2023-02-23 09:24:13,965 DEBUG TRAIN Batch 17/600 loss 9.345125 loss_att 9.344738 loss_ctc 10.469824 loss_rnnt 8.927455 hw_loss 0.502103 lr 0.00041896 rank 5
2023-02-23 09:24:13,965 DEBUG TRAIN Batch 17/600 loss 12.863868 loss_att 13.255129 loss_ctc 15.541539 loss_rnnt 12.087642 hw_loss 0.639283 lr 0.00041894 rank 3
2023-02-23 09:24:13,966 DEBUG TRAIN Batch 17/600 loss 14.589571 loss_att 17.048138 loss_ctc 20.648254 loss_rnnt 12.933519 hw_loss 0.668464 lr 0.00041904 rank 4
2023-02-23 09:24:13,970 DEBUG TRAIN Batch 17/600 loss 16.189627 loss_att 18.802155 loss_ctc 27.899099 loss_rnnt 13.901836 hw_loss 0.382539 lr 0.00041909 rank 1
2023-02-23 09:24:13,994 DEBUG TRAIN Batch 17/600 loss 17.737579 loss_att 16.236971 loss_ctc 22.030514 loss_rnnt 17.187513 hw_loss 0.520869 lr 0.00041906 rank 0
2023-02-23 09:24:14,002 DEBUG TRAIN Batch 17/600 loss 8.309744 loss_att 7.994744 loss_ctc 10.259218 loss_rnnt 7.731532 hw_loss 0.714902 lr 0.00041903 rank 6
2023-02-23 09:25:28,929 DEBUG TRAIN Batch 17/700 loss 6.182926 loss_att 11.140523 loss_ctc 9.781015 loss_rnnt 4.486633 hw_loss 0.421925 lr 0.00041894 rank 1
2023-02-23 09:25:28,940 DEBUG TRAIN Batch 17/700 loss 10.562054 loss_att 15.163216 loss_ctc 15.481327 loss_rnnt 8.765748 hw_loss 0.412818 lr 0.00041884 rank 2
2023-02-23 09:25:28,943 DEBUG TRAIN Batch 17/700 loss 8.584970 loss_att 11.562046 loss_ctc 13.207651 loss_rnnt 7.152944 hw_loss 0.412974 lr 0.00041882 rank 5
2023-02-23 09:25:28,953 DEBUG TRAIN Batch 17/700 loss 5.080984 loss_att 8.395563 loss_ctc 6.928037 loss_rnnt 3.971514 hw_loss 0.375527 lr 0.00041888 rank 6
2023-02-23 09:25:28,965 DEBUG TRAIN Batch 17/700 loss 16.110809 loss_att 23.023390 loss_ctc 22.721226 loss_rnnt 13.642683 hw_loss 0.382916 lr 0.00041882 rank 7
2023-02-23 09:25:28,966 DEBUG TRAIN Batch 17/700 loss 8.875723 loss_att 10.446299 loss_ctc 10.933138 loss_rnnt 8.090160 hw_loss 0.369612 lr 0.00041879 rank 3
2023-02-23 09:25:28,967 DEBUG TRAIN Batch 17/700 loss 10.140480 loss_att 12.999473 loss_ctc 10.219046 loss_rnnt 9.393970 hw_loss 0.307940 lr 0.00041889 rank 4
2023-02-23 09:25:28,975 DEBUG TRAIN Batch 17/700 loss 15.991790 loss_att 19.277439 loss_ctc 23.040100 loss_rnnt 14.184180 hw_loss 0.395072 lr 0.00041892 rank 0
2023-02-23 09:26:42,068 DEBUG TRAIN Batch 17/800 loss 5.571054 loss_att 8.559841 loss_ctc 6.944068 loss_rnnt 4.556194 hw_loss 0.438813 lr 0.00041875 rank 4
2023-02-23 09:26:42,070 DEBUG TRAIN Batch 17/800 loss 11.014570 loss_att 16.081875 loss_ctc 21.125923 loss_rnnt 8.436603 hw_loss 0.405612 lr 0.00041867 rank 7
2023-02-23 09:26:42,073 DEBUG TRAIN Batch 17/800 loss 5.245139 loss_att 7.531800 loss_ctc 7.995668 loss_rnnt 4.214537 hw_loss 0.387249 lr 0.00041877 rank 0
2023-02-23 09:26:42,074 DEBUG TRAIN Batch 17/800 loss 17.241678 loss_att 24.188046 loss_ctc 25.941229 loss_rnnt 14.492229 hw_loss 0.375442 lr 0.00041874 rank 6
2023-02-23 09:26:42,074 DEBUG TRAIN Batch 17/800 loss 15.163630 loss_att 18.743338 loss_ctc 23.033659 loss_rnnt 13.239445 hw_loss 0.297948 lr 0.00041864 rank 3
2023-02-23 09:26:42,076 DEBUG TRAIN Batch 17/800 loss 9.779692 loss_att 13.889771 loss_ctc 16.222633 loss_rnnt 7.922058 hw_loss 0.331050 lr 0.00041867 rank 5
2023-02-23 09:26:42,079 DEBUG TRAIN Batch 17/800 loss 9.601729 loss_att 14.454489 loss_ctc 11.001617 loss_rnnt 8.260444 hw_loss 0.345157 lr 0.00041870 rank 2
2023-02-23 09:26:42,081 DEBUG TRAIN Batch 17/800 loss 16.690851 loss_att 17.664795 loss_ctc 18.195280 loss_rnnt 16.080048 hw_loss 0.403922 lr 0.00041879 rank 1
2023-02-23 09:27:54,292 DEBUG TRAIN Batch 17/900 loss 7.316442 loss_att 10.727460 loss_ctc 9.861764 loss_rnnt 6.065263 hw_loss 0.430497 lr 0.00041852 rank 7
2023-02-23 09:27:54,293 DEBUG TRAIN Batch 17/900 loss 8.195327 loss_att 11.064002 loss_ctc 10.678165 loss_rnnt 7.057793 hw_loss 0.436414 lr 0.00041859 rank 6
2023-02-23 09:27:54,295 DEBUG TRAIN Batch 17/900 loss 11.126087 loss_att 13.272841 loss_ctc 16.607660 loss_rnnt 9.706076 hw_loss 0.487094 lr 0.00041862 rank 0
2023-02-23 09:27:54,296 DEBUG TRAIN Batch 17/900 loss 5.981722 loss_att 8.829166 loss_ctc 8.180029 loss_rnnt 4.910620 hw_loss 0.390949 lr 0.00041852 rank 5
2023-02-23 09:27:54,296 DEBUG TRAIN Batch 17/900 loss 6.879593 loss_att 7.997746 loss_ctc 5.461240 loss_rnnt 6.614294 hw_loss 0.432716 lr 0.00041850 rank 3
2023-02-23 09:27:54,297 DEBUG TRAIN Batch 17/900 loss 14.852265 loss_att 17.019817 loss_ctc 20.774677 loss_rnnt 13.408571 hw_loss 0.413488 lr 0.00041855 rank 2
2023-02-23 09:27:54,298 DEBUG TRAIN Batch 17/900 loss 6.277082 loss_att 8.605667 loss_ctc 10.378856 loss_rnnt 5.039661 hw_loss 0.421501 lr 0.00041865 rank 1
2023-02-23 09:27:54,302 DEBUG TRAIN Batch 17/900 loss 9.030088 loss_att 11.720527 loss_ctc 14.643776 loss_rnnt 7.520889 hw_loss 0.417414 lr 0.00041860 rank 4
2023-02-23 09:29:07,701 DEBUG TRAIN Batch 17/1000 loss 20.746532 loss_att 24.804993 loss_ctc 28.146898 loss_rnnt 18.718719 hw_loss 0.430138 lr 0.00041838 rank 5
2023-02-23 09:29:07,703 DEBUG TRAIN Batch 17/1000 loss 11.391770 loss_att 13.465281 loss_ctc 16.301491 loss_rnnt 10.108812 hw_loss 0.400548 lr 0.00041848 rank 0
2023-02-23 09:29:07,711 DEBUG TRAIN Batch 17/1000 loss 13.290396 loss_att 16.273340 loss_ctc 19.906322 loss_rnnt 11.572802 hw_loss 0.447903 lr 0.00041838 rank 7
2023-02-23 09:29:07,718 DEBUG TRAIN Batch 17/1000 loss 5.873913 loss_att 7.769130 loss_ctc 8.073082 loss_rnnt 4.950440 hw_loss 0.471013 lr 0.00041844 rank 6
2023-02-23 09:29:07,719 DEBUG TRAIN Batch 17/1000 loss 18.944540 loss_att 23.195358 loss_ctc 26.131996 loss_rnnt 16.924417 hw_loss 0.396810 lr 0.00041850 rank 1
2023-02-23 09:29:07,719 DEBUG TRAIN Batch 17/1000 loss 16.998014 loss_att 20.782604 loss_ctc 25.360182 loss_rnnt 14.899786 hw_loss 0.424415 lr 0.00041835 rank 3
2023-02-23 09:29:07,721 DEBUG TRAIN Batch 17/1000 loss 7.974336 loss_att 10.330518 loss_ctc 13.340162 loss_rnnt 6.562736 hw_loss 0.421724 lr 0.00041840 rank 2
2023-02-23 09:29:07,721 DEBUG TRAIN Batch 17/1000 loss 7.154799 loss_att 11.579335 loss_ctc 9.678241 loss_rnnt 5.764932 hw_loss 0.315939 lr 0.00041845 rank 4
2023-02-23 09:30:22,308 DEBUG TRAIN Batch 17/1100 loss 12.579819 loss_att 13.324451 loss_ctc 18.112982 loss_rnnt 11.499625 hw_loss 0.362834 lr 0.00041823 rank 7
2023-02-23 09:30:22,310 DEBUG TRAIN Batch 17/1100 loss 9.751277 loss_att 13.705303 loss_ctc 13.024706 loss_rnnt 8.281408 hw_loss 0.454886 lr 0.00041820 rank 3
2023-02-23 09:30:22,311 DEBUG TRAIN Batch 17/1100 loss 13.965140 loss_att 17.554707 loss_ctc 18.836809 loss_rnnt 12.323766 hw_loss 0.513570 lr 0.00041831 rank 4
2023-02-23 09:30:22,311 DEBUG TRAIN Batch 17/1100 loss 7.789539 loss_att 12.722116 loss_ctc 11.230122 loss_rnnt 6.092032 hw_loss 0.472963 lr 0.00041826 rank 2
2023-02-23 09:30:22,314 DEBUG TRAIN Batch 17/1100 loss 5.993702 loss_att 8.103310 loss_ctc 8.726704 loss_rnnt 4.994587 hw_loss 0.398986 lr 0.00041823 rank 5
2023-02-23 09:30:22,317 DEBUG TRAIN Batch 17/1100 loss 20.961569 loss_att 24.971996 loss_ctc 31.509398 loss_rnnt 18.512554 hw_loss 0.451034 lr 0.00041830 rank 6
2023-02-23 09:30:22,318 DEBUG TRAIN Batch 17/1100 loss 15.199998 loss_att 19.179810 loss_ctc 18.652031 loss_rnnt 13.742884 hw_loss 0.376652 lr 0.00041835 rank 1
2023-02-23 09:30:22,321 DEBUG TRAIN Batch 17/1100 loss 9.577838 loss_att 11.410837 loss_ctc 13.055564 loss_rnnt 8.549875 hw_loss 0.370625 lr 0.00041833 rank 0
2023-02-23 09:31:34,976 DEBUG TRAIN Batch 17/1200 loss 11.154530 loss_att 11.979836 loss_ctc 11.810480 loss_rnnt 10.621384 hw_loss 0.526171 lr 0.00041806 rank 3
2023-02-23 09:31:34,977 DEBUG TRAIN Batch 17/1200 loss 8.082989 loss_att 9.423187 loss_ctc 11.086050 loss_rnnt 7.114778 hw_loss 0.562055 lr 0.00041808 rank 7
2023-02-23 09:31:34,981 DEBUG TRAIN Batch 17/1200 loss 14.785848 loss_att 16.176382 loss_ctc 24.793442 loss_rnnt 12.928663 hw_loss 0.458873 lr 0.00041811 rank 2
2023-02-23 09:31:34,984 DEBUG TRAIN Batch 17/1200 loss 9.193721 loss_att 11.128651 loss_ctc 12.532288 loss_rnnt 8.160089 hw_loss 0.377820 lr 0.00041821 rank 1
2023-02-23 09:31:34,984 DEBUG TRAIN Batch 17/1200 loss 6.561439 loss_att 8.444921 loss_ctc 9.557673 loss_rnnt 5.573308 hw_loss 0.397380 lr 0.00041818 rank 0
2023-02-23 09:31:34,984 DEBUG TRAIN Batch 17/1200 loss 18.911495 loss_att 21.381632 loss_ctc 27.039503 loss_rnnt 17.095488 hw_loss 0.446709 lr 0.00041808 rank 5
2023-02-23 09:31:34,985 DEBUG TRAIN Batch 17/1200 loss 12.553741 loss_att 14.757778 loss_ctc 16.316254 loss_rnnt 11.377568 hw_loss 0.438183 lr 0.00041816 rank 4
2023-02-23 09:31:35,031 DEBUG TRAIN Batch 17/1200 loss 14.727897 loss_att 15.790139 loss_ctc 18.275595 loss_rnnt 13.758716 hw_loss 0.531951 lr 0.00041815 rank 6
2023-02-23 09:32:46,827 DEBUG TRAIN Batch 17/1300 loss 5.295175 loss_att 7.839060 loss_ctc 5.656129 loss_rnnt 4.524302 hw_loss 0.401191 lr 0.00041794 rank 7
2023-02-23 09:32:46,834 DEBUG TRAIN Batch 17/1300 loss 8.021466 loss_att 10.184690 loss_ctc 9.311086 loss_rnnt 7.146146 hw_loss 0.507610 lr 0.00041791 rank 3
2023-02-23 09:32:46,834 DEBUG TRAIN Batch 17/1300 loss 8.640381 loss_att 8.087694 loss_ctc 10.002593 loss_rnnt 8.261670 hw_loss 0.576786 lr 0.00041796 rank 2
2023-02-23 09:32:46,835 DEBUG TRAIN Batch 17/1300 loss 8.714327 loss_att 9.553339 loss_ctc 12.142162 loss_rnnt 7.733181 hw_loss 0.668061 lr 0.00041794 rank 5
2023-02-23 09:32:46,837 DEBUG TRAIN Batch 17/1300 loss 10.499868 loss_att 18.018408 loss_ctc 14.708467 loss_rnnt 8.169580 hw_loss 0.497690 lr 0.00041804 rank 0
2023-02-23 09:32:46,836 DEBUG TRAIN Batch 17/1300 loss 13.209162 loss_att 15.867254 loss_ctc 13.096744 loss_rnnt 12.478897 hw_loss 0.400567 lr 0.00041806 rank 1
2023-02-23 09:32:46,838 DEBUG TRAIN Batch 17/1300 loss 9.795096 loss_att 13.056309 loss_ctc 14.037634 loss_rnnt 8.389179 hw_loss 0.352507 lr 0.00041800 rank 6
2023-02-23 09:32:46,843 DEBUG TRAIN Batch 17/1300 loss 3.181423 loss_att 7.019098 loss_ctc 2.942768 loss_rnnt 2.240989 hw_loss 0.383850 lr 0.00041801 rank 4
2023-02-23 09:34:01,640 DEBUG TRAIN Batch 17/1400 loss 9.929791 loss_att 14.078178 loss_ctc 16.191719 loss_rnnt 8.056487 hw_loss 0.391320 lr 0.00041786 rank 6
2023-02-23 09:34:01,649 DEBUG TRAIN Batch 17/1400 loss 7.497717 loss_att 10.612245 loss_ctc 7.928422 loss_rnnt 6.592794 hw_loss 0.421106 lr 0.00041776 rank 3
2023-02-23 09:34:01,653 DEBUG TRAIN Batch 17/1400 loss 14.690665 loss_att 20.746424 loss_ctc 15.217368 loss_rnnt 13.197409 hw_loss 0.397271 lr 0.00041787 rank 4
2023-02-23 09:34:01,654 DEBUG TRAIN Batch 17/1400 loss 11.258886 loss_att 12.722609 loss_ctc 13.285099 loss_rnnt 10.511477 hw_loss 0.345942 lr 0.00041779 rank 7
2023-02-23 09:34:01,654 DEBUG TRAIN Batch 17/1400 loss 13.538373 loss_att 16.100422 loss_ctc 12.201462 loss_rnnt 13.027288 hw_loss 0.331742 lr 0.00041782 rank 2
2023-02-23 09:34:01,655 DEBUG TRAIN Batch 17/1400 loss 3.445270 loss_att 6.425775 loss_ctc 6.131239 loss_rnnt 2.311031 hw_loss 0.337516 lr 0.00041779 rank 5
2023-02-23 09:34:01,657 DEBUG TRAIN Batch 17/1400 loss 5.997972 loss_att 10.280607 loss_ctc 10.265868 loss_rnnt 4.390089 hw_loss 0.341821 lr 0.00041789 rank 0
2023-02-23 09:34:01,699 DEBUG TRAIN Batch 17/1400 loss 18.260176 loss_att 14.881845 loss_ctc 19.701368 loss_rnnt 18.517002 hw_loss 0.425027 lr 0.00041791 rank 1
2023-02-23 09:35:14,989 DEBUG TRAIN Batch 17/1500 loss 1.609951 loss_att 4.876609 loss_ctc 1.720434 loss_rnnt 0.730341 hw_loss 0.396652 lr 0.00041765 rank 7
2023-02-23 09:35:14,995 DEBUG TRAIN Batch 17/1500 loss 10.427124 loss_att 13.390128 loss_ctc 18.652809 loss_rnnt 8.513956 hw_loss 0.419642 lr 0.00041777 rank 1
2023-02-23 09:35:14,997 DEBUG TRAIN Batch 17/1500 loss 5.401764 loss_att 8.046557 loss_ctc 8.356620 loss_rnnt 4.275259 hw_loss 0.381686 lr 0.00041772 rank 4
2023-02-23 09:35:14,998 DEBUG TRAIN Batch 17/1500 loss 11.611238 loss_att 13.047194 loss_ctc 16.442518 loss_rnnt 10.456922 hw_loss 0.418039 lr 0.00041771 rank 6
2023-02-23 09:35:14,999 DEBUG TRAIN Batch 17/1500 loss 21.785633 loss_att 26.632673 loss_ctc 31.197458 loss_rnnt 19.351458 hw_loss 0.393483 lr 0.00041762 rank 3
2023-02-23 09:35:15,000 DEBUG TRAIN Batch 17/1500 loss 15.953921 loss_att 17.430080 loss_ctc 21.307249 loss_rnnt 14.766417 hw_loss 0.334679 lr 0.00041767 rank 2
2023-02-23 09:35:15,001 DEBUG TRAIN Batch 17/1500 loss 2.268841 loss_att 7.121610 loss_ctc 3.215747 loss_rnnt 0.985052 hw_loss 0.350591 lr 0.00041765 rank 5
2023-02-23 09:35:15,054 DEBUG TRAIN Batch 17/1500 loss 18.518911 loss_att 18.936243 loss_ctc 20.988623 loss_rnnt 17.878984 hw_loss 0.425933 lr 0.00041774 rank 0
2023-02-23 09:36:27,136 DEBUG TRAIN Batch 17/1600 loss 13.626496 loss_att 14.564413 loss_ctc 19.360249 loss_rnnt 12.431681 hw_loss 0.455123 lr 0.00041750 rank 7
2023-02-23 09:36:27,139 DEBUG TRAIN Batch 17/1600 loss 16.781248 loss_att 17.306904 loss_ctc 23.007568 loss_rnnt 15.603285 hw_loss 0.454982 lr 0.00041757 rank 6
2023-02-23 09:36:27,141 DEBUG TRAIN Batch 17/1600 loss 7.898795 loss_att 12.001270 loss_ctc 10.080130 loss_rnnt 6.547864 hw_loss 0.449233 lr 0.00041760 rank 0
2023-02-23 09:36:27,143 DEBUG TRAIN Batch 17/1600 loss 8.655716 loss_att 9.791624 loss_ctc 12.015936 loss_rnnt 7.781613 hw_loss 0.372922 lr 0.00041747 rank 3
2023-02-23 09:36:27,143 DEBUG TRAIN Batch 17/1600 loss 7.763259 loss_att 8.402189 loss_ctc 8.722886 loss_rnnt 7.306760 hw_loss 0.376430 lr 0.00041762 rank 1
2023-02-23 09:36:27,146 DEBUG TRAIN Batch 17/1600 loss 4.207785 loss_att 8.641561 loss_ctc 5.182674 loss_rnnt 2.983444 hw_loss 0.389250 lr 0.00041750 rank 5
2023-02-23 09:36:27,148 DEBUG TRAIN Batch 17/1600 loss 7.797375 loss_att 11.430722 loss_ctc 10.797911 loss_rnnt 6.455555 hw_loss 0.403272 lr 0.00041753 rank 2
2023-02-23 09:36:27,152 DEBUG TRAIN Batch 17/1600 loss 19.060211 loss_att 25.511406 loss_ctc 25.163506 loss_rnnt 16.753597 hw_loss 0.379879 lr 0.00041758 rank 4
2023-02-23 09:37:39,912 DEBUG TRAIN Batch 17/1700 loss 11.323298 loss_att 12.752563 loss_ctc 13.252352 loss_rnnt 10.529709 hw_loss 0.469740 lr 0.00041733 rank 3
2023-02-23 09:37:39,927 DEBUG TRAIN Batch 17/1700 loss 15.115867 loss_att 21.769569 loss_ctc 19.851095 loss_rnnt 12.927227 hw_loss 0.424755 lr 0.00041743 rank 4
2023-02-23 09:37:39,929 DEBUG TRAIN Batch 17/1700 loss 9.964869 loss_att 11.907727 loss_ctc 9.665674 loss_rnnt 9.375870 hw_loss 0.450599 lr 0.00041735 rank 7
2023-02-23 09:37:39,930 DEBUG TRAIN Batch 17/1700 loss 15.862337 loss_att 20.772982 loss_ctc 21.561029 loss_rnnt 13.941164 hw_loss 0.336032 lr 0.00041748 rank 1
2023-02-23 09:37:39,931 DEBUG TRAIN Batch 17/1700 loss 17.270365 loss_att 21.065828 loss_ctc 23.857256 loss_rnnt 15.366250 hw_loss 0.500191 lr 0.00041742 rank 6
2023-02-23 09:37:39,936 DEBUG TRAIN Batch 17/1700 loss 10.551919 loss_att 14.823730 loss_ctc 15.371271 loss_rnnt 8.858527 hw_loss 0.368341 lr 0.00041738 rank 2
2023-02-23 09:37:39,947 DEBUG TRAIN Batch 17/1700 loss 19.169796 loss_att 20.208584 loss_ctc 21.928381 loss_rnnt 18.393347 hw_loss 0.376647 lr 0.00041736 rank 5
2023-02-23 09:37:39,956 DEBUG TRAIN Batch 17/1700 loss 9.961827 loss_att 13.603144 loss_ctc 12.255174 loss_rnnt 8.699152 hw_loss 0.428687 lr 0.00041745 rank 0
2023-02-23 09:38:55,110 DEBUG TRAIN Batch 17/1800 loss 7.093631 loss_att 8.547471 loss_ctc 8.208106 loss_rnnt 6.361208 hw_loss 0.549482 lr 0.00041718 rank 3
2023-02-23 09:38:55,111 DEBUG TRAIN Batch 17/1800 loss 9.540891 loss_att 11.777795 loss_ctc 13.564257 loss_rnnt 8.326744 hw_loss 0.431846 lr 0.00041721 rank 7
2023-02-23 09:38:55,112 DEBUG TRAIN Batch 17/1800 loss 8.703990 loss_att 11.213740 loss_ctc 11.218209 loss_rnnt 7.638562 hw_loss 0.427967 lr 0.00041728 rank 4
2023-02-23 09:38:55,112 DEBUG TRAIN Batch 17/1800 loss 10.799530 loss_att 10.393778 loss_ctc 13.228065 loss_rnnt 10.320107 hw_loss 0.443944 lr 0.00041728 rank 6
2023-02-23 09:38:55,114 DEBUG TRAIN Batch 17/1800 loss 11.159182 loss_att 13.635334 loss_ctc 16.700441 loss_rnnt 9.717791 hw_loss 0.388735 lr 0.00041721 rank 5
2023-02-23 09:38:55,116 DEBUG TRAIN Batch 17/1800 loss 16.032812 loss_att 18.720373 loss_ctc 21.136909 loss_rnnt 14.602890 hw_loss 0.397243 lr 0.00041723 rank 2
2023-02-23 09:38:55,116 DEBUG TRAIN Batch 17/1800 loss 13.592409 loss_att 14.008438 loss_ctc 14.097653 loss_rnnt 13.193176 hw_loss 0.466239 lr 0.00041733 rank 1
2023-02-23 09:38:55,117 DEBUG TRAIN Batch 17/1800 loss 13.762159 loss_att 15.292726 loss_ctc 21.522602 loss_rnnt 12.225538 hw_loss 0.367094 lr 0.00041731 rank 0
2023-02-23 09:40:08,194 DEBUG TRAIN Batch 17/1900 loss 12.375221 loss_att 13.838230 loss_ctc 15.133312 loss_rnnt 11.453588 hw_loss 0.489911 lr 0.00041713 rank 6
2023-02-23 09:40:08,208 DEBUG TRAIN Batch 17/1900 loss 5.624178 loss_att 8.250799 loss_ctc 8.772194 loss_rnnt 4.439780 hw_loss 0.448761 lr 0.00041714 rank 4
2023-02-23 09:40:08,210 DEBUG TRAIN Batch 17/1900 loss 10.239153 loss_att 10.991657 loss_ctc 12.890930 loss_rnnt 9.514030 hw_loss 0.414471 lr 0.00041709 rank 2
2023-02-23 09:40:08,212 DEBUG TRAIN Batch 17/1900 loss 8.126980 loss_att 9.305848 loss_ctc 12.448912 loss_rnnt 7.123176 hw_loss 0.359574 lr 0.00041704 rank 3
2023-02-23 09:40:08,212 DEBUG TRAIN Batch 17/1900 loss 9.385243 loss_att 9.815630 loss_ctc 12.521923 loss_rnnt 8.568988 hw_loss 0.584914 lr 0.00041706 rank 7
2023-02-23 09:40:08,220 DEBUG TRAIN Batch 17/1900 loss 16.442125 loss_att 19.679585 loss_ctc 20.468344 loss_rnnt 15.079733 hw_loss 0.333887 lr 0.00041719 rank 1
2023-02-23 09:40:08,222 DEBUG TRAIN Batch 17/1900 loss 5.454372 loss_att 6.719707 loss_ctc 7.070964 loss_rnnt 4.736960 hw_loss 0.466498 lr 0.00041707 rank 5
2023-02-23 09:40:08,262 DEBUG TRAIN Batch 17/1900 loss 12.850171 loss_att 13.852963 loss_ctc 19.161530 loss_rnnt 11.494211 hw_loss 0.588537 lr 0.00041716 rank 0
2023-02-23 09:41:20,821 DEBUG TRAIN Batch 17/2000 loss 18.167877 loss_att 22.727589 loss_ctc 24.344534 loss_rnnt 16.246281 hw_loss 0.348942 lr 0.00041702 rank 0
2023-02-23 09:41:20,826 DEBUG TRAIN Batch 17/2000 loss 6.804060 loss_att 9.021781 loss_ctc 6.323829 loss_rnnt 6.188207 hw_loss 0.443136 lr 0.00041699 rank 4
2023-02-23 09:41:20,836 DEBUG TRAIN Batch 17/2000 loss 9.339534 loss_att 11.943735 loss_ctc 11.351242 loss_rnnt 8.394427 hw_loss 0.292572 lr 0.00041694 rank 2
2023-02-23 09:41:20,840 DEBUG TRAIN Batch 17/2000 loss 4.058451 loss_att 7.988058 loss_ctc 4.051406 loss_rnnt 3.074705 hw_loss 0.372683 lr 0.00041689 rank 3
2023-02-23 09:41:20,839 DEBUG TRAIN Batch 17/2000 loss 3.633183 loss_att 6.965099 loss_ctc 4.913182 loss_rnnt 2.563807 hw_loss 0.435612 lr 0.00041699 rank 6
2023-02-23 09:41:20,840 DEBUG TRAIN Batch 17/2000 loss 12.740252 loss_att 14.173187 loss_ctc 16.350010 loss_rnnt 11.738238 hw_loss 0.438987 lr 0.00041692 rank 7
2023-02-23 09:41:20,842 DEBUG TRAIN Batch 17/2000 loss 10.741132 loss_att 17.615238 loss_ctc 21.513403 loss_rnnt 7.752106 hw_loss 0.333564 lr 0.00041692 rank 5
2023-02-23 09:41:20,870 DEBUG TRAIN Batch 17/2000 loss 11.263488 loss_att 14.358067 loss_ctc 17.530819 loss_rnnt 9.620930 hw_loss 0.352495 lr 0.00041704 rank 1
2023-02-23 09:42:35,227 DEBUG TRAIN Batch 17/2100 loss 12.202169 loss_att 19.869917 loss_ctc 15.401935 loss_rnnt 9.944349 hw_loss 0.558065 lr 0.00041687 rank 0
2023-02-23 09:42:35,233 DEBUG TRAIN Batch 17/2100 loss 9.982254 loss_att 10.345518 loss_ctc 10.885226 loss_rnnt 9.543699 hw_loss 0.460325 lr 0.00041684 rank 6
2023-02-23 09:42:35,233 DEBUG TRAIN Batch 17/2100 loss 12.735525 loss_att 16.179081 loss_ctc 18.783794 loss_rnnt 11.046164 hw_loss 0.364150 lr 0.00041680 rank 2
2023-02-23 09:42:35,235 DEBUG TRAIN Batch 17/2100 loss 2.873272 loss_att 6.760637 loss_ctc 3.422055 loss_rnnt 1.823494 hw_loss 0.373377 lr 0.00041677 rank 7
2023-02-23 09:42:35,237 DEBUG TRAIN Batch 17/2100 loss 9.372809 loss_att 8.108146 loss_ctc 10.901491 loss_rnnt 9.211220 hw_loss 0.395059 lr 0.00041675 rank 3
2023-02-23 09:42:35,239 DEBUG TRAIN Batch 17/2100 loss 19.399628 loss_att 19.411026 loss_ctc 34.186882 loss_rnnt 17.235518 hw_loss 0.356624 lr 0.00041690 rank 1
2023-02-23 09:42:35,240 DEBUG TRAIN Batch 17/2100 loss 6.817608 loss_att 10.641049 loss_ctc 9.115919 loss_rnnt 5.547692 hw_loss 0.372722 lr 0.00041678 rank 5
2023-02-23 09:42:35,257 DEBUG TRAIN Batch 17/2100 loss 18.688213 loss_att 21.519352 loss_ctc 27.279821 loss_rnnt 16.765900 hw_loss 0.394759 lr 0.00041685 rank 4
2023-02-23 09:43:48,334 DEBUG TRAIN Batch 17/2200 loss 12.102499 loss_att 16.054558 loss_ctc 15.790786 loss_rnnt 10.629403 hw_loss 0.357960 lr 0.00041666 rank 2
2023-02-23 09:43:48,339 DEBUG TRAIN Batch 17/2200 loss 9.906042 loss_att 10.567039 loss_ctc 10.679311 loss_rnnt 9.436139 hw_loss 0.439875 lr 0.00041675 rank 1
2023-02-23 09:43:48,339 DEBUG TRAIN Batch 17/2200 loss 11.946720 loss_att 12.441360 loss_ctc 12.989714 loss_rnnt 11.464905 hw_loss 0.457166 lr 0.00041670 rank 6
2023-02-23 09:43:48,340 DEBUG TRAIN Batch 17/2200 loss 13.125514 loss_att 12.020178 loss_ctc 12.614163 loss_rnnt 13.203930 hw_loss 0.395307 lr 0.00041663 rank 7
2023-02-23 09:43:48,340 DEBUG TRAIN Batch 17/2200 loss 4.828868 loss_att 8.549799 loss_ctc 7.261795 loss_rnnt 3.564308 hw_loss 0.367470 lr 0.00041673 rank 0
2023-02-23 09:43:48,340 DEBUG TRAIN Batch 17/2200 loss 10.668492 loss_att 11.237112 loss_ctc 12.324829 loss_rnnt 10.157188 hw_loss 0.331378 lr 0.00041660 rank 3
2023-02-23 09:43:48,341 DEBUG TRAIN Batch 17/2200 loss 7.246782 loss_att 12.514024 loss_ctc 9.095343 loss_rnnt 5.758566 hw_loss 0.353049 lr 0.00041670 rank 4
2023-02-23 09:43:48,346 DEBUG TRAIN Batch 17/2200 loss 12.562357 loss_att 14.332361 loss_ctc 15.006186 loss_rnnt 11.673439 hw_loss 0.392013 lr 0.00041663 rank 5
2023-02-23 09:45:00,314 DEBUG TRAIN Batch 17/2300 loss 7.395334 loss_att 11.652031 loss_ctc 11.044087 loss_rnnt 5.807473 hw_loss 0.468789 lr 0.00041656 rank 4
2023-02-23 09:45:00,319 DEBUG TRAIN Batch 17/2300 loss 4.205827 loss_att 7.224777 loss_ctc 6.235565 loss_rnnt 3.119303 hw_loss 0.397689 lr 0.00041648 rank 7
2023-02-23 09:45:00,321 DEBUG TRAIN Batch 17/2300 loss 8.030354 loss_att 10.034555 loss_ctc 10.748518 loss_rnnt 7.054697 hw_loss 0.398243 lr 0.00041655 rank 6
2023-02-23 09:45:00,323 DEBUG TRAIN Batch 17/2300 loss 5.370393 loss_att 6.885248 loss_ctc 6.800734 loss_rnnt 4.640663 hw_loss 0.442588 lr 0.00041646 rank 3
2023-02-23 09:45:00,324 DEBUG TRAIN Batch 17/2300 loss 11.395818 loss_att 14.048225 loss_ctc 14.611628 loss_rnnt 10.194975 hw_loss 0.452974 lr 0.00041651 rank 2
2023-02-23 09:45:00,328 DEBUG TRAIN Batch 17/2300 loss 8.934832 loss_att 15.167500 loss_ctc 11.937911 loss_rnnt 7.072407 hw_loss 0.404024 lr 0.00041658 rank 0
2023-02-23 09:45:00,329 DEBUG TRAIN Batch 17/2300 loss 7.561398 loss_att 13.568657 loss_ctc 10.408504 loss_rnnt 5.791498 hw_loss 0.354065 lr 0.00041661 rank 1
2023-02-23 09:45:00,373 DEBUG TRAIN Batch 17/2300 loss 9.984624 loss_att 14.112466 loss_ctc 17.667240 loss_rnnt 7.940512 hw_loss 0.364116 lr 0.00041649 rank 5
2023-02-23 09:46:12,169 DEBUG TRAIN Batch 17/2400 loss 7.508372 loss_att 12.334229 loss_ctc 10.758726 loss_rnnt 5.883670 hw_loss 0.424034 lr 0.00041634 rank 7
2023-02-23 09:46:12,170 DEBUG TRAIN Batch 17/2400 loss 5.085322 loss_att 6.240354 loss_ctc 7.045663 loss_rnnt 4.280886 hw_loss 0.585095 lr 0.00041641 rank 6
2023-02-23 09:46:12,174 DEBUG TRAIN Batch 17/2400 loss 11.971725 loss_att 13.817879 loss_ctc 15.952273 loss_rnnt 10.853650 hw_loss 0.408944 lr 0.00041631 rank 3
2023-02-23 09:46:12,174 DEBUG TRAIN Batch 17/2400 loss 5.304090 loss_att 7.719611 loss_ctc 5.976964 loss_rnnt 4.531888 hw_loss 0.373838 lr 0.00041644 rank 0
2023-02-23 09:46:12,180 DEBUG TRAIN Batch 17/2400 loss 5.890769 loss_att 8.556376 loss_ctc 7.832834 loss_rnnt 4.840736 hw_loss 0.483693 lr 0.00041642 rank 4
2023-02-23 09:46:12,190 DEBUG TRAIN Batch 17/2400 loss 9.919926 loss_att 12.145641 loss_ctc 12.415739 loss_rnnt 8.932729 hw_loss 0.392396 lr 0.00041637 rank 2
2023-02-23 09:46:12,192 DEBUG TRAIN Batch 17/2400 loss 5.315644 loss_att 7.811466 loss_ctc 8.008443 loss_rnnt 4.209667 hw_loss 0.464575 lr 0.00041646 rank 1
2023-02-23 09:46:12,195 DEBUG TRAIN Batch 17/2400 loss 7.769824 loss_att 9.978741 loss_ctc 9.697851 loss_rnnt 6.834486 hw_loss 0.443409 lr 0.00041634 rank 5
2023-02-23 09:47:27,834 DEBUG TRAIN Batch 17/2500 loss 9.408193 loss_att 9.655966 loss_ctc 9.850267 loss_rnnt 9.007936 hw_loss 0.547047 lr 0.00041620 rank 7
2023-02-23 09:47:27,834 DEBUG TRAIN Batch 17/2500 loss 4.307845 loss_att 4.701170 loss_ctc 7.027600 loss_rnnt 3.581587 hw_loss 0.534298 lr 0.00041629 rank 0
2023-02-23 09:47:27,834 DEBUG TRAIN Batch 17/2500 loss 8.363247 loss_att 10.406343 loss_ctc 11.412633 loss_rnnt 7.329594 hw_loss 0.409592 lr 0.00041622 rank 2
2023-02-23 09:47:27,837 DEBUG TRAIN Batch 17/2500 loss 13.630160 loss_att 17.827642 loss_ctc 23.588720 loss_rnnt 11.258852 hw_loss 0.382504 lr 0.00041626 rank 6
2023-02-23 09:47:27,837 DEBUG TRAIN Batch 17/2500 loss 9.260652 loss_att 9.908466 loss_ctc 15.726830 loss_rnnt 7.978033 hw_loss 0.545435 lr 0.00041627 rank 4
2023-02-23 09:47:27,841 DEBUG TRAIN Batch 17/2500 loss 8.524338 loss_att 10.246315 loss_ctc 12.580545 loss_rnnt 7.427524 hw_loss 0.396730 lr 0.00041632 rank 1
2023-02-23 09:47:27,845 DEBUG TRAIN Batch 17/2500 loss 10.592687 loss_att 11.515847 loss_ctc 13.860065 loss_rnnt 9.714019 hw_loss 0.484474 lr 0.00041620 rank 5
2023-02-23 09:47:27,847 DEBUG TRAIN Batch 17/2500 loss 5.453729 loss_att 6.254509 loss_ctc 9.808969 loss_rnnt 4.364674 hw_loss 0.652877 lr 0.00041617 rank 3
2023-02-23 09:48:40,630 DEBUG TRAIN Batch 17/2600 loss 7.852238 loss_att 10.480952 loss_ctc 12.798556 loss_rnnt 6.346014 hw_loss 0.601823 lr 0.00041615 rank 0
2023-02-23 09:48:40,638 DEBUG TRAIN Batch 17/2600 loss 13.044167 loss_att 15.567163 loss_ctc 16.399105 loss_rnnt 11.870922 hw_loss 0.414974 lr 0.00041605 rank 7
2023-02-23 09:48:40,639 DEBUG TRAIN Batch 17/2600 loss 8.012308 loss_att 9.559301 loss_ctc 8.629720 loss_rnnt 7.355574 hw_loss 0.496902 lr 0.00041617 rank 1
2023-02-23 09:48:40,644 DEBUG TRAIN Batch 17/2600 loss 3.275248 loss_att 8.110053 loss_ctc 5.728580 loss_rnnt 1.762235 hw_loss 0.410514 lr 0.00041613 rank 4
2023-02-23 09:48:40,645 DEBUG TRAIN Batch 17/2600 loss 5.497870 loss_att 7.620723 loss_ctc 7.870170 loss_rnnt 4.545323 hw_loss 0.396883 lr 0.00041608 rank 2
2023-02-23 09:48:40,649 DEBUG TRAIN Batch 17/2600 loss 16.131384 loss_att 16.773344 loss_ctc 16.201797 loss_rnnt 15.766376 hw_loss 0.426050 lr 0.00041605 rank 5
2023-02-23 09:48:40,652 DEBUG TRAIN Batch 17/2600 loss 5.852029 loss_att 10.544878 loss_ctc 15.624901 loss_rnnt 3.424712 hw_loss 0.348183 lr 0.00041603 rank 3
2023-02-23 09:48:40,664 DEBUG TRAIN Batch 17/2600 loss 8.563743 loss_att 13.667144 loss_ctc 12.153323 loss_rnnt 6.885197 hw_loss 0.336100 lr 0.00041612 rank 6
2023-02-23 09:49:52,904 DEBUG TRAIN Batch 17/2700 loss 12.512099 loss_att 15.402757 loss_ctc 17.140379 loss_rnnt 11.050575 hw_loss 0.499292 lr 0.00041597 rank 6
2023-02-23 09:49:52,906 DEBUG TRAIN Batch 17/2700 loss 2.448847 loss_att 5.516775 loss_ctc 2.470266 loss_rnnt 1.552566 hw_loss 0.524699 lr 0.00041601 rank 0
2023-02-23 09:49:52,907 DEBUG TRAIN Batch 17/2700 loss 18.928585 loss_att 18.796612 loss_ctc 21.887348 loss_rnnt 18.361937 hw_loss 0.372265 lr 0.00041591 rank 7
2023-02-23 09:49:52,907 DEBUG TRAIN Batch 17/2700 loss 15.347777 loss_att 22.280317 loss_ctc 23.128971 loss_rnnt 12.766228 hw_loss 0.295405 lr 0.00041603 rank 1
2023-02-23 09:49:52,907 DEBUG TRAIN Batch 17/2700 loss 6.158159 loss_att 8.512642 loss_ctc 10.517665 loss_rnnt 4.908258 hw_loss 0.370757 lr 0.00041593 rank 2
2023-02-23 09:49:52,908 DEBUG TRAIN Batch 17/2700 loss 14.138491 loss_att 15.859969 loss_ctc 17.377758 loss_rnnt 13.147804 hw_loss 0.402165 lr 0.00041588 rank 3
2023-02-23 09:49:52,915 DEBUG TRAIN Batch 17/2700 loss 7.443430 loss_att 11.311315 loss_ctc 10.709103 loss_rnnt 6.066245 hw_loss 0.315347 lr 0.00041598 rank 4
2023-02-23 09:49:52,959 DEBUG TRAIN Batch 17/2700 loss 10.104844 loss_att 12.864688 loss_ctc 13.044131 loss_rnnt 8.939125 hw_loss 0.415963 lr 0.00041591 rank 5
2023-02-23 09:51:06,636 DEBUG TRAIN Batch 17/2800 loss 20.741070 loss_att 22.685141 loss_ctc 37.838707 loss_rnnt 17.776543 hw_loss 0.555054 lr 0.00041588 rank 1
2023-02-23 09:51:06,639 DEBUG TRAIN Batch 17/2800 loss 4.156283 loss_att 9.471951 loss_ctc 5.312329 loss_rnnt 2.734214 hw_loss 0.383994 lr 0.00041577 rank 5
2023-02-23 09:51:06,642 DEBUG TRAIN Batch 17/2800 loss 7.850967 loss_att 10.976009 loss_ctc 8.385431 loss_rnnt 6.980154 hw_loss 0.327271 lr 0.00041586 rank 0
2023-02-23 09:51:06,647 DEBUG TRAIN Batch 17/2800 loss 8.550257 loss_att 13.491671 loss_ctc 10.702373 loss_rnnt 7.058885 hw_loss 0.405264 lr 0.00041576 rank 7
2023-02-23 09:51:06,650 DEBUG TRAIN Batch 17/2800 loss 6.849265 loss_att 12.138299 loss_ctc 9.980841 loss_rnnt 5.189276 hw_loss 0.346198 lr 0.00041574 rank 3
2023-02-23 09:51:06,651 DEBUG TRAIN Batch 17/2800 loss 10.987244 loss_att 12.314846 loss_ctc 15.293850 loss_rnnt 9.916454 hw_loss 0.433228 lr 0.00041579 rank 2
2023-02-23 09:51:06,653 DEBUG TRAIN Batch 17/2800 loss 13.454686 loss_att 17.494530 loss_ctc 23.089842 loss_rnnt 11.178247 hw_loss 0.344589 lr 0.00041583 rank 6
2023-02-23 09:51:06,658 DEBUG TRAIN Batch 17/2800 loss 11.425586 loss_att 13.607464 loss_ctc 14.153041 loss_rnnt 10.428637 hw_loss 0.369212 lr 0.00041584 rank 4
2023-02-23 09:52:20,696 DEBUG TRAIN Batch 17/2900 loss 11.442418 loss_att 15.572214 loss_ctc 14.748491 loss_rnnt 9.980253 hw_loss 0.366365 lr 0.00041562 rank 7
2023-02-23 09:52:20,698 DEBUG TRAIN Batch 17/2900 loss 9.146768 loss_att 11.348882 loss_ctc 10.822330 loss_rnnt 8.285379 hw_loss 0.370419 lr 0.00041574 rank 1
2023-02-23 09:52:20,698 DEBUG TRAIN Batch 17/2900 loss 14.190997 loss_att 14.175942 loss_ctc 17.062094 loss_rnnt 13.567143 hw_loss 0.457597 lr 0.00041569 rank 6
2023-02-23 09:52:20,701 DEBUG TRAIN Batch 17/2900 loss 11.457483 loss_att 12.636898 loss_ctc 13.495447 loss_rnnt 10.716576 hw_loss 0.437432 lr 0.00041572 rank 0
2023-02-23 09:52:20,703 DEBUG TRAIN Batch 17/2900 loss 7.911853 loss_att 9.466368 loss_ctc 13.103655 loss_rnnt 6.676491 hw_loss 0.435410 lr 0.00041559 rank 3
2023-02-23 09:52:20,704 DEBUG TRAIN Batch 17/2900 loss 8.010136 loss_att 11.868711 loss_ctc 9.244478 loss_rnnt 6.858675 hw_loss 0.403436 lr 0.00041569 rank 4
2023-02-23 09:52:20,705 DEBUG TRAIN Batch 17/2900 loss 12.471114 loss_att 13.958151 loss_ctc 16.358297 loss_rnnt 11.441681 hw_loss 0.400751 lr 0.00041565 rank 2
2023-02-23 09:52:20,706 DEBUG TRAIN Batch 17/2900 loss 10.517502 loss_att 12.867249 loss_ctc 19.776840 loss_rnnt 8.619852 hw_loss 0.362103 lr 0.00041562 rank 5
2023-02-23 09:53:33,341 DEBUG TRAIN Batch 17/3000 loss 12.984691 loss_att 15.526138 loss_ctc 20.400272 loss_rnnt 11.255612 hw_loss 0.435082 lr 0.00041545 rank 3
2023-02-23 09:53:33,341 DEBUG TRAIN Batch 17/3000 loss 11.917268 loss_att 15.138951 loss_ctc 19.984047 loss_rnnt 9.976421 hw_loss 0.414262 lr 0.00041555 rank 4
2023-02-23 09:53:33,342 DEBUG TRAIN Batch 17/3000 loss 9.933023 loss_att 12.988024 loss_ctc 13.068844 loss_rnnt 8.682859 hw_loss 0.414476 lr 0.00041548 rank 7
2023-02-23 09:53:33,343 DEBUG TRAIN Batch 17/3000 loss 19.604790 loss_att 19.091549 loss_ctc 25.868725 loss_rnnt 18.663357 hw_loss 0.391663 lr 0.00041550 rank 2
2023-02-23 09:53:33,343 DEBUG TRAIN Batch 17/3000 loss 8.541171 loss_att 9.271711 loss_ctc 9.403011 loss_rnnt 8.024526 hw_loss 0.479298 lr 0.00041560 rank 1
2023-02-23 09:53:33,344 DEBUG TRAIN Batch 17/3000 loss 9.880686 loss_att 13.033298 loss_ctc 12.505112 loss_rnnt 8.674988 hw_loss 0.422346 lr 0.00041557 rank 0
2023-02-23 09:53:33,344 DEBUG TRAIN Batch 17/3000 loss 10.971987 loss_att 13.702109 loss_ctc 14.911082 loss_rnnt 9.625359 hw_loss 0.516357 lr 0.00041554 rank 6
2023-02-23 09:53:33,345 DEBUG TRAIN Batch 17/3000 loss 12.369924 loss_att 14.573370 loss_ctc 15.637645 loss_rnnt 11.266302 hw_loss 0.426068 lr 0.00041548 rank 5
2023-02-23 09:54:46,548 DEBUG TRAIN Batch 17/3100 loss 12.954200 loss_att 17.121284 loss_ctc 16.354956 loss_rnnt 11.461634 hw_loss 0.385716 lr 0.00041540 rank 6
2023-02-23 09:54:46,550 DEBUG TRAIN Batch 17/3100 loss 13.318009 loss_att 18.515207 loss_ctc 16.861980 loss_rnnt 11.596231 hw_loss 0.393389 lr 0.00041533 rank 7
2023-02-23 09:54:46,552 DEBUG TRAIN Batch 17/3100 loss 9.978968 loss_att 13.078043 loss_ctc 13.151648 loss_rnnt 8.692887 hw_loss 0.456077 lr 0.00041533 rank 5
2023-02-23 09:54:46,552 DEBUG TRAIN Batch 17/3100 loss 7.279198 loss_att 8.634949 loss_ctc 9.845889 loss_rnnt 6.326684 hw_loss 0.635884 lr 0.00041531 rank 3
2023-02-23 09:54:46,552 DEBUG TRAIN Batch 17/3100 loss 10.776940 loss_att 12.484341 loss_ctc 14.596909 loss_rnnt 9.688083 hw_loss 0.446340 lr 0.00041536 rank 2
2023-02-23 09:54:46,554 DEBUG TRAIN Batch 17/3100 loss 10.275648 loss_att 11.251615 loss_ctc 12.976967 loss_rnnt 9.535563 hw_loss 0.346341 lr 0.00041543 rank 0
2023-02-23 09:54:46,557 DEBUG TRAIN Batch 17/3100 loss 17.029652 loss_att 15.375011 loss_ctc 21.384808 loss_rnnt 16.517258 hw_loss 0.492438 lr 0.00041545 rank 1
2023-02-23 09:54:46,557 DEBUG TRAIN Batch 17/3100 loss 21.699396 loss_att 19.950825 loss_ctc 25.268673 loss_rnnt 21.356030 hw_loss 0.407208 lr 0.00041541 rank 4
2023-02-23 09:56:02,220 DEBUG TRAIN Batch 17/3200 loss 8.855430 loss_att 11.411501 loss_ctc 13.506132 loss_rnnt 7.505376 hw_loss 0.410148 lr 0.00041519 rank 7
2023-02-23 09:56:02,223 DEBUG TRAIN Batch 17/3200 loss 29.798100 loss_att 33.174286 loss_ctc 41.675003 loss_rnnt 27.353142 hw_loss 0.348999 lr 0.00041522 rank 2
2023-02-23 09:56:02,230 DEBUG TRAIN Batch 17/3200 loss 8.836889 loss_att 10.392906 loss_ctc 12.886009 loss_rnnt 7.791750 hw_loss 0.363851 lr 0.00041516 rank 3
2023-02-23 09:56:02,230 DEBUG TRAIN Batch 17/3200 loss 5.105263 loss_att 9.434435 loss_ctc 6.197153 loss_rnnt 3.864151 hw_loss 0.430672 lr 0.00041526 rank 6
2023-02-23 09:56:02,231 DEBUG TRAIN Batch 17/3200 loss 10.622345 loss_att 14.257332 loss_ctc 16.298203 loss_rnnt 8.936188 hw_loss 0.379463 lr 0.00041526 rank 4
2023-02-23 09:56:02,235 DEBUG TRAIN Batch 17/3200 loss 14.319937 loss_att 16.026390 loss_ctc 17.197739 loss_rnnt 13.354445 hw_loss 0.450928 lr 0.00041519 rank 5
2023-02-23 09:56:02,247 DEBUG TRAIN Batch 17/3200 loss 7.133830 loss_att 12.268772 loss_ctc 13.297028 loss_rnnt 5.118617 hw_loss 0.312122 lr 0.00041531 rank 1
2023-02-23 09:56:02,249 DEBUG TRAIN Batch 17/3200 loss 13.644464 loss_att 14.977154 loss_ctc 14.897024 loss_rnnt 13.014929 hw_loss 0.367481 lr 0.00041529 rank 0
2023-02-23 09:57:14,579 DEBUG TRAIN Batch 17/3300 loss 8.247568 loss_att 10.561296 loss_ctc 10.203685 loss_rnnt 7.221706 hw_loss 0.566813 lr 0.00041505 rank 7
2023-02-23 09:57:14,582 DEBUG TRAIN Batch 17/3300 loss 10.287899 loss_att 12.785803 loss_ctc 16.569241 loss_rnnt 8.760635 hw_loss 0.356569 lr 0.00041507 rank 2
2023-02-23 09:57:14,583 DEBUG TRAIN Batch 17/3300 loss 13.864669 loss_att 17.333378 loss_ctc 20.468464 loss_rnnt 12.044719 hw_loss 0.460691 lr 0.00041502 rank 3
2023-02-23 09:57:14,588 DEBUG TRAIN Batch 17/3300 loss 7.341887 loss_att 10.956778 loss_ctc 10.448114 loss_rnnt 5.974552 hw_loss 0.431612 lr 0.00041505 rank 5
2023-02-23 09:57:14,587 DEBUG TRAIN Batch 17/3300 loss 15.531134 loss_att 16.961777 loss_ctc 26.412010 loss_rnnt 13.586290 hw_loss 0.389869 lr 0.00041512 rank 4
2023-02-23 09:57:14,588 DEBUG TRAIN Batch 17/3300 loss 6.032565 loss_att 8.033068 loss_ctc 8.351040 loss_rnnt 5.087172 hw_loss 0.442805 lr 0.00041517 rank 1
2023-02-23 09:57:14,589 DEBUG TRAIN Batch 17/3300 loss 15.311735 loss_att 19.223843 loss_ctc 20.561077 loss_rnnt 13.587883 hw_loss 0.452849 lr 0.00041511 rank 6
2023-02-23 09:57:14,634 DEBUG TRAIN Batch 17/3300 loss 5.832044 loss_att 10.082506 loss_ctc 8.690598 loss_rnnt 4.378275 hw_loss 0.417254 lr 0.00041514 rank 0
2023-02-23 09:58:26,904 DEBUG TRAIN Batch 17/3400 loss 19.656298 loss_att 24.140772 loss_ctc 26.858664 loss_rnnt 17.591511 hw_loss 0.389206 lr 0.00041488 rank 3
2023-02-23 09:58:26,905 DEBUG TRAIN Batch 17/3400 loss 13.974390 loss_att 18.739155 loss_ctc 22.988569 loss_rnnt 11.585453 hw_loss 0.438926 lr 0.00041493 rank 2
2023-02-23 09:58:26,906 DEBUG TRAIN Batch 17/3400 loss 12.767604 loss_att 12.348651 loss_ctc 16.035873 loss_rnnt 12.223295 hw_loss 0.360620 lr 0.00041490 rank 7
2023-02-23 09:58:26,907 DEBUG TRAIN Batch 17/3400 loss 2.503878 loss_att 4.454456 loss_ctc 3.210855 loss_rnnt 1.838022 hw_loss 0.340268 lr 0.00041500 rank 0
2023-02-23 09:58:26,906 DEBUG TRAIN Batch 17/3400 loss 12.843918 loss_att 19.298418 loss_ctc 17.493214 loss_rnnt 10.694304 hw_loss 0.447765 lr 0.00041498 rank 4
2023-02-23 09:58:26,912 DEBUG TRAIN Batch 17/3400 loss 12.483927 loss_att 12.973274 loss_ctc 15.889341 loss_rnnt 11.703814 hw_loss 0.427851 lr 0.00041502 rank 1
2023-02-23 09:58:26,915 DEBUG TRAIN Batch 17/3400 loss 5.541786 loss_att 8.071736 loss_ctc 7.695829 loss_rnnt 4.534369 hw_loss 0.401662 lr 0.00041497 rank 6
2023-02-23 09:58:26,922 DEBUG TRAIN Batch 17/3400 loss 11.901913 loss_att 12.629087 loss_ctc 16.414375 loss_rnnt 10.949781 hw_loss 0.384441 lr 0.00041491 rank 5
2023-02-23 09:59:40,297 DEBUG TRAIN Batch 17/3500 loss 11.378365 loss_att 13.797672 loss_ctc 15.761314 loss_rnnt 10.107841 hw_loss 0.379256 lr 0.00041484 rank 4
2023-02-23 09:59:40,298 DEBUG TRAIN Batch 17/3500 loss 6.952268 loss_att 9.566284 loss_ctc 11.681320 loss_rnnt 5.589509 hw_loss 0.392652 lr 0.00041486 rank 0
2023-02-23 09:59:40,313 DEBUG TRAIN Batch 17/3500 loss 10.370287 loss_att 14.679219 loss_ctc 14.185439 loss_rnnt 8.801118 hw_loss 0.372553 lr 0.00041476 rank 5
2023-02-23 09:59:40,313 DEBUG TRAIN Batch 17/3500 loss 7.123901 loss_att 11.055969 loss_ctc 8.693061 loss_rnnt 5.944654 hw_loss 0.344273 lr 0.00041476 rank 7
2023-02-23 09:59:40,314 DEBUG TRAIN Batch 17/3500 loss 10.952350 loss_att 13.872785 loss_ctc 16.391624 loss_rnnt 9.466514 hw_loss 0.330961 lr 0.00041474 rank 3
2023-02-23 09:59:40,317 DEBUG TRAIN Batch 17/3500 loss 7.999228 loss_att 9.161257 loss_ctc 12.591539 loss_rnnt 6.928267 hw_loss 0.424212 lr 0.00041488 rank 1
2023-02-23 09:59:40,318 DEBUG TRAIN Batch 17/3500 loss 6.862860 loss_att 8.553779 loss_ctc 9.371390 loss_rnnt 5.995534 hw_loss 0.365008 lr 0.00041479 rank 2
2023-02-23 09:59:40,342 DEBUG TRAIN Batch 17/3500 loss 12.524567 loss_att 16.977001 loss_ctc 16.114548 loss_rnnt 10.921728 hw_loss 0.438166 lr 0.00041483 rank 6
2023-02-23 10:00:53,809 DEBUG TRAIN Batch 17/3600 loss 11.198156 loss_att 12.211335 loss_ctc 16.168623 loss_rnnt 10.106565 hw_loss 0.424172 lr 0.00041459 rank 3
2023-02-23 10:00:53,811 DEBUG TRAIN Batch 17/3600 loss 8.804959 loss_att 12.640629 loss_ctc 16.551769 loss_rnnt 6.782714 hw_loss 0.416629 lr 0.00041462 rank 5
2023-02-23 10:00:53,811 DEBUG TRAIN Batch 17/3600 loss 14.309731 loss_att 16.452789 loss_ctc 18.431150 loss_rnnt 13.097137 hw_loss 0.439612 lr 0.00041472 rank 0
2023-02-23 10:00:53,811 DEBUG TRAIN Batch 17/3600 loss 11.192389 loss_att 14.135815 loss_ctc 16.410658 loss_rnnt 9.701826 hw_loss 0.386452 lr 0.00041469 rank 4
2023-02-23 10:00:53,811 DEBUG TRAIN Batch 17/3600 loss 6.384755 loss_att 11.406013 loss_ctc 12.485014 loss_rnnt 4.354919 hw_loss 0.397905 lr 0.00041468 rank 6
2023-02-23 10:00:53,815 DEBUG TRAIN Batch 17/3600 loss 8.393867 loss_att 8.628479 loss_ctc 9.993595 loss_rnnt 7.925753 hw_loss 0.389803 lr 0.00041462 rank 7
2023-02-23 10:00:53,816 DEBUG TRAIN Batch 17/3600 loss 16.462986 loss_att 18.083065 loss_ctc 23.248236 loss_rnnt 15.008724 hw_loss 0.422897 lr 0.00041474 rank 1
2023-02-23 10:00:53,817 DEBUG TRAIN Batch 17/3600 loss 5.995276 loss_att 9.824553 loss_ctc 8.622944 loss_rnnt 4.683263 hw_loss 0.367129 lr 0.00041464 rank 2
2023-02-23 10:02:05,679 DEBUG TRAIN Batch 17/3700 loss 8.679582 loss_att 13.024727 loss_ctc 17.159302 loss_rnnt 6.498287 hw_loss 0.340569 lr 0.00041457 rank 0
2023-02-23 10:02:05,680 DEBUG TRAIN Batch 17/3700 loss 7.677874 loss_att 9.534932 loss_ctc 10.986229 loss_rnnt 6.649122 hw_loss 0.405425 lr 0.00041450 rank 2
2023-02-23 10:02:05,680 DEBUG TRAIN Batch 17/3700 loss 13.163696 loss_att 14.527174 loss_ctc 15.404658 loss_rnnt 12.405724 hw_loss 0.349653 lr 0.00041448 rank 7
2023-02-23 10:02:05,682 DEBUG TRAIN Batch 17/3700 loss 8.460284 loss_att 11.214964 loss_ctc 9.446045 loss_rnnt 7.547116 hw_loss 0.432745 lr 0.00041455 rank 4
2023-02-23 10:02:05,682 DEBUG TRAIN Batch 17/3700 loss 6.633876 loss_att 8.129498 loss_ctc 8.818787 loss_rnnt 5.801512 hw_loss 0.453597 lr 0.00041448 rank 5
2023-02-23 10:02:05,684 DEBUG TRAIN Batch 17/3700 loss 13.943718 loss_att 16.856174 loss_ctc 20.279016 loss_rnnt 12.248076 hw_loss 0.503332 lr 0.00041445 rank 3
2023-02-23 10:02:05,685 DEBUG TRAIN Batch 17/3700 loss 20.091465 loss_att 22.435516 loss_ctc 24.171030 loss_rnnt 18.842085 hw_loss 0.443679 lr 0.00041460 rank 1
2023-02-23 10:02:05,730 DEBUG TRAIN Batch 17/3700 loss 7.648539 loss_att 8.649504 loss_ctc 9.043624 loss_rnnt 6.984518 hw_loss 0.520905 lr 0.00041454 rank 6
2023-02-23 10:03:18,898 DEBUG TRAIN Batch 17/3800 loss 2.905683 loss_att 5.668571 loss_ctc 2.758660 loss_rnnt 2.212149 hw_loss 0.301050 lr 0.00041431 rank 3
2023-02-23 10:03:18,899 DEBUG TRAIN Batch 17/3800 loss 15.786704 loss_att 20.418392 loss_ctc 22.816912 loss_rnnt 13.612579 hw_loss 0.582049 lr 0.00041434 rank 5
2023-02-23 10:03:18,901 DEBUG TRAIN Batch 17/3800 loss 7.485403 loss_att 11.228823 loss_ctc 12.159605 loss_rnnt 5.904829 hw_loss 0.391245 lr 0.00041440 rank 6
2023-02-23 10:03:18,904 DEBUG TRAIN Batch 17/3800 loss 8.707009 loss_att 13.201294 loss_ctc 13.169047 loss_rnnt 7.031824 hw_loss 0.340106 lr 0.00041436 rank 2
2023-02-23 10:03:18,903 DEBUG TRAIN Batch 17/3800 loss 9.691418 loss_att 10.459602 loss_ctc 11.511782 loss_rnnt 9.026519 hw_loss 0.503525 lr 0.00041445 rank 1
2023-02-23 10:03:18,904 DEBUG TRAIN Batch 17/3800 loss 10.353164 loss_att 9.032155 loss_ctc 12.298303 loss_rnnt 10.039524 hw_loss 0.597168 lr 0.00041433 rank 7
2023-02-23 10:03:18,906 DEBUG TRAIN Batch 17/3800 loss 8.652018 loss_att 8.201261 loss_ctc 10.419652 loss_rnnt 8.113144 hw_loss 0.737511 lr 0.00041441 rank 4
2023-02-23 10:03:18,948 DEBUG TRAIN Batch 17/3800 loss 8.012437 loss_att 9.852434 loss_ctc 9.521732 loss_rnnt 7.129242 hw_loss 0.588667 lr 0.00041443 rank 0
2023-02-23 10:04:33,739 DEBUG TRAIN Batch 17/3900 loss 6.845186 loss_att 11.709745 loss_ctc 8.314344 loss_rnnt 5.456913 hw_loss 0.411516 lr 0.00041426 rank 6
2023-02-23 10:04:33,757 DEBUG TRAIN Batch 17/3900 loss 15.387742 loss_att 17.788052 loss_ctc 18.639938 loss_rnnt 14.310493 hw_loss 0.306676 lr 0.00041422 rank 2
2023-02-23 10:04:33,757 DEBUG TRAIN Batch 17/3900 loss 4.573609 loss_att 8.657453 loss_ctc 8.770039 loss_rnnt 3.050924 hw_loss 0.274486 lr 0.00041431 rank 1
2023-02-23 10:04:33,764 DEBUG TRAIN Batch 17/3900 loss 18.575636 loss_att 22.782011 loss_ctc 29.845257 loss_rnnt 16.078556 hw_loss 0.287227 lr 0.00041419 rank 5
2023-02-23 10:04:33,774 DEBUG TRAIN Batch 17/3900 loss 14.352858 loss_att 17.239027 loss_ctc 18.401762 loss_rnnt 12.942602 hw_loss 0.549688 lr 0.00041419 rank 7
2023-02-23 10:04:33,774 DEBUG TRAIN Batch 17/3900 loss 8.733812 loss_att 9.992741 loss_ctc 11.539427 loss_rnnt 7.858140 hw_loss 0.468383 lr 0.00041429 rank 0
2023-02-23 10:04:33,782 DEBUG TRAIN Batch 17/3900 loss 3.224443 loss_att 6.650621 loss_ctc 4.195929 loss_rnnt 2.228282 hw_loss 0.340113 lr 0.00041417 rank 3
2023-02-23 10:04:33,803 DEBUG TRAIN Batch 17/3900 loss 6.263161 loss_att 8.569519 loss_ctc 7.311659 loss_rnnt 5.425638 hw_loss 0.443347 lr 0.00041427 rank 4
2023-02-23 10:05:48,515 DEBUG TRAIN Batch 17/4000 loss 9.009861 loss_att 9.861850 loss_ctc 14.514945 loss_rnnt 7.929694 hw_loss 0.329547 lr 0.00041405 rank 7
2023-02-23 10:05:48,516 DEBUG TRAIN Batch 17/4000 loss 2.941527 loss_att 7.607129 loss_ctc 6.116041 loss_rnnt 1.430601 hw_loss 0.289757 lr 0.00041402 rank 3
2023-02-23 10:05:48,520 DEBUG TRAIN Batch 17/4000 loss 11.213694 loss_att 12.029261 loss_ctc 16.896454 loss_rnnt 10.089293 hw_loss 0.381725 lr 0.00041408 rank 2
2023-02-23 10:05:48,522 DEBUG TRAIN Batch 17/4000 loss 5.841476 loss_att 10.038603 loss_ctc 7.379151 loss_rnnt 4.616743 hw_loss 0.338033 lr 0.00041405 rank 5
2023-02-23 10:05:48,522 DEBUG TRAIN Batch 17/4000 loss 5.904871 loss_att 9.108704 loss_ctc 7.768071 loss_rnnt 4.813568 hw_loss 0.378954 lr 0.00041417 rank 1
2023-02-23 10:05:48,523 DEBUG TRAIN Batch 17/4000 loss 3.755857 loss_att 7.564669 loss_ctc 6.635818 loss_rnnt 2.412170 hw_loss 0.371119 lr 0.00041412 rank 6
2023-02-23 10:05:48,527 DEBUG TRAIN Batch 17/4000 loss 17.402870 loss_att 20.130306 loss_ctc 28.692192 loss_rnnt 15.170317 hw_loss 0.340919 lr 0.00041412 rank 4
2023-02-23 10:05:48,566 DEBUG TRAIN Batch 17/4000 loss 11.104691 loss_att 15.524642 loss_ctc 17.801357 loss_rnnt 9.152125 hw_loss 0.329412 lr 0.00041415 rank 0
2023-02-23 10:07:01,434 DEBUG TRAIN Batch 17/4100 loss 3.939669 loss_att 6.700521 loss_ctc 5.787124 loss_rnnt 2.923813 hw_loss 0.407547 lr 0.00041391 rank 7
2023-02-23 10:07:01,436 DEBUG TRAIN Batch 17/4100 loss 5.146368 loss_att 7.434225 loss_ctc 6.801644 loss_rnnt 4.246167 hw_loss 0.416111 lr 0.00041393 rank 2
2023-02-23 10:07:01,435 DEBUG TRAIN Batch 17/4100 loss 13.441073 loss_att 16.540447 loss_ctc 16.281597 loss_rnnt 12.263041 hw_loss 0.336416 lr 0.00041398 rank 4
2023-02-23 10:07:01,435 DEBUG TRAIN Batch 17/4100 loss 9.347651 loss_att 11.013147 loss_ctc 10.395751 loss_rnnt 8.641953 hw_loss 0.436596 lr 0.00041403 rank 1
2023-02-23 10:07:01,437 DEBUG TRAIN Batch 17/4100 loss 4.776862 loss_att 7.537081 loss_ctc 10.014015 loss_rnnt 3.297787 hw_loss 0.428894 lr 0.00041388 rank 3
2023-02-23 10:07:01,442 DEBUG TRAIN Batch 17/4100 loss 13.897148 loss_att 16.673855 loss_ctc 18.747280 loss_rnnt 12.492689 hw_loss 0.379564 lr 0.00041391 rank 5
2023-02-23 10:07:01,445 DEBUG TRAIN Batch 17/4100 loss 1.803755 loss_att 4.804955 loss_ctc 2.095980 loss_rnnt 0.972719 hw_loss 0.359686 lr 0.00041400 rank 0
2023-02-23 10:07:01,450 DEBUG TRAIN Batch 17/4100 loss 12.127040 loss_att 13.739981 loss_ctc 13.743225 loss_rnnt 11.393478 hw_loss 0.366528 lr 0.00041397 rank 6
2023-02-23 10:08:14,860 DEBUG TRAIN Batch 17/4200 loss 25.020357 loss_att 28.677328 loss_ctc 28.440880 loss_rnnt 23.603371 hw_loss 0.430354 lr 0.00041377 rank 7
2023-02-23 10:08:14,865 DEBUG TRAIN Batch 17/4200 loss 10.879092 loss_att 13.653031 loss_ctc 18.057350 loss_rnnt 9.151689 hw_loss 0.404092 lr 0.00041389 rank 1
2023-02-23 10:08:14,874 DEBUG TRAIN Batch 17/4200 loss 5.369966 loss_att 9.046677 loss_ctc 6.158273 loss_rnnt 4.305377 hw_loss 0.420260 lr 0.00041374 rank 3
2023-02-23 10:08:14,881 DEBUG TRAIN Batch 17/4200 loss 3.747793 loss_att 7.028292 loss_ctc 4.425616 loss_rnnt 2.810779 hw_loss 0.357260 lr 0.00041383 rank 6
2023-02-23 10:08:14,882 DEBUG TRAIN Batch 17/4200 loss 3.277117 loss_att 6.340960 loss_ctc 5.406836 loss_rnnt 2.111413 hw_loss 0.504325 lr 0.00041377 rank 5
2023-02-23 10:08:14,884 DEBUG TRAIN Batch 17/4200 loss 14.196672 loss_att 17.336115 loss_ctc 20.119469 loss_rnnt 12.559073 hw_loss 0.412510 lr 0.00041379 rank 2
2023-02-23 10:08:14,897 DEBUG TRAIN Batch 17/4200 loss 10.707588 loss_att 13.603795 loss_ctc 16.481033 loss_rnnt 9.169922 hw_loss 0.353683 lr 0.00041386 rank 0
2023-02-23 10:08:14,925 DEBUG TRAIN Batch 17/4200 loss 7.211192 loss_att 11.728897 loss_ctc 9.782495 loss_rnnt 5.716625 hw_loss 0.465347 lr 0.00041384 rank 4
2023-02-23 10:09:29,350 DEBUG TRAIN Batch 17/4300 loss 11.307740 loss_att 13.857738 loss_ctc 16.956215 loss_rnnt 9.813288 hw_loss 0.433730 lr 0.00041362 rank 7
2023-02-23 10:09:29,350 DEBUG TRAIN Batch 17/4300 loss 16.049894 loss_att 17.076075 loss_ctc 17.548456 loss_rnnt 15.416040 hw_loss 0.429014 lr 0.00041365 rank 2
2023-02-23 10:09:29,353 DEBUG TRAIN Batch 17/4300 loss 14.658009 loss_att 16.687654 loss_ctc 15.877803 loss_rnnt 13.863861 hw_loss 0.422962 lr 0.00041360 rank 3
2023-02-23 10:09:29,355 DEBUG TRAIN Batch 17/4300 loss 8.869961 loss_att 11.562186 loss_ctc 12.948086 loss_rnnt 7.611073 hw_loss 0.331298 lr 0.00041374 rank 1
2023-02-23 10:09:29,360 DEBUG TRAIN Batch 17/4300 loss 9.794025 loss_att 12.226981 loss_ctc 14.680189 loss_rnnt 8.412096 hw_loss 0.457218 lr 0.00041372 rank 0
2023-02-23 10:09:29,360 DEBUG TRAIN Batch 17/4300 loss 11.227787 loss_att 17.712765 loss_ctc 18.907738 loss_rnnt 8.700073 hw_loss 0.387608 lr 0.00041370 rank 4
2023-02-23 10:09:29,364 DEBUG TRAIN Batch 17/4300 loss 5.472520 loss_att 8.697923 loss_ctc 7.344347 loss_rnnt 4.352146 hw_loss 0.423219 lr 0.00041363 rank 5
2023-02-23 10:09:29,366 DEBUG TRAIN Batch 17/4300 loss 11.158445 loss_att 12.511295 loss_ctc 14.394035 loss_rnnt 10.228667 hw_loss 0.427118 lr 0.00041369 rank 6
2023-02-23 10:10:41,973 DEBUG TRAIN Batch 17/4400 loss 10.891350 loss_att 11.548013 loss_ctc 13.826930 loss_rnnt 10.167735 hw_loss 0.376634 lr 0.00041351 rank 2
2023-02-23 10:10:41,975 DEBUG TRAIN Batch 17/4400 loss 13.445886 loss_att 18.252487 loss_ctc 17.702593 loss_rnnt 11.724380 hw_loss 0.361172 lr 0.00041346 rank 3
2023-02-23 10:10:41,975 DEBUG TRAIN Batch 17/4400 loss 9.024292 loss_att 10.953924 loss_ctc 10.729218 loss_rnnt 8.178086 hw_loss 0.436792 lr 0.00041358 rank 0
2023-02-23 10:10:41,975 DEBUG TRAIN Batch 17/4400 loss 8.990659 loss_att 10.213379 loss_ctc 11.544322 loss_rnnt 8.137627 hw_loss 0.502498 lr 0.00041360 rank 1
2023-02-23 10:10:41,978 DEBUG TRAIN Batch 17/4400 loss 9.241110 loss_att 11.638403 loss_ctc 14.138279 loss_rnnt 7.887105 hw_loss 0.415479 lr 0.00041356 rank 4
2023-02-23 10:10:41,978 DEBUG TRAIN Batch 17/4400 loss 12.468821 loss_att 13.759807 loss_ctc 12.700466 loss_rnnt 11.918198 hw_loss 0.490389 lr 0.00041348 rank 7
2023-02-23 10:10:41,983 DEBUG TRAIN Batch 17/4400 loss 10.628765 loss_att 13.425790 loss_ctc 16.094658 loss_rnnt 9.102496 hw_loss 0.446397 lr 0.00041355 rank 6
2023-02-23 10:10:41,984 DEBUG TRAIN Batch 17/4400 loss 5.821283 loss_att 8.090192 loss_ctc 7.271142 loss_rnnt 4.936942 hw_loss 0.444834 lr 0.00041348 rank 5
2023-02-23 10:11:53,437 DEBUG TRAIN Batch 17/4500 loss 11.410506 loss_att 18.100344 loss_ctc 12.071575 loss_rnnt 9.814001 hw_loss 0.319490 lr 0.00041334 rank 7
2023-02-23 10:11:53,438 DEBUG TRAIN Batch 17/4500 loss 6.092089 loss_att 6.599383 loss_ctc 8.511639 loss_rnnt 5.426097 hw_loss 0.453611 lr 0.00041337 rank 2
2023-02-23 10:11:53,438 DEBUG TRAIN Batch 17/4500 loss 11.622330 loss_att 15.313379 loss_ctc 14.050379 loss_rnnt 10.330914 hw_loss 0.430246 lr 0.00041346 rank 1
2023-02-23 10:11:53,440 DEBUG TRAIN Batch 17/4500 loss 5.957223 loss_att 8.884117 loss_ctc 10.303343 loss_rnnt 4.580482 hw_loss 0.397272 lr 0.00041342 rank 4
2023-02-23 10:11:53,442 DEBUG TRAIN Batch 17/4500 loss 2.410072 loss_att 5.197612 loss_ctc 2.985922 loss_rnnt 1.565148 hw_loss 0.394942 lr 0.00041332 rank 3
2023-02-23 10:11:53,442 DEBUG TRAIN Batch 17/4500 loss 7.169068 loss_att 9.424067 loss_ctc 9.574676 loss_rnnt 6.187678 hw_loss 0.393081 lr 0.00041341 rank 6
2023-02-23 10:11:53,443 DEBUG TRAIN Batch 17/4500 loss 7.672629 loss_att 9.026899 loss_ctc 9.869080 loss_rnnt 6.859535 hw_loss 0.467589 lr 0.00041334 rank 5
2023-02-23 10:11:53,446 DEBUG TRAIN Batch 17/4500 loss 17.438681 loss_att 21.751694 loss_ctc 29.541927 loss_rnnt 14.771262 hw_loss 0.358219 lr 0.00041344 rank 0
2023-02-23 10:13:07,709 DEBUG TRAIN Batch 17/4600 loss 9.602777 loss_att 14.439799 loss_ctc 13.887117 loss_rnnt 7.886382 hw_loss 0.333275 lr 0.00041320 rank 5
2023-02-23 10:13:07,717 DEBUG TRAIN Batch 17/4600 loss 12.400597 loss_att 15.710310 loss_ctc 17.394329 loss_rnnt 10.861220 hw_loss 0.396754 lr 0.00041320 rank 7
2023-02-23 10:13:07,718 DEBUG TRAIN Batch 17/4600 loss 9.861148 loss_att 10.758654 loss_ctc 13.402014 loss_rnnt 8.908817 hw_loss 0.563839 lr 0.00041327 rank 4
2023-02-23 10:13:07,719 DEBUG TRAIN Batch 17/4600 loss 6.337923 loss_att 8.524902 loss_ctc 10.199046 loss_rnnt 5.215123 hw_loss 0.319853 lr 0.00041318 rank 3
2023-02-23 10:13:07,721 DEBUG TRAIN Batch 17/4600 loss 3.646576 loss_att 6.408587 loss_ctc 5.056195 loss_rnnt 2.743577 hw_loss 0.304963 lr 0.00041323 rank 2
2023-02-23 10:13:07,720 DEBUG TRAIN Batch 17/4600 loss 20.368887 loss_att 25.181606 loss_ctc 24.820623 loss_rnnt 18.565338 hw_loss 0.463947 lr 0.00041327 rank 6
2023-02-23 10:13:07,722 DEBUG TRAIN Batch 17/4600 loss 14.071458 loss_att 13.866892 loss_ctc 16.084463 loss_rnnt 13.637266 hw_loss 0.387572 lr 0.00041332 rank 1
2023-02-23 10:13:07,723 DEBUG TRAIN Batch 17/4600 loss 8.535698 loss_att 9.598395 loss_ctc 11.586708 loss_rnnt 7.697573 hw_loss 0.410222 lr 0.00041330 rank 0
2023-02-23 10:14:20,882 DEBUG TRAIN Batch 17/4700 loss 10.881643 loss_att 12.293533 loss_ctc 15.486660 loss_rnnt 9.785370 hw_loss 0.374800 lr 0.00041306 rank 7
2023-02-23 10:14:20,885 DEBUG TRAIN Batch 17/4700 loss 9.341699 loss_att 12.484805 loss_ctc 13.032618 loss_rnnt 8.051413 hw_loss 0.317891 lr 0.00041308 rank 2
2023-02-23 10:14:20,889 DEBUG TRAIN Batch 17/4700 loss 14.059914 loss_att 17.257202 loss_ctc 17.026871 loss_rnnt 12.857576 hw_loss 0.313658 lr 0.00041313 rank 4
2023-02-23 10:14:20,890 DEBUG TRAIN Batch 17/4700 loss 14.796754 loss_att 17.907608 loss_ctc 16.875477 loss_rnnt 13.678061 hw_loss 0.411300 lr 0.00041303 rank 3
2023-02-23 10:14:20,891 DEBUG TRAIN Batch 17/4700 loss 8.934093 loss_att 14.435152 loss_ctc 12.770572 loss_rnnt 7.112959 hw_loss 0.392609 lr 0.00041318 rank 1
2023-02-23 10:14:20,896 DEBUG TRAIN Batch 17/4700 loss 4.594653 loss_att 8.467499 loss_ctc 6.909853 loss_rnnt 3.336029 hw_loss 0.328801 lr 0.00041306 rank 5
2023-02-23 10:14:20,897 DEBUG TRAIN Batch 17/4700 loss 4.183776 loss_att 5.999698 loss_ctc 4.423182 loss_rnnt 3.590271 hw_loss 0.371999 lr 0.00041312 rank 6
2023-02-23 10:14:20,940 DEBUG TRAIN Batch 17/4700 loss 6.548504 loss_att 8.930182 loss_ctc 9.382512 loss_rnnt 5.464530 hw_loss 0.430821 lr 0.00041316 rank 0
2023-02-23 10:15:33,494 DEBUG TRAIN Batch 17/4800 loss 18.235701 loss_att 16.522253 loss_ctc 19.765718 loss_rnnt 18.169378 hw_loss 0.384392 lr 0.00041299 rank 4
2023-02-23 10:15:33,495 DEBUG TRAIN Batch 17/4800 loss 8.344840 loss_att 8.685877 loss_ctc 10.990107 loss_rnnt 7.701053 hw_loss 0.417898 lr 0.00041294 rank 2
2023-02-23 10:15:33,498 DEBUG TRAIN Batch 17/4800 loss 5.960278 loss_att 9.584909 loss_ctc 9.100571 loss_rnnt 4.611088 hw_loss 0.385420 lr 0.00041292 rank 5
2023-02-23 10:15:33,498 DEBUG TRAIN Batch 17/4800 loss 12.718188 loss_att 18.927834 loss_ctc 23.348942 loss_rnnt 9.816038 hw_loss 0.455226 lr 0.00041292 rank 7
2023-02-23 10:15:33,499 DEBUG TRAIN Batch 17/4800 loss 12.873469 loss_att 14.720334 loss_ctc 17.559183 loss_rnnt 11.608864 hw_loss 0.507134 lr 0.00041289 rank 3
2023-02-23 10:15:33,500 DEBUG TRAIN Batch 17/4800 loss 8.085876 loss_att 12.609235 loss_ctc 12.010530 loss_rnnt 6.475356 hw_loss 0.342301 lr 0.00041298 rank 6
2023-02-23 10:15:33,505 DEBUG TRAIN Batch 17/4800 loss 12.696821 loss_att 13.404030 loss_ctc 17.291281 loss_rnnt 11.702169 hw_loss 0.451154 lr 0.00041301 rank 0
2023-02-23 10:15:33,548 DEBUG TRAIN Batch 17/4800 loss 10.734127 loss_att 13.317316 loss_ctc 17.500864 loss_rnnt 9.076159 hw_loss 0.448307 lr 0.00041304 rank 1
2023-02-23 10:16:46,461 DEBUG TRAIN Batch 17/4900 loss 11.303524 loss_att 13.676353 loss_ctc 13.034588 loss_rnnt 10.393639 hw_loss 0.383458 lr 0.00041290 rank 1
2023-02-23 10:16:46,463 DEBUG TRAIN Batch 17/4900 loss 6.774359 loss_att 10.729284 loss_ctc 9.169621 loss_rnnt 5.470020 hw_loss 0.363725 lr 0.00041280 rank 2
2023-02-23 10:16:46,465 DEBUG TRAIN Batch 17/4900 loss 13.858542 loss_att 15.134981 loss_ctc 18.915779 loss_rnnt 12.712495 hw_loss 0.405865 lr 0.00041278 rank 5
2023-02-23 10:16:46,465 DEBUG TRAIN Batch 17/4900 loss 8.324105 loss_att 9.807098 loss_ctc 9.380280 loss_rnnt 7.688704 hw_loss 0.371210 lr 0.00041284 rank 6
2023-02-23 10:16:46,466 DEBUG TRAIN Batch 17/4900 loss 2.429924 loss_att 5.116673 loss_ctc 3.627295 loss_rnnt 1.473971 hw_loss 0.485538 lr 0.00041278 rank 7
2023-02-23 10:16:46,470 DEBUG TRAIN Batch 17/4900 loss 12.268863 loss_att 12.381881 loss_ctc 14.095993 loss_rnnt 11.778519 hw_loss 0.420229 lr 0.00041275 rank 3
2023-02-23 10:16:46,480 DEBUG TRAIN Batch 17/4900 loss 7.856139 loss_att 9.871779 loss_ctc 8.873448 loss_rnnt 7.090548 hw_loss 0.425290 lr 0.00041285 rank 4
2023-02-23 10:16:46,483 DEBUG TRAIN Batch 17/4900 loss 5.630399 loss_att 8.132902 loss_ctc 7.209658 loss_rnnt 4.671787 hw_loss 0.464145 lr 0.00041287 rank 0
2023-02-23 10:18:02,639 DEBUG TRAIN Batch 17/5000 loss 4.016438 loss_att 5.597743 loss_ctc 5.737846 loss_rnnt 3.178254 hw_loss 0.548251 lr 0.00041266 rank 2
2023-02-23 10:18:02,642 DEBUG TRAIN Batch 17/5000 loss 21.194542 loss_att 25.696083 loss_ctc 35.519310 loss_rnnt 18.163536 hw_loss 0.413864 lr 0.00041264 rank 5
2023-02-23 10:18:02,642 DEBUG TRAIN Batch 17/5000 loss 16.985851 loss_att 16.576458 loss_ctc 18.893034 loss_rnnt 16.673317 hw_loss 0.262729 lr 0.00041261 rank 3
2023-02-23 10:18:02,643 DEBUG TRAIN Batch 17/5000 loss 8.805273 loss_att 10.357803 loss_ctc 12.853106 loss_rnnt 7.716092 hw_loss 0.448054 lr 0.00041264 rank 7
2023-02-23 10:18:02,644 DEBUG TRAIN Batch 17/5000 loss 4.693867 loss_att 8.839774 loss_ctc 6.970117 loss_rnnt 3.350255 hw_loss 0.395495 lr 0.00041270 rank 6
2023-02-23 10:18:02,646 DEBUG TRAIN Batch 17/5000 loss 16.157087 loss_att 16.051437 loss_ctc 22.917793 loss_rnnt 15.038073 hw_loss 0.447591 lr 0.00041271 rank 4
2023-02-23 10:18:02,647 DEBUG TRAIN Batch 17/5000 loss 13.091720 loss_att 16.003624 loss_ctc 18.643789 loss_rnnt 11.524817 hw_loss 0.457961 lr 0.00041276 rank 1
2023-02-23 10:18:02,649 DEBUG TRAIN Batch 17/5000 loss 13.409944 loss_att 17.140690 loss_ctc 17.494158 loss_rnnt 11.871370 hw_loss 0.464742 lr 0.00041273 rank 0
2023-02-23 10:19:14,430 DEBUG TRAIN Batch 17/5100 loss 9.593659 loss_att 13.728094 loss_ctc 12.235209 loss_rnnt 8.267790 hw_loss 0.275203 lr 0.00041247 rank 3
2023-02-23 10:19:14,431 DEBUG TRAIN Batch 17/5100 loss 10.740553 loss_att 10.879158 loss_ctc 14.371916 loss_rnnt 9.867078 hw_loss 0.677946 lr 0.00041250 rank 7
2023-02-23 10:19:14,433 DEBUG TRAIN Batch 17/5100 loss 21.580435 loss_att 24.678787 loss_ctc 26.180462 loss_rnnt 20.165710 hw_loss 0.340717 lr 0.00041257 rank 4
2023-02-23 10:19:14,433 DEBUG TRAIN Batch 17/5100 loss 10.522403 loss_att 13.855184 loss_ctc 15.373329 loss_rnnt 8.987965 hw_loss 0.414547 lr 0.00041252 rank 2
2023-02-23 10:19:14,436 DEBUG TRAIN Batch 17/5100 loss 21.788467 loss_att 27.980312 loss_ctc 25.165726 loss_rnnt 19.865862 hw_loss 0.438622 lr 0.00041256 rank 6
2023-02-23 10:19:14,441 DEBUG TRAIN Batch 17/5100 loss 11.471421 loss_att 13.541079 loss_ctc 10.125258 loss_rnnt 11.061913 hw_loss 0.328247 lr 0.00041261 rank 1
2023-02-23 10:19:14,445 DEBUG TRAIN Batch 17/5100 loss 5.182618 loss_att 8.186817 loss_ctc 6.667316 loss_rnnt 4.101990 hw_loss 0.528429 lr 0.00041250 rank 5
2023-02-23 10:19:14,451 DEBUG TRAIN Batch 17/5100 loss 13.852768 loss_att 19.360725 loss_ctc 21.273243 loss_rnnt 11.613230 hw_loss 0.278530 lr 0.00041259 rank 0
2023-02-23 10:20:26,905 DEBUG TRAIN Batch 17/5200 loss 15.485068 loss_att 14.934216 loss_ctc 20.178795 loss_rnnt 14.756392 hw_loss 0.399409 lr 0.00041243 rank 4
2023-02-23 10:20:26,907 DEBUG TRAIN Batch 17/5200 loss 9.766949 loss_att 16.591370 loss_ctc 13.411418 loss_rnnt 7.693216 hw_loss 0.417973 lr 0.00041233 rank 3
2023-02-23 10:20:26,908 DEBUG TRAIN Batch 17/5200 loss 12.501856 loss_att 16.002193 loss_ctc 19.697433 loss_rnnt 10.653168 hw_loss 0.354767 lr 0.00041236 rank 7
2023-02-23 10:20:26,909 DEBUG TRAIN Batch 17/5200 loss 24.831722 loss_att 23.945835 loss_ctc 30.187307 loss_rnnt 24.101765 hw_loss 0.361984 lr 0.00041247 rank 1
2023-02-23 10:20:26,911 DEBUG TRAIN Batch 17/5200 loss 15.771978 loss_att 15.117086 loss_ctc 18.295929 loss_rnnt 15.385632 hw_loss 0.338999 lr 0.00041238 rank 2
2023-02-23 10:20:26,912 DEBUG TRAIN Batch 17/5200 loss 18.590801 loss_att 22.680626 loss_ctc 24.487507 loss_rnnt 16.757317 hw_loss 0.429920 lr 0.00041242 rank 6
2023-02-23 10:20:26,914 DEBUG TRAIN Batch 17/5200 loss 8.437745 loss_att 8.456332 loss_ctc 10.553448 loss_rnnt 7.773881 hw_loss 0.708849 lr 0.00041236 rank 5
2023-02-23 10:20:26,960 DEBUG TRAIN Batch 17/5200 loss 12.221013 loss_att 16.339270 loss_ctc 17.533375 loss_rnnt 10.446165 hw_loss 0.455403 lr 0.00041245 rank 0
2023-02-23 10:21:41,244 DEBUG TRAIN Batch 17/5300 loss 5.132291 loss_att 7.222506 loss_ctc 6.762565 loss_rnnt 4.303644 hw_loss 0.362317 lr 0.00041228 rank 6
2023-02-23 10:21:41,251 DEBUG TRAIN Batch 17/5300 loss 32.511921 loss_att 36.729637 loss_ctc 44.682861 loss_rnnt 29.824850 hw_loss 0.413875 lr 0.00041231 rank 0
2023-02-23 10:21:41,250 DEBUG TRAIN Batch 17/5300 loss 9.973052 loss_att 12.132784 loss_ctc 12.594317 loss_rnnt 8.989639 hw_loss 0.378683 lr 0.00041222 rank 7
2023-02-23 10:21:41,253 DEBUG TRAIN Batch 17/5300 loss 7.395977 loss_att 10.278334 loss_ctc 9.919071 loss_rnnt 6.327663 hw_loss 0.291430 lr 0.00041233 rank 1
2023-02-23 10:21:41,263 DEBUG TRAIN Batch 17/5300 loss 11.327874 loss_att 17.917015 loss_ctc 19.219320 loss_rnnt 8.763396 hw_loss 0.364607 lr 0.00041219 rank 3
2023-02-23 10:21:41,262 DEBUG TRAIN Batch 17/5300 loss 4.610912 loss_att 8.104656 loss_ctc 5.699485 loss_rnnt 3.548254 hw_loss 0.410186 lr 0.00041222 rank 5
2023-02-23 10:21:41,271 DEBUG TRAIN Batch 17/5300 loss 10.108384 loss_att 17.007713 loss_ctc 12.791943 loss_rnnt 8.125269 hw_loss 0.460202 lr 0.00041224 rank 2
2023-02-23 10:21:41,280 DEBUG TRAIN Batch 17/5300 loss 4.456465 loss_att 5.943639 loss_ctc 3.908096 loss_rnnt 4.021482 hw_loss 0.394994 lr 0.00041229 rank 4
2023-02-23 10:22:54,994 DEBUG TRAIN Batch 17/5400 loss 10.180508 loss_att 11.758297 loss_ctc 17.173752 loss_rnnt 8.764719 hw_loss 0.314623 lr 0.00041208 rank 5
2023-02-23 10:22:55,007 DEBUG TRAIN Batch 17/5400 loss 11.359256 loss_att 12.928637 loss_ctc 19.044743 loss_rnnt 9.827661 hw_loss 0.361851 lr 0.00041215 rank 4
2023-02-23 10:22:55,009 DEBUG TRAIN Batch 17/5400 loss 8.213368 loss_att 11.006383 loss_ctc 11.461265 loss_rnnt 6.996958 hw_loss 0.421417 lr 0.00041208 rank 7
2023-02-23 10:22:55,010 DEBUG TRAIN Batch 17/5400 loss 8.310643 loss_att 12.874980 loss_ctc 10.951468 loss_rnnt 6.869864 hw_loss 0.329629 lr 0.00041210 rank 2
2023-02-23 10:22:55,014 DEBUG TRAIN Batch 17/5400 loss 11.723720 loss_att 14.522024 loss_ctc 18.268887 loss_rnnt 10.088312 hw_loss 0.380733 lr 0.00041214 rank 6
2023-02-23 10:22:55,015 DEBUG TRAIN Batch 17/5400 loss 8.508200 loss_att 10.443789 loss_ctc 9.943250 loss_rnnt 7.717152 hw_loss 0.398608 lr 0.00041205 rank 3
2023-02-23 10:22:55,015 DEBUG TRAIN Batch 17/5400 loss 20.152653 loss_att 22.752050 loss_ctc 27.381954 loss_rnnt 18.442547 hw_loss 0.424346 lr 0.00041217 rank 0
2023-02-23 10:22:55,015 DEBUG TRAIN Batch 17/5400 loss 8.528682 loss_att 13.457126 loss_ctc 10.975053 loss_rnnt 7.027413 hw_loss 0.355118 lr 0.00041219 rank 1
2023-02-23 10:24:07,646 DEBUG TRAIN Batch 17/5500 loss 16.078234 loss_att 20.817904 loss_ctc 21.628654 loss_rnnt 14.191719 hw_loss 0.372233 lr 0.00041194 rank 5
2023-02-23 10:24:07,645 DEBUG TRAIN Batch 17/5500 loss 9.471553 loss_att 11.245770 loss_ctc 11.752365 loss_rnnt 8.630007 hw_loss 0.342365 lr 0.00041194 rank 7
2023-02-23 10:24:07,647 DEBUG TRAIN Batch 17/5500 loss 15.604193 loss_att 19.592171 loss_ctc 19.794273 loss_rnnt 14.042278 hw_loss 0.385578 lr 0.00041201 rank 4
2023-02-23 10:24:07,648 DEBUG TRAIN Batch 17/5500 loss 7.469944 loss_att 11.071558 loss_ctc 10.235106 loss_rnnt 6.136600 hw_loss 0.458124 lr 0.00041191 rank 3
2023-02-23 10:24:07,648 DEBUG TRAIN Batch 17/5500 loss 10.346153 loss_att 13.264822 loss_ctc 15.850402 loss_rnnt 8.777287 hw_loss 0.471060 lr 0.00041205 rank 1
2023-02-23 10:24:07,648 DEBUG TRAIN Batch 17/5500 loss 6.804446 loss_att 8.992165 loss_ctc 12.318655 loss_rnnt 5.410185 hw_loss 0.415294 lr 0.00041196 rank 2
2023-02-23 10:24:07,648 DEBUG TRAIN Batch 17/5500 loss 7.440642 loss_att 10.618757 loss_ctc 13.728998 loss_rnnt 5.742601 hw_loss 0.419945 lr 0.00041200 rank 6
2023-02-23 10:24:07,651 DEBUG TRAIN Batch 17/5500 loss 6.764616 loss_att 7.881760 loss_ctc 11.385286 loss_rnnt 5.677432 hw_loss 0.464373 lr 0.00041203 rank 0
2023-02-23 10:25:20,334 DEBUG TRAIN Batch 17/5600 loss 8.603674 loss_att 11.046058 loss_ctc 12.327421 loss_rnnt 7.373142 hw_loss 0.460415 lr 0.00041187 rank 4
2023-02-23 10:25:20,341 DEBUG TRAIN Batch 17/5600 loss 11.902045 loss_att 15.223726 loss_ctc 15.551660 loss_rnnt 10.545848 hw_loss 0.384836 lr 0.00041180 rank 7
2023-02-23 10:25:20,347 DEBUG TRAIN Batch 17/5600 loss 7.385838 loss_att 11.369638 loss_ctc 9.670441 loss_rnnt 6.080738 hw_loss 0.381987 lr 0.00041180 rank 5
2023-02-23 10:25:20,349 DEBUG TRAIN Batch 17/5600 loss 11.973068 loss_att 12.849713 loss_ctc 15.079758 loss_rnnt 11.112707 hw_loss 0.507765 lr 0.00041177 rank 3
2023-02-23 10:25:20,349 DEBUG TRAIN Batch 17/5600 loss 11.553928 loss_att 12.564959 loss_ctc 16.392151 loss_rnnt 10.458074 hw_loss 0.466034 lr 0.00041186 rank 6
2023-02-23 10:25:20,352 DEBUG TRAIN Batch 17/5600 loss 10.593215 loss_att 14.263016 loss_ctc 15.051862 loss_rnnt 9.046248 hw_loss 0.409725 lr 0.00041182 rank 2
2023-02-23 10:25:20,368 DEBUG TRAIN Batch 17/5600 loss 16.585068 loss_att 20.789551 loss_ctc 22.658838 loss_rnnt 14.700324 hw_loss 0.438770 lr 0.00041189 rank 0
2023-02-23 10:25:20,393 DEBUG TRAIN Batch 17/5600 loss 4.786471 loss_att 8.428698 loss_ctc 7.969157 loss_rnnt 3.437860 hw_loss 0.367140 lr 0.00041191 rank 1
2023-02-23 10:26:35,349 DEBUG TRAIN Batch 17/5700 loss 15.070408 loss_att 13.514314 loss_ctc 19.631357 loss_rnnt 14.461336 hw_loss 0.585306 lr 0.00041168 rank 2
2023-02-23 10:26:35,353 DEBUG TRAIN Batch 17/5700 loss 11.748727 loss_att 13.273176 loss_ctc 17.383289 loss_rnnt 10.462082 hw_loss 0.432151 lr 0.00041166 rank 7
2023-02-23 10:26:35,353 DEBUG TRAIN Batch 17/5700 loss 8.720487 loss_att 14.314856 loss_ctc 11.138003 loss_rnnt 7.117113 hw_loss 0.304058 lr 0.00041163 rank 3
2023-02-23 10:26:35,360 DEBUG TRAIN Batch 17/5700 loss 12.833002 loss_att 12.904815 loss_ctc 17.425047 loss_rnnt 11.976978 hw_loss 0.430102 lr 0.00041175 rank 0
2023-02-23 10:26:35,362 DEBUG TRAIN Batch 17/5700 loss 12.485353 loss_att 14.689548 loss_ctc 15.641800 loss_rnnt 11.334021 hw_loss 0.543062 lr 0.00041172 rank 6
2023-02-23 10:26:35,364 DEBUG TRAIN Batch 17/5700 loss 14.680289 loss_att 13.902388 loss_ctc 17.168194 loss_rnnt 14.333082 hw_loss 0.320752 lr 0.00041166 rank 5
2023-02-23 10:26:35,375 DEBUG TRAIN Batch 17/5700 loss 10.481390 loss_att 10.504436 loss_ctc 13.450565 loss_rnnt 9.771979 hw_loss 0.579208 lr 0.00041173 rank 4
2023-02-23 10:26:35,383 DEBUG TRAIN Batch 17/5700 loss 8.097183 loss_att 8.100988 loss_ctc 10.283414 loss_rnnt 7.482614 hw_loss 0.604333 lr 0.00041177 rank 1
2023-02-23 10:27:48,120 DEBUG TRAIN Batch 17/5800 loss 3.481800 loss_att 8.161140 loss_ctc 4.875030 loss_rnnt 2.167922 hw_loss 0.360462 lr 0.00041152 rank 7
2023-02-23 10:27:48,123 DEBUG TRAIN Batch 17/5800 loss 6.120048 loss_att 11.134949 loss_ctc 10.700931 loss_rnnt 4.269183 hw_loss 0.444562 lr 0.00041159 rank 4
2023-02-23 10:27:48,123 DEBUG TRAIN Batch 17/5800 loss 11.180074 loss_att 14.460165 loss_ctc 15.617508 loss_rnnt 9.741555 hw_loss 0.357830 lr 0.00041158 rank 6
2023-02-23 10:27:48,124 DEBUG TRAIN Batch 17/5800 loss 6.322156 loss_att 9.503122 loss_ctc 11.205366 loss_rnnt 4.843360 hw_loss 0.359076 lr 0.00041163 rank 1
2023-02-23 10:27:48,126 DEBUG TRAIN Batch 17/5800 loss 6.130624 loss_att 9.300762 loss_ctc 8.348124 loss_rnnt 4.958786 hw_loss 0.454021 lr 0.00041152 rank 5
2023-02-23 10:27:48,127 DEBUG TRAIN Batch 17/5800 loss 8.663070 loss_att 14.113337 loss_ctc 13.743643 loss_rnnt 6.701319 hw_loss 0.364289 lr 0.00041154 rank 2
2023-02-23 10:27:48,127 DEBUG TRAIN Batch 17/5800 loss 7.237271 loss_att 12.581562 loss_ctc 12.031313 loss_rnnt 5.383151 hw_loss 0.273856 lr 0.00041161 rank 0
2023-02-23 10:27:48,129 DEBUG TRAIN Batch 17/5800 loss 14.124649 loss_att 15.971734 loss_ctc 20.125946 loss_rnnt 12.763567 hw_loss 0.359047 lr 0.00041149 rank 3
2023-02-23 10:29:00,850 DEBUG TRAIN Batch 17/5900 loss 11.185181 loss_att 13.318943 loss_ctc 12.497437 loss_rnnt 10.368066 hw_loss 0.403864 lr 0.00041135 rank 3
2023-02-23 10:29:00,851 DEBUG TRAIN Batch 17/5900 loss 16.202938 loss_att 18.978132 loss_ctc 23.129730 loss_rnnt 14.520473 hw_loss 0.382226 lr 0.00041138 rank 7
2023-02-23 10:29:00,859 DEBUG TRAIN Batch 17/5900 loss 11.399202 loss_att 15.258531 loss_ctc 16.722900 loss_rnnt 9.755386 hw_loss 0.303984 lr 0.00041140 rank 2
2023-02-23 10:29:00,862 DEBUG TRAIN Batch 17/5900 loss 6.901996 loss_att 10.046774 loss_ctc 13.682453 loss_rnnt 5.187875 hw_loss 0.339572 lr 0.00041147 rank 0
2023-02-23 10:29:00,863 DEBUG TRAIN Batch 17/5900 loss 16.848642 loss_att 20.960455 loss_ctc 24.381874 loss_rnnt 14.788223 hw_loss 0.438045 lr 0.00041144 rank 6
2023-02-23 10:29:00,866 DEBUG TRAIN Batch 17/5900 loss 14.633375 loss_att 15.740515 loss_ctc 21.232216 loss_rnnt 13.348215 hw_loss 0.344787 lr 0.00041150 rank 1
2023-02-23 10:29:00,868 DEBUG TRAIN Batch 17/5900 loss 11.596387 loss_att 12.238248 loss_ctc 14.185214 loss_rnnt 10.869137 hw_loss 0.475690 lr 0.00041138 rank 5
2023-02-23 10:29:00,870 DEBUG TRAIN Batch 17/5900 loss 14.922465 loss_att 22.841814 loss_ctc 20.312490 loss_rnnt 12.455109 hw_loss 0.309031 lr 0.00041145 rank 4
2023-02-23 10:30:14,645 DEBUG TRAIN Batch 17/6000 loss 6.880836 loss_att 9.800213 loss_ctc 11.177979 loss_rnnt 5.505867 hw_loss 0.409014 lr 0.00041131 rank 4
2023-02-23 10:30:14,645 DEBUG TRAIN Batch 17/6000 loss 7.765731 loss_att 12.592418 loss_ctc 14.179814 loss_rnnt 5.715266 hw_loss 0.431094 lr 0.00041133 rank 0
2023-02-23 10:30:14,646 DEBUG TRAIN Batch 17/6000 loss 10.292314 loss_att 14.339243 loss_ctc 15.293365 loss_rnnt 8.612700 hw_loss 0.381415 lr 0.00041124 rank 5
2023-02-23 10:30:14,655 DEBUG TRAIN Batch 17/6000 loss 7.836967 loss_att 13.213094 loss_ctc 11.682417 loss_rnnt 6.066944 hw_loss 0.341382 lr 0.00041124 rank 7
2023-02-23 10:30:14,657 DEBUG TRAIN Batch 17/6000 loss 11.670847 loss_att 13.826694 loss_ctc 23.176409 loss_rnnt 9.461502 hw_loss 0.457687 lr 0.00041121 rank 3
2023-02-23 10:30:14,660 DEBUG TRAIN Batch 17/6000 loss 5.141493 loss_att 9.448080 loss_ctc 8.127665 loss_rnnt 3.669437 hw_loss 0.398593 lr 0.00041126 rank 2
2023-02-23 10:30:14,686 DEBUG TRAIN Batch 17/6000 loss 11.324839 loss_att 14.915348 loss_ctc 16.619041 loss_rnnt 9.669165 hw_loss 0.434400 lr 0.00041130 rank 6
2023-02-23 10:30:14,722 DEBUG TRAIN Batch 17/6000 loss 4.037675 loss_att 7.459023 loss_ctc 6.472117 loss_rnnt 2.799291 hw_loss 0.430355 lr 0.00041136 rank 1
2023-02-23 10:31:28,924 DEBUG TRAIN Batch 17/6100 loss 5.860629 loss_att 8.550399 loss_ctc 8.925296 loss_rnnt 4.720693 hw_loss 0.362549 lr 0.00041110 rank 5
2023-02-23 10:31:28,924 DEBUG TRAIN Batch 17/6100 loss 12.347949 loss_att 16.772003 loss_ctc 17.487274 loss_rnnt 10.570957 hw_loss 0.388008 lr 0.00041110 rank 7
2023-02-23 10:31:28,931 DEBUG TRAIN Batch 17/6100 loss 8.405516 loss_att 12.769893 loss_ctc 10.669481 loss_rnnt 6.996625 hw_loss 0.439037 lr 0.00041117 rank 4
2023-02-23 10:31:28,932 DEBUG TRAIN Batch 17/6100 loss 14.411827 loss_att 19.852266 loss_ctc 18.946880 loss_rnnt 12.523310 hw_loss 0.367041 lr 0.00041116 rank 6
2023-02-23 10:31:28,934 DEBUG TRAIN Batch 17/6100 loss 6.520679 loss_att 9.077303 loss_ctc 9.552317 loss_rnnt 5.376834 hw_loss 0.428067 lr 0.00041108 rank 3
2023-02-23 10:31:28,934 DEBUG TRAIN Batch 17/6100 loss 10.658255 loss_att 12.890927 loss_ctc 15.143317 loss_rnnt 9.386548 hw_loss 0.425930 lr 0.00041113 rank 2
2023-02-23 10:31:28,935 DEBUG TRAIN Batch 17/6100 loss 11.681787 loss_att 12.190598 loss_ctc 15.193637 loss_rnnt 10.906553 hw_loss 0.384801 lr 0.00041122 rank 1
2023-02-23 10:31:28,984 DEBUG TRAIN Batch 17/6100 loss 14.444026 loss_att 21.519917 loss_ctc 24.841089 loss_rnnt 11.438010 hw_loss 0.383555 lr 0.00041119 rank 0
2023-02-23 10:32:41,867 DEBUG TRAIN Batch 17/6200 loss 9.032320 loss_att 11.101501 loss_ctc 14.153968 loss_rnnt 7.700487 hw_loss 0.440831 lr 0.00041096 rank 7
2023-02-23 10:32:41,870 DEBUG TRAIN Batch 17/6200 loss 8.869328 loss_att 12.301149 loss_ctc 10.521376 loss_rnnt 7.741779 hw_loss 0.414207 lr 0.00041106 rank 0
2023-02-23 10:32:41,873 DEBUG TRAIN Batch 17/6200 loss 11.749824 loss_att 15.214470 loss_ctc 21.936695 loss_rnnt 9.458264 hw_loss 0.450714 lr 0.00041096 rank 5
2023-02-23 10:32:41,873 DEBUG TRAIN Batch 17/6200 loss 13.211026 loss_att 16.293652 loss_ctc 19.270960 loss_rnnt 11.580098 hw_loss 0.387020 lr 0.00041103 rank 6
2023-02-23 10:32:41,874 DEBUG TRAIN Batch 17/6200 loss 7.130797 loss_att 8.714444 loss_ctc 11.075109 loss_rnnt 6.019561 hw_loss 0.503623 lr 0.00041094 rank 3
2023-02-23 10:32:41,877 DEBUG TRAIN Batch 17/6200 loss 7.664312 loss_att 9.979643 loss_ctc 11.181198 loss_rnnt 6.510881 hw_loss 0.415213 lr 0.00041099 rank 2
2023-02-23 10:32:41,879 DEBUG TRAIN Batch 17/6200 loss 14.078809 loss_att 13.838835 loss_ctc 18.672991 loss_rnnt 13.296566 hw_loss 0.408151 lr 0.00041108 rank 1
2023-02-23 10:32:41,885 DEBUG TRAIN Batch 17/6200 loss 11.675008 loss_att 12.485586 loss_ctc 15.833017 loss_rnnt 10.735785 hw_loss 0.417575 lr 0.00041103 rank 4
2023-02-23 10:33:55,004 DEBUG TRAIN Batch 17/6300 loss 10.939852 loss_att 12.593503 loss_ctc 13.707612 loss_rnnt 10.113244 hw_loss 0.237829 lr 0.00041089 rank 4
2023-02-23 10:33:55,003 DEBUG TRAIN Batch 17/6300 loss 8.493509 loss_att 11.074675 loss_ctc 11.090369 loss_rnnt 7.370035 hw_loss 0.489364 lr 0.00041085 rank 2
2023-02-23 10:33:55,006 DEBUG TRAIN Batch 17/6300 loss 7.034919 loss_att 10.560427 loss_ctc 10.674701 loss_rnnt 5.646461 hw_loss 0.371347 lr 0.00041080 rank 3
2023-02-23 10:33:55,009 DEBUG TRAIN Batch 17/6300 loss 6.459622 loss_att 9.274200 loss_ctc 11.870863 loss_rnnt 4.957306 hw_loss 0.408567 lr 0.00041082 rank 5
2023-02-23 10:33:55,010 DEBUG TRAIN Batch 17/6300 loss 11.269583 loss_att 15.703595 loss_ctc 15.353909 loss_rnnt 9.575824 hw_loss 0.491961 lr 0.00041082 rank 7
2023-02-23 10:33:55,010 DEBUG TRAIN Batch 17/6300 loss 6.499801 loss_att 8.275824 loss_ctc 9.312967 loss_rnnt 5.508248 hw_loss 0.489862 lr 0.00041094 rank 1
2023-02-23 10:33:55,012 DEBUG TRAIN Batch 17/6300 loss 10.991258 loss_att 13.552773 loss_ctc 19.952131 loss_rnnt 9.003112 hw_loss 0.526986 lr 0.00041092 rank 0
2023-02-23 10:33:55,013 DEBUG TRAIN Batch 17/6300 loss 3.169686 loss_att 5.924978 loss_ctc 3.639023 loss_rnnt 2.360184 hw_loss 0.367247 lr 0.00041089 rank 6
2023-02-23 10:35:10,154 DEBUG TRAIN Batch 17/6400 loss 13.865853 loss_att 18.646889 loss_ctc 20.437923 loss_rnnt 11.824471 hw_loss 0.391685 lr 0.00041068 rank 7
2023-02-23 10:35:10,176 DEBUG TRAIN Batch 17/6400 loss 20.171377 loss_att 23.044048 loss_ctc 22.623283 loss_rnnt 19.022690 hw_loss 0.463558 lr 0.00041071 rank 2
2023-02-23 10:35:10,176 DEBUG TRAIN Batch 17/6400 loss 5.840502 loss_att 7.989508 loss_ctc 7.691677 loss_rnnt 4.942956 hw_loss 0.414227 lr 0.00041075 rank 6
2023-02-23 10:35:10,178 DEBUG TRAIN Batch 17/6400 loss 11.739588 loss_att 16.318089 loss_ctc 19.500113 loss_rnnt 9.629961 hw_loss 0.298481 lr 0.00041080 rank 1
2023-02-23 10:35:10,180 DEBUG TRAIN Batch 17/6400 loss 21.280905 loss_att 25.361177 loss_ctc 31.838865 loss_rnnt 18.875750 hw_loss 0.340075 lr 0.00041066 rank 3
2023-02-23 10:35:10,180 DEBUG TRAIN Batch 17/6400 loss 6.487026 loss_att 8.255861 loss_ctc 7.514731 loss_rnnt 5.763235 hw_loss 0.436872 lr 0.00041076 rank 4
2023-02-23 10:35:10,186 DEBUG TRAIN Batch 17/6400 loss 5.295677 loss_att 7.520419 loss_ctc 8.549201 loss_rnnt 4.165124 hw_loss 0.472126 lr 0.00041069 rank 5
2023-02-23 10:35:10,197 DEBUG TRAIN Batch 17/6400 loss 16.029482 loss_att 16.943626 loss_ctc 18.944691 loss_rnnt 15.191054 hw_loss 0.500443 lr 0.00041078 rank 0
2023-02-23 10:36:23,651 DEBUG TRAIN Batch 17/6500 loss 10.475756 loss_att 13.517156 loss_ctc 16.332729 loss_rnnt 8.862697 hw_loss 0.419716 lr 0.00041055 rank 7
2023-02-23 10:36:23,652 DEBUG TRAIN Batch 17/6500 loss 8.291109 loss_att 11.452499 loss_ctc 9.779566 loss_rnnt 7.189439 hw_loss 0.507995 lr 0.00041052 rank 3
2023-02-23 10:36:23,654 DEBUG TRAIN Batch 17/6500 loss 10.972209 loss_att 12.954750 loss_ctc 15.418757 loss_rnnt 9.790939 hw_loss 0.359789 lr 0.00041066 rank 1
2023-02-23 10:36:23,654 DEBUG TRAIN Batch 17/6500 loss 13.489133 loss_att 16.219105 loss_ctc 16.379660 loss_rnnt 12.326620 hw_loss 0.433340 lr 0.00041062 rank 4
2023-02-23 10:36:23,657 DEBUG TRAIN Batch 17/6500 loss 11.048234 loss_att 17.223597 loss_ctc 15.820655 loss_rnnt 8.945593 hw_loss 0.433586 lr 0.00041057 rank 2
2023-02-23 10:36:23,658 DEBUG TRAIN Batch 17/6500 loss 7.791017 loss_att 8.992056 loss_ctc 10.132755 loss_rnnt 7.088581 hw_loss 0.281243 lr 0.00041055 rank 5
2023-02-23 10:36:23,660 DEBUG TRAIN Batch 17/6500 loss 8.611912 loss_att 12.372648 loss_ctc 11.406216 loss_rnnt 7.260823 hw_loss 0.424441 lr 0.00041061 rank 6
2023-02-23 10:36:23,661 DEBUG TRAIN Batch 17/6500 loss 9.402042 loss_att 10.144320 loss_ctc 14.435682 loss_rnnt 8.393032 hw_loss 0.355129 lr 0.00041064 rank 0
2023-02-23 10:37:35,894 DEBUG TRAIN Batch 17/6600 loss 7.835986 loss_att 8.183568 loss_ctc 8.904493 loss_rnnt 7.408173 hw_loss 0.404679 lr 0.00041038 rank 3
2023-02-23 10:37:35,898 DEBUG TRAIN Batch 17/6600 loss 9.799219 loss_att 12.663517 loss_ctc 14.124839 loss_rnnt 8.457906 hw_loss 0.359444 lr 0.00041050 rank 0
2023-02-23 10:37:35,898 DEBUG TRAIN Batch 17/6600 loss 3.524337 loss_att 8.116850 loss_ctc 6.657454 loss_rnnt 1.950254 hw_loss 0.445934 lr 0.00041048 rank 4
2023-02-23 10:37:35,900 DEBUG TRAIN Batch 17/6600 loss 3.653394 loss_att 7.730518 loss_ctc 6.178370 loss_rnnt 2.343439 hw_loss 0.295999 lr 0.00041043 rank 2
2023-02-23 10:37:35,901 DEBUG TRAIN Batch 17/6600 loss 7.545222 loss_att 9.600913 loss_ctc 10.625773 loss_rnnt 6.503048 hw_loss 0.413054 lr 0.00041041 rank 7
2023-02-23 10:37:35,903 DEBUG TRAIN Batch 17/6600 loss 12.609234 loss_att 15.289536 loss_ctc 13.931439 loss_rnnt 11.717927 hw_loss 0.335537 lr 0.00041052 rank 1
2023-02-23 10:37:35,904 DEBUG TRAIN Batch 17/6600 loss 9.851942 loss_att 10.585999 loss_ctc 13.320574 loss_rnnt 9.007355 hw_loss 0.441172 lr 0.00041047 rank 6
2023-02-23 10:37:35,904 DEBUG TRAIN Batch 17/6600 loss 12.270579 loss_att 14.838396 loss_ctc 16.923637 loss_rnnt 10.916316 hw_loss 0.413049 lr 0.00041041 rank 5
2023-02-23 10:38:49,428 DEBUG TRAIN Batch 17/6700 loss 10.206115 loss_att 13.265248 loss_ctc 14.037372 loss_rnnt 8.862865 hw_loss 0.413604 lr 0.00041036 rank 0
2023-02-23 10:38:49,435 DEBUG TRAIN Batch 17/6700 loss 9.041293 loss_att 12.006117 loss_ctc 11.783251 loss_rnnt 7.888872 hw_loss 0.363491 lr 0.00041027 rank 7
2023-02-23 10:38:49,438 DEBUG TRAIN Batch 17/6700 loss 8.055590 loss_att 10.609440 loss_ctc 11.157784 loss_rnnt 6.942120 hw_loss 0.354513 lr 0.00041039 rank 1
2023-02-23 10:38:49,439 DEBUG TRAIN Batch 17/6700 loss 6.066366 loss_att 8.804224 loss_ctc 8.913967 loss_rnnt 4.928339 hw_loss 0.395203 lr 0.00041029 rank 2
2023-02-23 10:38:49,440 DEBUG TRAIN Batch 17/6700 loss 12.747678 loss_att 17.471628 loss_ctc 17.486578 loss_rnnt 10.940401 hw_loss 0.432435 lr 0.00041024 rank 3
2023-02-23 10:38:49,462 DEBUG TRAIN Batch 17/6700 loss 7.693319 loss_att 12.141695 loss_ctc 10.292569 loss_rnnt 6.221919 hw_loss 0.440922 lr 0.00041027 rank 5
2023-02-23 10:38:49,477 DEBUG TRAIN Batch 17/6700 loss 23.240751 loss_att 25.395220 loss_ctc 33.407768 loss_rnnt 21.243050 hw_loss 0.396011 lr 0.00041034 rank 4
2023-02-23 10:38:49,493 DEBUG TRAIN Batch 17/6700 loss 4.707057 loss_att 7.703022 loss_ctc 6.898479 loss_rnnt 3.580271 hw_loss 0.441382 lr 0.00041033 rank 6
2023-02-23 10:40:03,543 DEBUG TRAIN Batch 17/6800 loss 13.075338 loss_att 15.032207 loss_ctc 15.776762 loss_rnnt 12.108818 hw_loss 0.403043 lr 0.00041013 rank 7
2023-02-23 10:40:03,543 DEBUG TRAIN Batch 17/6800 loss 19.969376 loss_att 22.448132 loss_ctc 28.387262 loss_rnnt 18.141705 hw_loss 0.392878 lr 0.00041020 rank 4
2023-02-23 10:40:03,544 DEBUG TRAIN Batch 17/6800 loss 16.195423 loss_att 16.013321 loss_ctc 20.446156 loss_rnnt 15.424585 hw_loss 0.450926 lr 0.00041019 rank 6
2023-02-23 10:40:03,546 DEBUG TRAIN Batch 17/6800 loss 8.695419 loss_att 10.998629 loss_ctc 11.088454 loss_rnnt 7.663741 hw_loss 0.472433 lr 0.00041011 rank 3
2023-02-23 10:40:03,549 DEBUG TRAIN Batch 17/6800 loss 12.344610 loss_att 13.956020 loss_ctc 16.797020 loss_rnnt 11.206788 hw_loss 0.416036 lr 0.00041016 rank 2
2023-02-23 10:40:03,550 DEBUG TRAIN Batch 17/6800 loss 10.550441 loss_att 13.543098 loss_ctc 11.756540 loss_rnnt 9.577211 hw_loss 0.401035 lr 0.00041022 rank 0
2023-02-23 10:40:03,551 DEBUG TRAIN Batch 17/6800 loss 17.944544 loss_att 18.876575 loss_ctc 20.591255 loss_rnnt 17.205135 hw_loss 0.375201 lr 0.00041025 rank 1
2023-02-23 10:40:03,601 DEBUG TRAIN Batch 17/6800 loss 12.486402 loss_att 13.297876 loss_ctc 17.752657 loss_rnnt 11.382736 hw_loss 0.448506 lr 0.00041013 rank 5
2023-02-23 10:41:15,926 DEBUG TRAIN Batch 17/6900 loss 15.666920 loss_att 17.173666 loss_ctc 20.356730 loss_rnnt 14.549001 hw_loss 0.358614 lr 0.00041002 rank 2
2023-02-23 10:41:15,927 DEBUG TRAIN Batch 17/6900 loss 10.280804 loss_att 10.131019 loss_ctc 13.042248 loss_rnnt 9.676120 hw_loss 0.499591 lr 0.00040997 rank 3
2023-02-23 10:41:15,929 DEBUG TRAIN Batch 17/6900 loss 15.843861 loss_att 16.425838 loss_ctc 23.585148 loss_rnnt 14.453604 hw_loss 0.453169 lr 0.00040999 rank 7
2023-02-23 10:41:15,930 DEBUG TRAIN Batch 17/6900 loss 14.359089 loss_att 16.643738 loss_ctc 19.775578 loss_rnnt 12.925713 hw_loss 0.476714 lr 0.00041006 rank 4
2023-02-23 10:41:15,931 DEBUG TRAIN Batch 17/6900 loss 17.447020 loss_att 16.895794 loss_ctc 26.016676 loss_rnnt 16.148769 hw_loss 0.498515 lr 0.00041006 rank 6
2023-02-23 10:41:15,932 DEBUG TRAIN Batch 17/6900 loss 9.367144 loss_att 11.085805 loss_ctc 13.281229 loss_rnnt 8.210014 hw_loss 0.546599 lr 0.00041011 rank 1
2023-02-23 10:41:15,934 DEBUG TRAIN Batch 17/6900 loss 21.204123 loss_att 20.652506 loss_ctc 25.795738 loss_rnnt 20.485889 hw_loss 0.405641 lr 0.00040999 rank 5
2023-02-23 10:41:15,938 DEBUG TRAIN Batch 17/6900 loss 16.079357 loss_att 18.603809 loss_ctc 21.586510 loss_rnnt 14.618299 hw_loss 0.416025 lr 0.00041009 rank 0
2023-02-23 10:42:29,240 DEBUG TRAIN Batch 17/7000 loss 13.941675 loss_att 18.367920 loss_ctc 17.798687 loss_rnnt 12.351885 hw_loss 0.356760 lr 0.00040988 rank 2
2023-02-23 10:42:29,240 DEBUG TRAIN Batch 17/7000 loss 10.597553 loss_att 13.214307 loss_ctc 15.333488 loss_rnnt 9.235354 hw_loss 0.388856 lr 0.00040983 rank 3
2023-02-23 10:42:29,242 DEBUG TRAIN Batch 17/7000 loss 13.256588 loss_att 19.387394 loss_ctc 12.653258 loss_rnnt 11.937518 hw_loss 0.325037 lr 0.00040997 rank 1
2023-02-23 10:42:29,246 DEBUG TRAIN Batch 17/7000 loss 3.283168 loss_att 7.958526 loss_ctc 4.659610 loss_rnnt 1.935635 hw_loss 0.429253 lr 0.00040993 rank 4
2023-02-23 10:42:29,249 DEBUG TRAIN Batch 17/7000 loss 15.235275 loss_att 18.177481 loss_ctc 20.402246 loss_rnnt 13.755951 hw_loss 0.378661 lr 0.00040986 rank 7
2023-02-23 10:42:29,249 DEBUG TRAIN Batch 17/7000 loss 11.516319 loss_att 13.413696 loss_ctc 15.226559 loss_rnnt 10.395792 hw_loss 0.461912 lr 0.00040992 rank 6
2023-02-23 10:42:29,253 DEBUG TRAIN Batch 17/7000 loss 11.816668 loss_att 12.545014 loss_ctc 12.027581 loss_rnnt 11.406974 hw_loss 0.442319 lr 0.00040986 rank 5
2023-02-23 10:42:29,294 DEBUG TRAIN Batch 17/7000 loss 6.222776 loss_att 7.207449 loss_ctc 7.919395 loss_rnnt 5.504598 hw_loss 0.553176 lr 0.00040995 rank 0
2023-02-23 10:43:44,379 DEBUG TRAIN Batch 17/7100 loss 14.425709 loss_att 16.203278 loss_ctc 20.601332 loss_rnnt 13.010901 hw_loss 0.442272 lr 0.00040979 rank 4
2023-02-23 10:43:44,385 DEBUG TRAIN Batch 17/7100 loss 8.910732 loss_att 13.431161 loss_ctc 12.668430 loss_rnnt 7.306013 hw_loss 0.374264 lr 0.00040969 rank 3
2023-02-23 10:43:44,387 DEBUG TRAIN Batch 17/7100 loss 9.053576 loss_att 15.869410 loss_ctc 13.678452 loss_rnnt 6.898377 hw_loss 0.328839 lr 0.00040983 rank 1
2023-02-23 10:43:44,390 DEBUG TRAIN Batch 17/7100 loss 12.124923 loss_att 15.686146 loss_ctc 13.863586 loss_rnnt 10.952032 hw_loss 0.429045 lr 0.00040972 rank 7
2023-02-23 10:43:44,391 DEBUG TRAIN Batch 17/7100 loss 11.315247 loss_att 16.056255 loss_ctc 14.565166 loss_rnnt 9.753470 hw_loss 0.337972 lr 0.00040981 rank 0
2023-02-23 10:43:44,394 DEBUG TRAIN Batch 17/7100 loss 14.028248 loss_att 17.432331 loss_ctc 18.326412 loss_rnnt 12.509933 hw_loss 0.495764 lr 0.00040972 rank 5
2023-02-23 10:43:44,411 DEBUG TRAIN Batch 17/7100 loss 7.043019 loss_att 9.551729 loss_ctc 13.381165 loss_rnnt 5.528209 hw_loss 0.314965 lr 0.00040974 rank 2
2023-02-23 10:43:44,447 DEBUG TRAIN Batch 17/7100 loss 33.150730 loss_att 34.493042 loss_ctc 49.535625 loss_rnnt 30.524384 hw_loss 0.324800 lr 0.00040978 rank 6
2023-02-23 10:44:57,307 DEBUG TRAIN Batch 17/7200 loss 8.337326 loss_att 9.943905 loss_ctc 9.370077 loss_rnnt 7.671445 hw_loss 0.387871 lr 0.00040965 rank 4
2023-02-23 10:44:57,309 DEBUG TRAIN Batch 17/7200 loss 17.985382 loss_att 21.477394 loss_ctc 23.662420 loss_rnnt 16.293636 hw_loss 0.443257 lr 0.00040960 rank 2
2023-02-23 10:44:57,310 DEBUG TRAIN Batch 17/7200 loss 10.589402 loss_att 15.858618 loss_ctc 15.033177 loss_rnnt 8.732679 hw_loss 0.394456 lr 0.00040970 rank 1
2023-02-23 10:44:57,312 DEBUG TRAIN Batch 17/7200 loss 6.630066 loss_att 10.012362 loss_ctc 8.986855 loss_rnnt 5.394122 hw_loss 0.459838 lr 0.00040967 rank 0
2023-02-23 10:44:57,312 DEBUG TRAIN Batch 17/7200 loss 11.829881 loss_att 16.833122 loss_ctc 15.169903 loss_rnnt 10.186903 hw_loss 0.369363 lr 0.00040958 rank 7
2023-02-23 10:44:57,312 DEBUG TRAIN Batch 17/7200 loss 25.497967 loss_att 32.638535 loss_ctc 39.397758 loss_rnnt 22.015804 hw_loss 0.376396 lr 0.00040956 rank 3
2023-02-23 10:44:57,316 DEBUG TRAIN Batch 17/7200 loss 13.449982 loss_att 13.898500 loss_ctc 20.336777 loss_rnnt 12.175783 hw_loss 0.499227 lr 0.00040964 rank 6
2023-02-23 10:44:57,318 DEBUG TRAIN Batch 17/7200 loss 5.444767 loss_att 10.120977 loss_ctc 6.092364 loss_rnnt 4.228188 hw_loss 0.365606 lr 0.00040958 rank 5
2023-02-23 10:46:09,519 DEBUG TRAIN Batch 17/7300 loss 7.665125 loss_att 9.098444 loss_ctc 10.373024 loss_rnnt 6.865272 hw_loss 0.285255 lr 0.00040944 rank 7
2023-02-23 10:46:09,527 DEBUG TRAIN Batch 17/7300 loss 8.865842 loss_att 12.715629 loss_ctc 11.894592 loss_rnnt 7.487964 hw_loss 0.382665 lr 0.00040947 rank 2
2023-02-23 10:46:09,529 DEBUG TRAIN Batch 17/7300 loss 11.764730 loss_att 14.963973 loss_ctc 15.446780 loss_rnnt 10.431211 hw_loss 0.380120 lr 0.00040951 rank 6
2023-02-23 10:46:09,530 DEBUG TRAIN Batch 17/7300 loss 17.335730 loss_att 22.288551 loss_ctc 22.735054 loss_rnnt 15.442163 hw_loss 0.343296 lr 0.00040942 rank 3
2023-02-23 10:46:09,530 DEBUG TRAIN Batch 17/7300 loss 13.363507 loss_att 16.267275 loss_ctc 14.988146 loss_rnnt 12.357676 hw_loss 0.390860 lr 0.00040951 rank 4
2023-02-23 10:46:09,532 DEBUG TRAIN Batch 17/7300 loss 5.944382 loss_att 11.492970 loss_ctc 11.509589 loss_rnnt 3.933797 hw_loss 0.297824 lr 0.00040944 rank 5
2023-02-23 10:46:09,532 DEBUG TRAIN Batch 17/7300 loss 4.971634 loss_att 9.132705 loss_ctc 6.797655 loss_rnnt 3.668092 hw_loss 0.427235 lr 0.00040954 rank 0
2023-02-23 10:46:09,582 DEBUG TRAIN Batch 17/7300 loss 14.421851 loss_att 19.137251 loss_ctc 20.735277 loss_rnnt 12.420255 hw_loss 0.406358 lr 0.00040956 rank 1
2023-02-23 10:47:22,789 DEBUG TRAIN Batch 17/7400 loss 8.620690 loss_att 8.740455 loss_ctc 14.004662 loss_rnnt 7.671124 hw_loss 0.389531 lr 0.00040931 rank 5
2023-02-23 10:47:22,792 DEBUG TRAIN Batch 17/7400 loss 11.070103 loss_att 13.089378 loss_ctc 13.996446 loss_rnnt 10.023587 hw_loss 0.473400 lr 0.00040933 rank 2
2023-02-23 10:47:22,795 DEBUG TRAIN Batch 17/7400 loss 9.240213 loss_att 11.390392 loss_ctc 12.824236 loss_rnnt 8.126545 hw_loss 0.385806 lr 0.00040931 rank 7
2023-02-23 10:47:22,795 DEBUG TRAIN Batch 17/7400 loss 18.825979 loss_att 18.477806 loss_ctc 26.356838 loss_rnnt 17.700687 hw_loss 0.357773 lr 0.00040937 rank 6
2023-02-23 10:47:22,795 DEBUG TRAIN Batch 17/7400 loss 15.070583 loss_att 18.706722 loss_ctc 18.437057 loss_rnnt 13.674580 hw_loss 0.412336 lr 0.00040928 rank 3
2023-02-23 10:47:22,796 DEBUG TRAIN Batch 17/7400 loss 16.418076 loss_att 20.171656 loss_ctc 18.543570 loss_rnnt 15.224267 hw_loss 0.299427 lr 0.00040942 rank 1
2023-02-23 10:47:22,798 DEBUG TRAIN Batch 17/7400 loss 8.303880 loss_att 9.645817 loss_ctc 10.673028 loss_rnnt 7.457160 hw_loss 0.492087 lr 0.00040938 rank 4
2023-02-23 10:47:22,803 DEBUG TRAIN Batch 17/7400 loss 12.456526 loss_att 15.067272 loss_ctc 13.362671 loss_rnnt 11.620127 hw_loss 0.362680 lr 0.00040940 rank 0
2023-02-23 10:48:37,580 DEBUG TRAIN Batch 17/7500 loss 13.768764 loss_att 15.575193 loss_ctc 16.956047 loss_rnnt 12.743391 hw_loss 0.448345 lr 0.00040917 rank 7
2023-02-23 10:48:37,585 DEBUG TRAIN Batch 17/7500 loss 19.962868 loss_att 19.755291 loss_ctc 29.199148 loss_rnnt 18.508705 hw_loss 0.495330 lr 0.00040919 rank 2
2023-02-23 10:48:37,585 DEBUG TRAIN Batch 17/7500 loss 7.413991 loss_att 9.925123 loss_ctc 11.188682 loss_rnnt 6.207963 hw_loss 0.375955 lr 0.00040926 rank 0
2023-02-23 10:48:37,586 DEBUG TRAIN Batch 17/7500 loss 7.052310 loss_att 7.113704 loss_ctc 9.282965 loss_rnnt 6.466068 hw_loss 0.518517 lr 0.00040923 rank 6
2023-02-23 10:48:37,590 DEBUG TRAIN Batch 17/7500 loss 8.116951 loss_att 10.593639 loss_ctc 14.353831 loss_rnnt 6.583185 hw_loss 0.387831 lr 0.00040917 rank 5
2023-02-23 10:48:37,590 DEBUG TRAIN Batch 17/7500 loss 10.296796 loss_att 12.630874 loss_ctc 10.846751 loss_rnnt 9.527933 hw_loss 0.428848 lr 0.00040914 rank 3
2023-02-23 10:48:37,593 DEBUG TRAIN Batch 17/7500 loss 18.118469 loss_att 19.457033 loss_ctc 21.868439 loss_rnnt 17.057249 hw_loss 0.550336 lr 0.00040928 rank 1
2023-02-23 10:48:37,634 DEBUG TRAIN Batch 17/7500 loss 5.885178 loss_att 8.159516 loss_ctc 12.191133 loss_rnnt 4.370469 hw_loss 0.410713 lr 0.00040924 rank 4
2023-02-23 10:49:50,782 DEBUG TRAIN Batch 17/7600 loss 12.010490 loss_att 19.043388 loss_ctc 21.223080 loss_rnnt 9.141426 hw_loss 0.439012 lr 0.00040901 rank 3
2023-02-23 10:49:50,783 DEBUG TRAIN Batch 17/7600 loss 5.975431 loss_att 8.116344 loss_ctc 9.476504 loss_rnnt 4.827664 hw_loss 0.473952 lr 0.00040906 rank 2
2023-02-23 10:49:50,786 DEBUG TRAIN Batch 17/7600 loss 7.431697 loss_att 8.926045 loss_ctc 10.168251 loss_rnnt 6.551943 hw_loss 0.405018 lr 0.00040903 rank 7
2023-02-23 10:49:50,786 DEBUG TRAIN Batch 17/7600 loss 10.961182 loss_att 12.134284 loss_ctc 13.355417 loss_rnnt 10.099270 hw_loss 0.577612 lr 0.00040910 rank 4
2023-02-23 10:49:50,790 DEBUG TRAIN Batch 17/7600 loss 12.913429 loss_att 15.415190 loss_ctc 19.612789 loss_rnnt 11.338550 hw_loss 0.339899 lr 0.00040915 rank 1
2023-02-23 10:49:50,790 DEBUG TRAIN Batch 17/7600 loss 8.313494 loss_att 11.241265 loss_ctc 10.411886 loss_rnnt 7.238927 hw_loss 0.392300 lr 0.00040909 rank 6
2023-02-23 10:49:50,794 DEBUG TRAIN Batch 17/7600 loss 9.595915 loss_att 13.163078 loss_ctc 11.969018 loss_rnnt 8.308622 hw_loss 0.482711 lr 0.00040903 rank 5
2023-02-23 10:49:50,797 DEBUG TRAIN Batch 17/7600 loss 14.568328 loss_att 15.579277 loss_ctc 19.003315 loss_rnnt 13.562785 hw_loss 0.397541 lr 0.00040912 rank 0
2023-02-23 10:51:03,202 DEBUG TRAIN Batch 17/7700 loss 14.618992 loss_att 18.103977 loss_ctc 16.900267 loss_rnnt 13.423103 hw_loss 0.365104 lr 0.00040889 rank 7
2023-02-23 10:51:03,203 DEBUG TRAIN Batch 17/7700 loss 9.162634 loss_att 11.944930 loss_ctc 13.927673 loss_rnnt 7.746904 hw_loss 0.419871 lr 0.00040897 rank 4
2023-02-23 10:51:03,203 DEBUG TRAIN Batch 17/7700 loss 8.338492 loss_att 11.445374 loss_ctc 12.922675 loss_rnnt 6.904367 hw_loss 0.377858 lr 0.00040892 rank 2
2023-02-23 10:51:03,205 DEBUG TRAIN Batch 17/7700 loss 14.996673 loss_att 18.803833 loss_ctc 19.961433 loss_rnnt 13.398930 hw_loss 0.326894 lr 0.00040887 rank 3
2023-02-23 10:51:03,207 DEBUG TRAIN Batch 17/7700 loss 7.069000 loss_att 8.327899 loss_ctc 11.493683 loss_rnnt 5.967484 hw_loss 0.487087 lr 0.00040890 rank 5
2023-02-23 10:51:03,214 DEBUG TRAIN Batch 17/7700 loss 10.154263 loss_att 12.784725 loss_ctc 13.910202 loss_rnnt 8.867136 hw_loss 0.487951 lr 0.00040901 rank 1
2023-02-23 10:51:03,214 DEBUG TRAIN Batch 17/7700 loss 2.491679 loss_att 6.239095 loss_ctc 4.894443 loss_rnnt 1.241535 hw_loss 0.338049 lr 0.00040896 rank 6
2023-02-23 10:51:03,257 DEBUG TRAIN Batch 17/7700 loss 12.547873 loss_att 16.887663 loss_ctc 16.167454 loss_rnnt 10.982819 hw_loss 0.402162 lr 0.00040899 rank 0
2023-02-23 10:52:18,183 DEBUG TRAIN Batch 17/7800 loss 7.206824 loss_att 7.623487 loss_ctc 9.491356 loss_rnnt 6.446266 hw_loss 0.698666 lr 0.00040876 rank 5
2023-02-23 10:52:18,192 DEBUG TRAIN Batch 17/7800 loss 4.777535 loss_att 7.726823 loss_ctc 6.205621 loss_rnnt 3.797572 hw_loss 0.374427 lr 0.00040873 rank 3
2023-02-23 10:52:18,193 DEBUG TRAIN Batch 17/7800 loss 18.421560 loss_att 19.082306 loss_ctc 21.995832 loss_rnnt 17.605764 hw_loss 0.388271 lr 0.00040887 rank 1
2023-02-23 10:52:18,194 DEBUG TRAIN Batch 17/7800 loss 11.136615 loss_att 16.929016 loss_ctc 19.392307 loss_rnnt 8.680281 hw_loss 0.369554 lr 0.00040876 rank 7
2023-02-23 10:52:18,198 DEBUG TRAIN Batch 17/7800 loss 22.625601 loss_att 25.342880 loss_ctc 27.417446 loss_rnnt 21.227425 hw_loss 0.404639 lr 0.00040878 rank 2
2023-02-23 10:52:18,228 DEBUG TRAIN Batch 17/7800 loss 6.954559 loss_att 8.509639 loss_ctc 10.187178 loss_rnnt 5.987066 hw_loss 0.422740 lr 0.00040883 rank 4
2023-02-23 10:52:18,233 DEBUG TRAIN Batch 17/7800 loss 6.406603 loss_att 8.097502 loss_ctc 8.986246 loss_rnnt 5.562429 hw_loss 0.303828 lr 0.00040882 rank 6
2023-02-23 10:52:18,239 DEBUG TRAIN Batch 17/7800 loss 11.320520 loss_att 17.708607 loss_ctc 15.637510 loss_rnnt 9.257877 hw_loss 0.392676 lr 0.00040885 rank 0
2023-02-23 10:53:32,077 DEBUG TRAIN Batch 17/7900 loss 12.736056 loss_att 17.375143 loss_ctc 18.676725 loss_rnnt 10.845646 hw_loss 0.319693 lr 0.00040862 rank 7
2023-02-23 10:53:32,079 DEBUG TRAIN Batch 17/7900 loss 7.941451 loss_att 10.460898 loss_ctc 14.395210 loss_rnnt 6.331226 hw_loss 0.460940 lr 0.00040860 rank 3
2023-02-23 10:53:32,082 DEBUG TRAIN Batch 17/7900 loss 13.151121 loss_att 15.216300 loss_ctc 17.247543 loss_rnnt 11.986793 hw_loss 0.384570 lr 0.00040874 rank 1
2023-02-23 10:53:32,082 DEBUG TRAIN Batch 17/7900 loss 15.486385 loss_att 19.090103 loss_ctc 16.545849 loss_rnnt 14.357413 hw_loss 0.500562 lr 0.00040865 rank 2
2023-02-23 10:53:32,083 DEBUG TRAIN Batch 17/7900 loss 12.191834 loss_att 14.404694 loss_ctc 14.615705 loss_rnnt 11.186859 hw_loss 0.448539 lr 0.00040871 rank 0
2023-02-23 10:53:32,083 DEBUG TRAIN Batch 17/7900 loss 13.139565 loss_att 16.703533 loss_ctc 17.407948 loss_rnnt 11.621803 hw_loss 0.442218 lr 0.00040869 rank 4
2023-02-23 10:53:32,087 DEBUG TRAIN Batch 17/7900 loss 15.842376 loss_att 15.820673 loss_ctc 20.340847 loss_rnnt 15.056166 hw_loss 0.357667 lr 0.00040862 rank 5
2023-02-23 10:53:32,137 DEBUG TRAIN Batch 17/7900 loss 5.362035 loss_att 8.696771 loss_ctc 5.482398 loss_rnnt 4.491624 hw_loss 0.351404 lr 0.00040868 rank 6
2023-02-23 10:54:45,551 DEBUG TRAIN Batch 17/8000 loss 4.382058 loss_att 5.737056 loss_ctc 4.841372 loss_rnnt 3.784657 hw_loss 0.497173 lr 0.00040851 rank 2
2023-02-23 10:54:45,551 DEBUG TRAIN Batch 17/8000 loss 6.415676 loss_att 9.243395 loss_ctc 7.520411 loss_rnnt 5.478787 hw_loss 0.420087 lr 0.00040849 rank 7
2023-02-23 10:54:45,553 DEBUG TRAIN Batch 17/8000 loss 24.226366 loss_att 26.867706 loss_ctc 31.215437 loss_rnnt 22.580338 hw_loss 0.348532 lr 0.00040846 rank 3
2023-02-23 10:54:45,555 DEBUG TRAIN Batch 17/8000 loss 19.432991 loss_att 23.678478 loss_ctc 24.508732 loss_rnnt 17.709213 hw_loss 0.371087 lr 0.00040856 rank 4
2023-02-23 10:54:45,555 DEBUG TRAIN Batch 17/8000 loss 7.403280 loss_att 8.923744 loss_ctc 11.262320 loss_rnnt 6.342728 hw_loss 0.453601 lr 0.00040855 rank 6
2023-02-23 10:54:45,556 DEBUG TRAIN Batch 17/8000 loss 15.482923 loss_att 20.528223 loss_ctc 22.433462 loss_rnnt 13.320205 hw_loss 0.425475 lr 0.00040849 rank 5
2023-02-23 10:54:45,561 DEBUG TRAIN Batch 17/8000 loss 6.805772 loss_att 9.073013 loss_ctc 7.868710 loss_rnnt 5.983505 hw_loss 0.425801 lr 0.00040858 rank 0
2023-02-23 10:54:45,603 DEBUG TRAIN Batch 17/8000 loss 6.654762 loss_att 8.789494 loss_ctc 8.070229 loss_rnnt 5.749676 hw_loss 0.542644 lr 0.00040860 rank 1
2023-02-23 10:55:58,117 DEBUG TRAIN Batch 17/8100 loss 9.173867 loss_att 10.275526 loss_ctc 14.662403 loss_rnnt 8.002587 hw_loss 0.410893 lr 0.00040842 rank 4
2023-02-23 10:55:58,126 DEBUG TRAIN Batch 17/8100 loss 6.819220 loss_att 10.071720 loss_ctc 8.480172 loss_rnnt 5.706945 hw_loss 0.450590 lr 0.00040837 rank 2
2023-02-23 10:55:58,128 DEBUG TRAIN Batch 17/8100 loss 4.301095 loss_att 6.220454 loss_ctc 4.606632 loss_rnnt 3.651184 hw_loss 0.422438 lr 0.00040832 rank 3
2023-02-23 10:55:58,129 DEBUG TRAIN Batch 17/8100 loss 9.851478 loss_att 12.030654 loss_ctc 11.387959 loss_rnnt 8.992155 hw_loss 0.409918 lr 0.00040835 rank 7
2023-02-23 10:55:58,130 DEBUG TRAIN Batch 17/8100 loss 6.676864 loss_att 8.350578 loss_ctc 8.410671 loss_rnnt 5.892118 hw_loss 0.410302 lr 0.00040846 rank 1
2023-02-23 10:55:58,130 DEBUG TRAIN Batch 17/8100 loss 10.113488 loss_att 12.266515 loss_ctc 14.408764 loss_rnnt 8.876743 hw_loss 0.437694 lr 0.00040844 rank 0
2023-02-23 10:55:58,134 DEBUG TRAIN Batch 17/8100 loss 10.065831 loss_att 13.070766 loss_ctc 11.383479 loss_rnnt 9.069756 hw_loss 0.411380 lr 0.00040841 rank 6
2023-02-23 10:55:58,177 DEBUG TRAIN Batch 17/8100 loss 11.447515 loss_att 13.809896 loss_ctc 15.549268 loss_rnnt 10.224238 hw_loss 0.382311 lr 0.00040835 rank 5
2023-02-23 10:57:11,666 DEBUG TRAIN Batch 17/8200 loss 11.589135 loss_att 12.600477 loss_ctc 14.318807 loss_rnnt 10.762877 hw_loss 0.487564 lr 0.00040821 rank 7
2023-02-23 10:57:11,670 DEBUG TRAIN Batch 17/8200 loss 9.620399 loss_att 10.248891 loss_ctc 10.932702 loss_rnnt 9.038549 hw_loss 0.527207 lr 0.00040819 rank 3
2023-02-23 10:57:11,672 DEBUG TRAIN Batch 17/8200 loss 18.774677 loss_att 21.440544 loss_ctc 23.727819 loss_rnnt 17.375376 hw_loss 0.385703 lr 0.00040824 rank 2
2023-02-23 10:57:11,674 DEBUG TRAIN Batch 17/8200 loss 9.878013 loss_att 13.088667 loss_ctc 16.256689 loss_rnnt 8.173156 hw_loss 0.397942 lr 0.00040828 rank 6
2023-02-23 10:57:11,675 DEBUG TRAIN Batch 17/8200 loss 15.588711 loss_att 17.996559 loss_ctc 20.008011 loss_rnnt 14.269846 hw_loss 0.465101 lr 0.00040828 rank 4
2023-02-23 10:57:11,678 DEBUG TRAIN Batch 17/8200 loss 16.897680 loss_att 19.029327 loss_ctc 25.234364 loss_rnnt 15.132715 hw_loss 0.425769 lr 0.00040831 rank 0
2023-02-23 10:57:11,679 DEBUG TRAIN Batch 17/8200 loss 18.150646 loss_att 17.355453 loss_ctc 22.803015 loss_rnnt 17.381958 hw_loss 0.576394 lr 0.00040833 rank 1
2023-02-23 10:57:11,686 DEBUG TRAIN Batch 17/8200 loss 10.003557 loss_att 12.417645 loss_ctc 14.843386 loss_rnnt 8.667908 hw_loss 0.389100 lr 0.00040821 rank 5
2023-02-23 10:58:24,333 DEBUG TRAIN Batch 17/8300 loss 16.527031 loss_att 23.243412 loss_ctc 18.367563 loss_rnnt 14.740794 hw_loss 0.370419 lr 0.00040810 rank 2
2023-02-23 10:58:24,335 DEBUG TRAIN Batch 17/8300 loss 9.357651 loss_att 9.417007 loss_ctc 12.389600 loss_rnnt 8.579877 hw_loss 0.678078 lr 0.00040808 rank 7
2023-02-23 10:58:24,338 DEBUG TRAIN Batch 17/8300 loss 12.317304 loss_att 12.410632 loss_ctc 15.658751 loss_rnnt 11.647680 hw_loss 0.385185 lr 0.00040819 rank 1
2023-02-23 10:58:24,338 DEBUG TRAIN Batch 17/8300 loss 6.303123 loss_att 9.071943 loss_ctc 7.171514 loss_rnnt 5.397691 hw_loss 0.442280 lr 0.00040805 rank 3
2023-02-23 10:58:24,343 DEBUG TRAIN Batch 17/8300 loss 7.007922 loss_att 9.865016 loss_ctc 12.824774 loss_rnnt 5.471754 hw_loss 0.354693 lr 0.00040815 rank 4
2023-02-23 10:58:24,345 DEBUG TRAIN Batch 17/8300 loss 6.514562 loss_att 8.512204 loss_ctc 11.770834 loss_rnnt 5.178268 hw_loss 0.442366 lr 0.00040808 rank 5
2023-02-23 10:58:24,347 DEBUG TRAIN Batch 17/8300 loss 13.895462 loss_att 18.617813 loss_ctc 16.476324 loss_rnnt 12.381766 hw_loss 0.422082 lr 0.00040814 rank 6
2023-02-23 10:58:24,383 DEBUG TRAIN Batch 17/8300 loss 15.867519 loss_att 16.753691 loss_ctc 18.950907 loss_rnnt 14.956594 hw_loss 0.604823 lr 0.00040817 rank 0
2023-02-23 10:59:14,564 DEBUG CV Batch 17/0 loss 2.153443 loss_att 1.951893 loss_ctc 2.776120 loss_rnnt 1.670365 hw_loss 0.825684 history loss 2.073686 rank 6
2023-02-23 10:59:14,574 DEBUG CV Batch 17/0 loss 2.153443 loss_att 1.951893 loss_ctc 2.776120 loss_rnnt 1.670365 hw_loss 0.825684 history loss 2.073686 rank 2
2023-02-23 10:59:14,574 DEBUG CV Batch 17/0 loss 2.153443 loss_att 1.951893 loss_ctc 2.776120 loss_rnnt 1.670365 hw_loss 0.825684 history loss 2.073686 rank 7
2023-02-23 10:59:14,576 DEBUG CV Batch 17/0 loss 2.153443 loss_att 1.951893 loss_ctc 2.776120 loss_rnnt 1.670365 hw_loss 0.825684 history loss 2.073686 rank 4
2023-02-23 10:59:14,577 DEBUG CV Batch 17/0 loss 2.153443 loss_att 1.951893 loss_ctc 2.776120 loss_rnnt 1.670365 hw_loss 0.825684 history loss 2.073686 rank 3
2023-02-23 10:59:14,580 DEBUG CV Batch 17/0 loss 2.153443 loss_att 1.951893 loss_ctc 2.776120 loss_rnnt 1.670365 hw_loss 0.825684 history loss 2.073686 rank 5
2023-02-23 10:59:14,602 DEBUG CV Batch 17/0 loss 2.153443 loss_att 1.951893 loss_ctc 2.776120 loss_rnnt 1.670365 hw_loss 0.825684 history loss 2.073686 rank 1
2023-02-23 10:59:14,604 DEBUG CV Batch 17/0 loss 2.153443 loss_att 1.951893 loss_ctc 2.776120 loss_rnnt 1.670365 hw_loss 0.825684 history loss 2.073686 rank 0
2023-02-23 10:59:25,925 DEBUG CV Batch 17/100 loss 6.770480 loss_att 7.483598 loss_ctc 9.469498 loss_rnnt 6.023230 hw_loss 0.458919 history loss 3.700495 rank 6
2023-02-23 10:59:26,000 DEBUG CV Batch 17/100 loss 6.770480 loss_att 7.483598 loss_ctc 9.469498 loss_rnnt 6.023230 hw_loss 0.458919 history loss 3.700495 rank 1
2023-02-23 10:59:26,041 DEBUG CV Batch 17/100 loss 6.770480 loss_att 7.483598 loss_ctc 9.469498 loss_rnnt 6.023230 hw_loss 0.458919 history loss 3.700495 rank 5
2023-02-23 10:59:26,088 DEBUG CV Batch 17/100 loss 6.770480 loss_att 7.483598 loss_ctc 9.469498 loss_rnnt 6.023230 hw_loss 0.458919 history loss 3.700495 rank 4
2023-02-23 10:59:26,171 DEBUG CV Batch 17/100 loss 6.770480 loss_att 7.483598 loss_ctc 9.469498 loss_rnnt 6.023230 hw_loss 0.458919 history loss 3.700495 rank 0
2023-02-23 10:59:26,356 DEBUG CV Batch 17/100 loss 6.770480 loss_att 7.483598 loss_ctc 9.469498 loss_rnnt 6.023230 hw_loss 0.458919 history loss 3.700495 rank 7
2023-02-23 10:59:26,511 DEBUG CV Batch 17/100 loss 6.770480 loss_att 7.483598 loss_ctc 9.469498 loss_rnnt 6.023230 hw_loss 0.458919 history loss 3.700495 rank 3
2023-02-23 10:59:26,552 DEBUG CV Batch 17/100 loss 6.770480 loss_att 7.483598 loss_ctc 9.469498 loss_rnnt 6.023230 hw_loss 0.458919 history loss 3.700495 rank 2
2023-02-23 10:59:39,749 DEBUG CV Batch 17/200 loss 4.092196 loss_att 13.905075 loss_ctc 3.839034 loss_rnnt 2.008040 hw_loss 0.291252 history loss 4.374277 rank 6
2023-02-23 10:59:40,052 DEBUG CV Batch 17/200 loss 4.092196 loss_att 13.905075 loss_ctc 3.839034 loss_rnnt 2.008040 hw_loss 0.291252 history loss 4.374277 rank 5
2023-02-23 10:59:40,088 DEBUG CV Batch 17/200 loss 4.092196 loss_att 13.905075 loss_ctc 3.839034 loss_rnnt 2.008040 hw_loss 0.291252 history loss 4.374277 rank 7
2023-02-23 10:59:40,121 DEBUG CV Batch 17/200 loss 4.092196 loss_att 13.905075 loss_ctc 3.839034 loss_rnnt 2.008040 hw_loss 0.291252 history loss 4.374277 rank 1
2023-02-23 10:59:40,271 DEBUG CV Batch 17/200 loss 4.092196 loss_att 13.905075 loss_ctc 3.839034 loss_rnnt 2.008040 hw_loss 0.291252 history loss 4.374277 rank 4
2023-02-23 10:59:40,422 DEBUG CV Batch 17/200 loss 4.092196 loss_att 13.905075 loss_ctc 3.839034 loss_rnnt 2.008040 hw_loss 0.291252 history loss 4.374277 rank 0
2023-02-23 10:59:40,453 DEBUG CV Batch 17/200 loss 4.092196 loss_att 13.905075 loss_ctc 3.839034 loss_rnnt 2.008040 hw_loss 0.291252 history loss 4.374277 rank 3
2023-02-23 10:59:40,506 DEBUG CV Batch 17/200 loss 4.092196 loss_att 13.905075 loss_ctc 3.839034 loss_rnnt 2.008040 hw_loss 0.291252 history loss 4.374277 rank 2
2023-02-23 10:59:51,645 DEBUG CV Batch 17/300 loss 6.021790 loss_att 5.762461 loss_ctc 7.794794 loss_rnnt 5.555054 hw_loss 0.529126 history loss 4.487408 rank 6
2023-02-23 10:59:51,951 DEBUG CV Batch 17/300 loss 6.021790 loss_att 5.762461 loss_ctc 7.794794 loss_rnnt 5.555054 hw_loss 0.529126 history loss 4.487408 rank 5
2023-02-23 10:59:52,082 DEBUG CV Batch 17/300 loss 6.021790 loss_att 5.762461 loss_ctc 7.794794 loss_rnnt 5.555054 hw_loss 0.529126 history loss 4.487408 rank 1
2023-02-23 10:59:52,179 DEBUG CV Batch 17/300 loss 6.021790 loss_att 5.762461 loss_ctc 7.794794 loss_rnnt 5.555054 hw_loss 0.529126 history loss 4.487408 rank 4
2023-02-23 10:59:52,472 DEBUG CV Batch 17/300 loss 6.021790 loss_att 5.762461 loss_ctc 7.794794 loss_rnnt 5.555054 hw_loss 0.529126 history loss 4.487408 rank 0
2023-02-23 10:59:52,531 DEBUG CV Batch 17/300 loss 6.021790 loss_att 5.762461 loss_ctc 7.794794 loss_rnnt 5.555054 hw_loss 0.529126 history loss 4.487408 rank 7
2023-02-23 10:59:52,982 DEBUG CV Batch 17/300 loss 6.021790 loss_att 5.762461 loss_ctc 7.794794 loss_rnnt 5.555054 hw_loss 0.529126 history loss 4.487408 rank 3
2023-02-23 10:59:53,164 DEBUG CV Batch 17/300 loss 6.021790 loss_att 5.762461 loss_ctc 7.794794 loss_rnnt 5.555054 hw_loss 0.529126 history loss 4.487408 rank 2
2023-02-23 11:00:03,506 DEBUG CV Batch 17/400 loss 24.376461 loss_att 109.608780 loss_ctc 7.062495 loss_rnnt 9.501506 hw_loss 0.256910 history loss 5.479417 rank 6
2023-02-23 11:00:03,946 DEBUG CV Batch 17/400 loss 24.376461 loss_att 109.608780 loss_ctc 7.062495 loss_rnnt 9.501506 hw_loss 0.256910 history loss 5.479417 rank 4
2023-02-23 11:00:03,984 DEBUG CV Batch 17/400 loss 24.376461 loss_att 109.608780 loss_ctc 7.062495 loss_rnnt 9.501506 hw_loss 0.256910 history loss 5.479417 rank 1
2023-02-23 11:00:04,094 DEBUG CV Batch 17/400 loss 24.376461 loss_att 109.608780 loss_ctc 7.062495 loss_rnnt 9.501506 hw_loss 0.256910 history loss 5.479417 rank 5
2023-02-23 11:00:04,573 DEBUG CV Batch 17/400 loss 24.376461 loss_att 109.608780 loss_ctc 7.062495 loss_rnnt 9.501506 hw_loss 0.256910 history loss 5.479417 rank 0
2023-02-23 11:00:04,958 DEBUG CV Batch 17/400 loss 24.376461 loss_att 109.608780 loss_ctc 7.062495 loss_rnnt 9.501506 hw_loss 0.256910 history loss 5.479417 rank 7
2023-02-23 11:00:05,604 DEBUG CV Batch 17/400 loss 24.376461 loss_att 109.608780 loss_ctc 7.062495 loss_rnnt 9.501506 hw_loss 0.256910 history loss 5.479417 rank 3
2023-02-23 11:00:05,930 DEBUG CV Batch 17/400 loss 24.376461 loss_att 109.608780 loss_ctc 7.062495 loss_rnnt 9.501506 hw_loss 0.256910 history loss 5.479417 rank 2
2023-02-23 11:00:13,793 DEBUG CV Batch 17/500 loss 5.226021 loss_att 5.928328 loss_ctc 7.002585 loss_rnnt 4.601505 hw_loss 0.463462 history loss 6.322586 rank 6
2023-02-23 11:00:14,161 DEBUG CV Batch 17/500 loss 5.226021 loss_att 5.928328 loss_ctc 7.002585 loss_rnnt 4.601505 hw_loss 0.463462 history loss 6.322586 rank 4
2023-02-23 11:00:14,442 DEBUG CV Batch 17/500 loss 5.226021 loss_att 5.928328 loss_ctc 7.002585 loss_rnnt 4.601505 hw_loss 0.463462 history loss 6.322586 rank 1
2023-02-23 11:00:14,619 DEBUG CV Batch 17/500 loss 5.226021 loss_att 5.928328 loss_ctc 7.002585 loss_rnnt 4.601505 hw_loss 0.463462 history loss 6.322586 rank 5
2023-02-23 11:00:15,219 DEBUG CV Batch 17/500 loss 5.226021 loss_att 5.928328 loss_ctc 7.002585 loss_rnnt 4.601505 hw_loss 0.463462 history loss 6.322586 rank 0
2023-02-23 11:00:15,819 DEBUG CV Batch 17/500 loss 5.226021 loss_att 5.928328 loss_ctc 7.002585 loss_rnnt 4.601505 hw_loss 0.463462 history loss 6.322586 rank 7
2023-02-23 11:00:16,612 DEBUG CV Batch 17/500 loss 5.226021 loss_att 5.928328 loss_ctc 7.002585 loss_rnnt 4.601505 hw_loss 0.463462 history loss 6.322586 rank 3
2023-02-23 11:00:17,115 DEBUG CV Batch 17/500 loss 5.226021 loss_att 5.928328 loss_ctc 7.002585 loss_rnnt 4.601505 hw_loss 0.463462 history loss 6.322586 rank 2
2023-02-23 11:00:25,866 DEBUG CV Batch 17/600 loss 6.655173 loss_att 6.712769 loss_ctc 8.612301 loss_rnnt 6.031356 hw_loss 0.658776 history loss 7.238226 rank 6
2023-02-23 11:00:26,516 DEBUG CV Batch 17/600 loss 6.655173 loss_att 6.712769 loss_ctc 8.612301 loss_rnnt 6.031356 hw_loss 0.658776 history loss 7.238226 rank 4
2023-02-23 11:00:26,613 DEBUG CV Batch 17/600 loss 6.655173 loss_att 6.712769 loss_ctc 8.612301 loss_rnnt 6.031356 hw_loss 0.658777 history loss 7.238226 rank 1
2023-02-23 11:00:26,826 DEBUG CV Batch 17/600 loss 6.655173 loss_att 6.712769 loss_ctc 8.612301 loss_rnnt 6.031356 hw_loss 0.658776 history loss 7.238226 rank 5
2023-02-23 11:00:27,546 DEBUG CV Batch 17/600 loss 6.655173 loss_att 6.712769 loss_ctc 8.612301 loss_rnnt 6.031356 hw_loss 0.658777 history loss 7.238226 rank 0
2023-02-23 11:00:28,270 DEBUG CV Batch 17/600 loss 6.655173 loss_att 6.712769 loss_ctc 8.612301 loss_rnnt 6.031356 hw_loss 0.658777 history loss 7.238226 rank 7
2023-02-23 11:00:29,078 DEBUG CV Batch 17/600 loss 6.655173 loss_att 6.712769 loss_ctc 8.612301 loss_rnnt 6.031356 hw_loss 0.658777 history loss 7.238226 rank 3
2023-02-23 11:00:29,708 DEBUG CV Batch 17/600 loss 6.655173 loss_att 6.712769 loss_ctc 8.612301 loss_rnnt 6.031356 hw_loss 0.658777 history loss 7.238226 rank 2
2023-02-23 11:00:37,913 DEBUG CV Batch 17/700 loss 14.137892 loss_att 47.794163 loss_ctc 16.313763 loss_rnnt 6.951508 hw_loss 0.309401 history loss 7.926199 rank 6
2023-02-23 11:00:37,925 DEBUG CV Batch 17/700 loss 14.137892 loss_att 47.794163 loss_ctc 16.313763 loss_rnnt 6.951508 hw_loss 0.309401 history loss 7.926199 rank 4
2023-02-23 11:00:38,798 DEBUG CV Batch 17/700 loss 14.137892 loss_att 47.794163 loss_ctc 16.313763 loss_rnnt 6.951508 hw_loss 0.309401 history loss 7.926199 rank 1
2023-02-23 11:00:38,898 DEBUG CV Batch 17/700 loss 14.137892 loss_att 47.794163 loss_ctc 16.313763 loss_rnnt 6.951508 hw_loss 0.309401 history loss 7.926199 rank 5
2023-02-23 11:00:39,086 DEBUG CV Batch 17/700 loss 14.137892 loss_att 47.794163 loss_ctc 16.313763 loss_rnnt 6.951508 hw_loss 0.309401 history loss 7.926199 rank 0
2023-02-23 11:00:39,971 DEBUG CV Batch 17/700 loss 14.137892 loss_att 47.794163 loss_ctc 16.313763 loss_rnnt 6.951508 hw_loss 0.309401 history loss 7.926199 rank 7
2023-02-23 11:00:40,943 DEBUG CV Batch 17/700 loss 14.137892 loss_att 47.794163 loss_ctc 16.313763 loss_rnnt 6.951508 hw_loss 0.309401 history loss 7.926199 rank 3
2023-02-23 11:00:41,775 DEBUG CV Batch 17/700 loss 14.137892 loss_att 47.794163 loss_ctc 16.313763 loss_rnnt 6.951508 hw_loss 0.309401 history loss 7.926199 rank 2
2023-02-23 11:00:49,202 DEBUG CV Batch 17/800 loss 12.185065 loss_att 11.999039 loss_ctc 18.593435 loss_rnnt 11.070064 hw_loss 0.558296 history loss 7.356815 rank 4
2023-02-23 11:00:49,955 DEBUG CV Batch 17/800 loss 12.185065 loss_att 11.999039 loss_ctc 18.593435 loss_rnnt 11.070064 hw_loss 0.558296 history loss 7.356815 rank 6
2023-02-23 11:00:50,696 DEBUG CV Batch 17/800 loss 12.185065 loss_att 11.999039 loss_ctc 18.593435 loss_rnnt 11.070064 hw_loss 0.558296 history loss 7.356815 rank 0
2023-02-23 11:00:51,053 DEBUG CV Batch 17/800 loss 12.185065 loss_att 11.999039 loss_ctc 18.593435 loss_rnnt 11.070064 hw_loss 0.558296 history loss 7.356815 rank 1
2023-02-23 11:00:51,395 DEBUG CV Batch 17/800 loss 12.185065 loss_att 11.999039 loss_ctc 18.593435 loss_rnnt 11.070064 hw_loss 0.558296 history loss 7.356815 rank 5
2023-02-23 11:00:51,423 DEBUG CV Batch 17/800 loss 12.185065 loss_att 11.999039 loss_ctc 18.593435 loss_rnnt 11.070064 hw_loss 0.558296 history loss 7.356815 rank 7
2023-02-23 11:00:52,595 DEBUG CV Batch 17/800 loss 12.185065 loss_att 11.999039 loss_ctc 18.593435 loss_rnnt 11.070064 hw_loss 0.558296 history loss 7.356815 rank 3
2023-02-23 11:00:53,561 DEBUG CV Batch 17/800 loss 12.185065 loss_att 11.999039 loss_ctc 18.593435 loss_rnnt 11.070064 hw_loss 0.558296 history loss 7.356815 rank 2
2023-02-23 11:01:03,015 DEBUG CV Batch 17/900 loss 14.039289 loss_att 26.023390 loss_ctc 23.651789 loss_rnnt 10.227426 hw_loss 0.250083 history loss 7.158865 rank 4
2023-02-23 11:01:03,653 DEBUG CV Batch 17/900 loss 14.039289 loss_att 26.023390 loss_ctc 23.651789 loss_rnnt 10.227426 hw_loss 0.250083 history loss 7.158865 rank 6
2023-02-23 11:01:04,649 DEBUG CV Batch 17/900 loss 14.039289 loss_att 26.023390 loss_ctc 23.651789 loss_rnnt 10.227426 hw_loss 0.250083 history loss 7.158865 rank 0
2023-02-23 11:01:04,800 DEBUG CV Batch 17/900 loss 14.039289 loss_att 26.023390 loss_ctc 23.651789 loss_rnnt 10.227426 hw_loss 0.250083 history loss 7.158865 rank 5
2023-02-23 11:01:04,969 DEBUG CV Batch 17/900 loss 14.039289 loss_att 26.023390 loss_ctc 23.651789 loss_rnnt 10.227426 hw_loss 0.250083 history loss 7.158865 rank 1
2023-02-23 11:01:05,123 DEBUG CV Batch 17/900 loss 14.039289 loss_att 26.023390 loss_ctc 23.651789 loss_rnnt 10.227426 hw_loss 0.250083 history loss 7.158865 rank 7
2023-02-23 11:01:06,367 DEBUG CV Batch 17/900 loss 14.039289 loss_att 26.023390 loss_ctc 23.651789 loss_rnnt 10.227426 hw_loss 0.250083 history loss 7.158865 rank 3
2023-02-23 11:01:07,592 DEBUG CV Batch 17/900 loss 14.039289 loss_att 26.023390 loss_ctc 23.651789 loss_rnnt 10.227426 hw_loss 0.250083 history loss 7.158865 rank 2
2023-02-23 11:01:15,039 DEBUG CV Batch 17/1000 loss 4.627795 loss_att 5.037570 loss_ctc 6.716847 loss_rnnt 3.944964 hw_loss 0.604378 history loss 6.912340 rank 4
2023-02-23 11:01:15,804 DEBUG CV Batch 17/1000 loss 4.627795 loss_att 5.037570 loss_ctc 6.716847 loss_rnnt 3.944964 hw_loss 0.604378 history loss 6.912340 rank 6
2023-02-23 11:01:17,043 DEBUG CV Batch 17/1000 loss 4.627795 loss_att 5.037570 loss_ctc 6.716847 loss_rnnt 3.944964 hw_loss 0.604378 history loss 6.912340 rank 0
2023-02-23 11:01:17,055 DEBUG CV Batch 17/1000 loss 4.627795 loss_att 5.037570 loss_ctc 6.716847 loss_rnnt 3.944964 hw_loss 0.604378 history loss 6.912340 rank 1
2023-02-23 11:01:17,372 DEBUG CV Batch 17/1000 loss 4.627795 loss_att 5.037570 loss_ctc 6.716847 loss_rnnt 3.944964 hw_loss 0.604378 history loss 6.912340 rank 5
2023-02-23 11:01:17,799 DEBUG CV Batch 17/1000 loss 4.627795 loss_att 5.037570 loss_ctc 6.716847 loss_rnnt 3.944964 hw_loss 0.604378 history loss 6.912340 rank 7
2023-02-23 11:01:19,269 DEBUG CV Batch 17/1000 loss 4.627795 loss_att 5.037570 loss_ctc 6.716847 loss_rnnt 3.944964 hw_loss 0.604378 history loss 6.912340 rank 3
2023-02-23 11:01:20,489 DEBUG CV Batch 17/1000 loss 4.627795 loss_att 5.037570 loss_ctc 6.716847 loss_rnnt 3.944964 hw_loss 0.604378 history loss 6.912340 rank 2
2023-02-23 11:01:27,369 DEBUG CV Batch 17/1100 loss 7.957228 loss_att 6.913013 loss_ctc 9.461273 loss_rnnt 7.566110 hw_loss 0.748915 history loss 6.892225 rank 4
2023-02-23 11:01:27,607 DEBUG CV Batch 17/1100 loss 7.957228 loss_att 6.913013 loss_ctc 9.461273 loss_rnnt 7.566110 hw_loss 0.748915 history loss 6.892225 rank 6
2023-02-23 11:01:28,827 DEBUG CV Batch 17/1100 loss 7.957228 loss_att 6.913013 loss_ctc 9.461273 loss_rnnt 7.566110 hw_loss 0.748915 history loss 6.892225 rank 1
2023-02-23 11:01:29,155 DEBUG CV Batch 17/1100 loss 7.957228 loss_att 6.913013 loss_ctc 9.461273 loss_rnnt 7.566110 hw_loss 0.748915 history loss 6.892225 rank 0
2023-02-23 11:01:29,162 DEBUG CV Batch 17/1100 loss 7.957228 loss_att 6.913013 loss_ctc 9.461273 loss_rnnt 7.566110 hw_loss 0.748915 history loss 6.892225 rank 5
2023-02-23 11:01:30,174 DEBUG CV Batch 17/1100 loss 7.957228 loss_att 6.913013 loss_ctc 9.461273 loss_rnnt 7.566110 hw_loss 0.748915 history loss 6.892225 rank 7
2023-02-23 11:01:31,705 DEBUG CV Batch 17/1100 loss 7.957228 loss_att 6.913013 loss_ctc 9.461273 loss_rnnt 7.566110 hw_loss 0.748915 history loss 6.892225 rank 3
2023-02-23 11:01:33,285 DEBUG CV Batch 17/1100 loss 7.957228 loss_att 6.913013 loss_ctc 9.461273 loss_rnnt 7.566110 hw_loss 0.748915 history loss 6.892225 rank 2
2023-02-23 11:01:37,590 DEBUG CV Batch 17/1200 loss 6.938660 loss_att 8.088686 loss_ctc 8.192721 loss_rnnt 6.310823 hw_loss 0.432419 history loss 7.255468 rank 4
2023-02-23 11:01:38,130 DEBUG CV Batch 17/1200 loss 6.938660 loss_att 8.088686 loss_ctc 8.192721 loss_rnnt 6.310823 hw_loss 0.432419 history loss 7.255468 rank 6
2023-02-23 11:01:39,283 DEBUG CV Batch 17/1200 loss 6.938660 loss_att 8.088686 loss_ctc 8.192721 loss_rnnt 6.310823 hw_loss 0.432419 history loss 7.255468 rank 1
2023-02-23 11:01:39,597 DEBUG CV Batch 17/1200 loss 6.938660 loss_att 8.088686 loss_ctc 8.192721 loss_rnnt 6.310823 hw_loss 0.432419 history loss 7.255468 rank 5
2023-02-23 11:01:39,695 DEBUG CV Batch 17/1200 loss 6.938660 loss_att 8.088686 loss_ctc 8.192721 loss_rnnt 6.310823 hw_loss 0.432419 history loss 7.255468 rank 0
2023-02-23 11:01:41,174 DEBUG CV Batch 17/1200 loss 6.938660 loss_att 8.088686 loss_ctc 8.192721 loss_rnnt 6.310823 hw_loss 0.432419 history loss 7.255468 rank 7
2023-02-23 11:01:42,844 DEBUG CV Batch 17/1200 loss 6.938660 loss_att 8.088686 loss_ctc 8.192721 loss_rnnt 6.310823 hw_loss 0.432419 history loss 7.255468 rank 3
2023-02-23 11:01:44,597 DEBUG CV Batch 17/1200 loss 6.938660 loss_att 8.088686 loss_ctc 8.192721 loss_rnnt 6.310823 hw_loss 0.432419 history loss 7.255468 rank 2
2023-02-23 11:01:49,582 DEBUG CV Batch 17/1300 loss 4.490735 loss_att 4.321174 loss_ctc 6.171959 loss_rnnt 4.008362 hw_loss 0.547729 history loss 7.551472 rank 4
2023-02-23 11:01:50,080 DEBUG CV Batch 17/1300 loss 4.490735 loss_att 4.321174 loss_ctc 6.171959 loss_rnnt 4.008362 hw_loss 0.547729 history loss 7.551472 rank 6
2023-02-23 11:01:51,374 DEBUG CV Batch 17/1300 loss 4.490735 loss_att 4.321174 loss_ctc 6.171959 loss_rnnt 4.008362 hw_loss 0.547729 history loss 7.551472 rank 1
2023-02-23 11:01:51,570 DEBUG CV Batch 17/1300 loss 4.490735 loss_att 4.321174 loss_ctc 6.171959 loss_rnnt 4.008362 hw_loss 0.547729 history loss 7.551472 rank 5
2023-02-23 11:01:52,003 DEBUG CV Batch 17/1300 loss 4.490735 loss_att 4.321174 loss_ctc 6.171959 loss_rnnt 4.008362 hw_loss 0.547729 history loss 7.551472 rank 0
2023-02-23 11:01:53,524 DEBUG CV Batch 17/1300 loss 4.490735 loss_att 4.321174 loss_ctc 6.171959 loss_rnnt 4.008362 hw_loss 0.547729 history loss 7.551472 rank 7
2023-02-23 11:01:55,405 DEBUG CV Batch 17/1300 loss 4.490735 loss_att 4.321174 loss_ctc 6.171959 loss_rnnt 4.008362 hw_loss 0.547729 history loss 7.551472 rank 3
2023-02-23 11:01:57,294 DEBUG CV Batch 17/1300 loss 4.490735 loss_att 4.321174 loss_ctc 6.171959 loss_rnnt 4.008362 hw_loss 0.547729 history loss 7.551472 rank 2
2023-02-23 11:02:00,643 DEBUG CV Batch 17/1400 loss 7.752416 loss_att 24.318609 loss_ctc 6.108196 loss_rnnt 4.463426 hw_loss 0.365589 history loss 7.891817 rank 4
2023-02-23 11:02:01,852 DEBUG CV Batch 17/1400 loss 7.752416 loss_att 24.318609 loss_ctc 6.108196 loss_rnnt 4.463426 hw_loss 0.365589 history loss 7.891817 rank 6
2023-02-23 11:02:02,807 DEBUG CV Batch 17/1400 loss 7.752416 loss_att 24.318609 loss_ctc 6.108196 loss_rnnt 4.463426 hw_loss 0.365589 history loss 7.891817 rank 5
2023-02-23 11:02:02,968 DEBUG CV Batch 17/1400 loss 7.752416 loss_att 24.318609 loss_ctc 6.108196 loss_rnnt 4.463426 hw_loss 0.365589 history loss 7.891817 rank 1
2023-02-23 11:02:03,301 DEBUG CV Batch 17/1400 loss 7.752416 loss_att 24.318609 loss_ctc 6.108196 loss_rnnt 4.463426 hw_loss 0.365589 history loss 7.891817 rank 0
2023-02-23 11:02:05,330 DEBUG CV Batch 17/1400 loss 7.752416 loss_att 24.318609 loss_ctc 6.108196 loss_rnnt 4.463426 hw_loss 0.365589 history loss 7.891817 rank 7
2023-02-23 11:02:07,189 DEBUG CV Batch 17/1400 loss 7.752416 loss_att 24.318609 loss_ctc 6.108196 loss_rnnt 4.463426 hw_loss 0.365589 history loss 7.891817 rank 3
2023-02-23 11:02:09,330 DEBUG CV Batch 17/1400 loss 7.752416 loss_att 24.318609 loss_ctc 6.108196 loss_rnnt 4.463426 hw_loss 0.365589 history loss 7.891817 rank 2
2023-02-23 11:02:12,703 DEBUG CV Batch 17/1500 loss 7.419352 loss_att 8.065071 loss_ctc 7.136597 loss_rnnt 7.118686 hw_loss 0.392292 history loss 7.705218 rank 4
2023-02-23 11:02:13,958 DEBUG CV Batch 17/1500 loss 7.419352 loss_att 8.065071 loss_ctc 7.136597 loss_rnnt 7.118686 hw_loss 0.392292 history loss 7.705218 rank 6
2023-02-23 11:02:15,233 DEBUG CV Batch 17/1500 loss 7.419352 loss_att 8.065071 loss_ctc 7.136597 loss_rnnt 7.118686 hw_loss 0.392292 history loss 7.705218 rank 0
2023-02-23 11:02:15,343 DEBUG CV Batch 17/1500 loss 7.419352 loss_att 8.065071 loss_ctc 7.136597 loss_rnnt 7.118686 hw_loss 0.392292 history loss 7.705218 rank 1
2023-02-23 11:02:15,630 DEBUG CV Batch 17/1500 loss 7.419352 loss_att 8.065071 loss_ctc 7.136597 loss_rnnt 7.118686 hw_loss 0.392292 history loss 7.705218 rank 5
2023-02-23 11:02:17,095 DEBUG CV Batch 17/1500 loss 7.419352 loss_att 8.065071 loss_ctc 7.136597 loss_rnnt 7.118686 hw_loss 0.392292 history loss 7.705218 rank 7
2023-02-23 11:02:19,338 DEBUG CV Batch 17/1500 loss 7.419352 loss_att 8.065071 loss_ctc 7.136597 loss_rnnt 7.118686 hw_loss 0.392292 history loss 7.705218 rank 3
2023-02-23 11:02:21,600 DEBUG CV Batch 17/1500 loss 7.419352 loss_att 8.065071 loss_ctc 7.136597 loss_rnnt 7.118686 hw_loss 0.392292 history loss 7.705218 rank 2
2023-02-23 11:02:26,086 DEBUG CV Batch 17/1600 loss 8.064613 loss_att 12.944803 loss_ctc 11.981422 loss_rnnt 6.410732 hw_loss 0.291754 history loss 7.633751 rank 4
2023-02-23 11:02:27,910 DEBUG CV Batch 17/1600 loss 8.064613 loss_att 12.944803 loss_ctc 11.981422 loss_rnnt 6.410732 hw_loss 0.291754 history loss 7.633751 rank 6
2023-02-23 11:02:28,803 DEBUG CV Batch 17/1600 loss 8.064613 loss_att 12.944803 loss_ctc 11.981422 loss_rnnt 6.410732 hw_loss 0.291754 history loss 7.633751 rank 0
2023-02-23 11:02:28,992 DEBUG CV Batch 17/1600 loss 8.064613 loss_att 12.944803 loss_ctc 11.981422 loss_rnnt 6.410732 hw_loss 0.291754 history loss 7.633751 rank 1
2023-02-23 11:02:29,355 DEBUG CV Batch 17/1600 loss 8.064613 loss_att 12.944803 loss_ctc 11.981422 loss_rnnt 6.410732 hw_loss 0.291754 history loss 7.633751 rank 5
2023-02-23 11:02:30,764 DEBUG CV Batch 17/1600 loss 8.064613 loss_att 12.944803 loss_ctc 11.981422 loss_rnnt 6.410732 hw_loss 0.291754 history loss 7.633751 rank 7
2023-02-23 11:02:32,871 DEBUG CV Batch 17/1600 loss 8.064613 loss_att 12.944803 loss_ctc 11.981422 loss_rnnt 6.410732 hw_loss 0.291754 history loss 7.633751 rank 3
2023-02-23 11:02:35,330 DEBUG CV Batch 17/1600 loss 8.064613 loss_att 12.944803 loss_ctc 11.981422 loss_rnnt 6.410732 hw_loss 0.291754 history loss 7.633751 rank 2
2023-02-23 11:02:38,500 DEBUG CV Batch 17/1700 loss 9.034197 loss_att 8.087077 loss_ctc 13.640911 loss_rnnt 8.344030 hw_loss 0.497554 history loss 7.531134 rank 4
2023-02-23 11:02:40,349 DEBUG CV Batch 17/1700 loss 9.034197 loss_att 8.087077 loss_ctc 13.640911 loss_rnnt 8.344030 hw_loss 0.497554 history loss 7.531134 rank 6
2023-02-23 11:02:41,300 DEBUG CV Batch 17/1700 loss 9.034197 loss_att 8.087077 loss_ctc 13.640911 loss_rnnt 8.344030 hw_loss 0.497554 history loss 7.531134 rank 1
2023-02-23 11:02:41,723 DEBUG CV Batch 17/1700 loss 9.034197 loss_att 8.087077 loss_ctc 13.640911 loss_rnnt 8.344030 hw_loss 0.497554 history loss 7.531134 rank 5
2023-02-23 11:02:41,874 DEBUG CV Batch 17/1700 loss 9.034197 loss_att 8.087077 loss_ctc 13.640911 loss_rnnt 8.344030 hw_loss 0.497554 history loss 7.531134 rank 0
2023-02-23 11:02:43,761 DEBUG CV Batch 17/1700 loss 9.034197 loss_att 8.087077 loss_ctc 13.640911 loss_rnnt 8.344030 hw_loss 0.497554 history loss 7.531134 rank 7
2023-02-23 11:02:45,620 DEBUG CV Batch 17/1700 loss 9.034197 loss_att 8.087077 loss_ctc 13.640911 loss_rnnt 8.344030 hw_loss 0.497554 history loss 7.531134 rank 3
2023-02-23 11:02:47,818 INFO Epoch 17 CV info cv_loss 7.493017477914422
2023-02-23 11:02:47,818 INFO Epoch 18 TRAIN info lr 0.000408087807481702
2023-02-23 11:02:47,820 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:02:48,114 DEBUG CV Batch 17/1700 loss 9.034197 loss_att 8.087077 loss_ctc 13.640911 loss_rnnt 8.344030 hw_loss 0.497554 history loss 7.531134 rank 2
2023-02-23 11:02:49,679 INFO Epoch 17 CV info cv_loss 7.493017478657434
2023-02-23 11:02:49,680 INFO Epoch 18 TRAIN info lr 0.0004080565489345392
2023-02-23 11:02:49,681 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:02:50,534 INFO Epoch 17 CV info cv_loss 7.493017475517401
2023-02-23 11:02:50,534 INFO Epoch 18 TRAIN info lr 0.000408117713684427
2023-02-23 11:02:50,540 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:02:51,165 INFO Epoch 17 CV info cv_loss 7.493017479060168
2023-02-23 11:02:51,165 INFO Epoch 18 TRAIN info lr 0.00040806334365984826
2023-02-23 11:02:51,167 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:02:51,483 INFO Epoch 17 CV info cv_loss 7.493017479958244
2023-02-23 11:02:51,483 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/17.pt
2023-02-23 11:02:54,856 INFO Epoch 17 CV info cv_loss 7.4930174769431215
2023-02-23 11:02:54,857 INFO Epoch 17 CV info cv_loss 7.493017477515995
2023-02-23 11:02:54,857 INFO Epoch 18 TRAIN info lr 0.0004079913368365684
2023-02-23 11:02:54,857 INFO Epoch 18 TRAIN info lr 0.0004080280147942555
2023-02-23 11:02:54,862 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:02:54,862 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:02:55,089 INFO Epoch 18 TRAIN info lr 0.0004081231518822604
2023-02-23 11:02:55,094 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:02:57,418 INFO Epoch 17 CV info cv_loss 7.493017478250392
2023-02-23 11:02:57,418 INFO Epoch 18 TRAIN info lr 0.0004080470368893069
2023-02-23 11:02:57,420 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 11:03:57,114 DEBUG TRAIN Batch 18/0 loss 5.505599 loss_att 5.906509 loss_ctc 7.272513 loss_rnnt 4.864783 hw_loss 0.609463 lr 0.00040799 rank 3
2023-02-23 11:03:57,128 DEBUG TRAIN Batch 18/0 loss 8.016997 loss_att 7.552114 loss_ctc 9.468292 loss_rnnt 7.630639 hw_loss 0.535930 lr 0.00040809 rank 4
2023-02-23 11:03:57,129 DEBUG TRAIN Batch 18/0 loss 10.200968 loss_att 9.644430 loss_ctc 12.151385 loss_rnnt 9.698512 hw_loss 0.663201 lr 0.00040803 rank 7
2023-02-23 11:03:57,133 DEBUG TRAIN Batch 18/0 loss 9.734835 loss_att 8.921502 loss_ctc 11.458141 loss_rnnt 9.299410 hw_loss 0.690593 lr 0.00040812 rank 1
2023-02-23 11:03:57,134 DEBUG TRAIN Batch 18/0 loss 10.305902 loss_att 9.664642 loss_ctc 12.939160 loss_rnnt 9.754125 hw_loss 0.616738 lr 0.00040806 rank 6
2023-02-23 11:03:57,140 DEBUG TRAIN Batch 18/0 loss 10.750580 loss_att 10.171378 loss_ctc 11.469738 loss_rnnt 10.460575 hw_loss 0.581168 lr 0.00040806 rank 5
2023-02-23 11:03:57,151 DEBUG TRAIN Batch 18/0 loss 10.114694 loss_att 9.369995 loss_ctc 11.968228 loss_rnnt 9.642481 hw_loss 0.701276 lr 0.00040805 rank 2
2023-02-23 11:03:57,156 DEBUG TRAIN Batch 18/0 loss 13.467484 loss_att 12.397902 loss_ctc 14.234517 loss_rnnt 13.245998 hw_loss 0.624621 lr 0.00040812 rank 0
2023-02-23 11:05:09,310 DEBUG TRAIN Batch 18/100 loss 4.948907 loss_att 7.320226 loss_ctc 4.818426 loss_rnnt 4.296719 hw_loss 0.366229 lr 0.00040795 rank 4
2023-02-23 11:05:09,310 DEBUG TRAIN Batch 18/100 loss 6.776901 loss_att 8.677343 loss_ctc 7.036546 loss_rnnt 6.174772 hw_loss 0.351416 lr 0.00040789 rank 7
2023-02-23 11:05:09,314 DEBUG TRAIN Batch 18/100 loss 8.696041 loss_att 14.111397 loss_ctc 16.478384 loss_rnnt 6.381906 hw_loss 0.362658 lr 0.00040785 rank 3
2023-02-23 11:05:09,314 DEBUG TRAIN Batch 18/100 loss 16.752369 loss_att 17.595757 loss_ctc 19.224033 loss_rnnt 16.042463 hw_loss 0.396884 lr 0.00040798 rank 1
2023-02-23 11:05:09,317 DEBUG TRAIN Batch 18/100 loss 11.091930 loss_att 16.322489 loss_ctc 13.974197 loss_rnnt 9.467890 hw_loss 0.363049 lr 0.00040799 rank 0
2023-02-23 11:05:09,317 DEBUG TRAIN Batch 18/100 loss 12.227601 loss_att 16.063259 loss_ctc 13.018045 loss_rnnt 11.173488 hw_loss 0.340481 lr 0.00040791 rank 2
2023-02-23 11:05:09,318 DEBUG TRAIN Batch 18/100 loss 22.494814 loss_att 22.329153 loss_ctc 31.932821 loss_rnnt 21.077232 hw_loss 0.360587 lr 0.00040792 rank 6
2023-02-23 11:05:09,332 DEBUG TRAIN Batch 18/100 loss 4.861199 loss_att 8.520218 loss_ctc 7.246480 loss_rnnt 3.593536 hw_loss 0.408416 lr 0.00040793 rank 5
2023-02-23 11:06:21,761 DEBUG TRAIN Batch 18/200 loss 6.186634 loss_att 10.209116 loss_ctc 7.058915 loss_rnnt 5.091282 hw_loss 0.327283 lr 0.00040776 rank 7
2023-02-23 11:06:21,763 DEBUG TRAIN Batch 18/200 loss 9.180271 loss_att 13.534633 loss_ctc 13.436671 loss_rnnt 7.487356 hw_loss 0.477230 lr 0.00040778 rank 6
2023-02-23 11:06:21,766 DEBUG TRAIN Batch 18/200 loss 6.176291 loss_att 10.059226 loss_ctc 7.352981 loss_rnnt 5.021336 hw_loss 0.415269 lr 0.00040784 rank 1
2023-02-23 11:06:21,767 DEBUG TRAIN Batch 18/200 loss 7.170877 loss_att 16.453379 loss_ctc 8.716131 loss_rnnt 4.888666 hw_loss 0.411892 lr 0.00040779 rank 5
2023-02-23 11:06:21,768 DEBUG TRAIN Batch 18/200 loss 9.070698 loss_att 10.529550 loss_ctc 12.425594 loss_rnnt 8.083827 hw_loss 0.464591 lr 0.00040772 rank 3
2023-02-23 11:06:21,768 DEBUG TRAIN Batch 18/200 loss 4.953309 loss_att 8.333736 loss_ctc 7.310308 loss_rnnt 3.727093 hw_loss 0.442242 lr 0.00040777 rank 2
2023-02-23 11:06:21,771 DEBUG TRAIN Batch 18/200 loss 10.800995 loss_att 11.599167 loss_ctc 13.804197 loss_rnnt 10.024855 hw_loss 0.405147 lr 0.00040785 rank 0
2023-02-23 11:06:21,811 DEBUG TRAIN Batch 18/200 loss 6.633368 loss_att 8.853508 loss_ctc 13.475390 loss_rnnt 5.041382 hw_loss 0.441916 lr 0.00040781 rank 4
2023-02-23 11:07:35,465 DEBUG TRAIN Batch 18/300 loss 16.458679 loss_att 18.629534 loss_ctc 26.148697 loss_rnnt 14.532406 hw_loss 0.375185 lr 0.00040762 rank 7
2023-02-23 11:07:35,468 DEBUG TRAIN Batch 18/300 loss 12.786895 loss_att 13.848351 loss_ctc 11.279033 loss_rnnt 12.557476 hw_loss 0.409081 lr 0.00040765 rank 6
2023-02-23 11:07:35,469 DEBUG TRAIN Batch 18/300 loss 9.859370 loss_att 13.690522 loss_ctc 12.843541 loss_rnnt 8.501284 hw_loss 0.363687 lr 0.00040771 rank 1
2023-02-23 11:07:35,469 DEBUG TRAIN Batch 18/300 loss 11.971189 loss_att 12.998949 loss_ctc 15.230178 loss_rnnt 11.027201 hw_loss 0.569821 lr 0.00040765 rank 5
2023-02-23 11:07:35,472 DEBUG TRAIN Batch 18/300 loss 4.654474 loss_att 6.060675 loss_ctc 6.950488 loss_rnnt 3.840714 hw_loss 0.424473 lr 0.00040764 rank 2
2023-02-23 11:07:35,474 DEBUG TRAIN Batch 18/300 loss 16.039654 loss_att 19.899460 loss_ctc 20.828430 loss_rnnt 14.427174 hw_loss 0.378781 lr 0.00040758 rank 3
2023-02-23 11:07:35,479 DEBUG TRAIN Batch 18/300 loss 5.491656 loss_att 6.824015 loss_ctc 6.919072 loss_rnnt 4.785874 hw_loss 0.466852 lr 0.00040771 rank 0
2023-02-23 11:07:35,483 DEBUG TRAIN Batch 18/300 loss 1.543130 loss_att 4.006037 loss_ctc 2.057645 loss_rnnt 0.788300 hw_loss 0.363087 lr 0.00040768 rank 4
2023-02-23 11:08:48,748 DEBUG TRAIN Batch 18/400 loss 9.921796 loss_att 13.982041 loss_ctc 11.432547 loss_rnnt 8.717197 hw_loss 0.358340 lr 0.00040748 rank 7
2023-02-23 11:08:48,752 DEBUG TRAIN Batch 18/400 loss 16.561304 loss_att 18.236614 loss_ctc 19.314165 loss_rnnt 15.656478 hw_loss 0.380091 lr 0.00040745 rank 3
2023-02-23 11:08:48,753 DEBUG TRAIN Batch 18/400 loss 4.890347 loss_att 8.659365 loss_ctc 8.452476 loss_rnnt 3.440701 hw_loss 0.414175 lr 0.00040750 rank 2
2023-02-23 11:08:48,754 DEBUG TRAIN Batch 18/400 loss 9.490740 loss_att 11.940436 loss_ctc 15.445924 loss_rnnt 8.002234 hw_loss 0.383515 lr 0.00040754 rank 4
2023-02-23 11:08:48,755 DEBUG TRAIN Batch 18/400 loss 5.208694 loss_att 8.021038 loss_ctc 9.486160 loss_rnnt 3.899332 hw_loss 0.331060 lr 0.00040757 rank 1
2023-02-23 11:08:48,757 DEBUG TRAIN Batch 18/400 loss 13.994903 loss_att 15.821501 loss_ctc 20.566111 loss_rnnt 12.549618 hw_loss 0.382132 lr 0.00040752 rank 5
2023-02-23 11:08:48,760 DEBUG TRAIN Batch 18/400 loss 13.526072 loss_att 13.981819 loss_ctc 13.679975 loss_rnnt 13.199829 hw_loss 0.402324 lr 0.00040758 rank 0
2023-02-23 11:08:48,761 DEBUG TRAIN Batch 18/400 loss 9.865232 loss_att 15.376027 loss_ctc 15.474224 loss_rnnt 7.799502 hw_loss 0.404445 lr 0.00040751 rank 6
2023-02-23 11:10:01,299 DEBUG TRAIN Batch 18/500 loss 13.438586 loss_att 20.146252 loss_ctc 22.527637 loss_rnnt 10.642242 hw_loss 0.455506 lr 0.00040737 rank 2
2023-02-23 11:10:01,300 DEBUG TRAIN Batch 18/500 loss 19.669401 loss_att 21.346680 loss_ctc 24.836594 loss_rnnt 18.422241 hw_loss 0.417647 lr 0.00040735 rank 7
2023-02-23 11:10:01,301 DEBUG TRAIN Batch 18/500 loss 13.585909 loss_att 13.770308 loss_ctc 14.582037 loss_rnnt 13.201307 hw_loss 0.402949 lr 0.00040744 rank 1
2023-02-23 11:10:01,303 DEBUG TRAIN Batch 18/500 loss 11.305172 loss_att 11.028505 loss_ctc 14.888451 loss_rnnt 10.694284 hw_loss 0.353343 lr 0.00040738 rank 6
2023-02-23 11:10:01,304 DEBUG TRAIN Batch 18/500 loss 11.018228 loss_att 11.225229 loss_ctc 13.020555 loss_rnnt 10.454747 hw_loss 0.478319 lr 0.00040744 rank 0
2023-02-23 11:10:01,306 DEBUG TRAIN Batch 18/500 loss 10.114082 loss_att 12.227564 loss_ctc 13.757827 loss_rnnt 8.971890 hw_loss 0.438116 lr 0.00040741 rank 4
2023-02-23 11:10:01,307 DEBUG TRAIN Batch 18/500 loss 8.901680 loss_att 12.789938 loss_ctc 12.686843 loss_rnnt 7.378767 hw_loss 0.451073 lr 0.00040731 rank 3
2023-02-23 11:10:01,352 DEBUG TRAIN Batch 18/500 loss 6.005352 loss_att 8.039929 loss_ctc 7.507285 loss_rnnt 5.160929 hw_loss 0.444844 lr 0.00040738 rank 5
2023-02-23 11:11:13,981 DEBUG TRAIN Batch 18/600 loss 11.429849 loss_att 14.052424 loss_ctc 16.821272 loss_rnnt 9.941936 hw_loss 0.458517 lr 0.00040731 rank 0
2023-02-23 11:11:13,982 DEBUG TRAIN Batch 18/600 loss 7.860226 loss_att 9.304911 loss_ctc 9.304719 loss_rnnt 7.086740 hw_loss 0.547405 lr 0.00040721 rank 7
2023-02-23 11:11:13,982 DEBUG TRAIN Batch 18/600 loss 10.864511 loss_att 10.422015 loss_ctc 14.872364 loss_rnnt 10.107128 hw_loss 0.584069 lr 0.00040718 rank 3
2023-02-23 11:11:13,983 DEBUG TRAIN Batch 18/600 loss 10.654882 loss_att 11.071722 loss_ctc 12.666584 loss_rnnt 9.992781 hw_loss 0.582200 lr 0.00040727 rank 4
2023-02-23 11:11:13,984 DEBUG TRAIN Batch 18/600 loss 15.692313 loss_att 15.246782 loss_ctc 20.827902 loss_rnnt 14.846916 hw_loss 0.468294 lr 0.00040725 rank 5
2023-02-23 11:11:13,984 DEBUG TRAIN Batch 18/600 loss 10.972866 loss_att 11.881601 loss_ctc 13.810997 loss_rnnt 10.074895 hw_loss 0.633387 lr 0.00040730 rank 1
2023-02-23 11:11:13,988 DEBUG TRAIN Batch 18/600 loss 11.627641 loss_att 10.687160 loss_ctc 14.308839 loss_rnnt 11.073521 hw_loss 0.721354 lr 0.00040723 rank 2
2023-02-23 11:11:13,993 DEBUG TRAIN Batch 18/600 loss 14.891044 loss_att 13.638329 loss_ctc 14.673335 loss_rnnt 14.902495 hw_loss 0.502722 lr 0.00040724 rank 6
2023-02-23 11:12:29,122 DEBUG TRAIN Batch 18/700 loss 6.557355 loss_att 11.093385 loss_ctc 8.296312 loss_rnnt 5.232688 hw_loss 0.348002 lr 0.00040710 rank 2
2023-02-23 11:12:29,125 DEBUG TRAIN Batch 18/700 loss 9.391676 loss_att 13.412052 loss_ctc 16.355190 loss_rnnt 7.398326 hw_loss 0.489011 lr 0.00040711 rank 6
2023-02-23 11:12:29,128 DEBUG TRAIN Batch 18/700 loss 3.142609 loss_att 10.763103 loss_ctc 5.546083 loss_rnnt 1.120683 hw_loss 0.332558 lr 0.00040714 rank 4
2023-02-23 11:12:29,143 DEBUG TRAIN Batch 18/700 loss 13.233186 loss_att 13.673080 loss_ctc 18.704487 loss_rnnt 12.181923 hw_loss 0.438331 lr 0.00040708 rank 7
2023-02-23 11:12:29,146 DEBUG TRAIN Batch 18/700 loss 11.549302 loss_att 13.913007 loss_ctc 16.933388 loss_rnnt 10.175732 hw_loss 0.343035 lr 0.00040717 rank 0
2023-02-23 11:12:29,153 DEBUG TRAIN Batch 18/700 loss 9.955276 loss_att 11.954298 loss_ctc 13.432022 loss_rnnt 8.898948 hw_loss 0.361792 lr 0.00040717 rank 1
2023-02-23 11:12:29,156 DEBUG TRAIN Batch 18/700 loss 3.771067 loss_att 4.946974 loss_ctc 4.482883 loss_rnnt 3.268730 hw_loss 0.322963 lr 0.00040704 rank 3
2023-02-23 11:12:29,158 DEBUG TRAIN Batch 18/700 loss 14.028223 loss_att 15.931894 loss_ctc 17.241398 loss_rnnt 12.980001 hw_loss 0.448246 lr 0.00040711 rank 5
2023-02-23 11:13:41,808 DEBUG TRAIN Batch 18/800 loss 10.267522 loss_att 15.440428 loss_ctc 16.890362 loss_rnnt 8.147684 hw_loss 0.379146 lr 0.00040691 rank 3
2023-02-23 11:13:41,808 DEBUG TRAIN Batch 18/800 loss 7.867026 loss_att 9.959002 loss_ctc 9.559114 loss_rnnt 7.023692 hw_loss 0.373738 lr 0.00040696 rank 2
2023-02-23 11:13:41,809 DEBUG TRAIN Batch 18/800 loss 11.851710 loss_att 12.508880 loss_ctc 13.199946 loss_rnnt 11.367576 hw_loss 0.324253 lr 0.00040694 rank 7
2023-02-23 11:13:41,809 DEBUG TRAIN Batch 18/800 loss 14.509642 loss_att 17.542440 loss_ctc 28.867813 loss_rnnt 11.811121 hw_loss 0.332879 lr 0.00040700 rank 4
2023-02-23 11:13:41,811 DEBUG TRAIN Batch 18/800 loss 6.958533 loss_att 10.286842 loss_ctc 11.734637 loss_rnnt 5.451892 hw_loss 0.382809 lr 0.00040698 rank 5
2023-02-23 11:13:41,814 DEBUG TRAIN Batch 18/800 loss 6.134715 loss_att 7.602078 loss_ctc 7.797062 loss_rnnt 5.438613 hw_loss 0.339343 lr 0.00040704 rank 0
2023-02-23 11:13:41,817 DEBUG TRAIN Batch 18/800 loss 13.383091 loss_att 16.237558 loss_ctc 16.116634 loss_rnnt 12.252326 hw_loss 0.366373 lr 0.00040697 rank 6
2023-02-23 11:13:41,854 DEBUG TRAIN Batch 18/800 loss 9.968516 loss_att 12.638526 loss_ctc 14.877197 loss_rnnt 8.557417 hw_loss 0.417388 lr 0.00040703 rank 1
2023-02-23 11:14:53,390 DEBUG TRAIN Batch 18/900 loss 6.250993 loss_att 9.917885 loss_ctc 7.736549 loss_rnnt 5.116998 hw_loss 0.379767 lr 0.00040681 rank 7
2023-02-23 11:14:53,398 DEBUG TRAIN Batch 18/900 loss 5.088099 loss_att 8.563349 loss_ctc 8.505840 loss_rnnt 3.682947 hw_loss 0.477005 lr 0.00040690 rank 0
2023-02-23 11:14:53,399 DEBUG TRAIN Batch 18/900 loss 6.394586 loss_att 10.487951 loss_ctc 9.631429 loss_rnnt 4.960758 hw_loss 0.344206 lr 0.00040684 rank 5
2023-02-23 11:14:53,400 DEBUG TRAIN Batch 18/900 loss 5.373451 loss_att 7.155779 loss_ctc 7.597943 loss_rnnt 4.523671 hw_loss 0.368840 lr 0.00040687 rank 4
2023-02-23 11:14:53,403 DEBUG TRAIN Batch 18/900 loss 5.538288 loss_att 8.987891 loss_ctc 8.193314 loss_rnnt 4.256016 hw_loss 0.446902 lr 0.00040677 rank 3
2023-02-23 11:14:53,404 DEBUG TRAIN Batch 18/900 loss 8.562302 loss_att 12.493822 loss_ctc 17.093212 loss_rnnt 6.449794 hw_loss 0.353907 lr 0.00040684 rank 6
2023-02-23 11:14:53,406 DEBUG TRAIN Batch 18/900 loss 17.432671 loss_att 19.330635 loss_ctc 21.395824 loss_rnnt 16.345253 hw_loss 0.336378 lr 0.00040683 rank 2
2023-02-23 11:14:53,447 DEBUG TRAIN Batch 18/900 loss 10.192393 loss_att 11.126638 loss_ctc 10.955774 loss_rnnt 9.620222 hw_loss 0.531633 lr 0.00040690 rank 1
2023-02-23 11:16:06,580 DEBUG TRAIN Batch 18/1000 loss 13.520572 loss_att 14.720979 loss_ctc 20.590197 loss_rnnt 12.108034 hw_loss 0.430946 lr 0.00040671 rank 5
2023-02-23 11:16:06,588 DEBUG TRAIN Batch 18/1000 loss 3.038533 loss_att 5.016189 loss_ctc 3.600904 loss_rnnt 2.248730 hw_loss 0.598665 lr 0.00040669 rank 2
2023-02-23 11:16:06,597 DEBUG TRAIN Batch 18/1000 loss 7.667521 loss_att 10.463789 loss_ctc 10.543045 loss_rnnt 6.503268 hw_loss 0.415492 lr 0.00040664 rank 3
2023-02-23 11:16:06,599 DEBUG TRAIN Batch 18/1000 loss 10.812744 loss_att 14.877080 loss_ctc 15.296007 loss_rnnt 9.191923 hw_loss 0.394096 lr 0.00040673 rank 4
2023-02-23 11:16:06,601 DEBUG TRAIN Batch 18/1000 loss 6.511656 loss_att 9.122963 loss_ctc 9.530354 loss_rnnt 5.388712 hw_loss 0.371603 lr 0.00040667 rank 7
2023-02-23 11:16:06,618 DEBUG TRAIN Batch 18/1000 loss 28.838072 loss_att 33.774529 loss_ctc 42.480675 loss_rnnt 25.804054 hw_loss 0.426959 lr 0.00040676 rank 1
2023-02-23 11:16:06,640 DEBUG TRAIN Batch 18/1000 loss 11.378058 loss_att 14.281570 loss_ctc 16.474808 loss_rnnt 9.912708 hw_loss 0.384527 lr 0.00040677 rank 0
2023-02-23 11:16:06,647 DEBUG TRAIN Batch 18/1000 loss 9.453957 loss_att 13.078620 loss_ctc 14.062866 loss_rnnt 7.909438 hw_loss 0.384495 lr 0.00040670 rank 6
2023-02-23 11:17:21,323 DEBUG TRAIN Batch 18/1100 loss 16.534365 loss_att 18.039440 loss_ctc 18.738499 loss_rnnt 15.724578 hw_loss 0.402911 lr 0.00040660 rank 4
2023-02-23 11:17:21,324 DEBUG TRAIN Batch 18/1100 loss 22.922028 loss_att 25.142828 loss_ctc 29.787281 loss_rnnt 21.314438 hw_loss 0.465118 lr 0.00040654 rank 7
2023-02-23 11:17:21,326 DEBUG TRAIN Batch 18/1100 loss 7.588866 loss_att 8.545496 loss_ctc 8.824777 loss_rnnt 7.013852 hw_loss 0.410435 lr 0.00040650 rank 3
2023-02-23 11:17:21,331 DEBUG TRAIN Batch 18/1100 loss 6.812746 loss_att 7.956816 loss_ctc 10.535793 loss_rnnt 5.867973 hw_loss 0.411660 lr 0.00040663 rank 1
2023-02-23 11:17:21,332 DEBUG TRAIN Batch 18/1100 loss 9.124027 loss_att 12.617307 loss_ctc 15.749041 loss_rnnt 7.318335 hw_loss 0.419440 lr 0.00040656 rank 2
2023-02-23 11:17:21,332 DEBUG TRAIN Batch 18/1100 loss 8.665966 loss_att 12.268028 loss_ctc 12.075846 loss_rnnt 7.256035 hw_loss 0.440377 lr 0.00040658 rank 5
2023-02-23 11:17:21,332 DEBUG TRAIN Batch 18/1100 loss 7.341904 loss_att 10.390695 loss_ctc 9.067632 loss_rnnt 6.274430 hw_loss 0.426784 lr 0.00040657 rank 6
2023-02-23 11:17:21,337 DEBUG TRAIN Batch 18/1100 loss 12.117319 loss_att 13.861161 loss_ctc 19.514236 loss_rnnt 10.553226 hw_loss 0.429507 lr 0.00040663 rank 0
2023-02-23 11:18:33,671 DEBUG TRAIN Batch 18/1200 loss 10.575072 loss_att 10.628945 loss_ctc 13.944296 loss_rnnt 9.810427 hw_loss 0.571202 lr 0.00040641 rank 7
2023-02-23 11:18:33,672 DEBUG TRAIN Batch 18/1200 loss 10.928666 loss_att 12.619983 loss_ctc 13.016235 loss_rnnt 10.078909 hw_loss 0.437156 lr 0.00040637 rank 3
2023-02-23 11:18:33,678 DEBUG TRAIN Batch 18/1200 loss 12.273852 loss_att 12.800606 loss_ctc 17.947205 loss_rnnt 11.118244 hw_loss 0.550894 lr 0.00040642 rank 2
2023-02-23 11:18:33,679 DEBUG TRAIN Batch 18/1200 loss 16.470894 loss_att 17.886208 loss_ctc 23.138844 loss_rnnt 15.026526 hw_loss 0.510461 lr 0.00040643 rank 6
2023-02-23 11:18:33,680 DEBUG TRAIN Batch 18/1200 loss 12.548367 loss_att 14.700965 loss_ctc 18.465733 loss_rnnt 11.136355 hw_loss 0.360956 lr 0.00040644 rank 5
2023-02-23 11:18:33,688 DEBUG TRAIN Batch 18/1200 loss 8.963490 loss_att 9.945151 loss_ctc 11.854167 loss_rnnt 8.097473 hw_loss 0.532990 lr 0.00040650 rank 0
2023-02-23 11:18:33,705 DEBUG TRAIN Batch 18/1200 loss 7.287054 loss_att 10.961318 loss_ctc 11.459113 loss_rnnt 5.710696 hw_loss 0.534806 lr 0.00040647 rank 4
2023-02-23 11:18:33,749 DEBUG TRAIN Batch 18/1200 loss 12.625822 loss_att 16.932442 loss_ctc 16.758780 loss_rnnt 10.962814 hw_loss 0.469919 lr 0.00040649 rank 1
2023-02-23 11:19:45,745 DEBUG TRAIN Batch 18/1300 loss 16.103662 loss_att 21.830481 loss_ctc 23.229931 loss_rnnt 13.741977 hw_loss 0.499038 lr 0.00040629 rank 2
2023-02-23 11:19:45,745 DEBUG TRAIN Batch 18/1300 loss 10.595022 loss_att 10.686892 loss_ctc 12.946764 loss_rnnt 9.882586 hw_loss 0.713428 lr 0.00040624 rank 3
2023-02-23 11:19:45,746 DEBUG TRAIN Batch 18/1300 loss 12.504108 loss_att 15.403731 loss_ctc 17.229298 loss_rnnt 11.062918 hw_loss 0.433580 lr 0.00040633 rank 4
2023-02-23 11:19:45,747 DEBUG TRAIN Batch 18/1300 loss 8.753452 loss_att 9.816355 loss_ctc 12.169312 loss_rnnt 7.764175 hw_loss 0.602340 lr 0.00040636 rank 1
2023-02-23 11:19:45,747 DEBUG TRAIN Batch 18/1300 loss 12.142915 loss_att 11.780284 loss_ctc 15.163177 loss_rnnt 11.520450 hw_loss 0.548045 lr 0.00040630 rank 6
2023-02-23 11:19:45,747 DEBUG TRAIN Batch 18/1300 loss 6.646484 loss_att 10.015262 loss_ctc 10.847597 loss_rnnt 5.163211 hw_loss 0.467567 lr 0.00040627 rank 7
2023-02-23 11:19:45,755 DEBUG TRAIN Batch 18/1300 loss 7.944848 loss_att 8.390800 loss_ctc 8.889148 loss_rnnt 7.425687 hw_loss 0.570119 lr 0.00040631 rank 5
2023-02-23 11:19:45,756 DEBUG TRAIN Batch 18/1300 loss 7.256465 loss_att 12.563169 loss_ctc 12.597786 loss_rnnt 5.243048 hw_loss 0.449813 lr 0.00040637 rank 0
2023-02-23 11:21:00,122 DEBUG TRAIN Batch 18/1400 loss 12.820963 loss_att 14.263372 loss_ctc 14.030163 loss_rnnt 12.132408 hw_loss 0.447835 lr 0.00040620 rank 4
2023-02-23 11:21:00,129 DEBUG TRAIN Batch 18/1400 loss 17.767481 loss_att 19.613720 loss_ctc 20.611198 loss_rnnt 16.844492 hw_loss 0.327332 lr 0.00040623 rank 0
2023-02-23 11:21:00,132 DEBUG TRAIN Batch 18/1400 loss 6.453247 loss_att 10.110011 loss_ctc 11.805812 loss_rnnt 4.825171 hw_loss 0.343216 lr 0.00040610 rank 3
2023-02-23 11:21:00,133 DEBUG TRAIN Batch 18/1400 loss 5.502186 loss_att 8.227860 loss_ctc 7.338459 loss_rnnt 4.497817 hw_loss 0.401995 lr 0.00040616 rank 2
2023-02-23 11:21:00,134 DEBUG TRAIN Batch 18/1400 loss 11.022205 loss_att 12.574344 loss_ctc 12.284409 loss_rnnt 10.365429 hw_loss 0.333854 lr 0.00040614 rank 7
2023-02-23 11:21:00,137 DEBUG TRAIN Batch 18/1400 loss 16.726267 loss_att 19.648977 loss_ctc 18.848089 loss_rnnt 15.619329 hw_loss 0.449035 lr 0.00040623 rank 1
2023-02-23 11:21:00,145 DEBUG TRAIN Batch 18/1400 loss 18.144079 loss_att 19.568323 loss_ctc 23.601118 loss_rnnt 16.881664 hw_loss 0.468673 lr 0.00040617 rank 5
2023-02-23 11:21:00,154 DEBUG TRAIN Batch 18/1400 loss 3.917561 loss_att 8.558821 loss_ctc 8.230846 loss_rnnt 2.212205 hw_loss 0.378749 lr 0.00040617 rank 6
2023-02-23 11:22:12,980 DEBUG TRAIN Batch 18/1500 loss 10.095366 loss_att 14.604551 loss_ctc 16.832981 loss_rnnt 8.097411 hw_loss 0.370821 lr 0.00040600 rank 7
2023-02-23 11:22:12,983 DEBUG TRAIN Batch 18/1500 loss 3.844957 loss_att 5.875987 loss_ctc 4.150399 loss_rnnt 3.136494 hw_loss 0.490371 lr 0.00040602 rank 2
2023-02-23 11:22:12,985 DEBUG TRAIN Batch 18/1500 loss 13.948957 loss_att 16.431053 loss_ctc 16.105284 loss_rnnt 12.970691 hw_loss 0.364385 lr 0.00040603 rank 6
2023-02-23 11:22:12,988 DEBUG TRAIN Batch 18/1500 loss 16.997690 loss_att 20.205708 loss_ctc 22.964016 loss_rnnt 15.334148 hw_loss 0.424551 lr 0.00040597 rank 3
2023-02-23 11:22:12,990 DEBUG TRAIN Batch 18/1500 loss 3.627816 loss_att 7.636209 loss_ctc 4.531019 loss_rnnt 2.481983 hw_loss 0.419489 lr 0.00040604 rank 5
2023-02-23 11:22:12,992 DEBUG TRAIN Batch 18/1500 loss 8.059872 loss_att 10.454521 loss_ctc 10.882602 loss_rnnt 6.997383 hw_loss 0.388491 lr 0.00040609 rank 1
2023-02-23 11:22:13,017 DEBUG TRAIN Batch 18/1500 loss 7.924833 loss_att 12.602772 loss_ctc 7.815552 loss_rnnt 6.792022 hw_loss 0.397115 lr 0.00040610 rank 0
2023-02-23 11:22:13,019 DEBUG TRAIN Batch 18/1500 loss 10.543696 loss_att 17.290945 loss_ctc 16.839849 loss_rnnt 8.088336 hw_loss 0.499546 lr 0.00040606 rank 4
2023-02-23 11:23:25,454 DEBUG TRAIN Batch 18/1600 loss 6.474288 loss_att 10.344853 loss_ctc 10.127394 loss_rnnt 4.976466 hw_loss 0.443678 lr 0.00040593 rank 4
2023-02-23 11:23:25,462 DEBUG TRAIN Batch 18/1600 loss 6.746252 loss_att 11.350633 loss_ctc 13.760679 loss_rnnt 4.680513 hw_loss 0.393009 lr 0.00040589 rank 2
2023-02-23 11:23:25,468 DEBUG TRAIN Batch 18/1600 loss 4.327664 loss_att 6.319925 loss_ctc 5.125822 loss_rnnt 3.616711 hw_loss 0.386401 lr 0.00040587 rank 7
2023-02-23 11:23:25,469 DEBUG TRAIN Batch 18/1600 loss 7.979345 loss_att 10.061747 loss_ctc 8.371119 loss_rnnt 7.301349 hw_loss 0.392400 lr 0.00040596 rank 1
2023-02-23 11:23:25,470 DEBUG TRAIN Batch 18/1600 loss 13.254862 loss_att 14.584389 loss_ctc 15.221071 loss_rnnt 12.518250 hw_loss 0.391024 lr 0.00040583 rank 3
2023-02-23 11:23:25,471 DEBUG TRAIN Batch 18/1600 loss 11.004972 loss_att 14.633796 loss_ctc 17.211647 loss_rnnt 9.260366 hw_loss 0.358657 lr 0.00040590 rank 5
2023-02-23 11:23:25,474 DEBUG TRAIN Batch 18/1600 loss 12.725101 loss_att 15.166464 loss_ctc 17.180294 loss_rnnt 11.440636 hw_loss 0.379066 lr 0.00040590 rank 6
2023-02-23 11:23:25,485 DEBUG TRAIN Batch 18/1600 loss 13.144260 loss_att 16.284616 loss_ctc 19.094126 loss_rnnt 11.528500 hw_loss 0.364452 lr 0.00040596 rank 0
2023-02-23 11:24:37,980 DEBUG TRAIN Batch 18/1700 loss 12.193136 loss_att 14.810741 loss_ctc 19.911345 loss_rnnt 10.428022 hw_loss 0.398435 lr 0.00040574 rank 7
2023-02-23 11:24:37,984 DEBUG TRAIN Batch 18/1700 loss 7.345119 loss_att 8.250616 loss_ctc 10.823217 loss_rnnt 6.473731 hw_loss 0.424766 lr 0.00040582 rank 1
2023-02-23 11:24:37,985 DEBUG TRAIN Batch 18/1700 loss 10.240593 loss_att 13.210735 loss_ctc 12.190202 loss_rnnt 9.183157 hw_loss 0.381486 lr 0.00040576 rank 2
2023-02-23 11:24:37,986 DEBUG TRAIN Batch 18/1700 loss 11.388282 loss_att 13.105571 loss_ctc 17.028046 loss_rnnt 10.083328 hw_loss 0.392863 lr 0.00040577 rank 5
2023-02-23 11:24:37,986 DEBUG TRAIN Batch 18/1700 loss 11.579332 loss_att 12.717780 loss_ctc 12.229363 loss_rnnt 11.057027 hw_loss 0.389896 lr 0.00040580 rank 4
2023-02-23 11:24:37,986 DEBUG TRAIN Batch 18/1700 loss 12.938771 loss_att 16.243284 loss_ctc 17.068495 loss_rnnt 11.485668 hw_loss 0.452945 lr 0.00040576 rank 6
2023-02-23 11:24:37,988 DEBUG TRAIN Batch 18/1700 loss 13.204082 loss_att 18.627125 loss_ctc 21.823679 loss_rnnt 10.793768 hw_loss 0.330800 lr 0.00040583 rank 0
2023-02-23 11:24:37,991 DEBUG TRAIN Batch 18/1700 loss 14.264213 loss_att 17.104513 loss_ctc 18.912350 loss_rnnt 12.871008 hw_loss 0.385111 lr 0.00040570 rank 3
2023-02-23 11:25:53,065 DEBUG TRAIN Batch 18/1800 loss 12.985918 loss_att 16.885481 loss_ctc 14.280390 loss_rnnt 11.819241 hw_loss 0.401569 lr 0.00040566 rank 4
2023-02-23 11:25:53,067 DEBUG TRAIN Batch 18/1800 loss 10.049148 loss_att 12.610855 loss_ctc 13.076502 loss_rnnt 8.930560 hw_loss 0.379872 lr 0.00040557 rank 3
2023-02-23 11:25:53,068 DEBUG TRAIN Batch 18/1800 loss 8.011921 loss_att 7.860668 loss_ctc 11.105711 loss_rnnt 7.351538 hw_loss 0.521491 lr 0.00040560 rank 7
2023-02-23 11:25:53,069 DEBUG TRAIN Batch 18/1800 loss 15.070466 loss_att 15.444468 loss_ctc 17.832470 loss_rnnt 14.401451 hw_loss 0.423652 lr 0.00040569 rank 1
2023-02-23 11:25:53,071 DEBUG TRAIN Batch 18/1800 loss 10.320712 loss_att 12.498188 loss_ctc 13.641171 loss_rnnt 9.191317 hw_loss 0.470947 lr 0.00040562 rank 2
2023-02-23 11:25:53,071 DEBUG TRAIN Batch 18/1800 loss 7.727014 loss_att 10.375887 loss_ctc 11.876829 loss_rnnt 6.389053 hw_loss 0.477893 lr 0.00040564 rank 5
2023-02-23 11:25:53,071 DEBUG TRAIN Batch 18/1800 loss 7.214352 loss_att 10.647034 loss_ctc 9.033336 loss_rnnt 6.080967 hw_loss 0.383095 lr 0.00040563 rank 6
2023-02-23 11:25:53,071 DEBUG TRAIN Batch 18/1800 loss 8.232062 loss_att 12.446669 loss_ctc 12.432658 loss_rnnt 6.607766 hw_loss 0.414927 lr 0.00040570 rank 0
2023-02-23 11:27:06,012 DEBUG TRAIN Batch 18/1900 loss 13.382645 loss_att 14.993732 loss_ctc 16.410084 loss_rnnt 12.467928 hw_loss 0.354075 lr 0.00040547 rank 7
2023-02-23 11:27:06,016 DEBUG TRAIN Batch 18/1900 loss 9.169340 loss_att 11.231508 loss_ctc 11.761060 loss_rnnt 8.137207 hw_loss 0.514007 lr 0.00040553 rank 4
2023-02-23 11:27:06,015 DEBUG TRAIN Batch 18/1900 loss 4.203159 loss_att 6.919348 loss_ctc 5.174438 loss_rnnt 3.302499 hw_loss 0.427348 lr 0.00040549 rank 2
2023-02-23 11:27:06,015 DEBUG TRAIN Batch 18/1900 loss 8.683189 loss_att 11.783199 loss_ctc 14.490409 loss_rnnt 7.010522 hw_loss 0.521941 lr 0.00040543 rank 3
2023-02-23 11:27:06,017 DEBUG TRAIN Batch 18/1900 loss 7.065926 loss_att 8.755176 loss_ctc 11.608381 loss_rnnt 5.885470 hw_loss 0.444273 lr 0.00040550 rank 5
2023-02-23 11:27:06,016 DEBUG TRAIN Batch 18/1900 loss 13.234595 loss_att 12.486529 loss_ctc 16.729837 loss_rnnt 12.666133 hw_loss 0.472580 lr 0.00040556 rank 1
2023-02-23 11:27:06,021 DEBUG TRAIN Batch 18/1900 loss 15.828352 loss_att 16.529305 loss_ctc 20.116997 loss_rnnt 14.846262 hw_loss 0.506399 lr 0.00040550 rank 6
2023-02-23 11:27:06,025 DEBUG TRAIN Batch 18/1900 loss 8.440192 loss_att 9.418569 loss_ctc 9.735771 loss_rnnt 7.884574 hw_loss 0.350997 lr 0.00040556 rank 0
2023-02-23 11:28:18,330 DEBUG TRAIN Batch 18/2000 loss 9.186089 loss_att 11.818592 loss_ctc 10.313206 loss_rnnt 8.331123 hw_loss 0.334090 lr 0.00040534 rank 7
2023-02-23 11:28:18,334 DEBUG TRAIN Batch 18/2000 loss 9.446230 loss_att 12.490427 loss_ctc 11.485382 loss_rnnt 8.340167 hw_loss 0.422507 lr 0.00040530 rank 3
2023-02-23 11:28:18,335 DEBUG TRAIN Batch 18/2000 loss 6.260120 loss_att 7.751161 loss_ctc 5.885956 loss_rnnt 5.802361 hw_loss 0.392701 lr 0.00040542 rank 1
2023-02-23 11:28:18,335 DEBUG TRAIN Batch 18/2000 loss 4.555099 loss_att 7.745026 loss_ctc 6.609180 loss_rnnt 3.413481 hw_loss 0.430793 lr 0.00040543 rank 0
2023-02-23 11:28:18,336 DEBUG TRAIN Batch 18/2000 loss 9.199220 loss_att 13.863869 loss_ctc 12.896753 loss_rnnt 7.556249 hw_loss 0.406946 lr 0.00040539 rank 4
2023-02-23 11:28:18,337 DEBUG TRAIN Batch 18/2000 loss 14.501090 loss_att 15.161196 loss_ctc 20.460318 loss_rnnt 13.294840 hw_loss 0.524370 lr 0.00040536 rank 6
2023-02-23 11:28:18,338 DEBUG TRAIN Batch 18/2000 loss 9.295004 loss_att 12.817734 loss_ctc 11.575866 loss_rnnt 8.056501 hw_loss 0.430953 lr 0.00040535 rank 2
2023-02-23 11:28:18,342 DEBUG TRAIN Batch 18/2000 loss 15.979558 loss_att 20.530817 loss_ctc 24.053350 loss_rnnt 13.807566 hw_loss 0.347312 lr 0.00040537 rank 5
2023-02-23 11:29:32,540 DEBUG TRAIN Batch 18/2100 loss 5.129792 loss_att 8.105503 loss_ctc 8.108397 loss_rnnt 3.918768 hw_loss 0.410128 lr 0.00040526 rank 4
2023-02-23 11:29:32,550 DEBUG TRAIN Batch 18/2100 loss 13.747551 loss_att 18.719616 loss_ctc 15.454150 loss_rnnt 12.296288 hw_loss 0.429945 lr 0.00040529 rank 1
2023-02-23 11:29:32,550 DEBUG TRAIN Batch 18/2100 loss 9.445457 loss_att 10.708434 loss_ctc 10.447693 loss_rnnt 8.823980 hw_loss 0.441091 lr 0.00040520 rank 7
2023-02-23 11:29:32,555 DEBUG TRAIN Batch 18/2100 loss 2.503506 loss_att 5.085878 loss_ctc 3.294176 loss_rnnt 1.706877 hw_loss 0.327622 lr 0.00040523 rank 6
2023-02-23 11:29:32,556 DEBUG TRAIN Batch 18/2100 loss 12.398222 loss_att 12.585908 loss_ctc 13.701271 loss_rnnt 11.923775 hw_loss 0.493446 lr 0.00040522 rank 2
2023-02-23 11:29:32,558 DEBUG TRAIN Batch 18/2100 loss 11.939525 loss_att 15.573936 loss_ctc 14.820694 loss_rnnt 10.635925 hw_loss 0.361054 lr 0.00040530 rank 0
2023-02-23 11:29:32,562 DEBUG TRAIN Batch 18/2100 loss 10.365399 loss_att 13.369492 loss_ctc 15.234972 loss_rnnt 8.899815 hw_loss 0.404044 lr 0.00040524 rank 5
2023-02-23 11:29:32,561 DEBUG TRAIN Batch 18/2100 loss 7.306901 loss_att 11.058113 loss_ctc 5.739855 loss_rnnt 6.546863 hw_loss 0.410129 lr 0.00040517 rank 3
2023-02-23 11:30:45,179 DEBUG TRAIN Batch 18/2200 loss 5.079009 loss_att 10.596987 loss_ctc 8.798235 loss_rnnt 3.285814 hw_loss 0.363191 lr 0.00040510 rank 5
2023-02-23 11:30:45,178 DEBUG TRAIN Batch 18/2200 loss 9.509507 loss_att 13.144440 loss_ctc 8.813681 loss_rnnt 8.698292 hw_loss 0.331887 lr 0.00040516 rank 0
2023-02-23 11:30:45,179 DEBUG TRAIN Batch 18/2200 loss 7.119050 loss_att 9.840068 loss_ctc 8.861875 loss_rnnt 6.127864 hw_loss 0.402386 lr 0.00040507 rank 7
2023-02-23 11:30:45,182 DEBUG TRAIN Batch 18/2200 loss 14.013994 loss_att 16.883436 loss_ctc 19.067011 loss_rnnt 12.524027 hw_loss 0.454392 lr 0.00040509 rank 2
2023-02-23 11:30:45,185 DEBUG TRAIN Batch 18/2200 loss 6.730092 loss_att 12.119114 loss_ctc 9.501631 loss_rnnt 5.035687 hw_loss 0.463240 lr 0.00040510 rank 6
2023-02-23 11:30:45,185 DEBUG TRAIN Batch 18/2200 loss 9.101643 loss_att 11.865636 loss_ctc 13.368870 loss_rnnt 7.758317 hw_loss 0.415429 lr 0.00040513 rank 4
2023-02-23 11:30:45,186 DEBUG TRAIN Batch 18/2200 loss 6.603846 loss_att 8.231667 loss_ctc 7.247421 loss_rnnt 5.984792 hw_loss 0.389401 lr 0.00040516 rank 1
2023-02-23 11:30:45,186 DEBUG TRAIN Batch 18/2200 loss 7.392695 loss_att 10.635714 loss_ctc 9.747543 loss_rnnt 6.228026 hw_loss 0.378912 lr 0.00040503 rank 3
2023-02-23 11:31:56,837 DEBUG TRAIN Batch 18/2300 loss 5.765254 loss_att 10.211460 loss_ctc 8.986902 loss_rnnt 4.248437 hw_loss 0.371292 lr 0.00040502 rank 1
2023-02-23 11:31:56,842 DEBUG TRAIN Batch 18/2300 loss 18.125280 loss_att 22.415220 loss_ctc 23.561642 loss_rnnt 16.334192 hw_loss 0.390472 lr 0.00040500 rank 4
2023-02-23 11:31:56,844 DEBUG TRAIN Batch 18/2300 loss 10.093663 loss_att 14.371363 loss_ctc 15.451809 loss_rnnt 8.311943 hw_loss 0.397049 lr 0.00040494 rank 7
2023-02-23 11:31:56,844 DEBUG TRAIN Batch 18/2300 loss 19.032867 loss_att 21.046577 loss_ctc 24.523399 loss_rnnt 17.660877 hw_loss 0.444710 lr 0.00040490 rank 3
2023-02-23 11:31:56,847 DEBUG TRAIN Batch 18/2300 loss 9.092217 loss_att 12.135416 loss_ctc 10.661803 loss_rnnt 8.019866 hw_loss 0.477064 lr 0.00040497 rank 6
2023-02-23 11:31:56,851 DEBUG TRAIN Batch 18/2300 loss 5.814754 loss_att 9.658220 loss_ctc 10.583851 loss_rnnt 4.186968 hw_loss 0.418524 lr 0.00040496 rank 2
2023-02-23 11:31:56,854 DEBUG TRAIN Batch 18/2300 loss 19.645144 loss_att 21.742220 loss_ctc 25.324360 loss_rnnt 18.259560 hw_loss 0.391760 lr 0.00040497 rank 5
2023-02-23 11:31:56,857 DEBUG TRAIN Batch 18/2300 loss 7.303521 loss_att 10.697493 loss_ctc 11.141792 loss_rnnt 5.896180 hw_loss 0.406456 lr 0.00040503 rank 0
2023-02-23 11:33:10,281 DEBUG TRAIN Batch 18/2400 loss 11.679393 loss_att 14.895535 loss_ctc 16.561981 loss_rnnt 10.136947 hw_loss 0.465387 lr 0.00040480 rank 7
2023-02-23 11:33:10,283 DEBUG TRAIN Batch 18/2400 loss 8.842077 loss_att 12.182892 loss_ctc 11.953417 loss_rnnt 7.573740 hw_loss 0.347493 lr 0.00040482 rank 2
2023-02-23 11:33:10,282 DEBUG TRAIN Batch 18/2400 loss 15.353237 loss_att 18.333954 loss_ctc 20.323393 loss_rnnt 13.868155 hw_loss 0.424221 lr 0.00040477 rank 3
2023-02-23 11:33:10,284 DEBUG TRAIN Batch 18/2400 loss 7.305421 loss_att 8.449927 loss_ctc 10.006854 loss_rnnt 6.475997 hw_loss 0.450621 lr 0.00040489 rank 1
2023-02-23 11:33:10,287 DEBUG TRAIN Batch 18/2400 loss 10.899018 loss_att 14.958307 loss_ctc 17.701609 loss_rnnt 8.978775 hw_loss 0.377577 lr 0.00040486 rank 4
2023-02-23 11:33:10,289 DEBUG TRAIN Batch 18/2400 loss 17.347242 loss_att 19.788820 loss_ctc 21.021042 loss_rnnt 16.180017 hw_loss 0.354504 lr 0.00040484 rank 5
2023-02-23 11:33:10,293 DEBUG TRAIN Batch 18/2400 loss 11.328398 loss_att 13.533118 loss_ctc 18.804392 loss_rnnt 9.693122 hw_loss 0.370375 lr 0.00040490 rank 0
2023-02-23 11:33:10,313 DEBUG TRAIN Batch 18/2400 loss 10.934305 loss_att 15.743761 loss_ctc 14.457273 loss_rnnt 9.279654 hw_loss 0.418186 lr 0.00040483 rank 6
2023-02-23 11:34:25,626 DEBUG TRAIN Batch 18/2500 loss 6.536163 loss_att 9.288465 loss_ctc 11.268996 loss_rnnt 5.156410 hw_loss 0.371713 lr 0.00040467 rank 7
2023-02-23 11:34:25,626 DEBUG TRAIN Batch 18/2500 loss 6.401201 loss_att 8.429873 loss_ctc 7.875955 loss_rnnt 5.586836 hw_loss 0.397494 lr 0.00040476 rank 1
2023-02-23 11:34:25,628 DEBUG TRAIN Batch 18/2500 loss 7.132923 loss_att 8.504850 loss_ctc 8.955561 loss_rnnt 6.361635 hw_loss 0.476034 lr 0.00040470 rank 6
2023-02-23 11:34:25,629 DEBUG TRAIN Batch 18/2500 loss 8.469648 loss_att 9.627848 loss_ctc 11.825747 loss_rnnt 7.547622 hw_loss 0.455452 lr 0.00040469 rank 2
2023-02-23 11:34:25,630 DEBUG TRAIN Batch 18/2500 loss 5.472638 loss_att 6.000731 loss_ctc 6.401585 loss_rnnt 4.957500 hw_loss 0.535609 lr 0.00040473 rank 4
2023-02-23 11:34:25,630 DEBUG TRAIN Batch 18/2500 loss 7.809265 loss_att 8.665093 loss_ctc 11.210472 loss_rnnt 6.962399 hw_loss 0.416637 lr 0.00040464 rank 3
2023-02-23 11:34:25,631 DEBUG TRAIN Batch 18/2500 loss 16.279154 loss_att 17.646763 loss_ctc 21.943682 loss_rnnt 15.036301 hw_loss 0.401365 lr 0.00040471 rank 5
2023-02-23 11:34:25,682 DEBUG TRAIN Batch 18/2500 loss 8.541076 loss_att 10.778248 loss_ctc 13.097804 loss_rnnt 7.291912 hw_loss 0.364061 lr 0.00040476 rank 0
2023-02-23 11:35:38,383 DEBUG TRAIN Batch 18/2600 loss 13.410942 loss_att 17.302132 loss_ctc 19.958454 loss_rnnt 11.613147 hw_loss 0.274791 lr 0.00040450 rank 3
2023-02-23 11:35:38,400 DEBUG TRAIN Batch 18/2600 loss 3.890772 loss_att 9.140234 loss_ctc 5.377192 loss_rnnt 2.445169 hw_loss 0.370352 lr 0.00040456 rank 2
2023-02-23 11:35:38,400 DEBUG TRAIN Batch 18/2600 loss 11.147827 loss_att 16.050568 loss_ctc 19.528997 loss_rnnt 8.844891 hw_loss 0.384186 lr 0.00040454 rank 7
2023-02-23 11:35:38,402 DEBUG TRAIN Batch 18/2600 loss 19.013861 loss_att 25.408203 loss_ctc 25.559301 loss_rnnt 16.645933 hw_loss 0.405624 lr 0.00040460 rank 4
2023-02-23 11:35:38,404 DEBUG TRAIN Batch 18/2600 loss 7.829447 loss_att 11.201105 loss_ctc 7.201455 loss_rnnt 7.005018 hw_loss 0.438430 lr 0.00040463 rank 0
2023-02-23 11:35:38,405 DEBUG TRAIN Batch 18/2600 loss 10.667389 loss_att 9.098477 loss_ctc 13.304391 loss_rnnt 10.282539 hw_loss 0.650684 lr 0.00040457 rank 5
2023-02-23 11:35:38,406 DEBUG TRAIN Batch 18/2600 loss 8.841844 loss_att 12.239786 loss_ctc 16.640350 loss_rnnt 6.977476 hw_loss 0.271834 lr 0.00040457 rank 6
2023-02-23 11:35:38,409 DEBUG TRAIN Batch 18/2600 loss 6.593087 loss_att 7.982380 loss_ctc 10.805903 loss_rnnt 5.443718 hw_loss 0.580877 lr 0.00040463 rank 1
2023-02-23 11:36:50,914 DEBUG TRAIN Batch 18/2700 loss 29.374788 loss_att 29.748859 loss_ctc 37.944016 loss_rnnt 27.987286 hw_loss 0.318988 lr 0.00040441 rank 7
2023-02-23 11:36:50,919 DEBUG TRAIN Batch 18/2700 loss 8.449120 loss_att 11.579309 loss_ctc 11.266058 loss_rnnt 7.220760 hw_loss 0.425117 lr 0.00040443 rank 2
2023-02-23 11:36:50,923 DEBUG TRAIN Batch 18/2700 loss 14.648498 loss_att 18.795012 loss_ctc 17.785854 loss_rnnt 13.176326 hw_loss 0.421040 lr 0.00040447 rank 4
2023-02-23 11:36:50,923 DEBUG TRAIN Batch 18/2700 loss 13.356402 loss_att 14.001637 loss_ctc 14.269758 loss_rnnt 12.886009 hw_loss 0.411684 lr 0.00040437 rank 3
2023-02-23 11:36:50,923 DEBUG TRAIN Batch 18/2700 loss 11.848919 loss_att 12.514163 loss_ctc 15.371408 loss_rnnt 11.067451 hw_loss 0.335161 lr 0.00040449 rank 1
2023-02-23 11:36:50,927 DEBUG TRAIN Batch 18/2700 loss 17.964010 loss_att 20.850956 loss_ctc 25.001242 loss_rnnt 16.260506 hw_loss 0.352158 lr 0.00040443 rank 6
2023-02-23 11:36:50,927 DEBUG TRAIN Batch 18/2700 loss 4.368614 loss_att 7.485520 loss_ctc 4.668229 loss_rnnt 3.504191 hw_loss 0.377048 lr 0.00040444 rank 5
2023-02-23 11:36:50,928 DEBUG TRAIN Batch 18/2700 loss 9.523151 loss_att 13.650650 loss_ctc 12.256126 loss_rnnt 8.085420 hw_loss 0.464691 lr 0.00040450 rank 0
2023-02-23 11:38:05,192 DEBUG TRAIN Batch 18/2800 loss 5.128243 loss_att 9.845406 loss_ctc 7.896730 loss_rnnt 3.655340 hw_loss 0.300636 lr 0.00040436 rank 1
2023-02-23 11:38:05,198 DEBUG TRAIN Batch 18/2800 loss 12.254500 loss_att 13.474360 loss_ctc 13.633629 loss_rnnt 11.622114 hw_loss 0.383494 lr 0.00040431 rank 5
2023-02-23 11:38:05,198 DEBUG TRAIN Batch 18/2800 loss 11.785995 loss_att 14.568985 loss_ctc 16.819248 loss_rnnt 10.237877 hw_loss 0.600788 lr 0.00040433 rank 4
2023-02-23 11:38:05,203 DEBUG TRAIN Batch 18/2800 loss 10.842015 loss_att 15.361361 loss_ctc 15.930552 loss_rnnt 9.042329 hw_loss 0.407524 lr 0.00040430 rank 6
2023-02-23 11:38:05,204 DEBUG TRAIN Batch 18/2800 loss 8.452356 loss_att 11.031886 loss_ctc 11.743377 loss_rnnt 7.273666 hw_loss 0.419965 lr 0.00040427 rank 7
2023-02-23 11:38:05,207 DEBUG TRAIN Batch 18/2800 loss 5.747946 loss_att 9.474202 loss_ctc 8.134114 loss_rnnt 4.498746 hw_loss 0.348361 lr 0.00040429 rank 2
2023-02-23 11:38:05,210 DEBUG TRAIN Batch 18/2800 loss 7.634888 loss_att 11.141766 loss_ctc 12.272414 loss_rnnt 6.128850 hw_loss 0.349361 lr 0.00040424 rank 3
2023-02-23 11:38:05,232 DEBUG TRAIN Batch 18/2800 loss 13.695203 loss_att 15.151384 loss_ctc 17.045286 loss_rnnt 12.750761 hw_loss 0.387239 lr 0.00040437 rank 0
2023-02-23 11:39:18,758 DEBUG TRAIN Batch 18/2900 loss 14.791165 loss_att 14.879097 loss_ctc 22.167072 loss_rnnt 13.591951 hw_loss 0.371573 lr 0.00040416 rank 2
2023-02-23 11:39:18,763 DEBUG TRAIN Batch 18/2900 loss 7.696177 loss_att 9.824839 loss_ctc 10.025622 loss_rnnt 6.761644 hw_loss 0.371640 lr 0.00040418 rank 5
2023-02-23 11:39:18,763 DEBUG TRAIN Batch 18/2900 loss 6.156163 loss_att 9.960918 loss_ctc 11.045298 loss_rnnt 4.537232 hw_loss 0.386427 lr 0.00040411 rank 3
2023-02-23 11:39:18,764 DEBUG TRAIN Batch 18/2900 loss 11.098432 loss_att 14.101435 loss_ctc 14.495123 loss_rnnt 9.871893 hw_loss 0.324461 lr 0.00040420 rank 4
2023-02-23 11:39:18,764 DEBUG TRAIN Batch 18/2900 loss 12.383868 loss_att 13.836266 loss_ctc 15.823778 loss_rnnt 11.395481 hw_loss 0.448601 lr 0.00040423 rank 1
2023-02-23 11:39:18,766 DEBUG TRAIN Batch 18/2900 loss 14.186619 loss_att 16.910503 loss_ctc 20.118614 loss_rnnt 12.683547 hw_loss 0.313804 lr 0.00040414 rank 7
2023-02-23 11:39:18,769 DEBUG TRAIN Batch 18/2900 loss 5.257470 loss_att 8.573338 loss_ctc 6.237869 loss_rnnt 4.258737 hw_loss 0.384076 lr 0.00040424 rank 0
2023-02-23 11:39:18,817 DEBUG TRAIN Batch 18/2900 loss 8.665516 loss_att 13.338142 loss_ctc 10.938841 loss_rnnt 7.236550 hw_loss 0.358743 lr 0.00040417 rank 6
2023-02-23 11:40:31,247 DEBUG TRAIN Batch 18/3000 loss 7.281396 loss_att 9.442753 loss_ctc 9.348342 loss_rnnt 6.292241 hw_loss 0.527422 lr 0.00040401 rank 7
2023-02-23 11:40:31,255 DEBUG TRAIN Batch 18/3000 loss 12.037019 loss_att 14.172314 loss_ctc 20.576452 loss_rnnt 10.211714 hw_loss 0.486853 lr 0.00040410 rank 1
2023-02-23 11:40:31,258 DEBUG TRAIN Batch 18/3000 loss 15.316367 loss_att 17.819948 loss_ctc 23.120939 loss_rnnt 13.563622 hw_loss 0.396412 lr 0.00040398 rank 3
2023-02-23 11:40:31,260 DEBUG TRAIN Batch 18/3000 loss 9.743660 loss_att 11.672915 loss_ctc 11.950408 loss_rnnt 8.838601 hw_loss 0.421826 lr 0.00040410 rank 0
2023-02-23 11:40:31,260 DEBUG TRAIN Batch 18/3000 loss 2.857885 loss_att 5.315380 loss_ctc 4.413103 loss_rnnt 1.950812 hw_loss 0.390396 lr 0.00040407 rank 4
2023-02-23 11:40:31,262 DEBUG TRAIN Batch 18/3000 loss 3.907280 loss_att 6.032085 loss_ctc 4.715457 loss_rnnt 3.184516 hw_loss 0.356336 lr 0.00040403 rank 2
2023-02-23 11:40:31,264 DEBUG TRAIN Batch 18/3000 loss 13.839881 loss_att 18.393761 loss_ctc 18.662106 loss_rnnt 12.078270 hw_loss 0.389759 lr 0.00040404 rank 6
2023-02-23 11:40:31,303 DEBUG TRAIN Batch 18/3000 loss 7.721255 loss_att 10.024487 loss_ctc 11.563931 loss_rnnt 6.541368 hw_loss 0.387909 lr 0.00040405 rank 5
2023-02-23 11:41:43,899 DEBUG TRAIN Batch 18/3100 loss 15.030264 loss_att 16.161150 loss_ctc 20.863356 loss_rnnt 13.813944 hw_loss 0.398243 lr 0.00040394 rank 4
2023-02-23 11:41:43,906 DEBUG TRAIN Batch 18/3100 loss 10.544661 loss_att 10.028586 loss_ctc 13.174788 loss_rnnt 9.964135 hw_loss 0.624481 lr 0.00040388 rank 7
2023-02-23 11:41:43,909 DEBUG TRAIN Batch 18/3100 loss 20.591639 loss_att 22.052406 loss_ctc 28.919495 loss_rnnt 19.010235 hw_loss 0.335379 lr 0.00040391 rank 6
2023-02-23 11:41:43,910 DEBUG TRAIN Batch 18/3100 loss 7.104637 loss_att 10.185574 loss_ctc 10.101130 loss_rnnt 5.845847 hw_loss 0.455756 lr 0.00040397 rank 1
2023-02-23 11:41:43,913 DEBUG TRAIN Batch 18/3100 loss 11.835288 loss_att 13.908352 loss_ctc 18.249720 loss_rnnt 10.306349 hw_loss 0.485754 lr 0.00040391 rank 5
2023-02-23 11:41:43,913 DEBUG TRAIN Batch 18/3100 loss 8.972485 loss_att 10.564630 loss_ctc 11.400862 loss_rnnt 8.089939 hw_loss 0.450623 lr 0.00040384 rank 3
2023-02-23 11:41:43,922 DEBUG TRAIN Batch 18/3100 loss 12.093319 loss_att 15.340624 loss_ctc 18.643742 loss_rnnt 10.285419 hw_loss 0.534468 lr 0.00040390 rank 2
2023-02-23 11:41:43,950 DEBUG TRAIN Batch 18/3100 loss 15.070605 loss_att 15.330135 loss_ctc 19.758753 loss_rnnt 14.113981 hw_loss 0.524311 lr 0.00040397 rank 0
2023-02-23 11:42:59,493 DEBUG TRAIN Batch 18/3200 loss 21.411137 loss_att 21.989416 loss_ctc 31.375814 loss_rnnt 19.712126 hw_loss 0.477623 lr 0.00040378 rank 6
2023-02-23 11:42:59,494 DEBUG TRAIN Batch 18/3200 loss 10.889804 loss_att 13.478321 loss_ctc 14.736177 loss_rnnt 9.628881 hw_loss 0.431942 lr 0.00040375 rank 7
2023-02-23 11:42:59,496 DEBUG TRAIN Batch 18/3200 loss 14.154492 loss_att 14.760403 loss_ctc 19.611265 loss_rnnt 13.031311 hw_loss 0.514554 lr 0.00040381 rank 4
2023-02-23 11:42:59,497 DEBUG TRAIN Batch 18/3200 loss 7.607718 loss_att 7.211460 loss_ctc 10.398972 loss_rnnt 7.059757 hw_loss 0.478212 lr 0.00040383 rank 1
2023-02-23 11:42:59,498 DEBUG TRAIN Batch 18/3200 loss 7.809982 loss_att 12.289358 loss_ctc 11.488047 loss_rnnt 6.141281 hw_loss 0.529531 lr 0.00040377 rank 2
2023-02-23 11:42:59,502 DEBUG TRAIN Batch 18/3200 loss 10.320306 loss_att 15.782681 loss_ctc 16.906900 loss_rnnt 8.191799 hw_loss 0.295910 lr 0.00040371 rank 3
2023-02-23 11:42:59,506 DEBUG TRAIN Batch 18/3200 loss 9.745361 loss_att 13.956817 loss_ctc 20.174652 loss_rnnt 7.295897 hw_loss 0.406126 lr 0.00040384 rank 0
2023-02-23 11:42:59,542 DEBUG TRAIN Batch 18/3200 loss 17.280479 loss_att 16.615145 loss_ctc 22.939453 loss_rnnt 16.437918 hw_loss 0.414563 lr 0.00040378 rank 5
2023-02-23 11:44:12,413 DEBUG TRAIN Batch 18/3300 loss 15.322217 loss_att 19.267511 loss_ctc 20.575531 loss_rnnt 13.644688 hw_loss 0.352553 lr 0.00040365 rank 5
2023-02-23 11:44:12,421 DEBUG TRAIN Batch 18/3300 loss 6.489348 loss_att 9.701674 loss_ctc 10.930355 loss_rnnt 5.032184 hw_loss 0.417310 lr 0.00040370 rank 1
2023-02-23 11:44:12,422 DEBUG TRAIN Batch 18/3300 loss 11.214733 loss_att 11.872140 loss_ctc 13.988055 loss_rnnt 10.517376 hw_loss 0.367686 lr 0.00040358 rank 3
2023-02-23 11:44:12,422 DEBUG TRAIN Batch 18/3300 loss 8.316022 loss_att 10.217006 loss_ctc 12.919641 loss_rnnt 7.101730 hw_loss 0.413025 lr 0.00040363 rank 2
2023-02-23 11:44:12,426 DEBUG TRAIN Batch 18/3300 loss 9.618731 loss_att 13.562200 loss_ctc 11.571513 loss_rnnt 8.342270 hw_loss 0.426368 lr 0.00040371 rank 0
2023-02-23 11:44:12,427 DEBUG TRAIN Batch 18/3300 loss 12.911360 loss_att 14.491709 loss_ctc 18.450508 loss_rnnt 11.673101 hw_loss 0.344316 lr 0.00040362 rank 7
2023-02-23 11:44:12,429 DEBUG TRAIN Batch 18/3300 loss 9.372130 loss_att 12.906907 loss_ctc 9.749197 loss_rnnt 8.425594 hw_loss 0.354946 lr 0.00040364 rank 6
2023-02-23 11:44:12,467 DEBUG TRAIN Batch 18/3300 loss 13.369999 loss_att 16.714104 loss_ctc 17.431707 loss_rnnt 11.986983 hw_loss 0.323690 lr 0.00040367 rank 4
2023-02-23 11:45:25,662 DEBUG TRAIN Batch 18/3400 loss 5.966566 loss_att 8.502687 loss_ctc 8.322266 loss_rnnt 4.945817 hw_loss 0.373934 lr 0.00040348 rank 7
2023-02-23 11:45:25,664 DEBUG TRAIN Batch 18/3400 loss 6.430534 loss_att 8.029338 loss_ctc 10.043516 loss_rnnt 5.467135 hw_loss 0.303576 lr 0.00040351 rank 6
2023-02-23 11:45:25,665 DEBUG TRAIN Batch 18/3400 loss 6.639812 loss_att 10.093466 loss_ctc 7.756845 loss_rnnt 5.576175 hw_loss 0.419943 lr 0.00040345 rank 3
2023-02-23 11:45:25,667 DEBUG TRAIN Batch 18/3400 loss 5.698862 loss_att 10.615177 loss_ctc 8.338510 loss_rnnt 4.170551 hw_loss 0.362053 lr 0.00040358 rank 0
2023-02-23 11:45:25,669 DEBUG TRAIN Batch 18/3400 loss 4.768810 loss_att 7.583165 loss_ctc 6.995162 loss_rnnt 3.635695 hw_loss 0.512618 lr 0.00040357 rank 1
2023-02-23 11:45:25,669 DEBUG TRAIN Batch 18/3400 loss 3.581948 loss_att 5.916212 loss_ctc 4.022102 loss_rnnt 2.823223 hw_loss 0.437220 lr 0.00040350 rank 2
2023-02-23 11:45:25,670 DEBUG TRAIN Batch 18/3400 loss 4.963756 loss_att 6.162056 loss_ctc 6.204738 loss_rnnt 4.340607 hw_loss 0.408797 lr 0.00040354 rank 4
2023-02-23 11:45:25,674 DEBUG TRAIN Batch 18/3400 loss 4.056470 loss_att 6.407835 loss_ctc 7.441749 loss_rnnt 2.923956 hw_loss 0.395383 lr 0.00040352 rank 5
2023-02-23 11:46:39,927 DEBUG TRAIN Batch 18/3500 loss 17.977745 loss_att 19.469940 loss_ctc 24.057232 loss_rnnt 16.683392 hw_loss 0.347467 lr 0.00040341 rank 4
2023-02-23 11:46:39,937 DEBUG TRAIN Batch 18/3500 loss 5.777027 loss_att 7.421191 loss_ctc 8.026743 loss_rnnt 4.890302 hw_loss 0.483618 lr 0.00040338 rank 6
2023-02-23 11:46:39,944 DEBUG TRAIN Batch 18/3500 loss 5.816258 loss_att 9.044905 loss_ctc 8.437607 loss_rnnt 4.585029 hw_loss 0.442475 lr 0.00040337 rank 2
2023-02-23 11:46:39,945 DEBUG TRAIN Batch 18/3500 loss 12.839619 loss_att 14.236345 loss_ctc 15.851542 loss_rnnt 11.882658 hw_loss 0.517550 lr 0.00040335 rank 7
2023-02-23 11:46:39,947 DEBUG TRAIN Batch 18/3500 loss 3.568579 loss_att 5.558130 loss_ctc 4.253294 loss_rnnt 2.878548 hw_loss 0.376546 lr 0.00040339 rank 5
2023-02-23 11:46:39,949 DEBUG TRAIN Batch 18/3500 loss 9.678734 loss_att 11.296621 loss_ctc 13.910500 loss_rnnt 8.509233 hw_loss 0.528165 lr 0.00040344 rank 1
2023-02-23 11:46:39,950 DEBUG TRAIN Batch 18/3500 loss 8.999001 loss_att 11.831264 loss_ctc 13.446782 loss_rnnt 7.639208 hw_loss 0.375566 lr 0.00040332 rank 3
2023-02-23 11:46:39,950 DEBUG TRAIN Batch 18/3500 loss 9.210937 loss_att 10.217527 loss_ctc 10.865627 loss_rnnt 8.571791 hw_loss 0.407255 lr 0.00040344 rank 0
2023-02-23 11:47:54,284 DEBUG TRAIN Batch 18/3600 loss 6.588192 loss_att 9.146479 loss_ctc 8.931410 loss_rnnt 5.549669 hw_loss 0.402069 lr 0.00040328 rank 4
2023-02-23 11:47:54,286 DEBUG TRAIN Batch 18/3600 loss 17.867397 loss_att 20.624195 loss_ctc 20.983616 loss_rnnt 16.676308 hw_loss 0.420436 lr 0.00040325 rank 6
2023-02-23 11:47:54,288 DEBUG TRAIN Batch 18/3600 loss 13.916418 loss_att 14.822233 loss_ctc 18.963718 loss_rnnt 12.862885 hw_loss 0.373867 lr 0.00040324 rank 2
2023-02-23 11:47:54,290 DEBUG TRAIN Batch 18/3600 loss 10.087518 loss_att 12.710804 loss_ctc 16.363796 loss_rnnt 8.519679 hw_loss 0.386896 lr 0.00040322 rank 7
2023-02-23 11:47:54,292 DEBUG TRAIN Batch 18/3600 loss 11.071954 loss_att 17.808273 loss_ctc 16.578554 loss_rnnt 8.803640 hw_loss 0.350319 lr 0.00040331 rank 1
2023-02-23 11:47:54,294 DEBUG TRAIN Batch 18/3600 loss 13.958923 loss_att 16.756826 loss_ctc 20.092171 loss_rnnt 12.347317 hw_loss 0.439236 lr 0.00040326 rank 5
2023-02-23 11:47:54,294 DEBUG TRAIN Batch 18/3600 loss 11.201064 loss_att 15.977377 loss_ctc 13.926420 loss_rnnt 9.699181 hw_loss 0.343575 lr 0.00040319 rank 3
2023-02-23 11:47:54,294 DEBUG TRAIN Batch 18/3600 loss 13.370472 loss_att 16.367012 loss_ctc 18.720474 loss_rnnt 11.853713 hw_loss 0.382719 lr 0.00040331 rank 0
2023-02-23 11:49:06,835 DEBUG TRAIN Batch 18/3700 loss 10.661965 loss_att 12.742514 loss_ctc 16.174809 loss_rnnt 9.202227 hw_loss 0.578593 lr 0.00040309 rank 7
2023-02-23 11:49:06,837 DEBUG TRAIN Batch 18/3700 loss 6.936018 loss_att 7.316455 loss_ctc 9.887306 loss_rnnt 6.233319 hw_loss 0.437074 lr 0.00040311 rank 2
2023-02-23 11:49:06,837 DEBUG TRAIN Batch 18/3700 loss 11.085711 loss_att 10.432564 loss_ctc 15.572608 loss_rnnt 10.326832 hw_loss 0.546101 lr 0.00040306 rank 3
2023-02-23 11:49:06,840 DEBUG TRAIN Batch 18/3700 loss 21.973568 loss_att 24.116240 loss_ctc 24.630230 loss_rnnt 20.945087 hw_loss 0.460736 lr 0.00040318 rank 1
2023-02-23 11:49:06,841 DEBUG TRAIN Batch 18/3700 loss 14.095490 loss_att 16.249615 loss_ctc 19.295965 loss_rnnt 12.694224 hw_loss 0.519457 lr 0.00040318 rank 0
2023-02-23 11:49:06,843 DEBUG TRAIN Batch 18/3700 loss 6.971512 loss_att 9.756696 loss_ctc 9.398825 loss_rnnt 5.881280 hw_loss 0.392913 lr 0.00040315 rank 4
2023-02-23 11:49:06,846 DEBUG TRAIN Batch 18/3700 loss 9.937920 loss_att 10.378580 loss_ctc 11.297392 loss_rnnt 9.456135 hw_loss 0.398230 lr 0.00040312 rank 5
2023-02-23 11:49:06,888 DEBUG TRAIN Batch 18/3700 loss 3.973287 loss_att 5.330840 loss_ctc 4.929269 loss_rnnt 3.320495 hw_loss 0.475909 lr 0.00040312 rank 6
2023-02-23 11:50:19,704 DEBUG TRAIN Batch 18/3800 loss 8.328114 loss_att 13.510975 loss_ctc 13.874182 loss_rnnt 6.331111 hw_loss 0.414290 lr 0.00040296 rank 7
2023-02-23 11:50:19,706 DEBUG TRAIN Batch 18/3800 loss 9.822880 loss_att 11.370910 loss_ctc 10.457800 loss_rnnt 9.162367 hw_loss 0.499219 lr 0.00040292 rank 3
2023-02-23 11:50:19,706 DEBUG TRAIN Batch 18/3800 loss 10.898005 loss_att 11.624314 loss_ctc 14.245234 loss_rnnt 10.069350 hw_loss 0.444552 lr 0.00040299 rank 5
2023-02-23 11:50:19,708 DEBUG TRAIN Batch 18/3800 loss 8.060097 loss_att 10.876472 loss_ctc 11.730742 loss_rnnt 6.706481 hw_loss 0.564228 lr 0.00040305 rank 1
2023-02-23 11:50:19,710 DEBUG TRAIN Batch 18/3800 loss 8.260553 loss_att 13.951168 loss_ctc 9.618626 loss_rnnt 6.719451 hw_loss 0.416070 lr 0.00040298 rank 2
2023-02-23 11:50:19,712 DEBUG TRAIN Batch 18/3800 loss 15.318182 loss_att 15.154024 loss_ctc 20.498013 loss_rnnt 14.386909 hw_loss 0.512738 lr 0.00040299 rank 6
2023-02-23 11:50:19,715 DEBUG TRAIN Batch 18/3800 loss 10.781470 loss_att 15.652000 loss_ctc 17.156281 loss_rnnt 8.732635 hw_loss 0.421413 lr 0.00040305 rank 0
2023-02-23 11:50:19,763 DEBUG TRAIN Batch 18/3800 loss 16.914543 loss_att 17.375244 loss_ctc 23.450186 loss_rnnt 15.707043 hw_loss 0.457394 lr 0.00040302 rank 4
2023-02-23 11:51:34,480 DEBUG TRAIN Batch 18/3900 loss 10.010762 loss_att 13.888595 loss_ctc 14.599238 loss_rnnt 8.355896 hw_loss 0.501568 lr 0.00040285 rank 2
2023-02-23 11:51:34,488 DEBUG TRAIN Batch 18/3900 loss 7.362234 loss_att 12.478199 loss_ctc 12.952467 loss_rnnt 5.310103 hw_loss 0.531699 lr 0.00040286 rank 5
2023-02-23 11:51:34,492 DEBUG TRAIN Batch 18/3900 loss 5.557706 loss_att 12.556122 loss_ctc 10.343251 loss_rnnt 3.309333 hw_loss 0.394907 lr 0.00040283 rank 7
2023-02-23 11:51:34,495 DEBUG TRAIN Batch 18/3900 loss 1.554165 loss_att 5.115798 loss_ctc 3.480862 loss_rnnt 0.408624 hw_loss 0.330604 lr 0.00040279 rank 3
2023-02-23 11:51:34,497 DEBUG TRAIN Batch 18/3900 loss 11.670918 loss_att 15.916273 loss_ctc 16.740501 loss_rnnt 9.914710 hw_loss 0.433484 lr 0.00040289 rank 4
2023-02-23 11:51:34,497 DEBUG TRAIN Batch 18/3900 loss 8.025343 loss_att 10.431341 loss_ctc 8.471755 loss_rnnt 7.265829 hw_loss 0.410236 lr 0.00040286 rank 6
2023-02-23 11:51:34,501 DEBUG TRAIN Batch 18/3900 loss 10.330860 loss_att 13.512182 loss_ctc 15.625357 loss_rnnt 8.746650 hw_loss 0.453778 lr 0.00040292 rank 1
2023-02-23 11:51:34,502 DEBUG TRAIN Batch 18/3900 loss 5.064088 loss_att 8.620655 loss_ctc 8.400270 loss_rnnt 3.681226 hw_loss 0.425107 lr 0.00040292 rank 0
2023-02-23 11:52:47,395 DEBUG TRAIN Batch 18/4000 loss 11.749689 loss_att 13.920408 loss_ctc 22.742851 loss_rnnt 9.647238 hw_loss 0.379786 lr 0.00040276 rank 4
2023-02-23 11:52:47,396 DEBUG TRAIN Batch 18/4000 loss 2.573826 loss_att 5.876671 loss_ctc 3.709661 loss_rnnt 1.572157 hw_loss 0.355604 lr 0.00040266 rank 3
2023-02-23 11:52:47,396 DEBUG TRAIN Batch 18/4000 loss 5.281852 loss_att 7.684372 loss_ctc 8.127445 loss_rnnt 4.222537 hw_loss 0.373872 lr 0.00040270 rank 7
2023-02-23 11:52:47,399 DEBUG TRAIN Batch 18/4000 loss 9.366659 loss_att 11.334416 loss_ctc 15.318722 loss_rnnt 7.931449 hw_loss 0.465095 lr 0.00040273 rank 6
2023-02-23 11:52:47,401 DEBUG TRAIN Batch 18/4000 loss 11.113213 loss_att 16.792034 loss_ctc 20.162800 loss_rnnt 8.551455 hw_loss 0.411342 lr 0.00040278 rank 1
2023-02-23 11:52:47,402 DEBUG TRAIN Batch 18/4000 loss 12.026649 loss_att 17.159733 loss_ctc 18.637463 loss_rnnt 9.964041 hw_loss 0.289780 lr 0.00040273 rank 5
2023-02-23 11:52:47,405 DEBUG TRAIN Batch 18/4000 loss 9.067693 loss_att 10.926554 loss_ctc 9.223438 loss_rnnt 8.476177 hw_loss 0.373082 lr 0.00040272 rank 2
2023-02-23 11:52:47,453 DEBUG TRAIN Batch 18/4000 loss 12.448524 loss_att 14.824011 loss_ctc 18.160702 loss_rnnt 10.950519 hw_loss 0.489907 lr 0.00040279 rank 0
2023-02-23 11:53:59,653 DEBUG TRAIN Batch 18/4100 loss 2.836720 loss_att 3.567463 loss_ctc 1.837202 loss_rnnt 2.605052 hw_loss 0.410229 lr 0.00040263 rank 4
2023-02-23 11:53:59,656 DEBUG TRAIN Batch 18/4100 loss 10.208075 loss_att 13.342415 loss_ctc 13.898538 loss_rnnt 8.896193 hw_loss 0.361787 lr 0.00040259 rank 2
2023-02-23 11:53:59,657 DEBUG TRAIN Batch 18/4100 loss 13.428448 loss_att 15.479967 loss_ctc 19.715147 loss_rnnt 11.933197 hw_loss 0.462599 lr 0.00040257 rank 7
2023-02-23 11:53:59,659 DEBUG TRAIN Batch 18/4100 loss 6.878082 loss_att 8.857517 loss_ctc 9.269842 loss_rnnt 5.971437 hw_loss 0.359729 lr 0.00040260 rank 6
2023-02-23 11:53:59,660 DEBUG TRAIN Batch 18/4100 loss 9.927453 loss_att 16.433599 loss_ctc 15.854453 loss_rnnt 7.648872 hw_loss 0.350785 lr 0.00040260 rank 5
2023-02-23 11:53:59,659 DEBUG TRAIN Batch 18/4100 loss 2.665037 loss_att 5.530931 loss_ctc 5.036703 loss_rnnt 1.577480 hw_loss 0.371542 lr 0.00040265 rank 1
2023-02-23 11:53:59,662 DEBUG TRAIN Batch 18/4100 loss 3.484618 loss_att 5.096680 loss_ctc 4.478712 loss_rnnt 2.799166 hw_loss 0.432177 lr 0.00040253 rank 3
2023-02-23 11:53:59,670 DEBUG TRAIN Batch 18/4100 loss 6.713511 loss_att 9.326084 loss_ctc 9.200912 loss_rnnt 5.693392 hw_loss 0.311156 lr 0.00040266 rank 0
2023-02-23 11:55:12,862 DEBUG TRAIN Batch 18/4200 loss 10.584714 loss_att 12.021610 loss_ctc 13.934490 loss_rnnt 9.633259 hw_loss 0.407699 lr 0.00040246 rank 6
2023-02-23 11:55:12,871 DEBUG TRAIN Batch 18/4200 loss 8.055288 loss_att 11.895038 loss_ctc 13.997316 loss_rnnt 6.312801 hw_loss 0.341750 lr 0.00040240 rank 3
2023-02-23 11:55:12,875 DEBUG TRAIN Batch 18/4200 loss 6.413183 loss_att 8.778710 loss_ctc 7.049855 loss_rnnt 5.641120 hw_loss 0.401377 lr 0.00040249 rank 4
2023-02-23 11:55:12,877 DEBUG TRAIN Batch 18/4200 loss 9.721590 loss_att 13.154604 loss_ctc 12.624775 loss_rnnt 8.421611 hw_loss 0.424284 lr 0.00040244 rank 7
2023-02-23 11:55:12,877 DEBUG TRAIN Batch 18/4200 loss 11.314617 loss_att 12.443932 loss_ctc 19.564766 loss_rnnt 9.778793 hw_loss 0.393641 lr 0.00040246 rank 2
2023-02-23 11:55:12,885 DEBUG TRAIN Batch 18/4200 loss 17.639324 loss_att 18.583851 loss_ctc 22.286335 loss_rnnt 16.622620 hw_loss 0.390368 lr 0.00040253 rank 0
2023-02-23 11:55:12,891 DEBUG TRAIN Batch 18/4200 loss 8.095949 loss_att 11.512063 loss_ctc 14.121637 loss_rnnt 6.393208 hw_loss 0.405175 lr 0.00040252 rank 1
2023-02-23 11:55:12,903 DEBUG TRAIN Batch 18/4200 loss 12.537876 loss_att 17.128775 loss_ctc 20.614964 loss_rnnt 10.320306 hw_loss 0.417085 lr 0.00040247 rank 5
2023-02-23 11:56:27,383 DEBUG TRAIN Batch 18/4300 loss 13.509117 loss_att 14.908436 loss_ctc 18.933643 loss_rnnt 12.298590 hw_loss 0.388866 lr 0.00040227 rank 3
2023-02-23 11:56:27,386 DEBUG TRAIN Batch 18/4300 loss 6.128036 loss_att 8.685700 loss_ctc 8.494650 loss_rnnt 5.060596 hw_loss 0.450672 lr 0.00040233 rank 6
2023-02-23 11:56:27,387 DEBUG TRAIN Batch 18/4300 loss 12.157385 loss_att 12.220158 loss_ctc 16.450186 loss_rnnt 11.315897 hw_loss 0.481047 lr 0.00040231 rank 7
2023-02-23 11:56:27,389 DEBUG TRAIN Batch 18/4300 loss 15.694980 loss_att 16.755253 loss_ctc 16.499039 loss_rnnt 15.131477 hw_loss 0.457948 lr 0.00040239 rank 1
2023-02-23 11:56:27,392 DEBUG TRAIN Batch 18/4300 loss 13.492426 loss_att 17.608700 loss_ctc 20.904089 loss_rnnt 11.453003 hw_loss 0.427399 lr 0.00040240 rank 0
2023-02-23 11:56:27,392 DEBUG TRAIN Batch 18/4300 loss 8.720195 loss_att 11.008587 loss_ctc 12.771258 loss_rnnt 7.518526 hw_loss 0.382215 lr 0.00040233 rank 2
2023-02-23 11:56:27,393 DEBUG TRAIN Batch 18/4300 loss 23.475161 loss_att 23.702171 loss_ctc 28.271072 loss_rnnt 22.555908 hw_loss 0.439490 lr 0.00040234 rank 5
2023-02-23 11:56:27,396 DEBUG TRAIN Batch 18/4300 loss 8.764292 loss_att 11.488212 loss_ctc 14.399401 loss_rnnt 7.240335 hw_loss 0.427171 lr 0.00040236 rank 4
2023-02-23 11:57:39,473 DEBUG TRAIN Batch 18/4400 loss 9.183714 loss_att 12.244785 loss_ctc 11.553386 loss_rnnt 8.084688 hw_loss 0.320353 lr 0.00040220 rank 2
2023-02-23 11:57:39,488 DEBUG TRAIN Batch 18/4400 loss 20.046036 loss_att 22.379339 loss_ctc 33.113949 loss_rnnt 17.643291 hw_loss 0.363178 lr 0.00040218 rank 7
2023-02-23 11:57:39,490 DEBUG TRAIN Batch 18/4400 loss 15.805812 loss_att 20.538786 loss_ctc 18.877388 loss_rnnt 14.223871 hw_loss 0.423378 lr 0.00040221 rank 5
2023-02-23 11:57:39,490 DEBUG TRAIN Batch 18/4400 loss 13.713125 loss_att 14.666330 loss_ctc 15.599701 loss_rnnt 13.074195 hw_loss 0.368901 lr 0.00040226 rank 1
2023-02-23 11:57:39,492 DEBUG TRAIN Batch 18/4400 loss 5.210262 loss_att 6.734770 loss_ctc 10.470873 loss_rnnt 3.980157 hw_loss 0.419605 lr 0.00040223 rank 4
2023-02-23 11:57:39,494 DEBUG TRAIN Batch 18/4400 loss 6.018444 loss_att 8.165388 loss_ctc 7.727990 loss_rnnt 5.152318 hw_loss 0.391496 lr 0.00040214 rank 3
2023-02-23 11:57:39,497 DEBUG TRAIN Batch 18/4400 loss 16.649599 loss_att 18.172089 loss_ctc 23.130777 loss_rnnt 15.285193 hw_loss 0.367030 lr 0.00040220 rank 6
2023-02-23 11:57:39,539 DEBUG TRAIN Batch 18/4400 loss 10.306180 loss_att 11.400515 loss_ctc 14.854521 loss_rnnt 9.177263 hw_loss 0.569257 lr 0.00040227 rank 0
2023-02-23 11:58:52,438 DEBUG TRAIN Batch 18/4500 loss 11.677670 loss_att 13.439335 loss_ctc 15.020903 loss_rnnt 10.614322 hw_loss 0.497346 lr 0.00040201 rank 3
2023-02-23 11:58:52,445 DEBUG TRAIN Batch 18/4500 loss 5.154896 loss_att 9.876947 loss_ctc 10.675361 loss_rnnt 3.278709 hw_loss 0.366966 lr 0.00040205 rank 7
2023-02-23 11:58:52,448 DEBUG TRAIN Batch 18/4500 loss 9.757128 loss_att 10.099047 loss_ctc 12.428874 loss_rnnt 8.880471 hw_loss 0.847574 lr 0.00040210 rank 4
2023-02-23 11:58:52,449 DEBUG TRAIN Batch 18/4500 loss 7.876404 loss_att 11.979239 loss_ctc 11.888774 loss_rnnt 6.315124 hw_loss 0.385745 lr 0.00040207 rank 2
2023-02-23 11:58:52,450 DEBUG TRAIN Batch 18/4500 loss 7.751095 loss_att 11.718744 loss_ctc 13.706817 loss_rnnt 5.902573 hw_loss 0.489181 lr 0.00040207 rank 6
2023-02-23 11:58:52,452 DEBUG TRAIN Batch 18/4500 loss 5.479321 loss_att 9.616745 loss_ctc 7.660233 loss_rnnt 4.153778 hw_loss 0.388633 lr 0.00040213 rank 1
2023-02-23 11:58:52,456 DEBUG TRAIN Batch 18/4500 loss 9.183349 loss_att 8.926040 loss_ctc 12.462896 loss_rnnt 8.464645 hw_loss 0.624173 lr 0.00040208 rank 5
2023-02-23 11:58:52,501 DEBUG TRAIN Batch 18/4500 loss 16.161007 loss_att 17.427341 loss_ctc 17.364979 loss_rnnt 15.573672 hw_loss 0.325386 lr 0.00040214 rank 0
2023-02-23 12:00:06,940 DEBUG TRAIN Batch 18/4600 loss 10.890574 loss_att 11.896711 loss_ctc 17.274593 loss_rnnt 9.665653 hw_loss 0.323419 lr 0.00040200 rank 1
2023-02-23 12:00:06,952 DEBUG TRAIN Batch 18/4600 loss 1.721158 loss_att 4.698975 loss_ctc 4.148597 loss_rnnt 0.496561 hw_loss 0.572577 lr 0.00040197 rank 4
2023-02-23 12:00:06,953 DEBUG TRAIN Batch 18/4600 loss 9.174406 loss_att 13.721879 loss_ctc 13.386818 loss_rnnt 7.539647 hw_loss 0.306768 lr 0.00040194 rank 2
2023-02-23 12:00:06,954 DEBUG TRAIN Batch 18/4600 loss 12.029900 loss_att 14.415529 loss_ctc 15.933005 loss_rnnt 10.848296 hw_loss 0.345119 lr 0.00040192 rank 7
2023-02-23 12:00:06,955 DEBUG TRAIN Batch 18/4600 loss 9.548708 loss_att 11.236692 loss_ctc 12.113177 loss_rnnt 8.664112 hw_loss 0.384504 lr 0.00040195 rank 5
2023-02-23 12:00:06,956 DEBUG TRAIN Batch 18/4600 loss 6.122648 loss_att 9.944998 loss_ctc 9.270611 loss_rnnt 4.696913 hw_loss 0.452881 lr 0.00040188 rank 3
2023-02-23 12:00:06,982 DEBUG TRAIN Batch 18/4600 loss 10.102661 loss_att 15.343000 loss_ctc 12.522902 loss_rnnt 8.551509 hw_loss 0.338223 lr 0.00040201 rank 0
2023-02-23 12:00:07,011 DEBUG TRAIN Batch 18/4600 loss 9.531991 loss_att 13.034388 loss_ctc 12.139209 loss_rnnt 8.278419 hw_loss 0.385243 lr 0.00040194 rank 6
2023-02-23 12:01:20,189 DEBUG TRAIN Batch 18/4700 loss 13.942204 loss_att 18.231905 loss_ctc 21.167377 loss_rnnt 11.894634 hw_loss 0.424262 lr 0.00040179 rank 7
2023-02-23 12:01:20,193 DEBUG TRAIN Batch 18/4700 loss 12.944235 loss_att 13.765673 loss_ctc 17.256218 loss_rnnt 11.984686 hw_loss 0.413119 lr 0.00040181 rank 2
2023-02-23 12:01:20,194 DEBUG TRAIN Batch 18/4700 loss 17.532595 loss_att 19.079161 loss_ctc 24.177910 loss_rnnt 16.136738 hw_loss 0.375941 lr 0.00040184 rank 4
2023-02-23 12:01:20,197 DEBUG TRAIN Batch 18/4700 loss 16.744591 loss_att 21.541372 loss_ctc 30.587019 loss_rnnt 13.768856 hw_loss 0.320099 lr 0.00040175 rank 3
2023-02-23 12:01:20,197 DEBUG TRAIN Batch 18/4700 loss 11.619409 loss_att 15.691147 loss_ctc 14.446002 loss_rnnt 10.189642 hw_loss 0.447261 lr 0.00040187 rank 1
2023-02-23 12:01:20,202 DEBUG TRAIN Batch 18/4700 loss 8.621558 loss_att 13.646193 loss_ctc 11.989909 loss_rnnt 6.976556 hw_loss 0.358054 lr 0.00040182 rank 5
2023-02-23 12:01:20,203 DEBUG TRAIN Batch 18/4700 loss 16.200060 loss_att 15.679189 loss_ctc 22.249372 loss_rnnt 15.235153 hw_loss 0.492196 lr 0.00040181 rank 6
2023-02-23 12:01:20,209 DEBUG TRAIN Batch 18/4700 loss 5.976886 loss_att 8.498478 loss_ctc 7.768254 loss_rnnt 5.002793 hw_loss 0.432985 lr 0.00040188 rank 0
2023-02-23 12:02:32,170 DEBUG TRAIN Batch 18/4800 loss 16.066700 loss_att 18.616245 loss_ctc 21.853931 loss_rnnt 14.622844 hw_loss 0.304345 lr 0.00040171 rank 4
2023-02-23 12:02:32,171 DEBUG TRAIN Batch 18/4800 loss 16.678453 loss_att 21.775402 loss_ctc 24.070065 loss_rnnt 14.468340 hw_loss 0.384705 lr 0.00040174 rank 1
2023-02-23 12:02:32,173 DEBUG TRAIN Batch 18/4800 loss 11.604790 loss_att 15.705830 loss_ctc 17.458158 loss_rnnt 9.760962 hw_loss 0.455947 lr 0.00040166 rank 7
2023-02-23 12:02:32,173 DEBUG TRAIN Batch 18/4800 loss 12.195389 loss_att 13.367416 loss_ctc 15.458999 loss_rnnt 11.340855 hw_loss 0.346840 lr 0.00040162 rank 3
2023-02-23 12:02:32,173 DEBUG TRAIN Batch 18/4800 loss 7.882121 loss_att 12.452769 loss_ctc 12.723192 loss_rnnt 6.096166 hw_loss 0.424404 lr 0.00040168 rank 2
2023-02-23 12:02:32,174 DEBUG TRAIN Batch 18/4800 loss 21.663078 loss_att 24.373812 loss_ctc 26.312323 loss_rnnt 20.255583 hw_loss 0.460219 lr 0.00040175 rank 0
2023-02-23 12:02:32,179 DEBUG TRAIN Batch 18/4800 loss 18.564976 loss_att 19.327351 loss_ctc 25.704147 loss_rnnt 17.199423 hw_loss 0.489729 lr 0.00040169 rank 5
2023-02-23 12:02:32,222 DEBUG TRAIN Batch 18/4800 loss 5.719957 loss_att 9.527006 loss_ctc 9.609686 loss_rnnt 4.229631 hw_loss 0.394285 lr 0.00040168 rank 6
2023-02-23 12:03:45,701 DEBUG TRAIN Batch 18/4900 loss 4.743186 loss_att 6.407614 loss_ctc 5.977658 loss_rnnt 4.024129 hw_loss 0.415452 lr 0.00040153 rank 7
2023-02-23 12:03:45,708 DEBUG TRAIN Batch 18/4900 loss 8.452039 loss_att 10.529673 loss_ctc 10.039310 loss_rnnt 7.624104 hw_loss 0.376447 lr 0.00040149 rank 3
2023-02-23 12:03:45,709 DEBUG TRAIN Batch 18/4900 loss 14.626186 loss_att 16.560211 loss_ctc 20.153549 loss_rnnt 13.233428 hw_loss 0.504323 lr 0.00040155 rank 2
2023-02-23 12:03:45,709 DEBUG TRAIN Batch 18/4900 loss 4.687290 loss_att 9.005590 loss_ctc 5.983305 loss_rnnt 3.463551 hw_loss 0.351144 lr 0.00040159 rank 4
2023-02-23 12:03:45,709 DEBUG TRAIN Batch 18/4900 loss 4.641305 loss_att 8.126776 loss_ctc 5.882118 loss_rnnt 3.565592 hw_loss 0.399707 lr 0.00040162 rank 0
2023-02-23 12:03:45,716 DEBUG TRAIN Batch 18/4900 loss 9.975468 loss_att 13.273629 loss_ctc 13.589010 loss_rnnt 8.628389 hw_loss 0.385576 lr 0.00040156 rank 5
2023-02-23 12:03:45,721 DEBUG TRAIN Batch 18/4900 loss 5.741118 loss_att 8.885431 loss_ctc 10.902263 loss_rnnt 4.167121 hw_loss 0.481842 lr 0.00040156 rank 6
2023-02-23 12:03:45,772 DEBUG TRAIN Batch 18/4900 loss 12.135753 loss_att 17.910652 loss_ctc 19.260994 loss_rnnt 9.820883 hw_loss 0.393484 lr 0.00040161 rank 1
2023-02-23 12:05:00,995 DEBUG TRAIN Batch 18/5000 loss 19.440361 loss_att 22.116901 loss_ctc 23.295708 loss_rnnt 18.166983 hw_loss 0.420046 lr 0.00040136 rank 3
2023-02-23 12:05:00,995 DEBUG TRAIN Batch 18/5000 loss 11.817720 loss_att 14.800037 loss_ctc 15.333138 loss_rnnt 10.528806 hw_loss 0.419490 lr 0.00040140 rank 7
2023-02-23 12:05:00,999 DEBUG TRAIN Batch 18/5000 loss 8.126115 loss_att 9.713963 loss_ctc 12.087596 loss_rnnt 7.069127 hw_loss 0.396037 lr 0.00040149 rank 0
2023-02-23 12:05:00,999 DEBUG TRAIN Batch 18/5000 loss 9.432485 loss_att 12.067963 loss_ctc 13.515393 loss_rnnt 8.125004 hw_loss 0.442493 lr 0.00040146 rank 4
2023-02-23 12:05:01,000 DEBUG TRAIN Batch 18/5000 loss 9.360017 loss_att 10.664455 loss_ctc 12.655720 loss_rnnt 8.458106 hw_loss 0.377994 lr 0.00040143 rank 5
2023-02-23 12:05:01,002 DEBUG TRAIN Batch 18/5000 loss 13.616185 loss_att 15.455938 loss_ctc 16.237783 loss_rnnt 12.686234 hw_loss 0.398350 lr 0.00040148 rank 1
2023-02-23 12:05:01,002 DEBUG TRAIN Batch 18/5000 loss 10.338678 loss_att 12.830015 loss_ctc 16.268055 loss_rnnt 8.777566 hw_loss 0.510489 lr 0.00040143 rank 6
2023-02-23 12:05:01,005 DEBUG TRAIN Batch 18/5000 loss 9.241380 loss_att 9.030907 loss_ctc 13.231502 loss_rnnt 8.326636 hw_loss 0.796542 lr 0.00040142 rank 2
2023-02-23 12:06:13,264 DEBUG TRAIN Batch 18/5100 loss 5.395984 loss_att 5.318287 loss_ctc 6.450388 loss_rnnt 4.896978 hw_loss 0.701171 lr 0.00040130 rank 6
2023-02-23 12:06:13,280 DEBUG TRAIN Batch 18/5100 loss 7.271545 loss_att 6.725463 loss_ctc 8.332390 loss_rnnt 6.881964 hw_loss 0.670034 lr 0.00040133 rank 4
2023-02-23 12:06:13,282 DEBUG TRAIN Batch 18/5100 loss 6.918972 loss_att 8.964491 loss_ctc 12.977305 loss_rnnt 5.415861 hw_loss 0.536683 lr 0.00040129 rank 2
2023-02-23 12:06:13,283 DEBUG TRAIN Batch 18/5100 loss 11.261396 loss_att 10.973593 loss_ctc 14.540980 loss_rnnt 10.501599 hw_loss 0.712649 lr 0.00040123 rank 3
2023-02-23 12:06:13,285 DEBUG TRAIN Batch 18/5100 loss 16.215288 loss_att 17.067972 loss_ctc 27.855011 loss_rnnt 14.335814 hw_loss 0.294328 lr 0.00040136 rank 0
2023-02-23 12:06:13,285 DEBUG TRAIN Batch 18/5100 loss 6.403285 loss_att 9.018466 loss_ctc 10.642677 loss_rnnt 5.095569 hw_loss 0.411427 lr 0.00040130 rank 5
2023-02-23 12:06:13,286 DEBUG TRAIN Batch 18/5100 loss 6.612560 loss_att 10.101685 loss_ctc 8.431820 loss_rnnt 5.473184 hw_loss 0.373094 lr 0.00040127 rank 7
2023-02-23 12:06:13,289 DEBUG TRAIN Batch 18/5100 loss 6.545263 loss_att 8.392605 loss_ctc 7.844683 loss_rnnt 5.735286 hw_loss 0.501099 lr 0.00040135 rank 1
2023-02-23 12:07:26,002 DEBUG TRAIN Batch 18/5200 loss 12.332662 loss_att 14.064757 loss_ctc 19.226589 loss_rnnt 10.881369 hw_loss 0.348158 lr 0.00040123 rank 1
2023-02-23 12:07:26,008 DEBUG TRAIN Batch 18/5200 loss 10.596351 loss_att 13.755215 loss_ctc 14.663286 loss_rnnt 9.238598 hw_loss 0.344479 lr 0.00040111 rank 3
2023-02-23 12:07:26,011 DEBUG TRAIN Batch 18/5200 loss 8.961899 loss_att 12.383265 loss_ctc 12.660763 loss_rnnt 7.556899 hw_loss 0.426648 lr 0.00040114 rank 7
2023-02-23 12:07:26,021 DEBUG TRAIN Batch 18/5200 loss 14.674957 loss_att 17.144325 loss_ctc 16.106316 loss_rnnt 13.778543 hw_loss 0.396921 lr 0.00040116 rank 2
2023-02-23 12:07:26,021 DEBUG TRAIN Batch 18/5200 loss 19.892191 loss_att 21.518795 loss_ctc 32.626705 loss_rnnt 17.662893 hw_loss 0.386330 lr 0.00040123 rank 0
2023-02-23 12:07:26,022 DEBUG TRAIN Batch 18/5200 loss 11.151700 loss_att 18.118793 loss_ctc 14.476222 loss_rnnt 9.117846 hw_loss 0.369684 lr 0.00040117 rank 5
2023-02-23 12:07:26,023 DEBUG TRAIN Batch 18/5200 loss 5.113386 loss_att 7.437929 loss_ctc 8.925156 loss_rnnt 3.918740 hw_loss 0.415315 lr 0.00040120 rank 4
2023-02-23 12:07:26,024 DEBUG TRAIN Batch 18/5200 loss 7.810789 loss_att 11.075107 loss_ctc 9.159075 loss_rnnt 6.758419 hw_loss 0.412004 lr 0.00040117 rank 6
2023-02-23 12:08:40,202 DEBUG TRAIN Batch 18/5300 loss 17.209085 loss_att 22.075863 loss_ctc 19.850021 loss_rnnt 15.676439 hw_loss 0.388431 lr 0.00040101 rank 7
2023-02-23 12:08:40,215 DEBUG TRAIN Batch 18/5300 loss 3.958628 loss_att 6.438133 loss_ctc 4.536345 loss_rnnt 3.136939 hw_loss 0.466423 lr 0.00040107 rank 4
2023-02-23 12:08:40,216 DEBUG TRAIN Batch 18/5300 loss 14.012016 loss_att 16.352701 loss_ctc 17.969685 loss_rnnt 12.801288 hw_loss 0.402942 lr 0.00040098 rank 3
2023-02-23 12:08:40,217 DEBUG TRAIN Batch 18/5300 loss 7.816282 loss_att 9.608926 loss_ctc 9.781541 loss_rnnt 6.937845 hw_loss 0.483514 lr 0.00040110 rank 1
2023-02-23 12:08:40,219 DEBUG TRAIN Batch 18/5300 loss 3.298429 loss_att 5.609957 loss_ctc 4.083583 loss_rnnt 2.555541 hw_loss 0.329806 lr 0.00040104 rank 6
2023-02-23 12:08:40,223 DEBUG TRAIN Batch 18/5300 loss 11.661246 loss_att 13.356119 loss_ctc 17.683372 loss_rnnt 10.314444 hw_loss 0.384146 lr 0.00040103 rank 2
2023-02-23 12:08:40,232 DEBUG TRAIN Batch 18/5300 loss 4.236235 loss_att 6.121868 loss_ctc 5.454415 loss_rnnt 3.495708 hw_loss 0.376830 lr 0.00040104 rank 5
2023-02-23 12:08:40,268 DEBUG TRAIN Batch 18/5300 loss 6.540255 loss_att 10.534081 loss_ctc 9.185607 loss_rnnt 5.138413 hw_loss 0.469431 lr 0.00040110 rank 0
2023-02-23 12:09:54,011 DEBUG TRAIN Batch 18/5400 loss 10.458688 loss_att 13.703524 loss_ctc 13.728956 loss_rnnt 9.163632 hw_loss 0.393848 lr 0.00040088 rank 7
2023-02-23 12:09:54,013 DEBUG TRAIN Batch 18/5400 loss 15.519135 loss_att 16.239561 loss_ctc 22.104027 loss_rnnt 14.248723 hw_loss 0.465640 lr 0.00040090 rank 2
2023-02-23 12:09:54,018 DEBUG TRAIN Batch 18/5400 loss 11.514333 loss_att 15.288631 loss_ctc 18.143879 loss_rnnt 9.716826 hw_loss 0.297575 lr 0.00040092 rank 5
2023-02-23 12:09:54,017 DEBUG TRAIN Batch 18/5400 loss 8.185324 loss_att 10.542464 loss_ctc 11.602022 loss_rnnt 7.027991 hw_loss 0.431897 lr 0.00040085 rank 3
2023-02-23 12:09:54,018 DEBUG TRAIN Batch 18/5400 loss 7.518274 loss_att 9.788146 loss_ctc 11.527437 loss_rnnt 6.349335 hw_loss 0.338268 lr 0.00040097 rank 1
2023-02-23 12:09:54,020 DEBUG TRAIN Batch 18/5400 loss 7.804405 loss_att 9.774114 loss_ctc 10.401958 loss_rnnt 6.900200 hw_loss 0.307354 lr 0.00040097 rank 0
2023-02-23 12:09:54,023 DEBUG TRAIN Batch 18/5400 loss 13.672192 loss_att 14.987673 loss_ctc 18.970036 loss_rnnt 12.476430 hw_loss 0.424289 lr 0.00040094 rank 4
2023-02-23 12:09:54,025 DEBUG TRAIN Batch 18/5400 loss 7.350443 loss_att 11.179834 loss_ctc 8.511172 loss_rnnt 6.184484 hw_loss 0.459970 lr 0.00040091 rank 6
2023-02-23 12:11:07,259 DEBUG TRAIN Batch 18/5500 loss 16.280764 loss_att 18.141401 loss_ctc 22.282619 loss_rnnt 14.908665 hw_loss 0.374483 lr 0.00040072 rank 3
2023-02-23 12:11:07,259 DEBUG TRAIN Batch 18/5500 loss 2.700448 loss_att 5.976784 loss_ctc 4.165287 loss_rnnt 1.631922 hw_loss 0.408652 lr 0.00040077 rank 2
2023-02-23 12:11:07,260 DEBUG TRAIN Batch 18/5500 loss 7.940279 loss_att 12.635173 loss_ctc 13.107439 loss_rnnt 6.072040 hw_loss 0.450574 lr 0.00040075 rank 7
2023-02-23 12:11:07,260 DEBUG TRAIN Batch 18/5500 loss 14.888736 loss_att 15.832703 loss_ctc 18.639330 loss_rnnt 13.954569 hw_loss 0.459927 lr 0.00040078 rank 6
2023-02-23 12:11:07,262 DEBUG TRAIN Batch 18/5500 loss 5.759981 loss_att 8.861155 loss_ctc 6.972747 loss_rnnt 4.775379 hw_loss 0.379998 lr 0.00040084 rank 1
2023-02-23 12:11:07,265 DEBUG TRAIN Batch 18/5500 loss 11.803916 loss_att 16.053022 loss_ctc 18.334633 loss_rnnt 9.867626 hw_loss 0.404449 lr 0.00040079 rank 5
2023-02-23 12:11:07,267 DEBUG TRAIN Batch 18/5500 loss 8.659895 loss_att 9.838470 loss_ctc 14.778791 loss_rnnt 7.401969 hw_loss 0.386921 lr 0.00040081 rank 4
2023-02-23 12:11:07,306 DEBUG TRAIN Batch 18/5500 loss 21.400639 loss_att 24.765430 loss_ctc 25.377783 loss_rnnt 19.997284 hw_loss 0.375202 lr 0.00040084 rank 0
2023-02-23 12:12:20,955 DEBUG TRAIN Batch 18/5600 loss 13.824248 loss_att 14.500772 loss_ctc 17.244829 loss_rnnt 12.977926 hw_loss 0.478013 lr 0.00040068 rank 4
2023-02-23 12:12:20,959 DEBUG TRAIN Batch 18/5600 loss 8.559242 loss_att 8.794105 loss_ctc 11.406751 loss_rnnt 7.823351 hw_loss 0.579845 lr 0.00040062 rank 7
2023-02-23 12:12:20,961 DEBUG TRAIN Batch 18/5600 loss 9.050584 loss_att 9.615368 loss_ctc 12.019119 loss_rnnt 8.286677 hw_loss 0.478397 lr 0.00040064 rank 2
2023-02-23 12:12:20,962 DEBUG TRAIN Batch 18/5600 loss 22.432775 loss_att 23.293871 loss_ctc 34.918282 loss_rnnt 20.369869 hw_loss 0.423660 lr 0.00040059 rank 3
2023-02-23 12:12:20,964 DEBUG TRAIN Batch 18/5600 loss 10.302785 loss_att 12.564739 loss_ctc 14.604538 loss_rnnt 8.994529 hw_loss 0.529309 lr 0.00040071 rank 1
2023-02-23 12:12:20,964 DEBUG TRAIN Batch 18/5600 loss 20.180351 loss_att 20.937872 loss_ctc 27.395941 loss_rnnt 18.839899 hw_loss 0.425382 lr 0.00040071 rank 0
2023-02-23 12:12:20,973 DEBUG TRAIN Batch 18/5600 loss 9.283007 loss_att 13.415653 loss_ctc 12.012576 loss_rnnt 7.897178 hw_loss 0.366295 lr 0.00040065 rank 6
2023-02-23 12:12:21,019 DEBUG TRAIN Batch 18/5600 loss 7.924815 loss_att 9.363686 loss_ctc 11.265792 loss_rnnt 7.000744 hw_loss 0.357811 lr 0.00040066 rank 5
2023-02-23 12:13:36,442 DEBUG TRAIN Batch 18/5700 loss 7.503372 loss_att 8.796074 loss_ctc 8.950682 loss_rnnt 6.803129 hw_loss 0.466363 lr 0.00040055 rank 4
2023-02-23 12:13:36,446 DEBUG TRAIN Batch 18/5700 loss 8.538027 loss_att 7.023193 loss_ctc 9.411918 loss_rnnt 8.226586 hw_loss 0.933539 lr 0.00040059 rank 0
2023-02-23 12:13:36,447 DEBUG TRAIN Batch 18/5700 loss 15.756704 loss_att 16.738327 loss_ctc 22.088537 loss_rnnt 14.533652 hw_loss 0.342157 lr 0.00040051 rank 2
2023-02-23 12:13:36,447 DEBUG TRAIN Batch 18/5700 loss 14.981249 loss_att 21.303720 loss_ctc 22.767147 loss_rnnt 12.472159 hw_loss 0.387139 lr 0.00040050 rank 7
2023-02-23 12:13:36,448 DEBUG TRAIN Batch 18/5700 loss 7.618506 loss_att 8.015567 loss_ctc 9.173948 loss_rnnt 6.987248 hw_loss 0.645853 lr 0.00040046 rank 3
2023-02-23 12:13:36,452 DEBUG TRAIN Batch 18/5700 loss 8.176763 loss_att 10.033373 loss_ctc 8.921599 loss_rnnt 7.446996 hw_loss 0.485874 lr 0.00040053 rank 5
2023-02-23 12:13:36,456 DEBUG TRAIN Batch 18/5700 loss 9.999856 loss_att 11.874479 loss_ctc 13.423208 loss_rnnt 8.832483 hw_loss 0.630002 lr 0.00040052 rank 6
2023-02-23 12:13:36,456 DEBUG TRAIN Batch 18/5700 loss 9.606149 loss_att 10.704400 loss_ctc 17.616560 loss_rnnt 8.045701 hw_loss 0.511391 lr 0.00040058 rank 1
2023-02-23 12:14:49,200 DEBUG TRAIN Batch 18/5800 loss 8.539549 loss_att 12.171734 loss_ctc 13.471107 loss_rnnt 6.943691 hw_loss 0.397273 lr 0.00040037 rank 7
2023-02-23 12:14:49,200 DEBUG TRAIN Batch 18/5800 loss 15.801154 loss_att 20.973612 loss_ctc 21.747894 loss_rnnt 13.770553 hw_loss 0.381021 lr 0.00040033 rank 3
2023-02-23 12:14:49,202 DEBUG TRAIN Batch 18/5800 loss 8.683187 loss_att 10.280859 loss_ctc 11.432223 loss_rnnt 7.758113 hw_loss 0.448130 lr 0.00040042 rank 4
2023-02-23 12:14:49,207 DEBUG TRAIN Batch 18/5800 loss 6.244643 loss_att 10.783454 loss_ctc 11.503489 loss_rnnt 4.484385 hw_loss 0.283718 lr 0.00040040 rank 5
2023-02-23 12:14:49,209 DEBUG TRAIN Batch 18/5800 loss 4.666614 loss_att 6.284062 loss_ctc 3.617997 loss_rnnt 4.244459 hw_loss 0.447151 lr 0.00040046 rank 0
2023-02-23 12:14:49,212 DEBUG TRAIN Batch 18/5800 loss 10.167775 loss_att 12.484779 loss_ctc 16.504562 loss_rnnt 8.636065 hw_loss 0.418885 lr 0.00040045 rank 1
2023-02-23 12:14:49,217 DEBUG TRAIN Batch 18/5800 loss 5.402146 loss_att 7.275213 loss_ctc 6.641300 loss_rnnt 4.608507 hw_loss 0.475885 lr 0.00040039 rank 2
2023-02-23 12:14:49,256 DEBUG TRAIN Batch 18/5800 loss 10.354731 loss_att 12.647522 loss_ctc 14.413410 loss_rnnt 9.183784 hw_loss 0.321057 lr 0.00040039 rank 6
2023-02-23 12:16:01,780 DEBUG TRAIN Batch 18/5900 loss 5.795543 loss_att 8.941896 loss_ctc 6.188295 loss_rnnt 4.920825 hw_loss 0.362025 lr 0.00040030 rank 4
2023-02-23 12:16:01,780 DEBUG TRAIN Batch 18/5900 loss 11.614562 loss_att 12.123714 loss_ctc 12.290114 loss_rnnt 11.144008 hw_loss 0.522468 lr 0.00040024 rank 7
2023-02-23 12:16:01,786 DEBUG TRAIN Batch 18/5900 loss 5.009434 loss_att 8.419041 loss_ctc 9.667068 loss_rnnt 3.527276 hw_loss 0.336036 lr 0.00040027 rank 6
2023-02-23 12:16:01,786 DEBUG TRAIN Batch 18/5900 loss 5.400105 loss_att 9.622617 loss_ctc 5.636396 loss_rnnt 4.304545 hw_loss 0.411659 lr 0.00040020 rank 3
2023-02-23 12:16:01,788 DEBUG TRAIN Batch 18/5900 loss 12.631104 loss_att 17.890022 loss_ctc 19.438803 loss_rnnt 10.432608 hw_loss 0.448163 lr 0.00040027 rank 5
2023-02-23 12:16:01,788 DEBUG TRAIN Batch 18/5900 loss 18.569906 loss_att 23.314535 loss_ctc 21.974819 loss_rnnt 16.933531 hw_loss 0.437740 lr 0.00040026 rank 2
2023-02-23 12:16:01,789 DEBUG TRAIN Batch 18/5900 loss 8.599581 loss_att 11.060686 loss_ctc 12.808372 loss_rnnt 7.356476 hw_loss 0.355709 lr 0.00040033 rank 0
2023-02-23 12:16:01,791 DEBUG TRAIN Batch 18/5900 loss 8.995646 loss_att 13.626272 loss_ctc 12.482807 loss_rnnt 7.405241 hw_loss 0.373733 lr 0.00040032 rank 1
2023-02-23 12:17:15,512 DEBUG TRAIN Batch 18/6000 loss 15.328068 loss_att 13.494986 loss_ctc 14.408362 loss_rnnt 15.661818 hw_loss 0.291553 lr 0.00040014 rank 6
2023-02-23 12:17:15,517 DEBUG TRAIN Batch 18/6000 loss 7.622488 loss_att 9.495639 loss_ctc 10.141783 loss_rnnt 6.720237 hw_loss 0.359467 lr 0.00040020 rank 1
2023-02-23 12:17:15,530 DEBUG TRAIN Batch 18/6000 loss 9.494349 loss_att 14.256294 loss_ctc 15.095692 loss_rnnt 7.539337 hw_loss 0.479581 lr 0.00040008 rank 3
2023-02-23 12:17:15,531 DEBUG TRAIN Batch 18/6000 loss 12.337368 loss_att 17.718700 loss_ctc 17.841780 loss_rnnt 10.363966 hw_loss 0.306024 lr 0.00040011 rank 7
2023-02-23 12:17:15,531 DEBUG TRAIN Batch 18/6000 loss 12.738364 loss_att 16.535774 loss_ctc 16.865967 loss_rnnt 11.202259 hw_loss 0.424269 lr 0.00040017 rank 4
2023-02-23 12:17:15,533 DEBUG TRAIN Batch 18/6000 loss 7.789339 loss_att 13.105576 loss_ctc 13.968827 loss_rnnt 5.633183 hw_loss 0.504331 lr 0.00040013 rank 2
2023-02-23 12:17:15,561 DEBUG TRAIN Batch 18/6000 loss 4.528238 loss_att 8.645830 loss_ctc 7.348865 loss_rnnt 3.125040 hw_loss 0.381743 lr 0.00040014 rank 5
2023-02-23 12:17:15,563 DEBUG TRAIN Batch 18/6000 loss 11.459174 loss_att 14.623714 loss_ctc 15.214011 loss_rnnt 10.094553 hw_loss 0.433253 lr 0.00040020 rank 0
2023-02-23 12:18:29,824 DEBUG TRAIN Batch 18/6100 loss 6.768903 loss_att 10.029525 loss_ctc 11.817911 loss_rnnt 5.174670 hw_loss 0.504201 lr 0.00039998 rank 7
2023-02-23 12:18:29,825 DEBUG TRAIN Batch 18/6100 loss 9.789705 loss_att 12.961349 loss_ctc 16.980078 loss_rnnt 7.912140 hw_loss 0.533475 lr 0.00040007 rank 1
2023-02-23 12:18:29,829 DEBUG TRAIN Batch 18/6100 loss 6.425648 loss_att 8.793165 loss_ctc 6.504563 loss_rnnt 5.720225 hw_loss 0.415119 lr 0.00040001 rank 6
2023-02-23 12:18:29,830 DEBUG TRAIN Batch 18/6100 loss 6.081864 loss_att 9.704693 loss_ctc 8.388877 loss_rnnt 4.803368 hw_loss 0.461865 lr 0.00039995 rank 3
2023-02-23 12:18:29,832 DEBUG TRAIN Batch 18/6100 loss 11.865397 loss_att 18.633295 loss_ctc 17.570047 loss_rnnt 9.493494 hw_loss 0.483196 lr 0.00040000 rank 2
2023-02-23 12:18:29,833 DEBUG TRAIN Batch 18/6100 loss 6.155428 loss_att 9.745710 loss_ctc 8.737327 loss_rnnt 4.843455 hw_loss 0.468119 lr 0.00040007 rank 0
2023-02-23 12:18:29,834 DEBUG TRAIN Batch 18/6100 loss 9.661223 loss_att 14.381948 loss_ctc 15.593225 loss_rnnt 7.702511 hw_loss 0.419313 lr 0.00040002 rank 5
2023-02-23 12:18:29,834 DEBUG TRAIN Batch 18/6100 loss 11.815014 loss_att 15.621794 loss_ctc 18.972803 loss_rnnt 9.870610 hw_loss 0.428766 lr 0.00040004 rank 4
2023-02-23 12:19:42,339 DEBUG TRAIN Batch 18/6200 loss 16.003798 loss_att 16.421253 loss_ctc 24.555655 loss_rnnt 14.570630 hw_loss 0.392678 lr 0.00039988 rank 6
2023-02-23 12:19:42,339 DEBUG TRAIN Batch 18/6200 loss 6.968914 loss_att 9.193549 loss_ctc 10.732877 loss_rnnt 5.795202 hw_loss 0.425481 lr 0.00039982 rank 3
2023-02-23 12:19:42,342 DEBUG TRAIN Batch 18/6200 loss 12.391540 loss_att 11.658261 loss_ctc 15.347561 loss_rnnt 11.806536 hw_loss 0.632855 lr 0.00039986 rank 7
2023-02-23 12:19:42,344 DEBUG TRAIN Batch 18/6200 loss 11.731683 loss_att 14.946826 loss_ctc 15.134087 loss_rnnt 10.423021 hw_loss 0.397461 lr 0.00039987 rank 2
2023-02-23 12:19:42,344 DEBUG TRAIN Batch 18/6200 loss 3.627491 loss_att 6.863234 loss_ctc 5.922606 loss_rnnt 2.473389 hw_loss 0.376760 lr 0.00039994 rank 0
2023-02-23 12:19:42,345 DEBUG TRAIN Batch 18/6200 loss 15.888303 loss_att 21.685270 loss_ctc 27.981150 loss_rnnt 12.919758 hw_loss 0.368949 lr 0.00039994 rank 1
2023-02-23 12:19:42,346 DEBUG TRAIN Batch 18/6200 loss 6.631547 loss_att 8.361846 loss_ctc 7.539544 loss_rnnt 5.951943 hw_loss 0.398396 lr 0.00039991 rank 4
2023-02-23 12:19:42,349 DEBUG TRAIN Batch 18/6200 loss 17.318134 loss_att 17.728712 loss_ctc 26.295345 loss_rnnt 15.814535 hw_loss 0.420980 lr 0.00039989 rank 5
2023-02-23 12:20:55,007 DEBUG TRAIN Batch 18/6300 loss 6.822773 loss_att 8.806416 loss_ctc 9.912784 loss_rnnt 5.749919 hw_loss 0.495231 lr 0.00039975 rank 2
2023-02-23 12:20:55,015 DEBUG TRAIN Batch 18/6300 loss 4.586938 loss_att 8.316614 loss_ctc 7.265649 loss_rnnt 3.263869 hw_loss 0.412447 lr 0.00039973 rank 7
2023-02-23 12:20:55,015 DEBUG TRAIN Batch 18/6300 loss 10.293884 loss_att 11.672197 loss_ctc 15.053293 loss_rnnt 9.104021 hw_loss 0.524273 lr 0.00039969 rank 3
2023-02-23 12:20:55,016 DEBUG TRAIN Batch 18/6300 loss 12.404776 loss_att 14.608744 loss_ctc 18.196821 loss_rnnt 11.007835 hw_loss 0.344762 lr 0.00039976 rank 5
2023-02-23 12:20:55,016 DEBUG TRAIN Batch 18/6300 loss 12.665799 loss_att 12.061120 loss_ctc 17.374502 loss_rnnt 11.932328 hw_loss 0.424835 lr 0.00039975 rank 6
2023-02-23 12:20:55,016 DEBUG TRAIN Batch 18/6300 loss 15.531033 loss_att 16.664429 loss_ctc 19.771460 loss_rnnt 14.475636 hw_loss 0.493739 lr 0.00039982 rank 0
2023-02-23 12:20:55,018 DEBUG TRAIN Batch 18/6300 loss 5.773262 loss_att 6.987859 loss_ctc 6.492677 loss_rnnt 5.208811 hw_loss 0.423017 lr 0.00039981 rank 1
2023-02-23 12:20:55,020 DEBUG TRAIN Batch 18/6300 loss 7.170422 loss_att 9.651535 loss_ctc 11.622293 loss_rnnt 5.817955 hw_loss 0.492489 lr 0.00039978 rank 4
2023-02-23 12:22:10,185 DEBUG TRAIN Batch 18/6400 loss 7.182794 loss_att 7.554695 loss_ctc 7.961366 loss_rnnt 6.640538 hw_loss 0.682624 lr 0.00039966 rank 4
2023-02-23 12:22:10,186 DEBUG TRAIN Batch 18/6400 loss 5.580019 loss_att 7.993208 loss_ctc 8.222481 loss_rnnt 4.485742 hw_loss 0.486209 lr 0.00039963 rank 6
2023-02-23 12:22:10,187 DEBUG TRAIN Batch 18/6400 loss 10.229894 loss_att 10.675632 loss_ctc 14.646280 loss_rnnt 9.224817 hw_loss 0.613269 lr 0.00039968 rank 1
2023-02-23 12:22:10,189 DEBUG TRAIN Batch 18/6400 loss 10.400328 loss_att 12.368794 loss_ctc 11.443355 loss_rnnt 9.659274 hw_loss 0.390545 lr 0.00039960 rank 7
2023-02-23 12:22:10,190 DEBUG TRAIN Batch 18/6400 loss 7.048807 loss_att 12.085101 loss_ctc 12.705748 loss_rnnt 5.060970 hw_loss 0.424348 lr 0.00039969 rank 0
2023-02-23 12:22:10,191 DEBUG TRAIN Batch 18/6400 loss 8.632269 loss_att 8.769282 loss_ctc 12.682371 loss_rnnt 7.741914 hw_loss 0.605509 lr 0.00039963 rank 5
2023-02-23 12:22:10,193 DEBUG TRAIN Batch 18/6400 loss 7.623646 loss_att 10.912432 loss_ctc 11.725828 loss_rnnt 6.221522 hw_loss 0.370144 lr 0.00039957 rank 3
2023-02-23 12:22:10,194 DEBUG TRAIN Batch 18/6400 loss 4.045612 loss_att 7.919260 loss_ctc 8.296968 loss_rnnt 2.464494 hw_loss 0.449139 lr 0.00039962 rank 2
2023-02-23 12:23:22,519 DEBUG TRAIN Batch 18/6500 loss 10.541985 loss_att 16.860386 loss_ctc 16.368292 loss_rnnt 8.206882 hw_loss 0.552341 lr 0.00039949 rank 2
2023-02-23 12:23:22,520 DEBUG TRAIN Batch 18/6500 loss 6.486213 loss_att 8.866133 loss_ctc 7.756785 loss_rnnt 5.652258 hw_loss 0.353552 lr 0.00039951 rank 5
2023-02-23 12:23:22,524 DEBUG TRAIN Batch 18/6500 loss 17.536533 loss_att 21.487844 loss_ctc 29.628662 loss_rnnt 14.927550 hw_loss 0.387066 lr 0.00039956 rank 0
2023-02-23 12:23:22,524 DEBUG TRAIN Batch 18/6500 loss 13.317986 loss_att 15.385099 loss_ctc 19.414137 loss_rnnt 11.817366 hw_loss 0.514456 lr 0.00039947 rank 7
2023-02-23 12:23:22,525 DEBUG TRAIN Batch 18/6500 loss 10.972511 loss_att 15.321770 loss_ctc 11.010894 loss_rnnt 9.896834 hw_loss 0.376327 lr 0.00039953 rank 4
2023-02-23 12:23:22,527 DEBUG TRAIN Batch 18/6500 loss 23.821127 loss_att 30.814352 loss_ctc 35.297104 loss_rnnt 20.652639 hw_loss 0.449464 lr 0.00039944 rank 3
2023-02-23 12:23:22,527 DEBUG TRAIN Batch 18/6500 loss 8.019556 loss_att 9.554843 loss_ctc 13.010443 loss_rnnt 6.786605 hw_loss 0.488329 lr 0.00039956 rank 1
2023-02-23 12:23:22,532 DEBUG TRAIN Batch 18/6500 loss 6.727303 loss_att 10.425031 loss_ctc 5.815292 loss_rnnt 5.890590 hw_loss 0.410192 lr 0.00039950 rank 6
2023-02-23 12:24:35,506 DEBUG TRAIN Batch 18/6600 loss 3.365908 loss_att 8.193208 loss_ctc 3.764147 loss_rnnt 2.137176 hw_loss 0.394074 lr 0.00039936 rank 2
2023-02-23 12:24:35,507 DEBUG TRAIN Batch 18/6600 loss 4.500398 loss_att 7.360359 loss_ctc 5.980656 loss_rnnt 3.507273 hw_loss 0.419559 lr 0.00039931 rank 3
2023-02-23 12:24:35,510 DEBUG TRAIN Batch 18/6600 loss 8.087212 loss_att 10.384394 loss_ctc 10.115761 loss_rnnt 7.115912 hw_loss 0.452606 lr 0.00039934 rank 7
2023-02-23 12:24:35,512 DEBUG TRAIN Batch 18/6600 loss 5.178232 loss_att 8.739704 loss_ctc 5.283674 loss_rnnt 4.240239 hw_loss 0.396824 lr 0.00039937 rank 6
2023-02-23 12:24:35,512 DEBUG TRAIN Batch 18/6600 loss 11.422824 loss_att 14.434017 loss_ctc 13.752207 loss_rnnt 10.329560 hw_loss 0.338325 lr 0.00039943 rank 1
2023-02-23 12:24:35,513 DEBUG TRAIN Batch 18/6600 loss 17.728918 loss_att 21.083557 loss_ctc 22.008705 loss_rnnt 16.287697 hw_loss 0.374351 lr 0.00039940 rank 4
2023-02-23 12:24:35,515 DEBUG TRAIN Batch 18/6600 loss 5.553801 loss_att 9.571101 loss_ctc 9.404387 loss_rnnt 4.024470 hw_loss 0.398362 lr 0.00039938 rank 5
2023-02-23 12:24:35,517 DEBUG TRAIN Batch 18/6600 loss 9.267890 loss_att 11.784640 loss_ctc 14.375978 loss_rnnt 7.898444 hw_loss 0.346908 lr 0.00039943 rank 0
2023-02-23 12:25:48,826 DEBUG TRAIN Batch 18/6700 loss 7.077713 loss_att 10.399420 loss_ctc 12.748848 loss_rnnt 5.455094 hw_loss 0.378985 lr 0.00039931 rank 0
2023-02-23 12:25:48,828 DEBUG TRAIN Batch 18/6700 loss 3.476457 loss_att 6.601992 loss_ctc 5.525000 loss_rnnt 2.392705 hw_loss 0.347824 lr 0.00039924 rank 6
2023-02-23 12:25:48,834 DEBUG TRAIN Batch 18/6700 loss 8.917069 loss_att 10.993431 loss_ctc 16.729538 loss_rnnt 7.233026 hw_loss 0.425830 lr 0.00039922 rank 7
2023-02-23 12:25:48,836 DEBUG TRAIN Batch 18/6700 loss 3.970291 loss_att 5.934787 loss_ctc 4.397725 loss_rnnt 3.308905 hw_loss 0.396555 lr 0.00039927 rank 4
2023-02-23 12:25:48,840 DEBUG TRAIN Batch 18/6700 loss 17.637365 loss_att 22.316189 loss_ctc 24.465641 loss_rnnt 15.612364 hw_loss 0.335249 lr 0.00039925 rank 5
2023-02-23 12:25:48,842 DEBUG TRAIN Batch 18/6700 loss 4.772687 loss_att 8.925299 loss_ctc 5.680189 loss_rnnt 3.585483 hw_loss 0.441903 lr 0.00039930 rank 1
2023-02-23 12:25:48,843 DEBUG TRAIN Batch 18/6700 loss 7.496428 loss_att 12.720214 loss_ctc 12.703975 loss_rnnt 5.542618 hw_loss 0.402588 lr 0.00039918 rank 3
2023-02-23 12:25:48,850 DEBUG TRAIN Batch 18/6700 loss 9.296913 loss_att 13.399090 loss_ctc 14.888572 loss_rnnt 7.519590 hw_loss 0.396249 lr 0.00039924 rank 2
2023-02-23 12:27:02,846 DEBUG TRAIN Batch 18/6800 loss 14.064957 loss_att 16.488184 loss_ctc 20.128700 loss_rnnt 12.534396 hw_loss 0.445157 lr 0.00039909 rank 7
2023-02-23 12:27:02,855 DEBUG TRAIN Batch 18/6800 loss 6.647609 loss_att 11.124685 loss_ctc 7.160478 loss_rnnt 5.489340 hw_loss 0.364634 lr 0.00039912 rank 6
2023-02-23 12:27:02,857 DEBUG TRAIN Batch 18/6800 loss 5.949401 loss_att 7.501396 loss_ctc 9.590159 loss_rnnt 4.914985 hw_loss 0.447343 lr 0.00039912 rank 5
2023-02-23 12:27:02,858 DEBUG TRAIN Batch 18/6800 loss 6.235395 loss_att 10.391333 loss_ctc 9.733055 loss_rnnt 4.675555 hw_loss 0.491809 lr 0.00039915 rank 4
2023-02-23 12:27:02,858 DEBUG TRAIN Batch 18/6800 loss 8.043864 loss_att 12.717872 loss_ctc 11.143908 loss_rnnt 6.512588 hw_loss 0.343381 lr 0.00039906 rank 3
2023-02-23 12:27:02,863 DEBUG TRAIN Batch 18/6800 loss 15.968635 loss_att 17.101570 loss_ctc 15.831510 loss_rnnt 15.526192 hw_loss 0.439012 lr 0.00039918 rank 0
2023-02-23 12:27:02,863 DEBUG TRAIN Batch 18/6800 loss 10.720717 loss_att 13.562917 loss_ctc 12.490726 loss_rnnt 9.708841 hw_loss 0.388940 lr 0.00039917 rank 1
2023-02-23 12:27:02,867 DEBUG TRAIN Batch 18/6800 loss 8.777479 loss_att 11.589096 loss_ctc 11.286339 loss_rnnt 7.640302 hw_loss 0.450636 lr 0.00039911 rank 2
2023-02-23 12:28:14,906 DEBUG TRAIN Batch 18/6900 loss 5.312508 loss_att 9.032620 loss_ctc 5.231514 loss_rnnt 4.314957 hw_loss 0.495612 lr 0.00039896 rank 7
2023-02-23 12:28:14,907 DEBUG TRAIN Batch 18/6900 loss 7.228476 loss_att 9.073840 loss_ctc 8.938271 loss_rnnt 6.346556 hw_loss 0.534139 lr 0.00039898 rank 2
2023-02-23 12:28:14,908 DEBUG TRAIN Batch 18/6900 loss 10.393504 loss_att 12.124246 loss_ctc 12.555992 loss_rnnt 9.520252 hw_loss 0.447697 lr 0.00039899 rank 6
2023-02-23 12:28:14,911 DEBUG TRAIN Batch 18/6900 loss 9.141138 loss_att 11.036743 loss_ctc 12.129435 loss_rnnt 8.151198 hw_loss 0.398212 lr 0.00039893 rank 3
2023-02-23 12:28:14,914 DEBUG TRAIN Batch 18/6900 loss 8.518552 loss_att 9.433901 loss_ctc 9.612923 loss_rnnt 7.884977 hw_loss 0.571105 lr 0.00039905 rank 0
2023-02-23 12:28:14,914 DEBUG TRAIN Batch 18/6900 loss 14.955879 loss_att 16.294449 loss_ctc 20.126690 loss_rnnt 13.742409 hw_loss 0.480593 lr 0.00039902 rank 4
2023-02-23 12:28:14,917 DEBUG TRAIN Batch 18/6900 loss 11.092709 loss_att 11.771248 loss_ctc 14.639061 loss_rnnt 10.252099 hw_loss 0.435102 lr 0.00039900 rank 5
2023-02-23 12:28:14,965 DEBUG TRAIN Batch 18/6900 loss 6.426877 loss_att 9.022762 loss_ctc 10.338587 loss_rnnt 5.134884 hw_loss 0.471101 lr 0.00039905 rank 1
2023-02-23 12:29:27,850 DEBUG TRAIN Batch 18/7000 loss 12.212960 loss_att 14.285712 loss_ctc 16.307198 loss_rnnt 11.083307 hw_loss 0.317256 lr 0.00039884 rank 7
2023-02-23 12:29:27,854 DEBUG TRAIN Batch 18/7000 loss 6.536986 loss_att 7.782349 loss_ctc 6.367414 loss_rnnt 6.119279 hw_loss 0.358582 lr 0.00039880 rank 3
2023-02-23 12:29:27,857 DEBUG TRAIN Batch 18/7000 loss 5.603726 loss_att 11.442930 loss_ctc 9.641773 loss_rnnt 3.703009 hw_loss 0.364630 lr 0.00039885 rank 2
2023-02-23 12:29:27,858 DEBUG TRAIN Batch 18/7000 loss 14.194056 loss_att 16.281242 loss_ctc 19.544920 loss_rnnt 12.845634 hw_loss 0.407880 lr 0.00039893 rank 0
2023-02-23 12:29:27,859 DEBUG TRAIN Batch 18/7000 loss 8.228217 loss_att 10.710678 loss_ctc 12.757552 loss_rnnt 6.887119 hw_loss 0.451303 lr 0.00039887 rank 5
2023-02-23 12:29:27,859 DEBUG TRAIN Batch 18/7000 loss 10.451447 loss_att 13.195204 loss_ctc 14.923858 loss_rnnt 9.061458 hw_loss 0.459220 lr 0.00039886 rank 6
2023-02-23 12:29:27,860 DEBUG TRAIN Batch 18/7000 loss 5.651715 loss_att 7.090771 loss_ctc 7.649534 loss_rnnt 4.839605 hw_loss 0.483606 lr 0.00039889 rank 4
2023-02-23 12:29:27,908 DEBUG TRAIN Batch 18/7000 loss 9.369303 loss_att 9.575003 loss_ctc 10.887813 loss_rnnt 8.829988 hw_loss 0.554449 lr 0.00039892 rank 1
2023-02-23 12:30:42,681 DEBUG TRAIN Batch 18/7100 loss 14.312583 loss_att 18.135847 loss_ctc 19.679878 loss_rnnt 12.626468 hw_loss 0.385920 lr 0.00039877 rank 4
2023-02-23 12:30:42,684 DEBUG TRAIN Batch 18/7100 loss 12.240329 loss_att 17.905146 loss_ctc 17.846418 loss_rnnt 10.151417 hw_loss 0.390880 lr 0.00039873 rank 2
2023-02-23 12:30:42,686 DEBUG TRAIN Batch 18/7100 loss 3.565424 loss_att 6.289690 loss_ctc 5.699843 loss_rnnt 2.531889 hw_loss 0.382674 lr 0.00039879 rank 1
2023-02-23 12:30:42,687 DEBUG TRAIN Batch 18/7100 loss 6.331502 loss_att 8.525804 loss_ctc 7.963188 loss_rnnt 5.475130 hw_loss 0.374914 lr 0.00039868 rank 3
2023-02-23 12:30:42,687 DEBUG TRAIN Batch 18/7100 loss 3.313012 loss_att 7.556295 loss_ctc 5.234052 loss_rnnt 1.962950 hw_loss 0.459874 lr 0.00039871 rank 7
2023-02-23 12:30:42,693 DEBUG TRAIN Batch 18/7100 loss 12.281921 loss_att 13.682936 loss_ctc 16.458452 loss_rnnt 11.219565 hw_loss 0.422403 lr 0.00039880 rank 0
2023-02-23 12:30:42,709 DEBUG TRAIN Batch 18/7100 loss 10.704543 loss_att 17.612350 loss_ctc 16.306913 loss_rnnt 8.365211 hw_loss 0.395229 lr 0.00039874 rank 5
2023-02-23 12:30:42,721 DEBUG TRAIN Batch 18/7100 loss 7.623192 loss_att 11.280292 loss_ctc 10.500385 loss_rnnt 6.265939 hw_loss 0.454139 lr 0.00039874 rank 6
2023-02-23 12:31:56,117 DEBUG TRAIN Batch 18/7200 loss 8.921000 loss_att 13.837210 loss_ctc 15.254748 loss_rnnt 6.861770 hw_loss 0.434041 lr 0.00039858 rank 7
2023-02-23 12:31:56,117 DEBUG TRAIN Batch 18/7200 loss 9.810138 loss_att 12.065279 loss_ctc 13.598158 loss_rnnt 8.653791 hw_loss 0.375467 lr 0.00039855 rank 3
2023-02-23 12:31:56,123 DEBUG TRAIN Batch 18/7200 loss 7.826829 loss_att 9.812700 loss_ctc 8.493490 loss_rnnt 7.093641 hw_loss 0.463360 lr 0.00039861 rank 6
2023-02-23 12:31:56,124 DEBUG TRAIN Batch 18/7200 loss 11.015301 loss_att 12.400057 loss_ctc 14.059821 loss_rnnt 10.115696 hw_loss 0.406347 lr 0.00039862 rank 5
2023-02-23 12:31:56,124 DEBUG TRAIN Batch 18/7200 loss 11.123256 loss_att 13.304521 loss_ctc 19.798529 loss_rnnt 9.334215 hw_loss 0.367659 lr 0.00039867 rank 1
2023-02-23 12:31:56,125 DEBUG TRAIN Batch 18/7200 loss 3.900914 loss_att 8.106606 loss_ctc 5.374300 loss_rnnt 2.640234 hw_loss 0.418294 lr 0.00039864 rank 4
2023-02-23 12:31:56,126 DEBUG TRAIN Batch 18/7200 loss 6.778029 loss_att 12.049562 loss_ctc 8.434864 loss_rnnt 5.294319 hw_loss 0.390924 lr 0.00039860 rank 2
2023-02-23 12:31:56,127 DEBUG TRAIN Batch 18/7200 loss 7.644889 loss_att 13.222727 loss_ctc 11.189194 loss_rnnt 5.841810 hw_loss 0.403009 lr 0.00039867 rank 0
2023-02-23 12:33:10,081 DEBUG TRAIN Batch 18/7300 loss 20.662922 loss_att 21.292175 loss_ctc 25.623066 loss_rnnt 19.695311 hw_loss 0.338266 lr 0.00039846 rank 7
2023-02-23 12:33:10,099 DEBUG TRAIN Batch 18/7300 loss 12.855575 loss_att 14.734516 loss_ctc 15.688729 loss_rnnt 11.898241 hw_loss 0.382108 lr 0.00039847 rank 2
2023-02-23 12:33:10,101 DEBUG TRAIN Batch 18/7300 loss 7.605020 loss_att 10.079803 loss_ctc 9.633461 loss_rnnt 6.628539 hw_loss 0.395749 lr 0.00039842 rank 3
2023-02-23 12:33:10,103 DEBUG TRAIN Batch 18/7300 loss 9.454903 loss_att 13.855597 loss_ctc 14.689809 loss_rnnt 7.698677 hw_loss 0.333936 lr 0.00039854 rank 1
2023-02-23 12:33:10,103 DEBUG TRAIN Batch 18/7300 loss 5.813242 loss_att 8.536585 loss_ctc 7.272760 loss_rnnt 4.840579 hw_loss 0.437611 lr 0.00039854 rank 0
2023-02-23 12:33:10,104 DEBUG TRAIN Batch 18/7300 loss 15.865910 loss_att 17.036034 loss_ctc 20.009436 loss_rnnt 14.858963 hw_loss 0.413346 lr 0.00039848 rank 6
2023-02-23 12:33:10,106 DEBUG TRAIN Batch 18/7300 loss 5.501007 loss_att 10.532698 loss_ctc 6.305280 loss_rnnt 4.187799 hw_loss 0.374313 lr 0.00039849 rank 5
2023-02-23 12:33:10,149 DEBUG TRAIN Batch 18/7300 loss 5.197676 loss_att 10.547381 loss_ctc 8.622406 loss_rnnt 3.478942 hw_loss 0.360304 lr 0.00039851 rank 4
2023-02-23 12:34:23,805 DEBUG TRAIN Batch 18/7400 loss 8.435041 loss_att 9.988754 loss_ctc 7.651373 loss_rnnt 8.022983 hw_loss 0.385886 lr 0.00039841 rank 1
2023-02-23 12:34:23,824 DEBUG TRAIN Batch 18/7400 loss 23.112417 loss_att 27.340528 loss_ctc 28.933109 loss_rnnt 21.271713 hw_loss 0.410606 lr 0.00039833 rank 7
2023-02-23 12:34:23,831 DEBUG TRAIN Batch 18/7400 loss 3.512548 loss_att 6.408431 loss_ctc 6.322437 loss_rnnt 2.327509 hw_loss 0.433521 lr 0.00039830 rank 3
2023-02-23 12:34:23,833 DEBUG TRAIN Batch 18/7400 loss 8.631281 loss_att 11.050010 loss_ctc 14.085321 loss_rnnt 7.177014 hw_loss 0.456217 lr 0.00039835 rank 2
2023-02-23 12:34:23,857 DEBUG TRAIN Batch 18/7400 loss 15.404376 loss_att 18.913567 loss_ctc 23.884586 loss_rnnt 13.380291 hw_loss 0.359159 lr 0.00039836 rank 5
2023-02-23 12:34:23,856 DEBUG TRAIN Batch 18/7400 loss 5.994945 loss_att 8.716827 loss_ctc 9.026352 loss_rnnt 4.842553 hw_loss 0.382176 lr 0.00039839 rank 4
2023-02-23 12:34:23,858 DEBUG TRAIN Batch 18/7400 loss 10.638374 loss_att 13.711847 loss_ctc 14.371339 loss_rnnt 9.334040 hw_loss 0.359834 lr 0.00039842 rank 0
2023-02-23 12:34:23,859 DEBUG TRAIN Batch 18/7400 loss 6.615748 loss_att 8.947938 loss_ctc 7.625111 loss_rnnt 5.810951 hw_loss 0.382081 lr 0.00039836 rank 6
2023-02-23 12:35:38,172 DEBUG TRAIN Batch 18/7500 loss 11.181788 loss_att 13.551405 loss_ctc 15.181610 loss_rnnt 9.947883 hw_loss 0.425012 lr 0.00039829 rank 0
2023-02-23 12:35:38,182 DEBUG TRAIN Batch 18/7500 loss 7.677898 loss_att 12.851543 loss_ctc 12.312602 loss_rnnt 5.852074 hw_loss 0.324628 lr 0.00039826 rank 4
2023-02-23 12:35:38,184 DEBUG TRAIN Batch 18/7500 loss 8.924927 loss_att 11.550169 loss_ctc 12.340845 loss_rnnt 7.715882 hw_loss 0.428513 lr 0.00039817 rank 3
2023-02-23 12:35:38,185 DEBUG TRAIN Batch 18/7500 loss 17.433668 loss_att 18.641964 loss_ctc 17.956648 loss_rnnt 16.909031 hw_loss 0.399840 lr 0.00039824 rank 5
2023-02-23 12:35:38,186 DEBUG TRAIN Batch 18/7500 loss 11.355296 loss_att 12.530615 loss_ctc 14.187048 loss_rnnt 10.451214 hw_loss 0.546473 lr 0.00039820 rank 7
2023-02-23 12:35:38,189 DEBUG TRAIN Batch 18/7500 loss 3.703855 loss_att 5.758148 loss_ctc 5.720252 loss_rnnt 2.763275 hw_loss 0.489127 lr 0.00039822 rank 2
2023-02-23 12:35:38,191 DEBUG TRAIN Batch 18/7500 loss 12.983836 loss_att 13.141553 loss_ctc 16.284918 loss_rnnt 12.267348 hw_loss 0.459001 lr 0.00039829 rank 1
2023-02-23 12:35:38,191 DEBUG TRAIN Batch 18/7500 loss 3.455704 loss_att 5.226840 loss_ctc 5.024354 loss_rnnt 2.669448 hw_loss 0.417892 lr 0.00039823 rank 6
2023-02-23 12:36:50,617 DEBUG TRAIN Batch 18/7600 loss 16.287704 loss_att 17.112173 loss_ctc 19.894552 loss_rnnt 15.374903 hw_loss 0.500611 lr 0.00039813 rank 4
2023-02-23 12:36:50,617 DEBUG TRAIN Batch 18/7600 loss 8.779821 loss_att 9.528646 loss_ctc 16.057747 loss_rnnt 7.453422 hw_loss 0.386708 lr 0.00039808 rank 7
2023-02-23 12:36:50,619 DEBUG TRAIN Batch 18/7600 loss 14.831981 loss_att 16.297119 loss_ctc 20.887886 loss_rnnt 13.448930 hw_loss 0.529816 lr 0.00039804 rank 3
2023-02-23 12:36:50,621 DEBUG TRAIN Batch 18/7600 loss 21.233572 loss_att 21.881367 loss_ctc 28.609634 loss_rnnt 19.886000 hw_loss 0.439759 lr 0.00039810 rank 6
2023-02-23 12:36:50,622 DEBUG TRAIN Batch 18/7600 loss 19.780727 loss_att 19.862545 loss_ctc 23.730631 loss_rnnt 19.001654 hw_loss 0.442607 lr 0.00039816 rank 1
2023-02-23 12:36:50,622 DEBUG TRAIN Batch 18/7600 loss 9.666348 loss_att 11.853927 loss_ctc 13.711157 loss_rnnt 8.433235 hw_loss 0.480545 lr 0.00039809 rank 2
2023-02-23 12:36:50,625 DEBUG TRAIN Batch 18/7600 loss 5.975294 loss_att 9.534365 loss_ctc 6.734258 loss_rnnt 4.954576 hw_loss 0.389452 lr 0.00039811 rank 5
2023-02-23 12:36:50,627 DEBUG TRAIN Batch 18/7600 loss 9.011995 loss_att 8.665806 loss_ctc 11.774148 loss_rnnt 8.407598 hw_loss 0.572526 lr 0.00039817 rank 0
2023-02-23 12:38:02,762 DEBUG TRAIN Batch 18/7700 loss 6.912160 loss_att 7.317625 loss_ctc 9.461583 loss_rnnt 6.229656 hw_loss 0.490290 lr 0.00039801 rank 4
2023-02-23 12:38:02,766 DEBUG TRAIN Batch 18/7700 loss 3.181481 loss_att 6.012521 loss_ctc 5.406513 loss_rnnt 2.156593 hw_loss 0.303767 lr 0.00039792 rank 3
2023-02-23 12:38:02,766 DEBUG TRAIN Batch 18/7700 loss 8.401495 loss_att 9.128936 loss_ctc 10.944183 loss_rnnt 7.599279 hw_loss 0.595693 lr 0.00039798 rank 6
2023-02-23 12:38:02,765 DEBUG TRAIN Batch 18/7700 loss 10.291279 loss_att 12.768922 loss_ctc 15.941962 loss_rnnt 8.817080 hw_loss 0.422336 lr 0.00039804 rank 0
2023-02-23 12:38:02,767 DEBUG TRAIN Batch 18/7700 loss 19.149464 loss_att 19.115181 loss_ctc 22.292816 loss_rnnt 18.506817 hw_loss 0.431981 lr 0.00039798 rank 5
2023-02-23 12:38:02,768 DEBUG TRAIN Batch 18/7700 loss 5.150788 loss_att 10.538574 loss_ctc 7.276868 loss_rnnt 3.594849 hw_loss 0.365444 lr 0.00039803 rank 1
2023-02-23 12:38:02,768 DEBUG TRAIN Batch 18/7700 loss 11.735297 loss_att 14.041805 loss_ctc 12.214342 loss_rnnt 11.031109 hw_loss 0.335652 lr 0.00039795 rank 7
2023-02-23 12:38:02,775 DEBUG TRAIN Batch 18/7700 loss 7.060410 loss_att 9.331313 loss_ctc 11.540629 loss_rnnt 5.832270 hw_loss 0.331120 lr 0.00039797 rank 2
2023-02-23 12:39:17,326 DEBUG TRAIN Batch 18/7800 loss 6.412840 loss_att 10.917628 loss_ctc 10.918798 loss_rnnt 4.715463 hw_loss 0.366797 lr 0.00039788 rank 4
2023-02-23 12:39:17,327 DEBUG TRAIN Batch 18/7800 loss 14.255778 loss_att 14.951796 loss_ctc 22.479813 loss_rnnt 12.749128 hw_loss 0.507955 lr 0.00039784 rank 2
2023-02-23 12:39:17,339 DEBUG TRAIN Batch 18/7800 loss 8.051097 loss_att 12.098343 loss_ctc 10.756643 loss_rnnt 6.646284 hw_loss 0.439919 lr 0.00039783 rank 7
2023-02-23 12:39:17,342 DEBUG TRAIN Batch 18/7800 loss 3.582904 loss_att 5.237619 loss_ctc 5.572179 loss_rnnt 2.744239 hw_loss 0.454661 lr 0.00039779 rank 3
2023-02-23 12:39:17,344 DEBUG TRAIN Batch 18/7800 loss 9.189273 loss_att 12.688541 loss_ctc 11.813681 loss_rnnt 7.943039 hw_loss 0.368362 lr 0.00039791 rank 1
2023-02-23 12:39:17,347 DEBUG TRAIN Batch 18/7800 loss 8.304749 loss_att 14.633350 loss_ctc 9.714750 loss_rnnt 6.703235 hw_loss 0.277114 lr 0.00039786 rank 5
2023-02-23 12:39:17,347 DEBUG TRAIN Batch 18/7800 loss 8.418067 loss_att 12.882654 loss_ctc 12.898029 loss_rnnt 6.740475 hw_loss 0.351273 lr 0.00039791 rank 0
2023-02-23 12:39:17,383 DEBUG TRAIN Batch 18/7800 loss 14.849951 loss_att 17.725731 loss_ctc 19.609364 loss_rnnt 13.458076 hw_loss 0.341491 lr 0.00039785 rank 6
2023-02-23 12:40:30,286 DEBUG TRAIN Batch 18/7900 loss 18.118582 loss_att 18.989998 loss_ctc 27.773123 loss_rnnt 16.481869 hw_loss 0.328422 lr 0.00039770 rank 7
2023-02-23 12:40:30,288 DEBUG TRAIN Batch 18/7900 loss 14.143051 loss_att 14.172543 loss_ctc 18.591835 loss_rnnt 13.370256 hw_loss 0.325735 lr 0.00039772 rank 2
2023-02-23 12:40:30,289 DEBUG TRAIN Batch 18/7900 loss 15.297305 loss_att 16.796955 loss_ctc 16.737658 loss_rnnt 14.643176 hw_loss 0.304035 lr 0.00039775 rank 4
2023-02-23 12:40:30,289 DEBUG TRAIN Batch 18/7900 loss 7.350592 loss_att 9.132402 loss_ctc 11.601711 loss_rnnt 6.217909 hw_loss 0.392822 lr 0.00039767 rank 3
2023-02-23 12:40:30,292 DEBUG TRAIN Batch 18/7900 loss 10.728168 loss_att 15.059950 loss_ctc 13.310109 loss_rnnt 9.320230 hw_loss 0.369978 lr 0.00039779 rank 0
2023-02-23 12:40:30,297 DEBUG TRAIN Batch 18/7900 loss 6.491971 loss_att 10.230183 loss_ctc 8.974174 loss_rnnt 5.216296 hw_loss 0.369511 lr 0.00039773 rank 5
2023-02-23 12:40:30,296 DEBUG TRAIN Batch 18/7900 loss 19.880650 loss_att 24.435102 loss_ctc 24.302038 loss_rnnt 18.077461 hw_loss 0.567709 lr 0.00039778 rank 1
2023-02-23 12:40:30,340 DEBUG TRAIN Batch 18/7900 loss 9.758117 loss_att 13.639954 loss_ctc 17.260050 loss_rnnt 7.752431 hw_loss 0.429487 lr 0.00039773 rank 6
2023-02-23 12:41:42,965 DEBUG TRAIN Batch 18/8000 loss 4.660518 loss_att 5.343922 loss_ctc 6.152546 loss_rnnt 4.084133 hw_loss 0.451440 lr 0.00039759 rank 2
2023-02-23 12:41:42,966 DEBUG TRAIN Batch 18/8000 loss 4.150393 loss_att 6.685696 loss_ctc 5.023030 loss_rnnt 3.292873 hw_loss 0.438954 lr 0.00039766 rank 1
2023-02-23 12:41:42,967 DEBUG TRAIN Batch 18/8000 loss 8.066403 loss_att 12.031940 loss_ctc 9.997809 loss_rnnt 6.797474 hw_loss 0.409315 lr 0.00039757 rank 7
2023-02-23 12:41:42,966 DEBUG TRAIN Batch 18/8000 loss 9.159960 loss_att 10.018865 loss_ctc 13.192551 loss_rnnt 8.267910 hw_loss 0.342356 lr 0.00039766 rank 0
2023-02-23 12:41:42,968 DEBUG TRAIN Batch 18/8000 loss 5.960830 loss_att 8.832114 loss_ctc 10.140774 loss_rnnt 4.621839 hw_loss 0.388891 lr 0.00039763 rank 4
2023-02-23 12:41:42,971 DEBUG TRAIN Batch 18/8000 loss 14.327469 loss_att 16.217430 loss_ctc 20.584412 loss_rnnt 12.894559 hw_loss 0.413734 lr 0.00039754 rank 3
2023-02-23 12:41:42,974 DEBUG TRAIN Batch 18/8000 loss 4.767693 loss_att 7.592453 loss_ctc 6.536007 loss_rnnt 3.781224 hw_loss 0.348266 lr 0.00039760 rank 6
2023-02-23 12:41:42,978 DEBUG TRAIN Batch 18/8000 loss 12.556346 loss_att 17.530212 loss_ctc 19.880440 loss_rnnt 10.383389 hw_loss 0.378069 lr 0.00039761 rank 5
2023-02-23 12:42:55,278 DEBUG TRAIN Batch 18/8100 loss 5.685530 loss_att 7.205800 loss_ctc 8.483376 loss_rnnt 4.711745 hw_loss 0.556284 lr 0.00039745 rank 7
2023-02-23 12:42:55,277 DEBUG TRAIN Batch 18/8100 loss 8.597231 loss_att 12.184007 loss_ctc 11.758581 loss_rnnt 7.215355 hw_loss 0.455639 lr 0.00039747 rank 2
2023-02-23 12:42:55,281 DEBUG TRAIN Batch 18/8100 loss 10.754608 loss_att 15.810392 loss_ctc 16.984701 loss_rnnt 8.716210 hw_loss 0.368552 lr 0.00039753 rank 1
2023-02-23 12:42:55,281 DEBUG TRAIN Batch 18/8100 loss 9.381320 loss_att 13.296425 loss_ctc 12.808557 loss_rnnt 7.887048 hw_loss 0.476788 lr 0.00039741 rank 3
2023-02-23 12:42:55,285 DEBUG TRAIN Batch 18/8100 loss 7.027098 loss_att 11.274565 loss_ctc 9.054674 loss_rnnt 5.666880 hw_loss 0.450713 lr 0.00039748 rank 5
2023-02-23 12:42:55,285 DEBUG TRAIN Batch 18/8100 loss 13.656539 loss_att 19.296543 loss_ctc 20.303694 loss_rnnt 11.419404 hw_loss 0.417836 lr 0.00039750 rank 4
2023-02-23 12:42:55,287 DEBUG TRAIN Batch 18/8100 loss 13.096204 loss_att 15.272068 loss_ctc 16.322977 loss_rnnt 12.019060 hw_loss 0.397000 lr 0.00039754 rank 0
2023-02-23 12:42:55,289 DEBUG TRAIN Batch 18/8100 loss 7.653363 loss_att 10.443389 loss_ctc 10.510892 loss_rnnt 6.530352 hw_loss 0.345004 lr 0.00039747 rank 6
2023-02-23 12:44:08,298 DEBUG TRAIN Batch 18/8200 loss 16.295158 loss_att 18.454119 loss_ctc 23.541422 loss_rnnt 14.631742 hw_loss 0.497734 lr 0.00039729 rank 3
2023-02-23 12:44:08,298 DEBUG TRAIN Batch 18/8200 loss 16.823887 loss_att 13.547503 loss_ctc 14.596001 loss_rnnt 17.588505 hw_loss 0.351954 lr 0.00039732 rank 7
2023-02-23 12:44:08,300 DEBUG TRAIN Batch 18/8200 loss 15.221244 loss_att 15.727631 loss_ctc 18.602268 loss_rnnt 14.426308 hw_loss 0.455353 lr 0.00039738 rank 4
2023-02-23 12:44:08,303 DEBUG TRAIN Batch 18/8200 loss 6.311637 loss_att 8.120691 loss_ctc 8.367058 loss_rnnt 5.383462 hw_loss 0.548077 lr 0.00039734 rank 2
2023-02-23 12:44:08,305 DEBUG TRAIN Batch 18/8200 loss 6.273211 loss_att 9.091999 loss_ctc 10.765741 loss_rnnt 4.901737 hw_loss 0.391334 lr 0.00039736 rank 5
2023-02-23 12:44:08,304 DEBUG TRAIN Batch 18/8200 loss 6.937596 loss_att 8.584850 loss_ctc 10.523810 loss_rnnt 5.887039 hw_loss 0.455521 lr 0.00039741 rank 0
2023-02-23 12:44:08,308 DEBUG TRAIN Batch 18/8200 loss 19.146702 loss_att 22.221077 loss_ctc 24.730690 loss_rnnt 17.607048 hw_loss 0.337965 lr 0.00039741 rank 1
2023-02-23 12:44:08,309 DEBUG TRAIN Batch 18/8200 loss 13.395565 loss_att 16.855213 loss_ctc 14.966593 loss_rnnt 12.269218 hw_loss 0.421774 lr 0.00039735 rank 6
2023-02-23 12:45:20,285 DEBUG TRAIN Batch 18/8300 loss 10.944717 loss_att 14.591852 loss_ctc 13.504985 loss_rnnt 9.643877 hw_loss 0.431332 lr 0.00039722 rank 6
2023-02-23 12:45:20,299 DEBUG TRAIN Batch 18/8300 loss 8.390593 loss_att 11.146591 loss_ctc 13.363462 loss_rnnt 6.965595 hw_loss 0.395155 lr 0.00039721 rank 2
2023-02-23 12:45:20,301 DEBUG TRAIN Batch 18/8300 loss 9.251890 loss_att 12.634434 loss_ctc 16.596733 loss_rnnt 7.316551 hw_loss 0.524096 lr 0.00039720 rank 7
2023-02-23 12:45:20,302 DEBUG TRAIN Batch 18/8300 loss 8.508985 loss_att 11.195114 loss_ctc 11.187740 loss_rnnt 7.342376 hw_loss 0.510404 lr 0.00039725 rank 4
2023-02-23 12:45:20,304 DEBUG TRAIN Batch 18/8300 loss 8.276830 loss_att 8.853022 loss_ctc 12.185202 loss_rnnt 7.388671 hw_loss 0.472134 lr 0.00039728 rank 0
2023-02-23 12:45:20,308 DEBUG TRAIN Batch 18/8300 loss 8.548580 loss_att 12.112542 loss_ctc 15.930626 loss_rnnt 6.629611 hw_loss 0.416072 lr 0.00039716 rank 3
2023-02-23 12:45:20,309 DEBUG TRAIN Batch 18/8300 loss 12.434807 loss_att 14.564249 loss_ctc 16.789394 loss_rnnt 11.230430 hw_loss 0.371017 lr 0.00039728 rank 1
2023-02-23 12:45:20,311 DEBUG TRAIN Batch 18/8300 loss 9.297050 loss_att 9.500965 loss_ctc 9.942278 loss_rnnt 8.941788 hw_loss 0.428340 lr 0.00039723 rank 5
2023-02-23 12:46:11,587 DEBUG CV Batch 18/0 loss 1.688241 loss_att 1.527286 loss_ctc 2.143596 loss_rnnt 1.279198 hw_loss 0.713475 history loss 1.625714 rank 6
2023-02-23 12:46:11,591 DEBUG CV Batch 18/0 loss 1.688241 loss_att 1.527286 loss_ctc 2.143596 loss_rnnt 1.279198 hw_loss 0.713475 history loss 1.625714 rank 1
2023-02-23 12:46:11,593 DEBUG CV Batch 18/0 loss 1.688241 loss_att 1.527286 loss_ctc 2.143596 loss_rnnt 1.279198 hw_loss 0.713475 history loss 1.625714 rank 3
2023-02-23 12:46:11,594 DEBUG CV Batch 18/0 loss 1.688241 loss_att 1.527286 loss_ctc 2.143596 loss_rnnt 1.279198 hw_loss 0.713475 history loss 1.625714 rank 5
2023-02-23 12:46:11,597 DEBUG CV Batch 18/0 loss 1.688241 loss_att 1.527286 loss_ctc 2.143596 loss_rnnt 1.279198 hw_loss 0.713475 history loss 1.625714 rank 4
2023-02-23 12:46:11,603 DEBUG CV Batch 18/0 loss 1.688241 loss_att 1.527286 loss_ctc 2.143596 loss_rnnt 1.279198 hw_loss 0.713475 history loss 1.625714 rank 2
2023-02-23 12:46:11,612 DEBUG CV Batch 18/0 loss 1.688241 loss_att 1.527286 loss_ctc 2.143596 loss_rnnt 1.279198 hw_loss 0.713475 history loss 1.625714 rank 0
2023-02-23 12:46:11,613 DEBUG CV Batch 18/0 loss 1.688241 loss_att 1.527286 loss_ctc 2.143596 loss_rnnt 1.279198 hw_loss 0.713475 history loss 1.625714 rank 7
2023-02-23 12:46:23,092 DEBUG CV Batch 18/100 loss 5.477176 loss_att 7.202564 loss_ctc 8.803519 loss_rnnt 4.451308 hw_loss 0.444896 history loss 3.652461 rank 4
2023-02-23 12:46:23,147 DEBUG CV Batch 18/100 loss 5.477176 loss_att 7.202564 loss_ctc 8.803519 loss_rnnt 4.451308 hw_loss 0.444896 history loss 3.652461 rank 5
2023-02-23 12:46:23,224 DEBUG CV Batch 18/100 loss 5.477176 loss_att 7.202564 loss_ctc 8.803519 loss_rnnt 4.451308 hw_loss 0.444896 history loss 3.652461 rank 7
2023-02-23 12:46:23,253 DEBUG CV Batch 18/100 loss 5.477176 loss_att 7.202564 loss_ctc 8.803519 loss_rnnt 4.451308 hw_loss 0.444896 history loss 3.652461 rank 3
2023-02-23 12:46:23,289 DEBUG CV Batch 18/100 loss 5.477176 loss_att 7.202564 loss_ctc 8.803519 loss_rnnt 4.451308 hw_loss 0.444896 history loss 3.652461 rank 0
2023-02-23 12:46:23,290 DEBUG CV Batch 18/100 loss 5.477176 loss_att 7.202564 loss_ctc 8.803519 loss_rnnt 4.451308 hw_loss 0.444896 history loss 3.652461 rank 1
2023-02-23 12:46:23,312 DEBUG CV Batch 18/100 loss 5.477176 loss_att 7.202564 loss_ctc 8.803519 loss_rnnt 4.451308 hw_loss 0.444896 history loss 3.652461 rank 2
2023-02-23 12:46:23,363 DEBUG CV Batch 18/100 loss 5.477176 loss_att 7.202564 loss_ctc 8.803519 loss_rnnt 4.451308 hw_loss 0.444896 history loss 3.652461 rank 6
2023-02-23 12:46:36,751 DEBUG CV Batch 18/200 loss 7.701608 loss_att 17.196594 loss_ctc 7.237795 loss_rnnt 5.693410 hw_loss 0.320704 history loss 4.358575 rank 4
2023-02-23 12:46:36,886 DEBUG CV Batch 18/200 loss 7.701608 loss_att 17.196594 loss_ctc 7.237795 loss_rnnt 5.693410 hw_loss 0.320704 history loss 4.358575 rank 6
2023-02-23 12:46:36,889 DEBUG CV Batch 18/200 loss 7.701608 loss_att 17.196594 loss_ctc 7.237795 loss_rnnt 5.693410 hw_loss 0.320704 history loss 4.358575 rank 7
2023-02-23 12:46:36,905 DEBUG CV Batch 18/200 loss 7.701608 loss_att 17.196594 loss_ctc 7.237795 loss_rnnt 5.693410 hw_loss 0.320704 history loss 4.358575 rank 3
2023-02-23 12:46:37,008 DEBUG CV Batch 18/200 loss 7.701608 loss_att 17.196594 loss_ctc 7.237795 loss_rnnt 5.693410 hw_loss 0.320704 history loss 4.358575 rank 2
2023-02-23 12:46:37,093 DEBUG CV Batch 18/200 loss 7.701608 loss_att 17.196594 loss_ctc 7.237795 loss_rnnt 5.693410 hw_loss 0.320704 history loss 4.358575 rank 0
2023-02-23 12:46:37,301 DEBUG CV Batch 18/200 loss 7.701608 loss_att 17.196594 loss_ctc 7.237795 loss_rnnt 5.693410 hw_loss 0.320704 history loss 4.358575 rank 5
2023-02-23 12:46:37,539 DEBUG CV Batch 18/200 loss 7.701608 loss_att 17.196594 loss_ctc 7.237795 loss_rnnt 5.693410 hw_loss 0.320704 history loss 4.358575 rank 1
2023-02-23 12:46:48,814 DEBUG CV Batch 18/300 loss 5.241482 loss_att 5.350645 loss_ctc 7.159013 loss_rnnt 4.718006 hw_loss 0.461198 history loss 4.441258 rank 4
2023-02-23 12:46:49,141 DEBUG CV Batch 18/300 loss 5.241482 loss_att 5.350645 loss_ctc 7.159013 loss_rnnt 4.718006 hw_loss 0.461198 history loss 4.441258 rank 7
2023-02-23 12:46:49,334 DEBUG CV Batch 18/300 loss 5.241482 loss_att 5.350645 loss_ctc 7.159013 loss_rnnt 4.718006 hw_loss 0.461198 history loss 4.441258 rank 3
2023-02-23 12:46:49,396 DEBUG CV Batch 18/300 loss 5.241482 loss_att 5.350645 loss_ctc 7.159013 loss_rnnt 4.718006 hw_loss 0.461198 history loss 4.441258 rank 2
2023-02-23 12:46:49,407 DEBUG CV Batch 18/300 loss 5.241482 loss_att 5.350645 loss_ctc 7.159013 loss_rnnt 4.718006 hw_loss 0.461198 history loss 4.441258 rank 0
2023-02-23 12:46:49,482 DEBUG CV Batch 18/300 loss 5.241482 loss_att 5.350645 loss_ctc 7.159013 loss_rnnt 4.718006 hw_loss 0.461198 history loss 4.441258 rank 6
2023-02-23 12:46:49,573 DEBUG CV Batch 18/300 loss 5.241482 loss_att 5.350645 loss_ctc 7.159013 loss_rnnt 4.718006 hw_loss 0.461198 history loss 4.441258 rank 1
2023-02-23 12:46:49,748 DEBUG CV Batch 18/300 loss 5.241482 loss_att 5.350645 loss_ctc 7.159013 loss_rnnt 4.718006 hw_loss 0.461198 history loss 4.441258 rank 5
2023-02-23 12:47:00,857 DEBUG CV Batch 18/400 loss 19.446095 loss_att 90.088150 loss_ctc 9.551208 loss_rnnt 6.474756 hw_loss 0.304206 history loss 5.432713 rank 4
2023-02-23 12:47:01,437 DEBUG CV Batch 18/400 loss 19.446095 loss_att 90.088150 loss_ctc 9.551208 loss_rnnt 6.474756 hw_loss 0.304206 history loss 5.432713 rank 7
2023-02-23 12:47:01,634 DEBUG CV Batch 18/400 loss 19.446095 loss_att 90.088150 loss_ctc 9.551208 loss_rnnt 6.474756 hw_loss 0.304206 history loss 5.432713 rank 1
2023-02-23 12:47:01,671 DEBUG CV Batch 18/400 loss 19.446095 loss_att 90.088150 loss_ctc 9.551208 loss_rnnt 6.474756 hw_loss 0.304206 history loss 5.432713 rank 3
2023-02-23 12:47:01,696 DEBUG CV Batch 18/400 loss 19.446095 loss_att 90.088150 loss_ctc 9.551208 loss_rnnt 6.474756 hw_loss 0.304206 history loss 5.432713 rank 6
2023-02-23 12:47:01,725 DEBUG CV Batch 18/400 loss 19.446095 loss_att 90.088150 loss_ctc 9.551208 loss_rnnt 6.474756 hw_loss 0.304206 history loss 5.432713 rank 2
2023-02-23 12:47:01,869 DEBUG CV Batch 18/400 loss 19.446095 loss_att 90.088150 loss_ctc 9.551208 loss_rnnt 6.474756 hw_loss 0.304206 history loss 5.432713 rank 0
2023-02-23 12:47:02,113 DEBUG CV Batch 18/400 loss 19.446095 loss_att 90.088150 loss_ctc 9.551208 loss_rnnt 6.474756 hw_loss 0.304206 history loss 5.432713 rank 5
2023-02-23 12:47:11,297 DEBUG CV Batch 18/500 loss 5.048271 loss_att 5.933117 loss_ctc 6.626221 loss_rnnt 4.442737 hw_loss 0.409071 history loss 6.237883 rank 4
2023-02-23 12:47:12,070 DEBUG CV Batch 18/500 loss 5.048271 loss_att 5.933117 loss_ctc 6.626221 loss_rnnt 4.442737 hw_loss 0.409071 history loss 6.237883 rank 7
2023-02-23 12:47:12,132 DEBUG CV Batch 18/500 loss 5.048271 loss_att 5.933117 loss_ctc 6.626221 loss_rnnt 4.442737 hw_loss 0.409071 history loss 6.237883 rank 1
2023-02-23 12:47:12,207 DEBUG CV Batch 18/500 loss 5.048271 loss_att 5.933117 loss_ctc 6.626221 loss_rnnt 4.442737 hw_loss 0.409071 history loss 6.237883 rank 6
2023-02-23 12:47:12,448 DEBUG CV Batch 18/500 loss 5.048271 loss_att 5.933117 loss_ctc 6.626221 loss_rnnt 4.442737 hw_loss 0.409071 history loss 6.237883 rank 3
2023-02-23 12:47:12,514 DEBUG CV Batch 18/500 loss 5.048271 loss_att 5.933117 loss_ctc 6.626221 loss_rnnt 4.442737 hw_loss 0.409071 history loss 6.237883 rank 2
2023-02-23 12:47:12,640 DEBUG CV Batch 18/500 loss 5.048271 loss_att 5.933117 loss_ctc 6.626221 loss_rnnt 4.442737 hw_loss 0.409071 history loss 6.237883 rank 5
2023-02-23 12:47:13,091 DEBUG CV Batch 18/500 loss 5.048271 loss_att 5.933117 loss_ctc 6.626221 loss_rnnt 4.442737 hw_loss 0.409071 history loss 6.237883 rank 0
2023-02-23 12:47:23,426 DEBUG CV Batch 18/600 loss 6.519231 loss_att 6.484523 loss_ctc 7.859574 loss_rnnt 5.939694 hw_loss 0.764559 history loss 7.191966 rank 4
2023-02-23 12:47:24,341 DEBUG CV Batch 18/600 loss 6.519231 loss_att 6.484523 loss_ctc 7.859574 loss_rnnt 5.939694 hw_loss 0.764559 history loss 7.191966 rank 7
2023-02-23 12:47:24,369 DEBUG CV Batch 18/600 loss 6.519231 loss_att 6.484523 loss_ctc 7.859574 loss_rnnt 5.939694 hw_loss 0.764559 history loss 7.191966 rank 1
2023-02-23 12:47:24,506 DEBUG CV Batch 18/600 loss 6.519231 loss_att 6.484523 loss_ctc 7.859574 loss_rnnt 5.939694 hw_loss 0.764559 history loss 7.191966 rank 6
2023-02-23 12:47:24,857 DEBUG CV Batch 18/600 loss 6.519231 loss_att 6.484523 loss_ctc 7.859574 loss_rnnt 5.939694 hw_loss 0.764559 history loss 7.191966 rank 3
2023-02-23 12:47:25,045 DEBUG CV Batch 18/600 loss 6.519231 loss_att 6.484523 loss_ctc 7.859574 loss_rnnt 5.939694 hw_loss 0.764559 history loss 7.191966 rank 2
2023-02-23 12:47:25,335 DEBUG CV Batch 18/600 loss 6.519231 loss_att 6.484523 loss_ctc 7.859574 loss_rnnt 5.939694 hw_loss 0.764559 history loss 7.191966 rank 5
2023-02-23 12:47:25,651 DEBUG CV Batch 18/600 loss 6.519231 loss_att 6.484523 loss_ctc 7.859574 loss_rnnt 5.939694 hw_loss 0.764559 history loss 7.191966 rank 0
2023-02-23 12:47:35,315 DEBUG CV Batch 18/700 loss 23.983173 loss_att 63.517029 loss_ctc 20.526613 loss_rnnt 16.372656 hw_loss 0.308666 history loss 7.874127 rank 4
2023-02-23 12:47:35,995 DEBUG CV Batch 18/700 loss 23.983173 loss_att 63.517029 loss_ctc 20.526613 loss_rnnt 16.372656 hw_loss 0.308666 history loss 7.874127 rank 7
2023-02-23 12:47:36,046 DEBUG CV Batch 18/700 loss 23.983173 loss_att 63.517029 loss_ctc 20.526613 loss_rnnt 16.372656 hw_loss 0.308666 history loss 7.874127 rank 6
2023-02-23 12:47:36,467 DEBUG CV Batch 18/700 loss 23.983173 loss_att 63.517029 loss_ctc 20.526613 loss_rnnt 16.372656 hw_loss 0.308666 history loss 7.874127 rank 1
2023-02-23 12:47:36,599 DEBUG CV Batch 18/700 loss 23.983173 loss_att 63.517029 loss_ctc 20.526613 loss_rnnt 16.372656 hw_loss 0.308666 history loss 7.874127 rank 3
2023-02-23 12:47:36,842 DEBUG CV Batch 18/700 loss 23.983173 loss_att 63.517029 loss_ctc 20.526613 loss_rnnt 16.372656 hw_loss 0.308666 history loss 7.874127 rank 2
2023-02-23 12:47:36,928 DEBUG CV Batch 18/700 loss 23.983173 loss_att 63.517029 loss_ctc 20.526613 loss_rnnt 16.372656 hw_loss 0.308666 history loss 7.874127 rank 5
2023-02-23 12:47:37,469 DEBUG CV Batch 18/700 loss 23.983173 loss_att 63.517029 loss_ctc 20.526613 loss_rnnt 16.372656 hw_loss 0.308666 history loss 7.874127 rank 0
2023-02-23 12:47:47,119 DEBUG CV Batch 18/800 loss 11.640252 loss_att 11.549841 loss_ctc 17.128338 loss_rnnt 10.647388 hw_loss 0.523502 history loss 7.309236 rank 6
2023-02-23 12:47:47,368 DEBUG CV Batch 18/800 loss 11.640252 loss_att 11.549841 loss_ctc 17.128338 loss_rnnt 10.647388 hw_loss 0.523502 history loss 7.309236 rank 7
2023-02-23 12:47:47,397 DEBUG CV Batch 18/800 loss 11.640252 loss_att 11.549841 loss_ctc 17.128338 loss_rnnt 10.647388 hw_loss 0.523502 history loss 7.309236 rank 4
2023-02-23 12:47:48,033 DEBUG CV Batch 18/800 loss 11.640252 loss_att 11.549841 loss_ctc 17.128338 loss_rnnt 10.647388 hw_loss 0.523502 history loss 7.309236 rank 3
2023-02-23 12:47:48,336 DEBUG CV Batch 18/800 loss 11.640252 loss_att 11.549841 loss_ctc 17.128338 loss_rnnt 10.647388 hw_loss 0.523502 history loss 7.309236 rank 2
2023-02-23 12:47:48,621 DEBUG CV Batch 18/800 loss 11.640252 loss_att 11.549841 loss_ctc 17.128338 loss_rnnt 10.647388 hw_loss 0.523502 history loss 7.309236 rank 5
2023-02-23 12:47:48,777 DEBUG CV Batch 18/800 loss 11.640252 loss_att 11.549841 loss_ctc 17.128338 loss_rnnt 10.647388 hw_loss 0.523502 history loss 7.309236 rank 1
2023-02-23 12:47:48,845 DEBUG CV Batch 18/800 loss 11.640252 loss_att 11.549841 loss_ctc 17.128338 loss_rnnt 10.647388 hw_loss 0.523502 history loss 7.309236 rank 0
2023-02-23 12:48:00,601 DEBUG CV Batch 18/900 loss 11.353339 loss_att 15.440162 loss_ctc 20.277914 loss_rnnt 9.197276 hw_loss 0.278915 history loss 7.117521 rank 6
2023-02-23 12:48:01,000 DEBUG CV Batch 18/900 loss 11.353339 loss_att 15.440162 loss_ctc 20.277914 loss_rnnt 9.197276 hw_loss 0.278915 history loss 7.117521 rank 7
2023-02-23 12:48:01,262 DEBUG CV Batch 18/900 loss 11.353339 loss_att 15.440162 loss_ctc 20.277914 loss_rnnt 9.197276 hw_loss 0.278915 history loss 7.117521 rank 4
2023-02-23 12:48:01,761 DEBUG CV Batch 18/900 loss 11.353339 loss_att 15.440162 loss_ctc 20.277914 loss_rnnt 9.197276 hw_loss 0.278915 history loss 7.117521 rank 3
2023-02-23 12:48:02,145 DEBUG CV Batch 18/900 loss 11.353339 loss_att 15.440162 loss_ctc 20.277914 loss_rnnt 9.197276 hw_loss 0.278915 history loss 7.117521 rank 5
2023-02-23 12:48:02,241 DEBUG CV Batch 18/900 loss 11.353339 loss_att 15.440162 loss_ctc 20.277914 loss_rnnt 9.197276 hw_loss 0.278915 history loss 7.117521 rank 2
2023-02-23 12:48:02,865 DEBUG CV Batch 18/900 loss 11.353339 loss_att 15.440162 loss_ctc 20.277914 loss_rnnt 9.197276 hw_loss 0.278915 history loss 7.117521 rank 0
2023-02-23 12:48:02,892 DEBUG CV Batch 18/900 loss 11.353339 loss_att 15.440162 loss_ctc 20.277914 loss_rnnt 9.197276 hw_loss 0.278915 history loss 7.117521 rank 1
2023-02-23 12:48:13,168 DEBUG CV Batch 18/1000 loss 3.174283 loss_att 4.353397 loss_ctc 4.229238 loss_rnnt 2.489922 hw_loss 0.577269 history loss 6.860551 rank 6
2023-02-23 12:48:13,380 DEBUG CV Batch 18/1000 loss 3.174283 loss_att 4.353397 loss_ctc 4.229238 loss_rnnt 2.489922 hw_loss 0.577269 history loss 6.860551 rank 4
2023-02-23 12:48:13,657 DEBUG CV Batch 18/1000 loss 3.174283 loss_att 4.353397 loss_ctc 4.229238 loss_rnnt 2.489922 hw_loss 0.577269 history loss 6.860551 rank 7
2023-02-23 12:48:14,384 DEBUG CV Batch 18/1000 loss 3.174283 loss_att 4.353397 loss_ctc 4.229238 loss_rnnt 2.489922 hw_loss 0.577269 history loss 6.860551 rank 3
2023-02-23 12:48:14,889 DEBUG CV Batch 18/1000 loss 3.174283 loss_att 4.353397 loss_ctc 4.229238 loss_rnnt 2.489922 hw_loss 0.577269 history loss 6.860551 rank 5
2023-02-23 12:48:15,087 DEBUG CV Batch 18/1000 loss 3.174283 loss_att 4.353397 loss_ctc 4.229238 loss_rnnt 2.489922 hw_loss 0.577269 history loss 6.860551 rank 1
2023-02-23 12:48:15,102 DEBUG CV Batch 18/1000 loss 3.174283 loss_att 4.353397 loss_ctc 4.229238 loss_rnnt 2.489922 hw_loss 0.577269 history loss 6.860551 rank 2
2023-02-23 12:48:15,941 DEBUG CV Batch 18/1000 loss 3.174283 loss_att 4.353397 loss_ctc 4.229238 loss_rnnt 2.489922 hw_loss 0.577269 history loss 6.860551 rank 0
2023-02-23 12:48:25,198 DEBUG CV Batch 18/1100 loss 7.563727 loss_att 6.099149 loss_ctc 9.193304 loss_rnnt 7.242019 hw_loss 0.745027 history loss 6.846715 rank 4
2023-02-23 12:48:25,416 DEBUG CV Batch 18/1100 loss 7.563727 loss_att 6.099149 loss_ctc 9.193304 loss_rnnt 7.242019 hw_loss 0.745027 history loss 6.846715 rank 6
2023-02-23 12:48:25,903 DEBUG CV Batch 18/1100 loss 7.563727 loss_att 6.099149 loss_ctc 9.193304 loss_rnnt 7.242019 hw_loss 0.745027 history loss 6.846715 rank 7
2023-02-23 12:48:26,753 DEBUG CV Batch 18/1100 loss 7.563727 loss_att 6.099149 loss_ctc 9.193304 loss_rnnt 7.242019 hw_loss 0.745027 history loss 6.846715 rank 3
2023-02-23 12:48:26,907 DEBUG CV Batch 18/1100 loss 7.563727 loss_att 6.099149 loss_ctc 9.193304 loss_rnnt 7.242019 hw_loss 0.745027 history loss 6.846715 rank 1
2023-02-23 12:48:26,963 DEBUG CV Batch 18/1100 loss 7.563727 loss_att 6.099149 loss_ctc 9.193304 loss_rnnt 7.242019 hw_loss 0.745027 history loss 6.846715 rank 5
2023-02-23 12:48:27,546 DEBUG CV Batch 18/1100 loss 7.563727 loss_att 6.099149 loss_ctc 9.193304 loss_rnnt 7.242019 hw_loss 0.745027 history loss 6.846715 rank 2
2023-02-23 12:48:28,180 DEBUG CV Batch 18/1100 loss 7.563727 loss_att 6.099149 loss_ctc 9.193304 loss_rnnt 7.242019 hw_loss 0.745027 history loss 6.846715 rank 0
2023-02-23 12:48:35,705 DEBUG CV Batch 18/1200 loss 5.832257 loss_att 7.704917 loss_ctc 6.431380 loss_rnnt 5.059665 hw_loss 0.596581 history loss 7.192811 rank 4
2023-02-23 12:48:36,176 DEBUG CV Batch 18/1200 loss 5.832257 loss_att 7.704917 loss_ctc 6.431380 loss_rnnt 5.059665 hw_loss 0.596581 history loss 7.192811 rank 6
2023-02-23 12:48:36,983 DEBUG CV Batch 18/1200 loss 5.832257 loss_att 7.704917 loss_ctc 6.431380 loss_rnnt 5.059665 hw_loss 0.596581 history loss 7.192811 rank 7
2023-02-23 12:48:37,456 DEBUG CV Batch 18/1200 loss 5.832257 loss_att 7.704917 loss_ctc 6.431380 loss_rnnt 5.059665 hw_loss 0.596581 history loss 7.192811 rank 5
2023-02-23 12:48:37,482 DEBUG CV Batch 18/1200 loss 5.832257 loss_att 7.704917 loss_ctc 6.431380 loss_rnnt 5.059665 hw_loss 0.596581 history loss 7.192811 rank 1
2023-02-23 12:48:37,729 DEBUG CV Batch 18/1200 loss 5.832257 loss_att 7.704917 loss_ctc 6.431380 loss_rnnt 5.059665 hw_loss 0.596581 history loss 7.192811 rank 3
2023-02-23 12:48:38,865 DEBUG CV Batch 18/1200 loss 5.832257 loss_att 7.704917 loss_ctc 6.431380 loss_rnnt 5.059665 hw_loss 0.596581 history loss 7.192811 rank 2
2023-02-23 12:48:39,859 DEBUG CV Batch 18/1200 loss 5.832257 loss_att 7.704917 loss_ctc 6.431380 loss_rnnt 5.059665 hw_loss 0.596581 history loss 7.192811 rank 0
2023-02-23 12:48:47,955 DEBUG CV Batch 18/1300 loss 4.744219 loss_att 5.018563 loss_ctc 6.204654 loss_rnnt 4.106157 hw_loss 0.728380 history loss 7.513353 rank 4
2023-02-23 12:48:48,260 DEBUG CV Batch 18/1300 loss 4.744219 loss_att 5.018563 loss_ctc 6.204654 loss_rnnt 4.106157 hw_loss 0.728380 history loss 7.513353 rank 6
2023-02-23 12:48:49,263 DEBUG CV Batch 18/1300 loss 4.744219 loss_att 5.018563 loss_ctc 6.204654 loss_rnnt 4.106157 hw_loss 0.728380 history loss 7.513353 rank 7
2023-02-23 12:48:49,570 DEBUG CV Batch 18/1300 loss 4.744219 loss_att 5.018563 loss_ctc 6.204654 loss_rnnt 4.106157 hw_loss 0.728380 history loss 7.513353 rank 5
2023-02-23 12:48:49,709 DEBUG CV Batch 18/1300 loss 4.744219 loss_att 5.018563 loss_ctc 6.204654 loss_rnnt 4.106157 hw_loss 0.728380 history loss 7.513353 rank 1
2023-02-23 12:48:50,052 DEBUG CV Batch 18/1300 loss 4.744219 loss_att 5.018563 loss_ctc 6.204654 loss_rnnt 4.106157 hw_loss 0.728380 history loss 7.513353 rank 3
2023-02-23 12:48:51,391 DEBUG CV Batch 18/1300 loss 4.744219 loss_att 5.018563 loss_ctc 6.204654 loss_rnnt 4.106157 hw_loss 0.728380 history loss 7.513353 rank 2
2023-02-23 12:48:52,442 DEBUG CV Batch 18/1300 loss 4.744219 loss_att 5.018563 loss_ctc 6.204654 loss_rnnt 4.106157 hw_loss 0.728380 history loss 7.513353 rank 0
2023-02-23 12:49:00,000 DEBUG CV Batch 18/1400 loss 3.149695 loss_att 12.355861 loss_ctc 2.283993 loss_rnnt 1.223311 hw_loss 0.376083 history loss 7.840398 rank 6
2023-02-23 12:49:00,139 DEBUG CV Batch 18/1400 loss 3.149695 loss_att 12.355861 loss_ctc 2.283993 loss_rnnt 1.223311 hw_loss 0.376083 history loss 7.840398 rank 4
2023-02-23 12:49:00,867 DEBUG CV Batch 18/1400 loss 3.149695 loss_att 12.355861 loss_ctc 2.283993 loss_rnnt 1.223311 hw_loss 0.376083 history loss 7.840398 rank 7
2023-02-23 12:49:01,084 DEBUG CV Batch 18/1400 loss 3.149695 loss_att 12.355861 loss_ctc 2.283993 loss_rnnt 1.223311 hw_loss 0.376083 history loss 7.840398 rank 5
2023-02-23 12:49:01,705 DEBUG CV Batch 18/1400 loss 3.149695 loss_att 12.355861 loss_ctc 2.283993 loss_rnnt 1.223311 hw_loss 0.376083 history loss 7.840398 rank 3
2023-02-23 12:49:01,733 DEBUG CV Batch 18/1400 loss 3.149695 loss_att 12.355861 loss_ctc 2.283993 loss_rnnt 1.223311 hw_loss 0.376083 history loss 7.840398 rank 1
2023-02-23 12:49:03,240 DEBUG CV Batch 18/1400 loss 3.149695 loss_att 12.355861 loss_ctc 2.283993 loss_rnnt 1.223311 hw_loss 0.376083 history loss 7.840398 rank 2
2023-02-23 12:49:04,048 DEBUG CV Batch 18/1400 loss 3.149695 loss_att 12.355861 loss_ctc 2.283993 loss_rnnt 1.223311 hw_loss 0.376083 history loss 7.840398 rank 0
2023-02-23 12:49:11,569 DEBUG CV Batch 18/1500 loss 7.761489 loss_att 8.488362 loss_ctc 9.031242 loss_rnnt 7.180865 hw_loss 0.498653 history loss 7.662213 rank 6
2023-02-23 12:49:12,699 DEBUG CV Batch 18/1500 loss 7.761489 loss_att 8.488362 loss_ctc 9.031242 loss_rnnt 7.180865 hw_loss 0.498653 history loss 7.662213 rank 7
2023-02-23 12:49:12,883 DEBUG CV Batch 18/1500 loss 7.761489 loss_att 8.488362 loss_ctc 9.031242 loss_rnnt 7.180865 hw_loss 0.498653 history loss 7.662213 rank 4
2023-02-23 12:49:13,594 DEBUG CV Batch 18/1500 loss 7.761489 loss_att 8.488362 loss_ctc 9.031242 loss_rnnt 7.180865 hw_loss 0.498653 history loss 7.662213 rank 3
2023-02-23 12:49:14,121 DEBUG CV Batch 18/1500 loss 7.761489 loss_att 8.488362 loss_ctc 9.031242 loss_rnnt 7.180865 hw_loss 0.498653 history loss 7.662213 rank 5
2023-02-23 12:49:14,404 DEBUG CV Batch 18/1500 loss 7.761489 loss_att 8.488362 loss_ctc 9.031242 loss_rnnt 7.180865 hw_loss 0.498653 history loss 7.662213 rank 1
2023-02-23 12:49:15,184 DEBUG CV Batch 18/1500 loss 7.761489 loss_att 8.488362 loss_ctc 9.031242 loss_rnnt 7.180865 hw_loss 0.498653 history loss 7.662213 rank 2
2023-02-23 12:49:16,057 DEBUG CV Batch 18/1500 loss 7.761489 loss_att 8.488362 loss_ctc 9.031242 loss_rnnt 7.180865 hw_loss 0.498653 history loss 7.662213 rank 0
2023-02-23 12:49:24,981 DEBUG CV Batch 18/1600 loss 12.986946 loss_att 12.649149 loss_ctc 13.538757 loss_rnnt 12.737888 hw_loss 0.455704 history loss 7.601454 rank 6
2023-02-23 12:49:26,092 DEBUG CV Batch 18/1600 loss 12.986946 loss_att 12.649149 loss_ctc 13.538757 loss_rnnt 12.737888 hw_loss 0.455704 history loss 7.601454 rank 7
2023-02-23 12:49:26,386 DEBUG CV Batch 18/1600 loss 12.986946 loss_att 12.649149 loss_ctc 13.538757 loss_rnnt 12.737888 hw_loss 0.455704 history loss 7.601454 rank 4
2023-02-23 12:49:27,032 DEBUG CV Batch 18/1600 loss 12.986946 loss_att 12.649149 loss_ctc 13.538757 loss_rnnt 12.737888 hw_loss 0.455704 history loss 7.601454 rank 3
2023-02-23 12:49:27,989 DEBUG CV Batch 18/1600 loss 12.986946 loss_att 12.649149 loss_ctc 13.538757 loss_rnnt 12.737888 hw_loss 0.455704 history loss 7.601454 rank 5
2023-02-23 12:49:28,252 DEBUG CV Batch 18/1600 loss 12.986946 loss_att 12.649149 loss_ctc 13.538757 loss_rnnt 12.737888 hw_loss 0.455704 history loss 7.601454 rank 1
2023-02-23 12:49:28,720 DEBUG CV Batch 18/1600 loss 12.986946 loss_att 12.649149 loss_ctc 13.538757 loss_rnnt 12.737888 hw_loss 0.455704 history loss 7.601454 rank 2
2023-02-23 12:49:29,799 DEBUG CV Batch 18/1600 loss 12.986946 loss_att 12.649149 loss_ctc 13.538757 loss_rnnt 12.737888 hw_loss 0.455704 history loss 7.601454 rank 0
2023-02-23 12:49:37,970 DEBUG CV Batch 18/1700 loss 8.512156 loss_att 8.304792 loss_ctc 12.691272 loss_rnnt 7.670498 hw_loss 0.611091 history loss 7.493055 rank 6
2023-02-23 12:49:38,773 DEBUG CV Batch 18/1700 loss 8.512156 loss_att 8.304792 loss_ctc 12.691272 loss_rnnt 7.670498 hw_loss 0.611091 history loss 7.493055 rank 7
2023-02-23 12:49:38,859 DEBUG CV Batch 18/1700 loss 8.512156 loss_att 8.304792 loss_ctc 12.691272 loss_rnnt 7.670498 hw_loss 0.611091 history loss 7.493055 rank 4
2023-02-23 12:49:39,743 DEBUG CV Batch 18/1700 loss 8.512156 loss_att 8.304792 loss_ctc 12.691272 loss_rnnt 7.670498 hw_loss 0.611091 history loss 7.493055 rank 3
2023-02-23 12:49:40,577 DEBUG CV Batch 18/1700 loss 8.512156 loss_att 8.304792 loss_ctc 12.691272 loss_rnnt 7.670498 hw_loss 0.611091 history loss 7.493055 rank 5
2023-02-23 12:49:40,651 DEBUG CV Batch 18/1700 loss 8.512156 loss_att 8.304792 loss_ctc 12.691272 loss_rnnt 7.670498 hw_loss 0.611091 history loss 7.493055 rank 1
2023-02-23 12:49:41,354 DEBUG CV Batch 18/1700 loss 8.512156 loss_att 8.304792 loss_ctc 12.691272 loss_rnnt 7.670498 hw_loss 0.611091 history loss 7.493055 rank 2
2023-02-23 12:49:42,983 DEBUG CV Batch 18/1700 loss 8.512156 loss_att 8.304792 loss_ctc 12.691272 loss_rnnt 7.670498 hw_loss 0.611091 history loss 7.493055 rank 0
2023-02-23 12:49:47,539 INFO Epoch 18 CV info cv_loss 7.46000452006909
2023-02-23 12:49:47,540 INFO Epoch 19 TRAIN info lr 0.00039719459210851854
2023-02-23 12:49:47,545 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 12:49:47,864 INFO Epoch 18 CV info cv_loss 7.460004517669915
2023-02-23 12:49:47,864 INFO Epoch 19 TRAIN info lr 0.000397116912997078
2023-02-23 12:49:47,869 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 12:49:48,222 INFO Epoch 18 CV info cv_loss 7.460004517777597
2023-02-23 12:49:48,222 INFO Epoch 19 TRAIN info lr 0.0003972184061240939
2023-02-23 12:49:48,226 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 12:49:48,884 INFO Epoch 18 CV info cv_loss 7.460004518893193
2023-02-23 12:49:48,884 INFO Epoch 19 TRAIN info lr 0.0003971093980821299
2023-02-23 12:49:48,889 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 12:49:49,845 INFO Epoch 18 CV info cv_loss 7.460004519237778
2023-02-23 12:49:49,846 INFO Epoch 19 TRAIN info lr 0.0003972472393483442
2023-02-23 12:49:49,848 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 12:49:49,942 INFO Epoch 18 CV info cv_loss 7.460004520430904
2023-02-23 12:49:49,942 INFO Epoch 19 TRAIN info lr 0.000397199605229546
2023-02-23 12:49:49,944 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 12:49:50,584 INFO Epoch 18 CV info cv_loss 7.460004518699363
2023-02-23 12:49:50,584 INFO Epoch 19 TRAIN info lr 0.0003971532411013554
2023-02-23 12:49:50,586 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 12:49:52,535 INFO Epoch 18 CV info cv_loss 7.460004520809949
2023-02-23 12:49:52,536 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/18.pt
2023-02-23 12:49:53,115 INFO Epoch 19 TRAIN info lr 0.0003972422244235292
2023-02-23 12:49:53,119 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 12:50:53,894 DEBUG TRAIN Batch 19/0 loss 7.892770 loss_att 6.960073 loss_ctc 10.094614 loss_rnnt 7.380159 hw_loss 0.760447 lr 0.00039711 rank 3
2023-02-23 12:50:53,895 DEBUG TRAIN Batch 19/0 loss 9.055668 loss_att 8.403685 loss_ctc 12.023043 loss_rnnt 8.365288 hw_loss 0.797113 lr 0.00039722 rank 4
2023-02-23 12:50:53,897 DEBUG TRAIN Batch 19/0 loss 8.536671 loss_att 8.689153 loss_ctc 12.114244 loss_rnnt 7.646842 hw_loss 0.716855 lr 0.00039715 rank 2
2023-02-23 12:50:53,898 DEBUG TRAIN Batch 19/0 loss 11.314781 loss_att 11.013042 loss_ctc 13.944979 loss_rnnt 10.621948 hw_loss 0.754665 lr 0.00039712 rank 7
2023-02-23 12:50:53,898 DEBUG TRAIN Batch 19/0 loss 8.344818 loss_att 7.570369 loss_ctc 10.500684 loss_rnnt 7.860869 hw_loss 0.658855 lr 0.00039725 rank 1
2023-02-23 12:50:53,898 DEBUG TRAIN Batch 19/0 loss 8.356052 loss_att 8.011008 loss_ctc 9.600272 loss_rnnt 7.902260 hw_loss 0.669195 lr 0.00039720 rank 5
2023-02-23 12:50:53,904 DEBUG TRAIN Batch 19/0 loss 13.326875 loss_att 12.446712 loss_ctc 16.841005 loss_rnnt 12.633883 hw_loss 0.750886 lr 0.00039719 rank 6
2023-02-23 12:50:53,923 DEBUG TRAIN Batch 19/0 loss 8.999876 loss_att 7.808727 loss_ctc 9.827448 loss_rnnt 8.661477 hw_loss 0.874285 lr 0.00039724 rank 0
2023-02-23 12:52:05,942 DEBUG TRAIN Batch 19/100 loss 9.357630 loss_att 13.416446 loss_ctc 9.940264 loss_rnnt 8.286426 hw_loss 0.340796 lr 0.00039699 rank 7
2023-02-23 12:52:05,950 DEBUG TRAIN Batch 19/100 loss 6.154408 loss_att 9.273157 loss_ctc 6.043929 loss_rnnt 5.348530 hw_loss 0.369111 lr 0.00039698 rank 3
2023-02-23 12:52:05,951 DEBUG TRAIN Batch 19/100 loss 6.717639 loss_att 10.442628 loss_ctc 11.220213 loss_rnnt 5.170382 hw_loss 0.378592 lr 0.00039703 rank 2
2023-02-23 12:52:05,953 DEBUG TRAIN Batch 19/100 loss 4.926290 loss_att 7.025949 loss_ctc 6.951996 loss_rnnt 4.025560 hw_loss 0.395069 lr 0.00039712 rank 1
2023-02-23 12:52:05,955 DEBUG TRAIN Batch 19/100 loss 5.158946 loss_att 8.018770 loss_ctc 6.393028 loss_rnnt 4.173916 hw_loss 0.465975 lr 0.00039709 rank 4
2023-02-23 12:52:05,955 DEBUG TRAIN Batch 19/100 loss 12.307899 loss_att 15.268639 loss_ctc 14.684100 loss_rnnt 11.200974 hw_loss 0.371156 lr 0.00039712 rank 0
2023-02-23 12:52:05,956 DEBUG TRAIN Batch 19/100 loss 4.905089 loss_att 9.330900 loss_ctc 6.192577 loss_rnnt 3.602940 hw_loss 0.459978 lr 0.00039707 rank 5
2023-02-23 12:52:05,957 DEBUG TRAIN Batch 19/100 loss 7.561255 loss_att 10.690432 loss_ctc 8.653615 loss_rnnt 6.550307 hw_loss 0.448994 lr 0.00039707 rank 6
2023-02-23 12:53:18,349 DEBUG TRAIN Batch 19/200 loss 7.844956 loss_att 9.124517 loss_ctc 9.961905 loss_rnnt 7.072211 hw_loss 0.439825 lr 0.00039686 rank 3
2023-02-23 12:53:18,353 DEBUG TRAIN Batch 19/200 loss 11.470110 loss_att 14.589162 loss_ctc 17.039907 loss_rnnt 9.918601 hw_loss 0.346985 lr 0.00039687 rank 7
2023-02-23 12:53:18,354 DEBUG TRAIN Batch 19/200 loss 5.684449 loss_att 9.919272 loss_ctc 9.250849 loss_rnnt 4.158659 hw_loss 0.381198 lr 0.00039694 rank 6
2023-02-23 12:53:18,355 DEBUG TRAIN Batch 19/200 loss 6.845233 loss_att 10.636274 loss_ctc 9.439507 loss_rnnt 5.552214 hw_loss 0.354202 lr 0.00039697 rank 4
2023-02-23 12:53:18,358 DEBUG TRAIN Batch 19/200 loss 11.579538 loss_att 12.956388 loss_ctc 13.257609 loss_rnnt 10.877963 hw_loss 0.379618 lr 0.00039690 rank 2
2023-02-23 12:53:18,359 DEBUG TRAIN Batch 19/200 loss 17.267000 loss_att 18.788332 loss_ctc 22.297619 loss_rnnt 16.030445 hw_loss 0.490386 lr 0.00039695 rank 5
2023-02-23 12:53:18,362 DEBUG TRAIN Batch 19/200 loss 6.868061 loss_att 9.117412 loss_ctc 12.137140 loss_rnnt 5.544788 hw_loss 0.320360 lr 0.00039699 rank 0
2023-02-23 12:53:18,406 DEBUG TRAIN Batch 19/200 loss 8.982492 loss_att 10.112917 loss_ctc 9.560256 loss_rnnt 8.436066 hw_loss 0.456200 lr 0.00039700 rank 1
2023-02-23 12:54:31,861 DEBUG TRAIN Batch 19/300 loss 8.426646 loss_att 12.493326 loss_ctc 9.548814 loss_rnnt 7.257659 hw_loss 0.386305 lr 0.00039682 rank 6
2023-02-23 12:54:31,867 DEBUG TRAIN Batch 19/300 loss 15.777256 loss_att 15.880972 loss_ctc 20.916779 loss_rnnt 14.842693 hw_loss 0.428532 lr 0.00039687 rank 0
2023-02-23 12:54:31,874 DEBUG TRAIN Batch 19/300 loss 12.984661 loss_att 15.496174 loss_ctc 18.394800 loss_rnnt 11.568539 hw_loss 0.360874 lr 0.00039674 rank 7
2023-02-23 12:54:31,878 DEBUG TRAIN Batch 19/300 loss 13.726169 loss_att 19.164146 loss_ctc 23.119926 loss_rnnt 11.204224 hw_loss 0.340966 lr 0.00039678 rank 2
2023-02-23 12:54:31,879 DEBUG TRAIN Batch 19/300 loss 5.377691 loss_att 7.290082 loss_ctc 7.702662 loss_rnnt 4.429017 hw_loss 0.480375 lr 0.00039682 rank 5
2023-02-23 12:54:31,879 DEBUG TRAIN Batch 19/300 loss 6.982728 loss_att 9.540297 loss_ctc 12.276346 loss_rnnt 5.567466 hw_loss 0.371125 lr 0.00039687 rank 1
2023-02-23 12:54:31,880 DEBUG TRAIN Batch 19/300 loss 10.698894 loss_att 12.753651 loss_ctc 15.805334 loss_rnnt 9.376257 hw_loss 0.432800 lr 0.00039673 rank 3
2023-02-23 12:54:31,904 DEBUG TRAIN Batch 19/300 loss 10.927182 loss_att 12.967983 loss_ctc 15.241610 loss_rnnt 9.686901 hw_loss 0.481617 lr 0.00039684 rank 4
2023-02-23 12:55:44,942 DEBUG TRAIN Batch 19/400 loss 7.793366 loss_att 10.179174 loss_ctc 8.143643 loss_rnnt 7.053452 hw_loss 0.405088 lr 0.00039662 rank 7
2023-02-23 12:55:44,945 DEBUG TRAIN Batch 19/400 loss 9.455305 loss_att 10.186658 loss_ctc 10.247443 loss_rnnt 8.938915 hw_loss 0.495940 lr 0.00039661 rank 3
2023-02-23 12:55:44,945 DEBUG TRAIN Batch 19/400 loss 11.005778 loss_att 13.131016 loss_ctc 13.540261 loss_rnnt 10.005259 hw_loss 0.445390 lr 0.00039672 rank 4
2023-02-23 12:55:44,949 DEBUG TRAIN Batch 19/400 loss 10.925364 loss_att 14.752139 loss_ctc 13.907040 loss_rnnt 9.515396 hw_loss 0.463232 lr 0.00039669 rank 6
2023-02-23 12:55:44,952 DEBUG TRAIN Batch 19/400 loss 13.121118 loss_att 15.081179 loss_ctc 19.662815 loss_rnnt 11.654163 hw_loss 0.380089 lr 0.00039670 rank 5
2023-02-23 12:55:44,951 DEBUG TRAIN Batch 19/400 loss 7.667243 loss_att 9.942624 loss_ctc 10.692673 loss_rnnt 6.596767 hw_loss 0.397518 lr 0.00039665 rank 2
2023-02-23 12:55:44,958 DEBUG TRAIN Batch 19/400 loss 7.527001 loss_att 10.654201 loss_ctc 9.071500 loss_rnnt 6.455328 hw_loss 0.450561 lr 0.00039674 rank 0
2023-02-23 12:55:44,999 DEBUG TRAIN Batch 19/400 loss 17.154593 loss_att 19.628607 loss_ctc 20.613388 loss_rnnt 15.935968 hw_loss 0.492462 lr 0.00039675 rank 1
2023-02-23 12:56:57,954 DEBUG TRAIN Batch 19/500 loss 10.746217 loss_att 12.027286 loss_ctc 13.574043 loss_rnnt 9.907321 hw_loss 0.385573 lr 0.00039649 rank 7
2023-02-23 12:56:57,972 DEBUG TRAIN Batch 19/500 loss 6.505373 loss_att 9.825453 loss_ctc 11.953930 loss_rnnt 4.829625 hw_loss 0.534859 lr 0.00039653 rank 2
2023-02-23 12:56:57,973 DEBUG TRAIN Batch 19/500 loss 7.723323 loss_att 9.936073 loss_ctc 11.038912 loss_rnnt 6.546412 hw_loss 0.548030 lr 0.00039648 rank 3
2023-02-23 12:56:57,976 DEBUG TRAIN Batch 19/500 loss 13.090146 loss_att 17.439327 loss_ctc 22.649580 loss_rnnt 10.731793 hw_loss 0.401108 lr 0.00039662 rank 0
2023-02-23 12:56:57,977 DEBUG TRAIN Batch 19/500 loss 9.828609 loss_att 13.954093 loss_ctc 17.215351 loss_rnnt 7.774581 hw_loss 0.457559 lr 0.00039659 rank 4
2023-02-23 12:56:57,978 DEBUG TRAIN Batch 19/500 loss 12.960633 loss_att 15.388042 loss_ctc 15.857806 loss_rnnt 11.889006 hw_loss 0.374730 lr 0.00039657 rank 6
2023-02-23 12:56:57,981 DEBUG TRAIN Batch 19/500 loss 2.963055 loss_att 6.242225 loss_ctc 4.575626 loss_rnnt 1.873020 hw_loss 0.410984 lr 0.00039657 rank 5
2023-02-23 12:56:58,018 DEBUG TRAIN Batch 19/500 loss 21.819477 loss_att 22.763212 loss_ctc 31.947170 loss_rnnt 20.074890 hw_loss 0.385277 lr 0.00039662 rank 1
2023-02-23 12:58:10,291 DEBUG TRAIN Batch 19/600 loss 9.917171 loss_att 11.095717 loss_ctc 14.552482 loss_rnnt 8.845149 hw_loss 0.409257 lr 0.00039637 rank 7
2023-02-23 12:58:10,293 DEBUG TRAIN Batch 19/600 loss 9.326858 loss_att 10.707657 loss_ctc 13.394770 loss_rnnt 8.230192 hw_loss 0.521469 lr 0.00039640 rank 2
2023-02-23 12:58:10,302 DEBUG TRAIN Batch 19/600 loss 12.131185 loss_att 14.263643 loss_ctc 14.961382 loss_rnnt 11.082484 hw_loss 0.459091 lr 0.00039644 rank 6
2023-02-23 12:58:10,301 DEBUG TRAIN Batch 19/600 loss 10.958803 loss_att 11.567152 loss_ctc 16.856916 loss_rnnt 9.804623 hw_loss 0.461429 lr 0.00039647 rank 4
2023-02-23 12:58:10,302 DEBUG TRAIN Batch 19/600 loss 7.837278 loss_att 8.615370 loss_ctc 11.232677 loss_rnnt 6.906685 hw_loss 0.604229 lr 0.00039636 rank 3
2023-02-23 12:58:10,305 DEBUG TRAIN Batch 19/600 loss 10.619465 loss_att 14.506021 loss_ctc 16.573141 loss_rnnt 8.772313 hw_loss 0.517531 lr 0.00039650 rank 1
2023-02-23 12:58:10,329 DEBUG TRAIN Batch 19/600 loss 8.278107 loss_att 9.328534 loss_ctc 10.464127 loss_rnnt 7.512376 hw_loss 0.495328 lr 0.00039649 rank 0
2023-02-23 12:58:10,339 DEBUG TRAIN Batch 19/600 loss 8.008015 loss_att 9.367669 loss_ctc 12.101317 loss_rnnt 7.015909 hw_loss 0.327002 lr 0.00039645 rank 5
2023-02-23 12:59:25,292 DEBUG TRAIN Batch 19/700 loss 6.711270 loss_att 8.760380 loss_ctc 9.351723 loss_rnnt 5.776443 hw_loss 0.324271 lr 0.00039624 rank 7
2023-02-23 12:59:25,295 DEBUG TRAIN Batch 19/700 loss 3.434346 loss_att 6.479569 loss_ctc 5.620708 loss_rnnt 2.272610 hw_loss 0.489704 lr 0.00039632 rank 5
2023-02-23 12:59:25,297 DEBUG TRAIN Batch 19/700 loss 11.205459 loss_att 17.076834 loss_ctc 15.439493 loss_rnnt 9.295278 hw_loss 0.321313 lr 0.00039628 rank 2
2023-02-23 12:59:25,298 DEBUG TRAIN Batch 19/700 loss 7.187886 loss_att 12.864120 loss_ctc 9.883010 loss_rnnt 5.494155 hw_loss 0.373376 lr 0.00039637 rank 0
2023-02-23 12:59:25,301 DEBUG TRAIN Batch 19/700 loss 9.183259 loss_att 10.042139 loss_ctc 11.666977 loss_rnnt 8.524394 hw_loss 0.292363 lr 0.00039637 rank 1
2023-02-23 12:59:25,303 DEBUG TRAIN Batch 19/700 loss 13.767039 loss_att 16.727592 loss_ctc 17.981197 loss_rnnt 12.415031 hw_loss 0.371268 lr 0.00039632 rank 6
2023-02-23 12:59:25,318 DEBUG TRAIN Batch 19/700 loss 17.367594 loss_att 19.111828 loss_ctc 24.115276 loss_rnnt 15.938633 hw_loss 0.338291 lr 0.00039634 rank 4
2023-02-23 12:59:25,331 DEBUG TRAIN Batch 19/700 loss 12.459167 loss_att 14.086584 loss_ctc 18.390043 loss_rnnt 11.168974 hw_loss 0.326113 lr 0.00039623 rank 3
2023-02-23 13:00:38,514 DEBUG TRAIN Batch 19/800 loss 10.723269 loss_att 14.804663 loss_ctc 12.556768 loss_rnnt 9.478741 hw_loss 0.344594 lr 0.00039612 rank 7
2023-02-23 13:00:38,518 DEBUG TRAIN Batch 19/800 loss 20.315588 loss_att 27.534279 loss_ctc 18.926672 loss_rnnt 18.855867 hw_loss 0.377191 lr 0.00039622 rank 4
2023-02-23 13:00:38,520 DEBUG TRAIN Batch 19/800 loss 10.304617 loss_att 15.123709 loss_ctc 16.922565 loss_rnnt 8.207584 hw_loss 0.470290 lr 0.00039624 rank 0
2023-02-23 13:00:38,523 DEBUG TRAIN Batch 19/800 loss 14.356157 loss_att 20.675165 loss_ctc 18.323521 loss_rnnt 12.412005 hw_loss 0.283819 lr 0.00039625 rank 1
2023-02-23 13:00:38,527 DEBUG TRAIN Batch 19/800 loss 7.670065 loss_att 8.031743 loss_ctc 10.921242 loss_rnnt 6.866207 hw_loss 0.558811 lr 0.00039619 rank 6
2023-02-23 13:00:38,526 DEBUG TRAIN Batch 19/800 loss 5.957862 loss_att 8.920584 loss_ctc 7.267867 loss_rnnt 5.001828 hw_loss 0.354040 lr 0.00039615 rank 2
2023-02-23 13:00:38,528 DEBUG TRAIN Batch 19/800 loss 4.174294 loss_att 8.725412 loss_ctc 5.001544 loss_rnnt 2.912630 hw_loss 0.452138 lr 0.00039611 rank 3
2023-02-23 13:00:38,532 DEBUG TRAIN Batch 19/800 loss 17.293425 loss_att 15.914424 loss_ctc 20.630987 loss_rnnt 16.907482 hw_loss 0.406374 lr 0.00039620 rank 5
2023-02-23 13:01:50,226 DEBUG TRAIN Batch 19/900 loss 4.914838 loss_att 9.309862 loss_ctc 6.378416 loss_rnnt 3.632883 hw_loss 0.389639 lr 0.00039599 rank 7
2023-02-23 13:01:50,230 DEBUG TRAIN Batch 19/900 loss 9.045418 loss_att 12.237181 loss_ctc 12.872057 loss_rnnt 7.673688 hw_loss 0.418422 lr 0.00039609 rank 4
2023-02-23 13:01:50,233 DEBUG TRAIN Batch 19/900 loss 7.278076 loss_att 10.776080 loss_ctc 9.006943 loss_rnnt 6.121751 hw_loss 0.424141 lr 0.00039607 rank 6
2023-02-23 13:01:50,232 DEBUG TRAIN Batch 19/900 loss 9.886738 loss_att 13.969012 loss_ctc 13.014097 loss_rnnt 8.424564 hw_loss 0.428883 lr 0.00039603 rank 2
2023-02-23 13:01:50,232 DEBUG TRAIN Batch 19/900 loss 3.414932 loss_att 7.124682 loss_ctc 6.981056 loss_rnnt 1.961289 hw_loss 0.442892 lr 0.00039599 rank 3
2023-02-23 13:01:50,233 DEBUG TRAIN Batch 19/900 loss 27.912973 loss_att 29.854000 loss_ctc 40.690990 loss_rnnt 25.612160 hw_loss 0.391635 lr 0.00039612 rank 0
2023-02-23 13:01:50,233 DEBUG TRAIN Batch 19/900 loss 10.322614 loss_att 14.236022 loss_ctc 13.479183 loss_rnnt 8.876330 hw_loss 0.455109 lr 0.00039608 rank 5
2023-02-23 13:01:50,237 DEBUG TRAIN Batch 19/900 loss 9.967795 loss_att 11.247576 loss_ctc 9.678337 loss_rnnt 9.517267 hw_loss 0.437186 lr 0.00039612 rank 1
2023-02-23 13:03:03,232 DEBUG TRAIN Batch 19/1000 loss 3.091795 loss_att 6.573548 loss_ctc 5.924492 loss_rnnt 1.835749 hw_loss 0.341255 lr 0.00039599 rank 0
2023-02-23 13:03:03,237 DEBUG TRAIN Batch 19/1000 loss 25.442713 loss_att 26.427242 loss_ctc 32.805367 loss_rnnt 24.042980 hw_loss 0.414639 lr 0.00039587 rank 7
2023-02-23 13:03:03,240 DEBUG TRAIN Batch 19/1000 loss 7.830153 loss_att 11.241555 loss_ctc 10.373319 loss_rnnt 6.625249 hw_loss 0.344128 lr 0.00039591 rank 2
2023-02-23 13:03:03,244 DEBUG TRAIN Batch 19/1000 loss 8.283744 loss_att 12.713529 loss_ctc 14.968063 loss_rnnt 6.316160 hw_loss 0.356973 lr 0.00039586 rank 3
2023-02-23 13:03:03,245 DEBUG TRAIN Batch 19/1000 loss 9.710405 loss_att 14.165446 loss_ctc 13.874617 loss_rnnt 8.058786 hw_loss 0.385090 lr 0.00039600 rank 1
2023-02-23 13:03:03,245 DEBUG TRAIN Batch 19/1000 loss 13.331797 loss_att 15.718309 loss_ctc 16.238247 loss_rnnt 12.258137 hw_loss 0.391556 lr 0.00039597 rank 4
2023-02-23 13:03:03,248 DEBUG TRAIN Batch 19/1000 loss 4.184556 loss_att 7.547901 loss_ctc 6.796420 loss_rnnt 2.928811 hw_loss 0.440302 lr 0.00039595 rank 6
2023-02-23 13:03:03,248 DEBUG TRAIN Batch 19/1000 loss 11.440859 loss_att 18.603582 loss_ctc 18.362816 loss_rnnt 8.881646 hw_loss 0.382014 lr 0.00039595 rank 5
2023-02-23 13:04:18,203 DEBUG TRAIN Batch 19/1100 loss 6.444044 loss_att 9.157172 loss_ctc 10.375427 loss_rnnt 5.133175 hw_loss 0.457610 lr 0.00039585 rank 4
2023-02-23 13:04:18,207 DEBUG TRAIN Batch 19/1100 loss 10.264649 loss_att 12.027165 loss_ctc 16.634422 loss_rnnt 8.821173 hw_loss 0.453132 lr 0.00039575 rank 7
2023-02-23 13:04:18,208 DEBUG TRAIN Batch 19/1100 loss 12.833286 loss_att 15.072716 loss_ctc 15.915109 loss_rnnt 11.783072 hw_loss 0.358914 lr 0.00039578 rank 2
2023-02-23 13:04:18,211 DEBUG TRAIN Batch 19/1100 loss 8.755990 loss_att 9.749780 loss_ctc 12.333344 loss_rnnt 7.827787 hw_loss 0.473371 lr 0.00039583 rank 5
2023-02-23 13:04:18,212 DEBUG TRAIN Batch 19/1100 loss 11.839140 loss_att 14.377565 loss_ctc 17.885918 loss_rnnt 10.354216 hw_loss 0.320630 lr 0.00039587 rank 0
2023-02-23 13:04:18,214 DEBUG TRAIN Batch 19/1100 loss 18.399361 loss_att 22.881567 loss_ctc 23.311848 loss_rnnt 16.621914 hw_loss 0.423760 lr 0.00039587 rank 1
2023-02-23 13:04:18,221 DEBUG TRAIN Batch 19/1100 loss 12.578603 loss_att 14.529730 loss_ctc 17.926163 loss_rnnt 11.287656 hw_loss 0.351963 lr 0.00039574 rank 3
2023-02-23 13:04:18,259 DEBUG TRAIN Batch 19/1100 loss 24.588669 loss_att 27.597868 loss_ctc 29.480982 loss_rnnt 23.153887 hw_loss 0.338682 lr 0.00039582 rank 6
2023-02-23 13:05:31,616 DEBUG TRAIN Batch 19/1200 loss 10.646478 loss_att 10.199533 loss_ctc 12.973529 loss_rnnt 10.131718 hw_loss 0.551016 lr 0.00039570 rank 5
2023-02-23 13:05:31,617 DEBUG TRAIN Batch 19/1200 loss 10.144240 loss_att 13.449408 loss_ctc 11.668775 loss_rnnt 9.060171 hw_loss 0.412060 lr 0.00039562 rank 7
2023-02-23 13:05:31,618 DEBUG TRAIN Batch 19/1200 loss 7.912215 loss_att 9.381334 loss_ctc 13.027373 loss_rnnt 6.693407 hw_loss 0.455558 lr 0.00039561 rank 3
2023-02-23 13:05:31,618 DEBUG TRAIN Batch 19/1200 loss 6.358871 loss_att 9.208172 loss_ctc 11.197371 loss_rnnt 4.884954 hw_loss 0.485480 lr 0.00039566 rank 2
2023-02-23 13:05:31,620 DEBUG TRAIN Batch 19/1200 loss 12.479544 loss_att 13.330345 loss_ctc 17.640493 loss_rnnt 11.351730 hw_loss 0.505361 lr 0.00039570 rank 6
2023-02-23 13:05:31,620 DEBUG TRAIN Batch 19/1200 loss 8.356611 loss_att 12.550817 loss_ctc 12.845532 loss_rnnt 6.733744 hw_loss 0.347816 lr 0.00039572 rank 4
2023-02-23 13:05:31,620 DEBUG TRAIN Batch 19/1200 loss 13.805770 loss_att 15.114692 loss_ctc 17.637423 loss_rnnt 12.784377 hw_loss 0.466353 lr 0.00039575 rank 0
2023-02-23 13:05:31,678 DEBUG TRAIN Batch 19/1200 loss 5.105071 loss_att 9.273785 loss_ctc 6.207593 loss_rnnt 3.897160 hw_loss 0.425934 lr 0.00039575 rank 1
2023-02-23 13:06:44,218 DEBUG TRAIN Batch 19/1300 loss 11.768575 loss_att 13.240196 loss_ctc 16.037281 loss_rnnt 10.673289 hw_loss 0.434625 lr 0.00039550 rank 7
2023-02-23 13:06:44,219 DEBUG TRAIN Batch 19/1300 loss 5.972801 loss_att 7.157509 loss_ctc 10.574829 loss_rnnt 4.901788 hw_loss 0.413378 lr 0.00039558 rank 5
2023-02-23 13:06:44,221 DEBUG TRAIN Batch 19/1300 loss 8.734367 loss_att 9.450552 loss_ctc 10.108917 loss_rnnt 8.086271 hw_loss 0.602971 lr 0.00039562 rank 0
2023-02-23 13:06:44,222 DEBUG TRAIN Batch 19/1300 loss 4.645141 loss_att 7.792355 loss_ctc 8.162928 loss_rnnt 3.332644 hw_loss 0.401280 lr 0.00039549 rank 3
2023-02-23 13:06:44,222 DEBUG TRAIN Batch 19/1300 loss 8.371275 loss_att 12.509562 loss_ctc 9.733540 loss_rnnt 7.177711 hw_loss 0.345512 lr 0.00039553 rank 2
2023-02-23 13:06:44,224 DEBUG TRAIN Batch 19/1300 loss 6.482521 loss_att 9.572366 loss_ctc 8.465612 loss_rnnt 5.354634 hw_loss 0.460320 lr 0.00039563 rank 1
2023-02-23 13:06:44,225 DEBUG TRAIN Batch 19/1300 loss 8.598347 loss_att 7.795116 loss_ctc 10.623163 loss_rnnt 8.192328 hw_loss 0.556292 lr 0.00039560 rank 4
2023-02-23 13:06:44,227 DEBUG TRAIN Batch 19/1300 loss 25.785824 loss_att 24.922960 loss_ctc 37.271004 loss_rnnt 24.249546 hw_loss 0.332801 lr 0.00039557 rank 6
2023-02-23 13:07:58,385 DEBUG TRAIN Batch 19/1400 loss 9.892444 loss_att 13.588545 loss_ctc 15.353996 loss_rnnt 8.169532 hw_loss 0.479033 lr 0.00039547 rank 4
2023-02-23 13:07:58,393 DEBUG TRAIN Batch 19/1400 loss 6.788903 loss_att 10.321623 loss_ctc 10.866615 loss_rnnt 5.300114 hw_loss 0.447283 lr 0.00039541 rank 2
2023-02-23 13:07:58,400 DEBUG TRAIN Batch 19/1400 loss 5.914790 loss_att 8.722220 loss_ctc 12.087714 loss_rnnt 4.318323 hw_loss 0.397358 lr 0.00039537 rank 7
2023-02-23 13:07:58,401 DEBUG TRAIN Batch 19/1400 loss 8.509380 loss_att 9.871206 loss_ctc 9.951969 loss_rnnt 7.825146 hw_loss 0.411608 lr 0.00039537 rank 3
2023-02-23 13:07:58,403 DEBUG TRAIN Batch 19/1400 loss 7.892217 loss_att 12.675282 loss_ctc 10.710451 loss_rnnt 6.378585 hw_loss 0.339851 lr 0.00039550 rank 1
2023-02-23 13:07:58,407 DEBUG TRAIN Batch 19/1400 loss 11.862692 loss_att 17.529388 loss_ctc 20.796412 loss_rnnt 9.301139 hw_loss 0.444470 lr 0.00039545 rank 6
2023-02-23 13:07:58,429 DEBUG TRAIN Batch 19/1400 loss 4.695176 loss_att 11.443464 loss_ctc 6.829328 loss_rnnt 2.804061 hw_loss 0.481694 lr 0.00039550 rank 0
2023-02-23 13:07:58,449 DEBUG TRAIN Batch 19/1400 loss 4.698381 loss_att 7.129020 loss_ctc 7.828739 loss_rnnt 3.531713 hw_loss 0.493423 lr 0.00039546 rank 5
2023-02-23 13:09:12,182 DEBUG TRAIN Batch 19/1500 loss 8.340876 loss_att 12.137551 loss_ctc 11.198642 loss_rnnt 6.936490 hw_loss 0.495026 lr 0.00039533 rank 6
2023-02-23 13:09:12,183 DEBUG TRAIN Batch 19/1500 loss 11.956074 loss_att 11.769718 loss_ctc 15.175494 loss_rnnt 11.269039 hw_loss 0.553217 lr 0.00039524 rank 3
2023-02-23 13:09:12,182 DEBUG TRAIN Batch 19/1500 loss 11.548423 loss_att 14.248778 loss_ctc 13.678856 loss_rnnt 10.526754 hw_loss 0.370389 lr 0.00039533 rank 5
2023-02-23 13:09:12,183 DEBUG TRAIN Batch 19/1500 loss 7.330738 loss_att 10.208748 loss_ctc 10.192935 loss_rnnt 6.175661 hw_loss 0.370968 lr 0.00039535 rank 4
2023-02-23 13:09:12,186 DEBUG TRAIN Batch 19/1500 loss 5.476653 loss_att 9.404608 loss_ctc 6.203886 loss_rnnt 4.417095 hw_loss 0.331879 lr 0.00039529 rank 2
2023-02-23 13:09:12,188 DEBUG TRAIN Batch 19/1500 loss 9.438201 loss_att 11.353877 loss_ctc 11.292419 loss_rnnt 8.620789 hw_loss 0.350716 lr 0.00039537 rank 0
2023-02-23 13:09:12,191 DEBUG TRAIN Batch 19/1500 loss 6.786472 loss_att 12.031203 loss_ctc 8.141685 loss_rnnt 5.344376 hw_loss 0.398355 lr 0.00039525 rank 7
2023-02-23 13:09:12,236 DEBUG TRAIN Batch 19/1500 loss 7.185974 loss_att 11.188024 loss_ctc 9.520500 loss_rnnt 5.821451 hw_loss 0.474081 lr 0.00039538 rank 1
2023-02-23 13:10:23,702 DEBUG TRAIN Batch 19/1600 loss 11.439519 loss_att 15.917545 loss_ctc 16.559671 loss_rnnt 9.671039 hw_loss 0.356606 lr 0.00039513 rank 7
2023-02-23 13:10:23,703 DEBUG TRAIN Batch 19/1600 loss 7.508540 loss_att 11.178947 loss_ctc 9.457900 loss_rnnt 6.303067 hw_loss 0.396519 lr 0.00039512 rank 3
2023-02-23 13:10:23,704 DEBUG TRAIN Batch 19/1600 loss 12.161211 loss_att 15.840638 loss_ctc 17.036974 loss_rnnt 10.519108 hw_loss 0.480218 lr 0.00039523 rank 4
2023-02-23 13:10:23,707 DEBUG TRAIN Batch 19/1600 loss 8.590239 loss_att 10.587988 loss_ctc 12.994789 loss_rnnt 7.392891 hw_loss 0.394730 lr 0.00039520 rank 6
2023-02-23 13:10:23,709 DEBUG TRAIN Batch 19/1600 loss 13.202356 loss_att 14.500755 loss_ctc 15.820483 loss_rnnt 12.359547 hw_loss 0.438836 lr 0.00039516 rank 2
2023-02-23 13:10:23,709 DEBUG TRAIN Batch 19/1600 loss 11.408407 loss_att 14.006982 loss_ctc 18.797716 loss_rnnt 9.670822 hw_loss 0.436179 lr 0.00039526 rank 1
2023-02-23 13:10:23,713 DEBUG TRAIN Batch 19/1600 loss 7.607902 loss_att 9.570387 loss_ctc 9.658528 loss_rnnt 6.725944 hw_loss 0.405083 lr 0.00039525 rank 0
2023-02-23 13:10:23,715 DEBUG TRAIN Batch 19/1600 loss 2.780752 loss_att 4.701365 loss_ctc 2.989733 loss_rnnt 2.149694 hw_loss 0.410758 lr 0.00039521 rank 5
2023-02-23 13:11:36,885 DEBUG TRAIN Batch 19/1700 loss 10.341003 loss_att 13.002579 loss_ctc 12.507994 loss_rnnt 9.315424 hw_loss 0.383121 lr 0.00039508 rank 5
2023-02-23 13:11:36,895 DEBUG TRAIN Batch 19/1700 loss 8.305771 loss_att 10.409488 loss_ctc 11.009712 loss_rnnt 7.326359 hw_loss 0.371518 lr 0.00039500 rank 7
2023-02-23 13:11:36,896 DEBUG TRAIN Batch 19/1700 loss 10.563509 loss_att 12.894863 loss_ctc 13.561727 loss_rnnt 9.518025 hw_loss 0.336468 lr 0.00039510 rank 4
2023-02-23 13:11:36,903 DEBUG TRAIN Batch 19/1700 loss 9.830070 loss_att 10.214718 loss_ctc 10.074112 loss_rnnt 9.450569 hw_loss 0.506310 lr 0.00039500 rank 3
2023-02-23 13:11:36,905 DEBUG TRAIN Batch 19/1700 loss 11.991056 loss_att 13.525654 loss_ctc 14.490295 loss_rnnt 11.138130 hw_loss 0.398950 lr 0.00039504 rank 2
2023-02-23 13:11:36,918 DEBUG TRAIN Batch 19/1700 loss 10.480597 loss_att 11.990567 loss_ctc 12.490322 loss_rnnt 9.725768 hw_loss 0.346634 lr 0.00039513 rank 1
2023-02-23 13:11:36,928 DEBUG TRAIN Batch 19/1700 loss 4.409618 loss_att 7.613456 loss_ctc 7.092016 loss_rnnt 3.165050 hw_loss 0.461525 lr 0.00039513 rank 0
2023-02-23 13:11:36,952 DEBUG TRAIN Batch 19/1700 loss 10.628789 loss_att 11.850422 loss_ctc 16.523348 loss_rnnt 9.371686 hw_loss 0.425315 lr 0.00039508 rank 6
2023-02-23 13:12:52,262 DEBUG TRAIN Batch 19/1800 loss 5.678310 loss_att 8.159072 loss_ctc 8.547093 loss_rnnt 4.536717 hw_loss 0.493006 lr 0.00039500 rank 0
2023-02-23 13:12:52,263 DEBUG TRAIN Batch 19/1800 loss 10.020894 loss_att 12.230470 loss_ctc 17.617867 loss_rnnt 8.368842 hw_loss 0.369763 lr 0.00039501 rank 1
2023-02-23 13:12:52,268 DEBUG TRAIN Batch 19/1800 loss 12.583432 loss_att 12.732922 loss_ctc 18.560822 loss_rnnt 11.522655 hw_loss 0.438553 lr 0.00039496 rank 6
2023-02-23 13:12:52,268 DEBUG TRAIN Batch 19/1800 loss 8.446888 loss_att 10.842927 loss_ctc 12.723082 loss_rnnt 7.196486 hw_loss 0.376939 lr 0.00039488 rank 7
2023-02-23 13:12:52,269 DEBUG TRAIN Batch 19/1800 loss 10.490059 loss_att 13.513238 loss_ctc 16.030838 loss_rnnt 8.883916 hw_loss 0.492629 lr 0.00039492 rank 2
2023-02-23 13:12:52,270 DEBUG TRAIN Batch 19/1800 loss 16.520189 loss_att 16.660103 loss_ctc 23.182455 loss_rnnt 15.327761 hw_loss 0.517771 lr 0.00039496 rank 5
2023-02-23 13:12:52,270 DEBUG TRAIN Batch 19/1800 loss 7.446312 loss_att 8.134170 loss_ctc 8.477469 loss_rnnt 6.879761 hw_loss 0.546549 lr 0.00039487 rank 3
2023-02-23 13:12:52,317 DEBUG TRAIN Batch 19/1800 loss 6.535501 loss_att 10.191882 loss_ctc 10.799305 loss_rnnt 4.951372 hw_loss 0.533146 lr 0.00039498 rank 4
2023-02-23 13:14:05,545 DEBUG TRAIN Batch 19/1900 loss 11.880598 loss_att 14.419781 loss_ctc 14.544783 loss_rnnt 10.796259 hw_loss 0.414897 lr 0.00039475 rank 3
2023-02-23 13:14:05,546 DEBUG TRAIN Batch 19/1900 loss 8.971612 loss_att 10.563475 loss_ctc 15.763166 loss_rnnt 7.477435 hw_loss 0.506744 lr 0.00039486 rank 4
2023-02-23 13:14:05,551 DEBUG TRAIN Batch 19/1900 loss 6.760939 loss_att 8.608976 loss_ctc 9.265040 loss_rnnt 5.787910 hw_loss 0.505389 lr 0.00039489 rank 1
2023-02-23 13:14:05,551 DEBUG TRAIN Batch 19/1900 loss 10.816909 loss_att 10.374195 loss_ctc 13.782421 loss_rnnt 10.251522 hw_loss 0.484738 lr 0.00039476 rank 7
2023-02-23 13:14:05,553 DEBUG TRAIN Batch 19/1900 loss 10.637516 loss_att 10.866598 loss_ctc 14.761519 loss_rnnt 9.738396 hw_loss 0.568944 lr 0.00039479 rank 2
2023-02-23 13:14:05,554 DEBUG TRAIN Batch 19/1900 loss 14.882896 loss_att 21.490250 loss_ctc 25.779142 loss_rnnt 11.861647 hw_loss 0.463026 lr 0.00039484 rank 5
2023-02-23 13:14:05,555 DEBUG TRAIN Batch 19/1900 loss 12.550249 loss_att 13.993671 loss_ctc 15.246645 loss_rnnt 11.671674 hw_loss 0.431947 lr 0.00039488 rank 0
2023-02-23 13:14:05,556 DEBUG TRAIN Batch 19/1900 loss 11.616918 loss_att 16.102711 loss_ctc 17.610447 loss_rnnt 9.758659 hw_loss 0.303681 lr 0.00039483 rank 6
2023-02-23 13:15:17,999 DEBUG TRAIN Batch 19/2000 loss 7.022178 loss_att 9.747280 loss_ctc 12.111357 loss_rnnt 5.618953 hw_loss 0.336839 lr 0.00039471 rank 6
2023-02-23 13:15:18,000 DEBUG TRAIN Batch 19/2000 loss 7.497249 loss_att 11.742715 loss_ctc 13.481283 loss_rnnt 5.669330 hw_loss 0.339290 lr 0.00039476 rank 0
2023-02-23 13:15:18,007 DEBUG TRAIN Batch 19/2000 loss 14.139938 loss_att 17.636150 loss_ctc 20.102367 loss_rnnt 12.474197 hw_loss 0.321577 lr 0.00039463 rank 7
2023-02-23 13:15:18,008 DEBUG TRAIN Batch 19/2000 loss 6.287643 loss_att 11.216276 loss_ctc 9.006625 loss_rnnt 4.687303 hw_loss 0.472655 lr 0.00039473 rank 4
2023-02-23 13:15:18,009 DEBUG TRAIN Batch 19/2000 loss 8.181831 loss_att 9.863031 loss_ctc 8.896983 loss_rnnt 7.575775 hw_loss 0.327117 lr 0.00039476 rank 1
2023-02-23 13:15:18,009 DEBUG TRAIN Batch 19/2000 loss 6.590635 loss_att 9.109261 loss_ctc 10.499693 loss_rnnt 5.351310 hw_loss 0.401984 lr 0.00039463 rank 3
2023-02-23 13:15:18,011 DEBUG TRAIN Batch 19/2000 loss 9.764417 loss_att 12.674704 loss_ctc 17.598782 loss_rnnt 7.938320 hw_loss 0.373981 lr 0.00039467 rank 2
2023-02-23 13:15:18,064 DEBUG TRAIN Batch 19/2000 loss 6.281039 loss_att 10.435446 loss_ctc 8.494549 loss_rnnt 4.938676 hw_loss 0.405650 lr 0.00039472 rank 5
2023-02-23 13:16:32,955 DEBUG TRAIN Batch 19/2100 loss 13.587504 loss_att 15.042897 loss_ctc 18.918104 loss_rnnt 12.360519 hw_loss 0.422176 lr 0.00039451 rank 7
2023-02-23 13:16:32,962 DEBUG TRAIN Batch 19/2100 loss 5.734424 loss_att 8.053850 loss_ctc 6.472942 loss_rnnt 4.945402 hw_loss 0.425003 lr 0.00039464 rank 1
2023-02-23 13:16:32,964 DEBUG TRAIN Batch 19/2100 loss 11.188851 loss_att 13.724557 loss_ctc 16.926491 loss_rnnt 9.749930 hw_loss 0.312679 lr 0.00039461 rank 4
2023-02-23 13:16:32,964 DEBUG TRAIN Batch 19/2100 loss 4.200959 loss_att 6.846601 loss_ctc 5.710522 loss_rnnt 3.213331 hw_loss 0.482297 lr 0.00039450 rank 3
2023-02-23 13:16:32,965 DEBUG TRAIN Batch 19/2100 loss 11.031805 loss_att 13.450235 loss_ctc 12.746078 loss_rnnt 10.089691 hw_loss 0.430985 lr 0.00039459 rank 6
2023-02-23 13:16:32,967 DEBUG TRAIN Batch 19/2100 loss 7.687215 loss_att 10.963236 loss_ctc 11.016586 loss_rnnt 6.398995 hw_loss 0.354560 lr 0.00039459 rank 5
2023-02-23 13:16:32,967 DEBUG TRAIN Batch 19/2100 loss 8.772249 loss_att 8.647034 loss_ctc 12.960391 loss_rnnt 8.007773 hw_loss 0.433314 lr 0.00039455 rank 2
2023-02-23 13:16:33,009 DEBUG TRAIN Batch 19/2100 loss 6.481248 loss_att 12.058843 loss_ctc 10.369589 loss_rnnt 4.656556 hw_loss 0.357614 lr 0.00039463 rank 0
2023-02-23 13:17:46,849 DEBUG TRAIN Batch 19/2200 loss 11.879879 loss_att 15.732458 loss_ctc 19.439217 loss_rnnt 9.888437 hw_loss 0.399400 lr 0.00039438 rank 3
2023-02-23 13:17:46,853 DEBUG TRAIN Batch 19/2200 loss 12.998666 loss_att 15.910202 loss_ctc 19.858692 loss_rnnt 11.292276 hw_loss 0.392644 lr 0.00039452 rank 1
2023-02-23 13:17:46,853 DEBUG TRAIN Batch 19/2200 loss 9.448788 loss_att 10.940825 loss_ctc 12.387835 loss_rnnt 8.506031 hw_loss 0.473395 lr 0.00039451 rank 0
2023-02-23 13:17:46,854 DEBUG TRAIN Batch 19/2200 loss 9.710525 loss_att 12.228230 loss_ctc 12.971217 loss_rnnt 8.597899 hw_loss 0.326860 lr 0.00039449 rank 4
2023-02-23 13:17:46,854 DEBUG TRAIN Batch 19/2200 loss 5.879195 loss_att 9.423117 loss_ctc 6.495225 loss_rnnt 4.896783 hw_loss 0.359044 lr 0.00039442 rank 2
2023-02-23 13:17:46,855 DEBUG TRAIN Batch 19/2200 loss 9.205014 loss_att 13.933878 loss_ctc 14.724201 loss_rnnt 7.274574 hw_loss 0.466453 lr 0.00039439 rank 7
2023-02-23 13:17:46,862 DEBUG TRAIN Batch 19/2200 loss 7.190762 loss_att 11.476393 loss_ctc 9.913572 loss_rnnt 5.762279 hw_loss 0.390591 lr 0.00039446 rank 6
2023-02-23 13:17:46,906 DEBUG TRAIN Batch 19/2200 loss 6.559431 loss_att 8.109977 loss_ctc 7.290246 loss_rnnt 5.949449 hw_loss 0.379559 lr 0.00039447 rank 5
2023-02-23 13:18:59,132 DEBUG TRAIN Batch 19/2300 loss 8.142928 loss_att 9.805931 loss_ctc 8.270411 loss_rnnt 7.530523 hw_loss 0.492764 lr 0.00039430 rank 2
2023-02-23 13:18:59,133 DEBUG TRAIN Batch 19/2300 loss 6.877359 loss_att 9.589754 loss_ctc 10.837207 loss_rnnt 5.621903 hw_loss 0.346870 lr 0.00039437 rank 4
2023-02-23 13:18:59,136 DEBUG TRAIN Batch 19/2300 loss 11.407231 loss_att 15.530835 loss_ctc 16.428732 loss_rnnt 9.612047 hw_loss 0.564243 lr 0.00039434 rank 6
2023-02-23 13:18:59,135 DEBUG TRAIN Batch 19/2300 loss 5.232751 loss_att 7.008626 loss_ctc 6.239662 loss_rnnt 4.555733 hw_loss 0.351726 lr 0.00039427 rank 7
2023-02-23 13:18:59,136 DEBUG TRAIN Batch 19/2300 loss 3.688334 loss_att 5.973909 loss_ctc 6.277004 loss_rnnt 2.678370 hw_loss 0.389423 lr 0.00039426 rank 3
2023-02-23 13:18:59,138 DEBUG TRAIN Batch 19/2300 loss 3.270581 loss_att 8.483963 loss_ctc 7.645405 loss_rnnt 1.446519 hw_loss 0.371391 lr 0.00039435 rank 5
2023-02-23 13:18:59,139 DEBUG TRAIN Batch 19/2300 loss 7.520067 loss_att 9.971891 loss_ctc 11.703175 loss_rnnt 6.215285 hw_loss 0.481254 lr 0.00039439 rank 1
2023-02-23 13:18:59,149 DEBUG TRAIN Batch 19/2300 loss 10.291636 loss_att 14.146840 loss_ctc 15.537468 loss_rnnt 8.633520 hw_loss 0.351805 lr 0.00039439 rank 0
2023-02-23 13:20:11,957 DEBUG TRAIN Batch 19/2400 loss 11.828673 loss_att 13.527237 loss_ctc 17.708637 loss_rnnt 10.387589 hw_loss 0.595080 lr 0.00039414 rank 7
2023-02-23 13:20:11,962 DEBUG TRAIN Batch 19/2400 loss 2.531705 loss_att 4.723621 loss_ctc 3.938226 loss_rnnt 1.697444 hw_loss 0.390639 lr 0.00039427 rank 1
2023-02-23 13:20:11,963 DEBUG TRAIN Batch 19/2400 loss 12.787072 loss_att 14.152048 loss_ctc 15.898355 loss_rnnt 11.860088 hw_loss 0.448408 lr 0.00039424 rank 4
2023-02-23 13:20:11,964 DEBUG TRAIN Batch 19/2400 loss 13.610872 loss_att 16.406731 loss_ctc 20.366907 loss_rnnt 11.922126 hw_loss 0.428944 lr 0.00039414 rank 3
2023-02-23 13:20:11,965 DEBUG TRAIN Batch 19/2400 loss 9.496778 loss_att 12.744618 loss_ctc 18.740105 loss_rnnt 7.414262 hw_loss 0.375945 lr 0.00039427 rank 0
2023-02-23 13:20:11,965 DEBUG TRAIN Batch 19/2400 loss 10.671614 loss_att 12.913706 loss_ctc 15.169623 loss_rnnt 9.362175 hw_loss 0.489911 lr 0.00039422 rank 5
2023-02-23 13:20:11,989 DEBUG TRAIN Batch 19/2400 loss 5.638336 loss_att 7.779075 loss_ctc 7.368744 loss_rnnt 4.724955 hw_loss 0.477210 lr 0.00039418 rank 2
2023-02-23 13:20:12,014 DEBUG TRAIN Batch 19/2400 loss 10.543093 loss_att 11.391171 loss_ctc 13.517070 loss_rnnt 9.786410 hw_loss 0.357257 lr 0.00039422 rank 6
2023-02-23 13:21:27,547 DEBUG TRAIN Batch 19/2500 loss 22.334356 loss_att 21.680700 loss_ctc 28.183426 loss_rnnt 21.442574 hw_loss 0.454946 lr 0.00039402 rank 7
2023-02-23 13:21:27,549 DEBUG TRAIN Batch 19/2500 loss 12.447716 loss_att 15.646687 loss_ctc 15.591673 loss_rnnt 11.161860 hw_loss 0.425376 lr 0.00039401 rank 3
2023-02-23 13:21:27,550 DEBUG TRAIN Batch 19/2500 loss 3.164209 loss_att 6.285901 loss_ctc 4.500611 loss_rnnt 2.161503 hw_loss 0.375339 lr 0.00039410 rank 6
2023-02-23 13:21:27,551 DEBUG TRAIN Batch 19/2500 loss 8.156911 loss_att 11.864325 loss_ctc 11.597160 loss_rnnt 6.743249 hw_loss 0.400273 lr 0.00039412 rank 4
2023-02-23 13:21:27,553 DEBUG TRAIN Batch 19/2500 loss 10.218760 loss_att 11.812243 loss_ctc 13.357539 loss_rnnt 9.129196 hw_loss 0.660679 lr 0.00039415 rank 1
2023-02-23 13:21:27,554 DEBUG TRAIN Batch 19/2500 loss 8.592198 loss_att 10.813269 loss_ctc 12.972597 loss_rnnt 7.341222 hw_loss 0.417579 lr 0.00039406 rank 2
2023-02-23 13:21:27,576 DEBUG TRAIN Batch 19/2500 loss 15.276750 loss_att 19.777140 loss_ctc 24.620621 loss_rnnt 12.946352 hw_loss 0.345882 lr 0.00039410 rank 5
2023-02-23 13:21:27,603 DEBUG TRAIN Batch 19/2500 loss 5.027990 loss_att 7.446283 loss_ctc 5.993851 loss_rnnt 4.144682 hw_loss 0.507877 lr 0.00039414 rank 0
2023-02-23 13:22:40,912 DEBUG TRAIN Batch 19/2600 loss 12.021607 loss_att 15.210473 loss_ctc 18.465893 loss_rnnt 10.249898 hw_loss 0.515057 lr 0.00039389 rank 3
2023-02-23 13:22:40,912 DEBUG TRAIN Batch 19/2600 loss 14.144927 loss_att 19.212072 loss_ctc 20.622751 loss_rnnt 12.078074 hw_loss 0.355713 lr 0.00039390 rank 7
2023-02-23 13:22:40,916 DEBUG TRAIN Batch 19/2600 loss 13.157670 loss_att 15.112583 loss_ctc 17.500895 loss_rnnt 11.943264 hw_loss 0.458111 lr 0.00039403 rank 1
2023-02-23 13:22:40,917 DEBUG TRAIN Batch 19/2600 loss 6.227964 loss_att 9.964143 loss_ctc 9.527145 loss_rnnt 4.852665 hw_loss 0.352824 lr 0.00039393 rank 2
2023-02-23 13:22:40,918 DEBUG TRAIN Batch 19/2600 loss 9.455080 loss_att 9.634105 loss_ctc 12.681226 loss_rnnt 8.582100 hw_loss 0.763166 lr 0.00039400 rank 4
2023-02-23 13:22:40,919 DEBUG TRAIN Batch 19/2600 loss 6.026859 loss_att 7.500194 loss_ctc 8.426875 loss_rnnt 5.165289 hw_loss 0.462937 lr 0.00039402 rank 0
2023-02-23 13:22:40,923 DEBUG TRAIN Batch 19/2600 loss 5.893639 loss_att 10.491981 loss_ctc 7.682918 loss_rnnt 4.527805 hw_loss 0.389238 lr 0.00039397 rank 6
2023-02-23 13:22:40,967 DEBUG TRAIN Batch 19/2600 loss 16.095968 loss_att 16.638966 loss_ctc 21.074383 loss_rnnt 15.033461 hw_loss 0.543978 lr 0.00039398 rank 5
2023-02-23 13:23:53,374 DEBUG TRAIN Batch 19/2700 loss 11.921609 loss_att 12.736988 loss_ctc 11.289870 loss_rnnt 11.658213 hw_loss 0.346033 lr 0.00039378 rank 7
2023-02-23 13:23:53,376 DEBUG TRAIN Batch 19/2700 loss 11.653833 loss_att 13.897797 loss_ctc 16.869991 loss_rnnt 10.275866 hw_loss 0.438167 lr 0.00039377 rank 3
2023-02-23 13:23:53,377 DEBUG TRAIN Batch 19/2700 loss 14.372915 loss_att 14.247112 loss_ctc 18.386448 loss_rnnt 13.669487 hw_loss 0.362720 lr 0.00039390 rank 1
2023-02-23 13:23:53,380 DEBUG TRAIN Batch 19/2700 loss 15.497502 loss_att 20.346951 loss_ctc 23.501822 loss_rnnt 13.275585 hw_loss 0.346470 lr 0.00039381 rank 2
2023-02-23 13:23:53,382 DEBUG TRAIN Batch 19/2700 loss 12.044093 loss_att 13.916574 loss_ctc 16.677034 loss_rnnt 10.785378 hw_loss 0.499674 lr 0.00039386 rank 5
2023-02-23 13:23:53,383 DEBUG TRAIN Batch 19/2700 loss 7.782633 loss_att 10.742088 loss_ctc 10.879711 loss_rnnt 6.587868 hw_loss 0.356120 lr 0.00039385 rank 6
2023-02-23 13:23:53,387 DEBUG TRAIN Batch 19/2700 loss 4.592711 loss_att 7.560531 loss_ctc 5.251359 loss_rnnt 3.694147 hw_loss 0.407213 lr 0.00039390 rank 0
2023-02-23 13:23:53,390 DEBUG TRAIN Batch 19/2700 loss 5.627872 loss_att 10.372597 loss_ctc 12.262343 loss_rnnt 3.571914 hw_loss 0.417032 lr 0.00039388 rank 4
2023-02-23 13:25:08,139 DEBUG TRAIN Batch 19/2800 loss 10.091290 loss_att 14.651541 loss_ctc 12.850700 loss_rnnt 8.639837 hw_loss 0.321528 lr 0.00039375 rank 4
2023-02-23 13:25:08,144 DEBUG TRAIN Batch 19/2800 loss 11.037645 loss_att 17.087559 loss_ctc 14.501005 loss_rnnt 9.112659 hw_loss 0.474788 lr 0.00039378 rank 1
2023-02-23 13:25:08,155 DEBUG TRAIN Batch 19/2800 loss 6.840814 loss_att 9.880704 loss_ctc 10.048401 loss_rnnt 5.582204 hw_loss 0.418038 lr 0.00039373 rank 6
2023-02-23 13:25:08,156 DEBUG TRAIN Batch 19/2800 loss 24.375540 loss_att 28.870834 loss_ctc 31.715832 loss_rnnt 22.287064 hw_loss 0.395085 lr 0.00039378 rank 0
2023-02-23 13:25:08,156 DEBUG TRAIN Batch 19/2800 loss 13.398458 loss_att 14.053835 loss_ctc 15.041883 loss_rnnt 12.820972 hw_loss 0.426162 lr 0.00039365 rank 3
2023-02-23 13:25:08,157 DEBUG TRAIN Batch 19/2800 loss 6.074491 loss_att 8.291981 loss_ctc 8.899285 loss_rnnt 5.065320 hw_loss 0.354438 lr 0.00039369 rank 2
2023-02-23 13:25:08,159 DEBUG TRAIN Batch 19/2800 loss 5.357675 loss_att 8.312592 loss_ctc 7.441867 loss_rnnt 4.276413 hw_loss 0.398223 lr 0.00039365 rank 7
2023-02-23 13:25:08,179 DEBUG TRAIN Batch 19/2800 loss 12.788164 loss_att 15.222602 loss_ctc 14.770112 loss_rnnt 11.764482 hw_loss 0.511003 lr 0.00039373 rank 5
2023-02-23 13:26:22,402 DEBUG TRAIN Batch 19/2900 loss 15.965042 loss_att 19.533623 loss_ctc 25.749596 loss_rnnt 13.717718 hw_loss 0.429377 lr 0.00039353 rank 3
2023-02-23 13:26:22,403 DEBUG TRAIN Batch 19/2900 loss 14.799725 loss_att 19.273575 loss_ctc 17.987539 loss_rnnt 13.307611 hw_loss 0.323067 lr 0.00039366 rank 1
2023-02-23 13:26:22,406 DEBUG TRAIN Batch 19/2900 loss 10.862031 loss_att 11.853945 loss_ctc 16.501701 loss_rnnt 9.691482 hw_loss 0.412896 lr 0.00039353 rank 7
2023-02-23 13:26:22,408 DEBUG TRAIN Batch 19/2900 loss 10.791343 loss_att 14.157168 loss_ctc 13.536136 loss_rnnt 9.524182 hw_loss 0.427542 lr 0.00039361 rank 6
2023-02-23 13:26:22,408 DEBUG TRAIN Batch 19/2900 loss 7.727770 loss_att 11.676838 loss_ctc 13.532172 loss_rnnt 5.936211 hw_loss 0.427173 lr 0.00039357 rank 2
2023-02-23 13:26:22,414 DEBUG TRAIN Batch 19/2900 loss 8.207565 loss_att 11.560553 loss_ctc 12.288879 loss_rnnt 6.757368 hw_loss 0.441420 lr 0.00039365 rank 0
2023-02-23 13:26:22,413 DEBUG TRAIN Batch 19/2900 loss 10.671548 loss_att 12.004566 loss_ctc 15.755548 loss_rnnt 9.537432 hw_loss 0.355586 lr 0.00039361 rank 5
2023-02-23 13:26:22,418 DEBUG TRAIN Batch 19/2900 loss 10.686282 loss_att 16.303894 loss_ctc 20.110598 loss_rnnt 8.125681 hw_loss 0.338446 lr 0.00039363 rank 4
2023-02-23 13:27:35,333 DEBUG TRAIN Batch 19/3000 loss 4.662433 loss_att 7.413815 loss_ctc 8.208323 loss_rnnt 3.420429 hw_loss 0.410515 lr 0.00039354 rank 1
2023-02-23 13:27:35,335 DEBUG TRAIN Batch 19/3000 loss 8.751000 loss_att 10.323833 loss_ctc 13.516244 loss_rnnt 7.613132 hw_loss 0.352379 lr 0.00039349 rank 6
2023-02-23 13:27:35,335 DEBUG TRAIN Batch 19/3000 loss 12.576030 loss_att 16.760723 loss_ctc 19.694597 loss_rnnt 10.591944 hw_loss 0.371258 lr 0.00039345 rank 2
2023-02-23 13:27:35,335 DEBUG TRAIN Batch 19/3000 loss 13.129377 loss_att 13.645073 loss_ctc 16.908638 loss_rnnt 12.266757 hw_loss 0.479211 lr 0.00039340 rank 3
2023-02-23 13:27:35,336 DEBUG TRAIN Batch 19/3000 loss 7.356297 loss_att 12.125530 loss_ctc 13.872042 loss_rnnt 5.342144 hw_loss 0.359140 lr 0.00039351 rank 4
2023-02-23 13:27:35,338 DEBUG TRAIN Batch 19/3000 loss 6.415538 loss_att 9.636118 loss_ctc 6.661471 loss_rnnt 5.514162 hw_loss 0.420879 lr 0.00039341 rank 7
2023-02-23 13:27:35,340 DEBUG TRAIN Batch 19/3000 loss 11.064703 loss_att 13.219294 loss_ctc 14.549221 loss_rnnt 9.989599 hw_loss 0.336718 lr 0.00039353 rank 0
2023-02-23 13:27:35,379 DEBUG TRAIN Batch 19/3000 loss 10.106512 loss_att 11.502897 loss_ctc 13.059245 loss_rnnt 9.209651 hw_loss 0.419789 lr 0.00039349 rank 5
2023-02-23 13:28:48,001 DEBUG TRAIN Batch 19/3100 loss 17.733889 loss_att 19.156160 loss_ctc 21.387753 loss_rnnt 16.757225 hw_loss 0.384424 lr 0.00039342 rank 1
2023-02-23 13:28:48,003 DEBUG TRAIN Batch 19/3100 loss 27.971136 loss_att 32.117073 loss_ctc 34.405529 loss_rnnt 26.114487 hw_loss 0.317891 lr 0.00039337 rank 5
2023-02-23 13:28:48,012 DEBUG TRAIN Batch 19/3100 loss 7.312169 loss_att 7.622846 loss_ctc 8.458338 loss_rnnt 6.871659 hw_loss 0.422909 lr 0.00039329 rank 7
2023-02-23 13:28:48,016 DEBUG TRAIN Batch 19/3100 loss 7.623693 loss_att 12.829236 loss_ctc 12.353293 loss_rnnt 5.733361 hw_loss 0.409893 lr 0.00039341 rank 0
2023-02-23 13:28:48,018 DEBUG TRAIN Batch 19/3100 loss 10.020570 loss_att 12.219713 loss_ctc 13.844212 loss_rnnt 8.723532 hw_loss 0.651356 lr 0.00039336 rank 6
2023-02-23 13:28:48,017 DEBUG TRAIN Batch 19/3100 loss 16.202360 loss_att 16.438246 loss_ctc 21.681517 loss_rnnt 15.130506 hw_loss 0.551479 lr 0.00039332 rank 2
2023-02-23 13:28:48,022 DEBUG TRAIN Batch 19/3100 loss 3.864567 loss_att 8.800819 loss_ctc 5.416953 loss_rnnt 2.403937 hw_loss 0.499490 lr 0.00039328 rank 3
2023-02-23 13:28:48,022 DEBUG TRAIN Batch 19/3100 loss 10.117047 loss_att 10.825300 loss_ctc 10.953459 loss_rnnt 9.609329 hw_loss 0.477273 lr 0.00039339 rank 4
2023-02-23 13:30:03,240 DEBUG TRAIN Batch 19/3200 loss 12.857630 loss_att 11.871428 loss_ctc 17.105251 loss_rnnt 12.073243 hw_loss 0.778645 lr 0.00039317 rank 7
2023-02-23 13:30:03,242 DEBUG TRAIN Batch 19/3200 loss 10.371934 loss_att 13.499876 loss_ctc 15.098990 loss_rnnt 8.918656 hw_loss 0.370152 lr 0.00039324 rank 6
2023-02-23 13:30:03,243 DEBUG TRAIN Batch 19/3200 loss 11.724783 loss_att 13.321104 loss_ctc 13.995032 loss_rnnt 10.885847 hw_loss 0.406821 lr 0.00039329 rank 1
2023-02-23 13:30:03,244 DEBUG TRAIN Batch 19/3200 loss 7.368221 loss_att 8.520597 loss_ctc 11.425612 loss_rnnt 6.332068 hw_loss 0.496298 lr 0.00039327 rank 4
2023-02-23 13:30:03,247 DEBUG TRAIN Batch 19/3200 loss 10.964461 loss_att 11.591114 loss_ctc 14.633286 loss_rnnt 10.134409 hw_loss 0.404145 lr 0.00039329 rank 0
2023-02-23 13:30:03,249 DEBUG TRAIN Batch 19/3200 loss 8.144615 loss_att 8.424448 loss_ctc 8.785864 loss_rnnt 7.679864 hw_loss 0.606159 lr 0.00039320 rank 2
2023-02-23 13:30:03,251 DEBUG TRAIN Batch 19/3200 loss 7.953268 loss_att 10.080015 loss_ctc 14.336449 loss_rnnt 6.457301 hw_loss 0.411612 lr 0.00039325 rank 5
2023-02-23 13:30:03,251 DEBUG TRAIN Batch 19/3200 loss 7.453955 loss_att 10.319405 loss_ctc 13.268825 loss_rnnt 5.891529 hw_loss 0.401287 lr 0.00039316 rank 3
2023-02-23 13:31:15,652 DEBUG TRAIN Batch 19/3300 loss 12.555630 loss_att 18.494226 loss_ctc 15.908393 loss_rnnt 10.679069 hw_loss 0.453386 lr 0.00039305 rank 7
2023-02-23 13:31:15,652 DEBUG TRAIN Batch 19/3300 loss 7.987941 loss_att 10.883512 loss_ctc 13.337713 loss_rnnt 6.467361 hw_loss 0.427804 lr 0.00039314 rank 4
2023-02-23 13:31:15,653 DEBUG TRAIN Batch 19/3300 loss 9.223483 loss_att 12.407192 loss_ctc 11.704234 loss_rnnt 8.057415 hw_loss 0.372298 lr 0.00039317 rank 1
2023-02-23 13:31:15,655 DEBUG TRAIN Batch 19/3300 loss 19.850704 loss_att 22.185293 loss_ctc 26.100874 loss_rnnt 18.366678 hw_loss 0.344533 lr 0.00039308 rank 2
2023-02-23 13:31:15,657 DEBUG TRAIN Batch 19/3300 loss 8.253679 loss_att 10.725496 loss_ctc 8.599432 loss_rnnt 7.520067 hw_loss 0.362153 lr 0.00039317 rank 0
2023-02-23 13:31:15,657 DEBUG TRAIN Batch 19/3300 loss 10.858655 loss_att 13.832147 loss_ctc 15.279539 loss_rnnt 9.490036 hw_loss 0.345879 lr 0.00039312 rank 6
2023-02-23 13:31:15,658 DEBUG TRAIN Batch 19/3300 loss 8.823871 loss_att 15.045040 loss_ctc 11.532011 loss_rnnt 6.997818 hw_loss 0.413876 lr 0.00039304 rank 3
2023-02-23 13:31:15,707 DEBUG TRAIN Batch 19/3300 loss 4.647252 loss_att 6.608968 loss_ctc 7.358131 loss_rnnt 3.685069 hw_loss 0.390729 lr 0.00039313 rank 5
2023-02-23 13:32:27,981 DEBUG TRAIN Batch 19/3400 loss 2.733402 loss_att 4.838265 loss_ctc 4.866998 loss_rnnt 1.798070 hw_loss 0.431024 lr 0.00039302 rank 4
2023-02-23 13:32:27,981 DEBUG TRAIN Batch 19/3400 loss 9.007432 loss_att 12.084167 loss_ctc 14.350407 loss_rnnt 7.489707 hw_loss 0.356215 lr 0.00039296 rank 2
2023-02-23 13:32:27,981 DEBUG TRAIN Batch 19/3400 loss 7.482821 loss_att 8.545298 loss_ctc 10.858681 loss_rnnt 6.628362 hw_loss 0.359719 lr 0.00039305 rank 1
2023-02-23 13:32:27,984 DEBUG TRAIN Batch 19/3400 loss 11.946093 loss_att 16.438047 loss_ctc 19.170006 loss_rnnt 9.894771 hw_loss 0.355769 lr 0.00039292 rank 7
2023-02-23 13:32:27,984 DEBUG TRAIN Batch 19/3400 loss 6.537483 loss_att 9.988035 loss_ctc 11.112553 loss_rnnt 4.993927 hw_loss 0.456445 lr 0.00039300 rank 5
2023-02-23 13:32:27,987 DEBUG TRAIN Batch 19/3400 loss 9.679764 loss_att 9.588317 loss_ctc 12.786024 loss_rnnt 9.089190 hw_loss 0.365055 lr 0.00039292 rank 3
2023-02-23 13:32:27,989 DEBUG TRAIN Batch 19/3400 loss 5.446696 loss_att 8.834875 loss_ctc 6.636055 loss_rnnt 4.427117 hw_loss 0.343803 lr 0.00039305 rank 0
2023-02-23 13:32:27,990 DEBUG TRAIN Batch 19/3400 loss 11.989794 loss_att 14.429977 loss_ctc 23.939629 loss_rnnt 9.644293 hw_loss 0.495288 lr 0.00039300 rank 6
2023-02-23 13:33:41,329 DEBUG TRAIN Batch 19/3500 loss 3.038617 loss_att 6.665441 loss_ctc 3.869350 loss_rnnt 1.954284 hw_loss 0.465381 lr 0.00039292 rank 0
2023-02-23 13:33:41,338 DEBUG TRAIN Batch 19/3500 loss 26.171736 loss_att 26.404137 loss_ctc 41.529259 loss_rnnt 23.882622 hw_loss 0.365558 lr 0.00039284 rank 2
2023-02-23 13:33:41,338 DEBUG TRAIN Batch 19/3500 loss 14.530970 loss_att 17.822399 loss_ctc 18.867212 loss_rnnt 13.070251 hw_loss 0.420499 lr 0.00039280 rank 3
2023-02-23 13:33:41,338 DEBUG TRAIN Batch 19/3500 loss 14.004257 loss_att 16.375206 loss_ctc 20.406731 loss_rnnt 12.469614 hw_loss 0.387731 lr 0.00039280 rank 7
2023-02-23 13:33:41,338 DEBUG TRAIN Batch 19/3500 loss 15.661236 loss_att 17.504450 loss_ctc 21.068161 loss_rnnt 14.348885 hw_loss 0.417720 lr 0.00039290 rank 4
2023-02-23 13:33:41,341 DEBUG TRAIN Batch 19/3500 loss 8.039986 loss_att 9.837903 loss_ctc 8.837727 loss_rnnt 7.382507 hw_loss 0.359117 lr 0.00039293 rank 1
2023-02-23 13:33:41,346 DEBUG TRAIN Batch 19/3500 loss 6.206553 loss_att 10.087578 loss_ctc 13.938913 loss_rnnt 4.134552 hw_loss 0.496526 lr 0.00039288 rank 6
2023-02-23 13:33:41,377 DEBUG TRAIN Batch 19/3500 loss 14.035185 loss_att 20.568769 loss_ctc 22.893810 loss_rnnt 11.331996 hw_loss 0.403729 lr 0.00039288 rank 5
2023-02-23 13:34:55,349 DEBUG TRAIN Batch 19/3600 loss 7.571972 loss_att 10.213627 loss_ctc 9.315778 loss_rnnt 6.624170 hw_loss 0.350557 lr 0.00039268 rank 7
2023-02-23 13:34:55,354 DEBUG TRAIN Batch 19/3600 loss 9.221050 loss_att 11.898525 loss_ctc 14.440358 loss_rnnt 7.746209 hw_loss 0.456445 lr 0.00039272 rank 2
2023-02-23 13:34:55,358 DEBUG TRAIN Batch 19/3600 loss 11.072500 loss_att 14.300821 loss_ctc 14.351410 loss_rnnt 9.762508 hw_loss 0.425888 lr 0.00039281 rank 1
2023-02-23 13:34:55,358 DEBUG TRAIN Batch 19/3600 loss 22.953268 loss_att 23.399029 loss_ctc 31.425068 loss_rnnt 21.489082 hw_loss 0.460232 lr 0.00039267 rank 3
2023-02-23 13:34:55,359 DEBUG TRAIN Batch 19/3600 loss 6.468498 loss_att 7.519372 loss_ctc 7.066967 loss_rnnt 5.963949 hw_loss 0.402334 lr 0.00039280 rank 0
2023-02-23 13:34:55,360 DEBUG TRAIN Batch 19/3600 loss 7.238023 loss_att 10.765651 loss_ctc 10.296606 loss_rnnt 5.889860 hw_loss 0.440298 lr 0.00039278 rank 4
2023-02-23 13:34:55,391 DEBUG TRAIN Batch 19/3600 loss 11.109366 loss_att 10.606411 loss_ctc 9.968491 loss_rnnt 11.184751 hw_loss 0.332482 lr 0.00039276 rank 5
2023-02-23 13:34:55,424 DEBUG TRAIN Batch 19/3600 loss 15.470044 loss_att 18.358782 loss_ctc 22.187794 loss_rnnt 13.764507 hw_loss 0.435166 lr 0.00039276 rank 6
2023-02-23 13:36:08,491 DEBUG TRAIN Batch 19/3700 loss 10.585903 loss_att 14.336468 loss_ctc 15.933136 loss_rnnt 8.890720 hw_loss 0.435199 lr 0.00039256 rank 7
2023-02-23 13:36:08,493 DEBUG TRAIN Batch 19/3700 loss 11.866899 loss_att 11.606111 loss_ctc 15.658076 loss_rnnt 11.128394 hw_loss 0.534698 lr 0.00039255 rank 3
2023-02-23 13:36:08,498 DEBUG TRAIN Batch 19/3700 loss 11.484340 loss_att 12.622637 loss_ctc 17.306707 loss_rnnt 10.241691 hw_loss 0.447515 lr 0.00039268 rank 0
2023-02-23 13:36:08,499 DEBUG TRAIN Batch 19/3700 loss 9.887000 loss_att 14.002094 loss_ctc 13.655891 loss_rnnt 8.304741 hw_loss 0.481353 lr 0.00039266 rank 4
2023-02-23 13:36:08,499 DEBUG TRAIN Batch 19/3700 loss 13.839102 loss_att 16.274229 loss_ctc 17.177971 loss_rnnt 12.638368 hw_loss 0.503485 lr 0.00039260 rank 2
2023-02-23 13:36:08,500 DEBUG TRAIN Batch 19/3700 loss 6.967093 loss_att 9.930661 loss_ctc 11.738962 loss_rnnt 5.528127 hw_loss 0.393756 lr 0.00039269 rank 1
2023-02-23 13:36:08,531 DEBUG TRAIN Batch 19/3700 loss 10.363946 loss_att 10.680503 loss_ctc 14.430274 loss_rnnt 9.478348 hw_loss 0.525206 lr 0.00039264 rank 5
2023-02-23 13:36:08,535 DEBUG TRAIN Batch 19/3700 loss 10.729464 loss_att 11.743594 loss_ctc 15.777781 loss_rnnt 9.598894 hw_loss 0.477438 lr 0.00039264 rank 6
2023-02-23 13:37:21,587 DEBUG TRAIN Batch 19/3800 loss 11.105210 loss_att 12.736748 loss_ctc 18.501400 loss_rnnt 9.552670 hw_loss 0.450140 lr 0.00039254 rank 4
2023-02-23 13:37:21,588 DEBUG TRAIN Batch 19/3800 loss 9.077950 loss_att 12.570822 loss_ctc 14.160525 loss_rnnt 7.483521 hw_loss 0.409082 lr 0.00039243 rank 3
2023-02-23 13:37:21,589 DEBUG TRAIN Batch 19/3800 loss 7.051748 loss_att 8.924339 loss_ctc 10.768370 loss_rnnt 5.886620 hw_loss 0.553238 lr 0.00039244 rank 7
2023-02-23 13:37:21,590 DEBUG TRAIN Batch 19/3800 loss 6.598209 loss_att 10.765738 loss_ctc 11.410069 loss_rnnt 4.939675 hw_loss 0.343963 lr 0.00039252 rank 5
2023-02-23 13:37:21,592 DEBUG TRAIN Batch 19/3800 loss 18.074091 loss_att 18.999405 loss_ctc 22.545948 loss_rnnt 17.042850 hw_loss 0.468621 lr 0.00039248 rank 2
2023-02-23 13:37:21,593 DEBUG TRAIN Batch 19/3800 loss 8.975446 loss_att 10.626189 loss_ctc 11.910758 loss_rnnt 7.978901 hw_loss 0.515663 lr 0.00039257 rank 1
2023-02-23 13:37:21,595 DEBUG TRAIN Batch 19/3800 loss 3.049644 loss_att 6.026611 loss_ctc 4.120753 loss_rnnt 2.131269 hw_loss 0.337813 lr 0.00039251 rank 6
2023-02-23 13:37:21,595 DEBUG TRAIN Batch 19/3800 loss 14.740622 loss_att 16.386221 loss_ctc 21.754286 loss_rnnt 13.266098 hw_loss 0.394217 lr 0.00039256 rank 0
2023-02-23 13:38:36,555 DEBUG TRAIN Batch 19/3900 loss 8.337114 loss_att 11.523376 loss_ctc 12.798793 loss_rnnt 6.845956 hw_loss 0.485654 lr 0.00039232 rank 7
2023-02-23 13:38:36,556 DEBUG TRAIN Batch 19/3900 loss 13.475994 loss_att 17.973682 loss_ctc 16.708813 loss_rnnt 11.898312 hw_loss 0.463316 lr 0.00039231 rank 3
2023-02-23 13:38:36,561 DEBUG TRAIN Batch 19/3900 loss 6.379839 loss_att 10.281206 loss_ctc 10.004446 loss_rnnt 4.889348 hw_loss 0.425507 lr 0.00039242 rank 4
2023-02-23 13:38:36,562 DEBUG TRAIN Batch 19/3900 loss 8.903224 loss_att 9.841009 loss_ctc 12.278880 loss_rnnt 8.081345 hw_loss 0.345439 lr 0.00039235 rank 2
2023-02-23 13:38:36,563 DEBUG TRAIN Batch 19/3900 loss 7.517711 loss_att 7.420259 loss_ctc 9.167581 loss_rnnt 6.958145 hw_loss 0.673263 lr 0.00039244 rank 1
2023-02-23 13:38:36,565 DEBUG TRAIN Batch 19/3900 loss 10.334439 loss_att 13.456652 loss_ctc 15.064448 loss_rnnt 8.851915 hw_loss 0.426401 lr 0.00039240 rank 5
2023-02-23 13:38:36,564 DEBUG TRAIN Batch 19/3900 loss 11.393678 loss_att 10.011614 loss_ctc 11.438470 loss_rnnt 11.322774 hw_loss 0.640020 lr 0.00039244 rank 0
2023-02-23 13:38:36,585 DEBUG TRAIN Batch 19/3900 loss 6.879184 loss_att 10.667686 loss_ctc 7.358324 loss_rnnt 5.876647 hw_loss 0.339283 lr 0.00039239 rank 6
2023-02-23 13:39:49,475 DEBUG TRAIN Batch 19/4000 loss 7.968321 loss_att 9.991627 loss_ctc 10.322150 loss_rnnt 7.059993 hw_loss 0.355920 lr 0.00039227 rank 6
2023-02-23 13:39:49,492 DEBUG TRAIN Batch 19/4000 loss 9.292667 loss_att 11.695161 loss_ctc 11.581688 loss_rnnt 8.287310 hw_loss 0.411855 lr 0.00039232 rank 1
2023-02-23 13:39:49,492 DEBUG TRAIN Batch 19/4000 loss 9.385677 loss_att 12.934958 loss_ctc 10.695173 loss_rnnt 8.252755 hw_loss 0.465875 lr 0.00039220 rank 7
2023-02-23 13:39:49,496 DEBUG TRAIN Batch 19/4000 loss 9.736763 loss_att 10.622105 loss_ctc 11.414835 loss_rnnt 9.143675 hw_loss 0.360516 lr 0.00039219 rank 3
2023-02-23 13:39:49,497 DEBUG TRAIN Batch 19/4000 loss 9.908875 loss_att 12.755150 loss_ctc 21.018993 loss_rnnt 7.637794 hw_loss 0.413391 lr 0.00039223 rank 2
2023-02-23 13:39:49,499 DEBUG TRAIN Batch 19/4000 loss 8.941125 loss_att 10.609913 loss_ctc 10.424428 loss_rnnt 8.187151 hw_loss 0.417080 lr 0.00039230 rank 4
2023-02-23 13:39:49,500 DEBUG TRAIN Batch 19/4000 loss 3.450835 loss_att 8.659496 loss_ctc 4.587030 loss_rnnt 2.086310 hw_loss 0.321187 lr 0.00039232 rank 0
2023-02-23 13:39:49,540 DEBUG TRAIN Batch 19/4000 loss 7.412872 loss_att 11.289591 loss_ctc 11.216814 loss_rnnt 5.900930 hw_loss 0.430134 lr 0.00039228 rank 5
2023-02-23 13:41:01,610 DEBUG TRAIN Batch 19/4100 loss 6.810873 loss_att 8.543339 loss_ctc 8.503456 loss_rnnt 6.028772 hw_loss 0.393619 lr 0.00039208 rank 7
2023-02-23 13:41:01,616 DEBUG TRAIN Batch 19/4100 loss 5.408392 loss_att 7.480117 loss_ctc 9.359139 loss_rnnt 4.185842 hw_loss 0.527698 lr 0.00039215 rank 6
2023-02-23 13:41:01,617 DEBUG TRAIN Batch 19/4100 loss 4.308413 loss_att 8.009502 loss_ctc 6.053499 loss_rnnt 3.122999 hw_loss 0.398472 lr 0.00039218 rank 4
2023-02-23 13:41:01,617 DEBUG TRAIN Batch 19/4100 loss 9.030460 loss_att 13.197767 loss_ctc 13.943645 loss_rnnt 7.331367 hw_loss 0.394764 lr 0.00039207 rank 3
2023-02-23 13:41:01,621 DEBUG TRAIN Batch 19/4100 loss 4.516961 loss_att 6.814947 loss_ctc 7.110360 loss_rnnt 3.505968 hw_loss 0.385518 lr 0.00039211 rank 2
2023-02-23 13:41:01,621 DEBUG TRAIN Batch 19/4100 loss 7.704461 loss_att 9.755572 loss_ctc 11.048138 loss_rnnt 6.625863 hw_loss 0.417285 lr 0.00039220 rank 1
2023-02-23 13:41:01,623 DEBUG TRAIN Batch 19/4100 loss 6.880113 loss_att 10.251114 loss_ctc 10.126589 loss_rnnt 5.532094 hw_loss 0.451790 lr 0.00039220 rank 0
2023-02-23 13:41:01,625 DEBUG TRAIN Batch 19/4100 loss 10.419055 loss_att 11.565492 loss_ctc 14.410361 loss_rnnt 9.473307 hw_loss 0.345538 lr 0.00039216 rank 5
2023-02-23 13:42:14,077 DEBUG TRAIN Batch 19/4200 loss 8.022964 loss_att 12.072280 loss_ctc 10.620716 loss_rnnt 6.678003 hw_loss 0.353871 lr 0.00039204 rank 5
2023-02-23 13:42:14,085 DEBUG TRAIN Batch 19/4200 loss 9.762239 loss_att 12.165937 loss_ctc 15.690040 loss_rnnt 8.284890 hw_loss 0.386689 lr 0.00039196 rank 7
2023-02-23 13:42:14,089 DEBUG TRAIN Batch 19/4200 loss 12.051889 loss_att 15.524343 loss_ctc 18.767130 loss_rnnt 10.277764 hw_loss 0.345504 lr 0.00039208 rank 0
2023-02-23 13:42:14,091 DEBUG TRAIN Batch 19/4200 loss 5.303797 loss_att 6.709633 loss_ctc 6.585457 loss_rnnt 4.636842 hw_loss 0.402937 lr 0.00039205 rank 4
2023-02-23 13:42:14,093 DEBUG TRAIN Batch 19/4200 loss 13.544463 loss_att 18.290941 loss_ctc 17.523693 loss_rnnt 11.868459 hw_loss 0.367773 lr 0.00039208 rank 1
2023-02-23 13:42:14,095 DEBUG TRAIN Batch 19/4200 loss 11.750827 loss_att 15.790878 loss_ctc 14.221475 loss_rnnt 10.371620 hw_loss 0.453332 lr 0.00039203 rank 6
2023-02-23 13:42:14,098 DEBUG TRAIN Batch 19/4200 loss 6.750628 loss_att 8.388004 loss_ctc 10.532582 loss_rnnt 5.703734 hw_loss 0.403419 lr 0.00039199 rank 2
2023-02-23 13:42:14,099 DEBUG TRAIN Batch 19/4200 loss 13.319377 loss_att 15.563912 loss_ctc 18.816498 loss_rnnt 11.941303 hw_loss 0.367907 lr 0.00039195 rank 3
2023-02-23 13:43:28,948 DEBUG TRAIN Batch 19/4300 loss 8.762445 loss_att 12.980293 loss_ctc 15.148267 loss_rnnt 6.790031 hw_loss 0.520129 lr 0.00039183 rank 3
2023-02-23 13:43:28,949 DEBUG TRAIN Batch 19/4300 loss 20.186621 loss_att 22.920860 loss_ctc 27.093988 loss_rnnt 18.499989 hw_loss 0.410251 lr 0.00039184 rank 7
2023-02-23 13:43:28,949 DEBUG TRAIN Batch 19/4300 loss 12.207015 loss_att 12.268560 loss_ctc 16.959703 loss_rnnt 11.347061 hw_loss 0.401161 lr 0.00039187 rank 2
2023-02-23 13:43:28,951 DEBUG TRAIN Batch 19/4300 loss 8.350857 loss_att 12.327078 loss_ctc 12.537901 loss_rnnt 6.788878 hw_loss 0.390865 lr 0.00039196 rank 0
2023-02-23 13:43:28,951 DEBUG TRAIN Batch 19/4300 loss 5.740461 loss_att 7.897448 loss_ctc 8.523929 loss_rnnt 4.700757 hw_loss 0.444708 lr 0.00039193 rank 4
2023-02-23 13:43:28,951 DEBUG TRAIN Batch 19/4300 loss 11.125337 loss_att 11.220650 loss_ctc 15.351043 loss_rnnt 10.324438 hw_loss 0.409514 lr 0.00039192 rank 5
2023-02-23 13:43:28,954 DEBUG TRAIN Batch 19/4300 loss 12.233829 loss_att 12.206366 loss_ctc 16.594868 loss_rnnt 11.350526 hw_loss 0.576235 lr 0.00039191 rank 6
2023-02-23 13:43:28,956 DEBUG TRAIN Batch 19/4300 loss 10.697508 loss_att 12.966027 loss_ctc 13.407378 loss_rnnt 9.684264 hw_loss 0.371672 lr 0.00039196 rank 1
2023-02-23 13:44:41,965 DEBUG TRAIN Batch 19/4400 loss 3.702002 loss_att 6.421095 loss_ctc 5.394369 loss_rnnt 2.739850 hw_loss 0.361285 lr 0.00039171 rank 3
2023-02-23 13:44:41,968 DEBUG TRAIN Batch 19/4400 loss 7.596313 loss_att 11.368890 loss_ctc 10.613997 loss_rnnt 6.246834 hw_loss 0.361134 lr 0.00039172 rank 7
2023-02-23 13:44:41,969 DEBUG TRAIN Batch 19/4400 loss 7.184753 loss_att 9.664703 loss_ctc 7.937462 loss_rnnt 6.287750 hw_loss 0.563723 lr 0.00039175 rank 2
2023-02-23 13:44:41,969 DEBUG TRAIN Batch 19/4400 loss 12.971235 loss_att 17.125809 loss_ctc 21.041388 loss_rnnt 10.890726 hw_loss 0.325449 lr 0.00039181 rank 4
2023-02-23 13:44:41,971 DEBUG TRAIN Batch 19/4400 loss 8.880239 loss_att 12.367479 loss_ctc 14.642099 loss_rnnt 7.217809 hw_loss 0.368876 lr 0.00039180 rank 5
2023-02-23 13:44:41,973 DEBUG TRAIN Batch 19/4400 loss 8.122854 loss_att 10.104256 loss_ctc 12.788336 loss_rnnt 6.876228 hw_loss 0.428027 lr 0.00039184 rank 1
2023-02-23 13:44:41,974 DEBUG TRAIN Batch 19/4400 loss 9.278644 loss_att 14.268978 loss_ctc 14.551252 loss_rnnt 7.372343 hw_loss 0.384786 lr 0.00039184 rank 0
2023-02-23 13:44:41,976 DEBUG TRAIN Batch 19/4400 loss 17.171652 loss_att 20.046276 loss_ctc 24.856625 loss_rnnt 15.368406 hw_loss 0.381863 lr 0.00039179 rank 6
2023-02-23 13:45:55,079 DEBUG TRAIN Batch 19/4500 loss 7.213796 loss_att 10.470514 loss_ctc 8.395271 loss_rnnt 6.144890 hw_loss 0.487560 lr 0.00039159 rank 3
2023-02-23 13:45:55,098 DEBUG TRAIN Batch 19/4500 loss 8.021826 loss_att 7.453619 loss_ctc 10.529573 loss_rnnt 7.458529 hw_loss 0.642322 lr 0.00039172 rank 1
2023-02-23 13:45:55,100 DEBUG TRAIN Batch 19/4500 loss 11.097219 loss_att 11.938194 loss_ctc 16.081631 loss_rnnt 10.023510 hw_loss 0.451734 lr 0.00039169 rank 4
2023-02-23 13:45:55,101 DEBUG TRAIN Batch 19/4500 loss 6.242939 loss_att 6.756573 loss_ctc 7.426832 loss_rnnt 5.613076 hw_loss 0.692405 lr 0.00039160 rank 7
2023-02-23 13:45:55,103 DEBUG TRAIN Batch 19/4500 loss 6.668667 loss_att 12.263322 loss_ctc 11.483059 loss_rnnt 4.707014 hw_loss 0.376506 lr 0.00039167 rank 6
2023-02-23 13:45:55,106 DEBUG TRAIN Batch 19/4500 loss 12.942835 loss_att 11.873545 loss_ctc 16.991198 loss_rnnt 12.240707 hw_loss 0.705383 lr 0.00039163 rank 2
2023-02-23 13:45:55,107 DEBUG TRAIN Batch 19/4500 loss 6.633111 loss_att 10.244627 loss_ctc 7.221176 loss_rnnt 5.616610 hw_loss 0.404606 lr 0.00039168 rank 5
2023-02-23 13:45:55,152 DEBUG TRAIN Batch 19/4500 loss 8.197565 loss_att 10.932293 loss_ctc 15.207239 loss_rnnt 6.397861 hw_loss 0.596504 lr 0.00039172 rank 0
2023-02-23 13:47:09,355 DEBUG TRAIN Batch 19/4600 loss 4.572896 loss_att 7.507251 loss_ctc 7.551507 loss_rnnt 3.321452 hw_loss 0.501421 lr 0.00039156 rank 5
2023-02-23 13:47:09,358 DEBUG TRAIN Batch 19/4600 loss 9.376865 loss_att 10.275905 loss_ctc 12.100940 loss_rnnt 8.591751 hw_loss 0.453930 lr 0.00039147 rank 3
2023-02-23 13:47:09,358 DEBUG TRAIN Batch 19/4600 loss 6.436740 loss_att 8.661615 loss_ctc 9.499949 loss_rnnt 5.393745 hw_loss 0.355485 lr 0.00039151 rank 2
2023-02-23 13:47:09,358 DEBUG TRAIN Batch 19/4600 loss 18.752224 loss_att 19.871161 loss_ctc 21.532499 loss_rnnt 17.945608 hw_loss 0.397736 lr 0.00039155 rank 6
2023-02-23 13:47:09,360 DEBUG TRAIN Batch 19/4600 loss 11.573984 loss_att 13.390083 loss_ctc 16.699059 loss_rnnt 10.210373 hw_loss 0.594465 lr 0.00039157 rank 4
2023-02-23 13:47:09,363 DEBUG TRAIN Batch 19/4600 loss 6.941642 loss_att 10.037659 loss_ctc 11.669998 loss_rnnt 5.467331 hw_loss 0.421240 lr 0.00039160 rank 0
2023-02-23 13:47:09,365 DEBUG TRAIN Batch 19/4600 loss 6.466173 loss_att 9.877264 loss_ctc 10.006394 loss_rnnt 5.056938 hw_loss 0.478101 lr 0.00039160 rank 1
2023-02-23 13:47:09,383 DEBUG TRAIN Batch 19/4600 loss 7.820369 loss_att 8.790220 loss_ctc 8.930891 loss_rnnt 7.281311 hw_loss 0.369409 lr 0.00039148 rank 7
2023-02-23 13:48:22,074 DEBUG TRAIN Batch 19/4700 loss 10.244858 loss_att 10.760676 loss_ctc 12.576893 loss_rnnt 9.627014 hw_loss 0.382015 lr 0.00039135 rank 3
2023-02-23 13:48:22,076 DEBUG TRAIN Batch 19/4700 loss 14.091532 loss_att 15.884996 loss_ctc 16.633764 loss_rnnt 13.100996 hw_loss 0.549148 lr 0.00039136 rank 7
2023-02-23 13:48:22,077 DEBUG TRAIN Batch 19/4700 loss 14.206016 loss_att 19.416649 loss_ctc 16.029175 loss_rnnt 12.679248 hw_loss 0.452912 lr 0.00039139 rank 2
2023-02-23 13:48:22,076 DEBUG TRAIN Batch 19/4700 loss 11.843155 loss_att 13.829580 loss_ctc 13.637156 loss_rnnt 10.965021 hw_loss 0.453087 lr 0.00039145 rank 4
2023-02-23 13:48:22,077 DEBUG TRAIN Batch 19/4700 loss 8.112765 loss_att 12.741435 loss_ctc 11.865062 loss_rnnt 6.482993 hw_loss 0.381997 lr 0.00039148 rank 1
2023-02-23 13:48:22,078 DEBUG TRAIN Batch 19/4700 loss 21.825188 loss_att 23.491629 loss_ctc 30.890469 loss_rnnt 20.047289 hw_loss 0.442324 lr 0.00039143 rank 6
2023-02-23 13:48:22,080 DEBUG TRAIN Batch 19/4700 loss 12.816550 loss_att 13.333680 loss_ctc 15.367340 loss_rnnt 12.187519 hw_loss 0.347812 lr 0.00039144 rank 5
2023-02-23 13:48:22,128 DEBUG TRAIN Batch 19/4700 loss 5.292361 loss_att 9.933355 loss_ctc 7.008755 loss_rnnt 3.850140 hw_loss 0.534692 lr 0.00039148 rank 0
2023-02-23 13:49:35,076 DEBUG TRAIN Batch 19/4800 loss 3.835521 loss_att 5.325073 loss_ctc 5.295978 loss_rnnt 3.124496 hw_loss 0.409475 lr 0.00039124 rank 7
2023-02-23 13:49:35,076 DEBUG TRAIN Batch 19/4800 loss 12.537933 loss_att 13.841335 loss_ctc 15.698450 loss_rnnt 11.655357 hw_loss 0.375924 lr 0.00039133 rank 4
2023-02-23 13:49:35,080 DEBUG TRAIN Batch 19/4800 loss 5.505736 loss_att 9.237963 loss_ctc 7.260167 loss_rnnt 4.337054 hw_loss 0.353084 lr 0.00039123 rank 3
2023-02-23 13:49:35,082 DEBUG TRAIN Batch 19/4800 loss 3.920979 loss_att 8.300473 loss_ctc 3.937526 loss_rnnt 2.808819 hw_loss 0.438853 lr 0.00039136 rank 0
2023-02-23 13:49:35,085 DEBUG TRAIN Batch 19/4800 loss 13.496752 loss_att 14.634953 loss_ctc 17.784632 loss_rnnt 12.471939 hw_loss 0.422728 lr 0.00039132 rank 5
2023-02-23 13:49:35,087 DEBUG TRAIN Batch 19/4800 loss 14.188204 loss_att 19.750134 loss_ctc 17.059280 loss_rnnt 12.454728 hw_loss 0.446773 lr 0.00039127 rank 2
2023-02-23 13:49:35,087 DEBUG TRAIN Batch 19/4800 loss 7.933649 loss_att 8.886570 loss_ctc 10.215715 loss_rnnt 7.230402 hw_loss 0.390726 lr 0.00039136 rank 1
2023-02-23 13:49:35,089 DEBUG TRAIN Batch 19/4800 loss 3.925115 loss_att 7.419651 loss_ctc 4.348800 loss_rnnt 2.949533 hw_loss 0.412842 lr 0.00039131 rank 6
2023-02-23 13:50:47,779 DEBUG TRAIN Batch 19/4900 loss 11.018595 loss_att 14.182549 loss_ctc 16.913925 loss_rnnt 9.393841 hw_loss 0.386098 lr 0.00039121 rank 4
2023-02-23 13:50:47,779 DEBUG TRAIN Batch 19/4900 loss 5.819804 loss_att 9.628166 loss_ctc 8.935963 loss_rnnt 4.418972 hw_loss 0.419384 lr 0.00039124 rank 1
2023-02-23 13:50:47,780 DEBUG TRAIN Batch 19/4900 loss 9.740064 loss_att 13.192327 loss_ctc 13.447629 loss_rnnt 8.319206 hw_loss 0.442616 lr 0.00039115 rank 2
2023-02-23 13:50:47,784 DEBUG TRAIN Batch 19/4900 loss 8.330519 loss_att 11.419840 loss_ctc 12.117626 loss_rnnt 6.966213 hw_loss 0.452801 lr 0.00039112 rank 7
2023-02-23 13:50:47,786 DEBUG TRAIN Batch 19/4900 loss 11.732659 loss_att 15.148960 loss_ctc 16.150002 loss_rnnt 10.235259 hw_loss 0.422181 lr 0.00039124 rank 0
2023-02-23 13:50:47,785 DEBUG TRAIN Batch 19/4900 loss 10.770957 loss_att 14.427807 loss_ctc 15.098988 loss_rnnt 9.258189 hw_loss 0.383113 lr 0.00039111 rank 3
2023-02-23 13:50:47,814 DEBUG TRAIN Batch 19/4900 loss 6.696114 loss_att 7.851363 loss_ctc 8.221705 loss_rnnt 5.975445 hw_loss 0.536635 lr 0.00039120 rank 5
2023-02-23 13:50:47,824 DEBUG TRAIN Batch 19/4900 loss 12.592108 loss_att 14.822943 loss_ctc 17.897673 loss_rnnt 11.207034 hw_loss 0.434061 lr 0.00039119 rank 6
2023-02-23 13:52:02,831 DEBUG TRAIN Batch 19/5000 loss 8.315608 loss_att 9.663043 loss_ctc 14.729317 loss_rnnt 6.975013 hw_loss 0.404902 lr 0.00039099 rank 3
2023-02-23 13:52:02,833 DEBUG TRAIN Batch 19/5000 loss 4.497907 loss_att 6.055372 loss_ctc 4.849539 loss_rnnt 3.878738 hw_loss 0.488984 lr 0.00039100 rank 7
2023-02-23 13:52:02,834 DEBUG TRAIN Batch 19/5000 loss 6.860706 loss_att 10.435022 loss_ctc 9.166709 loss_rnnt 5.627364 hw_loss 0.395649 lr 0.00039103 rank 2
2023-02-23 13:52:02,837 DEBUG TRAIN Batch 19/5000 loss 12.180317 loss_att 12.883468 loss_ctc 16.276953 loss_rnnt 11.234153 hw_loss 0.486219 lr 0.00039108 rank 5
2023-02-23 13:52:02,838 DEBUG TRAIN Batch 19/5000 loss 14.428657 loss_att 16.021156 loss_ctc 21.584360 loss_rnnt 12.935536 hw_loss 0.413487 lr 0.00039112 rank 1
2023-02-23 13:52:02,840 DEBUG TRAIN Batch 19/5000 loss 8.654299 loss_att 14.375256 loss_ctc 11.303215 loss_rnnt 6.959601 hw_loss 0.369970 lr 0.00039107 rank 6
2023-02-23 13:52:02,845 DEBUG TRAIN Batch 19/5000 loss 5.441099 loss_att 7.118983 loss_ctc 8.360792 loss_rnnt 4.450383 hw_loss 0.498462 lr 0.00039109 rank 4
2023-02-23 13:52:02,845 DEBUG TRAIN Batch 19/5000 loss 16.016445 loss_att 18.465273 loss_ctc 25.321201 loss_rnnt 14.055380 hw_loss 0.432499 lr 0.00039112 rank 0
2023-02-23 13:53:15,662 DEBUG TRAIN Batch 19/5100 loss 10.925570 loss_att 11.840725 loss_ctc 12.786601 loss_rnnt 10.303657 hw_loss 0.357646 lr 0.00039097 rank 4
2023-02-23 13:53:15,661 DEBUG TRAIN Batch 19/5100 loss 26.735367 loss_att 37.105865 loss_ctc 34.351913 loss_rnnt 23.455151 hw_loss 0.357333 lr 0.00039087 rank 3
2023-02-23 13:53:15,662 DEBUG TRAIN Batch 19/5100 loss 12.849280 loss_att 13.979393 loss_ctc 17.237595 loss_rnnt 11.702472 hw_loss 0.629395 lr 0.00039100 rank 1
2023-02-23 13:53:15,662 DEBUG TRAIN Batch 19/5100 loss 6.565307 loss_att 6.032910 loss_ctc 7.052999 loss_rnnt 6.265920 hw_loss 0.639077 lr 0.00039088 rank 7
2023-02-23 13:53:15,664 DEBUG TRAIN Batch 19/5100 loss 6.251133 loss_att 7.133445 loss_ctc 7.810892 loss_rnnt 5.601671 hw_loss 0.496935 lr 0.00039091 rank 2
2023-02-23 13:53:15,674 DEBUG TRAIN Batch 19/5100 loss 11.153529 loss_att 14.117077 loss_ctc 11.185985 loss_rnnt 10.352656 hw_loss 0.382192 lr 0.00039096 rank 5
2023-02-23 13:53:15,674 DEBUG TRAIN Batch 19/5100 loss 6.941808 loss_att 6.816173 loss_ctc 9.145480 loss_rnnt 6.424299 hw_loss 0.466526 lr 0.00039095 rank 6
2023-02-23 13:53:15,722 DEBUG TRAIN Batch 19/5100 loss 16.028507 loss_att 21.775881 loss_ctc 22.540762 loss_rnnt 13.766724 hw_loss 0.457515 lr 0.00039100 rank 0
2023-02-23 13:54:28,344 DEBUG TRAIN Batch 19/5200 loss 7.750636 loss_att 10.561563 loss_ctc 11.672438 loss_rnnt 6.461163 hw_loss 0.383213 lr 0.00039084 rank 5
2023-02-23 13:54:28,351 DEBUG TRAIN Batch 19/5200 loss 18.407953 loss_att 19.929138 loss_ctc 24.764576 loss_rnnt 17.042858 hw_loss 0.399951 lr 0.00039076 rank 7
2023-02-23 13:54:28,354 DEBUG TRAIN Batch 19/5200 loss 7.996433 loss_att 9.047921 loss_ctc 11.801034 loss_rnnt 7.018780 hw_loss 0.487642 lr 0.00039088 rank 0
2023-02-23 13:54:28,354 DEBUG TRAIN Batch 19/5200 loss 11.897953 loss_att 13.573225 loss_ctc 18.112209 loss_rnnt 10.467909 hw_loss 0.499541 lr 0.00039079 rank 2
2023-02-23 13:54:28,356 DEBUG TRAIN Batch 19/5200 loss 9.905541 loss_att 11.395738 loss_ctc 12.412733 loss_rnnt 9.078603 hw_loss 0.364889 lr 0.00039086 rank 4
2023-02-23 13:54:28,357 DEBUG TRAIN Batch 19/5200 loss 12.097845 loss_att 15.231054 loss_ctc 15.473960 loss_rnnt 10.803024 hw_loss 0.408807 lr 0.00039075 rank 3
2023-02-23 13:54:28,360 DEBUG TRAIN Batch 19/5200 loss 5.290675 loss_att 9.198617 loss_ctc 7.693609 loss_rnnt 3.906273 hw_loss 0.529542 lr 0.00039083 rank 6
2023-02-23 13:54:28,408 DEBUG TRAIN Batch 19/5200 loss 4.134964 loss_att 7.996524 loss_ctc 5.763371 loss_rnnt 2.958019 hw_loss 0.351587 lr 0.00039088 rank 1
2023-02-23 13:55:42,747 DEBUG TRAIN Batch 19/5300 loss 5.825376 loss_att 8.782915 loss_ctc 5.128805 loss_rnnt 5.143950 hw_loss 0.342739 lr 0.00039074 rank 4
2023-02-23 13:55:42,757 DEBUG TRAIN Batch 19/5300 loss 12.045449 loss_att 14.433577 loss_ctc 17.263906 loss_rnnt 10.641862 hw_loss 0.431564 lr 0.00039064 rank 7
2023-02-23 13:55:42,761 DEBUG TRAIN Batch 19/5300 loss 6.076370 loss_att 8.528032 loss_ctc 10.808689 loss_rnnt 4.724636 hw_loss 0.432048 lr 0.00039071 rank 6
2023-02-23 13:55:42,762 DEBUG TRAIN Batch 19/5300 loss 7.242136 loss_att 11.322346 loss_ctc 10.799352 loss_rnnt 5.765573 hw_loss 0.349175 lr 0.00039063 rank 3
2023-02-23 13:55:42,764 DEBUG TRAIN Batch 19/5300 loss 12.452847 loss_att 15.230993 loss_ctc 14.181223 loss_rnnt 11.458422 hw_loss 0.390647 lr 0.00039076 rank 1
2023-02-23 13:55:42,763 DEBUG TRAIN Batch 19/5300 loss 26.608385 loss_att 28.289257 loss_ctc 33.243896 loss_rnnt 25.187347 hw_loss 0.375236 lr 0.00039067 rank 2
2023-02-23 13:55:42,765 DEBUG TRAIN Batch 19/5300 loss 1.853886 loss_att 2.763291 loss_ctc 2.571928 loss_rnnt 1.322825 hw_loss 0.475201 lr 0.00039072 rank 5
2023-02-23 13:55:42,791 DEBUG TRAIN Batch 19/5300 loss 9.621494 loss_att 12.819832 loss_ctc 12.537475 loss_rnnt 8.424166 hw_loss 0.316619 lr 0.00039076 rank 0
2023-02-23 13:56:55,933 DEBUG TRAIN Batch 19/5400 loss 9.449916 loss_att 14.157589 loss_ctc 14.408245 loss_rnnt 7.680022 hw_loss 0.313591 lr 0.00039052 rank 7
2023-02-23 13:56:55,936 DEBUG TRAIN Batch 19/5400 loss 8.120519 loss_att 10.816778 loss_ctc 11.063740 loss_rnnt 7.013033 hw_loss 0.329634 lr 0.00039051 rank 3
2023-02-23 13:56:55,940 DEBUG TRAIN Batch 19/5400 loss 8.954710 loss_att 14.192409 loss_ctc 14.570918 loss_rnnt 6.980606 hw_loss 0.333257 lr 0.00039055 rank 2
2023-02-23 13:56:55,941 DEBUG TRAIN Batch 19/5400 loss 22.110186 loss_att 21.033804 loss_ctc 24.011486 loss_rnnt 21.863148 hw_loss 0.391513 lr 0.00039064 rank 1
2023-02-23 13:56:55,941 DEBUG TRAIN Batch 19/5400 loss 9.420753 loss_att 11.019818 loss_ctc 11.204361 loss_rnnt 8.673641 hw_loss 0.355284 lr 0.00039064 rank 0
2023-02-23 13:56:55,943 DEBUG TRAIN Batch 19/5400 loss 8.702436 loss_att 11.495604 loss_ctc 13.975037 loss_rnnt 7.252318 hw_loss 0.353385 lr 0.00039060 rank 5
2023-02-23 13:56:55,944 DEBUG TRAIN Batch 19/5400 loss 5.295321 loss_att 7.700677 loss_ctc 5.698037 loss_rnnt 4.558290 hw_loss 0.379244 lr 0.00039059 rank 6
2023-02-23 13:56:55,944 DEBUG TRAIN Batch 19/5400 loss 11.677064 loss_att 14.751941 loss_ctc 13.536350 loss_rnnt 10.577572 hw_loss 0.443647 lr 0.00039062 rank 4
2023-02-23 13:58:08,811 DEBUG TRAIN Batch 19/5500 loss 12.542421 loss_att 14.278334 loss_ctc 17.843941 loss_rnnt 11.253546 hw_loss 0.440295 lr 0.00039048 rank 5
2023-02-23 13:58:08,812 DEBUG TRAIN Batch 19/5500 loss 12.037152 loss_att 13.768390 loss_ctc 15.637079 loss_rnnt 10.991570 hw_loss 0.411269 lr 0.00039052 rank 1
2023-02-23 13:58:08,827 DEBUG TRAIN Batch 19/5500 loss 4.213241 loss_att 7.459962 loss_ctc 5.758010 loss_rnnt 3.174774 hw_loss 0.343414 lr 0.00039040 rank 7
2023-02-23 13:58:08,833 DEBUG TRAIN Batch 19/5500 loss 10.252257 loss_att 13.461283 loss_ctc 18.742834 loss_rnnt 8.279754 hw_loss 0.372414 lr 0.00039052 rank 0
2023-02-23 13:58:08,833 DEBUG TRAIN Batch 19/5500 loss 17.139574 loss_att 19.194897 loss_ctc 20.452377 loss_rnnt 16.094778 hw_loss 0.360044 lr 0.00039039 rank 3
2023-02-23 13:58:08,839 DEBUG TRAIN Batch 19/5500 loss 5.438484 loss_att 8.435006 loss_ctc 6.986742 loss_rnnt 4.442724 hw_loss 0.356290 lr 0.00039044 rank 2
2023-02-23 13:58:08,843 DEBUG TRAIN Batch 19/5500 loss 9.547514 loss_att 11.591119 loss_ctc 13.193264 loss_rnnt 8.408980 hw_loss 0.456961 lr 0.00039047 rank 6
2023-02-23 13:58:08,847 DEBUG TRAIN Batch 19/5500 loss 15.268063 loss_att 20.547089 loss_ctc 26.194315 loss_rnnt 12.572515 hw_loss 0.342954 lr 0.00039050 rank 4
2023-02-23 13:59:21,665 DEBUG TRAIN Batch 19/5600 loss 8.049454 loss_att 10.235712 loss_ctc 8.651839 loss_rnnt 7.322117 hw_loss 0.393312 lr 0.00039032 rank 2
2023-02-23 13:59:21,666 DEBUG TRAIN Batch 19/5600 loss 7.362705 loss_att 11.394886 loss_ctc 8.905972 loss_rnnt 6.161931 hw_loss 0.353566 lr 0.00039028 rank 7
2023-02-23 13:59:21,666 DEBUG TRAIN Batch 19/5600 loss 7.355763 loss_att 12.620386 loss_ctc 13.483991 loss_rnnt 5.280261 hw_loss 0.385276 lr 0.00039040 rank 0
2023-02-23 13:59:21,666 DEBUG TRAIN Batch 19/5600 loss 21.924240 loss_att 27.793758 loss_ctc 25.879206 loss_rnnt 20.033876 hw_loss 0.354618 lr 0.00039038 rank 4
2023-02-23 13:59:21,669 DEBUG TRAIN Batch 19/5600 loss 8.824463 loss_att 11.679288 loss_ctc 10.024380 loss_rnnt 7.837118 hw_loss 0.480732 lr 0.00039027 rank 3
2023-02-23 13:59:21,672 DEBUG TRAIN Batch 19/5600 loss 12.745625 loss_att 14.509336 loss_ctc 17.894161 loss_rnnt 11.376041 hw_loss 0.619441 lr 0.00039036 rank 6
2023-02-23 13:59:21,684 DEBUG TRAIN Batch 19/5600 loss 6.485300 loss_att 7.072167 loss_ctc 8.748282 loss_rnnt 5.809978 hw_loss 0.480407 lr 0.00039036 rank 5
2023-02-23 13:59:21,720 DEBUG TRAIN Batch 19/5600 loss 3.483476 loss_att 6.635869 loss_ctc 7.346201 loss_rnnt 2.111470 hw_loss 0.424684 lr 0.00039041 rank 1
2023-02-23 14:00:37,268 DEBUG TRAIN Batch 19/5700 loss 7.823667 loss_att 12.511923 loss_ctc 12.820293 loss_rnnt 6.000648 hw_loss 0.410907 lr 0.00039016 rank 3
2023-02-23 14:00:37,273 DEBUG TRAIN Batch 19/5700 loss 6.587745 loss_att 9.474337 loss_ctc 8.605873 loss_rnnt 5.533733 hw_loss 0.389269 lr 0.00039016 rank 7
2023-02-23 14:00:37,281 DEBUG TRAIN Batch 19/5700 loss 25.936922 loss_att 27.725309 loss_ctc 42.480492 loss_rnnt 23.079718 hw_loss 0.550723 lr 0.00039024 rank 6
2023-02-23 14:00:37,291 DEBUG TRAIN Batch 19/5700 loss 9.123092 loss_att 10.927452 loss_ctc 12.934146 loss_rnnt 8.015336 hw_loss 0.447642 lr 0.00039020 rank 2
2023-02-23 14:00:37,295 DEBUG TRAIN Batch 19/5700 loss 10.982929 loss_att 11.567026 loss_ctc 15.517782 loss_rnnt 10.029406 hw_loss 0.435106 lr 0.00039029 rank 1
2023-02-23 14:00:37,295 DEBUG TRAIN Batch 19/5700 loss 9.928551 loss_att 14.570661 loss_ctc 14.269148 loss_rnnt 8.239340 hw_loss 0.341331 lr 0.00039024 rank 5
2023-02-23 14:00:37,313 DEBUG TRAIN Batch 19/5700 loss 4.509743 loss_att 7.324686 loss_ctc 6.925036 loss_rnnt 3.406867 hw_loss 0.408464 lr 0.00039026 rank 4
2023-02-23 14:00:37,345 DEBUG TRAIN Batch 19/5700 loss 10.923734 loss_att 14.027692 loss_ctc 17.355141 loss_rnnt 9.160856 hw_loss 0.533560 lr 0.00039028 rank 0
2023-02-23 14:01:49,116 DEBUG TRAIN Batch 19/5800 loss 6.212123 loss_att 8.618902 loss_ctc 9.305738 loss_rnnt 5.096361 hw_loss 0.416110 lr 0.00039004 rank 7
2023-02-23 14:01:49,124 DEBUG TRAIN Batch 19/5800 loss 4.958543 loss_att 9.080058 loss_ctc 6.989043 loss_rnnt 3.654947 hw_loss 0.391049 lr 0.00039004 rank 3
2023-02-23 14:01:49,127 DEBUG TRAIN Batch 19/5800 loss 8.977753 loss_att 12.983854 loss_ctc 14.245297 loss_rnnt 7.237729 hw_loss 0.443367 lr 0.00039016 rank 0
2023-02-23 14:01:49,128 DEBUG TRAIN Batch 19/5800 loss 12.044282 loss_att 13.475842 loss_ctc 15.300005 loss_rnnt 11.057445 hw_loss 0.499553 lr 0.00039008 rank 2
2023-02-23 14:01:49,129 DEBUG TRAIN Batch 19/5800 loss 19.045744 loss_att 22.613775 loss_ctc 27.167278 loss_rnnt 17.007713 hw_loss 0.452909 lr 0.00039012 rank 6
2023-02-23 14:01:49,130 DEBUG TRAIN Batch 19/5800 loss 8.067654 loss_att 8.796999 loss_ctc 9.691639 loss_rnnt 7.378949 hw_loss 0.611819 lr 0.00039014 rank 4
2023-02-23 14:01:49,130 DEBUG TRAIN Batch 19/5800 loss 13.483150 loss_att 14.932700 loss_ctc 28.677673 loss_rnnt 10.940715 hw_loss 0.424855 lr 0.00039017 rank 1
2023-02-23 14:01:49,130 DEBUG TRAIN Batch 19/5800 loss 9.920551 loss_att 13.217205 loss_ctc 14.405615 loss_rnnt 8.451455 hw_loss 0.397044 lr 0.00039012 rank 5
2023-02-23 14:03:01,548 DEBUG TRAIN Batch 19/5900 loss 23.441330 loss_att 30.496597 loss_ctc 40.218723 loss_rnnt 19.563681 hw_loss 0.430515 lr 0.00038993 rank 7
2023-02-23 14:03:01,553 DEBUG TRAIN Batch 19/5900 loss 8.050104 loss_att 10.044437 loss_ctc 14.873108 loss_rnnt 6.514667 hw_loss 0.425321 lr 0.00038996 rank 2
2023-02-23 14:03:01,557 DEBUG TRAIN Batch 19/5900 loss 8.539743 loss_att 10.433899 loss_ctc 15.720909 loss_rnnt 6.969911 hw_loss 0.437837 lr 0.00038992 rank 3
2023-02-23 14:03:01,559 DEBUG TRAIN Batch 19/5900 loss 7.539396 loss_att 12.358212 loss_ctc 12.498993 loss_rnnt 5.741086 hw_loss 0.324876 lr 0.00039002 rank 4
2023-02-23 14:03:01,560 DEBUG TRAIN Batch 19/5900 loss 2.925453 loss_att 9.389258 loss_ctc 4.246831 loss_rnnt 1.235903 hw_loss 0.413636 lr 0.00039000 rank 5
2023-02-23 14:03:01,560 DEBUG TRAIN Batch 19/5900 loss 4.900743 loss_att 6.231674 loss_ctc 6.350474 loss_rnnt 4.195882 hw_loss 0.460084 lr 0.00039000 rank 6
2023-02-23 14:03:01,561 DEBUG TRAIN Batch 19/5900 loss 8.494885 loss_att 10.694669 loss_ctc 12.077804 loss_rnnt 7.392896 hw_loss 0.345581 lr 0.00039005 rank 1
2023-02-23 14:03:01,952 DEBUG TRAIN Batch 19/5900 loss 10.257710 loss_att 10.612714 loss_ctc 12.087781 loss_rnnt 9.646256 hw_loss 0.555830 lr 0.00039004 rank 0
2023-02-23 14:04:16,010 DEBUG TRAIN Batch 19/6000 loss 7.644612 loss_att 8.120962 loss_ctc 9.371042 loss_rnnt 7.097193 hw_loss 0.416173 lr 0.00038989 rank 5
2023-02-23 14:04:16,010 DEBUG TRAIN Batch 19/6000 loss 6.550681 loss_att 8.528086 loss_ctc 8.268269 loss_rnnt 5.707458 hw_loss 0.410118 lr 0.00038993 rank 1
2023-02-23 14:04:16,017 DEBUG TRAIN Batch 19/6000 loss 15.175512 loss_att 15.120480 loss_ctc 19.332306 loss_rnnt 14.403542 hw_loss 0.428885 lr 0.00038981 rank 7
2023-02-23 14:04:16,018 DEBUG TRAIN Batch 19/6000 loss 3.984095 loss_att 7.968295 loss_ctc 5.549294 loss_rnnt 2.774759 hw_loss 0.382130 lr 0.00038990 rank 4
2023-02-23 14:04:16,018 DEBUG TRAIN Batch 19/6000 loss 10.064528 loss_att 10.850203 loss_ctc 13.688896 loss_rnnt 9.214504 hw_loss 0.393076 lr 0.00038993 rank 0
2023-02-23 14:04:16,019 DEBUG TRAIN Batch 19/6000 loss 6.436497 loss_att 7.874827 loss_ctc 8.554665 loss_rnnt 5.658249 hw_loss 0.390299 lr 0.00038988 rank 6
2023-02-23 14:04:16,023 DEBUG TRAIN Batch 19/6000 loss 3.188917 loss_att 8.102021 loss_ctc 7.040645 loss_rnnt 1.495599 hw_loss 0.369625 lr 0.00038984 rank 2
2023-02-23 14:04:16,024 DEBUG TRAIN Batch 19/6000 loss 7.940728 loss_att 10.174130 loss_ctc 9.891510 loss_rnnt 7.020986 hw_loss 0.399295 lr 0.00038980 rank 3
2023-02-23 14:05:30,167 DEBUG TRAIN Batch 19/6100 loss 7.024661 loss_att 9.166283 loss_ctc 7.273082 loss_rnnt 6.334749 hw_loss 0.428373 lr 0.00038969 rank 7
2023-02-23 14:05:30,167 DEBUG TRAIN Batch 19/6100 loss 8.094018 loss_att 11.041456 loss_ctc 13.619899 loss_rnnt 6.556021 hw_loss 0.396985 lr 0.00038981 rank 0
2023-02-23 14:05:30,169 DEBUG TRAIN Batch 19/6100 loss 11.548761 loss_att 12.605417 loss_ctc 13.719656 loss_rnnt 10.815536 hw_loss 0.435829 lr 0.00038972 rank 2
2023-02-23 14:05:30,172 DEBUG TRAIN Batch 19/6100 loss 11.526855 loss_att 14.951056 loss_ctc 18.095387 loss_rnnt 9.715116 hw_loss 0.470801 lr 0.00038981 rank 1
2023-02-23 14:05:30,175 DEBUG TRAIN Batch 19/6100 loss 5.608731 loss_att 9.522110 loss_ctc 8.306427 loss_rnnt 4.263133 hw_loss 0.381055 lr 0.00038977 rank 5
2023-02-23 14:05:30,175 DEBUG TRAIN Batch 19/6100 loss 5.713184 loss_att 8.839448 loss_ctc 8.448324 loss_rnnt 4.514149 hw_loss 0.392055 lr 0.00038968 rank 3
2023-02-23 14:05:30,176 DEBUG TRAIN Batch 19/6100 loss 14.421868 loss_att 16.198389 loss_ctc 19.295715 loss_rnnt 13.192434 hw_loss 0.420536 lr 0.00038976 rank 6
2023-02-23 14:05:30,221 DEBUG TRAIN Batch 19/6100 loss 7.654847 loss_att 12.676455 loss_ctc 14.638216 loss_rnnt 5.525629 hw_loss 0.363339 lr 0.00038978 rank 4
2023-02-23 14:06:43,077 DEBUG TRAIN Batch 19/6200 loss 10.229680 loss_att 15.009486 loss_ctc 17.765270 loss_rnnt 8.048510 hw_loss 0.413369 lr 0.00038961 rank 2
2023-02-23 14:06:43,089 DEBUG TRAIN Batch 19/6200 loss 7.725022 loss_att 10.421280 loss_ctc 11.016183 loss_rnnt 6.537221 hw_loss 0.393239 lr 0.00038956 rank 3
2023-02-23 14:06:43,089 DEBUG TRAIN Batch 19/6200 loss 8.628252 loss_att 13.195672 loss_ctc 11.358418 loss_rnnt 7.127057 hw_loss 0.419417 lr 0.00038957 rank 7
2023-02-23 14:06:43,092 DEBUG TRAIN Batch 19/6200 loss 9.907925 loss_att 12.327333 loss_ctc 14.265174 loss_rnnt 8.636839 hw_loss 0.386697 lr 0.00038969 rank 0
2023-02-23 14:06:43,092 DEBUG TRAIN Batch 19/6200 loss 7.446983 loss_att 9.896616 loss_ctc 9.391998 loss_rnnt 6.481361 hw_loss 0.405677 lr 0.00038969 rank 1
2023-02-23 14:06:43,094 DEBUG TRAIN Batch 19/6200 loss 11.991758 loss_att 13.831551 loss_ctc 15.003852 loss_rnnt 11.019574 hw_loss 0.379898 lr 0.00038967 rank 4
2023-02-23 14:06:43,094 DEBUG TRAIN Batch 19/6200 loss 10.515035 loss_att 11.860590 loss_ctc 13.711191 loss_rnnt 9.603920 hw_loss 0.404716 lr 0.00038965 rank 5
2023-02-23 14:06:43,095 DEBUG TRAIN Batch 19/6200 loss 11.888003 loss_att 10.593405 loss_ctc 14.990115 loss_rnnt 11.533892 hw_loss 0.373901 lr 0.00038964 rank 6
2023-02-23 14:07:55,302 DEBUG TRAIN Batch 19/6300 loss 13.528002 loss_att 15.387509 loss_ctc 20.556963 loss_rnnt 11.953003 hw_loss 0.498567 lr 0.00038958 rank 1
2023-02-23 14:07:55,304 DEBUG TRAIN Batch 19/6300 loss 8.372309 loss_att 10.314129 loss_ctc 11.719927 loss_rnnt 7.337405 hw_loss 0.375356 lr 0.00038945 rank 7
2023-02-23 14:07:55,305 DEBUG TRAIN Batch 19/6300 loss 9.749533 loss_att 11.796127 loss_ctc 16.465273 loss_rnnt 8.212597 hw_loss 0.435348 lr 0.00038949 rank 2
2023-02-23 14:07:55,305 DEBUG TRAIN Batch 19/6300 loss 7.624002 loss_att 10.853377 loss_ctc 11.663993 loss_rnnt 6.215400 hw_loss 0.420116 lr 0.00038957 rank 0
2023-02-23 14:07:55,309 DEBUG TRAIN Batch 19/6300 loss 7.124623 loss_att 6.741290 loss_ctc 8.283929 loss_rnnt 6.660367 hw_loss 0.724403 lr 0.00038945 rank 3
2023-02-23 14:07:55,309 DEBUG TRAIN Batch 19/6300 loss 12.779059 loss_att 13.936337 loss_ctc 14.689423 loss_rnnt 12.070544 hw_loss 0.416896 lr 0.00038953 rank 5
2023-02-23 14:07:55,311 DEBUG TRAIN Batch 19/6300 loss 15.291129 loss_att 19.900387 loss_ctc 21.343601 loss_rnnt 13.327774 hw_loss 0.439701 lr 0.00038955 rank 4
2023-02-23 14:07:55,314 DEBUG TRAIN Batch 19/6300 loss 5.196985 loss_att 9.048644 loss_ctc 7.028912 loss_rnnt 3.970870 hw_loss 0.396612 lr 0.00038953 rank 6
2023-02-23 14:09:10,652 DEBUG TRAIN Batch 19/6400 loss 10.999734 loss_att 10.998013 loss_ctc 14.885101 loss_rnnt 10.340115 hw_loss 0.266089 lr 0.00038933 rank 7
2023-02-23 14:09:10,659 DEBUG TRAIN Batch 19/6400 loss 5.753311 loss_att 7.167072 loss_ctc 8.138165 loss_rnnt 4.874003 hw_loss 0.522328 lr 0.00038943 rank 4
2023-02-23 14:09:10,659 DEBUG TRAIN Batch 19/6400 loss 12.072381 loss_att 15.338339 loss_ctc 16.258234 loss_rnnt 10.572974 hw_loss 0.540192 lr 0.00038933 rank 3
2023-02-23 14:09:10,665 DEBUG TRAIN Batch 19/6400 loss 5.222264 loss_att 10.786315 loss_ctc 8.914897 loss_rnnt 3.421073 hw_loss 0.367555 lr 0.00038945 rank 0
2023-02-23 14:09:10,665 DEBUG TRAIN Batch 19/6400 loss 2.715995 loss_att 5.746382 loss_ctc 4.101152 loss_rnnt 1.736617 hw_loss 0.353649 lr 0.00038941 rank 6
2023-02-23 14:09:10,667 DEBUG TRAIN Batch 19/6400 loss 10.354444 loss_att 10.928656 loss_ctc 8.569566 loss_rnnt 10.273655 hw_loss 0.382369 lr 0.00038941 rank 5
2023-02-23 14:09:10,669 DEBUG TRAIN Batch 19/6400 loss 6.541731 loss_att 7.588542 loss_ctc 10.370663 loss_rnnt 5.523665 hw_loss 0.559087 lr 0.00038937 rank 2
2023-02-23 14:09:10,686 DEBUG TRAIN Batch 19/6400 loss 11.085451 loss_att 12.859108 loss_ctc 20.636299 loss_rnnt 9.240661 hw_loss 0.406150 lr 0.00038946 rank 1
2023-02-23 14:10:23,919 DEBUG TRAIN Batch 19/6500 loss 11.129261 loss_att 15.270086 loss_ctc 17.149937 loss_rnnt 9.287715 hw_loss 0.394921 lr 0.00038925 rank 2
2023-02-23 14:10:23,919 DEBUG TRAIN Batch 19/6500 loss 9.149318 loss_att 13.672776 loss_ctc 12.421600 loss_rnnt 7.584375 hw_loss 0.419901 lr 0.00038922 rank 7
2023-02-23 14:10:23,920 DEBUG TRAIN Batch 19/6500 loss 11.513526 loss_att 12.183331 loss_ctc 15.676424 loss_rnnt 10.526370 hw_loss 0.559018 lr 0.00038933 rank 0
2023-02-23 14:10:23,921 DEBUG TRAIN Batch 19/6500 loss 14.708710 loss_att 18.070303 loss_ctc 22.197615 loss_rnnt 12.790781 hw_loss 0.463289 lr 0.00038929 rank 5
2023-02-23 14:10:23,923 DEBUG TRAIN Batch 19/6500 loss 3.830272 loss_att 6.754209 loss_ctc 4.198172 loss_rnnt 2.956975 hw_loss 0.448981 lr 0.00038934 rank 1
2023-02-23 14:10:23,924 DEBUG TRAIN Batch 19/6500 loss 13.505535 loss_att 15.823302 loss_ctc 20.585861 loss_rnnt 11.892993 hw_loss 0.384273 lr 0.00038921 rank 3
2023-02-23 14:10:23,925 DEBUG TRAIN Batch 19/6500 loss 16.711535 loss_att 19.441393 loss_ctc 27.450827 loss_rnnt 14.508661 hw_loss 0.421868 lr 0.00038929 rank 6
2023-02-23 14:10:23,928 DEBUG TRAIN Batch 19/6500 loss 2.738835 loss_att 6.381845 loss_ctc 3.761158 loss_rnnt 1.658522 hw_loss 0.403878 lr 0.00038931 rank 4
2023-02-23 14:11:36,833 DEBUG TRAIN Batch 19/6600 loss 5.911045 loss_att 7.343363 loss_ctc 8.871988 loss_rnnt 5.068959 hw_loss 0.301556 lr 0.00038917 rank 6
2023-02-23 14:11:36,833 DEBUG TRAIN Batch 19/6600 loss 4.703395 loss_att 5.852727 loss_ctc 6.049472 loss_rnnt 4.094389 hw_loss 0.374366 lr 0.00038910 rank 7
2023-02-23 14:11:36,834 DEBUG TRAIN Batch 19/6600 loss 13.838149 loss_att 17.410164 loss_ctc 18.151478 loss_rnnt 12.323122 hw_loss 0.422838 lr 0.00038913 rank 2
2023-02-23 14:11:36,835 DEBUG TRAIN Batch 19/6600 loss 15.547048 loss_att 20.374352 loss_ctc 24.812416 loss_rnnt 13.110497 hw_loss 0.441954 lr 0.00038922 rank 0
2023-02-23 14:11:36,839 DEBUG TRAIN Batch 19/6600 loss 8.536056 loss_att 11.081057 loss_ctc 14.228720 loss_rnnt 7.039897 hw_loss 0.427755 lr 0.00038909 rank 3
2023-02-23 14:11:36,839 DEBUG TRAIN Batch 19/6600 loss 15.806971 loss_att 17.376822 loss_ctc 22.936005 loss_rnnt 14.262625 hw_loss 0.524698 lr 0.00038922 rank 1
2023-02-23 14:11:36,840 DEBUG TRAIN Batch 19/6600 loss 7.187746 loss_att 10.661528 loss_ctc 9.725108 loss_rnnt 5.895002 hw_loss 0.486887 lr 0.00038919 rank 4
2023-02-23 14:11:36,842 DEBUG TRAIN Batch 19/6600 loss 18.541033 loss_att 20.401941 loss_ctc 26.103451 loss_rnnt 16.888147 hw_loss 0.510715 lr 0.00038918 rank 5
2023-02-23 14:12:51,020 DEBUG TRAIN Batch 19/6700 loss 7.053647 loss_att 8.433922 loss_ctc 10.043681 loss_rnnt 6.134131 hw_loss 0.458978 lr 0.00038910 rank 1
2023-02-23 14:12:51,025 DEBUG TRAIN Batch 19/6700 loss 5.623829 loss_att 6.803617 loss_ctc 9.260096 loss_rnnt 4.659598 hw_loss 0.456445 lr 0.00038898 rank 7
2023-02-23 14:12:51,031 DEBUG TRAIN Batch 19/6700 loss 8.802347 loss_att 11.063913 loss_ctc 9.968794 loss_rnnt 7.969369 hw_loss 0.422133 lr 0.00038908 rank 4
2023-02-23 14:12:51,032 DEBUG TRAIN Batch 19/6700 loss 14.522956 loss_att 15.584471 loss_ctc 17.290342 loss_rnnt 13.699657 hw_loss 0.453769 lr 0.00038901 rank 2
2023-02-23 14:12:51,033 DEBUG TRAIN Batch 19/6700 loss 4.537790 loss_att 7.133129 loss_ctc 6.372424 loss_rnnt 3.564211 hw_loss 0.393549 lr 0.00038906 rank 5
2023-02-23 14:12:51,035 DEBUG TRAIN Batch 19/6700 loss 13.030967 loss_att 17.334404 loss_ctc 16.366314 loss_rnnt 11.524899 hw_loss 0.376252 lr 0.00038897 rank 3
2023-02-23 14:12:51,045 DEBUG TRAIN Batch 19/6700 loss 9.735284 loss_att 13.269606 loss_ctc 15.582586 loss_rnnt 8.029270 hw_loss 0.411579 lr 0.00038905 rank 6
2023-02-23 14:12:51,046 DEBUG TRAIN Batch 19/6700 loss 7.802728 loss_att 8.587586 loss_ctc 9.741426 loss_rnnt 7.194356 hw_loss 0.361700 lr 0.00038910 rank 0
2023-02-23 14:14:05,760 DEBUG TRAIN Batch 19/6800 loss 14.361797 loss_att 18.801510 loss_ctc 22.116924 loss_rnnt 12.192677 hw_loss 0.463426 lr 0.00038886 rank 3
2023-02-23 14:14:05,761 DEBUG TRAIN Batch 19/6800 loss 5.421446 loss_att 9.561496 loss_ctc 7.977159 loss_rnnt 4.026049 hw_loss 0.424922 lr 0.00038899 rank 1
2023-02-23 14:14:05,763 DEBUG TRAIN Batch 19/6800 loss 7.736731 loss_att 10.556541 loss_ctc 14.315106 loss_rnnt 6.054661 hw_loss 0.451859 lr 0.00038894 rank 6
2023-02-23 14:14:05,765 DEBUG TRAIN Batch 19/6800 loss 5.865684 loss_att 6.460029 loss_ctc 7.232949 loss_rnnt 5.328718 hw_loss 0.442116 lr 0.00038890 rank 2
2023-02-23 14:14:05,764 DEBUG TRAIN Batch 19/6800 loss 6.599619 loss_att 9.244749 loss_ctc 10.367854 loss_rnnt 5.318080 hw_loss 0.468903 lr 0.00038896 rank 4
2023-02-23 14:14:05,765 DEBUG TRAIN Batch 19/6800 loss 2.589497 loss_att 5.332855 loss_ctc 3.789337 loss_rnnt 1.680530 hw_loss 0.375594 lr 0.00038898 rank 0
2023-02-23 14:14:05,765 DEBUG TRAIN Batch 19/6800 loss 6.192486 loss_att 8.719295 loss_ctc 6.660058 loss_rnnt 5.440196 hw_loss 0.346099 lr 0.00038894 rank 5
2023-02-23 14:14:05,771 DEBUG TRAIN Batch 19/6800 loss 8.539689 loss_att 13.380541 loss_ctc 13.206332 loss_rnnt 6.764851 hw_loss 0.345841 lr 0.00038886 rank 7
2023-02-23 14:15:19,351 DEBUG TRAIN Batch 19/6900 loss 13.315946 loss_att 16.895706 loss_ctc 16.826141 loss_rnnt 11.868851 hw_loss 0.493343 lr 0.00038882 rank 5
2023-02-23 14:15:19,359 DEBUG TRAIN Batch 19/6900 loss 8.217081 loss_att 8.640989 loss_ctc 11.219492 loss_rnnt 7.434813 hw_loss 0.557185 lr 0.00038882 rank 6
2023-02-23 14:15:19,360 DEBUG TRAIN Batch 19/6900 loss 8.600891 loss_att 9.946831 loss_ctc 9.583611 loss_rnnt 7.996038 hw_loss 0.383691 lr 0.00038875 rank 7
2023-02-23 14:15:19,360 DEBUG TRAIN Batch 19/6900 loss 14.089478 loss_att 14.446999 loss_ctc 15.427570 loss_rnnt 13.654575 hw_loss 0.346849 lr 0.00038884 rank 4
2023-02-23 14:15:19,360 DEBUG TRAIN Batch 19/6900 loss 7.964936 loss_att 9.811973 loss_ctc 10.664999 loss_rnnt 6.973454 hw_loss 0.491376 lr 0.00038887 rank 1
2023-02-23 14:15:19,363 DEBUG TRAIN Batch 19/6900 loss 12.973367 loss_att 16.195023 loss_ctc 20.501183 loss_rnnt 11.129197 hw_loss 0.367741 lr 0.00038878 rank 2
2023-02-23 14:15:19,364 DEBUG TRAIN Batch 19/6900 loss 15.138188 loss_att 16.269661 loss_ctc 21.787146 loss_rnnt 13.761381 hw_loss 0.494973 lr 0.00038874 rank 3
2023-02-23 14:15:19,411 DEBUG TRAIN Batch 19/6900 loss 8.123433 loss_att 12.678148 loss_ctc 12.148224 loss_rnnt 6.462138 hw_loss 0.400712 lr 0.00038886 rank 0
2023-02-23 14:16:32,075 DEBUG TRAIN Batch 19/7000 loss 5.735006 loss_att 6.901303 loss_ctc 8.970077 loss_rnnt 4.809953 hw_loss 0.488344 lr 0.00038866 rank 2
2023-02-23 14:16:32,077 DEBUG TRAIN Batch 19/7000 loss 15.893585 loss_att 19.598001 loss_ctc 19.716017 loss_rnnt 14.432014 hw_loss 0.395681 lr 0.00038875 rank 1
2023-02-23 14:16:32,076 DEBUG TRAIN Batch 19/7000 loss 9.255033 loss_att 9.439876 loss_ctc 10.662186 loss_rnnt 8.622604 hw_loss 0.764702 lr 0.00038863 rank 7
2023-02-23 14:16:32,077 DEBUG TRAIN Batch 19/7000 loss 15.456004 loss_att 14.712004 loss_ctc 20.934599 loss_rnnt 14.498870 hw_loss 0.703978 lr 0.00038871 rank 5
2023-02-23 14:16:32,080 DEBUG TRAIN Batch 19/7000 loss 8.057950 loss_att 9.694857 loss_ctc 10.259450 loss_rnnt 7.222180 hw_loss 0.402851 lr 0.00038862 rank 3
2023-02-23 14:16:32,081 DEBUG TRAIN Batch 19/7000 loss 6.895976 loss_att 9.938888 loss_ctc 13.760778 loss_rnnt 5.184023 hw_loss 0.352618 lr 0.00038870 rank 6
2023-02-23 14:16:32,081 DEBUG TRAIN Batch 19/7000 loss 9.266689 loss_att 14.126974 loss_ctc 15.136432 loss_rnnt 7.285527 hw_loss 0.424634 lr 0.00038872 rank 4
2023-02-23 14:16:32,083 DEBUG TRAIN Batch 19/7000 loss 8.372576 loss_att 10.211104 loss_ctc 10.784639 loss_rnnt 7.402344 hw_loss 0.526722 lr 0.00038875 rank 0
2023-02-23 14:17:47,224 DEBUG TRAIN Batch 19/7100 loss 17.772171 loss_att 18.699741 loss_ctc 25.288916 loss_rnnt 16.345638 hw_loss 0.447720 lr 0.00038859 rank 5
2023-02-23 14:17:47,225 DEBUG TRAIN Batch 19/7100 loss 7.010153 loss_att 9.150811 loss_ctc 12.086943 loss_rnnt 5.658106 hw_loss 0.463143 lr 0.00038854 rank 2
2023-02-23 14:17:47,227 DEBUG TRAIN Batch 19/7100 loss 7.598722 loss_att 8.513573 loss_ctc 9.107780 loss_rnnt 7.046587 hw_loss 0.314919 lr 0.00038858 rank 6
2023-02-23 14:17:47,230 DEBUG TRAIN Batch 19/7100 loss 11.126604 loss_att 12.387976 loss_ctc 15.662578 loss_rnnt 9.965297 hw_loss 0.570446 lr 0.00038863 rank 0
2023-02-23 14:17:47,231 DEBUG TRAIN Batch 19/7100 loss 5.408957 loss_att 8.572380 loss_ctc 8.708517 loss_rnnt 4.128933 hw_loss 0.388873 lr 0.00038863 rank 1
2023-02-23 14:17:47,236 DEBUG TRAIN Batch 19/7100 loss 12.126804 loss_att 14.000991 loss_ctc 15.356730 loss_rnnt 11.130919 hw_loss 0.356984 lr 0.00038850 rank 3
2023-02-23 14:17:47,244 DEBUG TRAIN Batch 19/7100 loss 13.440010 loss_att 14.770685 loss_ctc 16.441914 loss_rnnt 12.495279 hw_loss 0.521889 lr 0.00038851 rank 7
2023-02-23 14:17:47,264 DEBUG TRAIN Batch 19/7100 loss 10.886736 loss_att 12.912528 loss_ctc 13.938910 loss_rnnt 9.865097 hw_loss 0.392856 lr 0.00038861 rank 4
2023-02-23 14:18:59,855 DEBUG TRAIN Batch 19/7200 loss 9.525160 loss_att 12.973997 loss_ctc 15.945701 loss_rnnt 7.768894 hw_loss 0.394549 lr 0.00038849 rank 4
2023-02-23 14:18:59,857 DEBUG TRAIN Batch 19/7200 loss 7.877469 loss_att 10.888730 loss_ctc 12.056014 loss_rnnt 6.483207 hw_loss 0.440382 lr 0.00038839 rank 7
2023-02-23 14:18:59,857 DEBUG TRAIN Batch 19/7200 loss 7.522601 loss_att 12.036734 loss_ctc 13.441109 loss_rnnt 5.666452 hw_loss 0.307852 lr 0.00038847 rank 5
2023-02-23 14:18:59,858 DEBUG TRAIN Batch 19/7200 loss 6.403807 loss_att 8.551149 loss_ctc 7.257011 loss_rnnt 5.675264 hw_loss 0.347464 lr 0.00038839 rank 3
2023-02-23 14:18:59,859 DEBUG TRAIN Batch 19/7200 loss 10.695530 loss_att 15.642827 loss_ctc 18.548508 loss_rnnt 8.402212 hw_loss 0.481490 lr 0.00038852 rank 1
2023-02-23 14:18:59,860 DEBUG TRAIN Batch 19/7200 loss 4.530320 loss_att 5.773560 loss_ctc 4.783021 loss_rnnt 4.015003 hw_loss 0.436831 lr 0.00038843 rank 2
2023-02-23 14:18:59,865 DEBUG TRAIN Batch 19/7200 loss 9.434769 loss_att 11.038641 loss_ctc 12.222727 loss_rnnt 8.526737 hw_loss 0.404118 lr 0.00038847 rank 6
2023-02-23 14:18:59,870 DEBUG TRAIN Batch 19/7200 loss 9.688807 loss_att 11.756897 loss_ctc 11.139786 loss_rnnt 8.829138 hw_loss 0.473599 lr 0.00038851 rank 0
2023-02-23 14:20:11,519 DEBUG TRAIN Batch 19/7300 loss 11.483805 loss_att 14.599415 loss_ctc 19.822010 loss_rnnt 9.533082 hw_loss 0.404701 lr 0.00038827 rank 3
2023-02-23 14:20:11,522 DEBUG TRAIN Batch 19/7300 loss 8.191648 loss_att 11.040595 loss_ctc 11.477769 loss_rnnt 6.975311 hw_loss 0.390746 lr 0.00038828 rank 7
2023-02-23 14:20:11,522 DEBUG TRAIN Batch 19/7300 loss 23.664534 loss_att 26.224747 loss_ctc 37.801682 loss_rnnt 21.066586 hw_loss 0.376789 lr 0.00038839 rank 0
2023-02-23 14:20:11,522 DEBUG TRAIN Batch 19/7300 loss 15.144935 loss_att 18.664366 loss_ctc 21.176933 loss_rnnt 13.425982 hw_loss 0.395251 lr 0.00038835 rank 5
2023-02-23 14:20:11,525 DEBUG TRAIN Batch 19/7300 loss 10.352041 loss_att 13.635268 loss_ctc 14.296132 loss_rnnt 8.986752 hw_loss 0.342687 lr 0.00038831 rank 2
2023-02-23 14:20:11,525 DEBUG TRAIN Batch 19/7300 loss 13.250311 loss_att 14.936379 loss_ctc 18.687292 loss_rnnt 12.008321 hw_loss 0.337211 lr 0.00038840 rank 1
2023-02-23 14:20:11,527 DEBUG TRAIN Batch 19/7300 loss 6.006544 loss_att 7.846842 loss_ctc 7.574224 loss_rnnt 5.187827 hw_loss 0.453062 lr 0.00038837 rank 4
2023-02-23 14:20:11,537 DEBUG TRAIN Batch 19/7300 loss 8.434544 loss_att 10.555546 loss_ctc 10.640654 loss_rnnt 7.495471 hw_loss 0.413856 lr 0.00038835 rank 6
2023-02-23 14:21:24,258 DEBUG TRAIN Batch 19/7400 loss 8.501805 loss_att 10.974741 loss_ctc 10.164049 loss_rnnt 7.598415 hw_loss 0.350943 lr 0.00038823 rank 6
2023-02-23 14:21:24,269 DEBUG TRAIN Batch 19/7400 loss 9.217450 loss_att 11.577011 loss_ctc 13.295172 loss_rnnt 7.982666 hw_loss 0.410956 lr 0.00038828 rank 0
2023-02-23 14:21:24,270 DEBUG TRAIN Batch 19/7400 loss 5.662571 loss_att 7.594918 loss_ctc 7.469470 loss_rnnt 4.804564 hw_loss 0.432409 lr 0.00038828 rank 1
2023-02-23 14:21:24,273 DEBUG TRAIN Batch 19/7400 loss 12.614547 loss_att 14.086618 loss_ctc 13.318878 loss_rnnt 11.994599 hw_loss 0.434292 lr 0.00038816 rank 7
2023-02-23 14:21:24,274 DEBUG TRAIN Batch 19/7400 loss 10.183401 loss_att 11.194506 loss_ctc 15.700932 loss_rnnt 9.014358 hw_loss 0.433409 lr 0.00038819 rank 2
2023-02-23 14:21:24,281 DEBUG TRAIN Batch 19/7400 loss 7.843983 loss_att 11.060089 loss_ctc 11.175245 loss_rnnt 6.505432 hw_loss 0.470929 lr 0.00038825 rank 4
2023-02-23 14:21:24,281 DEBUG TRAIN Batch 19/7400 loss 16.516714 loss_att 19.626383 loss_ctc 22.390202 loss_rnnt 14.929549 hw_loss 0.341432 lr 0.00038824 rank 5
2023-02-23 14:21:24,282 DEBUG TRAIN Batch 19/7400 loss 15.232546 loss_att 21.104799 loss_ctc 20.407942 loss_rnnt 13.177117 hw_loss 0.357984 lr 0.00038815 rank 3
2023-02-23 14:22:39,162 DEBUG TRAIN Batch 19/7500 loss 4.609983 loss_att 5.426728 loss_ctc 4.867119 loss_rnnt 4.187693 hw_loss 0.421230 lr 0.00038804 rank 7
2023-02-23 14:22:39,163 DEBUG TRAIN Batch 19/7500 loss 10.061976 loss_att 12.759542 loss_ctc 15.576719 loss_rnnt 8.560108 hw_loss 0.425729 lr 0.00038814 rank 4
2023-02-23 14:22:39,165 DEBUG TRAIN Batch 19/7500 loss 8.945220 loss_att 10.713642 loss_ctc 10.198402 loss_rnnt 8.175129 hw_loss 0.467467 lr 0.00038816 rank 1
2023-02-23 14:22:39,165 DEBUG TRAIN Batch 19/7500 loss 11.276915 loss_att 14.532724 loss_ctc 17.015743 loss_rnnt 9.664489 hw_loss 0.367663 lr 0.00038812 rank 5
2023-02-23 14:22:39,168 DEBUG TRAIN Batch 19/7500 loss 5.921381 loss_att 8.030167 loss_ctc 8.440253 loss_rnnt 4.881328 hw_loss 0.529588 lr 0.00038804 rank 3
2023-02-23 14:22:39,169 DEBUG TRAIN Batch 19/7500 loss 13.174887 loss_att 14.006841 loss_ctc 18.621342 loss_rnnt 11.992948 hw_loss 0.542538 lr 0.00038812 rank 6
2023-02-23 14:22:39,171 DEBUG TRAIN Batch 19/7500 loss 9.257854 loss_att 12.283195 loss_ctc 10.670345 loss_rnnt 8.263235 hw_loss 0.377286 lr 0.00038808 rank 2
2023-02-23 14:22:39,172 DEBUG TRAIN Batch 19/7500 loss 6.586034 loss_att 8.570089 loss_ctc 9.015961 loss_rnnt 5.585196 hw_loss 0.525069 lr 0.00038816 rank 0
2023-02-23 14:23:51,659 DEBUG TRAIN Batch 19/7600 loss 15.691714 loss_att 16.133972 loss_ctc 21.349560 loss_rnnt 14.579895 hw_loss 0.504354 lr 0.00038793 rank 7
2023-02-23 14:23:51,660 DEBUG TRAIN Batch 19/7600 loss 9.142792 loss_att 13.101411 loss_ctc 11.708556 loss_rnnt 7.771217 hw_loss 0.445781 lr 0.00038800 rank 5
2023-02-23 14:23:51,661 DEBUG TRAIN Batch 19/7600 loss 12.088481 loss_att 12.171037 loss_ctc 15.081610 loss_rnnt 11.311933 hw_loss 0.676786 lr 0.00038792 rank 3
2023-02-23 14:23:51,662 DEBUG TRAIN Batch 19/7600 loss 13.369599 loss_att 13.085869 loss_ctc 15.591844 loss_rnnt 12.939360 hw_loss 0.357541 lr 0.00038796 rank 2
2023-02-23 14:23:51,663 DEBUG TRAIN Batch 19/7600 loss 7.702651 loss_att 9.280825 loss_ctc 10.579448 loss_rnnt 6.701632 hw_loss 0.565895 lr 0.00038805 rank 1
2023-02-23 14:23:51,664 DEBUG TRAIN Batch 19/7600 loss 9.423674 loss_att 11.830537 loss_ctc 12.696387 loss_rnnt 8.289961 hw_loss 0.404960 lr 0.00038800 rank 6
2023-02-23 14:23:51,668 DEBUG TRAIN Batch 19/7600 loss 10.395518 loss_att 13.031536 loss_ctc 17.779686 loss_rnnt 8.676646 hw_loss 0.388338 lr 0.00038804 rank 0
2023-02-23 14:23:51,708 DEBUG TRAIN Batch 19/7600 loss 2.532841 loss_att 4.924653 loss_ctc 4.906289 loss_rnnt 1.531500 hw_loss 0.387222 lr 0.00038802 rank 4
2023-02-23 14:25:04,692 DEBUG TRAIN Batch 19/7700 loss 6.344428 loss_att 8.462869 loss_ctc 9.429792 loss_rnnt 5.318809 hw_loss 0.357278 lr 0.00038781 rank 7
2023-02-23 14:25:04,694 DEBUG TRAIN Batch 19/7700 loss 3.120744 loss_att 7.688960 loss_ctc 5.367044 loss_rnnt 1.703356 hw_loss 0.382948 lr 0.00038780 rank 3
2023-02-23 14:25:04,695 DEBUG TRAIN Batch 19/7700 loss 8.796966 loss_att 9.072735 loss_ctc 9.992055 loss_rnnt 8.388766 hw_loss 0.363189 lr 0.00038790 rank 4
2023-02-23 14:25:04,696 DEBUG TRAIN Batch 19/7700 loss 9.640165 loss_att 12.495232 loss_ctc 12.118052 loss_rnnt 8.500351 hw_loss 0.447030 lr 0.00038793 rank 0
2023-02-23 14:25:04,697 DEBUG TRAIN Batch 19/7700 loss 9.180588 loss_att 11.903255 loss_ctc 11.420650 loss_rnnt 8.098076 hw_loss 0.448693 lr 0.00038789 rank 5
2023-02-23 14:25:04,699 DEBUG TRAIN Batch 19/7700 loss 9.024957 loss_att 10.231340 loss_ctc 11.167233 loss_rnnt 8.276065 hw_loss 0.416210 lr 0.00038793 rank 1
2023-02-23 14:25:04,699 DEBUG TRAIN Batch 19/7700 loss 3.638526 loss_att 9.110742 loss_ctc 4.119647 loss_rnnt 2.279234 hw_loss 0.376312 lr 0.00038788 rank 6
2023-02-23 14:25:04,700 DEBUG TRAIN Batch 19/7700 loss 7.817502 loss_att 8.511372 loss_ctc 9.839075 loss_rnnt 7.126210 hw_loss 0.530576 lr 0.00038784 rank 2
2023-02-23 14:26:19,625 DEBUG TRAIN Batch 19/7800 loss 9.619278 loss_att 10.499447 loss_ctc 14.296649 loss_rnnt 8.631999 hw_loss 0.351742 lr 0.00038769 rank 7
2023-02-23 14:26:19,625 DEBUG TRAIN Batch 19/7800 loss 8.530267 loss_att 10.514535 loss_ctc 9.220381 loss_rnnt 7.823322 hw_loss 0.408894 lr 0.00038781 rank 1
2023-02-23 14:26:19,627 DEBUG TRAIN Batch 19/7800 loss 6.362394 loss_att 8.781985 loss_ctc 9.171743 loss_rnnt 5.336494 hw_loss 0.313879 lr 0.00038769 rank 3
2023-02-23 14:26:19,629 DEBUG TRAIN Batch 19/7800 loss 4.558712 loss_att 8.192266 loss_ctc 6.008083 loss_rnnt 3.443430 hw_loss 0.366229 lr 0.00038777 rank 5
2023-02-23 14:26:19,630 DEBUG TRAIN Batch 19/7800 loss 15.073077 loss_att 17.617651 loss_ctc 25.885868 loss_rnnt 12.904486 hw_loss 0.408698 lr 0.00038779 rank 4
2023-02-23 14:26:19,630 DEBUG TRAIN Batch 19/7800 loss 5.699086 loss_att 5.465945 loss_ctc 8.130594 loss_rnnt 5.044842 hw_loss 0.706259 lr 0.00038781 rank 0
2023-02-23 14:26:19,630 DEBUG TRAIN Batch 19/7800 loss 10.031847 loss_att 14.745642 loss_ctc 15.380248 loss_rnnt 8.149910 hw_loss 0.423858 lr 0.00038773 rank 2
2023-02-23 14:26:19,652 DEBUG TRAIN Batch 19/7800 loss 12.374675 loss_att 15.215097 loss_ctc 15.975040 loss_rnnt 11.072886 hw_loss 0.475606 lr 0.00038776 rank 6
2023-02-23 14:27:32,951 DEBUG TRAIN Batch 19/7900 loss 5.708478 loss_att 9.022558 loss_ctc 9.692723 loss_rnnt 4.304683 hw_loss 0.393274 lr 0.00038758 rank 7
2023-02-23 14:27:32,952 DEBUG TRAIN Batch 19/7900 loss 10.110621 loss_att 12.821663 loss_ctc 17.890427 loss_rnnt 8.345448 hw_loss 0.348111 lr 0.00038757 rank 3
2023-02-23 14:27:32,957 DEBUG TRAIN Batch 19/7900 loss 10.268697 loss_att 13.552978 loss_ctc 14.137828 loss_rnnt 8.857546 hw_loss 0.447020 lr 0.00038767 rank 4
2023-02-23 14:27:32,958 DEBUG TRAIN Batch 19/7900 loss 7.254197 loss_att 11.176697 loss_ctc 10.173985 loss_rnnt 5.870103 hw_loss 0.394290 lr 0.00038765 rank 6
2023-02-23 14:27:32,961 DEBUG TRAIN Batch 19/7900 loss 5.672751 loss_att 9.700013 loss_ctc 8.817022 loss_rnnt 4.223688 hw_loss 0.420704 lr 0.00038765 rank 5
2023-02-23 14:27:32,961 DEBUG TRAIN Batch 19/7900 loss 4.221238 loss_att 6.688706 loss_ctc 4.207227 loss_rnnt 3.512990 hw_loss 0.406166 lr 0.00038770 rank 1
2023-02-23 14:27:32,962 DEBUG TRAIN Batch 19/7900 loss 6.111299 loss_att 10.197080 loss_ctc 9.939486 loss_rnnt 4.507864 hw_loss 0.517227 lr 0.00038761 rank 2
2023-02-23 14:27:32,961 DEBUG TRAIN Batch 19/7900 loss 4.614017 loss_att 7.128920 loss_ctc 5.600055 loss_rnnt 3.773309 hw_loss 0.386728 lr 0.00038769 rank 0
2023-02-23 14:28:45,462 DEBUG TRAIN Batch 19/8000 loss 6.198980 loss_att 8.568544 loss_ctc 8.947395 loss_rnnt 5.124071 hw_loss 0.439765 lr 0.00038746 rank 7
2023-02-23 14:28:45,463 DEBUG TRAIN Batch 19/8000 loss 9.645558 loss_att 11.983757 loss_ctc 12.179699 loss_rnnt 8.641440 hw_loss 0.372362 lr 0.00038749 rank 2
2023-02-23 14:28:45,463 DEBUG TRAIN Batch 19/8000 loss 14.633936 loss_att 19.228559 loss_ctc 21.378984 loss_rnnt 12.601010 hw_loss 0.402488 lr 0.00038755 rank 4
2023-02-23 14:28:45,466 DEBUG TRAIN Batch 19/8000 loss 4.810806 loss_att 6.667779 loss_ctc 4.864874 loss_rnnt 4.237524 hw_loss 0.365023 lr 0.00038758 rank 1
2023-02-23 14:28:45,466 DEBUG TRAIN Batch 19/8000 loss 6.614378 loss_att 8.052242 loss_ctc 9.612552 loss_rnnt 5.720173 hw_loss 0.387891 lr 0.00038753 rank 6
2023-02-23 14:28:45,468 DEBUG TRAIN Batch 19/8000 loss 11.622358 loss_att 13.556334 loss_ctc 15.143141 loss_rnnt 10.568670 hw_loss 0.370228 lr 0.00038745 rank 3
2023-02-23 14:28:45,471 DEBUG TRAIN Batch 19/8000 loss 7.008504 loss_att 10.298127 loss_ctc 9.218034 loss_rnnt 5.861297 hw_loss 0.365023 lr 0.00038754 rank 5
2023-02-23 14:28:45,474 DEBUG TRAIN Batch 19/8000 loss 6.531168 loss_att 8.416729 loss_ctc 10.571345 loss_rnnt 5.407304 hw_loss 0.390117 lr 0.00038758 rank 0
2023-02-23 14:29:58,170 DEBUG TRAIN Batch 19/8100 loss 10.085618 loss_att 13.003152 loss_ctc 15.207174 loss_rnnt 8.573955 hw_loss 0.459906 lr 0.00038734 rank 7
2023-02-23 14:29:58,174 DEBUG TRAIN Batch 19/8100 loss 11.621694 loss_att 14.063726 loss_ctc 19.638351 loss_rnnt 9.824085 hw_loss 0.450590 lr 0.00038746 rank 1
2023-02-23 14:29:58,175 DEBUG TRAIN Batch 19/8100 loss 8.764616 loss_att 11.552335 loss_ctc 9.905581 loss_rnnt 7.856736 hw_loss 0.371637 lr 0.00038744 rank 4
2023-02-23 14:29:58,175 DEBUG TRAIN Batch 19/8100 loss 6.951469 loss_att 12.488401 loss_ctc 10.498528 loss_rnnt 5.142842 hw_loss 0.428060 lr 0.00038738 rank 2
2023-02-23 14:29:58,177 DEBUG TRAIN Batch 19/8100 loss 4.043983 loss_att 6.352647 loss_ctc 5.116000 loss_rnnt 3.235077 hw_loss 0.382945 lr 0.00038746 rank 0
2023-02-23 14:29:58,177 DEBUG TRAIN Batch 19/8100 loss 5.529522 loss_att 5.924500 loss_ctc 7.865559 loss_rnnt 4.892886 hw_loss 0.461567 lr 0.00038734 rank 3
2023-02-23 14:29:58,188 DEBUG TRAIN Batch 19/8100 loss 7.606930 loss_att 8.380410 loss_ctc 8.940626 loss_rnnt 7.034098 hw_loss 0.450583 lr 0.00038742 rank 6
2023-02-23 14:29:58,226 DEBUG TRAIN Batch 19/8100 loss 4.777051 loss_att 8.522498 loss_ctc 7.749166 loss_rnnt 3.389972 hw_loss 0.453202 lr 0.00038742 rank 5
2023-02-23 14:31:11,560 DEBUG TRAIN Batch 19/8200 loss 13.003457 loss_att 15.239399 loss_ctc 18.211451 loss_rnnt 11.636144 hw_loss 0.423235 lr 0.00038723 rank 7
2023-02-23 14:31:11,563 DEBUG TRAIN Batch 19/8200 loss 9.251965 loss_att 10.672524 loss_ctc 10.506790 loss_rnnt 8.473309 hw_loss 0.613564 lr 0.00038722 rank 3
2023-02-23 14:31:11,566 DEBUG TRAIN Batch 19/8200 loss 6.482272 loss_att 11.682417 loss_ctc 8.539740 loss_rnnt 4.960135 hw_loss 0.389584 lr 0.00038730 rank 6
2023-02-23 14:31:11,569 DEBUG TRAIN Batch 19/8200 loss 12.115214 loss_att 14.710747 loss_ctc 16.293991 loss_rnnt 10.820018 hw_loss 0.410472 lr 0.00038732 rank 4
2023-02-23 14:31:11,570 DEBUG TRAIN Batch 19/8200 loss 10.332386 loss_att 11.860052 loss_ctc 13.866293 loss_rnnt 9.358709 hw_loss 0.369292 lr 0.00038734 rank 0
2023-02-23 14:31:11,571 DEBUG TRAIN Batch 19/8200 loss 9.336900 loss_att 12.811042 loss_ctc 11.645182 loss_rnnt 8.135869 hw_loss 0.372056 lr 0.00038730 rank 5
2023-02-23 14:31:11,571 DEBUG TRAIN Batch 19/8200 loss 7.580608 loss_att 9.180049 loss_ctc 10.647927 loss_rnnt 6.587983 hw_loss 0.494551 lr 0.00038726 rank 2
2023-02-23 14:31:11,613 DEBUG TRAIN Batch 19/8200 loss 9.076012 loss_att 10.309857 loss_ctc 13.424666 loss_rnnt 8.017178 hw_loss 0.435458 lr 0.00038735 rank 1
2023-02-23 14:32:24,450 DEBUG TRAIN Batch 19/8300 loss 7.001416 loss_att 10.924401 loss_ctc 9.180341 loss_rnnt 5.659650 hw_loss 0.499959 lr 0.00038711 rank 7
2023-02-23 14:32:24,451 DEBUG TRAIN Batch 19/8300 loss 13.807163 loss_att 14.593076 loss_ctc 23.294931 loss_rnnt 12.158292 hw_loss 0.424973 lr 0.00038721 rank 4
2023-02-23 14:32:24,455 DEBUG TRAIN Batch 19/8300 loss 14.809338 loss_att 16.355558 loss_ctc 25.665489 loss_rnnt 12.824833 hw_loss 0.427075 lr 0.00038719 rank 5
2023-02-23 14:32:24,457 DEBUG TRAIN Batch 19/8300 loss 7.712430 loss_att 7.631649 loss_ctc 15.005079 loss_rnnt 6.502327 hw_loss 0.476076 lr 0.00038714 rank 2
2023-02-23 14:32:24,459 DEBUG TRAIN Batch 19/8300 loss 4.293926 loss_att 9.133493 loss_ctc 5.367547 loss_rnnt 2.966539 hw_loss 0.405606 lr 0.00038710 rank 3
2023-02-23 14:32:24,459 DEBUG TRAIN Batch 19/8300 loss 9.018726 loss_att 10.625146 loss_ctc 12.055346 loss_rnnt 8.109455 hw_loss 0.343322 lr 0.00038723 rank 1
2023-02-23 14:32:24,460 DEBUG TRAIN Batch 19/8300 loss 7.277680 loss_att 7.197013 loss_ctc 9.153034 loss_rnnt 6.843043 hw_loss 0.376356 lr 0.00038723 rank 0
2023-02-23 14:32:24,513 DEBUG TRAIN Batch 19/8300 loss 14.720603 loss_att 12.661502 loss_ctc 20.064167 loss_rnnt 14.165263 hw_loss 0.477537 lr 0.00038718 rank 6
2023-02-23 14:33:18,447 DEBUG CV Batch 19/0 loss 2.768748 loss_att 2.426616 loss_ctc 3.431653 loss_rnnt 2.349230 hw_loss 0.749171 history loss 2.666202 rank 0
2023-02-23 14:33:18,447 DEBUG CV Batch 19/0 loss 2.768748 loss_att 2.426616 loss_ctc 3.431653 loss_rnnt 2.349230 hw_loss 0.749171 history loss 2.666202 rank 2
2023-02-23 14:33:18,448 DEBUG CV Batch 19/0 loss 2.768748 loss_att 2.426616 loss_ctc 3.431653 loss_rnnt 2.349230 hw_loss 0.749171 history loss 2.666202 rank 5
2023-02-23 14:33:18,457 DEBUG CV Batch 19/0 loss 2.768748 loss_att 2.426616 loss_ctc 3.431653 loss_rnnt 2.349230 hw_loss 0.749171 history loss 2.666202 rank 6
2023-02-23 14:33:18,462 DEBUG CV Batch 19/0 loss 2.768748 loss_att 2.426616 loss_ctc 3.431653 loss_rnnt 2.349230 hw_loss 0.749171 history loss 2.666202 rank 3
2023-02-23 14:33:18,465 DEBUG CV Batch 19/0 loss 2.768748 loss_att 2.426616 loss_ctc 3.431653 loss_rnnt 2.349230 hw_loss 0.749171 history loss 2.666202 rank 7
2023-02-23 14:33:18,466 DEBUG CV Batch 19/0 loss 2.768748 loss_att 2.426616 loss_ctc 3.431653 loss_rnnt 2.349230 hw_loss 0.749171 history loss 2.666202 rank 1
2023-02-23 14:33:18,478 DEBUG CV Batch 19/0 loss 2.768748 loss_att 2.426616 loss_ctc 3.431653 loss_rnnt 2.349230 hw_loss 0.749171 history loss 2.666202 rank 4
2023-02-23 14:33:29,896 DEBUG CV Batch 19/100 loss 7.781132 loss_att 8.255128 loss_ctc 10.452032 loss_rnnt 7.085182 hw_loss 0.459432 history loss 3.752326 rank 6
2023-02-23 14:33:29,900 DEBUG CV Batch 19/100 loss 7.781132 loss_att 8.255128 loss_ctc 10.452032 loss_rnnt 7.085182 hw_loss 0.459432 history loss 3.752326 rank 5
2023-02-23 14:33:30,042 DEBUG CV Batch 19/100 loss 7.781132 loss_att 8.255128 loss_ctc 10.452032 loss_rnnt 7.085182 hw_loss 0.459432 history loss 3.752326 rank 4
2023-02-23 14:33:30,112 DEBUG CV Batch 19/100 loss 7.781132 loss_att 8.255128 loss_ctc 10.452032 loss_rnnt 7.085182 hw_loss 0.459432 history loss 3.752326 rank 7
2023-02-23 14:33:30,197 DEBUG CV Batch 19/100 loss 7.781132 loss_att 8.255128 loss_ctc 10.452032 loss_rnnt 7.085182 hw_loss 0.459432 history loss 3.752326 rank 3
2023-02-23 14:33:30,272 DEBUG CV Batch 19/100 loss 7.781132 loss_att 8.255128 loss_ctc 10.452032 loss_rnnt 7.085182 hw_loss 0.459432 history loss 3.752326 rank 2
2023-02-23 14:33:30,319 DEBUG CV Batch 19/100 loss 7.781132 loss_att 8.255128 loss_ctc 10.452032 loss_rnnt 7.085182 hw_loss 0.459432 history loss 3.752326 rank 1
2023-02-23 14:33:30,323 DEBUG CV Batch 19/100 loss 7.781132 loss_att 8.255128 loss_ctc 10.452032 loss_rnnt 7.085182 hw_loss 0.459432 history loss 3.752326 rank 0
2023-02-23 14:33:43,835 DEBUG CV Batch 19/200 loss 10.080763 loss_att 19.170303 loss_ctc 8.878555 loss_rnnt 8.290332 hw_loss 0.249032 history loss 4.386325 rank 7
2023-02-23 14:33:43,913 DEBUG CV Batch 19/200 loss 10.080763 loss_att 19.170303 loss_ctc 8.878555 loss_rnnt 8.290332 hw_loss 0.249032 history loss 4.386325 rank 1
2023-02-23 14:33:43,913 DEBUG CV Batch 19/200 loss 10.080763 loss_att 19.170303 loss_ctc 8.878555 loss_rnnt 8.290332 hw_loss 0.249032 history loss 4.386325 rank 3
2023-02-23 14:33:43,975 DEBUG CV Batch 19/200 loss 10.080763 loss_att 19.170303 loss_ctc 8.878555 loss_rnnt 8.290332 hw_loss 0.249032 history loss 4.386325 rank 5
2023-02-23 14:33:44,008 DEBUG CV Batch 19/200 loss 10.080763 loss_att 19.170303 loss_ctc 8.878555 loss_rnnt 8.290332 hw_loss 0.249032 history loss 4.386325 rank 6
2023-02-23 14:33:44,032 DEBUG CV Batch 19/200 loss 10.080763 loss_att 19.170303 loss_ctc 8.878555 loss_rnnt 8.290332 hw_loss 0.249032 history loss 4.386325 rank 4
2023-02-23 14:33:44,081 DEBUG CV Batch 19/200 loss 10.080763 loss_att 19.170303 loss_ctc 8.878555 loss_rnnt 8.290332 hw_loss 0.249032 history loss 4.386325 rank 2
2023-02-23 14:33:44,104 DEBUG CV Batch 19/200 loss 10.080763 loss_att 19.170303 loss_ctc 8.878555 loss_rnnt 8.290332 hw_loss 0.249032 history loss 4.386325 rank 0
2023-02-23 14:33:55,983 DEBUG CV Batch 19/300 loss 6.110309 loss_att 6.228318 loss_ctc 8.767855 loss_rnnt 5.449428 hw_loss 0.530514 history loss 4.521933 rank 1
2023-02-23 14:33:56,079 DEBUG CV Batch 19/300 loss 6.110309 loss_att 6.228318 loss_ctc 8.767855 loss_rnnt 5.449428 hw_loss 0.530514 history loss 4.521933 rank 7
2023-02-23 14:33:56,094 DEBUG CV Batch 19/300 loss 6.110309 loss_att 6.228318 loss_ctc 8.767855 loss_rnnt 5.449428 hw_loss 0.530514 history loss 4.521933 rank 4
2023-02-23 14:33:56,241 DEBUG CV Batch 19/300 loss 6.110309 loss_att 6.228318 loss_ctc 8.767855 loss_rnnt 5.449428 hw_loss 0.530514 history loss 4.521933 rank 3
2023-02-23 14:33:56,355 DEBUG CV Batch 19/300 loss 6.110309 loss_att 6.228318 loss_ctc 8.767855 loss_rnnt 5.449428 hw_loss 0.530514 history loss 4.521933 rank 6
2023-02-23 14:33:56,410 DEBUG CV Batch 19/300 loss 6.110309 loss_att 6.228318 loss_ctc 8.767855 loss_rnnt 5.449428 hw_loss 0.530514 history loss 4.521933 rank 0
2023-02-23 14:33:56,452 DEBUG CV Batch 19/300 loss 6.110309 loss_att 6.228318 loss_ctc 8.767855 loss_rnnt 5.449428 hw_loss 0.530514 history loss 4.521933 rank 2
2023-02-23 14:33:56,689 DEBUG CV Batch 19/300 loss 6.110309 loss_att 6.228318 loss_ctc 8.767855 loss_rnnt 5.449428 hw_loss 0.530514 history loss 4.521933 rank 5
2023-02-23 14:34:08,088 DEBUG CV Batch 19/400 loss 15.171548 loss_att 73.495766 loss_ctc 7.001094 loss_rnnt 4.451498 hw_loss 0.271125 history loss 5.533741 rank 1
2023-02-23 14:34:08,279 DEBUG CV Batch 19/400 loss 15.171548 loss_att 73.495766 loss_ctc 7.001094 loss_rnnt 4.451498 hw_loss 0.271125 history loss 5.533741 rank 4
2023-02-23 14:34:08,287 DEBUG CV Batch 19/400 loss 15.171548 loss_att 73.495766 loss_ctc 7.001094 loss_rnnt 4.451498 hw_loss 0.271125 history loss 5.533741 rank 6
2023-02-23 14:34:08,365 DEBUG CV Batch 19/400 loss 15.171548 loss_att 73.495766 loss_ctc 7.001094 loss_rnnt 4.451498 hw_loss 0.271125 history loss 5.533741 rank 7
2023-02-23 14:34:08,633 DEBUG CV Batch 19/400 loss 15.171548 loss_att 73.495766 loss_ctc 7.001094 loss_rnnt 4.451498 hw_loss 0.271125 history loss 5.533741 rank 3
2023-02-23 14:34:08,742 DEBUG CV Batch 19/400 loss 15.171548 loss_att 73.495766 loss_ctc 7.001094 loss_rnnt 4.451498 hw_loss 0.271125 history loss 5.533741 rank 5
2023-02-23 14:34:08,921 DEBUG CV Batch 19/400 loss 15.171548 loss_att 73.495766 loss_ctc 7.001094 loss_rnnt 4.451498 hw_loss 0.271125 history loss 5.533741 rank 2
2023-02-23 14:34:09,435 DEBUG CV Batch 19/400 loss 15.171548 loss_att 73.495766 loss_ctc 7.001094 loss_rnnt 4.451498 hw_loss 0.271125 history loss 5.533741 rank 0
2023-02-23 14:34:18,610 DEBUG CV Batch 19/500 loss 5.017046 loss_att 5.285960 loss_ctc 6.795414 loss_rnnt 4.503808 hw_loss 0.416886 history loss 6.339353 rank 1
2023-02-23 14:34:18,681 DEBUG CV Batch 19/500 loss 5.017046 loss_att 5.285960 loss_ctc 6.795414 loss_rnnt 4.503808 hw_loss 0.416886 history loss 6.339353 rank 4
2023-02-23 14:34:18,847 DEBUG CV Batch 19/500 loss 5.017046 loss_att 5.285960 loss_ctc 6.795414 loss_rnnt 4.503808 hw_loss 0.416886 history loss 6.339353 rank 6
2023-02-23 14:34:19,108 DEBUG CV Batch 19/500 loss 5.017046 loss_att 5.285960 loss_ctc 6.795414 loss_rnnt 4.503808 hw_loss 0.416886 history loss 6.339353 rank 7
2023-02-23 14:34:19,156 DEBUG CV Batch 19/500 loss 5.017046 loss_att 5.285960 loss_ctc 6.795414 loss_rnnt 4.503808 hw_loss 0.416886 history loss 6.339353 rank 5
2023-02-23 14:34:19,466 DEBUG CV Batch 19/500 loss 5.017046 loss_att 5.285960 loss_ctc 6.795414 loss_rnnt 4.503808 hw_loss 0.416886 history loss 6.339353 rank 3
2023-02-23 14:34:19,737 DEBUG CV Batch 19/500 loss 5.017046 loss_att 5.285960 loss_ctc 6.795414 loss_rnnt 4.503808 hw_loss 0.416886 history loss 6.339353 rank 2
2023-02-23 14:34:20,301 DEBUG CV Batch 19/500 loss 5.017046 loss_att 5.285960 loss_ctc 6.795414 loss_rnnt 4.503808 hw_loss 0.416886 history loss 6.339353 rank 0
2023-02-23 14:34:30,710 DEBUG CV Batch 19/600 loss 6.876597 loss_att 7.222331 loss_ctc 8.876453 loss_rnnt 6.142454 hw_loss 0.746904 history loss 7.362455 rank 1
2023-02-23 14:34:30,981 DEBUG CV Batch 19/600 loss 6.876597 loss_att 7.222331 loss_ctc 8.876453 loss_rnnt 6.142454 hw_loss 0.746904 history loss 7.362455 rank 4
2023-02-23 14:34:31,212 DEBUG CV Batch 19/600 loss 6.876597 loss_att 7.222331 loss_ctc 8.876453 loss_rnnt 6.142454 hw_loss 0.746904 history loss 7.362455 rank 6
2023-02-23 14:34:31,419 DEBUG CV Batch 19/600 loss 6.876597 loss_att 7.222331 loss_ctc 8.876453 loss_rnnt 6.142454 hw_loss 0.746904 history loss 7.362455 rank 7
2023-02-23 14:34:31,605 DEBUG CV Batch 19/600 loss 6.876597 loss_att 7.222331 loss_ctc 8.876453 loss_rnnt 6.142454 hw_loss 0.746904 history loss 7.362455 rank 5
2023-02-23 14:34:31,801 DEBUG CV Batch 19/600 loss 6.876597 loss_att 7.222331 loss_ctc 8.876453 loss_rnnt 6.142454 hw_loss 0.746904 history loss 7.362455 rank 3
2023-02-23 14:34:32,157 DEBUG CV Batch 19/600 loss 6.876597 loss_att 7.222331 loss_ctc 8.876453 loss_rnnt 6.142454 hw_loss 0.746904 history loss 7.362455 rank 2
2023-02-23 14:34:32,672 DEBUG CV Batch 19/600 loss 6.876597 loss_att 7.222331 loss_ctc 8.876453 loss_rnnt 6.142454 hw_loss 0.746904 history loss 7.362455 rank 0
2023-02-23 14:34:41,958 DEBUG CV Batch 19/700 loss 21.460352 loss_att 54.190392 loss_ctc 19.588167 loss_rnnt 14.939488 hw_loss 0.420899 history loss 8.029606 rank 1
2023-02-23 14:34:42,642 DEBUG CV Batch 19/700 loss 21.460352 loss_att 54.190392 loss_ctc 19.588167 loss_rnnt 14.939488 hw_loss 0.420899 history loss 8.029606 rank 4
2023-02-23 14:34:43,100 DEBUG CV Batch 19/700 loss 21.460352 loss_att 54.190392 loss_ctc 19.588167 loss_rnnt 14.939488 hw_loss 0.420899 history loss 8.029606 rank 7
2023-02-23 14:34:43,153 DEBUG CV Batch 19/700 loss 21.460352 loss_att 54.190392 loss_ctc 19.588167 loss_rnnt 14.939488 hw_loss 0.420899 history loss 8.029606 rank 5
2023-02-23 14:34:43,454 DEBUG CV Batch 19/700 loss 21.460352 loss_att 54.190392 loss_ctc 19.588167 loss_rnnt 14.939488 hw_loss 0.420899 history loss 8.029606 rank 3
2023-02-23 14:34:43,512 DEBUG CV Batch 19/700 loss 21.460352 loss_att 54.190392 loss_ctc 19.588167 loss_rnnt 14.939488 hw_loss 0.420899 history loss 8.029606 rank 6
2023-02-23 14:34:43,925 DEBUG CV Batch 19/700 loss 21.460352 loss_att 54.190392 loss_ctc 19.588167 loss_rnnt 14.939488 hw_loss 0.420899 history loss 8.029606 rank 2
2023-02-23 14:34:44,731 DEBUG CV Batch 19/700 loss 21.460352 loss_att 54.190392 loss_ctc 19.588167 loss_rnnt 14.939488 hw_loss 0.420899 history loss 8.029606 rank 0
2023-02-23 14:34:53,250 DEBUG CV Batch 19/800 loss 12.438335 loss_att 11.163159 loss_ctc 17.421215 loss_rnnt 11.772457 hw_loss 0.480993 history loss 7.453274 rank 1
2023-02-23 14:34:54,537 DEBUG CV Batch 19/800 loss 12.438335 loss_att 11.163159 loss_ctc 17.421215 loss_rnnt 11.772457 hw_loss 0.480993 history loss 7.453274 rank 7
2023-02-23 14:34:54,787 DEBUG CV Batch 19/800 loss 12.438335 loss_att 11.163159 loss_ctc 17.421215 loss_rnnt 11.772457 hw_loss 0.480993 history loss 7.453274 rank 4
2023-02-23 14:34:54,861 DEBUG CV Batch 19/800 loss 12.438335 loss_att 11.163159 loss_ctc 17.421215 loss_rnnt 11.772457 hw_loss 0.480993 history loss 7.453274 rank 3
2023-02-23 14:34:55,511 DEBUG CV Batch 19/800 loss 12.438335 loss_att 11.163159 loss_ctc 17.421215 loss_rnnt 11.772457 hw_loss 0.480993 history loss 7.453274 rank 5
2023-02-23 14:34:55,522 DEBUG CV Batch 19/800 loss 12.438335 loss_att 11.163159 loss_ctc 17.421215 loss_rnnt 11.772457 hw_loss 0.480993 history loss 7.453274 rank 2
2023-02-23 14:34:55,630 DEBUG CV Batch 19/800 loss 12.438335 loss_att 11.163159 loss_ctc 17.421215 loss_rnnt 11.772457 hw_loss 0.480993 history loss 7.453274 rank 6
2023-02-23 14:34:56,140 DEBUG CV Batch 19/800 loss 12.438335 loss_att 11.163159 loss_ctc 17.421215 loss_rnnt 11.772457 hw_loss 0.480993 history loss 7.453274 rank 0
2023-02-23 14:35:06,627 DEBUG CV Batch 19/900 loss 14.059122 loss_att 18.321564 loss_ctc 23.655312 loss_rnnt 11.768092 hw_loss 0.298219 history loss 7.241138 rank 1
2023-02-23 14:35:08,212 DEBUG CV Batch 19/900 loss 14.059122 loss_att 18.321564 loss_ctc 23.655312 loss_rnnt 11.768092 hw_loss 0.298219 history loss 7.241138 rank 7
2023-02-23 14:35:08,649 DEBUG CV Batch 19/900 loss 14.059122 loss_att 18.321564 loss_ctc 23.655312 loss_rnnt 11.768092 hw_loss 0.298219 history loss 7.241138 rank 3
2023-02-23 14:35:08,764 DEBUG CV Batch 19/900 loss 14.059122 loss_att 18.321564 loss_ctc 23.655312 loss_rnnt 11.768092 hw_loss 0.298219 history loss 7.241138 rank 4
2023-02-23 14:35:09,438 DEBUG CV Batch 19/900 loss 14.059122 loss_att 18.321564 loss_ctc 23.655312 loss_rnnt 11.768092 hw_loss 0.298219 history loss 7.241138 rank 6
2023-02-23 14:35:09,484 DEBUG CV Batch 19/900 loss 14.059122 loss_att 18.321564 loss_ctc 23.655312 loss_rnnt 11.768092 hw_loss 0.298219 history loss 7.241138 rank 2
2023-02-23 14:35:09,651 DEBUG CV Batch 19/900 loss 14.059122 loss_att 18.321564 loss_ctc 23.655312 loss_rnnt 11.768092 hw_loss 0.298219 history loss 7.241138 rank 5
2023-02-23 14:35:10,055 DEBUG CV Batch 19/900 loss 14.059122 loss_att 18.321564 loss_ctc 23.655312 loss_rnnt 11.768092 hw_loss 0.298219 history loss 7.241138 rank 0
2023-02-23 14:35:18,696 DEBUG CV Batch 19/1000 loss 5.322733 loss_att 5.577085 loss_ctc 6.966363 loss_rnnt 4.740542 hw_loss 0.585317 history loss 6.998036 rank 1
2023-02-23 14:35:20,882 DEBUG CV Batch 19/1000 loss 5.322733 loss_att 5.577085 loss_ctc 6.966363 loss_rnnt 4.740542 hw_loss 0.585317 history loss 6.998036 rank 7
2023-02-23 14:35:20,957 DEBUG CV Batch 19/1000 loss 5.322733 loss_att 5.577085 loss_ctc 6.966363 loss_rnnt 4.740542 hw_loss 0.585317 history loss 6.998036 rank 4
2023-02-23 14:35:21,320 DEBUG CV Batch 19/1000 loss 5.322733 loss_att 5.577085 loss_ctc 6.966363 loss_rnnt 4.740542 hw_loss 0.585317 history loss 6.998036 rank 3
2023-02-23 14:35:21,910 DEBUG CV Batch 19/1000 loss 5.322733 loss_att 5.577085 loss_ctc 6.966363 loss_rnnt 4.740542 hw_loss 0.585317 history loss 6.998036 rank 5
2023-02-23 14:35:22,169 DEBUG CV Batch 19/1000 loss 5.322733 loss_att 5.577085 loss_ctc 6.966363 loss_rnnt 4.740542 hw_loss 0.585317 history loss 6.998036 rank 6
2023-02-23 14:35:22,405 DEBUG CV Batch 19/1000 loss 5.322733 loss_att 5.577085 loss_ctc 6.966363 loss_rnnt 4.740542 hw_loss 0.585317 history loss 6.998036 rank 2
2023-02-23 14:35:22,938 DEBUG CV Batch 19/1000 loss 5.322733 loss_att 5.577085 loss_ctc 6.966363 loss_rnnt 4.740542 hw_loss 0.585317 history loss 6.998036 rank 0
2023-02-23 14:35:30,752 DEBUG CV Batch 19/1100 loss 7.253979 loss_att 6.216333 loss_ctc 8.937675 loss_rnnt 6.812474 hw_loss 0.796014 history loss 6.986639 rank 1
2023-02-23 14:35:32,832 DEBUG CV Batch 19/1100 loss 7.253979 loss_att 6.216333 loss_ctc 8.937675 loss_rnnt 6.812474 hw_loss 0.796014 history loss 6.986639 rank 4
2023-02-23 14:35:33,152 DEBUG CV Batch 19/1100 loss 7.253979 loss_att 6.216333 loss_ctc 8.937675 loss_rnnt 6.812474 hw_loss 0.796014 history loss 6.986639 rank 7
2023-02-23 14:35:33,710 DEBUG CV Batch 19/1100 loss 7.253979 loss_att 6.216333 loss_ctc 8.937675 loss_rnnt 6.812474 hw_loss 0.796014 history loss 6.986639 rank 3
2023-02-23 14:35:33,922 DEBUG CV Batch 19/1100 loss 7.253979 loss_att 6.216333 loss_ctc 8.937675 loss_rnnt 6.812474 hw_loss 0.796014 history loss 6.986639 rank 5
2023-02-23 14:35:34,479 DEBUG CV Batch 19/1100 loss 7.253979 loss_att 6.216333 loss_ctc 8.937675 loss_rnnt 6.812474 hw_loss 0.796014 history loss 6.986639 rank 6
2023-02-23 14:35:34,927 DEBUG CV Batch 19/1100 loss 7.253979 loss_att 6.216333 loss_ctc 8.937675 loss_rnnt 6.812474 hw_loss 0.796014 history loss 6.986639 rank 2
2023-02-23 14:35:35,261 DEBUG CV Batch 19/1100 loss 7.253979 loss_att 6.216333 loss_ctc 8.937675 loss_rnnt 6.812474 hw_loss 0.796014 history loss 6.986639 rank 0
2023-02-23 14:35:41,036 DEBUG CV Batch 19/1200 loss 7.430489 loss_att 7.526305 loss_ctc 8.362590 loss_rnnt 7.031180 hw_loss 0.479747 history loss 7.320255 rank 1
2023-02-23 14:35:43,091 DEBUG CV Batch 19/1200 loss 7.430489 loss_att 7.526305 loss_ctc 8.362590 loss_rnnt 7.031180 hw_loss 0.479747 history loss 7.320255 rank 4
2023-02-23 14:35:44,219 DEBUG CV Batch 19/1200 loss 7.430489 loss_att 7.526305 loss_ctc 8.362590 loss_rnnt 7.031180 hw_loss 0.479747 history loss 7.320255 rank 7
2023-02-23 14:35:44,560 DEBUG CV Batch 19/1200 loss 7.430489 loss_att 7.526305 loss_ctc 8.362590 loss_rnnt 7.031180 hw_loss 0.479747 history loss 7.320255 rank 5
2023-02-23 14:35:44,757 DEBUG CV Batch 19/1200 loss 7.430489 loss_att 7.526305 loss_ctc 8.362590 loss_rnnt 7.031180 hw_loss 0.479747 history loss 7.320255 rank 3
2023-02-23 14:35:45,043 DEBUG CV Batch 19/1200 loss 7.430489 loss_att 7.526305 loss_ctc 8.362590 loss_rnnt 7.031180 hw_loss 0.479747 history loss 7.320255 rank 6
2023-02-23 14:35:46,112 DEBUG CV Batch 19/1200 loss 7.430489 loss_att 7.526305 loss_ctc 8.362590 loss_rnnt 7.031180 hw_loss 0.479747 history loss 7.320255 rank 2
2023-02-23 14:35:46,866 DEBUG CV Batch 19/1200 loss 7.430489 loss_att 7.526305 loss_ctc 8.362590 loss_rnnt 7.031180 hw_loss 0.479747 history loss 7.320255 rank 0
2023-02-23 14:35:53,449 DEBUG CV Batch 19/1300 loss 5.941199 loss_att 6.006272 loss_ctc 8.546375 loss_rnnt 5.274448 hw_loss 0.574462 history loss 7.668155 rank 1
2023-02-23 14:35:55,047 DEBUG CV Batch 19/1300 loss 5.941199 loss_att 6.006272 loss_ctc 8.546375 loss_rnnt 5.274448 hw_loss 0.574462 history loss 7.668155 rank 4
2023-02-23 14:35:56,476 DEBUG CV Batch 19/1300 loss 5.941199 loss_att 6.006272 loss_ctc 8.546375 loss_rnnt 5.274448 hw_loss 0.574462 history loss 7.668155 rank 7
2023-02-23 14:35:56,913 DEBUG CV Batch 19/1300 loss 5.941199 loss_att 6.006272 loss_ctc 8.546375 loss_rnnt 5.274448 hw_loss 0.574462 history loss 7.668155 rank 5
2023-02-23 14:35:57,140 DEBUG CV Batch 19/1300 loss 5.941199 loss_att 6.006272 loss_ctc 8.546375 loss_rnnt 5.274448 hw_loss 0.574462 history loss 7.668155 rank 3
2023-02-23 14:35:57,213 DEBUG CV Batch 19/1300 loss 5.941199 loss_att 6.006272 loss_ctc 8.546375 loss_rnnt 5.274448 hw_loss 0.574462 history loss 7.668155 rank 6
2023-02-23 14:35:58,651 DEBUG CV Batch 19/1300 loss 5.941199 loss_att 6.006272 loss_ctc 8.546375 loss_rnnt 5.274448 hw_loss 0.574462 history loss 7.668155 rank 2
2023-02-23 14:35:59,411 DEBUG CV Batch 19/1300 loss 5.941199 loss_att 6.006272 loss_ctc 8.546375 loss_rnnt 5.274448 hw_loss 0.574462 history loss 7.668155 rank 0
2023-02-23 14:36:04,569 DEBUG CV Batch 19/1400 loss 4.924033 loss_att 16.301786 loss_ctc 3.609902 loss_rnnt 2.629065 hw_loss 0.364939 history loss 7.989318 rank 1
2023-02-23 14:36:06,846 DEBUG CV Batch 19/1400 loss 4.924033 loss_att 16.301786 loss_ctc 3.609902 loss_rnnt 2.629065 hw_loss 0.364939 history loss 7.989318 rank 4
2023-02-23 14:36:08,068 DEBUG CV Batch 19/1400 loss 4.924033 loss_att 16.301786 loss_ctc 3.609902 loss_rnnt 2.629065 hw_loss 0.364939 history loss 7.989318 rank 7
2023-02-23 14:36:08,771 DEBUG CV Batch 19/1400 loss 4.924033 loss_att 16.301786 loss_ctc 3.609902 loss_rnnt 2.629065 hw_loss 0.364939 history loss 7.989318 rank 3
2023-02-23 14:36:08,772 DEBUG CV Batch 19/1400 loss 4.924033 loss_att 16.301786 loss_ctc 3.609902 loss_rnnt 2.629065 hw_loss 0.364939 history loss 7.989318 rank 5
2023-02-23 14:36:09,297 DEBUG CV Batch 19/1400 loss 4.924033 loss_att 16.301786 loss_ctc 3.609902 loss_rnnt 2.629065 hw_loss 0.364939 history loss 7.989318 rank 6
2023-02-23 14:36:10,490 DEBUG CV Batch 19/1400 loss 4.924033 loss_att 16.301786 loss_ctc 3.609902 loss_rnnt 2.629065 hw_loss 0.364939 history loss 7.989318 rank 2
2023-02-23 14:36:11,102 DEBUG CV Batch 19/1400 loss 4.924033 loss_att 16.301786 loss_ctc 3.609902 loss_rnnt 2.629065 hw_loss 0.364939 history loss 7.989318 rank 0
2023-02-23 14:36:16,140 DEBUG CV Batch 19/1500 loss 6.357050 loss_att 6.982946 loss_ctc 6.677829 loss_rnnt 5.969333 hw_loss 0.412065 history loss 7.812522 rank 1
2023-02-23 14:36:19,618 DEBUG CV Batch 19/1500 loss 6.357050 loss_att 6.982946 loss_ctc 6.677829 loss_rnnt 5.969333 hw_loss 0.412065 history loss 7.812522 rank 4
2023-02-23 14:36:19,932 DEBUG CV Batch 19/1500 loss 6.357050 loss_att 6.982946 loss_ctc 6.677829 loss_rnnt 5.969333 hw_loss 0.412065 history loss 7.812522 rank 7
2023-02-23 14:36:20,772 DEBUG CV Batch 19/1500 loss 6.357050 loss_att 6.982946 loss_ctc 6.677829 loss_rnnt 5.969333 hw_loss 0.412065 history loss 7.812522 rank 3
2023-02-23 14:36:21,734 DEBUG CV Batch 19/1500 loss 6.357050 loss_att 6.982946 loss_ctc 6.677829 loss_rnnt 5.969333 hw_loss 0.412065 history loss 7.812522 rank 6
2023-02-23 14:36:21,792 DEBUG CV Batch 19/1500 loss 6.357050 loss_att 6.982946 loss_ctc 6.677829 loss_rnnt 5.969333 hw_loss 0.412065 history loss 7.812522 rank 5
2023-02-23 14:36:22,495 DEBUG CV Batch 19/1500 loss 6.357050 loss_att 6.982946 loss_ctc 6.677829 loss_rnnt 5.969333 hw_loss 0.412065 history loss 7.812522 rank 2
2023-02-23 14:36:23,101 DEBUG CV Batch 19/1500 loss 6.357050 loss_att 6.982946 loss_ctc 6.677829 loss_rnnt 5.969333 hw_loss 0.412065 history loss 7.812522 rank 0
2023-02-23 14:36:29,438 DEBUG CV Batch 19/1600 loss 13.537643 loss_att 15.224298 loss_ctc 12.746037 loss_rnnt 13.127522 hw_loss 0.334382 history loss 7.731058 rank 1
2023-02-23 14:36:33,263 DEBUG CV Batch 19/1600 loss 13.537643 loss_att 15.224298 loss_ctc 12.746037 loss_rnnt 13.127522 hw_loss 0.334382 history loss 7.731058 rank 4
2023-02-23 14:36:33,381 DEBUG CV Batch 19/1600 loss 13.537643 loss_att 15.224298 loss_ctc 12.746037 loss_rnnt 13.127522 hw_loss 0.334382 history loss 7.731058 rank 7
2023-02-23 14:36:34,165 DEBUG CV Batch 19/1600 loss 13.537643 loss_att 15.224298 loss_ctc 12.746037 loss_rnnt 13.127522 hw_loss 0.334382 history loss 7.731058 rank 3
2023-02-23 14:36:35,338 DEBUG CV Batch 19/1600 loss 13.537643 loss_att 15.224298 loss_ctc 12.746037 loss_rnnt 13.127522 hw_loss 0.334382 history loss 7.731058 rank 6
2023-02-23 14:36:35,534 DEBUG CV Batch 19/1600 loss 13.537643 loss_att 15.224298 loss_ctc 12.746037 loss_rnnt 13.127522 hw_loss 0.334382 history loss 7.731058 rank 5
2023-02-23 14:36:36,033 DEBUG CV Batch 19/1600 loss 13.537643 loss_att 15.224298 loss_ctc 12.746037 loss_rnnt 13.127522 hw_loss 0.334382 history loss 7.731058 rank 2
2023-02-23 14:36:36,888 DEBUG CV Batch 19/1600 loss 13.537643 loss_att 15.224298 loss_ctc 12.746037 loss_rnnt 13.127522 hw_loss 0.334382 history loss 7.731058 rank 0
2023-02-23 14:36:42,147 DEBUG CV Batch 19/1700 loss 10.099783 loss_att 8.365310 loss_ctc 14.538204 loss_rnnt 9.558686 hw_loss 0.555376 history loss 7.643859 rank 1
2023-02-23 14:36:45,626 DEBUG CV Batch 19/1700 loss 10.099783 loss_att 8.365310 loss_ctc 14.538204 loss_rnnt 9.558686 hw_loss 0.555376 history loss 7.643859 rank 4
2023-02-23 14:36:46,187 DEBUG CV Batch 19/1700 loss 10.099783 loss_att 8.365310 loss_ctc 14.538204 loss_rnnt 9.558686 hw_loss 0.555376 history loss 7.643859 rank 7
2023-02-23 14:36:46,846 DEBUG CV Batch 19/1700 loss 10.099783 loss_att 8.365310 loss_ctc 14.538204 loss_rnnt 9.558686 hw_loss 0.555376 history loss 7.643859 rank 3
2023-02-23 14:36:47,974 DEBUG CV Batch 19/1700 loss 10.099783 loss_att 8.365310 loss_ctc 14.538204 loss_rnnt 9.558686 hw_loss 0.555376 history loss 7.643859 rank 6
2023-02-23 14:36:48,156 DEBUG CV Batch 19/1700 loss 10.099783 loss_att 8.365310 loss_ctc 14.538204 loss_rnnt 9.558686 hw_loss 0.555376 history loss 7.643859 rank 5
2023-02-23 14:36:48,835 DEBUG CV Batch 19/1700 loss 10.099783 loss_att 8.365310 loss_ctc 14.538204 loss_rnnt 9.558686 hw_loss 0.555376 history loss 7.643859 rank 2
2023-02-23 14:36:50,054 DEBUG CV Batch 19/1700 loss 10.099783 loss_att 8.365310 loss_ctc 14.538204 loss_rnnt 9.558686 hw_loss 0.555376 history loss 7.643859 rank 0
2023-02-23 14:36:51,780 INFO Epoch 19 CV info cv_loss 7.604554451348413
2023-02-23 14:36:51,781 INFO Epoch 20 TRAIN info lr 0.00038718181043005445
2023-02-23 14:36:51,787 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 14:36:54,975 INFO Epoch 19 CV info cv_loss 7.604554448389287
2023-02-23 14:36:54,975 INFO Epoch 20 TRAIN info lr 0.00038716904174910236
2023-02-23 14:36:54,980 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 14:36:55,362 INFO Epoch 19 CV info cv_loss 7.604554449087072
2023-02-23 14:36:55,362 INFO Epoch 20 TRAIN info lr 0.0003870622985876495
2023-02-23 14:36:55,364 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 14:36:55,958 INFO Epoch 19 CV info cv_loss 7.604554450030374
2023-02-23 14:36:55,958 INFO Epoch 20 TRAIN info lr 0.00038704954172662763
2023-02-23 14:36:55,960 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 14:36:57,365 INFO Epoch 19 CV info cv_loss 7.604554449858082
2023-02-23 14:36:57,366 INFO Epoch 20 TRAIN info lr 0.00038710405713547494
2023-02-23 14:36:57,370 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 14:36:57,548 INFO Epoch 19 CV info cv_loss 7.604554448066238
2023-02-23 14:36:57,549 INFO Epoch 20 TRAIN info lr 0.0003871562743313435
2023-02-23 14:36:57,553 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 14:36:58,203 INFO Epoch 19 CV info cv_loss 7.604554450340501
2023-02-23 14:36:58,203 INFO Epoch 20 TRAIN info lr 0.00038711449888439444
2023-02-23 14:36:58,208 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 14:36:59,566 INFO Epoch 19 CV info cv_loss 7.604554450013145
2023-02-23 14:36:59,566 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_both_more_layer_3word_finetune/19.pt
2023-02-23 14:37:00,147 INFO Epoch 20 TRAIN info lr 0.0003872061905112445
2023-02-23 14:37:00,150 INFO using accumulate grad, new batch size is 1 times larger than before
2023-02-23 14:37:59,106 DEBUG TRAIN Batch 20/0 loss 7.404083 loss_att 7.287056 loss_ctc 8.825946 loss_rnnt 6.912241 hw_loss 0.610624 lr 0.00038706 rank 7
2023-02-23 14:37:59,107 DEBUG TRAIN Batch 20/0 loss 9.290254 loss_att 8.702405 loss_ctc 11.471084 loss_rnnt 8.792650 hw_loss 0.608241 lr 0.00038717 rank 4
2023-02-23 14:37:59,113 DEBUG TRAIN Batch 20/0 loss 11.366880 loss_att 10.782229 loss_ctc 14.313771 loss_rnnt 10.666351 hw_loss 0.796014 lr 0.00038718 rank 1
2023-02-23 14:37:59,118 DEBUG TRAIN Batch 20/0 loss 10.928317 loss_att 10.415582 loss_ctc 13.126423 loss_rnnt 10.359078 hw_loss 0.710072 lr 0.00038705 rank 3
2023-02-23 14:37:59,149 DEBUG TRAIN Batch 20/0 loss 7.632682 loss_att 6.597241 loss_ctc 8.596504 loss_rnnt 7.273624 hw_loss 0.820569 lr 0.00038716 rank 5
2023-02-23 14:37:59,150 DEBUG TRAIN Batch 20/0 loss 11.350187 loss_att 11.401157 loss_ctc 14.164915 loss_rnnt 10.675941 hw_loss 0.541418 lr 0.00038710 rank 6
2023-02-23 14:37:59,165 DEBUG TRAIN Batch 20/0 loss 13.074643 loss_att 12.236987 loss_ctc 16.071072 loss_rnnt 12.493604 hw_loss 0.654462 lr 0.00038711 rank 2
2023-02-23 14:37:59,169 DEBUG TRAIN Batch 20/0 loss 8.074957 loss_att 8.099406 loss_ctc 10.318222 loss_rnnt 7.360801 hw_loss 0.769059 lr 0.00038721 rank 0
2023-02-23 14:39:11,404 DEBUG TRAIN Batch 20/100 loss 13.105178 loss_att 16.030931 loss_ctc 14.798031 loss_rnnt 12.067450 hw_loss 0.425371 lr 0.00038695 rank 7
2023-02-23 14:39:11,411 DEBUG TRAIN Batch 20/100 loss 12.481841 loss_att 15.849203 loss_ctc 20.174961 loss_rnnt 10.601616 hw_loss 0.339383 lr 0.00038693 rank 3
2023-02-23 14:39:11,414 DEBUG TRAIN Batch 20/100 loss 2.273096 loss_att 6.638003 loss_ctc 3.282845 loss_rnnt 1.049684 hw_loss 0.404619 lr 0.00038699 rank 6
2023-02-23 14:39:11,414 DEBUG TRAIN Batch 20/100 loss 12.447815 loss_att 13.877579 loss_ctc 13.804358 loss_rnnt 11.773874 hw_loss 0.388341 lr 0.00038709 rank 0
2023-02-23 14:39:11,416 DEBUG TRAIN Batch 20/100 loss 4.179777 loss_att 8.678771 loss_ctc 7.903055 loss_rnnt 2.541344 hw_loss 0.454120 lr 0.00038705 rank 4
2023-02-23 14:39:11,417 DEBUG TRAIN Batch 20/100 loss 7.018271 loss_att 11.711014 loss_ctc 9.291972 loss_rnnt 5.574080 hw_loss 0.379655 lr 0.00038706 rank 1
2023-02-23 14:39:11,416 DEBUG TRAIN Batch 20/100 loss 4.332359 loss_att 7.413976 loss_ctc 5.293301 loss_rnnt 3.331753 hw_loss 0.480293 lr 0.00038700 rank 2
2023-02-23 14:39:11,418 DEBUG TRAIN Batch 20/100 loss 11.846864 loss_att 15.268150 loss_ctc 15.390208 loss_rnnt 10.520512 hw_loss 0.318091 lr 0.00038704 rank 5
2023-02-23 14:40:23,906 DEBUG TRAIN Batch 20/200 loss 3.693878 loss_att 7.637599 loss_ctc 3.651798 loss_rnnt 2.728624 hw_loss 0.341474 lr 0.00038682 rank 3
2023-02-23 14:40:23,907 DEBUG TRAIN Batch 20/200 loss 15.470112 loss_att 16.534801 loss_ctc 20.169186 loss_rnnt 14.411901 hw_loss 0.410119 lr 0.00038694 rank 4
2023-02-23 14:40:23,907 DEBUG TRAIN Batch 20/200 loss 4.740533 loss_att 6.252031 loss_ctc 5.958789 loss_rnnt 4.058035 hw_loss 0.408308 lr 0.00038697 rank 0
2023-02-23 14:40:23,907 DEBUG TRAIN Batch 20/200 loss 16.858814 loss_att 17.725990 loss_ctc 19.872070 loss_rnnt 16.076229 hw_loss 0.388839 lr 0.00038687 rank 6
2023-02-23 14:40:23,909 DEBUG TRAIN Batch 20/200 loss 6.227064 loss_att 6.735797 loss_ctc 4.164974 loss_rnnt 6.213918 hw_loss 0.349396 lr 0.00038688 rank 2
2023-02-23 14:40:23,909 DEBUG TRAIN Batch 20/200 loss 2.384583 loss_att 5.129387 loss_ctc 4.305685 loss_rnnt 1.414757 hw_loss 0.308846 lr 0.00038683 rank 7
2023-02-23 14:40:23,910 DEBUG TRAIN Batch 20/200 loss 7.140476 loss_att 12.326981 loss_ctc 12.146636 loss_rnnt 5.194211 hw_loss 0.452768 lr 0.00038695 rank 1
2023-02-23 14:40:23,957 DEBUG TRAIN Batch 20/200 loss 8.054731 loss_att 12.878187 loss_ctc 14.717090 loss_rnnt 5.980152 hw_loss 0.415450 lr 0.00038692 rank 5
2023-02-23 14:41:37,754 DEBUG TRAIN Batch 20/300 loss 9.511105 loss_att 12.903599 loss_ctc 12.202304 loss_rnnt 8.253244 hw_loss 0.413500 lr 0.00038670 rank 3
2023-02-23 14:41:37,765 DEBUG TRAIN Batch 20/300 loss 13.286018 loss_att 13.511061 loss_ctc 16.279461 loss_rnnt 12.651072 hw_loss 0.357774 lr 0.00038682 rank 4
2023-02-23 14:41:37,768 DEBUG TRAIN Batch 20/300 loss 13.883204 loss_att 15.224834 loss_ctc 16.469997 loss_rnnt 13.062468 hw_loss 0.389072 lr 0.00038677 rank 2
2023-02-23 14:41:37,771 DEBUG TRAIN Batch 20/300 loss 6.422163 loss_att 9.564403 loss_ctc 9.569702 loss_rnnt 5.159907 hw_loss 0.401505 lr 0.00038671 rank 7
2023-02-23 14:41:37,771 DEBUG TRAIN Batch 20/300 loss 9.311343 loss_att 11.459429 loss_ctc 13.067489 loss_rnnt 8.164316 hw_loss 0.406107 lr 0.00038683 rank 1
2023-02-23 14:41:37,773 DEBUG TRAIN Batch 20/300 loss 8.853588 loss_att 11.065209 loss_ctc 14.441771 loss_rnnt 7.448732 hw_loss 0.407701 lr 0.00038686 rank 0
2023-02-23 14:41:37,773 DEBUG TRAIN Batch 20/300 loss 5.128220 loss_att 8.118012 loss_ctc 7.403482 loss_rnnt 3.957852 hw_loss 0.504451 lr 0.00038676 rank 6
2023-02-23 14:41:37,781 DEBUG TRAIN Batch 20/300 loss 13.578877 loss_att 19.007429 loss_ctc 16.165157 loss_rnnt 11.915897 hw_loss 0.435811 lr 0.00038681 rank 5
2023-02-23 14:42:51,058 DEBUG TRAIN Batch 20/400 loss 9.582601 loss_att 11.598619 loss_ctc 11.687043 loss_rnnt 8.671303 hw_loss 0.426564 lr 0.00038660 rank 7
2023-02-23 14:42:51,062 DEBUG TRAIN Batch 20/400 loss 13.422969 loss_att 17.396383 loss_ctc 16.901199 loss_rnnt 11.954897 hw_loss 0.393045 lr 0.00038659 rank 3
2023-02-23 14:42:51,063 DEBUG TRAIN Batch 20/400 loss 4.387481 loss_att 6.817549 loss_ctc 8.768358 loss_rnnt 3.116748 hw_loss 0.376129 lr 0.00038665 rank 2
2023-02-23 14:42:51,063 DEBUG TRAIN Batch 20/400 loss 7.068109 loss_att 9.274766 loss_ctc 11.358715 loss_rnnt 5.841091 hw_loss 0.400511 lr 0.00038670 rank 4
2023-02-23 14:42:51,066 DEBUG TRAIN Batch 20/400 loss 13.288176 loss_att 13.417544 loss_ctc 19.682327 loss_rnnt 12.170870 hw_loss 0.447896 lr 0.00038669 rank 5
2023-02-23 14:42:51,067 DEBUG TRAIN Batch 20/400 loss 5.961297 loss_att 10.780116 loss_ctc 8.270671 loss_rnnt 4.490549 hw_loss 0.373251 lr 0.00038664 rank 6
2023-02-23 14:42:51,068 DEBUG TRAIN Batch 20/400 loss 10.412209 loss_att 12.303775 loss_ctc 13.946588 loss_rnnt 9.328156 hw_loss 0.439667 lr 0.00038674 rank 0
2023-02-23 14:42:51,070 DEBUG TRAIN Batch 20/400 loss 7.506235 loss_att 11.124380 loss_ctc 12.773363 loss_rnnt 5.882223 hw_loss 0.371435 lr 0.00038672 rank 1
2023-02-23 14:44:04,503 DEBUG TRAIN Batch 20/500 loss 17.918226 loss_att 21.239782 loss_ctc 29.107174 loss_rnnt 15.551372 hw_loss 0.395038 lr 0.00038660 rank 1
2023-02-23 14:44:04,503 DEBUG TRAIN Batch 20/500 loss 7.910367 loss_att 9.842674 loss_ctc 11.379439 loss_rnnt 6.820714 hw_loss 0.451218 lr 0.00038648 rank 7
2023-02-23 14:44:04,504 DEBUG TRAIN Batch 20/500 loss 16.645224 loss_att 20.067602 loss_ctc 24.340294 loss_rnnt 14.651455 hw_loss 0.531154 lr 0.00038653 rank 2
2023-02-23 14:44:04,504 DEBUG TRAIN Batch 20/500 loss 9.997334 loss_att 12.951739 loss_ctc 11.357190 loss_rnnt 9.055941 hw_loss 0.317244 lr 0.00038647 rank 3
2023-02-23 14:44:04,505 DEBUG TRAIN Batch 20/500 loss 7.771667 loss_att 9.003679 loss_ctc 10.659583 loss_rnnt 6.936452 hw_loss 0.382044 lr 0.00038659 rank 4
2023-02-23 14:44:04,506 DEBUG TRAIN Batch 20/500 loss 22.457987 loss_att 23.440111 loss_ctc 27.811962 loss_rnnt 21.320459 hw_loss 0.426073 lr 0.00038652 rank 6
2023-02-23 14:44:04,514 DEBUG TRAIN Batch 20/500 loss 11.695716 loss_att 14.531708 loss_ctc 17.276796 loss_rnnt 10.113331 hw_loss 0.508205 lr 0.00038658 rank 5
2023-02-23 14:44:04,515 DEBUG TRAIN Batch 20/500 loss 10.827559 loss_att 15.708165 loss_ctc 20.934572 loss_rnnt 8.296767 hw_loss 0.388256 lr 0.00038663 rank 0
2023-02-23 14:45:17,939 DEBUG TRAIN Batch 20/600 loss 11.574705 loss_att 12.350066 loss_ctc 14.969943 loss_rnnt 10.659180 hw_loss 0.577041 lr 0.00038646 rank 5
2023-02-23 14:45:17,939 DEBUG TRAIN Batch 20/600 loss 11.652673 loss_att 12.761560 loss_ctc 16.529932 loss_rnnt 10.484514 hw_loss 0.555150 lr 0.00038649 rank 1
2023-02-23 14:45:17,942 DEBUG TRAIN Batch 20/600 loss 9.787923 loss_att 11.325620 loss_ctc 12.101733 loss_rnnt 8.927784 hw_loss 0.457671 lr 0.00038637 rank 7
2023-02-23 14:45:17,944 DEBUG TRAIN Batch 20/600 loss 10.102181 loss_att 10.148826 loss_ctc 12.687757 loss_rnnt 9.450862 hw_loss 0.557340 lr 0.00038647 rank 4
2023-02-23 14:45:17,945 DEBUG TRAIN Batch 20/600 loss 13.662101 loss_att 14.183479 loss_ctc 17.088194 loss_rnnt 12.744605 hw_loss 0.668266 lr 0.00038642 rank 2
2023-02-23 14:45:17,947 DEBUG TRAIN Batch 20/600 loss 12.401489 loss_att 12.857517 loss_ctc 12.606830 loss_rnnt 12.043629 hw_loss 0.448641 lr 0.00038635 rank 3
2023-02-23 14:45:17,952 DEBUG TRAIN Batch 20/600 loss 6.717727 loss_att 8.773603 loss_ctc 10.787399 loss_rnnt 5.471446 hw_loss 0.548406 lr 0.00038651 rank 0
2023-02-23 14:45:17,990 DEBUG TRAIN Batch 20/600 loss 8.296447 loss_att 9.430602 loss_ctc 9.559987 loss_rnnt 7.641297 hw_loss 0.487210 lr 0.00038641 rank 6
run_2_20_rnnt_bias_both_2_class_more_layers_0-3word_fintune.sh: line 166: 43926 Terminated              python wenet/bin/train.py --gpu $gpu_id --config $train_config --data_type raw --symbol_table $dict --bpe_model ${bpemodel}.model --train_data $wave_data/$train_set/data.list --cv_data $wave_data/$dev_set/data.list ${checkpoint:+--checkpoint $checkpoint} --model_dir $dir --ddp.init_method $init_method --ddp.world_size $num_gpus --ddp.rank $i --ddp.dist_backend $dist_backend --num_workers 1 $cmvn_opts --pin_memory
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [192.168.0.37]:43698: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [192.168.0.37]:28072: Connection reset by peer
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:42420
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:19853
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:12013
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:46512
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.37]:19329
