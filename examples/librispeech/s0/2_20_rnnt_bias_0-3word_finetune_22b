/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_20_rnnt_bias_0-3word_finetune_22.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_20_rnnt_bias_loss_2_class_0-3word_22/ddp_init
2023-02-21 09:25:21,421 INFO training on multiple gpus, this gpu 3
2023-02-21 09:25:21,421 INFO training on multiple gpus, this gpu 2
2023-02-21 09:25:21,421 INFO training on multiple gpus, this gpu 5
2023-02-21 09:25:21,422 INFO training on multiple gpus, this gpu 4
2023-02-21 09:25:21,422 INFO training on multiple gpus, this gpu 6
2023-02-21 09:25:21,422 INFO training on multiple gpus, this gpu 7
2023-02-21 09:25:21,422 INFO training on multiple gpus, this gpu 1
2023-02-21 09:25:21,422 INFO training on multiple gpus, this gpu 0
2023-02-21 09:25:39,889 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-21 09:25:39,912 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-21 09:25:39,919 INFO Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-21 09:25:39,950 INFO Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-21 09:25:39,960 INFO Added key: store_based_barrier_key:1 to store for rank: 5
2023-02-21 09:25:39,961 INFO Added key: store_based_barrier_key:1 to store for rank: 7
2023-02-21 09:25:41,143 INFO Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-21 09:25:43,166 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-21 09:25:43,166 INFO Added key: store_based_barrier_key:1 to store for rank: 6
2023-02-21 09:25:43,833 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-21 09:25:43,897 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-21 09:25:44,089 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-21 09:25:44,185 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-21 09:25:45,913 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-21 09:25:47,293 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-21 09:25:48,317 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-02-21 09:25:54,335 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/12.pt for GPU
2023-02-21 09:25:54,362 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/12.pt for GPU
2023-02-21 09:25:54,393 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/12.pt for GPU
2023-02-21 09:25:54,427 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/12.pt for GPU
2023-02-21 09:25:54,464 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/12.pt for GPU
2023-02-21 09:25:54,495 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/12.pt for GPU
2023-02-21 09:25:54,554 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/12.pt for GPU
2023-02-21 09:25:54,621 INFO Checkpoint: loading from checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/12.pt for GPU
2023-02-21 09:26:22,852 INFO Epoch 13 TRAIN info lr 4e-08
2023-02-21 09:26:22,854 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 09:26:22,864 INFO Epoch 13 TRAIN info lr 4e-08
2023-02-21 09:26:22,866 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
2023-02-21 09:26:22,976 INFO Epoch 13 TRAIN info lr 4e-08
2023-02-21 09:26:22,977 INFO Epoch 13 TRAIN info lr 4e-08
2023-02-21 09:26:22,978 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 09:26:22,979 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
2023-02-21 09:26:22,986 INFO Epoch 13 TRAIN info lr 4e-08
2023-02-21 09:26:22,988 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
2023-02-21 09:26:23,032 INFO Epoch 13 TRAIN info lr 4e-08
2023-02-21 09:26:23,034 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
2023-02-21 09:26:23,114 INFO Epoch 13 TRAIN info lr 4e-08
2023-02-21 09:26:23,116 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
2023-02-21 09:26:23,130 INFO Epoch 13 TRAIN info lr 4e-08
2023-02-21 09:26:23,132 INFO using accumulate grad, new batch size is 2 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=51, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58200743
2023-02-21 09:28:15,903 DEBUG TRAIN Batch 13/0 loss 17.536379 loss_att 15.432816 loss_ctc 17.950321 loss_rnnt 17.742220 hw_loss 0.299396 lr 0.00047256 rank 7
2023-02-21 09:28:15,903 DEBUG TRAIN Batch 13/0 loss 14.270468 loss_att 11.561603 loss_ctc 13.609256 loss_rnnt 14.622125 hw_loss 0.521768 lr 0.00047256 rank 0
2023-02-21 09:28:15,908 DEBUG TRAIN Batch 13/0 loss 8.098952 loss_att 8.259394 loss_ctc 10.039941 loss_rnnt 7.608091 hw_loss 0.374952 lr 0.00047256 rank 4
2023-02-21 09:28:15,909 DEBUG TRAIN Batch 13/0 loss 19.164602 loss_att 16.903763 loss_ctc 20.384474 loss_rnnt 19.293608 hw_loss 0.300963 lr 0.00047256 rank 1
2023-02-21 09:28:15,913 DEBUG TRAIN Batch 13/0 loss 15.376058 loss_att 13.970362 loss_ctc 16.327259 loss_rnnt 15.294893 hw_loss 0.441518 lr 0.00047256 rank 3
2023-02-21 09:28:15,917 DEBUG TRAIN Batch 13/0 loss 18.174664 loss_att 15.855567 loss_ctc 21.563501 loss_rnnt 17.964645 hw_loss 0.416235 lr 0.00047256 rank 5
2023-02-21 09:28:15,922 DEBUG TRAIN Batch 13/0 loss 10.861156 loss_att 8.577783 loss_ctc 10.520459 loss_rnnt 11.060475 hw_loss 0.567712 lr 0.00047256 rank 6
2023-02-21 09:28:15,965 DEBUG TRAIN Batch 13/0 loss 10.605057 loss_att 8.746033 loss_ctc 10.864263 loss_rnnt 10.758656 hw_loss 0.344334 lr 0.00047256 rank 2
2023-02-21 09:29:13,057 DEBUG TRAIN Batch 13/100 loss 14.231364 loss_att 19.106121 loss_ctc 16.487637 loss_rnnt 12.955366 hw_loss 0.000396 lr 0.00047245 rank 5
2023-02-21 09:29:13,059 DEBUG TRAIN Batch 13/100 loss 4.511793 loss_att 8.743629 loss_ctc 7.344446 loss_rnnt 3.147537 hw_loss 0.262877 lr 0.00047245 rank 7
2023-02-21 09:29:13,060 DEBUG TRAIN Batch 13/100 loss 9.090679 loss_att 12.772200 loss_ctc 12.216113 loss_rnnt 7.726453 hw_loss 0.395995 lr 0.00047245 rank 0
2023-02-21 09:29:13,063 DEBUG TRAIN Batch 13/100 loss 23.586056 loss_att 30.778324 loss_ctc 28.740713 loss_rnnt 21.338900 hw_loss 0.227652 lr 0.00047245 rank 2
2023-02-21 09:29:13,063 DEBUG TRAIN Batch 13/100 loss 25.653440 loss_att 30.699434 loss_ctc 35.103912 loss_rnnt 23.181564 hw_loss 0.379899 lr 0.00047245 rank 4
2023-02-21 09:29:13,066 DEBUG TRAIN Batch 13/100 loss 8.875861 loss_att 12.370584 loss_ctc 12.397995 loss_rnnt 7.531712 hw_loss 0.329226 lr 0.00047245 rank 3
2023-02-21 09:29:13,074 DEBUG TRAIN Batch 13/100 loss 7.553375 loss_att 11.621105 loss_ctc 9.248752 loss_rnnt 6.372445 hw_loss 0.264999 lr 0.00047245 rank 1
2023-02-21 09:29:13,079 DEBUG TRAIN Batch 13/100 loss 15.723026 loss_att 20.898701 loss_ctc 15.757191 loss_rnnt 14.457350 hw_loss 0.423728 lr 0.00047245 rank 6
2023-02-21 09:30:11,610 DEBUG TRAIN Batch 13/200 loss 11.534449 loss_att 20.233669 loss_ctc 18.613928 loss_rnnt 8.756617 hw_loss 0.176358 lr 0.00047235 rank 5
2023-02-21 09:30:11,617 DEBUG TRAIN Batch 13/200 loss 19.762182 loss_att 34.275108 loss_ctc 36.555454 loss_rnnt 14.338391 hw_loss 0.528942 lr 0.00047235 rank 2
2023-02-21 09:30:11,617 DEBUG TRAIN Batch 13/200 loss 6.830081 loss_att 11.393906 loss_ctc 7.814269 loss_rnnt 5.567103 hw_loss 0.410603 lr 0.00047235 rank 3
2023-02-21 09:30:11,619 DEBUG TRAIN Batch 13/200 loss 4.133874 loss_att 8.968894 loss_ctc 7.843578 loss_rnnt 2.671860 hw_loss 0.000716 lr 0.00047235 rank 0
2023-02-21 09:30:11,621 DEBUG TRAIN Batch 13/200 loss 7.073572 loss_att 14.995256 loss_ctc 10.591035 loss_rnnt 4.855679 hw_loss 0.308552 lr 0.00047235 rank 4
2023-02-21 09:30:11,624 DEBUG TRAIN Batch 13/200 loss 33.459187 loss_att 48.482491 loss_ctc 49.920887 loss_rnnt 28.179592 hw_loss 0.150083 lr 0.00047235 rank 6
2023-02-21 09:30:11,626 DEBUG TRAIN Batch 13/200 loss 1.237663 loss_att 4.872488 loss_ctc 0.825242 loss_rnnt 0.565305 hw_loss 0.000716 lr 0.00047235 rank 1
2023-02-21 09:30:11,627 DEBUG TRAIN Batch 13/200 loss 14.215981 loss_att 19.543470 loss_ctc 25.493216 loss_rnnt 11.517658 hw_loss 0.242241 lr 0.00047235 rank 7
2023-02-21 09:31:07,030 DEBUG TRAIN Batch 13/300 loss 15.352202 loss_att 16.383358 loss_ctc 18.481594 loss_rnnt 14.473864 hw_loss 0.477850 lr 0.00047224 rank 0
2023-02-21 09:31:07,033 DEBUG TRAIN Batch 13/300 loss 15.609316 loss_att 17.496737 loss_ctc 22.758751 loss_rnnt 14.044505 hw_loss 0.438879 lr 0.00047224 rank 6
2023-02-21 09:31:07,035 DEBUG TRAIN Batch 13/300 loss 18.001289 loss_att 18.849575 loss_ctc 19.889524 loss_rnnt 17.465305 hw_loss 0.214804 lr 0.00047224 rank 2
2023-02-21 09:31:07,036 DEBUG TRAIN Batch 13/300 loss 14.984978 loss_att 17.080812 loss_ctc 17.107019 loss_rnnt 14.098111 hw_loss 0.346428 lr 0.00047224 rank 3
2023-02-21 09:31:07,036 DEBUG TRAIN Batch 13/300 loss 19.909864 loss_att 19.683920 loss_ctc 25.056580 loss_rnnt 19.241879 hw_loss 0.050522 lr 0.00047224 rank 4
2023-02-21 09:31:07,041 DEBUG TRAIN Batch 13/300 loss 9.895580 loss_att 9.730024 loss_ctc 11.954703 loss_rnnt 9.379396 hw_loss 0.515146 lr 0.00047224 rank 1
2023-02-21 09:31:07,049 DEBUG TRAIN Batch 13/300 loss 13.790406 loss_att 13.564018 loss_ctc 16.579306 loss_rnnt 13.205823 hw_loss 0.483762 lr 0.00047224 rank 7
2023-02-21 09:31:07,069 DEBUG TRAIN Batch 13/300 loss 11.447769 loss_att 12.157351 loss_ctc 14.588827 loss_rnnt 10.649482 hw_loss 0.445432 lr 0.00047224 rank 5
2023-02-21 09:32:03,276 DEBUG TRAIN Batch 13/400 loss 20.195862 loss_att 21.788017 loss_ctc 25.207001 loss_rnnt 19.123171 hw_loss 0.161449 lr 0.00047214 rank 2
2023-02-21 09:32:03,284 DEBUG TRAIN Batch 13/400 loss 12.020974 loss_att 15.168076 loss_ctc 16.379333 loss_rnnt 10.655624 hw_loss 0.290277 lr 0.00047214 rank 4
2023-02-21 09:32:03,285 DEBUG TRAIN Batch 13/400 loss 7.665701 loss_att 11.308677 loss_ctc 8.249864 loss_rnnt 6.682172 hw_loss 0.331959 lr 0.00047214 rank 0
2023-02-21 09:32:03,285 DEBUG TRAIN Batch 13/400 loss 14.956793 loss_att 18.058666 loss_ctc 21.084824 loss_rnnt 13.311913 hw_loss 0.388939 lr 0.00047214 rank 6
2023-02-21 09:32:03,287 DEBUG TRAIN Batch 13/400 loss 15.549849 loss_att 18.143656 loss_ctc 22.426971 loss_rnnt 13.937708 hw_loss 0.330806 lr 0.00047214 rank 5
2023-02-21 09:32:03,289 DEBUG TRAIN Batch 13/400 loss 11.738396 loss_att 12.724387 loss_ctc 12.787384 loss_rnnt 11.234713 hw_loss 0.312412 lr 0.00047214 rank 7
2023-02-21 09:32:03,290 DEBUG TRAIN Batch 13/400 loss 18.488726 loss_att 24.802219 loss_ctc 22.242851 loss_rnnt 16.624508 hw_loss 0.189320 lr 0.00047214 rank 3
2023-02-21 09:32:03,296 DEBUG TRAIN Batch 13/400 loss 12.635175 loss_att 14.202060 loss_ctc 18.951115 loss_rnnt 11.404824 hw_loss 0.140341 lr 0.00047214 rank 1
2023-02-21 09:33:00,671 DEBUG TRAIN Batch 13/500 loss 18.149130 loss_att 15.318725 loss_ctc 23.895473 loss_rnnt 17.866835 hw_loss 0.154117 lr 0.00047203 rank 5
2023-02-21 09:33:00,672 DEBUG TRAIN Batch 13/500 loss 16.788395 loss_att 16.185333 loss_ctc 22.172312 loss_rnnt 16.095766 hw_loss 0.178845 lr 0.00047203 rank 0
2023-02-21 09:33:00,672 DEBUG TRAIN Batch 13/500 loss 2.992831 loss_att 7.576785 loss_ctc 6.331659 loss_rnnt 1.412770 hw_loss 0.408926 lr 0.00047203 rank 2
2023-02-21 09:33:00,676 DEBUG TRAIN Batch 13/500 loss 9.473599 loss_att 17.007076 loss_ctc 12.073762 loss_rnnt 7.504110 hw_loss 0.217697 lr 0.00047203 rank 4
2023-02-21 09:33:00,685 DEBUG TRAIN Batch 13/500 loss 11.434474 loss_att 14.664833 loss_ctc 11.720639 loss_rnnt 10.602245 hw_loss 0.277502 lr 0.00047203 rank 7
2023-02-21 09:33:00,687 DEBUG TRAIN Batch 13/500 loss 14.270775 loss_att 11.468232 loss_ctc 18.056009 loss_rnnt 14.232599 hw_loss 0.176222 lr 0.00047203 rank 6
2023-02-21 09:33:00,687 DEBUG TRAIN Batch 13/500 loss 3.335631 loss_att 6.722400 loss_ctc 3.477089 loss_rnnt 2.498099 hw_loss 0.264970 lr 0.00047203 rank 1
2023-02-21 09:33:00,732 DEBUG TRAIN Batch 13/500 loss 14.124780 loss_att 13.123408 loss_ctc 11.164939 loss_rnnt 14.719419 hw_loss 0.000524 lr 0.00047203 rank 3
2023-02-21 09:33:56,227 DEBUG TRAIN Batch 13/600 loss 29.686008 loss_att 32.529671 loss_ctc 37.683910 loss_rnnt 27.874228 hw_loss 0.331237 lr 0.00047193 rank 2
2023-02-21 09:33:56,233 DEBUG TRAIN Batch 13/600 loss 14.397137 loss_att 16.292366 loss_ctc 16.461256 loss_rnnt 13.593761 hw_loss 0.279586 lr 0.00047193 rank 0
2023-02-21 09:33:56,234 DEBUG TRAIN Batch 13/600 loss 15.103440 loss_att 17.939911 loss_ctc 22.462772 loss_rnnt 13.382887 hw_loss 0.322531 lr 0.00047193 rank 5
2023-02-21 09:33:56,235 DEBUG TRAIN Batch 13/600 loss 22.487528 loss_att 22.832222 loss_ctc 33.597237 loss_rnnt 20.722982 hw_loss 0.401834 lr 0.00047193 rank 7
2023-02-21 09:33:56,236 DEBUG TRAIN Batch 13/600 loss 4.891831 loss_att 9.990366 loss_ctc 7.023894 loss_rnnt 3.460279 hw_loss 0.239194 lr 0.00047193 rank 6
2023-02-21 09:33:56,241 DEBUG TRAIN Batch 13/600 loss 4.252573 loss_att 5.426259 loss_ctc 4.712871 loss_rnnt 3.807680 hw_loss 0.278967 lr 0.00047193 rank 3
2023-02-21 09:33:56,241 DEBUG TRAIN Batch 13/600 loss 24.132200 loss_att 25.598419 loss_ctc 31.845154 loss_rnnt 22.649710 hw_loss 0.301600 lr 0.00047193 rank 1
2023-02-21 09:33:56,300 DEBUG TRAIN Batch 13/600 loss 13.290020 loss_att 15.301271 loss_ctc 16.246820 loss_rnnt 12.335382 hw_loss 0.296527 lr 0.00047193 rank 4
2023-02-21 09:34:52,856 DEBUG TRAIN Batch 13/700 loss 10.339127 loss_att 14.921310 loss_ctc 9.860755 loss_rnnt 9.380352 hw_loss 0.198977 lr 0.00047182 rank 5
2023-02-21 09:34:52,862 DEBUG TRAIN Batch 13/700 loss 4.268113 loss_att 8.910506 loss_ctc 3.206914 loss_rnnt 3.374245 hw_loss 0.200404 lr 0.00047182 rank 2
2023-02-21 09:34:52,864 DEBUG TRAIN Batch 13/700 loss 24.787153 loss_att 31.520502 loss_ctc 40.527229 loss_rnnt 21.185507 hw_loss 0.293063 lr 0.00047182 rank 6
2023-02-21 09:34:52,864 DEBUG TRAIN Batch 13/700 loss 13.307287 loss_att 17.321836 loss_ctc 21.691921 loss_rnnt 11.256271 hw_loss 0.244040 lr 0.00047182 rank 0
2023-02-21 09:34:52,864 DEBUG TRAIN Batch 13/700 loss 15.257020 loss_att 21.876051 loss_ctc 18.647623 loss_rnnt 13.302840 hw_loss 0.334300 lr 0.00047182 rank 4
2023-02-21 09:34:52,866 DEBUG TRAIN Batch 13/700 loss 12.137758 loss_att 17.375713 loss_ctc 17.390135 loss_rnnt 10.241014 hw_loss 0.279068 lr 0.00047182 rank 7
2023-02-21 09:34:52,885 DEBUG TRAIN Batch 13/700 loss 16.465879 loss_att 18.035484 loss_ctc 17.234112 loss_rnnt 15.846764 hw_loss 0.380183 lr 0.00047182 rank 1
2023-02-21 09:34:52,924 DEBUG TRAIN Batch 13/700 loss 12.778051 loss_att 15.717253 loss_ctc 16.993439 loss_rnnt 11.467600 hw_loss 0.301048 lr 0.00047182 rank 3
2023-02-21 09:35:50,434 DEBUG TRAIN Batch 13/800 loss 8.753443 loss_att 14.863441 loss_ctc 19.241938 loss_rnnt 5.836075 hw_loss 0.556691 lr 0.00047172 rank 2
2023-02-21 09:35:50,445 DEBUG TRAIN Batch 13/800 loss 7.381215 loss_att 14.421669 loss_ctc 7.211698 loss_rnnt 5.881043 hw_loss 0.215030 lr 0.00047172 rank 0
2023-02-21 09:35:50,447 DEBUG TRAIN Batch 13/800 loss 30.558361 loss_att 33.620930 loss_ctc 36.629139 loss_rnnt 29.136345 hw_loss 0.000126 lr 0.00047172 rank 6
2023-02-21 09:35:50,447 DEBUG TRAIN Batch 13/800 loss 26.012503 loss_att 23.846279 loss_ctc 34.770420 loss_rnnt 25.202847 hw_loss 0.140961 lr 0.00047172 rank 7
2023-02-21 09:35:50,451 DEBUG TRAIN Batch 13/800 loss 1.958354 loss_att 4.199507 loss_ctc 0.827724 loss_rnnt 1.430352 hw_loss 0.432227 lr 0.00047172 rank 4
2023-02-21 09:35:50,453 DEBUG TRAIN Batch 13/800 loss 8.257318 loss_att 16.291786 loss_ctc 15.339230 loss_rnnt 5.505141 hw_loss 0.376929 lr 0.00047172 rank 5
2023-02-21 09:35:50,453 DEBUG TRAIN Batch 13/800 loss 14.527608 loss_att 17.983738 loss_ctc 26.773308 loss_rnnt 11.953088 hw_loss 0.469750 lr 0.00047172 rank 1
2023-02-21 09:35:50,497 DEBUG TRAIN Batch 13/800 loss 3.190450 loss_att 11.915398 loss_ctc 1.581756 loss_rnnt 1.419137 hw_loss 0.451530 lr 0.00047172 rank 3
2023-02-21 09:37:43,032 DEBUG TRAIN Batch 13/900 loss 7.190235 loss_att 13.064077 loss_ctc 8.672219 loss_rnnt 5.631408 hw_loss 0.349615 lr 0.00047161 rank 4
2023-02-21 09:37:43,032 DEBUG TRAIN Batch 13/900 loss 10.311840 loss_att 10.692918 loss_ctc 10.158681 loss_rnnt 10.120298 hw_loss 0.254526 lr 0.00047161 rank 0
2023-02-21 09:37:43,033 DEBUG TRAIN Batch 13/900 loss 16.776352 loss_att 19.125595 loss_ctc 22.184525 loss_rnnt 15.548368 hw_loss 0.069460 lr 0.00047161 rank 2
2023-02-21 09:37:43,039 DEBUG TRAIN Batch 13/900 loss 17.869232 loss_att 18.520552 loss_ctc 23.788841 loss_rnnt 16.794643 hw_loss 0.290709 lr 0.00047161 rank 5
2023-02-21 09:37:43,038 DEBUG TRAIN Batch 13/900 loss 15.509964 loss_att 16.196554 loss_ctc 14.252143 loss_rnnt 15.431898 hw_loss 0.203357 lr 0.00047161 rank 3
2023-02-21 09:37:43,039 DEBUG TRAIN Batch 13/900 loss 9.966030 loss_att 9.893465 loss_ctc 13.090949 loss_rnnt 9.484451 hw_loss 0.148940 lr 0.00047161 rank 6
2023-02-21 09:37:43,040 DEBUG TRAIN Batch 13/900 loss 8.429987 loss_att 11.198114 loss_ctc 10.861744 loss_rnnt 7.434777 hw_loss 0.220029 lr 0.00047161 rank 1
2023-02-21 09:37:43,041 DEBUG TRAIN Batch 13/900 loss 16.215586 loss_att 17.845783 loss_ctc 22.097069 loss_rnnt 14.996525 hw_loss 0.204045 lr 0.00047161 rank 7
2023-02-21 09:38:40,869 DEBUG TRAIN Batch 13/1000 loss 8.686073 loss_att 12.405834 loss_ctc 12.964603 loss_rnnt 7.242913 hw_loss 0.241383 lr 0.00047151 rank 0
2023-02-21 09:38:40,873 DEBUG TRAIN Batch 13/1000 loss 12.897407 loss_att 14.131460 loss_ctc 17.071360 loss_rnnt 12.093367 hw_loss 0.001319 lr 0.00047151 rank 1
2023-02-21 09:38:40,876 DEBUG TRAIN Batch 13/1000 loss 16.510708 loss_att 19.340984 loss_ctc 18.495731 loss_rnnt 15.679278 hw_loss 0.001319 lr 0.00047151 rank 3
2023-02-21 09:38:40,876 DEBUG TRAIN Batch 13/1000 loss 8.603804 loss_att 8.959806 loss_ctc 8.032754 loss_rnnt 8.413326 hw_loss 0.366407 lr 0.00047151 rank 2
2023-02-21 09:38:40,877 DEBUG TRAIN Batch 13/1000 loss 9.964206 loss_att 12.851323 loss_ctc 14.577888 loss_rnnt 8.663855 hw_loss 0.202069 lr 0.00047151 rank 4
2023-02-21 09:38:40,878 DEBUG TRAIN Batch 13/1000 loss 11.799703 loss_att 12.486700 loss_ctc 15.412984 loss_rnnt 11.179829 hw_loss 0.001319 lr 0.00047151 rank 6
2023-02-21 09:38:40,897 DEBUG TRAIN Batch 13/1000 loss 15.598218 loss_att 16.514425 loss_ctc 18.065718 loss_rnnt 15.036608 hw_loss 0.092566 lr 0.00047151 rank 7
2023-02-21 09:38:40,898 DEBUG TRAIN Batch 13/1000 loss 1.709094 loss_att 4.795451 loss_ctc 3.831846 loss_rnnt 0.776114 hw_loss 0.061267 lr 0.00047151 rank 5
2023-02-21 09:39:35,919 DEBUG TRAIN Batch 13/1100 loss 10.778172 loss_att 13.478212 loss_ctc 16.380661 loss_rnnt 9.328493 hw_loss 0.305009 lr 0.00047140 rank 2
2023-02-21 09:39:35,921 DEBUG TRAIN Batch 13/1100 loss 35.127014 loss_att 34.433620 loss_ctc 42.405533 loss_rnnt 34.082947 hw_loss 0.398019 lr 0.00047140 rank 0
2023-02-21 09:39:35,926 DEBUG TRAIN Batch 13/1100 loss 41.622353 loss_att 43.728470 loss_ctc 47.037594 loss_rnnt 40.478931 hw_loss 0.000307 lr 0.00047140 rank 7
2023-02-21 09:39:35,928 DEBUG TRAIN Batch 13/1100 loss 14.328155 loss_att 35.851093 loss_ctc 18.166683 loss_rnnt 9.417171 hw_loss 0.177358 lr 0.00047140 rank 6
2023-02-21 09:39:35,930 DEBUG TRAIN Batch 13/1100 loss 2.995475 loss_att 5.249564 loss_ctc 1.984990 loss_rnnt 2.498413 hw_loss 0.339328 lr 0.00047140 rank 5
2023-02-21 09:39:35,939 DEBUG TRAIN Batch 13/1100 loss 22.738970 loss_att 25.337677 loss_ctc 38.669518 loss_rnnt 20.013607 hw_loss 0.152904 lr 0.00047140 rank 1
2023-02-21 09:39:35,949 DEBUG TRAIN Batch 13/1100 loss 15.827789 loss_att 21.852062 loss_ctc 19.834284 loss_rnnt 14.041251 hw_loss 0.089034 lr 0.00047140 rank 3
2023-02-21 09:39:35,952 DEBUG TRAIN Batch 13/1100 loss 19.230656 loss_att 24.722961 loss_ctc 32.306904 loss_rnnt 16.199375 hw_loss 0.354971 lr 0.00047140 rank 4
2023-02-21 09:40:31,166 DEBUG TRAIN Batch 13/1200 loss 8.680949 loss_att 16.949381 loss_ctc 11.565948 loss_rnnt 6.408835 hw_loss 0.438301 lr 0.00047130 rank 0
2023-02-21 09:40:31,171 DEBUG TRAIN Batch 13/1200 loss 8.140608 loss_att 10.225431 loss_ctc 12.421026 loss_rnnt 6.920066 hw_loss 0.436601 lr 0.00047130 rank 2
2023-02-21 09:40:31,174 DEBUG TRAIN Batch 13/1200 loss 13.847320 loss_att 22.136641 loss_ctc 24.203560 loss_rnnt 10.808582 hw_loss 0.000077 lr 0.00047130 rank 5
2023-02-21 09:40:31,175 DEBUG TRAIN Batch 13/1200 loss 7.164581 loss_att 15.102293 loss_ctc 10.319761 loss_rnnt 5.156306 hw_loss 0.000077 lr 0.00047130 rank 4
2023-02-21 09:40:31,175 DEBUG TRAIN Batch 13/1200 loss 17.549402 loss_att 22.849958 loss_ctc 22.538034 loss_rnnt 15.670844 hw_loss 0.287432 lr 0.00047130 rank 7
2023-02-21 09:40:31,176 DEBUG TRAIN Batch 13/1200 loss 4.290500 loss_att 6.245959 loss_ctc 4.187938 loss_rnnt 3.719859 hw_loss 0.362296 lr 0.00047130 rank 3
2023-02-21 09:40:31,177 DEBUG TRAIN Batch 13/1200 loss 7.012565 loss_att 9.940327 loss_ctc 13.258894 loss_rnnt 5.516844 hw_loss 0.144984 lr 0.00047130 rank 6
2023-02-21 09:40:31,182 DEBUG TRAIN Batch 13/1200 loss 3.856160 loss_att 6.376037 loss_ctc 4.705949 loss_rnnt 3.108500 hw_loss 0.244461 lr 0.00047130 rank 1
2023-02-21 09:41:29,416 DEBUG TRAIN Batch 13/1300 loss 22.422657 loss_att 26.562239 loss_ctc 28.045208 loss_rnnt 20.660364 hw_loss 0.346317 lr 0.00047119 rank 0
2023-02-21 09:41:29,418 DEBUG TRAIN Batch 13/1300 loss 17.678543 loss_att 20.749363 loss_ctc 21.136911 loss_rnnt 16.448509 hw_loss 0.290163 lr 0.00047119 rank 3
2023-02-21 09:41:29,419 DEBUG TRAIN Batch 13/1300 loss 11.591560 loss_att 16.496571 loss_ctc 18.058323 loss_rnnt 9.721689 hw_loss 0.049938 lr 0.00047119 rank 2
2023-02-21 09:41:29,422 DEBUG TRAIN Batch 13/1300 loss 14.860599 loss_att 16.079014 loss_ctc 16.450085 loss_rnnt 14.404793 hw_loss 0.000361 lr 0.00047119 rank 7
2023-02-21 09:41:29,423 DEBUG TRAIN Batch 13/1300 loss 7.255470 loss_att 12.613863 loss_ctc 6.716783 loss_rnnt 6.006714 hw_loss 0.466693 lr 0.00047119 rank 6
2023-02-21 09:41:29,426 DEBUG TRAIN Batch 13/1300 loss 18.903946 loss_att 20.470512 loss_ctc 21.734119 loss_rnnt 17.916843 hw_loss 0.555809 lr 0.00047119 rank 5
2023-02-21 09:41:29,430 DEBUG TRAIN Batch 13/1300 loss 59.171364 loss_att 61.324432 loss_ctc 71.831459 loss_rnnt 56.837502 hw_loss 0.403556 lr 0.00047119 rank 4
2023-02-21 09:41:29,437 DEBUG TRAIN Batch 13/1300 loss 18.159683 loss_att 22.037331 loss_ctc 25.638466 loss_rnnt 16.338505 hw_loss 0.090895 lr 0.00047119 rank 1
2023-02-21 09:42:23,533 DEBUG TRAIN Batch 13/1400 loss 13.137531 loss_att 21.044285 loss_ctc 15.626215 loss_rnnt 11.097281 hw_loss 0.238265 lr 0.00047109 rank 2
2023-02-21 09:42:23,534 DEBUG TRAIN Batch 13/1400 loss 19.959667 loss_att 20.344921 loss_ctc 27.157711 loss_rnnt 18.922834 hw_loss 0.000083 lr 0.00047109 rank 7
2023-02-21 09:42:23,537 DEBUG TRAIN Batch 13/1400 loss 32.442787 loss_att 33.001755 loss_ctc 41.362103 loss_rnnt 31.141705 hw_loss 0.000083 lr 0.00047109 rank 0
2023-02-21 09:42:23,538 DEBUG TRAIN Batch 13/1400 loss 5.997110 loss_att 11.306183 loss_ctc 7.600907 loss_rnnt 4.507479 hw_loss 0.401206 lr 0.00047109 rank 6
2023-02-21 09:42:23,541 DEBUG TRAIN Batch 13/1400 loss 2.132899 loss_att 5.490375 loss_ctc 4.055964 loss_rnnt 1.151571 hw_loss 0.100170 lr 0.00047109 rank 5
2023-02-21 09:42:23,546 DEBUG TRAIN Batch 13/1400 loss 6.398751 loss_att 7.478809 loss_ctc 4.289149 loss_rnnt 6.463975 hw_loss 0.000083 lr 0.00047109 rank 3
2023-02-21 09:42:23,606 DEBUG TRAIN Batch 13/1400 loss 6.999811 loss_att 11.371983 loss_ctc 13.159715 loss_rnnt 5.304011 hw_loss 0.000083 lr 0.00047109 rank 1
2023-02-21 09:42:23,630 DEBUG TRAIN Batch 13/1400 loss 3.295251 loss_att 8.951038 loss_ctc 5.093882 loss_rnnt 1.924231 hw_loss 0.000083 lr 0.00047109 rank 4
2023-02-21 09:43:19,582 DEBUG TRAIN Batch 13/1500 loss 4.374144 loss_att 6.345984 loss_ctc 5.263592 loss_rnnt 3.771441 hw_loss 0.168264 lr 0.00047098 rank 2
2023-02-21 09:43:19,583 DEBUG TRAIN Batch 13/1500 loss 41.195381 loss_att 47.740177 loss_ctc 54.402664 loss_rnnt 38.125401 hw_loss 0.000098 lr 0.00047098 rank 3
2023-02-21 09:43:19,585 DEBUG TRAIN Batch 13/1500 loss 26.887794 loss_att 23.976568 loss_ctc 25.033924 loss_rnnt 27.525295 hw_loss 0.359865 lr 0.00047098 rank 7
2023-02-21 09:43:19,587 DEBUG TRAIN Batch 13/1500 loss 23.361433 loss_att 23.577517 loss_ctc 33.436218 loss_rnnt 21.795223 hw_loss 0.336920 lr 0.00047098 rank 5
2023-02-21 09:43:19,589 DEBUG TRAIN Batch 13/1500 loss 7.461267 loss_att 9.692524 loss_ctc 8.800650 loss_rnnt 6.638208 hw_loss 0.371670 lr 0.00047098 rank 0
2023-02-21 09:43:19,591 DEBUG TRAIN Batch 13/1500 loss 10.541927 loss_att 15.116055 loss_ctc 15.982235 loss_rnnt 8.798689 hw_loss 0.193198 lr 0.00047098 rank 4
2023-02-21 09:43:19,599 DEBUG TRAIN Batch 13/1500 loss 29.455231 loss_att 29.502026 loss_ctc 33.406807 loss_rnnt 28.867680 hw_loss 0.096211 lr 0.00047098 rank 1
2023-02-21 09:43:19,600 DEBUG TRAIN Batch 13/1500 loss 12.478790 loss_att 16.894100 loss_ctc 9.937695 loss_rnnt 11.934488 hw_loss 0.000098 lr 0.00047098 rank 6
2023-02-21 09:44:17,796 DEBUG TRAIN Batch 13/1600 loss 73.236565 loss_att 74.326225 loss_ctc 106.217781 loss_rnnt 68.524490 hw_loss 0.181214 lr 0.00047088 rank 2
2023-02-21 09:44:17,800 DEBUG TRAIN Batch 13/1600 loss 12.549644 loss_att 13.978587 loss_ctc 13.702639 loss_rnnt 12.000628 hw_loss 0.205300 lr 0.00047088 rank 5
2023-02-21 09:44:17,801 DEBUG TRAIN Batch 13/1600 loss 17.124674 loss_att 17.389322 loss_ctc 12.401083 loss_rnnt 17.701260 hw_loss 0.000556 lr 0.00047088 rank 0
2023-02-21 09:44:17,803 DEBUG TRAIN Batch 13/1600 loss 36.583229 loss_att 41.918976 loss_ctc 55.402561 loss_rnnt 33.006542 hw_loss 0.000556 lr 0.00047088 rank 4
2023-02-21 09:44:17,805 DEBUG TRAIN Batch 13/1600 loss 14.553795 loss_att 18.579472 loss_ctc 16.918537 loss_rnnt 13.324100 hw_loss 0.204863 lr 0.00047088 rank 7
2023-02-21 09:44:17,811 DEBUG TRAIN Batch 13/1600 loss 1.814865 loss_att 4.247706 loss_ctc 1.980421 loss_rnnt 1.181510 hw_loss 0.233835 lr 0.00047088 rank 6
2023-02-21 09:44:17,821 DEBUG TRAIN Batch 13/1600 loss 1.878089 loss_att 6.721259 loss_ctc 1.380172 loss_rnnt 0.792359 hw_loss 0.344034 lr 0.00047088 rank 1
2023-02-21 09:44:17,865 DEBUG TRAIN Batch 13/1600 loss 10.483960 loss_att 9.629249 loss_ctc 10.387572 loss_rnnt 10.499849 hw_loss 0.314823 lr 0.00047088 rank 3
2023-02-21 09:46:10,365 DEBUG TRAIN Batch 13/1700 loss 13.210590 loss_att 18.597481 loss_ctc 18.871090 loss_rnnt 11.239463 hw_loss 0.260655 lr 0.00047078 rank 2
2023-02-21 09:46:10,369 DEBUG TRAIN Batch 13/1700 loss 9.128381 loss_att 8.319130 loss_ctc 10.714980 loss_rnnt 8.849388 hw_loss 0.429931 lr 0.00047078 rank 3
2023-02-21 09:46:10,374 DEBUG TRAIN Batch 13/1700 loss 29.752810 loss_att 33.699417 loss_ctc 40.849461 loss_rnnt 27.289686 hw_loss 0.364218 lr 0.00047078 rank 5
2023-02-21 09:46:10,378 DEBUG TRAIN Batch 13/1700 loss 4.871944 loss_att 9.648313 loss_ctc 4.640611 loss_rnnt 3.746327 hw_loss 0.377228 lr 0.00047078 rank 0
2023-02-21 09:46:10,382 DEBUG TRAIN Batch 13/1700 loss 8.076186 loss_att 7.118032 loss_ctc 7.099160 loss_rnnt 8.143790 hw_loss 0.476807 lr 0.00047078 rank 7
2023-02-21 09:46:10,387 DEBUG TRAIN Batch 13/1700 loss 10.885472 loss_att 17.788261 loss_ctc 10.936668 loss_rnnt 9.248037 hw_loss 0.468845 lr 0.00047078 rank 6
2023-02-21 09:46:10,389 DEBUG TRAIN Batch 13/1700 loss 6.288229 loss_att 7.548154 loss_ctc 9.015899 loss_rnnt 5.558116 hw_loss 0.214574 lr 0.00047078 rank 1
2023-02-21 09:46:10,431 DEBUG TRAIN Batch 13/1700 loss 29.969589 loss_att 23.438896 loss_ctc 31.953918 loss_rnnt 30.850643 hw_loss 0.300950 lr 0.00047078 rank 4
2023-02-21 09:47:07,117 DEBUG TRAIN Batch 13/1800 loss 12.380457 loss_att 17.463085 loss_ctc 15.458195 loss_rnnt 10.741392 hw_loss 0.397828 lr 0.00047067 rank 2
2023-02-21 09:47:07,121 DEBUG TRAIN Batch 13/1800 loss 20.862490 loss_att 27.248632 loss_ctc 26.325504 loss_rnnt 18.552418 hw_loss 0.570827 lr 0.00047067 rank 5
2023-02-21 09:47:07,122 DEBUG TRAIN Batch 13/1800 loss 22.108614 loss_att 25.328032 loss_ctc 28.965086 loss_rnnt 20.456493 hw_loss 0.176329 lr 0.00047067 rank 3
2023-02-21 09:47:07,123 DEBUG TRAIN Batch 13/1800 loss 21.366421 loss_att 26.017159 loss_ctc 27.179382 loss_rnnt 19.514950 hw_loss 0.274238 lr 0.00047067 rank 0
2023-02-21 09:47:07,126 DEBUG TRAIN Batch 13/1800 loss 22.460949 loss_att 31.427174 loss_ctc 24.585812 loss_rnnt 20.284227 hw_loss 0.187797 lr 0.00047067 rank 6
2023-02-21 09:47:07,126 DEBUG TRAIN Batch 13/1800 loss 4.653644 loss_att 8.473837 loss_ctc 6.129961 loss_rnnt 3.557501 hw_loss 0.253616 lr 0.00047067 rank 4
2023-02-21 09:47:07,128 DEBUG TRAIN Batch 13/1800 loss 18.734430 loss_att 20.455647 loss_ctc 24.157784 loss_rnnt 17.515377 hw_loss 0.284430 lr 0.00047067 rank 7
2023-02-21 09:47:07,133 DEBUG TRAIN Batch 13/1800 loss 14.958592 loss_att 18.028452 loss_ctc 19.766769 loss_rnnt 13.703484 hw_loss 0.000088 lr 0.00047067 rank 1
2023-02-21 09:48:04,919 DEBUG TRAIN Batch 13/1900 loss 6.127436 loss_att 10.617825 loss_ctc 6.875506 loss_rnnt 4.972708 hw_loss 0.294201 lr 0.00047057 rank 2
2023-02-21 09:48:04,920 DEBUG TRAIN Batch 13/1900 loss 4.568093 loss_att 10.713440 loss_ctc 4.094438 loss_rnnt 3.267798 hw_loss 0.251962 lr 0.00047057 rank 5
2023-02-21 09:48:04,921 DEBUG TRAIN Batch 13/1900 loss 7.560468 loss_att 13.302486 loss_ctc 7.708304 loss_rnnt 6.263017 hw_loss 0.242506 lr 0.00047057 rank 6
2023-02-21 09:48:04,921 DEBUG TRAIN Batch 13/1900 loss 13.041165 loss_att 18.131706 loss_ctc 10.870019 loss_rnnt 12.310198 hw_loss 0.004397 lr 0.00047057 rank 0
2023-02-21 09:48:04,927 DEBUG TRAIN Batch 13/1900 loss 15.748454 loss_att 18.091034 loss_ctc 21.413704 loss_rnnt 14.315143 hw_loss 0.392677 lr 0.00047057 rank 4
2023-02-21 09:48:04,928 DEBUG TRAIN Batch 13/1900 loss 6.581976 loss_att 11.253418 loss_ctc 7.447771 loss_rnnt 5.372936 hw_loss 0.298709 lr 0.00047057 rank 3
2023-02-21 09:48:04,930 DEBUG TRAIN Batch 13/1900 loss 55.540951 loss_att 65.530319 loss_ctc 50.096569 loss_rnnt 54.106415 hw_loss 0.304835 lr 0.00047057 rank 7
2023-02-21 09:48:04,936 DEBUG TRAIN Batch 13/1900 loss 19.829815 loss_att 25.840170 loss_ctc 25.826332 loss_rnnt 17.614786 hw_loss 0.400166 lr 0.00047057 rank 1
2023-02-21 09:49:00,495 DEBUG TRAIN Batch 13/2000 loss 8.382085 loss_att 10.893207 loss_ctc 16.343048 loss_rnnt 6.728821 hw_loss 0.167957 lr 0.00047046 rank 2
2023-02-21 09:49:00,502 DEBUG TRAIN Batch 13/2000 loss 10.747380 loss_att 10.064405 loss_ctc 12.605925 loss_rnnt 10.391979 hw_loss 0.457855 lr 0.00047046 rank 4
2023-02-21 09:49:00,505 DEBUG TRAIN Batch 13/2000 loss 24.448978 loss_att 28.553642 loss_ctc 24.678720 loss_rnnt 23.597366 hw_loss 0.000086 lr 0.00047046 rank 7
2023-02-21 09:49:00,508 DEBUG TRAIN Batch 13/2000 loss 26.127558 loss_att 35.198334 loss_ctc 36.324825 loss_rnnt 22.697453 hw_loss 0.480587 lr 0.00047046 rank 0
2023-02-21 09:49:00,512 DEBUG TRAIN Batch 13/2000 loss 8.461911 loss_att 9.878475 loss_ctc 6.464178 loss_rnnt 8.444918 hw_loss 0.000086 lr 0.00047046 rank 1
2023-02-21 09:49:00,512 DEBUG TRAIN Batch 13/2000 loss 4.874400 loss_att 9.400416 loss_ctc 3.415637 loss_rnnt 3.900381 hw_loss 0.493720 lr 0.00047046 rank 5
2023-02-21 09:49:00,513 DEBUG TRAIN Batch 13/2000 loss 23.415539 loss_att 25.661438 loss_ctc 37.360058 loss_rnnt 20.888103 hw_loss 0.410597 lr 0.00047046 rank 6
2023-02-21 09:49:00,569 DEBUG TRAIN Batch 13/2000 loss 9.331656 loss_att 9.748665 loss_ctc 10.612971 loss_rnnt 8.834162 hw_loss 0.456094 lr 0.00047046 rank 3
2023-02-21 09:49:56,657 DEBUG TRAIN Batch 13/2100 loss 22.491108 loss_att 24.843552 loss_ctc 28.366741 loss_rnnt 21.209736 hw_loss 0.051498 lr 0.00047036 rank 2
2023-02-21 09:49:56,661 DEBUG TRAIN Batch 13/2100 loss 3.534880 loss_att 11.262336 loss_ctc 6.073450 loss_rnnt 1.569317 hw_loss 0.152992 lr 0.00047036 rank 5
2023-02-21 09:49:56,666 DEBUG TRAIN Batch 13/2100 loss 7.463402 loss_att 9.484426 loss_ctc 8.033527 loss_rnnt 6.853388 hw_loss 0.243359 lr 0.00047036 rank 7
2023-02-21 09:49:56,666 DEBUG TRAIN Batch 13/2100 loss 13.828061 loss_att 19.541090 loss_ctc 16.313215 loss_rnnt 12.281099 hw_loss 0.136876 lr 0.00047036 rank 4
2023-02-21 09:49:56,670 DEBUG TRAIN Batch 13/2100 loss 7.640136 loss_att 13.473240 loss_ctc 9.324650 loss_rnnt 6.025985 hw_loss 0.417991 lr 0.00047036 rank 1
2023-02-21 09:49:56,670 DEBUG TRAIN Batch 13/2100 loss 30.536921 loss_att 31.915108 loss_ctc 36.358971 loss_rnnt 29.339964 hw_loss 0.271962 lr 0.00047036 rank 0
2023-02-21 09:49:56,672 DEBUG TRAIN Batch 13/2100 loss 34.778812 loss_att 41.618217 loss_ctc 57.070282 loss_rnnt 30.328821 hw_loss 0.206088 lr 0.00047036 rank 6
2023-02-21 09:49:56,673 DEBUG TRAIN Batch 13/2100 loss 7.705887 loss_att 11.555471 loss_ctc 10.869934 loss_rnnt 6.397481 hw_loss 0.218656 lr 0.00047036 rank 3
2023-02-21 09:50:55,273 DEBUG TRAIN Batch 13/2200 loss 3.858325 loss_att 6.473190 loss_ctc 6.034813 loss_rnnt 2.886155 hw_loss 0.298122 lr 0.00047025 rank 2
2023-02-21 09:50:55,273 DEBUG TRAIN Batch 13/2200 loss 8.097380 loss_att 17.691168 loss_ctc 11.669644 loss_rnnt 5.527759 hw_loss 0.327301 lr 0.00047025 rank 7
2023-02-21 09:50:55,274 DEBUG TRAIN Batch 13/2200 loss 3.069930 loss_att 5.017663 loss_ctc 1.825800 loss_rnnt 2.687069 hw_loss 0.298496 lr 0.00047025 rank 0
2023-02-21 09:50:55,277 DEBUG TRAIN Batch 13/2200 loss 4.385683 loss_att 7.151575 loss_ctc 5.410949 loss_rnnt 3.563543 hw_loss 0.247988 lr 0.00047025 rank 6
2023-02-21 09:50:55,279 DEBUG TRAIN Batch 13/2200 loss 3.043422 loss_att 6.747712 loss_ctc 5.297265 loss_rnnt 1.848539 hw_loss 0.287836 lr 0.00047025 rank 4
2023-02-21 09:50:55,279 DEBUG TRAIN Batch 13/2200 loss 24.917582 loss_att 23.560627 loss_ctc 33.443245 loss_rnnt 24.051208 hw_loss 0.001894 lr 0.00047025 rank 3
2023-02-21 09:50:55,281 DEBUG TRAIN Batch 13/2200 loss 34.043541 loss_att 33.730991 loss_ctc 44.943993 loss_rnnt 32.472725 hw_loss 0.337367 lr 0.00047025 rank 5
2023-02-21 09:50:55,287 DEBUG TRAIN Batch 13/2200 loss 31.243429 loss_att 23.108799 loss_ctc 30.181347 loss_rnnt 33.010956 hw_loss 0.001894 lr 0.00047025 rank 1
2023-02-21 09:51:51,001 DEBUG TRAIN Batch 13/2300 loss 17.189739 loss_att 24.293701 loss_ctc 21.343641 loss_rnnt 15.039251 hw_loss 0.329706 lr 0.00047015 rank 0
2023-02-21 09:51:51,008 DEBUG TRAIN Batch 13/2300 loss 6.720557 loss_att 13.333437 loss_ctc 10.802707 loss_rnnt 4.853456 hw_loss 0.000447 lr 0.00047015 rank 2
2023-02-21 09:51:51,013 DEBUG TRAIN Batch 13/2300 loss 12.290851 loss_att 17.716728 loss_ctc 15.491123 loss_rnnt 10.561053 hw_loss 0.408599 lr 0.00047015 rank 4
2023-02-21 09:51:51,013 DEBUG TRAIN Batch 13/2300 loss 6.205297 loss_att 10.366798 loss_ctc 8.158401 loss_rnnt 5.009723 hw_loss 0.192862 lr 0.00047015 rank 3
2023-02-21 09:51:51,015 DEBUG TRAIN Batch 13/2300 loss 3.145916 loss_att 10.743943 loss_ctc 5.241276 loss_rnnt 1.346691 hw_loss 0.000447 lr 0.00047015 rank 7
2023-02-21 09:51:51,017 DEBUG TRAIN Batch 13/2300 loss 11.031925 loss_att 14.015508 loss_ctc 13.202560 loss_rnnt 10.016651 hw_loss 0.242136 lr 0.00047015 rank 6
2023-02-21 09:51:51,018 DEBUG TRAIN Batch 13/2300 loss 24.890532 loss_att 37.510670 loss_ctc 38.758518 loss_rnnt 20.369341 hw_loss 0.277682 lr 0.00047015 rank 5
2023-02-21 09:51:51,019 DEBUG TRAIN Batch 13/2300 loss 5.446080 loss_att 5.591290 loss_ctc 4.213466 loss_rnnt 5.369442 hw_loss 0.397394 lr 0.00047015 rank 1
2023-02-21 09:52:48,015 DEBUG TRAIN Batch 13/2400 loss 15.039399 loss_att 19.572565 loss_ctc 25.279793 loss_rnnt 12.597255 hw_loss 0.318987 lr 0.00047005 rank 2
2023-02-21 09:52:48,016 DEBUG TRAIN Batch 13/2400 loss 19.744179 loss_att 19.966383 loss_ctc 23.678810 loss_rnnt 18.952412 hw_loss 0.417580 lr 0.00047005 rank 5
2023-02-21 09:52:48,022 DEBUG TRAIN Batch 13/2400 loss 12.509064 loss_att 20.129059 loss_ctc 22.162294 loss_rnnt 9.483369 hw_loss 0.402372 lr 0.00047005 rank 0
2023-02-21 09:52:48,023 DEBUG TRAIN Batch 13/2400 loss 22.641325 loss_att 21.080002 loss_ctc 25.252266 loss_rnnt 22.514204 hw_loss 0.171112 lr 0.00047005 rank 3
2023-02-21 09:52:48,026 DEBUG TRAIN Batch 13/2400 loss 13.375338 loss_att 16.178946 loss_ctc 17.195982 loss_rnnt 12.219825 hw_loss 0.160075 lr 0.00047005 rank 7
2023-02-21 09:52:48,027 DEBUG TRAIN Batch 13/2400 loss 10.898118 loss_att 16.720276 loss_ctc 19.227737 loss_rnnt 8.448916 hw_loss 0.326540 lr 0.00047005 rank 6
2023-02-21 09:52:48,033 DEBUG TRAIN Batch 13/2400 loss 11.541970 loss_att 12.702148 loss_ctc 15.948327 loss_rnnt 10.562531 hw_loss 0.299793 lr 0.00047005 rank 1
2023-02-21 09:52:48,079 DEBUG TRAIN Batch 13/2400 loss 29.611452 loss_att 31.413721 loss_ctc 39.078175 loss_rnnt 27.874580 hw_loss 0.214099 lr 0.00047005 rank 4
2023-02-21 09:53:45,983 DEBUG TRAIN Batch 13/2500 loss 2.768054 loss_att 8.021023 loss_ctc 3.213916 loss_rnnt 1.472454 hw_loss 0.347921 lr 0.00046994 rank 2
2023-02-21 09:53:45,988 DEBUG TRAIN Batch 13/2500 loss 27.000195 loss_att 29.578102 loss_ctc 41.010784 loss_rnnt 24.477951 hw_loss 0.259842 lr 0.00046994 rank 0
2023-02-21 09:53:45,988 DEBUG TRAIN Batch 13/2500 loss 27.680531 loss_att 32.061279 loss_ctc 37.884487 loss_rnnt 25.330654 hw_loss 0.212248 lr 0.00046994 rank 3
2023-02-21 09:53:45,992 DEBUG TRAIN Batch 13/2500 loss 13.266769 loss_att 16.803835 loss_ctc 13.418508 loss_rnnt 12.539055 hw_loss 0.000131 lr 0.00046994 rank 4
2023-02-21 09:53:45,993 DEBUG TRAIN Batch 13/2500 loss 5.280581 loss_att 11.466434 loss_ctc 9.576202 loss_rnnt 3.303860 hw_loss 0.312751 lr 0.00046994 rank 7
2023-02-21 09:53:45,994 DEBUG TRAIN Batch 13/2500 loss 4.807871 loss_att 7.168444 loss_ctc 2.466943 loss_rnnt 4.530241 hw_loss 0.220573 lr 0.00046994 rank 6
2023-02-21 09:53:46,005 DEBUG TRAIN Batch 13/2500 loss 7.575831 loss_att 10.366788 loss_ctc 12.543988 loss_rnnt 6.235034 hw_loss 0.225347 lr 0.00046994 rank 5
2023-02-21 09:53:46,031 DEBUG TRAIN Batch 13/2500 loss 5.481287 loss_att 8.961275 loss_ctc 5.689930 loss_rnnt 4.596528 hw_loss 0.301767 lr 0.00046994 rank 1
2023-02-21 09:55:39,484 DEBUG TRAIN Batch 13/2600 loss 9.685083 loss_att 10.454727 loss_ctc 11.516715 loss_rnnt 9.127172 hw_loss 0.299558 lr 0.00046984 rank 0
2023-02-21 09:55:39,492 DEBUG TRAIN Batch 13/2600 loss 17.466116 loss_att 19.226521 loss_ctc 15.827935 loss_rnnt 17.332375 hw_loss 0.000159 lr 0.00046984 rank 5
2023-02-21 09:55:39,495 DEBUG TRAIN Batch 13/2600 loss 11.534038 loss_att 13.497607 loss_ctc 12.175015 loss_rnnt 11.055775 hw_loss 0.000159 lr 0.00046984 rank 4
2023-02-21 09:55:39,496 DEBUG TRAIN Batch 13/2600 loss 13.902934 loss_att 18.732159 loss_ctc 24.087513 loss_rnnt 11.397156 hw_loss 0.341230 lr 0.00046984 rank 2
2023-02-21 09:55:39,499 DEBUG TRAIN Batch 13/2600 loss 5.584218 loss_att 7.652593 loss_ctc 5.545563 loss_rnnt 5.021472 hw_loss 0.289172 lr 0.00046984 rank 7
2023-02-21 09:55:39,500 DEBUG TRAIN Batch 13/2600 loss 18.125912 loss_att 21.255941 loss_ctc 17.946756 loss_rnnt 17.369642 hw_loss 0.289036 lr 0.00046984 rank 1
2023-02-21 09:55:39,501 DEBUG TRAIN Batch 13/2600 loss 9.016187 loss_att 15.600208 loss_ctc 10.929198 loss_rnnt 7.444229 hw_loss 0.000159 lr 0.00046984 rank 6
2023-02-21 09:55:39,520 DEBUG TRAIN Batch 13/2600 loss 6.679258 loss_att 10.546175 loss_ctc 8.740079 loss_rnnt 5.393182 hw_loss 0.446094 lr 0.00046984 rank 3
2023-02-21 09:56:36,109 DEBUG TRAIN Batch 13/2700 loss 2.777137 loss_att 6.654932 loss_ctc 4.635753 loss_rnnt 1.627478 hw_loss 0.236785 lr 0.00046974 rank 2
2023-02-21 09:56:36,110 DEBUG TRAIN Batch 13/2700 loss 6.669983 loss_att 11.211069 loss_ctc 10.294743 loss_rnnt 5.131989 hw_loss 0.274641 lr 0.00046974 rank 3
2023-02-21 09:56:36,111 DEBUG TRAIN Batch 13/2700 loss 21.682562 loss_att 23.726614 loss_ctc 28.905930 loss_rnnt 20.179947 hw_loss 0.245040 lr 0.00046974 rank 5
2023-02-21 09:56:36,117 DEBUG TRAIN Batch 13/2700 loss 16.733442 loss_att 21.334589 loss_ctc 15.978292 loss_rnnt 15.772176 hw_loss 0.265732 lr 0.00046974 rank 7
2023-02-21 09:56:36,118 DEBUG TRAIN Batch 13/2700 loss 10.170530 loss_att 11.242821 loss_ctc 11.800465 loss_rnnt 9.682511 hw_loss 0.105442 lr 0.00046974 rank 6
2023-02-21 09:56:36,119 DEBUG TRAIN Batch 13/2700 loss 6.037147 loss_att 7.882426 loss_ctc 5.291047 loss_rnnt 5.537427 hw_loss 0.431519 lr 0.00046974 rank 0
2023-02-21 09:56:36,120 DEBUG TRAIN Batch 13/2700 loss 8.084083 loss_att 9.352171 loss_ctc 7.781466 loss_rnnt 7.712059 hw_loss 0.297666 lr 0.00046974 rank 4
2023-02-21 09:56:36,128 DEBUG TRAIN Batch 13/2700 loss 11.672910 loss_att 15.165643 loss_ctc 15.334210 loss_rnnt 10.344049 hw_loss 0.266513 lr 0.00046974 rank 1
2023-02-21 09:57:33,150 DEBUG TRAIN Batch 13/2800 loss 16.874651 loss_att 18.358574 loss_ctc 24.024729 loss_rnnt 15.624487 hw_loss 0.000068 lr 0.00046963 rank 2
2023-02-21 09:57:33,155 DEBUG TRAIN Batch 13/2800 loss 2.392958 loss_att 5.524583 loss_ctc 2.128244 loss_rnnt 1.801892 hw_loss 0.000068 lr 0.00046963 rank 5
2023-02-21 09:57:33,157 DEBUG TRAIN Batch 13/2800 loss 11.940943 loss_att 19.456867 loss_ctc 16.511477 loss_rnnt 9.717667 hw_loss 0.207535 lr 0.00046963 rank 7
2023-02-21 09:57:33,157 DEBUG TRAIN Batch 13/2800 loss 3.915391 loss_att 9.181866 loss_ctc 7.307111 loss_rnnt 2.328311 hw_loss 0.152917 lr 0.00046963 rank 1
2023-02-21 09:57:33,158 DEBUG TRAIN Batch 13/2800 loss 27.371784 loss_att 28.550716 loss_ctc 29.161079 loss_rnnt 26.671196 hw_loss 0.424179 lr 0.00046963 rank 0
2023-02-21 09:57:33,160 DEBUG TRAIN Batch 13/2800 loss 10.010141 loss_att 11.605468 loss_ctc 10.665961 loss_rnnt 9.508238 hw_loss 0.178867 lr 0.00046963 rank 6
2023-02-21 09:57:33,163 DEBUG TRAIN Batch 13/2800 loss 28.723511 loss_att 34.609200 loss_ctc 35.701443 loss_rnnt 26.615948 hw_loss 0.000068 lr 0.00046963 rank 3
2023-02-21 09:57:33,209 DEBUG TRAIN Batch 13/2800 loss 14.128291 loss_att 20.435846 loss_ctc 23.754261 loss_rnnt 11.435328 hw_loss 0.277480 lr 0.00046963 rank 4
2023-02-21 09:58:28,407 DEBUG TRAIN Batch 13/2900 loss 25.096895 loss_att 38.078979 loss_ctc 23.911448 loss_rnnt 22.544069 hw_loss 0.214626 lr 0.00046953 rank 2
2023-02-21 09:58:28,414 DEBUG TRAIN Batch 13/2900 loss 9.124924 loss_att 12.691135 loss_ctc 10.751848 loss_rnnt 8.099089 hw_loss 0.179378 lr 0.00046953 rank 4
2023-02-21 09:58:28,415 DEBUG TRAIN Batch 13/2900 loss 18.638430 loss_att 19.045284 loss_ctc 22.321579 loss_rnnt 17.833492 hw_loss 0.435901 lr 0.00046953 rank 0
2023-02-21 09:58:28,417 DEBUG TRAIN Batch 13/2900 loss 24.072580 loss_att 25.168505 loss_ctc 33.120621 loss_rnnt 22.449081 hw_loss 0.371079 lr 0.00046953 rank 3
2023-02-21 09:58:28,421 DEBUG TRAIN Batch 13/2900 loss 12.738300 loss_att 15.660056 loss_ctc 22.316051 loss_rnnt 10.684921 hw_loss 0.359989 lr 0.00046953 rank 1
2023-02-21 09:58:28,422 DEBUG TRAIN Batch 13/2900 loss 11.805599 loss_att 11.871954 loss_ctc 14.884025 loss_rnnt 11.095559 hw_loss 0.536836 lr 0.00046953 rank 5
2023-02-21 09:58:28,424 DEBUG TRAIN Batch 13/2900 loss 17.664484 loss_att 17.716898 loss_ctc 22.551628 loss_rnnt 16.861063 hw_loss 0.264969 lr 0.00046953 rank 7
2023-02-21 09:58:28,426 DEBUG TRAIN Batch 13/2900 loss 9.062681 loss_att 14.032174 loss_ctc 11.510918 loss_rnnt 7.592910 hw_loss 0.280204 lr 0.00046953 rank 6
2023-02-21 09:59:25,603 DEBUG TRAIN Batch 13/3000 loss 3.133476 loss_att 4.483357 loss_ctc 5.636804 loss_rnnt 2.354709 hw_loss 0.328151 lr 0.00046942 rank 2
2023-02-21 09:59:25,607 DEBUG TRAIN Batch 13/3000 loss 17.470606 loss_att 24.544458 loss_ctc 30.768185 loss_rnnt 14.200796 hw_loss 0.153804 lr 0.00046942 rank 4
2023-02-21 09:59:25,610 DEBUG TRAIN Batch 13/3000 loss 13.645867 loss_att 20.874672 loss_ctc 16.054426 loss_rnnt 11.641617 hw_loss 0.445027 lr 0.00046942 rank 3
2023-02-21 09:59:25,613 DEBUG TRAIN Batch 13/3000 loss 16.937836 loss_att 24.770018 loss_ctc 23.465439 loss_rnnt 14.364925 hw_loss 0.255238 lr 0.00046942 rank 7
2023-02-21 09:59:25,614 DEBUG TRAIN Batch 13/3000 loss 21.272079 loss_att 29.927660 loss_ctc 29.557932 loss_rnnt 18.325319 hw_loss 0.207868 lr 0.00046942 rank 0
2023-02-21 09:59:25,619 DEBUG TRAIN Batch 13/3000 loss 27.033897 loss_att 29.351978 loss_ctc 33.281769 loss_rnnt 25.736908 hw_loss 0.000609 lr 0.00046942 rank 5
2023-02-21 09:59:25,619 DEBUG TRAIN Batch 13/3000 loss 7.642167 loss_att 11.241625 loss_ctc 9.884310 loss_rnnt 6.454106 hw_loss 0.317280 lr 0.00046942 rank 1
2023-02-21 09:59:25,624 DEBUG TRAIN Batch 13/3000 loss 23.766808 loss_att 22.598974 loss_ctc 28.096533 loss_rnnt 23.314281 hw_loss 0.203991 lr 0.00046942 rank 6
2023-02-21 10:00:22,290 DEBUG TRAIN Batch 13/3100 loss 7.000417 loss_att 9.188704 loss_ctc 8.932793 loss_rnnt 6.304955 hw_loss 0.000292 lr 0.00046932 rank 0
2023-02-21 10:00:22,291 DEBUG TRAIN Batch 13/3100 loss 12.946456 loss_att 13.905666 loss_ctc 12.177939 loss_rnnt 12.600504 hw_loss 0.481084 lr 0.00046932 rank 4
2023-02-21 10:00:22,292 DEBUG TRAIN Batch 13/3100 loss 24.916981 loss_att 20.511881 loss_ctc 23.884834 loss_rnnt 25.853069 hw_loss 0.154787 lr 0.00046932 rank 5
2023-02-21 10:00:22,294 DEBUG TRAIN Batch 13/3100 loss 18.722998 loss_att 22.438820 loss_ctc 23.139248 loss_rnnt 17.274015 hw_loss 0.219344 lr 0.00046932 rank 3
2023-02-21 10:00:22,295 DEBUG TRAIN Batch 13/3100 loss 30.654961 loss_att 32.767174 loss_ctc 42.681267 loss_rnnt 28.400354 hw_loss 0.428726 lr 0.00046932 rank 1
2023-02-21 10:00:22,299 DEBUG TRAIN Batch 13/3100 loss 5.757665 loss_att 11.856588 loss_ctc 9.969673 loss_rnnt 3.778713 hw_loss 0.370437 lr 0.00046932 rank 6
2023-02-21 10:00:22,301 DEBUG TRAIN Batch 13/3100 loss 15.101755 loss_att 20.165791 loss_ctc 14.031528 loss_rnnt 14.231489 hw_loss 0.000292 lr 0.00046932 rank 7
2023-02-21 10:00:22,343 DEBUG TRAIN Batch 13/3100 loss 28.585686 loss_att 28.307720 loss_ctc 44.392181 loss_rnnt 26.533588 hw_loss 0.000292 lr 0.00046932 rank 2
2023-02-21 10:01:18,855 DEBUG TRAIN Batch 13/3200 loss 29.257130 loss_att 27.967455 loss_ctc 38.464859 loss_rnnt 28.185230 hw_loss 0.191507 lr 0.00046922 rank 2
2023-02-21 10:01:18,856 DEBUG TRAIN Batch 13/3200 loss 25.901501 loss_att 27.697193 loss_ctc 29.423157 loss_rnnt 24.937775 hw_loss 0.253184 lr 0.00046922 rank 0
2023-02-21 10:01:18,856 DEBUG TRAIN Batch 13/3200 loss 25.291504 loss_att 23.574329 loss_ctc 33.824539 loss_rnnt 24.356968 hw_loss 0.262934 lr 0.00046922 rank 1
2023-02-21 10:01:18,858 DEBUG TRAIN Batch 13/3200 loss 21.455627 loss_att 24.516628 loss_ctc 24.354485 loss_rnnt 20.373203 hw_loss 0.156955 lr 0.00046922 rank 4
2023-02-21 10:01:18,858 DEBUG TRAIN Batch 13/3200 loss 12.122102 loss_att 16.746998 loss_ctc 18.020033 loss_rnnt 10.322741 hw_loss 0.164983 lr 0.00046922 rank 7
2023-02-21 10:01:18,862 DEBUG TRAIN Batch 13/3200 loss 13.251936 loss_att 25.046902 loss_ctc 16.718817 loss_rnnt 10.430663 hw_loss 0.000055 lr 0.00046922 rank 3
2023-02-21 10:01:18,864 DEBUG TRAIN Batch 13/3200 loss 8.963590 loss_att 10.790183 loss_ctc 13.473171 loss_rnnt 7.913015 hw_loss 0.157458 lr 0.00046922 rank 5
2023-02-21 10:01:18,865 DEBUG TRAIN Batch 13/3200 loss 17.038239 loss_att 19.965395 loss_ctc 24.078888 loss_rnnt 15.380291 hw_loss 0.250804 lr 0.00046922 rank 6
2023-02-21 10:02:16,076 DEBUG TRAIN Batch 13/3300 loss 4.824857 loss_att 7.818106 loss_ctc 7.577806 loss_rnnt 3.748921 hw_loss 0.206673 lr 0.00046911 rank 2
2023-02-21 10:02:16,079 DEBUG TRAIN Batch 13/3300 loss 11.090578 loss_att 17.346584 loss_ctc 10.376728 loss_rnnt 9.934415 hw_loss 0.000265 lr 0.00046911 rank 6
2023-02-21 10:02:16,081 DEBUG TRAIN Batch 13/3300 loss 14.644894 loss_att 17.603710 loss_ctc 20.420702 loss_rnnt 13.077634 hw_loss 0.385104 lr 0.00046911 rank 4
2023-02-21 10:02:16,083 DEBUG TRAIN Batch 13/3300 loss 9.262732 loss_att 12.534378 loss_ctc 12.025622 loss_rnnt 7.969956 hw_loss 0.506363 lr 0.00046911 rank 3
2023-02-21 10:02:16,086 DEBUG TRAIN Batch 13/3300 loss 15.797308 loss_att 12.868258 loss_ctc 19.491161 loss_rnnt 15.776433 hw_loss 0.214072 lr 0.00046911 rank 7
2023-02-21 10:02:16,090 DEBUG TRAIN Batch 13/3300 loss 7.044415 loss_att 9.830038 loss_ctc 8.353917 loss_rnnt 6.177340 hw_loss 0.253779 lr 0.00046911 rank 0
2023-02-21 10:02:16,090 DEBUG TRAIN Batch 13/3300 loss 8.176416 loss_att 15.372795 loss_ctc 12.526603 loss_rnnt 6.045811 hw_loss 0.208694 lr 0.00046911 rank 5
2023-02-21 10:02:16,097 DEBUG TRAIN Batch 13/3300 loss 11.984198 loss_att 13.535255 loss_ctc 19.996387 loss_rnnt 10.476761 hw_loss 0.241749 lr 0.00046911 rank 1
2023-02-21 10:03:13,325 DEBUG TRAIN Batch 13/3400 loss 9.184553 loss_att 13.192279 loss_ctc 16.757454 loss_rnnt 7.373255 hw_loss 0.000060 lr 0.00046901 rank 5
2023-02-21 10:03:13,328 DEBUG TRAIN Batch 13/3400 loss 51.293423 loss_att 49.811806 loss_ctc 64.373985 loss_rnnt 49.845642 hw_loss 0.000060 lr 0.00046901 rank 2
2023-02-21 10:03:13,329 DEBUG TRAIN Batch 13/3400 loss 13.636315 loss_att 17.382473 loss_ctc 15.133528 loss_rnnt 12.489710 hw_loss 0.370773 lr 0.00046901 rank 3
2023-02-21 10:03:13,333 DEBUG TRAIN Batch 13/3400 loss 22.473618 loss_att 24.017263 loss_ctc 28.985315 loss_rnnt 21.211428 hw_loss 0.159817 lr 0.00046901 rank 4
2023-02-21 10:03:13,336 DEBUG TRAIN Batch 13/3400 loss 1.919246 loss_att 3.886257 loss_ctc 1.262052 loss_rnnt 1.404593 hw_loss 0.391644 lr 0.00046901 rank 6
2023-02-21 10:03:13,341 DEBUG TRAIN Batch 13/3400 loss 2.220756 loss_att 7.253962 loss_ctc 3.179147 loss_rnnt 0.934523 hw_loss 0.284637 lr 0.00046901 rank 0
2023-02-21 10:03:13,344 DEBUG TRAIN Batch 13/3400 loss 13.664905 loss_att 20.313766 loss_ctc 16.629930 loss_rnnt 11.823677 hw_loss 0.217722 lr 0.00046901 rank 7
2023-02-21 10:03:13,346 DEBUG TRAIN Batch 13/3400 loss 31.219698 loss_att 27.090101 loss_ctc 38.034401 loss_rnnt 31.136961 hw_loss 0.000060 lr 0.00046901 rank 1
2023-02-21 10:05:06,650 DEBUG TRAIN Batch 13/3500 loss 20.213373 loss_att 23.697548 loss_ctc 31.894249 loss_rnnt 17.862921 hw_loss 0.180315 lr 0.00046891 rank 0
2023-02-21 10:05:06,651 DEBUG TRAIN Batch 13/3500 loss 15.585780 loss_att 15.519127 loss_ctc 17.967932 loss_rnnt 15.162377 hw_loss 0.223336 lr 0.00046891 rank 3
2023-02-21 10:05:06,657 DEBUG TRAIN Batch 13/3500 loss 16.082623 loss_att 18.057875 loss_ctc 16.851757 loss_rnnt 15.508640 hw_loss 0.143211 lr 0.00046891 rank 1
2023-02-21 10:05:06,657 DEBUG TRAIN Batch 13/3500 loss 13.589655 loss_att 16.424370 loss_ctc 22.602554 loss_rnnt 11.664894 hw_loss 0.292683 lr 0.00046891 rank 4
2023-02-21 10:05:06,660 DEBUG TRAIN Batch 13/3500 loss 5.911597 loss_att 8.419296 loss_ctc 9.023865 loss_rnnt 4.939612 hw_loss 0.104016 lr 0.00046891 rank 2
2023-02-21 10:05:06,661 DEBUG TRAIN Batch 13/3500 loss 11.690762 loss_att 14.185289 loss_ctc 16.191458 loss_rnnt 10.349573 hw_loss 0.454105 lr 0.00046891 rank 5
2023-02-21 10:05:06,662 DEBUG TRAIN Batch 13/3500 loss 11.103909 loss_att 14.203135 loss_ctc 12.596003 loss_rnnt 10.066090 hw_loss 0.410678 lr 0.00046891 rank 7
2023-02-21 10:05:06,668 DEBUG TRAIN Batch 13/3500 loss 8.245390 loss_att 9.307615 loss_ctc 14.806948 loss_rnnt 6.937282 hw_loss 0.413982 lr 0.00046891 rank 6
2023-02-21 10:06:04,738 DEBUG TRAIN Batch 13/3600 loss 7.624871 loss_att 15.741436 loss_ctc 10.201457 loss_rnnt 5.403191 hw_loss 0.477792 lr 0.00046881 rank 4
2023-02-21 10:06:04,739 DEBUG TRAIN Batch 13/3600 loss 4.532606 loss_att 6.893944 loss_ctc 6.258443 loss_rnnt 3.705356 hw_loss 0.234133 lr 0.00046881 rank 2
2023-02-21 10:06:04,746 DEBUG TRAIN Batch 13/3600 loss 6.198732 loss_att 10.298422 loss_ctc 11.018949 loss_rnnt 4.562820 hw_loss 0.324898 lr 0.00046881 rank 3
2023-02-21 10:06:04,748 DEBUG TRAIN Batch 13/3600 loss 12.497186 loss_att 18.024792 loss_ctc 28.083031 loss_rnnt 9.119258 hw_loss 0.364301 lr 0.00046881 rank 7
2023-02-21 10:06:04,749 DEBUG TRAIN Batch 13/3600 loss 19.285488 loss_att 21.291466 loss_ctc 22.104504 loss_rnnt 18.405952 hw_loss 0.192133 lr 0.00046881 rank 5
2023-02-21 10:06:04,749 DEBUG TRAIN Batch 13/3600 loss 8.015917 loss_att 13.466951 loss_ctc 11.568134 loss_rnnt 6.331449 hw_loss 0.226183 lr 0.00046881 rank 6
2023-02-21 10:06:04,757 DEBUG TRAIN Batch 13/3600 loss 5.925168 loss_att 9.320246 loss_ctc 8.117613 loss_rnnt 4.907787 hw_loss 0.086321 lr 0.00046881 rank 0
2023-02-21 10:06:04,762 DEBUG TRAIN Batch 13/3600 loss 4.616427 loss_att 8.102931 loss_ctc 6.455491 loss_rnnt 3.422094 hw_loss 0.472171 lr 0.00046881 rank 1
2023-02-21 10:07:00,791 DEBUG TRAIN Batch 13/3700 loss 14.663404 loss_att 18.018251 loss_ctc 17.579063 loss_rnnt 13.469352 hw_loss 0.251868 lr 0.00046870 rank 4
2023-02-21 10:07:00,793 DEBUG TRAIN Batch 13/3700 loss 15.412642 loss_att 26.465759 loss_ctc 19.629091 loss_rnnt 12.460785 hw_loss 0.335700 lr 0.00046870 rank 5
2023-02-21 10:07:00,795 DEBUG TRAIN Batch 13/3700 loss 17.970839 loss_att 29.028639 loss_ctc 27.442713 loss_rnnt 14.356743 hw_loss 0.261785 lr 0.00046870 rank 2
2023-02-21 10:07:00,797 DEBUG TRAIN Batch 13/3700 loss 14.769032 loss_att 18.850838 loss_ctc 20.700558 loss_rnnt 13.051293 hw_loss 0.207199 lr 0.00046870 rank 6
2023-02-21 10:07:00,800 DEBUG TRAIN Batch 13/3700 loss 1.654565 loss_att 6.475317 loss_ctc 2.040377 loss_rnnt 0.452544 hw_loss 0.349554 lr 0.00046870 rank 3
2023-02-21 10:07:00,801 DEBUG TRAIN Batch 13/3700 loss 14.840865 loss_att 19.411198 loss_ctc 22.043100 loss_rnnt 12.966433 hw_loss 0.000126 lr 0.00046870 rank 1
2023-02-21 10:07:00,820 DEBUG TRAIN Batch 13/3700 loss 8.805378 loss_att 14.116278 loss_ctc 13.495877 loss_rnnt 7.008614 hw_loss 0.204721 lr 0.00046870 rank 0
2023-02-21 10:07:00,821 DEBUG TRAIN Batch 13/3700 loss 15.031279 loss_att 15.844058 loss_ctc 19.938442 loss_rnnt 14.055742 hw_loss 0.297547 lr 0.00046870 rank 7
2023-02-21 10:07:57,556 DEBUG TRAIN Batch 13/3800 loss 25.138901 loss_att 25.144972 loss_ctc 36.495964 loss_rnnt 23.455223 hw_loss 0.315354 lr 0.00046860 rank 2
2023-02-21 10:07:57,555 DEBUG TRAIN Batch 13/3800 loss 11.969104 loss_att 19.349092 loss_ctc 23.085247 loss_rnnt 8.875628 hw_loss 0.253738 lr 0.00046860 rank 7
2023-02-21 10:07:57,556 DEBUG TRAIN Batch 13/3800 loss 6.433006 loss_att 10.099754 loss_ctc 8.033391 loss_rnnt 5.278251 hw_loss 0.390039 lr 0.00046860 rank 0
2023-02-21 10:07:57,560 DEBUG TRAIN Batch 13/3800 loss 10.290950 loss_att 14.078857 loss_ctc 8.232193 loss_rnnt 9.774105 hw_loss 0.063307 lr 0.00046860 rank 6
2023-02-21 10:07:57,561 DEBUG TRAIN Batch 13/3800 loss 20.435715 loss_att 19.972406 loss_ctc 20.568972 loss_rnnt 20.460266 hw_loss 0.094393 lr 0.00046860 rank 4
2023-02-21 10:07:57,566 DEBUG TRAIN Batch 13/3800 loss 27.081079 loss_att 33.494667 loss_ctc 36.025597 loss_rnnt 24.459768 hw_loss 0.273733 lr 0.00046860 rank 3
2023-02-21 10:07:57,570 DEBUG TRAIN Batch 13/3800 loss 12.302279 loss_att 19.497829 loss_ctc 16.633698 loss_rnnt 10.195156 hw_loss 0.169669 lr 0.00046860 rank 1
2023-02-21 10:07:57,571 DEBUG TRAIN Batch 13/3800 loss 20.990084 loss_att 23.935383 loss_ctc 24.680201 loss_rnnt 19.839445 hw_loss 0.130434 lr 0.00046860 rank 5
2023-02-21 10:08:56,360 DEBUG TRAIN Batch 13/3900 loss 3.971925 loss_att 12.766951 loss_ctc 5.592650 loss_rnnt 1.920123 hw_loss 0.143812 lr 0.00046850 rank 4
2023-02-21 10:08:56,360 DEBUG TRAIN Batch 13/3900 loss 12.143761 loss_att 14.716760 loss_ctc 15.449007 loss_rnnt 10.935161 hw_loss 0.474939 lr 0.00046850 rank 2
2023-02-21 10:08:56,366 DEBUG TRAIN Batch 13/3900 loss 21.602406 loss_att 32.424767 loss_ctc 30.764091 loss_rnnt 18.041046 hw_loss 0.328739 lr 0.00046850 rank 3
2023-02-21 10:08:56,366 DEBUG TRAIN Batch 13/3900 loss 13.596968 loss_att 13.345108 loss_ctc 14.235960 loss_rnnt 13.483942 hw_loss 0.146620 lr 0.00046850 rank 7
2023-02-21 10:08:56,367 DEBUG TRAIN Batch 13/3900 loss 1.629658 loss_att 5.281238 loss_ctc 3.167932 loss_rnnt 0.533776 hw_loss 0.300867 lr 0.00046850 rank 0
2023-02-21 10:08:56,368 DEBUG TRAIN Batch 13/3900 loss 20.486551 loss_att 18.804451 loss_ctc 22.991083 loss_rnnt 20.359959 hw_loss 0.242016 lr 0.00046850 rank 5
2023-02-21 10:08:56,369 DEBUG TRAIN Batch 13/3900 loss 10.151836 loss_att 19.000381 loss_ctc 19.947975 loss_rnnt 6.852772 hw_loss 0.418505 lr 0.00046850 rank 6
2023-02-21 10:08:56,378 DEBUG TRAIN Batch 13/3900 loss 14.759931 loss_att 20.932571 loss_ctc 13.345518 loss_rnnt 13.514570 hw_loss 0.373914 lr 0.00046850 rank 1
2023-02-21 10:09:51,070 DEBUG TRAIN Batch 13/4000 loss 14.235602 loss_att 27.804188 loss_ctc 14.646425 loss_rnnt 11.288955 hw_loss 0.334039 lr 0.00046839 rank 5
2023-02-21 10:09:51,072 DEBUG TRAIN Batch 13/4000 loss 5.908038 loss_att 13.463803 loss_ctc 6.385313 loss_rnnt 4.333211 hw_loss 0.000068 lr 0.00046839 rank 2
2023-02-21 10:09:51,072 DEBUG TRAIN Batch 13/4000 loss 7.460917 loss_att 10.939386 loss_ctc 7.306494 loss_rnnt 6.688840 hw_loss 0.181823 lr 0.00046839 rank 0
2023-02-21 10:09:51,073 DEBUG TRAIN Batch 13/4000 loss 12.641195 loss_att 12.319522 loss_ctc 13.905016 loss_rnnt 12.254653 hw_loss 0.529438 lr 0.00046839 rank 3
2023-02-21 10:09:51,078 DEBUG TRAIN Batch 13/4000 loss 3.673860 loss_att 8.886925 loss_ctc 6.044147 loss_rnnt 2.225457 hw_loss 0.168284 lr 0.00046839 rank 7
2023-02-21 10:09:51,085 DEBUG TRAIN Batch 13/4000 loss 28.919941 loss_att 31.212507 loss_ctc 57.699348 loss_rnnt 24.624138 hw_loss 0.000068 lr 0.00046839 rank 6
2023-02-21 10:09:51,091 DEBUG TRAIN Batch 13/4000 loss 11.968501 loss_att 17.474447 loss_ctc 13.097572 loss_rnnt 10.516321 hw_loss 0.375837 lr 0.00046839 rank 1
2023-02-21 10:09:51,135 DEBUG TRAIN Batch 13/4000 loss 16.810711 loss_att 14.083370 loss_ctc 19.446518 loss_rnnt 16.781954 hw_loss 0.417719 lr 0.00046839 rank 4
2023-02-21 10:10:47,320 DEBUG TRAIN Batch 13/4100 loss 21.436190 loss_att 26.612387 loss_ctc 22.921173 loss_rnnt 19.955822 hw_loss 0.463372 lr 0.00046829 rank 0
2023-02-21 10:10:47,323 DEBUG TRAIN Batch 13/4100 loss 5.337646 loss_att 9.257556 loss_ctc 6.667233 loss_rnnt 4.266498 hw_loss 0.206039 lr 0.00046829 rank 2
2023-02-21 10:10:47,323 DEBUG TRAIN Batch 13/4100 loss 8.597478 loss_att 8.559365 loss_ctc 10.410759 loss_rnnt 8.257272 hw_loss 0.198859 lr 0.00046829 rank 4
2023-02-21 10:10:47,326 DEBUG TRAIN Batch 13/4100 loss 9.516569 loss_att 13.494081 loss_ctc 13.826770 loss_rnnt 7.997959 hw_loss 0.278277 lr 0.00046829 rank 7
2023-02-21 10:10:47,326 DEBUG TRAIN Batch 13/4100 loss 11.699279 loss_att 12.734708 loss_ctc 19.670851 loss_rnnt 10.297212 hw_loss 0.247697 lr 0.00046829 rank 5
2023-02-21 10:10:47,329 DEBUG TRAIN Batch 13/4100 loss 28.039433 loss_att 28.367727 loss_ctc 37.551117 loss_rnnt 26.488632 hw_loss 0.406717 lr 0.00046829 rank 6
2023-02-21 10:10:47,329 DEBUG TRAIN Batch 13/4100 loss 8.479273 loss_att 12.622675 loss_ctc 9.386612 loss_rnnt 7.421949 hw_loss 0.201872 lr 0.00046829 rank 3
2023-02-21 10:10:47,335 DEBUG TRAIN Batch 13/4100 loss 7.251801 loss_att 9.671987 loss_ctc 8.578383 loss_rnnt 6.560954 hw_loss 0.056122 lr 0.00046829 rank 1
2023-02-21 10:11:45,697 DEBUG TRAIN Batch 13/4200 loss 10.009098 loss_att 12.836464 loss_ctc 16.242842 loss_rnnt 8.493711 hw_loss 0.222653 lr 0.00046819 rank 5
2023-02-21 10:11:45,697 DEBUG TRAIN Batch 13/4200 loss 7.345843 loss_att 9.399782 loss_ctc 6.881853 loss_rnnt 6.939457 hw_loss 0.107745 lr 0.00046819 rank 3
2023-02-21 10:11:45,698 DEBUG TRAIN Batch 13/4200 loss 6.800145 loss_att 12.102103 loss_ctc 8.944042 loss_rnnt 5.284178 hw_loss 0.318230 lr 0.00046819 rank 0
2023-02-21 10:11:45,703 DEBUG TRAIN Batch 13/4200 loss 11.843586 loss_att 24.154484 loss_ctc 16.952940 loss_rnnt 8.533939 hw_loss 0.311662 lr 0.00046819 rank 6
2023-02-21 10:11:45,703 DEBUG TRAIN Batch 13/4200 loss 25.984098 loss_att 25.021563 loss_ctc 29.117201 loss_rnnt 25.757996 hw_loss 0.001617 lr 0.00046819 rank 7
2023-02-21 10:11:45,706 DEBUG TRAIN Batch 13/4200 loss 16.346535 loss_att 22.361300 loss_ctc 27.985300 loss_rnnt 13.450905 hw_loss 0.264079 lr 0.00046819 rank 4
2023-02-21 10:11:45,714 DEBUG TRAIN Batch 13/4200 loss 4.253268 loss_att 7.702723 loss_ctc 12.392808 loss_rnnt 2.284761 hw_loss 0.362521 lr 0.00046819 rank 1
2023-02-21 10:11:45,756 DEBUG TRAIN Batch 13/4200 loss 20.662596 loss_att 23.613173 loss_ctc 28.546255 loss_rnnt 18.821445 hw_loss 0.374781 lr 0.00046819 rank 2
2023-02-21 10:13:39,366 DEBUG TRAIN Batch 13/4300 loss 5.328847 loss_att 5.419339 loss_ctc 2.204054 loss_rnnt 5.727349 hw_loss 0.000071 lr 0.00046809 rank 5
2023-02-21 10:13:39,368 DEBUG TRAIN Batch 13/4300 loss 33.914585 loss_att 53.114044 loss_ctc 44.108597 loss_rnnt 28.588062 hw_loss 0.238921 lr 0.00046809 rank 2
2023-02-21 10:13:39,372 DEBUG TRAIN Batch 13/4300 loss 16.332626 loss_att 16.507465 loss_ctc 19.945650 loss_rnnt 15.592639 hw_loss 0.418653 lr 0.00046809 rank 4
2023-02-21 10:13:39,372 DEBUG TRAIN Batch 13/4300 loss 3.582782 loss_att 8.498321 loss_ctc 7.633564 loss_rnnt 2.059531 hw_loss 0.000071 lr 0.00046809 rank 0
2023-02-21 10:13:39,377 DEBUG TRAIN Batch 13/4300 loss 11.198008 loss_att 11.491344 loss_ctc 13.558692 loss_rnnt 10.678902 hw_loss 0.273152 lr 0.00046809 rank 3
2023-02-21 10:13:39,378 DEBUG TRAIN Batch 13/4300 loss 14.548446 loss_att 16.334959 loss_ctc 18.177734 loss_rnnt 13.520504 hw_loss 0.350126 lr 0.00046809 rank 6
2023-02-21 10:13:39,400 DEBUG TRAIN Batch 13/4300 loss 3.067953 loss_att 11.203278 loss_ctc 5.919129 loss_rnnt 0.879420 hw_loss 0.339959 lr 0.00046809 rank 7
2023-02-21 10:13:39,421 DEBUG TRAIN Batch 13/4300 loss 2.392472 loss_att 5.349179 loss_ctc 0.891745 loss_rnnt 1.902850 hw_loss 0.184456 lr 0.00046809 rank 1
2023-02-21 10:14:36,722 DEBUG TRAIN Batch 13/4400 loss 5.601347 loss_att 8.247377 loss_ctc 5.404191 loss_rnnt 5.029552 hw_loss 0.129144 lr 0.00046798 rank 4
2023-02-21 10:14:36,723 DEBUG TRAIN Batch 13/4400 loss 12.291950 loss_att 15.860149 loss_ctc 11.412949 loss_rnnt 11.560602 hw_loss 0.252952 lr 0.00046798 rank 6
2023-02-21 10:14:36,727 DEBUG TRAIN Batch 13/4400 loss 9.786252 loss_att 12.257109 loss_ctc 9.809925 loss_rnnt 9.142097 hw_loss 0.275299 lr 0.00046798 rank 2
2023-02-21 10:14:36,727 DEBUG TRAIN Batch 13/4400 loss 11.825966 loss_att 14.272806 loss_ctc 13.992502 loss_rnnt 10.953612 hw_loss 0.176463 lr 0.00046798 rank 7
2023-02-21 10:14:36,730 DEBUG TRAIN Batch 13/4400 loss 24.485432 loss_att 25.996929 loss_ctc 29.319374 loss_rnnt 23.286804 hw_loss 0.472127 lr 0.00046798 rank 0
2023-02-21 10:14:36,732 DEBUG TRAIN Batch 13/4400 loss 7.652513 loss_att 12.017402 loss_ctc 7.597367 loss_rnnt 6.706566 hw_loss 0.150604 lr 0.00046798 rank 3
2023-02-21 10:14:36,733 DEBUG TRAIN Batch 13/4400 loss 15.971818 loss_att 15.286085 loss_ctc 16.718979 loss_rnnt 15.859362 hw_loss 0.281213 lr 0.00046798 rank 5
2023-02-21 10:14:36,738 DEBUG TRAIN Batch 13/4400 loss 8.619007 loss_att 11.180111 loss_ctc 9.070934 loss_rnnt 7.904229 hw_loss 0.266813 lr 0.00046798 rank 1
2023-02-21 10:15:34,909 DEBUG TRAIN Batch 13/4500 loss 5.716471 loss_att 11.676981 loss_ctc 8.926139 loss_rnnt 3.862240 hw_loss 0.439074 lr 0.00046788 rank 2
2023-02-21 10:15:34,910 DEBUG TRAIN Batch 13/4500 loss 6.925591 loss_att 9.065334 loss_ctc 7.497472 loss_rnnt 6.261324 hw_loss 0.300126 lr 0.00046788 rank 0
2023-02-21 10:15:34,911 DEBUG TRAIN Batch 13/4500 loss 15.750943 loss_att 18.212681 loss_ctc 19.807922 loss_rnnt 14.590434 hw_loss 0.238559 lr 0.00046788 rank 5
2023-02-21 10:15:34,913 DEBUG TRAIN Batch 13/4500 loss 20.170393 loss_att 23.332485 loss_ctc 23.020443 loss_rnnt 18.919678 hw_loss 0.446795 lr 0.00046788 rank 4
2023-02-21 10:15:34,914 DEBUG TRAIN Batch 13/4500 loss 8.259405 loss_att 10.029825 loss_ctc 8.472899 loss_rnnt 7.816964 hw_loss 0.112294 lr 0.00046788 rank 3
2023-02-21 10:15:34,919 DEBUG TRAIN Batch 13/4500 loss 34.570572 loss_att 40.976250 loss_ctc 42.282795 loss_rnnt 32.260990 hw_loss 0.000281 lr 0.00046788 rank 1
2023-02-21 10:15:34,920 DEBUG TRAIN Batch 13/4500 loss 10.296632 loss_att 19.381472 loss_ctc 13.255575 loss_rnnt 7.885019 hw_loss 0.375220 lr 0.00046788 rank 7
2023-02-21 10:15:34,926 DEBUG TRAIN Batch 13/4500 loss 13.070575 loss_att 15.095122 loss_ctc 21.799494 loss_rnnt 11.356079 hw_loss 0.273245 lr 0.00046788 rank 6
2023-02-21 10:16:29,753 DEBUG TRAIN Batch 13/4600 loss 13.570646 loss_att 19.000498 loss_ctc 14.587150 loss_rnnt 12.157394 hw_loss 0.359526 lr 0.00046778 rank 2
2023-02-21 10:16:29,760 DEBUG TRAIN Batch 13/4600 loss 6.978392 loss_att 12.527117 loss_ctc 8.925382 loss_rnnt 5.314825 hw_loss 0.551668 lr 0.00046778 rank 0
2023-02-21 10:16:29,760 DEBUG TRAIN Batch 13/4600 loss 3.766895 loss_att 6.880102 loss_ctc 5.916631 loss_rnnt 2.680470 hw_loss 0.332159 lr 0.00046778 rank 5
2023-02-21 10:16:29,762 DEBUG TRAIN Batch 13/4600 loss 11.374187 loss_att 12.058874 loss_ctc 13.307974 loss_rnnt 10.763459 hw_loss 0.404910 lr 0.00046778 rank 3
2023-02-21 10:16:29,761 DEBUG TRAIN Batch 13/4600 loss 8.050349 loss_att 11.824691 loss_ctc 13.222038 loss_rnnt 6.499087 hw_loss 0.200316 lr 0.00046778 rank 6
2023-02-21 10:16:29,789 DEBUG TRAIN Batch 13/4600 loss 4.779536 loss_att 7.354972 loss_ctc 6.468879 loss_rnnt 3.863285 hw_loss 0.329847 lr 0.00046778 rank 1
2023-02-21 10:16:29,804 DEBUG TRAIN Batch 13/4600 loss 12.172540 loss_att 15.373305 loss_ctc 13.988664 loss_rnnt 11.075729 hw_loss 0.402200 lr 0.00046778 rank 7
2023-02-21 10:16:29,819 DEBUG TRAIN Batch 13/4600 loss 10.138717 loss_att 13.812023 loss_ctc 15.788972 loss_rnnt 8.475499 hw_loss 0.328477 lr 0.00046778 rank 4
2023-02-21 10:17:27,147 DEBUG TRAIN Batch 13/4700 loss 13.420577 loss_att 13.914679 loss_ctc 18.835985 loss_rnnt 12.449463 hw_loss 0.281698 lr 0.00046768 rank 7
2023-02-21 10:17:27,148 DEBUG TRAIN Batch 13/4700 loss 28.928190 loss_att 29.399536 loss_ctc 34.079338 loss_rnnt 28.054596 hw_loss 0.173443 lr 0.00046768 rank 2
2023-02-21 10:17:27,150 DEBUG TRAIN Batch 13/4700 loss 13.100441 loss_att 20.764761 loss_ctc 20.751387 loss_rnnt 10.446182 hw_loss 0.189876 lr 0.00046768 rank 5
2023-02-21 10:17:27,152 DEBUG TRAIN Batch 13/4700 loss 30.177893 loss_att 41.910469 loss_ctc 35.919319 loss_rnnt 26.833433 hw_loss 0.435787 lr 0.00046768 rank 6
2023-02-21 10:17:27,151 DEBUG TRAIN Batch 13/4700 loss 18.021091 loss_att 24.589031 loss_ctc 20.084814 loss_rnnt 16.348700 hw_loss 0.156826 lr 0.00046768 rank 4
2023-02-21 10:17:27,153 DEBUG TRAIN Batch 13/4700 loss 22.973839 loss_att 36.253418 loss_ctc 33.202335 loss_rnnt 18.954050 hw_loss 0.000135 lr 0.00046768 rank 0
2023-02-21 10:17:27,169 DEBUG TRAIN Batch 13/4700 loss 7.241927 loss_att 9.356477 loss_ctc 11.118016 loss_rnnt 6.194968 hw_loss 0.201070 lr 0.00046768 rank 1
2023-02-21 10:17:27,207 DEBUG TRAIN Batch 13/4700 loss 11.016545 loss_att 12.367876 loss_ctc 13.362251 loss_rnnt 10.306773 hw_loss 0.237646 lr 0.00046768 rank 3
2023-02-21 10:18:25,280 DEBUG TRAIN Batch 13/4800 loss 1.574752 loss_att 4.856071 loss_ctc 1.820707 loss_rnnt 0.657332 hw_loss 0.428179 lr 0.00046757 rank 2
2023-02-21 10:18:25,285 DEBUG TRAIN Batch 13/4800 loss 5.856308 loss_att 7.939778 loss_ctc 5.420280 loss_rnnt 5.445233 hw_loss 0.098468 lr 0.00046757 rank 4
2023-02-21 10:18:25,287 DEBUG TRAIN Batch 13/4800 loss 1.952227 loss_att 4.049547 loss_ctc 2.244826 loss_rnnt 1.273252 hw_loss 0.413433 lr 0.00046757 rank 5
2023-02-21 10:18:25,287 DEBUG TRAIN Batch 13/4800 loss 3.143309 loss_att 5.669065 loss_ctc 3.772076 loss_rnnt 2.420665 hw_loss 0.250606 lr 0.00046757 rank 0
2023-02-21 10:18:25,289 DEBUG TRAIN Batch 13/4800 loss 18.370600 loss_att 56.130146 loss_ctc 28.103399 loss_rnnt 9.520958 hw_loss 0.000047 lr 0.00046757 rank 3
2023-02-21 10:18:25,291 DEBUG TRAIN Batch 13/4800 loss 20.676846 loss_att 22.441238 loss_ctc 20.104181 loss_rnnt 20.400295 hw_loss 0.000047 lr 0.00046757 rank 6
2023-02-21 10:18:25,293 DEBUG TRAIN Batch 13/4800 loss 7.887154 loss_att 13.753284 loss_ctc 7.721825 loss_rnnt 6.627669 hw_loss 0.203066 lr 0.00046757 rank 7
2023-02-21 10:18:25,294 DEBUG TRAIN Batch 13/4800 loss 15.060097 loss_att 13.729803 loss_ctc 17.725191 loss_rnnt 14.839426 hw_loss 0.246343 lr 0.00046757 rank 1
2023-02-21 10:19:20,931 DEBUG TRAIN Batch 13/4900 loss 22.489149 loss_att 22.903835 loss_ctc 38.218418 loss_rnnt 20.112022 hw_loss 0.369286 lr 0.00046747 rank 4
2023-02-21 10:19:20,932 DEBUG TRAIN Batch 13/4900 loss 13.641825 loss_att 12.915375 loss_ctc 16.126377 loss_rnnt 13.193881 hw_loss 0.491175 lr 0.00046747 rank 0
2023-02-21 10:19:20,935 DEBUG TRAIN Batch 13/4900 loss 10.034342 loss_att 10.300681 loss_ctc 13.249226 loss_rnnt 9.476881 hw_loss 0.141643 lr 0.00046747 rank 3
2023-02-21 10:19:20,934 DEBUG TRAIN Batch 13/4900 loss 22.176867 loss_att 22.389807 loss_ctc 22.519995 loss_rnnt 21.963463 hw_loss 0.234500 lr 0.00046747 rank 2
2023-02-21 10:19:20,936 DEBUG TRAIN Batch 13/4900 loss 23.056320 loss_att 30.564857 loss_ctc 35.736267 loss_rnnt 19.782734 hw_loss 0.152288 lr 0.00046747 rank 5
2023-02-21 10:19:20,938 DEBUG TRAIN Batch 13/4900 loss 8.835983 loss_att 9.030657 loss_ctc 11.170354 loss_rnnt 8.301084 hw_loss 0.346342 lr 0.00046747 rank 7
2023-02-21 10:19:20,941 DEBUG TRAIN Batch 13/4900 loss 5.152107 loss_att 6.390488 loss_ctc 5.567888 loss_rnnt 4.701845 hw_loss 0.275904 lr 0.00046747 rank 6
2023-02-21 10:19:20,950 DEBUG TRAIN Batch 13/4900 loss 26.833214 loss_att 26.826168 loss_ctc 29.613432 loss_rnnt 26.283524 hw_loss 0.338254 lr 0.00046747 rank 1
2023-02-21 10:20:17,453 DEBUG TRAIN Batch 13/5000 loss 14.898879 loss_att 18.903835 loss_ctc 15.496429 loss_rnnt 13.839036 hw_loss 0.335961 lr 0.00046737 rank 0
2023-02-21 10:20:17,454 DEBUG TRAIN Batch 13/5000 loss 16.488247 loss_att 19.412636 loss_ctc 20.185520 loss_rnnt 15.161043 hw_loss 0.467542 lr 0.00046737 rank 5
2023-02-21 10:20:17,457 DEBUG TRAIN Batch 13/5000 loss 23.923006 loss_att 26.224974 loss_ctc 29.531015 loss_rnnt 22.541260 hw_loss 0.325533 lr 0.00046737 rank 2
2023-02-21 10:20:17,460 DEBUG TRAIN Batch 13/5000 loss 5.189657 loss_att 9.122718 loss_ctc 5.786062 loss_rnnt 4.203960 hw_loss 0.224182 lr 0.00046737 rank 3
2023-02-21 10:20:17,460 DEBUG TRAIN Batch 13/5000 loss 29.011160 loss_att 37.486965 loss_ctc 40.362499 loss_rnnt 25.608223 hw_loss 0.364245 lr 0.00046737 rank 4
2023-02-21 10:20:17,464 DEBUG TRAIN Batch 13/5000 loss 7.316147 loss_att 9.339676 loss_ctc 8.004425 loss_rnnt 6.584048 hw_loss 0.441792 lr 0.00046737 rank 1
2023-02-21 10:20:17,468 DEBUG TRAIN Batch 13/5000 loss 7.361308 loss_att 10.722741 loss_ctc 5.772232 loss_rnnt 6.734255 hw_loss 0.312456 lr 0.00046737 rank 7
2023-02-21 10:20:17,470 DEBUG TRAIN Batch 13/5000 loss 19.795780 loss_att 24.901386 loss_ctc 19.099676 loss_rnnt 18.639870 hw_loss 0.426756 lr 0.00046737 rank 6
2023-02-21 10:21:15,476 DEBUG TRAIN Batch 13/5100 loss 23.660408 loss_att 27.222738 loss_ctc 45.172050 loss_rnnt 20.025484 hw_loss 0.101697 lr 0.00046727 rank 2
2023-02-21 10:21:15,479 DEBUG TRAIN Batch 13/5100 loss 16.948202 loss_att 20.366476 loss_ctc 23.448740 loss_rnnt 15.275492 hw_loss 0.229343 lr 0.00046727 rank 0
2023-02-21 10:21:15,484 DEBUG TRAIN Batch 13/5100 loss 18.437918 loss_att 26.597166 loss_ctc 19.225811 loss_rnnt 16.700829 hw_loss 0.000353 lr 0.00046727 rank 5
2023-02-21 10:21:15,486 DEBUG TRAIN Batch 13/5100 loss 6.453296 loss_att 8.828032 loss_ctc 7.132299 loss_rnnt 5.690093 hw_loss 0.370727 lr 0.00046727 rank 3
2023-02-21 10:21:15,488 DEBUG TRAIN Batch 13/5100 loss 9.196351 loss_att 11.398251 loss_ctc 13.405023 loss_rnnt 8.194627 hw_loss 0.000353 lr 0.00046727 rank 6
2023-02-21 10:21:15,498 DEBUG TRAIN Batch 13/5100 loss 26.751343 loss_att 26.426029 loss_ctc 27.516020 loss_rnnt 26.714260 hw_loss 0.000353 lr 0.00046727 rank 1
2023-02-21 10:21:15,508 DEBUG TRAIN Batch 13/5100 loss 27.463737 loss_att 25.464783 loss_ctc 24.143642 loss_rnnt 28.069836 hw_loss 0.443200 lr 0.00046727 rank 7
2023-02-21 10:21:15,540 DEBUG TRAIN Batch 13/5100 loss 21.053446 loss_att 20.364090 loss_ctc 23.139936 loss_rnnt 20.912930 hw_loss 0.000353 lr 0.00046727 rank 4
2023-02-21 10:23:09,209 DEBUG TRAIN Batch 13/5200 loss 1.734650 loss_att 5.876237 loss_ctc 2.055337 loss_rnnt 0.645098 hw_loss 0.409643 lr 0.00046717 rank 2
2023-02-21 10:23:09,216 DEBUG TRAIN Batch 13/5200 loss 15.977858 loss_att 16.535105 loss_ctc 22.683882 loss_rnnt 14.881639 hw_loss 0.169934 lr 0.00046717 rank 4
2023-02-21 10:23:09,218 DEBUG TRAIN Batch 13/5200 loss 16.786011 loss_att 23.487804 loss_ctc 24.593483 loss_rnnt 14.178933 hw_loss 0.423227 lr 0.00046717 rank 5
2023-02-21 10:23:09,221 DEBUG TRAIN Batch 13/5200 loss 11.846557 loss_att 12.654328 loss_ctc 12.746864 loss_rnnt 11.493387 hw_loss 0.134199 lr 0.00046717 rank 3
2023-02-21 10:23:09,221 DEBUG TRAIN Batch 13/5200 loss 5.056139 loss_att 8.329998 loss_ctc 8.200274 loss_rnnt 3.897754 hw_loss 0.158242 lr 0.00046717 rank 6
2023-02-21 10:23:09,221 DEBUG TRAIN Batch 13/5200 loss 5.383433 loss_att 8.224409 loss_ctc 6.559658 loss_rnnt 4.540417 hw_loss 0.221232 lr 0.00046717 rank 7
2023-02-21 10:23:09,232 DEBUG TRAIN Batch 13/5200 loss 10.907192 loss_att 11.404391 loss_ctc 15.607605 loss_rnnt 9.885676 hw_loss 0.553789 lr 0.00046717 rank 0
2023-02-21 10:23:09,260 DEBUG TRAIN Batch 13/5200 loss 16.814314 loss_att 16.685768 loss_ctc 18.384266 loss_rnnt 16.421471 hw_loss 0.392298 lr 0.00046717 rank 1
2023-02-21 10:24:07,006 DEBUG TRAIN Batch 13/5300 loss 10.050388 loss_att 13.653877 loss_ctc 17.485624 loss_rnnt 8.338304 hw_loss 0.000041 lr 0.00046706 rank 0
2023-02-21 10:24:07,008 DEBUG TRAIN Batch 13/5300 loss 31.767244 loss_att 32.570038 loss_ctc 42.692966 loss_rnnt 30.149900 hw_loss 0.000041 lr 0.00046706 rank 4
2023-02-21 10:24:07,011 DEBUG TRAIN Batch 13/5300 loss 5.831224 loss_att 12.202805 loss_ctc 8.822098 loss_rnnt 4.002547 hw_loss 0.291708 lr 0.00046706 rank 5
2023-02-21 10:24:07,011 DEBUG TRAIN Batch 13/5300 loss 9.718922 loss_att 10.750394 loss_ctc 11.388058 loss_rnnt 9.182940 hw_loss 0.200879 lr 0.00046706 rank 2
2023-02-21 10:24:07,011 DEBUG TRAIN Batch 13/5300 loss 7.998875 loss_att 13.774356 loss_ctc 11.768440 loss_rnnt 6.228529 hw_loss 0.211199 lr 0.00046706 rank 3
2023-02-21 10:24:07,016 DEBUG TRAIN Batch 13/5300 loss 8.291040 loss_att 10.092160 loss_ctc 9.073127 loss_rnnt 7.622132 hw_loss 0.383259 lr 0.00046706 rank 7
2023-02-21 10:24:07,019 DEBUG TRAIN Batch 13/5300 loss 25.046455 loss_att 25.596182 loss_ctc 44.508736 loss_rnnt 22.212021 hw_loss 0.242843 lr 0.00046706 rank 6
2023-02-21 10:24:07,024 DEBUG TRAIN Batch 13/5300 loss 5.194086 loss_att 10.525889 loss_ctc 11.268992 loss_rnnt 3.178386 hw_loss 0.261285 lr 0.00046706 rank 1
2023-02-21 10:25:03,350 DEBUG TRAIN Batch 13/5400 loss 23.856443 loss_att 31.828663 loss_ctc 23.385006 loss_rnnt 22.324368 hw_loss 0.000922 lr 0.00046696 rank 2
2023-02-21 10:25:03,359 DEBUG TRAIN Batch 13/5400 loss 6.546427 loss_att 7.120835 loss_ctc 7.812139 loss_rnnt 6.099522 hw_loss 0.306118 lr 0.00046696 rank 6
2023-02-21 10:25:03,360 DEBUG TRAIN Batch 13/5400 loss 5.000940 loss_att 9.595977 loss_ctc 5.515316 loss_rnnt 4.012857 hw_loss 0.000922 lr 0.00046696 rank 5
2023-02-21 10:25:03,361 DEBUG TRAIN Batch 13/5400 loss 8.026481 loss_att 14.625427 loss_ctc 15.120362 loss_rnnt 5.579499 hw_loss 0.340015 lr 0.00046696 rank 0
2023-02-21 10:25:03,362 DEBUG TRAIN Batch 13/5400 loss 38.065140 loss_att 36.733330 loss_ctc 21.036053 loss_rnnt 40.426285 hw_loss 0.329559 lr 0.00046696 rank 3
2023-02-21 10:25:03,362 DEBUG TRAIN Batch 13/5400 loss 4.304117 loss_att 11.599838 loss_ctc 12.045932 loss_rnnt 1.812239 hw_loss 0.000922 lr 0.00046696 rank 4
2023-02-21 10:25:03,363 DEBUG TRAIN Batch 13/5400 loss 4.129061 loss_att 6.792238 loss_ctc 4.261171 loss_rnnt 3.463722 hw_loss 0.215791 lr 0.00046696 rank 7
2023-02-21 10:25:03,369 DEBUG TRAIN Batch 13/5400 loss 16.681324 loss_att 18.639669 loss_ctc 23.293041 loss_rnnt 15.256384 hw_loss 0.284458 lr 0.00046696 rank 1
2023-02-21 10:25:58,666 DEBUG TRAIN Batch 13/5500 loss 8.192322 loss_att 8.404336 loss_ctc 7.927692 loss_rnnt 7.961505 hw_loss 0.419433 lr 0.00046686 rank 2
2023-02-21 10:25:58,668 DEBUG TRAIN Batch 13/5500 loss 14.523737 loss_att 18.308861 loss_ctc 17.719921 loss_rnnt 13.259807 hw_loss 0.151404 lr 0.00046686 rank 0
2023-02-21 10:25:58,668 DEBUG TRAIN Batch 13/5500 loss 9.542461 loss_att 15.093657 loss_ctc 19.119923 loss_rnnt 7.082571 hw_loss 0.136229 lr 0.00046686 rank 6
2023-02-21 10:25:58,672 DEBUG TRAIN Batch 13/5500 loss 19.703590 loss_att 21.465885 loss_ctc 22.045540 loss_rnnt 18.842182 hw_loss 0.368792 lr 0.00046686 rank 3
2023-02-21 10:25:58,672 DEBUG TRAIN Batch 13/5500 loss 13.334164 loss_att 17.811991 loss_ctc 12.600142 loss_rnnt 12.418412 hw_loss 0.221353 lr 0.00046686 rank 5
2023-02-21 10:25:58,674 DEBUG TRAIN Batch 13/5500 loss 7.179583 loss_att 11.340183 loss_ctc 7.944861 loss_rnnt 6.132031 hw_loss 0.212614 lr 0.00046686 rank 4
2023-02-21 10:25:58,682 DEBUG TRAIN Batch 13/5500 loss 3.224119 loss_att 5.362813 loss_ctc 5.207871 loss_rnnt 2.412154 hw_loss 0.224485 lr 0.00046686 rank 7
2023-02-21 10:25:58,683 DEBUG TRAIN Batch 13/5500 loss 7.984963 loss_att 11.682621 loss_ctc 13.614528 loss_rnnt 6.290363 hw_loss 0.383364 lr 0.00046686 rank 1
2023-02-21 10:26:56,556 DEBUG TRAIN Batch 13/5600 loss 15.007428 loss_att 16.128689 loss_ctc 14.227205 loss_rnnt 14.766979 hw_loss 0.225422 lr 0.00046676 rank 2
2023-02-21 10:26:56,561 DEBUG TRAIN Batch 13/5600 loss 7.664980 loss_att 11.513514 loss_ctc 10.441245 loss_rnnt 6.393665 hw_loss 0.246450 lr 0.00046676 rank 5
2023-02-21 10:26:56,563 DEBUG TRAIN Batch 13/5600 loss 16.834066 loss_att 19.264229 loss_ctc 18.908155 loss_rnnt 15.967971 hw_loss 0.194096 lr 0.00046676 rank 0
2023-02-21 10:26:56,563 DEBUG TRAIN Batch 13/5600 loss 0.589469 loss_att 2.483758 loss_ctc 0.209145 loss_rnnt 0.087177 hw_loss 0.326520 lr 0.00046676 rank 4
2023-02-21 10:26:56,566 DEBUG TRAIN Batch 13/5600 loss 7.356810 loss_att 12.300721 loss_ctc 16.656454 loss_rnnt 4.747853 hw_loss 0.712915 lr 0.00046676 rank 6
2023-02-21 10:26:56,566 DEBUG TRAIN Batch 13/5600 loss 4.335344 loss_att 6.838967 loss_ctc 7.875546 loss_rnnt 3.148992 hw_loss 0.400501 lr 0.00046676 rank 1
2023-02-21 10:26:56,570 DEBUG TRAIN Batch 13/5600 loss 8.326752 loss_att 12.893627 loss_ctc 14.790695 loss_rnnt 6.431211 hw_loss 0.225574 lr 0.00046676 rank 3
2023-02-21 10:26:56,582 DEBUG TRAIN Batch 13/5600 loss 11.028505 loss_att 16.543358 loss_ctc 10.856482 loss_rnnt 9.846785 hw_loss 0.190663 lr 0.00046676 rank 7
2023-02-21 10:27:53,350 DEBUG TRAIN Batch 13/5700 loss 8.882410 loss_att 9.438144 loss_ctc 11.088991 loss_rnnt 8.477012 hw_loss 0.000076 lr 0.00046666 rank 2
2023-02-21 10:27:53,355 DEBUG TRAIN Batch 13/5700 loss 3.563595 loss_att 6.110518 loss_ctc 4.972729 loss_rnnt 2.675490 hw_loss 0.357816 lr 0.00046666 rank 3
2023-02-21 10:27:53,356 DEBUG TRAIN Batch 13/5700 loss 9.055633 loss_att 13.258841 loss_ctc 17.795017 loss_rnnt 7.049699 hw_loss 0.000076 lr 0.00046666 rank 5
2023-02-21 10:27:53,357 DEBUG TRAIN Batch 13/5700 loss 24.755808 loss_att 21.744495 loss_ctc 35.229156 loss_rnnt 23.689127 hw_loss 0.510929 lr 0.00046666 rank 0
2023-02-21 10:27:53,358 DEBUG TRAIN Batch 13/5700 loss 2.177662 loss_att 6.586435 loss_ctc 1.433158 loss_rnnt 1.248580 hw_loss 0.274865 lr 0.00046666 rank 4
2023-02-21 10:27:53,363 DEBUG TRAIN Batch 13/5700 loss 24.936361 loss_att 28.342258 loss_ctc 31.831854 loss_rnnt 23.335741 hw_loss 0.000076 lr 0.00046666 rank 7
2023-02-21 10:27:53,363 DEBUG TRAIN Batch 13/5700 loss 30.582428 loss_att 25.477200 loss_ctc 32.228706 loss_rnnt 31.209440 hw_loss 0.327244 lr 0.00046666 rank 1
2023-02-21 10:27:53,374 DEBUG TRAIN Batch 13/5700 loss 3.210389 loss_att 6.354020 loss_ctc 5.429749 loss_rnnt 2.285707 hw_loss 0.000076 lr 0.00046666 rank 6
2023-02-21 10:28:49,128 DEBUG TRAIN Batch 13/5800 loss 10.522562 loss_att 13.697013 loss_ctc 16.843012 loss_rnnt 8.960827 hw_loss 0.157720 lr 0.00046655 rank 2
2023-02-21 10:28:49,132 DEBUG TRAIN Batch 13/5800 loss 11.887761 loss_att 9.461751 loss_ctc 10.541613 loss_rnnt 12.389023 hw_loss 0.306424 lr 0.00046655 rank 5
2023-02-21 10:28:49,132 DEBUG TRAIN Batch 13/5800 loss 1.505962 loss_att 5.064269 loss_ctc 1.764832 loss_rnnt 0.704493 hw_loss 0.103672 lr 0.00046655 rank 0
2023-02-21 10:28:49,133 DEBUG TRAIN Batch 13/5800 loss 29.441612 loss_att 33.452377 loss_ctc 37.206139 loss_rnnt 27.479794 hw_loss 0.233245 lr 0.00046655 rank 4
2023-02-21 10:28:49,133 DEBUG TRAIN Batch 13/5800 loss 9.172747 loss_att 11.471833 loss_ctc 13.445884 loss_rnnt 8.010283 hw_loss 0.249178 lr 0.00046655 rank 3
2023-02-21 10:28:49,133 DEBUG TRAIN Batch 13/5800 loss 3.006417 loss_att 6.066967 loss_ctc 3.163700 loss_rnnt 2.205101 hw_loss 0.315442 lr 0.00046655 rank 6
2023-02-21 10:28:49,143 DEBUG TRAIN Batch 13/5800 loss 12.017827 loss_att 11.613305 loss_ctc 9.881260 loss_rnnt 12.167311 hw_loss 0.405558 lr 0.00046655 rank 7
2023-02-21 10:28:49,150 DEBUG TRAIN Batch 13/5800 loss 25.457157 loss_att 22.160976 loss_ctc 31.424786 loss_rnnt 25.252068 hw_loss 0.128703 lr 0.00046655 rank 1
2023-02-21 10:29:48,723 DEBUG TRAIN Batch 13/5900 loss 19.287252 loss_att 21.588226 loss_ctc 26.881872 loss_rnnt 17.647717 hw_loss 0.312609 lr 0.00046645 rank 4
2023-02-21 10:29:48,727 DEBUG TRAIN Batch 13/5900 loss 8.708904 loss_att 12.738968 loss_ctc 9.025338 loss_rnnt 7.676198 hw_loss 0.345942 lr 0.00046645 rank 6
2023-02-21 10:29:48,728 DEBUG TRAIN Batch 13/5900 loss 12.087039 loss_att 16.766449 loss_ctc 20.888828 loss_rnnt 9.848022 hw_loss 0.242932 lr 0.00046645 rank 2
2023-02-21 10:29:48,730 DEBUG TRAIN Batch 13/5900 loss 11.860213 loss_att 14.905804 loss_ctc 12.462607 loss_rnnt 11.169540 hw_loss 0.002318 lr 0.00046645 rank 5
2023-02-21 10:29:48,732 DEBUG TRAIN Batch 13/5900 loss 2.250319 loss_att 6.875211 loss_ctc 6.028508 loss_rnnt 0.692708 hw_loss 0.241639 lr 0.00046645 rank 0
2023-02-21 10:29:48,736 DEBUG TRAIN Batch 13/5900 loss 9.756948 loss_att 12.939486 loss_ctc 17.034855 loss_rnnt 8.001159 hw_loss 0.279178 lr 0.00046645 rank 7
2023-02-21 10:29:48,742 DEBUG TRAIN Batch 13/5900 loss 4.320608 loss_att 6.805971 loss_ctc 4.533492 loss_rnnt 3.793914 hw_loss 0.002318 lr 0.00046645 rank 1
2023-02-21 10:29:48,784 DEBUG TRAIN Batch 13/5900 loss 15.368676 loss_att 15.369944 loss_ctc 17.969019 loss_rnnt 15.020475 hw_loss 0.002318 lr 0.00046645 rank 3
2023-02-21 10:30:45,974 DEBUG TRAIN Batch 13/6000 loss 20.942884 loss_att 24.701328 loss_ctc 24.707069 loss_rnnt 19.410271 hw_loss 0.523184 lr 0.00046635 rank 0
2023-02-21 10:30:45,974 DEBUG TRAIN Batch 13/6000 loss 6.420412 loss_att 16.786362 loss_ctc 6.449381 loss_rnnt 4.266336 hw_loss 0.144418 lr 0.00046635 rank 4
2023-02-21 10:30:45,977 DEBUG TRAIN Batch 13/6000 loss 16.765026 loss_att 17.786591 loss_ctc 21.404995 loss_rnnt 15.760643 hw_loss 0.340135 lr 0.00046635 rank 2
2023-02-21 10:30:45,978 DEBUG TRAIN Batch 13/6000 loss 3.497833 loss_att 7.751745 loss_ctc 4.323379 loss_rnnt 2.463171 hw_loss 0.138387 lr 0.00046635 rank 1
2023-02-21 10:30:45,979 DEBUG TRAIN Batch 13/6000 loss 2.455702 loss_att 6.902885 loss_ctc 4.077122 loss_rnnt 1.129083 hw_loss 0.414361 lr 0.00046635 rank 3
2023-02-21 10:30:45,983 DEBUG TRAIN Batch 13/6000 loss 8.206443 loss_att 12.835957 loss_ctc 10.340030 loss_rnnt 6.827557 hw_loss 0.315944 lr 0.00046635 rank 5
2023-02-21 10:30:45,986 DEBUG TRAIN Batch 13/6000 loss 15.055578 loss_att 27.032902 loss_ctc 34.461472 loss_rnnt 9.944464 hw_loss 0.240369 lr 0.00046635 rank 7
2023-02-21 10:30:45,988 DEBUG TRAIN Batch 13/6000 loss 1.819880 loss_att 6.067234 loss_ctc 1.530333 loss_rnnt 1.008955 hw_loss 0.000114 lr 0.00046635 rank 6
2023-02-21 10:32:43,971 DEBUG TRAIN Batch 13/6100 loss 14.745658 loss_att 14.051607 loss_ctc 18.655502 loss_rnnt 14.247449 hw_loss 0.216950 lr 0.00046625 rank 2
2023-02-21 10:32:43,973 DEBUG TRAIN Batch 13/6100 loss 18.274969 loss_att 18.171556 loss_ctc 19.787334 loss_rnnt 17.963022 hw_loss 0.245592 lr 0.00046625 rank 3
2023-02-21 10:32:43,977 DEBUG TRAIN Batch 13/6100 loss 7.360559 loss_att 9.089421 loss_ctc 9.527321 loss_rnnt 6.635414 hw_loss 0.169634 lr 0.00046625 rank 4
2023-02-21 10:32:43,978 DEBUG TRAIN Batch 13/6100 loss 12.866439 loss_att 13.574095 loss_ctc 13.300170 loss_rnnt 12.575660 hw_loss 0.171406 lr 0.00046625 rank 0
2023-02-21 10:32:43,979 DEBUG TRAIN Batch 13/6100 loss 12.228062 loss_att 15.334048 loss_ctc 16.484484 loss_rnnt 10.972913 hw_loss 0.124555 lr 0.00046625 rank 7
2023-02-21 10:32:43,979 DEBUG TRAIN Batch 13/6100 loss 12.998596 loss_att 19.092060 loss_ctc 13.520202 loss_rnnt 11.481992 hw_loss 0.428180 lr 0.00046625 rank 6
2023-02-21 10:32:43,984 DEBUG TRAIN Batch 13/6100 loss 8.714972 loss_att 11.400396 loss_ctc 7.170660 loss_rnnt 8.223728 hw_loss 0.300129 lr 0.00046625 rank 5
2023-02-21 10:32:43,985 DEBUG TRAIN Batch 13/6100 loss 7.231142 loss_att 12.264730 loss_ctc 15.417879 loss_rnnt 5.051156 hw_loss 0.153192 lr 0.00046625 rank 1
2023-02-21 10:33:42,289 DEBUG TRAIN Batch 13/6200 loss 8.959224 loss_att 14.977499 loss_ctc 9.559166 loss_rnnt 7.620794 hw_loss 0.102717 lr 0.00046615 rank 2
2023-02-21 10:33:42,295 DEBUG TRAIN Batch 13/6200 loss 7.535851 loss_att 6.697274 loss_ctc 7.509582 loss_rnnt 7.573624 hw_loss 0.250208 lr 0.00046615 rank 4
2023-02-21 10:33:42,296 DEBUG TRAIN Batch 13/6200 loss 4.000411 loss_att 6.665078 loss_ctc 7.538958 loss_rnnt 2.832697 hw_loss 0.305576 lr 0.00046615 rank 0
2023-02-21 10:33:42,297 DEBUG TRAIN Batch 13/6200 loss 41.883747 loss_att 57.214432 loss_ctc 68.672699 loss_rnnt 35.065205 hw_loss 0.338519 lr 0.00046615 rank 3
2023-02-21 10:33:42,301 DEBUG TRAIN Batch 13/6200 loss 9.938306 loss_att 12.231157 loss_ctc 15.047654 loss_rnnt 8.631466 hw_loss 0.313167 lr 0.00046615 rank 5
2023-02-21 10:33:42,309 DEBUG TRAIN Batch 13/6200 loss 20.542646 loss_att 19.093414 loss_ctc 23.575411 loss_rnnt 20.427351 hw_loss 0.001450 lr 0.00046615 rank 7
2023-02-21 10:33:42,309 DEBUG TRAIN Batch 13/6200 loss 2.565124 loss_att 5.655979 loss_ctc 6.050691 loss_rnnt 1.382000 hw_loss 0.187895 lr 0.00046615 rank 6
2023-02-21 10:33:42,310 DEBUG TRAIN Batch 13/6200 loss 19.876440 loss_att 26.385744 loss_ctc 21.654514 loss_rnnt 18.160089 hw_loss 0.332649 lr 0.00046615 rank 1
2023-02-21 10:34:40,112 DEBUG TRAIN Batch 13/6300 loss 35.260639 loss_att 48.582264 loss_ctc 47.720787 loss_rnnt 30.768200 hw_loss 0.312679 lr 0.00046605 rank 6
2023-02-21 10:34:40,116 DEBUG TRAIN Batch 13/6300 loss 1.095865 loss_att 3.829655 loss_ctc 0.586689 loss_rnnt 0.398859 hw_loss 0.409010 lr 0.00046605 rank 0
2023-02-21 10:34:40,116 DEBUG TRAIN Batch 13/6300 loss 18.052408 loss_att 32.247692 loss_ctc 24.819477 loss_rnnt 14.182043 hw_loss 0.241936 lr 0.00046605 rank 4
2023-02-21 10:34:40,119 DEBUG TRAIN Batch 13/6300 loss 27.158375 loss_att 28.059429 loss_ctc 26.673971 loss_rnnt 26.815149 hw_loss 0.426749 lr 0.00046605 rank 5
2023-02-21 10:34:40,119 DEBUG TRAIN Batch 13/6300 loss 12.483998 loss_att 15.012569 loss_ctc 15.594501 loss_rnnt 11.563346 hw_loss 0.000384 lr 0.00046605 rank 1
2023-02-21 10:34:40,121 DEBUG TRAIN Batch 13/6300 loss 3.165975 loss_att 9.742586 loss_ctc 6.387394 loss_rnnt 1.246089 hw_loss 0.328202 lr 0.00046605 rank 2
2023-02-21 10:34:40,121 DEBUG TRAIN Batch 13/6300 loss 5.709070 loss_att 11.625889 loss_ctc 4.992418 loss_rnnt 4.510649 hw_loss 0.207395 lr 0.00046605 rank 7
2023-02-21 10:34:40,177 DEBUG TRAIN Batch 13/6300 loss 19.223022 loss_att 21.954815 loss_ctc 22.116781 loss_rnnt 18.136017 hw_loss 0.290270 lr 0.00046605 rank 3
2023-02-21 10:35:36,422 DEBUG TRAIN Batch 13/6400 loss 12.194564 loss_att 15.115085 loss_ctc 19.600914 loss_rnnt 10.458364 hw_loss 0.308593 lr 0.00046595 rank 2
2023-02-21 10:35:36,423 DEBUG TRAIN Batch 13/6400 loss 10.873594 loss_att 14.400311 loss_ctc 15.767536 loss_rnnt 9.402630 hw_loss 0.212055 lr 0.00046595 rank 5
2023-02-21 10:35:36,429 DEBUG TRAIN Batch 13/6400 loss 8.078669 loss_att 11.869097 loss_ctc 15.836536 loss_rnnt 6.136075 hw_loss 0.281484 lr 0.00046595 rank 6
2023-02-21 10:35:36,430 DEBUG TRAIN Batch 13/6400 loss 22.672178 loss_att 23.036829 loss_ctc 24.016457 loss_rnnt 22.265312 hw_loss 0.290061 lr 0.00046595 rank 4
2023-02-21 10:35:36,430 DEBUG TRAIN Batch 13/6400 loss 2.906330 loss_att 5.673198 loss_ctc 5.068516 loss_rnnt 1.818163 hw_loss 0.462192 lr 0.00046595 rank 7
2023-02-21 10:35:36,430 DEBUG TRAIN Batch 13/6400 loss 28.173899 loss_att 34.189621 loss_ctc 31.458050 loss_rnnt 26.408092 hw_loss 0.233953 lr 0.00046595 rank 0
2023-02-21 10:35:36,442 DEBUG TRAIN Batch 13/6400 loss 9.821877 loss_att 12.432163 loss_ctc 14.107857 loss_rnnt 8.582222 hw_loss 0.274001 lr 0.00046595 rank 1
2023-02-21 10:35:36,490 DEBUG TRAIN Batch 13/6400 loss 23.494265 loss_att 26.295650 loss_ctc 36.457150 loss_rnnt 21.088726 hw_loss 0.219145 lr 0.00046595 rank 3
2023-02-21 10:36:35,059 DEBUG TRAIN Batch 13/6500 loss 14.267201 loss_att 16.827454 loss_ctc 14.406350 loss_rnnt 13.649511 hw_loss 0.163289 lr 0.00046585 rank 2
2023-02-21 10:36:35,060 DEBUG TRAIN Batch 13/6500 loss 12.911098 loss_att 14.993649 loss_ctc 16.103022 loss_rnnt 11.928774 hw_loss 0.262921 lr 0.00046585 rank 5
2023-02-21 10:36:35,061 DEBUG TRAIN Batch 13/6500 loss 16.221165 loss_att 19.556530 loss_ctc 18.594316 loss_rnnt 15.152935 hw_loss 0.158884 lr 0.00046585 rank 0
2023-02-21 10:36:35,065 DEBUG TRAIN Batch 13/6500 loss 13.236554 loss_att 16.540321 loss_ctc 13.004155 loss_rnnt 12.606604 hw_loss 0.000345 lr 0.00046585 rank 4
2023-02-21 10:36:35,067 DEBUG TRAIN Batch 13/6500 loss 16.647577 loss_att 13.111996 loss_ctc 16.189125 loss_rnnt 17.132215 hw_loss 0.531757 lr 0.00046585 rank 7
2023-02-21 10:36:35,067 DEBUG TRAIN Batch 13/6500 loss 28.502895 loss_att 27.782051 loss_ctc 28.717768 loss_rnnt 28.618233 hw_loss 0.000345 lr 0.00046585 rank 6
2023-02-21 10:36:35,070 DEBUG TRAIN Batch 13/6500 loss 15.661689 loss_att 17.742388 loss_ctc 21.917511 loss_rnnt 14.411254 hw_loss 0.000345 lr 0.00046585 rank 3
2023-02-21 10:36:35,073 DEBUG TRAIN Batch 13/6500 loss 3.404258 loss_att 9.759202 loss_ctc 3.362346 loss_rnnt 1.825135 hw_loss 0.588227 lr 0.00046585 rank 1
2023-02-21 10:37:30,947 DEBUG TRAIN Batch 13/6600 loss 5.066460 loss_att 9.482598 loss_ctc 11.901583 loss_rnnt 3.090200 hw_loss 0.340652 lr 0.00046574 rank 4
2023-02-21 10:37:30,949 DEBUG TRAIN Batch 13/6600 loss 14.021974 loss_att 13.940252 loss_ctc 15.638489 loss_rnnt 13.574753 hw_loss 0.465058 lr 0.00046574 rank 6
2023-02-21 10:37:30,949 DEBUG TRAIN Batch 13/6600 loss 22.762108 loss_att 22.422981 loss_ctc 30.654619 loss_rnnt 21.776499 hw_loss 0.002062 lr 0.00046574 rank 5
2023-02-21 10:37:30,950 DEBUG TRAIN Batch 13/6600 loss 21.741789 loss_att 26.515972 loss_ctc 38.416138 loss_rnnt 18.483170 hw_loss 0.151007 lr 0.00046574 rank 2
2023-02-21 10:37:30,950 DEBUG TRAIN Batch 13/6600 loss 43.252022 loss_att 44.193577 loss_ctc 33.856365 loss_rnnt 44.145226 hw_loss 0.321084 lr 0.00046574 rank 0
2023-02-21 10:37:30,952 DEBUG TRAIN Batch 13/6600 loss 33.554272 loss_att 45.068043 loss_ctc 42.224430 loss_rnnt 29.960979 hw_loss 0.252217 lr 0.00046574 rank 7
2023-02-21 10:37:30,954 DEBUG TRAIN Batch 13/6600 loss 14.538091 loss_att 14.002676 loss_ctc 15.008128 loss_rnnt 14.363299 hw_loss 0.411007 lr 0.00046574 rank 3
2023-02-21 10:37:30,958 DEBUG TRAIN Batch 13/6600 loss 13.702770 loss_att 15.137563 loss_ctc 15.919127 loss_rnnt 12.878833 hw_loss 0.452745 lr 0.00046574 rank 1
2023-02-21 10:38:27,390 DEBUG TRAIN Batch 13/6700 loss 13.503073 loss_att 18.401894 loss_ctc 13.537723 loss_rnnt 12.412995 hw_loss 0.198173 lr 0.00046564 rank 2
2023-02-21 10:38:27,398 DEBUG TRAIN Batch 13/6700 loss 25.433764 loss_att 27.118916 loss_ctc 27.987635 loss_rnnt 24.606462 hw_loss 0.280785 lr 0.00046564 rank 4
2023-02-21 10:38:27,400 DEBUG TRAIN Batch 13/6700 loss 14.210696 loss_att 18.980764 loss_ctc 15.975065 loss_rnnt 12.935552 hw_loss 0.161028 lr 0.00046564 rank 5
2023-02-21 10:38:27,399 DEBUG TRAIN Batch 13/6700 loss 20.519072 loss_att 16.731527 loss_ctc 20.933556 loss_rnnt 21.092335 hw_loss 0.241835 lr 0.00046564 rank 0
2023-02-21 10:38:27,402 DEBUG TRAIN Batch 13/6700 loss 13.695328 loss_att 15.185505 loss_ctc 16.567337 loss_rnnt 13.014152 hw_loss 0.000387 lr 0.00046564 rank 3
2023-02-21 10:38:27,403 DEBUG TRAIN Batch 13/6700 loss 22.983761 loss_att 24.628094 loss_ctc 27.211952 loss_rnnt 21.951349 hw_loss 0.262100 lr 0.00046564 rank 7
2023-02-21 10:38:27,405 DEBUG TRAIN Batch 13/6700 loss 11.646117 loss_att 13.367667 loss_ctc 12.692322 loss_rnnt 10.995189 hw_loss 0.313361 lr 0.00046564 rank 6
2023-02-21 10:38:27,407 DEBUG TRAIN Batch 13/6700 loss 25.565817 loss_att 31.501480 loss_ctc 28.612722 loss_rnnt 23.921816 hw_loss 0.094901 lr 0.00046564 rank 1
2023-02-21 10:39:25,720 DEBUG TRAIN Batch 13/6800 loss 18.803068 loss_att 26.616726 loss_ctc 28.509491 loss_rnnt 15.628946 hw_loss 0.594750 lr 0.00046554 rank 2
2023-02-21 10:39:25,730 DEBUG TRAIN Batch 13/6800 loss 13.325325 loss_att 20.629267 loss_ctc 16.743511 loss_rnnt 11.408758 hw_loss 0.000040 lr 0.00046554 rank 5
2023-02-21 10:39:25,729 DEBUG TRAIN Batch 13/6800 loss 9.351980 loss_att 12.019777 loss_ctc 15.122488 loss_rnnt 8.048998 hw_loss 0.000040 lr 0.00046554 rank 7
2023-02-21 10:39:25,731 DEBUG TRAIN Batch 13/6800 loss 6.709192 loss_att 13.566510 loss_ctc 12.575465 loss_rnnt 4.555537 hw_loss 0.000040 lr 0.00046554 rank 6
2023-02-21 10:39:25,733 DEBUG TRAIN Batch 13/6800 loss 12.734838 loss_att 16.010015 loss_ctc 28.908817 loss_rnnt 9.923249 hw_loss 0.000040 lr 0.00046554 rank 4
2023-02-21 10:39:25,735 DEBUG TRAIN Batch 13/6800 loss 6.632569 loss_att 12.709484 loss_ctc 8.841108 loss_rnnt 5.064393 hw_loss 0.109351 lr 0.00046554 rank 0
2023-02-21 10:39:25,739 DEBUG TRAIN Batch 13/6800 loss 12.024558 loss_att 18.888279 loss_ctc 16.865353 loss_rnnt 9.848239 hw_loss 0.296505 lr 0.00046554 rank 1
2023-02-21 10:39:25,793 DEBUG TRAIN Batch 13/6800 loss 4.426213 loss_att 5.721045 loss_ctc 5.271067 loss_rnnt 3.824968 hw_loss 0.430559 lr 0.00046554 rank 3
2023-02-21 10:41:18,551 DEBUG TRAIN Batch 13/6900 loss 7.545076 loss_att 9.145265 loss_ctc 12.403544 loss_rnnt 6.423573 hw_loss 0.288131 lr 0.00046544 rank 0
2023-02-21 10:41:18,553 DEBUG TRAIN Batch 13/6900 loss 9.247178 loss_att 24.606741 loss_ctc 19.744322 loss_rnnt 4.638321 hw_loss 0.257483 lr 0.00046544 rank 2
2023-02-21 10:41:18,554 DEBUG TRAIN Batch 13/6900 loss 14.011040 loss_att 15.184381 loss_ctc 14.629019 loss_rnnt 13.440458 hw_loss 0.475342 lr 0.00046544 rank 5
2023-02-21 10:41:18,555 DEBUG TRAIN Batch 13/6900 loss 11.300645 loss_att 10.967758 loss_ctc 12.977249 loss_rnnt 11.015356 hw_loss 0.240601 lr 0.00046544 rank 6
2023-02-21 10:41:18,555 DEBUG TRAIN Batch 13/6900 loss 10.847865 loss_att 14.258078 loss_ctc 12.340046 loss_rnnt 9.916819 hw_loss 0.093837 lr 0.00046544 rank 3
2023-02-21 10:41:18,556 DEBUG TRAIN Batch 13/6900 loss 14.186138 loss_att 16.405302 loss_ctc 16.246895 loss_rnnt 13.275792 hw_loss 0.359524 lr 0.00046544 rank 4
2023-02-21 10:41:18,564 DEBUG TRAIN Batch 13/6900 loss 8.032799 loss_att 10.923042 loss_ctc 9.608578 loss_rnnt 6.993639 hw_loss 0.470640 lr 0.00046544 rank 1
2023-02-21 10:41:18,564 DEBUG TRAIN Batch 13/6900 loss 39.532978 loss_att 51.625225 loss_ctc 67.698235 loss_rnnt 33.163074 hw_loss 0.367664 lr 0.00046544 rank 7
2023-02-21 10:42:16,013 DEBUG TRAIN Batch 13/7000 loss 23.441328 loss_att 31.112156 loss_ctc 36.840420 loss_rnnt 19.922943 hw_loss 0.370642 lr 0.00046534 rank 2
2023-02-21 10:42:16,017 DEBUG TRAIN Batch 13/7000 loss 19.019356 loss_att 18.812088 loss_ctc 16.653423 loss_rnnt 19.185707 hw_loss 0.357301 lr 0.00046534 rank 4
2023-02-21 10:42:16,024 DEBUG TRAIN Batch 13/7000 loss 9.509194 loss_att 9.733291 loss_ctc 9.601470 loss_rnnt 9.318227 hw_loss 0.250959 lr 0.00046534 rank 3
2023-02-21 10:42:16,025 DEBUG TRAIN Batch 13/7000 loss 9.738461 loss_att 13.490919 loss_ctc 11.648361 loss_rnnt 8.701931 hw_loss 0.058849 lr 0.00046534 rank 7
2023-02-21 10:42:16,025 DEBUG TRAIN Batch 13/7000 loss 10.734224 loss_att 13.121427 loss_ctc 13.400002 loss_rnnt 9.678944 hw_loss 0.417005 lr 0.00046534 rank 6
2023-02-21 10:42:16,033 DEBUG TRAIN Batch 13/7000 loss 10.902694 loss_att 13.448996 loss_ctc 12.866047 loss_rnnt 10.046995 hw_loss 0.158734 lr 0.00046534 rank 1
2023-02-21 10:42:16,048 DEBUG TRAIN Batch 13/7000 loss 12.635511 loss_att 14.912761 loss_ctc 15.563953 loss_rnnt 11.604140 hw_loss 0.347742 lr 0.00046534 rank 5
2023-02-21 10:42:16,053 DEBUG TRAIN Batch 13/7000 loss 16.282892 loss_att 17.032248 loss_ctc 19.798668 loss_rnnt 15.663622 hw_loss 0.001179 lr 0.00046534 rank 0
2023-02-21 10:43:14,147 DEBUG TRAIN Batch 13/7100 loss 2.120196 loss_att 6.239074 loss_ctc 1.764816 loss_rnnt 1.216815 hw_loss 0.238105 lr 0.00046524 rank 2
2023-02-21 10:43:14,150 DEBUG TRAIN Batch 13/7100 loss 27.230387 loss_att 31.962412 loss_ctc 34.613335 loss_rnnt 25.036552 hw_loss 0.493186 lr 0.00046524 rank 4
2023-02-21 10:43:14,153 DEBUG TRAIN Batch 13/7100 loss 10.930658 loss_att 27.414892 loss_ctc 12.398204 loss_rnnt 7.260692 hw_loss 0.332714 lr 0.00046524 rank 0
2023-02-21 10:43:14,153 DEBUG TRAIN Batch 13/7100 loss 7.307887 loss_att 9.051840 loss_ctc 5.653940 loss_rnnt 7.012171 hw_loss 0.313971 lr 0.00046524 rank 5
2023-02-21 10:43:14,154 DEBUG TRAIN Batch 13/7100 loss 13.441671 loss_att 20.556557 loss_ctc 15.591068 loss_rnnt 11.732018 hw_loss 0.000168 lr 0.00046524 rank 1
2023-02-21 10:43:14,156 DEBUG TRAIN Batch 13/7100 loss 2.484462 loss_att 8.078930 loss_ctc 6.246084 loss_rnnt 0.653013 hw_loss 0.395635 lr 0.00046524 rank 7
2023-02-21 10:43:14,165 DEBUG TRAIN Batch 13/7100 loss 27.752775 loss_att 39.796646 loss_ctc 45.535099 loss_rnnt 22.859869 hw_loss 0.212164 lr 0.00046524 rank 3
2023-02-21 10:43:14,167 DEBUG TRAIN Batch 13/7100 loss 9.091125 loss_att 8.281201 loss_ctc 14.663171 loss_rnnt 8.234867 hw_loss 0.516195 lr 0.00046524 rank 6
2023-02-21 10:44:09,467 DEBUG TRAIN Batch 13/7200 loss 10.663881 loss_att 13.739167 loss_ctc 19.443771 loss_rnnt 8.878122 hw_loss 0.000093 lr 0.00046514 rank 3
2023-02-21 10:44:09,469 DEBUG TRAIN Batch 13/7200 loss 6.316272 loss_att 6.909147 loss_ctc 6.546777 loss_rnnt 6.098467 hw_loss 0.128431 lr 0.00046514 rank 7
2023-02-21 10:44:09,470 DEBUG TRAIN Batch 13/7200 loss 15.266533 loss_att 18.339645 loss_ctc 18.762922 loss_rnnt 13.970711 hw_loss 0.403151 lr 0.00046514 rank 4
2023-02-21 10:44:09,472 DEBUG TRAIN Batch 13/7200 loss 26.280396 loss_att 31.453409 loss_ctc 27.387877 loss_rnnt 24.993496 hw_loss 0.196187 lr 0.00046514 rank 2
2023-02-21 10:44:09,473 DEBUG TRAIN Batch 13/7200 loss 9.527609 loss_att 10.363638 loss_ctc 14.136027 loss_rnnt 8.603985 hw_loss 0.266180 lr 0.00046514 rank 6
2023-02-21 10:44:09,480 DEBUG TRAIN Batch 13/7200 loss 7.453348 loss_att 10.022736 loss_ctc 8.254036 loss_rnnt 6.712915 hw_loss 0.224619 lr 0.00046514 rank 1
2023-02-21 10:44:09,505 DEBUG TRAIN Batch 13/7200 loss 32.039433 loss_att 40.263245 loss_ctc 34.901787 loss_rnnt 29.870245 hw_loss 0.267705 lr 0.00046514 rank 5
2023-02-21 10:44:09,505 DEBUG TRAIN Batch 13/7200 loss 29.655092 loss_att 44.075363 loss_ctc 32.496605 loss_rnnt 26.082565 hw_loss 0.580505 lr 0.00046514 rank 0
2023-02-21 10:45:06,608 DEBUG TRAIN Batch 13/7300 loss 12.285984 loss_att 15.289627 loss_ctc 14.202585 loss_rnnt 11.329891 hw_loss 0.187158 lr 0.00046504 rank 6
2023-02-21 10:45:06,610 DEBUG TRAIN Batch 13/7300 loss 23.948713 loss_att 28.487804 loss_ctc 36.208809 loss_rnnt 21.312016 hw_loss 0.176628 lr 0.00046504 rank 5
2023-02-21 10:45:06,611 DEBUG TRAIN Batch 13/7300 loss 34.684082 loss_att 35.222374 loss_ctc 43.197426 loss_rnnt 33.310638 hw_loss 0.245010 lr 0.00046504 rank 0
2023-02-21 10:45:06,612 DEBUG TRAIN Batch 13/7300 loss 16.462364 loss_att 18.835060 loss_ctc 19.438496 loss_rnnt 15.507241 hw_loss 0.157059 lr 0.00046504 rank 2
2023-02-21 10:45:06,614 DEBUG TRAIN Batch 13/7300 loss 10.359990 loss_att 10.896034 loss_ctc 13.204950 loss_rnnt 9.814644 hw_loss 0.110268 lr 0.00046504 rank 7
2023-02-21 10:45:06,620 DEBUG TRAIN Batch 13/7300 loss 7.424736 loss_att 14.496608 loss_ctc 9.554667 loss_rnnt 5.589646 hw_loss 0.256361 lr 0.00046504 rank 1
2023-02-21 10:45:06,620 DEBUG TRAIN Batch 13/7300 loss 16.842712 loss_att 21.356480 loss_ctc 18.196974 loss_rnnt 15.512352 hw_loss 0.463201 lr 0.00046504 rank 3
2023-02-21 10:45:06,621 DEBUG TRAIN Batch 13/7300 loss 17.789534 loss_att 16.665567 loss_ctc 18.305042 loss_rnnt 17.865673 hw_loss 0.149848 lr 0.00046504 rank 4
2023-02-21 10:46:03,975 DEBUG TRAIN Batch 13/7400 loss 13.697584 loss_att 19.906441 loss_ctc 16.861313 loss_rnnt 11.910603 hw_loss 0.231337 lr 0.00046494 rank 0
2023-02-21 10:46:03,978 DEBUG TRAIN Batch 13/7400 loss 8.326238 loss_att 15.763581 loss_ctc 13.359813 loss_rnnt 5.818494 hw_loss 0.654621 lr 0.00046494 rank 4
2023-02-21 10:46:03,980 DEBUG TRAIN Batch 13/7400 loss 36.491550 loss_att 40.031029 loss_ctc 49.140564 loss_rnnt 33.899490 hw_loss 0.370551 lr 0.00046494 rank 2
2023-02-21 10:46:03,982 DEBUG TRAIN Batch 13/7400 loss 43.800945 loss_att 52.604923 loss_ctc 57.729706 loss_rnnt 40.182789 hw_loss 0.000356 lr 0.00046494 rank 5
2023-02-21 10:46:03,985 DEBUG TRAIN Batch 13/7400 loss 31.192776 loss_att 27.618790 loss_ctc 30.302635 loss_rnnt 31.800785 hw_loss 0.422758 lr 0.00046494 rank 3
2023-02-21 10:46:03,987 DEBUG TRAIN Batch 13/7400 loss 23.292078 loss_att 24.167492 loss_ctc 25.553919 loss_rnnt 22.645523 hw_loss 0.318550 lr 0.00046494 rank 7
2023-02-21 10:46:03,989 DEBUG TRAIN Batch 13/7400 loss 14.789403 loss_att 17.483271 loss_ctc 16.719532 loss_rnnt 13.862701 hw_loss 0.244833 lr 0.00046494 rank 6
2023-02-21 10:46:03,999 DEBUG TRAIN Batch 13/7400 loss 13.260130 loss_att 14.560726 loss_ctc 22.107851 loss_rnnt 11.820126 hw_loss 0.000356 lr 0.00046494 rank 1
2023-02-21 10:46:59,382 DEBUG TRAIN Batch 13/7500 loss 7.397687 loss_att 10.371599 loss_ctc 10.822309 loss_rnnt 6.202959 hw_loss 0.268743 lr 0.00046484 rank 7
2023-02-21 10:46:59,384 DEBUG TRAIN Batch 13/7500 loss 26.460669 loss_att 24.949261 loss_ctc 30.206406 loss_rnnt 26.147045 hw_loss 0.218388 lr 0.00046484 rank 2
2023-02-21 10:46:59,385 DEBUG TRAIN Batch 13/7500 loss 12.170547 loss_att 12.585182 loss_ctc 16.553009 loss_rnnt 11.246898 hw_loss 0.480740 lr 0.00046484 rank 0
2023-02-21 10:46:59,391 DEBUG TRAIN Batch 13/7500 loss 27.262341 loss_att 31.073795 loss_ctc 29.951237 loss_rnnt 26.141287 hw_loss 0.000459 lr 0.00046484 rank 5
2023-02-21 10:46:59,396 DEBUG TRAIN Batch 13/7500 loss 15.171027 loss_att 14.821304 loss_ctc 19.097708 loss_rnnt 14.574831 hw_loss 0.267341 lr 0.00046484 rank 6
2023-02-21 10:46:59,404 DEBUG TRAIN Batch 13/7500 loss 18.389175 loss_att 22.924328 loss_ctc 25.886940 loss_rnnt 16.482197 hw_loss 0.000459 lr 0.00046484 rank 1
2023-02-21 10:46:59,424 DEBUG TRAIN Batch 13/7500 loss 11.498376 loss_att 15.392955 loss_ctc 13.072714 loss_rnnt 10.328897 hw_loss 0.338718 lr 0.00046484 rank 3
2023-02-21 10:46:59,476 DEBUG TRAIN Batch 13/7500 loss 20.637535 loss_att 23.365316 loss_ctc 25.194433 loss_rnnt 19.484148 hw_loss 0.000459 lr 0.00046484 rank 4
2023-02-21 10:47:57,929 DEBUG TRAIN Batch 13/7600 loss 31.290335 loss_att 35.953392 loss_ctc 38.167263 loss_rnnt 29.304628 hw_loss 0.255317 lr 0.00046474 rank 2
2023-02-21 10:47:57,932 DEBUG TRAIN Batch 13/7600 loss 10.147384 loss_att 12.229368 loss_ctc 13.152245 loss_rnnt 9.141991 hw_loss 0.353152 lr 0.00046474 rank 3
2023-02-21 10:47:57,932 DEBUG TRAIN Batch 13/7600 loss 12.144809 loss_att 15.122706 loss_ctc 18.259680 loss_rnnt 10.664450 hw_loss 0.130244 lr 0.00046474 rank 0
2023-02-21 10:47:57,932 DEBUG TRAIN Batch 13/7600 loss 16.956568 loss_att 17.458832 loss_ctc 17.583054 loss_rnnt 16.698696 hw_loss 0.138534 lr 0.00046474 rank 5
2023-02-21 10:47:57,938 DEBUG TRAIN Batch 13/7600 loss 12.229135 loss_att 15.258490 loss_ctc 24.354017 loss_rnnt 9.834978 hw_loss 0.321815 lr 0.00046474 rank 4
2023-02-21 10:47:57,938 DEBUG TRAIN Batch 13/7600 loss 7.767926 loss_att 12.321476 loss_ctc 9.652195 loss_rnnt 6.486287 hw_loss 0.224424 lr 0.00046474 rank 6
2023-02-21 10:47:57,939 DEBUG TRAIN Batch 13/7600 loss 6.169815 loss_att 9.068032 loss_ctc 9.070992 loss_rnnt 4.973393 hw_loss 0.431165 lr 0.00046474 rank 7
2023-02-21 10:47:57,939 DEBUG TRAIN Batch 13/7600 loss 9.351341 loss_att 12.223714 loss_ctc 12.769688 loss_rnnt 8.170628 hw_loss 0.282112 lr 0.00046474 rank 1
2023-02-21 10:48:57,195 DEBUG TRAIN Batch 13/7700 loss 3.663940 loss_att 11.362411 loss_ctc 6.445058 loss_rnnt 1.611729 hw_loss 0.265689 lr 0.00046464 rank 3
2023-02-21 10:48:57,197 DEBUG TRAIN Batch 13/7700 loss 16.650446 loss_att 24.846954 loss_ctc 27.778461 loss_rnnt 13.326509 hw_loss 0.376684 lr 0.00046464 rank 5
2023-02-21 10:48:57,198 DEBUG TRAIN Batch 13/7700 loss 48.392017 loss_att 44.778847 loss_ctc 50.261795 loss_rnnt 48.863953 hw_loss 0.002613 lr 0.00046464 rank 4
2023-02-21 10:48:57,199 DEBUG TRAIN Batch 13/7700 loss 2.796750 loss_att 6.005244 loss_ctc 3.506325 loss_rnnt 1.823488 hw_loss 0.444289 lr 0.00046464 rank 2
2023-02-21 10:48:57,201 DEBUG TRAIN Batch 13/7700 loss 0.612992 loss_att 2.665466 loss_ctc 0.371253 loss_rnnt 0.100091 hw_loss 0.252445 lr 0.00046464 rank 0
2023-02-21 10:48:57,204 DEBUG TRAIN Batch 13/7700 loss 14.459405 loss_att 24.520723 loss_ctc 25.066399 loss_rnnt 10.916824 hw_loss 0.217598 lr 0.00046464 rank 7
2023-02-21 10:48:57,204 DEBUG TRAIN Batch 13/7700 loss 2.617069 loss_att 10.678508 loss_ctc 3.947289 loss_rnnt 0.826025 hw_loss 0.002613 lr 0.00046464 rank 6
2023-02-21 10:48:57,207 DEBUG TRAIN Batch 13/7700 loss 13.762294 loss_att 18.123913 loss_ctc 19.101093 loss_rnnt 12.061284 hw_loss 0.219085 lr 0.00046464 rank 1
2023-02-21 10:50:49,248 DEBUG TRAIN Batch 13/7800 loss 10.671016 loss_att 12.522039 loss_ctc 12.209224 loss_rnnt 9.898486 hw_loss 0.369806 lr 0.00046454 rank 0
2023-02-21 10:50:49,249 DEBUG TRAIN Batch 13/7800 loss 12.162577 loss_att 11.429822 loss_ctc 16.734875 loss_rnnt 11.546041 hw_loss 0.287711 lr 0.00046454 rank 5
2023-02-21 10:50:49,250 DEBUG TRAIN Batch 13/7800 loss 41.492249 loss_att 43.695602 loss_ctc 58.025314 loss_rnnt 38.786003 hw_loss 0.114687 lr 0.00046454 rank 3
2023-02-21 10:50:49,252 DEBUG TRAIN Batch 13/7800 loss 27.632328 loss_att 23.890608 loss_ctc 34.144276 loss_rnnt 27.512383 hw_loss 0.000057 lr 0.00046454 rank 6
2023-02-21 10:50:49,255 DEBUG TRAIN Batch 13/7800 loss 27.999294 loss_att 30.033920 loss_ctc 33.227428 loss_rnnt 26.634031 hw_loss 0.489852 lr 0.00046454 rank 2
2023-02-21 10:50:49,262 DEBUG TRAIN Batch 13/7800 loss 6.782207 loss_att 11.701433 loss_ctc 10.714899 loss_rnnt 5.070185 hw_loss 0.382157 lr 0.00046454 rank 7
2023-02-21 10:50:49,262 DEBUG TRAIN Batch 13/7800 loss 6.303379 loss_att 8.930043 loss_ctc 6.919333 loss_rnnt 5.695889 hw_loss 0.000057 lr 0.00046454 rank 1
2023-02-21 10:50:49,307 DEBUG TRAIN Batch 13/7800 loss 10.659138 loss_att 16.947292 loss_ctc 12.886312 loss_rnnt 8.913195 hw_loss 0.358792 lr 0.00046454 rank 4
2023-02-21 10:52:34,352 DEBUG TRAIN Batch 13/7900 loss 13.474710 loss_att 14.491169 loss_ctc 17.792847 loss_rnnt 12.475687 hw_loss 0.412459 lr 0.00046444 rank 5
2023-02-21 10:52:34,354 DEBUG TRAIN Batch 13/7900 loss 27.598372 loss_att 27.631340 loss_ctc 34.730839 loss_rnnt 26.525543 hw_loss 0.216073 lr 0.00046444 rank 2
2023-02-21 10:52:34,356 DEBUG TRAIN Batch 13/7900 loss 18.941040 loss_att 12.902284 loss_ctc 23.089735 loss_rnnt 19.414328 hw_loss 0.339947 lr 0.00046444 rank 4
2023-02-21 10:52:34,357 DEBUG TRAIN Batch 13/7900 loss 7.711049 loss_att 10.654019 loss_ctc 12.269071 loss_rnnt 6.289400 hw_loss 0.422471 lr 0.00046444 rank 0
2023-02-21 10:52:34,358 DEBUG TRAIN Batch 13/7900 loss 12.583179 loss_att 13.582312 loss_ctc 13.235609 loss_rnnt 12.056754 hw_loss 0.449263 lr 0.00046444 rank 3
2023-02-21 10:52:34,363 DEBUG TRAIN Batch 13/7900 loss 16.055294 loss_att 22.713406 loss_ctc 24.943119 loss_rnnt 13.388216 hw_loss 0.282023 lr 0.00046444 rank 6
2023-02-21 10:52:34,365 DEBUG TRAIN Batch 13/7900 loss 4.380388 loss_att 7.286693 loss_ctc 3.227213 loss_rnnt 3.893952 hw_loss 0.110497 lr 0.00046444 rank 7
2023-02-21 10:52:34,374 DEBUG TRAIN Batch 13/7900 loss 20.313513 loss_att 26.729830 loss_ctc 40.734653 loss_rnnt 16.199436 hw_loss 0.202491 lr 0.00046444 rank 1
2023-02-21 10:53:30,840 DEBUG TRAIN Batch 13/8000 loss 39.537746 loss_att 32.868961 loss_ctc 42.645618 loss_rnnt 40.457092 hw_loss 0.000055 lr 0.00046434 rank 7
2023-02-21 10:53:30,842 DEBUG TRAIN Batch 13/8000 loss 16.308876 loss_att 20.418674 loss_ctc 21.402512 loss_rnnt 14.667656 hw_loss 0.262702 lr 0.00046434 rank 4
2023-02-21 10:53:30,844 DEBUG TRAIN Batch 13/8000 loss 24.068632 loss_att 24.542150 loss_ctc 27.485220 loss_rnnt 23.264534 hw_loss 0.475971 lr 0.00046434 rank 6
2023-02-21 10:53:30,846 DEBUG TRAIN Batch 13/8000 loss 2.529473 loss_att 5.960734 loss_ctc 4.470479 loss_rnnt 1.405778 hw_loss 0.334953 lr 0.00046434 rank 3
2023-02-21 10:53:30,846 DEBUG TRAIN Batch 13/8000 loss 25.143791 loss_att 26.512762 loss_ctc 34.428440 loss_rnnt 23.542789 hw_loss 0.167352 lr 0.00046434 rank 5
2023-02-21 10:53:30,848 DEBUG TRAIN Batch 13/8000 loss 13.103256 loss_att 13.668855 loss_ctc 11.129471 loss_rnnt 13.115318 hw_loss 0.258729 lr 0.00046434 rank 0
2023-02-21 10:53:30,849 DEBUG TRAIN Batch 13/8000 loss 5.906033 loss_att 9.385473 loss_ctc 7.208858 loss_rnnt 4.811104 hw_loss 0.422495 lr 0.00046434 rank 2
2023-02-21 10:53:30,850 DEBUG TRAIN Batch 13/8000 loss 17.063387 loss_att 18.862801 loss_ctc 12.525326 loss_rnnt 17.243385 hw_loss 0.122239 lr 0.00046434 rank 1
2023-02-21 10:54:27,025 DEBUG TRAIN Batch 13/8100 loss 17.400276 loss_att 21.329411 loss_ctc 21.918655 loss_rnnt 15.866873 hw_loss 0.272110 lr 0.00046424 rank 0
2023-02-21 10:54:27,030 DEBUG TRAIN Batch 13/8100 loss 13.484401 loss_att 21.455677 loss_ctc 26.973211 loss_rnnt 9.990237 hw_loss 0.190128 lr 0.00046424 rank 2
2023-02-21 10:54:27,036 DEBUG TRAIN Batch 13/8100 loss 4.538166 loss_att 9.243320 loss_ctc 7.419973 loss_rnnt 3.152100 hw_loss 0.113988 lr 0.00046424 rank 7
2023-02-21 10:54:27,035 DEBUG TRAIN Batch 13/8100 loss 8.847743 loss_att 15.078600 loss_ctc 13.935753 loss_rnnt 6.774840 hw_loss 0.278120 lr 0.00046424 rank 4
2023-02-21 10:54:27,038 DEBUG TRAIN Batch 13/8100 loss 7.365606 loss_att 11.750518 loss_ctc 11.528424 loss_rnnt 5.796209 hw_loss 0.257573 lr 0.00046424 rank 6
2023-02-21 10:54:27,042 DEBUG TRAIN Batch 13/8100 loss 21.124908 loss_att 21.566320 loss_ctc 24.634077 loss_rnnt 20.320877 hw_loss 0.464737 lr 0.00046424 rank 5
2023-02-21 10:54:27,046 DEBUG TRAIN Batch 13/8100 loss 9.565125 loss_att 11.343861 loss_ctc 8.678038 loss_rnnt 9.327255 hw_loss 0.000752 lr 0.00046424 rank 3
2023-02-21 10:54:27,091 DEBUG TRAIN Batch 13/8100 loss 17.324648 loss_att 18.358707 loss_ctc 14.472271 loss_rnnt 17.423786 hw_loss 0.139435 lr 0.00046424 rank 1
2023-02-21 10:55:25,138 DEBUG TRAIN Batch 13/8200 loss 16.254131 loss_att 18.985373 loss_ctc 21.834511 loss_rnnt 14.795367 hw_loss 0.315875 lr 0.00046414 rank 2
2023-02-21 10:55:25,139 DEBUG TRAIN Batch 13/8200 loss 9.724658 loss_att 13.350317 loss_ctc 14.202686 loss_rnnt 8.278017 hw_loss 0.233324 lr 0.00046414 rank 5
2023-02-21 10:55:25,144 DEBUG TRAIN Batch 13/8200 loss 19.303347 loss_att 25.421165 loss_ctc 25.488682 loss_rnnt 17.107973 hw_loss 0.275805 lr 0.00046414 rank 4
2023-02-21 10:55:25,148 DEBUG TRAIN Batch 13/8200 loss 15.388696 loss_att 16.765404 loss_ctc 17.384207 loss_rnnt 14.847202 hw_loss 0.000158 lr 0.00046414 rank 6
2023-02-21 10:55:25,149 DEBUG TRAIN Batch 13/8200 loss 6.046389 loss_att 16.303598 loss_ctc 9.668961 loss_rnnt 3.511853 hw_loss 0.000158 lr 0.00046414 rank 7
2023-02-21 10:55:25,157 DEBUG TRAIN Batch 13/8200 loss 20.231352 loss_att 22.246344 loss_ctc 24.939663 loss_rnnt 19.140495 hw_loss 0.112654 lr 0.00046414 rank 1
2023-02-21 10:55:25,203 DEBUG TRAIN Batch 13/8200 loss 0.991778 loss_att 4.080222 loss_ctc 0.444430 loss_rnnt 0.173718 hw_loss 0.512533 lr 0.00046414 rank 3
2023-02-21 10:55:25,204 DEBUG TRAIN Batch 13/8200 loss 8.711078 loss_att 10.510114 loss_ctc 13.537202 loss_rnnt 7.657238 hw_loss 0.094782 lr 0.00046414 rank 0
2023-02-21 10:56:22,001 DEBUG TRAIN Batch 13/8300 loss 5.367592 loss_att 10.787663 loss_ctc 5.329023 loss_rnnt 4.288652 hw_loss 0.000127 lr 0.00046404 rank 2
2023-02-21 10:56:22,002 DEBUG TRAIN Batch 13/8300 loss 15.305035 loss_att 15.553987 loss_ctc 18.759783 loss_rnnt 14.519218 hw_loss 0.516359 lr 0.00046404 rank 0
2023-02-21 10:56:22,004 DEBUG TRAIN Batch 13/8300 loss 28.670046 loss_att 39.582668 loss_ctc 41.140961 loss_rnnt 24.698410 hw_loss 0.236854 lr 0.00046404 rank 6
2023-02-21 10:56:22,005 DEBUG TRAIN Batch 13/8300 loss 6.393956 loss_att 8.897987 loss_ctc 9.314920 loss_rnnt 5.266679 hw_loss 0.444392 lr 0.00046404 rank 3
2023-02-21 10:56:22,006 DEBUG TRAIN Batch 13/8300 loss 3.578541 loss_att 11.114819 loss_ctc 8.772253 loss_rnnt 1.247672 hw_loss 0.245846 lr 0.00046404 rank 7
2023-02-21 10:56:22,007 DEBUG TRAIN Batch 13/8300 loss 7.429818 loss_att 7.876581 loss_ctc 6.743264 loss_rnnt 7.271689 hw_loss 0.300593 lr 0.00046404 rank 5
2023-02-21 10:56:22,007 DEBUG TRAIN Batch 13/8300 loss 21.678438 loss_att 25.701336 loss_ctc 25.986574 loss_rnnt 20.125933 hw_loss 0.325329 lr 0.00046404 rank 1
2023-02-21 10:56:22,065 DEBUG TRAIN Batch 13/8300 loss 5.550110 loss_att 7.806551 loss_ctc 4.146950 loss_rnnt 5.104986 hw_loss 0.339232 lr 0.00046404 rank 4
2023-02-21 10:57:18,047 DEBUG TRAIN Batch 13/8400 loss 19.277815 loss_att 21.875082 loss_ctc 22.193659 loss_rnnt 18.257692 hw_loss 0.209795 lr 0.00046394 rank 2
2023-02-21 10:57:18,047 DEBUG TRAIN Batch 13/8400 loss 7.270189 loss_att 9.602077 loss_ctc 8.966230 loss_rnnt 6.387507 hw_loss 0.356559 lr 0.00046394 rank 5
2023-02-21 10:57:18,049 DEBUG TRAIN Batch 13/8400 loss 13.728518 loss_att 18.655632 loss_ctc 19.212463 loss_rnnt 11.899493 hw_loss 0.210769 lr 0.00046394 rank 7
2023-02-21 10:57:18,051 DEBUG TRAIN Batch 13/8400 loss 12.910732 loss_att 19.855568 loss_ctc 20.966206 loss_rnnt 10.209917 hw_loss 0.445844 lr 0.00046394 rank 4
2023-02-21 10:57:18,055 DEBUG TRAIN Batch 13/8400 loss 15.695318 loss_att 27.052353 loss_ctc 21.234661 loss_rnnt 12.577940 hw_loss 0.201360 lr 0.00046394 rank 1
2023-02-21 10:57:18,055 DEBUG TRAIN Batch 13/8400 loss 9.233573 loss_att 11.267982 loss_ctc 11.770029 loss_rnnt 8.389050 hw_loss 0.186464 lr 0.00046394 rank 3
2023-02-21 10:57:18,059 DEBUG TRAIN Batch 13/8400 loss 6.155067 loss_att 9.449518 loss_ctc 7.348012 loss_rnnt 5.179840 hw_loss 0.294895 lr 0.00046394 rank 6
2023-02-21 10:57:18,059 DEBUG TRAIN Batch 13/8400 loss 21.350054 loss_att 22.034580 loss_ctc 29.863079 loss_rnnt 19.884268 hw_loss 0.363398 lr 0.00046394 rank 0
2023-02-21 10:58:16,706 DEBUG TRAIN Batch 13/8500 loss 0.474744 loss_att 2.709284 loss_ctc 0.146782 loss_rnnt 0.071415 hw_loss 0.000280 lr 0.00046384 rank 6
2023-02-21 10:58:16,710 DEBUG TRAIN Batch 13/8500 loss 21.054077 loss_att 23.640589 loss_ctc 26.039600 loss_rnnt 19.632298 hw_loss 0.449512 lr 0.00046384 rank 3
2023-02-21 10:58:16,710 DEBUG TRAIN Batch 13/8500 loss 4.369569 loss_att 6.513522 loss_ctc 8.304719 loss_rnnt 3.359224 hw_loss 0.106627 lr 0.00046384 rank 2
2023-02-21 10:58:16,711 DEBUG TRAIN Batch 13/8500 loss 16.353449 loss_att 18.480701 loss_ctc 21.766037 loss_rnnt 15.024153 hw_loss 0.341565 lr 0.00046384 rank 0
2023-02-21 10:58:16,715 DEBUG TRAIN Batch 13/8500 loss 15.858426 loss_att 16.932541 loss_ctc 26.021265 loss_rnnt 14.113100 hw_loss 0.328981 lr 0.00046384 rank 7
2023-02-21 10:58:16,719 DEBUG TRAIN Batch 13/8500 loss 8.536016 loss_att 13.102322 loss_ctc 12.955868 loss_rnnt 6.871404 hw_loss 0.303821 lr 0.00046384 rank 5
2023-02-21 10:58:16,721 DEBUG TRAIN Batch 13/8500 loss 9.523385 loss_att 20.066227 loss_ctc 14.988260 loss_rnnt 6.555404 hw_loss 0.245181 lr 0.00046384 rank 4
2023-02-21 10:58:16,728 DEBUG TRAIN Batch 13/8500 loss 26.000322 loss_att 25.801600 loss_ctc 31.680569 loss_rnnt 25.123745 hw_loss 0.298045 lr 0.00046384 rank 1
2023-02-21 11:00:10,088 DEBUG TRAIN Batch 13/8600 loss 26.640097 loss_att 26.606789 loss_ctc 35.144333 loss_rnnt 25.334293 hw_loss 0.334813 lr 0.00046374 rank 2
2023-02-21 11:00:10,097 DEBUG TRAIN Batch 13/8600 loss 7.980012 loss_att 7.726263 loss_ctc 17.366758 loss_rnnt 6.595456 hw_loss 0.344512 lr 0.00046374 rank 5
2023-02-21 11:00:10,097 DEBUG TRAIN Batch 13/8600 loss 10.540178 loss_att 13.772264 loss_ctc 12.479091 loss_rnnt 9.467625 hw_loss 0.314280 lr 0.00046374 rank 0
2023-02-21 11:00:10,098 DEBUG TRAIN Batch 13/8600 loss 20.328560 loss_att 23.240875 loss_ctc 34.943638 loss_rnnt 17.560564 hw_loss 0.444103 lr 0.00046374 rank 3
2023-02-21 11:00:10,100 DEBUG TRAIN Batch 13/8600 loss 14.837401 loss_att 24.507477 loss_ctc 24.316126 loss_rnnt 11.472899 hw_loss 0.312482 lr 0.00046374 rank 7
2023-02-21 11:00:10,115 DEBUG TRAIN Batch 13/8600 loss 7.900563 loss_att 7.695517 loss_ctc 9.962676 loss_rnnt 7.403996 hw_loss 0.492425 lr 0.00046374 rank 1
2023-02-21 11:00:10,129 DEBUG TRAIN Batch 13/8600 loss 7.218561 loss_att 12.753786 loss_ctc 5.454923 loss_rnnt 6.346539 hw_loss 0.000241 lr 0.00046374 rank 6
2023-02-21 11:00:10,152 DEBUG TRAIN Batch 13/8600 loss 17.084581 loss_att 17.228409 loss_ctc 24.276966 loss_rnnt 16.044319 hw_loss 0.098462 lr 0.00046374 rank 4
2023-02-21 11:01:09,301 DEBUG TRAIN Batch 13/8700 loss 4.563314 loss_att 9.260416 loss_ctc 8.621917 loss_rnnt 2.946123 hw_loss 0.256170 lr 0.00046364 rank 2
2023-02-21 11:01:09,310 DEBUG TRAIN Batch 13/8700 loss 19.321659 loss_att 21.619961 loss_ctc 26.198660 loss_rnnt 17.745956 hw_loss 0.373325 lr 0.00046364 rank 3
2023-02-21 11:01:09,310 DEBUG TRAIN Batch 13/8700 loss 7.412439 loss_att 9.475677 loss_ctc 8.778212 loss_rnnt 6.598975 hw_loss 0.410088 lr 0.00046364 rank 5
2023-02-21 11:01:09,311 DEBUG TRAIN Batch 13/8700 loss 14.324158 loss_att 15.680575 loss_ctc 17.273706 loss_rnnt 13.444693 hw_loss 0.402956 lr 0.00046364 rank 0
2023-02-21 11:01:09,311 DEBUG TRAIN Batch 13/8700 loss 30.273933 loss_att 35.011108 loss_ctc 40.878944 loss_rnnt 27.737045 hw_loss 0.328971 lr 0.00046364 rank 4
2023-02-21 11:01:09,315 DEBUG TRAIN Batch 13/8700 loss 11.364605 loss_att 16.837910 loss_ctc 17.388439 loss_rnnt 9.412706 hw_loss 0.101361 lr 0.00046364 rank 6
2023-02-21 11:01:09,318 DEBUG TRAIN Batch 13/8700 loss 2.468930 loss_att 5.080420 loss_ctc 4.130691 loss_rnnt 1.581587 hw_loss 0.269020 lr 0.00046364 rank 7
2023-02-21 11:01:09,321 DEBUG TRAIN Batch 13/8700 loss 20.588552 loss_att 21.357151 loss_ctc 20.628574 loss_rnnt 20.252466 hw_loss 0.331932 lr 0.00046364 rank 1
2023-02-21 11:02:08,591 DEBUG TRAIN Batch 13/8800 loss 3.646601 loss_att 8.172634 loss_ctc 2.854622 loss_rnnt 2.634170 hw_loss 0.399040 lr 0.00046354 rank 0
2023-02-21 11:02:08,595 DEBUG TRAIN Batch 13/8800 loss 11.481916 loss_att 19.312117 loss_ctc 16.527878 loss_rnnt 9.241365 hw_loss 0.003219 lr 0.00046354 rank 5
2023-02-21 11:02:08,598 DEBUG TRAIN Batch 13/8800 loss 7.985681 loss_att 10.749060 loss_ctc 14.936607 loss_rnnt 6.254666 hw_loss 0.471652 lr 0.00046354 rank 4
2023-02-21 11:02:08,599 DEBUG TRAIN Batch 13/8800 loss 19.941824 loss_att 25.581484 loss_ctc 24.440100 loss_rnnt 18.029987 hw_loss 0.345254 lr 0.00046354 rank 2
2023-02-21 11:02:08,603 DEBUG TRAIN Batch 13/8800 loss 11.703920 loss_att 16.350773 loss_ctc 17.877453 loss_rnnt 9.901259 hw_loss 0.094037 lr 0.00046354 rank 6
2023-02-21 11:02:08,604 DEBUG TRAIN Batch 13/8800 loss 33.373764 loss_att 38.987522 loss_ctc 46.583950 loss_rnnt 30.326996 hw_loss 0.304983 lr 0.00046354 rank 1
2023-02-21 11:02:08,605 DEBUG TRAIN Batch 13/8800 loss 12.066069 loss_att 14.498062 loss_ctc 14.922101 loss_rnnt 11.197148 hw_loss 0.003219 lr 0.00046354 rank 7
2023-02-21 11:02:08,605 DEBUG TRAIN Batch 13/8800 loss 5.763825 loss_att 11.892220 loss_ctc 5.935015 loss_rnnt 4.272710 hw_loss 0.454896 lr 0.00046354 rank 3
2023-02-21 11:03:03,470 DEBUG TRAIN Batch 13/8900 loss 12.294973 loss_att 13.836020 loss_ctc 13.016268 loss_rnnt 11.804876 hw_loss 0.160715 lr 0.00046344 rank 0
2023-02-21 11:03:03,471 DEBUG TRAIN Batch 13/8900 loss 6.778578 loss_att 8.683114 loss_ctc 6.142319 loss_rnnt 6.482472 hw_loss 0.000063 lr 0.00046344 rank 2
2023-02-21 11:03:03,474 DEBUG TRAIN Batch 13/8900 loss 7.874354 loss_att 12.840990 loss_ctc 13.650340 loss_rnnt 5.800404 hw_loss 0.582171 lr 0.00046344 rank 5
2023-02-21 11:03:03,474 DEBUG TRAIN Batch 13/8900 loss 18.839003 loss_att 18.757423 loss_ctc 25.500723 loss_rnnt 17.700073 hw_loss 0.500654 lr 0.00046344 rank 7
2023-02-21 11:03:03,474 DEBUG TRAIN Batch 13/8900 loss 11.051641 loss_att 17.249994 loss_ctc 11.415665 loss_rnnt 9.719349 hw_loss 0.082656 lr 0.00046344 rank 6
2023-02-21 11:03:03,478 DEBUG TRAIN Batch 13/8900 loss 10.676224 loss_att 14.605701 loss_ctc 16.269232 loss_rnnt 8.972244 hw_loss 0.323156 lr 0.00046344 rank 4
2023-02-21 11:03:03,480 DEBUG TRAIN Batch 13/8900 loss 17.767660 loss_att 20.435905 loss_ctc 28.307240 loss_rnnt 15.593706 hw_loss 0.440675 lr 0.00046344 rank 3
2023-02-21 11:03:03,489 DEBUG TRAIN Batch 13/8900 loss 17.595974 loss_att 17.630337 loss_ctc 19.336643 loss_rnnt 17.252663 hw_loss 0.195657 lr 0.00046344 rank 1
2023-02-21 11:04:00,310 DEBUG TRAIN Batch 13/9000 loss 29.555256 loss_att 35.953949 loss_ctc 37.247993 loss_rnnt 27.101465 hw_loss 0.278156 lr 0.00046334 rank 6
2023-02-21 11:04:00,311 DEBUG TRAIN Batch 13/9000 loss 22.369303 loss_att 24.476427 loss_ctc 32.215611 loss_rnnt 20.414877 hw_loss 0.412797 lr 0.00046334 rank 0
2023-02-21 11:04:00,311 DEBUG TRAIN Batch 13/9000 loss 12.683805 loss_att 12.860793 loss_ctc 15.693183 loss_rnnt 11.984867 hw_loss 0.491792 lr 0.00046334 rank 4
2023-02-21 11:04:00,313 DEBUG TRAIN Batch 13/9000 loss 11.923241 loss_att 14.078571 loss_ctc 10.375853 loss_rnnt 11.611654 hw_loss 0.162824 lr 0.00046334 rank 5
2023-02-21 11:04:00,316 DEBUG TRAIN Batch 13/9000 loss 14.515133 loss_att 22.763752 loss_ctc 12.489027 loss_rnnt 12.936873 hw_loss 0.372531 lr 0.00046334 rank 3
2023-02-21 11:04:00,319 DEBUG TRAIN Batch 13/9000 loss 10.403599 loss_att 13.668243 loss_ctc 12.471407 loss_rnnt 9.301670 hw_loss 0.324922 lr 0.00046334 rank 7
2023-02-21 11:04:00,321 DEBUG TRAIN Batch 13/9000 loss 15.030915 loss_att 19.034962 loss_ctc 17.523941 loss_rnnt 13.802692 hw_loss 0.178144 lr 0.00046334 rank 1
2023-02-21 11:04:00,368 DEBUG TRAIN Batch 13/9000 loss 20.567781 loss_att 28.302807 loss_ctc 30.427132 loss_rnnt 17.498795 hw_loss 0.388877 lr 0.00046334 rank 2
2023-02-21 11:04:58,350 DEBUG TRAIN Batch 13/9100 loss 16.237720 loss_att 18.993563 loss_ctc 18.258492 loss_rnnt 15.197245 hw_loss 0.412253 lr 0.00046324 rank 5
2023-02-21 11:04:58,351 DEBUG TRAIN Batch 13/9100 loss 2.100142 loss_att 5.451612 loss_ctc 5.484117 loss_rnnt 0.779380 hw_loss 0.373633 lr 0.00046324 rank 7
2023-02-21 11:04:58,352 DEBUG TRAIN Batch 13/9100 loss 5.134583 loss_att 6.238577 loss_ctc 8.180581 loss_rnnt 4.506274 hw_loss 0.002583 lr 0.00046324 rank 0
2023-02-21 11:04:58,354 DEBUG TRAIN Batch 13/9100 loss 14.661003 loss_att 19.347143 loss_ctc 12.273926 loss_rnnt 14.040674 hw_loss 0.002583 lr 0.00046324 rank 6
2023-02-21 11:04:58,355 DEBUG TRAIN Batch 13/9100 loss 27.955666 loss_att 37.142136 loss_ctc 54.857525 loss_rnnt 22.530081 hw_loss 0.002583 lr 0.00046324 rank 2
2023-02-21 11:04:58,360 DEBUG TRAIN Batch 13/9100 loss 11.529895 loss_att 13.318474 loss_ctc 17.772240 loss_rnnt 10.338489 hw_loss 0.002583 lr 0.00046324 rank 3
2023-02-21 11:04:58,360 DEBUG TRAIN Batch 13/9100 loss 10.791511 loss_att 18.360342 loss_ctc 10.531319 loss_rnnt 9.076744 hw_loss 0.441924 lr 0.00046324 rank 1
2023-02-21 11:04:58,359 DEBUG TRAIN Batch 13/9100 loss 49.988010 loss_att 62.628155 loss_ctc 65.368759 loss_rnnt 45.407837 hw_loss 0.002583 lr 0.00046324 rank 4
2023-02-21 11:05:53,971 DEBUG TRAIN Batch 13/9200 loss 11.098169 loss_att 9.849459 loss_ctc 19.052269 loss_rnnt 10.013991 hw_loss 0.512576 lr 0.00046314 rank 2
2023-02-21 11:05:53,975 DEBUG TRAIN Batch 13/9200 loss 6.394746 loss_att 9.822531 loss_ctc 7.227459 loss_rnnt 5.419067 hw_loss 0.335800 lr 0.00046314 rank 4
2023-02-21 11:05:53,977 DEBUG TRAIN Batch 13/9200 loss 30.708305 loss_att 32.347290 loss_ctc 53.958588 loss_rnnt 27.196505 hw_loss 0.157434 lr 0.00046314 rank 0
2023-02-21 11:05:53,978 DEBUG TRAIN Batch 13/9200 loss 17.711494 loss_att 18.822180 loss_ctc 18.535006 loss_rnnt 17.267143 hw_loss 0.210772 lr 0.00046314 rank 3
2023-02-21 11:05:53,983 DEBUG TRAIN Batch 13/9200 loss 17.595858 loss_att 16.622679 loss_ctc 14.664920 loss_rnnt 17.911058 hw_loss 0.506675 lr 0.00046314 rank 6
2023-02-21 11:05:53,989 DEBUG TRAIN Batch 13/9200 loss 12.709052 loss_att 15.491482 loss_ctc 17.450159 loss_rnnt 11.494978 hw_loss 0.047699 lr 0.00046314 rank 1
2023-02-21 11:05:54,013 DEBUG TRAIN Batch 13/9200 loss 12.084994 loss_att 12.771198 loss_ctc 13.728325 loss_rnnt 11.495390 hw_loss 0.437349 lr 0.00046314 rank 7
2023-02-21 11:05:54,025 DEBUG TRAIN Batch 13/9200 loss 14.288547 loss_att 21.036201 loss_ctc 17.848352 loss_rnnt 12.211465 hw_loss 0.474208 lr 0.00046314 rank 5
2023-02-21 11:06:50,929 DEBUG TRAIN Batch 13/9300 loss 12.868144 loss_att 13.189407 loss_ctc 15.226620 loss_rnnt 12.236764 hw_loss 0.473746 lr 0.00046304 rank 0
2023-02-21 11:06:50,929 DEBUG TRAIN Batch 13/9300 loss 2.156946 loss_att 4.951485 loss_ctc 3.713264 loss_rnnt 1.169042 hw_loss 0.415288 lr 0.00046304 rank 2
2023-02-21 11:06:50,931 DEBUG TRAIN Batch 13/9300 loss 24.054504 loss_att 27.506615 loss_ctc 27.399830 loss_rnnt 22.769855 hw_loss 0.277845 lr 0.00046304 rank 4
2023-02-21 11:06:50,935 DEBUG TRAIN Batch 13/9300 loss 1.388654 loss_att 3.471222 loss_ctc 0.978032 loss_rnnt 0.640327 hw_loss 0.724806 lr 0.00046304 rank 3
2023-02-21 11:06:50,936 DEBUG TRAIN Batch 13/9300 loss 7.222165 loss_att 10.210900 loss_ctc 10.966077 loss_rnnt 5.913906 hw_loss 0.396232 lr 0.00046304 rank 7
2023-02-21 11:06:50,938 DEBUG TRAIN Batch 13/9300 loss 13.513835 loss_att 19.643684 loss_ctc 21.870956 loss_rnnt 10.955656 hw_loss 0.408613 lr 0.00046304 rank 1
2023-02-21 11:06:50,938 DEBUG TRAIN Batch 13/9300 loss 10.082449 loss_att 17.196878 loss_ctc 11.599828 loss_rnnt 8.370824 hw_loss 0.162041 lr 0.00046304 rank 5
2023-02-21 11:06:50,942 DEBUG TRAIN Batch 13/9300 loss 17.389887 loss_att 21.633575 loss_ctc 23.013353 loss_rnnt 15.568913 hw_loss 0.417075 lr 0.00046304 rank 6
2023-02-21 11:07:49,881 DEBUG TRAIN Batch 13/9400 loss 3.826890 loss_att 10.937905 loss_ctc 4.087393 loss_rnnt 2.275871 hw_loss 0.176406 lr 0.00046294 rank 2
2023-02-21 11:07:49,883 DEBUG TRAIN Batch 13/9400 loss 25.424431 loss_att 31.405973 loss_ctc 30.824587 loss_rnnt 23.358480 hw_loss 0.280541 lr 0.00046294 rank 5
2023-02-21 11:07:49,889 DEBUG TRAIN Batch 13/9400 loss 13.298110 loss_att 18.754646 loss_ctc 18.644312 loss_rnnt 11.224263 hw_loss 0.505710 lr 0.00046294 rank 1
2023-02-21 11:07:49,890 DEBUG TRAIN Batch 13/9400 loss 4.177640 loss_att 9.550582 loss_ctc 11.133443 loss_rnnt 1.890833 hw_loss 0.533960 lr 0.00046294 rank 0
2023-02-21 11:07:49,900 DEBUG TRAIN Batch 13/9400 loss 17.036684 loss_att 17.398521 loss_ctc 14.620235 loss_rnnt 17.073408 hw_loss 0.399568 lr 0.00046294 rank 6
2023-02-21 11:07:49,909 DEBUG TRAIN Batch 13/9400 loss 28.876413 loss_att 25.912468 loss_ctc 40.791935 loss_rnnt 27.790756 hw_loss 0.168205 lr 0.00046294 rank 7
2023-02-21 11:07:49,910 DEBUG TRAIN Batch 13/9400 loss 10.351151 loss_att 12.289612 loss_ctc 13.432084 loss_rnnt 9.552647 hw_loss 0.000040 lr 0.00046294 rank 4
2023-02-21 11:07:49,910 DEBUG TRAIN Batch 13/9400 loss 5.419842 loss_att 14.432589 loss_ctc 8.942022 loss_rnnt 3.018363 hw_loss 0.242448 lr 0.00046294 rank 3
2023-02-21 11:09:42,096 DEBUG TRAIN Batch 13/9500 loss 10.162219 loss_att 12.938013 loss_ctc 10.863526 loss_rnnt 9.513083 hw_loss 0.000879 lr 0.00046284 rank 2
2023-02-21 11:09:42,098 DEBUG TRAIN Batch 13/9500 loss 9.074868 loss_att 11.068560 loss_ctc 13.796653 loss_rnnt 7.889433 hw_loss 0.294611 lr 0.00046284 rank 3
2023-02-21 11:09:42,098 DEBUG TRAIN Batch 13/9500 loss 19.691624 loss_att 22.520939 loss_ctc 22.474533 loss_rnnt 18.510586 hw_loss 0.457724 lr 0.00046284 rank 5
2023-02-21 11:09:42,105 DEBUG TRAIN Batch 13/9500 loss 15.424002 loss_att 23.090416 loss_ctc 27.636208 loss_rnnt 12.116161 hw_loss 0.274243 lr 0.00046284 rank 6
2023-02-21 11:09:42,114 DEBUG TRAIN Batch 13/9500 loss 6.154593 loss_att 9.057386 loss_ctc 10.275658 loss_rnnt 4.812284 hw_loss 0.398017 lr 0.00046284 rank 4
2023-02-21 11:09:42,116 DEBUG TRAIN Batch 13/9500 loss 13.693256 loss_att 18.711954 loss_ctc 18.663979 loss_rnnt 11.890024 hw_loss 0.256366 lr 0.00046284 rank 1
2023-02-21 11:09:42,122 DEBUG TRAIN Batch 13/9500 loss 3.523381 loss_att 7.819457 loss_ctc 6.010298 loss_rnnt 2.248789 hw_loss 0.157103 lr 0.00046284 rank 7
2023-02-21 11:09:42,164 DEBUG TRAIN Batch 13/9500 loss 16.563850 loss_att 17.916500 loss_ctc 15.813603 loss_rnnt 16.219954 hw_loss 0.325127 lr 0.00046284 rank 0
2023-02-21 11:10:40,523 DEBUG TRAIN Batch 13/9600 loss 13.603003 loss_att 16.703989 loss_ctc 22.044712 loss_rnnt 11.781934 hw_loss 0.141205 lr 0.00046274 rank 2
2023-02-21 11:10:40,526 DEBUG TRAIN Batch 13/9600 loss 18.254171 loss_att 24.094595 loss_ctc 26.826950 loss_rnnt 15.751953 hw_loss 0.358304 lr 0.00046274 rank 5
2023-02-21 11:10:40,527 DEBUG TRAIN Batch 13/9600 loss 11.083453 loss_att 13.031515 loss_ctc 17.870491 loss_rnnt 9.623067 hw_loss 0.310941 lr 0.00046274 rank 0
2023-02-21 11:10:40,529 DEBUG TRAIN Batch 13/9600 loss 7.914986 loss_att 8.426158 loss_ctc 12.517868 loss_rnnt 6.968115 hw_loss 0.432975 lr 0.00046274 rank 7
2023-02-21 11:10:40,530 DEBUG TRAIN Batch 13/9600 loss 26.403732 loss_att 31.291819 loss_ctc 39.947193 loss_rnnt 23.415617 hw_loss 0.383818 lr 0.00046274 rank 3
2023-02-21 11:10:40,536 DEBUG TRAIN Batch 13/9600 loss 31.750679 loss_att 34.110844 loss_ctc 40.151722 loss_rnnt 29.880772 hw_loss 0.520748 lr 0.00046274 rank 6
2023-02-21 11:10:40,536 DEBUG TRAIN Batch 13/9600 loss 5.944150 loss_att 6.881518 loss_ctc 5.351830 loss_rnnt 5.628988 hw_loss 0.387496 lr 0.00046274 rank 1
2023-02-21 11:10:40,587 DEBUG TRAIN Batch 13/9600 loss 20.328972 loss_att 21.352047 loss_ctc 22.370329 loss_rnnt 19.673389 hw_loss 0.335225 lr 0.00046274 rank 4
2023-02-21 11:11:37,843 DEBUG TRAIN Batch 13/9700 loss 6.332904 loss_att 8.825523 loss_ctc 16.803228 loss_rnnt 4.254819 hw_loss 0.344097 lr 0.00046264 rank 0
2023-02-21 11:11:37,847 DEBUG TRAIN Batch 13/9700 loss 7.411984 loss_att 9.788940 loss_ctc 7.907973 loss_rnnt 6.695105 hw_loss 0.328794 lr 0.00046264 rank 2
2023-02-21 11:11:37,850 DEBUG TRAIN Batch 13/9700 loss 11.850986 loss_att 19.878059 loss_ctc 17.537254 loss_rnnt 9.410435 hw_loss 0.144314 lr 0.00046264 rank 3
2023-02-21 11:11:37,853 DEBUG TRAIN Batch 13/9700 loss 9.925259 loss_att 21.102680 loss_ctc 11.233379 loss_rnnt 7.410773 hw_loss 0.196096 lr 0.00046264 rank 5
2023-02-21 11:11:37,854 DEBUG TRAIN Batch 13/9700 loss 14.049260 loss_att 13.449017 loss_ctc 12.405227 loss_rnnt 14.116548 hw_loss 0.509937 lr 0.00046264 rank 6
2023-02-21 11:11:37,855 DEBUG TRAIN Batch 13/9700 loss 7.055993 loss_att 10.948963 loss_ctc 9.531425 loss_rnnt 5.796113 hw_loss 0.283553 lr 0.00046264 rank 7
2023-02-21 11:11:37,858 DEBUG TRAIN Batch 13/9700 loss 2.602340 loss_att 7.412904 loss_ctc 2.581172 loss_rnnt 1.396710 hw_loss 0.461886 lr 0.00046264 rank 4
2023-02-21 11:11:37,860 DEBUG TRAIN Batch 13/9700 loss 11.364080 loss_att 17.034349 loss_ctc 13.006247 loss_rnnt 9.937504 hw_loss 0.137939 lr 0.00046264 rank 1
2023-02-21 11:12:34,224 DEBUG TRAIN Batch 13/9800 loss 33.322868 loss_att 33.592335 loss_ctc 49.249683 loss_rnnt 30.924549 hw_loss 0.414091 lr 0.00046254 rank 4
2023-02-21 11:12:34,224 DEBUG TRAIN Batch 13/9800 loss 11.913785 loss_att 11.719852 loss_ctc 11.395938 loss_rnnt 11.836467 hw_loss 0.347158 lr 0.00046254 rank 2
2023-02-21 11:12:34,225 DEBUG TRAIN Batch 13/9800 loss 20.112255 loss_att 19.321293 loss_ctc 21.208767 loss_rnnt 19.886618 hw_loss 0.445558 lr 0.00046254 rank 0
2023-02-21 11:12:34,226 DEBUG TRAIN Batch 13/9800 loss 8.750702 loss_att 11.452185 loss_ctc 10.356029 loss_rnnt 7.937621 hw_loss 0.110140 lr 0.00046254 rank 3
2023-02-21 11:12:34,226 DEBUG TRAIN Batch 13/9800 loss 9.245145 loss_att 13.749228 loss_ctc 13.203732 loss_rnnt 7.695869 hw_loss 0.226214 lr 0.00046254 rank 5
2023-02-21 11:12:34,229 DEBUG TRAIN Batch 13/9800 loss 12.681252 loss_att 14.568447 loss_ctc 17.354513 loss_rnnt 11.484218 hw_loss 0.368424 lr 0.00046254 rank 6
2023-02-21 11:12:34,232 DEBUG TRAIN Batch 13/9800 loss 14.526720 loss_att 17.848879 loss_ctc 22.529648 loss_rnnt 12.631914 hw_loss 0.306218 lr 0.00046254 rank 1
2023-02-21 11:12:34,239 DEBUG TRAIN Batch 13/9800 loss 15.304742 loss_att 14.690590 loss_ctc 21.870752 loss_rnnt 14.491226 hw_loss 0.114146 lr 0.00046254 rank 7
2023-02-21 11:13:32,102 DEBUG TRAIN Batch 13/9900 loss 18.629467 loss_att 19.342632 loss_ctc 19.116583 loss_rnnt 18.226194 hw_loss 0.366922 lr 0.00046245 rank 2
2023-02-21 11:13:32,104 DEBUG TRAIN Batch 13/9900 loss 16.079241 loss_att 20.545156 loss_ctc 25.526861 loss_rnnt 13.727604 hw_loss 0.372697 lr 0.00046245 rank 3
2023-02-21 11:13:32,107 DEBUG TRAIN Batch 13/9900 loss 9.701860 loss_att 13.283440 loss_ctc 11.876481 loss_rnnt 8.509314 hw_loss 0.349278 lr 0.00046245 rank 5
2023-02-21 11:13:32,109 DEBUG TRAIN Batch 13/9900 loss 9.279170 loss_att 19.229008 loss_ctc 11.271230 loss_rnnt 6.908398 hw_loss 0.215996 lr 0.00046245 rank 0
2023-02-21 11:13:32,112 DEBUG TRAIN Batch 13/9900 loss 12.117858 loss_att 15.658424 loss_ctc 23.760670 loss_rnnt 9.634541 hw_loss 0.417804 lr 0.00046245 rank 4
2023-02-21 11:13:32,113 DEBUG TRAIN Batch 13/9900 loss 8.806084 loss_att 9.065672 loss_ctc 9.422910 loss_rnnt 8.669409 hw_loss 0.004713 lr 0.00046245 rank 6
2023-02-21 11:13:32,114 DEBUG TRAIN Batch 13/9900 loss 19.327862 loss_att 18.082790 loss_ctc 30.365685 loss_rnnt 18.102655 hw_loss 0.004713 lr 0.00046245 rank 7
2023-02-21 11:13:32,119 DEBUG TRAIN Batch 13/9900 loss 19.277079 loss_att 30.869976 loss_ctc 23.536097 loss_rnnt 16.388117 hw_loss 0.004713 lr 0.00046245 rank 1
2023-02-21 11:14:29,272 DEBUG TRAIN Batch 13/10000 loss 8.613276 loss_att 9.546014 loss_ctc 10.261994 loss_rnnt 8.206764 hw_loss 0.000252 lr 0.00046235 rank 4
2023-02-21 11:14:29,274 DEBUG TRAIN Batch 13/10000 loss 22.327646 loss_att 18.316326 loss_ctc 17.624296 loss_rnnt 23.756886 hw_loss 0.000252 lr 0.00046235 rank 5
2023-02-21 11:14:29,274 DEBUG TRAIN Batch 13/10000 loss 20.634426 loss_att 20.625303 loss_ctc 20.737019 loss_rnnt 20.456955 hw_loss 0.310531 lr 0.00046235 rank 0
2023-02-21 11:14:29,277 DEBUG TRAIN Batch 13/10000 loss 16.593884 loss_att 17.526192 loss_ctc 20.934956 loss_rnnt 15.628512 hw_loss 0.375190 lr 0.00046235 rank 3
2023-02-21 11:14:29,279 DEBUG TRAIN Batch 13/10000 loss 4.138491 loss_att 6.971698 loss_ctc 7.626150 loss_rnnt 2.925932 hw_loss 0.339181 lr 0.00046235 rank 1
2023-02-21 11:14:29,279 DEBUG TRAIN Batch 13/10000 loss 18.679945 loss_att 22.380327 loss_ctc 22.903908 loss_rnnt 17.376537 hw_loss 0.000252 lr 0.00046235 rank 7
2023-02-21 11:14:29,281 DEBUG TRAIN Batch 13/10000 loss 5.305532 loss_att 9.577267 loss_ctc 7.446544 loss_rnnt 3.933932 hw_loss 0.434595 lr 0.00046235 rank 6
2023-02-21 11:14:29,329 DEBUG TRAIN Batch 13/10000 loss 12.442700 loss_att 8.799412 loss_ctc 11.342843 loss_rnnt 13.186193 hw_loss 0.247151 lr 0.00046235 rank 2
2023-02-21 11:15:26,006 DEBUG TRAIN Batch 13/10100 loss 21.516052 loss_att 22.101637 loss_ctc 24.966419 loss_rnnt 20.839338 hw_loss 0.186651 lr 0.00046225 rank 0
2023-02-21 11:15:26,016 DEBUG TRAIN Batch 13/10100 loss 10.931440 loss_att 10.588313 loss_ctc 13.786310 loss_rnnt 10.344252 hw_loss 0.515932 lr 0.00046225 rank 2
2023-02-21 11:15:26,016 DEBUG TRAIN Batch 13/10100 loss 14.966105 loss_att 17.617392 loss_ctc 18.181759 loss_rnnt 13.840878 hw_loss 0.311657 lr 0.00046225 rank 7
2023-02-21 11:15:26,020 DEBUG TRAIN Batch 13/10100 loss 24.238651 loss_att 31.737057 loss_ctc 31.371750 loss_rnnt 21.655495 hw_loss 0.248239 lr 0.00046225 rank 3
2023-02-21 11:15:26,021 DEBUG TRAIN Batch 13/10100 loss 41.836327 loss_att 41.556274 loss_ctc 62.990513 loss_rnnt 39.019482 hw_loss 0.098068 lr 0.00046225 rank 5
2023-02-21 11:15:26,023 DEBUG TRAIN Batch 13/10100 loss 14.183870 loss_att 17.552631 loss_ctc 22.103758 loss_rnnt 12.330176 hw_loss 0.232421 lr 0.00046225 rank 4
2023-02-21 11:15:26,026 DEBUG TRAIN Batch 13/10100 loss 19.043592 loss_att 21.155128 loss_ctc 25.695745 loss_rnnt 17.631674 hw_loss 0.192487 lr 0.00046225 rank 6
2023-02-21 11:15:26,084 DEBUG TRAIN Batch 13/10100 loss 4.124850 loss_att 8.221773 loss_ctc 7.618583 loss_rnnt 2.738088 hw_loss 0.190399 lr 0.00046225 rank 1
2023-02-21 11:16:24,532 DEBUG TRAIN Batch 13/10200 loss 12.980038 loss_att 13.828117 loss_ctc 18.576067 loss_rnnt 12.064075 hw_loss 0.000394 lr 0.00046215 rank 0
2023-02-21 11:16:24,538 DEBUG TRAIN Batch 13/10200 loss 26.546822 loss_att 26.180786 loss_ctc 29.843832 loss_rnnt 26.026491 hw_loss 0.288632 lr 0.00046215 rank 3
2023-02-21 11:16:24,538 DEBUG TRAIN Batch 13/10200 loss 10.909958 loss_att 10.375883 loss_ctc 9.911656 loss_rnnt 11.026213 hw_loss 0.231879 lr 0.00046215 rank 5
2023-02-21 11:16:24,539 DEBUG TRAIN Batch 13/10200 loss 23.073641 loss_att 26.971447 loss_ctc 29.926086 loss_rnnt 21.163567 hw_loss 0.406600 lr 0.00046215 rank 2
2023-02-21 11:16:24,543 DEBUG TRAIN Batch 13/10200 loss 5.290497 loss_att 10.256296 loss_ctc 7.410410 loss_rnnt 3.845597 hw_loss 0.317035 lr 0.00046215 rank 1
2023-02-21 11:16:24,543 DEBUG TRAIN Batch 13/10200 loss 2.675066 loss_att 4.859738 loss_ctc 2.655575 loss_rnnt 2.065539 hw_loss 0.328485 lr 0.00046215 rank 4
2023-02-21 11:16:24,544 DEBUG TRAIN Batch 13/10200 loss 11.996779 loss_att 15.609007 loss_ctc 15.163070 loss_rnnt 10.691624 hw_loss 0.301011 lr 0.00046215 rank 6
2023-02-21 11:16:24,548 DEBUG TRAIN Batch 13/10200 loss 8.615422 loss_att 10.588648 loss_ctc 8.211425 loss_rnnt 8.191838 hw_loss 0.155258 lr 0.00046215 rank 7
2023-02-21 11:18:12,547 DEBUG TRAIN Batch 13/10300 loss 28.187910 loss_att 28.209200 loss_ctc 30.260559 loss_rnnt 27.789312 hw_loss 0.221223 lr 0.00046205 rank 0
2023-02-21 11:18:12,549 DEBUG TRAIN Batch 13/10300 loss 1.654200 loss_att 5.457621 loss_ctc 2.375545 loss_rnnt 0.615423 hw_loss 0.341088 lr 0.00046205 rank 2
2023-02-21 11:18:12,551 DEBUG TRAIN Batch 13/10300 loss 4.676024 loss_att 7.991118 loss_ctc 8.174714 loss_rnnt 3.350070 hw_loss 0.368331 lr 0.00046205 rank 4
2023-02-21 11:18:12,554 DEBUG TRAIN Batch 13/10300 loss 19.859364 loss_att 26.056753 loss_ctc 22.517107 loss_rnnt 18.043587 hw_loss 0.416125 lr 0.00046205 rank 6
2023-02-21 11:18:12,555 DEBUG TRAIN Batch 13/10300 loss 3.865319 loss_att 7.918915 loss_ctc 2.343719 loss_rnnt 3.053773 hw_loss 0.381948 lr 0.00046205 rank 5
2023-02-21 11:18:12,556 DEBUG TRAIN Batch 13/10300 loss 2.716369 loss_att 12.318653 loss_ctc 1.883139 loss_rnnt 0.797886 hw_loss 0.204606 lr 0.00046205 rank 1
2023-02-21 11:18:12,558 DEBUG TRAIN Batch 13/10300 loss 13.456426 loss_att 15.463411 loss_ctc 14.814908 loss_rnnt 12.655326 hw_loss 0.409821 lr 0.00046205 rank 3
2023-02-21 11:18:12,559 DEBUG TRAIN Batch 13/10300 loss 2.348484 loss_att 10.320871 loss_ctc 3.356108 loss_rnnt 0.519219 hw_loss 0.188320 lr 0.00046205 rank 7
2023-02-21 11:19:19,316 DEBUG TRAIN Batch 13/10400 loss 12.823488 loss_att 12.830419 loss_ctc 15.480403 loss_rnnt 12.259207 hw_loss 0.391202 lr 0.00046195 rank 5
2023-02-21 11:19:19,318 DEBUG TRAIN Batch 13/10400 loss 16.879196 loss_att 16.888294 loss_ctc 20.933819 loss_rnnt 16.196667 hw_loss 0.262673 lr 0.00046195 rank 7
2023-02-21 11:19:19,319 DEBUG TRAIN Batch 13/10400 loss 31.602280 loss_att 31.397537 loss_ctc 41.552425 loss_rnnt 30.181753 hw_loss 0.252731 lr 0.00046195 rank 4
2023-02-21 11:19:19,319 DEBUG TRAIN Batch 13/10400 loss 4.522842 loss_att 10.127000 loss_ctc 5.153719 loss_rnnt 3.132937 hw_loss 0.346794 lr 0.00046195 rank 0
2023-02-21 11:19:19,321 DEBUG TRAIN Batch 13/10400 loss 9.112009 loss_att 8.494104 loss_ctc 13.466171 loss_rnnt 8.509859 hw_loss 0.272205 lr 0.00046195 rank 2
2023-02-21 11:19:19,324 DEBUG TRAIN Batch 13/10400 loss 13.142507 loss_att 13.460226 loss_ctc 15.876480 loss_rnnt 12.565292 hw_loss 0.279638 lr 0.00046195 rank 6
2023-02-21 11:19:19,329 DEBUG TRAIN Batch 13/10400 loss 27.324917 loss_att 29.769703 loss_ctc 30.780190 loss_rnnt 26.154562 hw_loss 0.413805 lr 0.00046195 rank 1
2023-02-21 11:19:19,377 DEBUG TRAIN Batch 13/10400 loss 11.275150 loss_att 11.755067 loss_ctc 12.857036 loss_rnnt 10.823483 hw_loss 0.271434 lr 0.00046195 rank 3
2023-02-21 11:20:17,578 DEBUG TRAIN Batch 13/10500 loss 9.471359 loss_att 12.011780 loss_ctc 19.218510 loss_rnnt 7.365962 hw_loss 0.558175 lr 0.00046185 rank 2
2023-02-21 11:20:17,581 DEBUG TRAIN Batch 13/10500 loss 11.443332 loss_att 12.598976 loss_ctc 13.111171 loss_rnnt 10.989759 hw_loss 0.000122 lr 0.00046185 rank 5
2023-02-21 11:20:17,587 DEBUG TRAIN Batch 13/10500 loss 11.108761 loss_att 22.272945 loss_ctc 19.405764 loss_rnnt 7.576679 hw_loss 0.361833 lr 0.00046185 rank 3
2023-02-21 11:20:17,587 DEBUG TRAIN Batch 13/10500 loss 7.031741 loss_att 10.030848 loss_ctc 9.169853 loss_rnnt 6.046665 hw_loss 0.187824 lr 0.00046185 rank 0
2023-02-21 11:20:17,591 DEBUG TRAIN Batch 13/10500 loss 16.992962 loss_att 17.431959 loss_ctc 16.668676 loss_rnnt 16.948334 hw_loss 0.000122 lr 0.00046185 rank 1
2023-02-21 11:20:17,594 DEBUG TRAIN Batch 13/10500 loss 8.668562 loss_att 10.558129 loss_ctc 13.198257 loss_rnnt 7.493476 hw_loss 0.362274 lr 0.00046185 rank 4
2023-02-21 11:20:17,598 DEBUG TRAIN Batch 13/10500 loss 1.059687 loss_att 2.747190 loss_ctc 0.506293 loss_rnnt 0.296577 hw_loss 0.936366 lr 0.00046185 rank 7
2023-02-21 11:20:17,598 DEBUG TRAIN Batch 13/10500 loss 5.281052 loss_att 8.889359 loss_ctc 8.121422 loss_rnnt 4.045534 hw_loss 0.253389 lr 0.00046185 rank 6
2023-02-21 11:21:13,700 DEBUG TRAIN Batch 13/10600 loss 23.000284 loss_att 30.851435 loss_ctc 23.716032 loss_rnnt 21.088112 hw_loss 0.462205 lr 0.00046176 rank 2
2023-02-21 11:21:13,701 DEBUG TRAIN Batch 13/10600 loss 6.169419 loss_att 6.391332 loss_ctc 3.273471 loss_rnnt 6.380426 hw_loss 0.245132 lr 0.00046176 rank 0
2023-02-21 11:21:13,701 DEBUG TRAIN Batch 13/10600 loss 39.103928 loss_att 39.036827 loss_ctc 46.778912 loss_rnnt 37.876507 hw_loss 0.407829 lr 0.00046176 rank 5
2023-02-21 11:21:13,702 DEBUG TRAIN Batch 13/10600 loss 14.403340 loss_att 19.154726 loss_ctc 19.210381 loss_rnnt 12.622547 hw_loss 0.355459 lr 0.00046176 rank 3
2023-02-21 11:21:13,704 DEBUG TRAIN Batch 13/10600 loss 13.494075 loss_att 13.303422 loss_ctc 14.020599 loss_rnnt 13.165176 hw_loss 0.556550 lr 0.00046176 rank 6
2023-02-21 11:21:13,715 DEBUG TRAIN Batch 13/10600 loss 4.154586 loss_att 9.532549 loss_ctc 10.580622 loss_rnnt 2.222096 hw_loss 0.000175 lr 0.00046176 rank 1
2023-02-21 11:21:13,715 DEBUG TRAIN Batch 13/10600 loss 8.714701 loss_att 9.570587 loss_ctc 3.886713 loss_rnnt 8.945263 hw_loss 0.453735 lr 0.00046176 rank 7
2023-02-21 11:21:13,758 DEBUG TRAIN Batch 13/10600 loss 17.626081 loss_att 20.944845 loss_ctc 37.981789 loss_rnnt 14.136321 hw_loss 0.209836 lr 0.00046176 rank 4
2023-02-21 11:22:11,367 DEBUG TRAIN Batch 13/10700 loss 6.063820 loss_att 7.410646 loss_ctc 5.684045 loss_rnnt 5.640556 hw_loss 0.383505 lr 0.00046166 rank 2
2023-02-21 11:22:11,369 DEBUG TRAIN Batch 13/10700 loss 11.025952 loss_att 10.593400 loss_ctc 14.001861 loss_rnnt 10.566492 hw_loss 0.279717 lr 0.00046166 rank 5
2023-02-21 11:22:11,371 DEBUG TRAIN Batch 13/10700 loss 4.000815 loss_att 5.643661 loss_ctc 5.763056 loss_rnnt 3.437257 hw_loss 0.000043 lr 0.00046166 rank 0
2023-02-21 11:22:11,374 DEBUG TRAIN Batch 13/10700 loss 16.761843 loss_att 19.910376 loss_ctc 17.169449 loss_rnnt 15.839265 hw_loss 0.447231 lr 0.00046166 rank 4
2023-02-21 11:22:11,379 DEBUG TRAIN Batch 13/10700 loss 12.761780 loss_att 13.980395 loss_ctc 17.526955 loss_rnnt 11.668839 hw_loss 0.400992 lr 0.00046166 rank 3
2023-02-21 11:22:11,384 DEBUG TRAIN Batch 13/10700 loss 9.589141 loss_att 15.185334 loss_ctc 17.542227 loss_rnnt 7.333063 hw_loss 0.143301 lr 0.00046166 rank 6
2023-02-21 11:22:11,387 DEBUG TRAIN Batch 13/10700 loss 13.080046 loss_att 15.248711 loss_ctc 17.329689 loss_rnnt 11.932954 hw_loss 0.275135 lr 0.00046166 rank 7
2023-02-21 11:22:11,389 DEBUG TRAIN Batch 13/10700 loss 17.731684 loss_att 20.552771 loss_ctc 20.793087 loss_rnnt 16.584408 hw_loss 0.327881 lr 0.00046166 rank 1
2023-02-21 11:23:10,686 DEBUG TRAIN Batch 13/10800 loss 12.303636 loss_att 16.868378 loss_ctc 17.567186 loss_rnnt 10.468070 hw_loss 0.414021 lr 0.00046156 rank 2
2023-02-21 11:23:10,692 DEBUG TRAIN Batch 13/10800 loss 6.554979 loss_att 9.814067 loss_ctc 6.412537 loss_rnnt 5.678120 hw_loss 0.457566 lr 0.00046156 rank 3
2023-02-21 11:23:10,693 DEBUG TRAIN Batch 13/10800 loss 15.602686 loss_att 15.787149 loss_ctc 20.779003 loss_rnnt 14.736492 hw_loss 0.260858 lr 0.00046156 rank 0
2023-02-21 11:23:10,694 DEBUG TRAIN Batch 13/10800 loss 28.010481 loss_att 33.312950 loss_ctc 41.201260 loss_rnnt 25.055412 hw_loss 0.254629 lr 0.00046156 rank 7
2023-02-21 11:23:10,697 DEBUG TRAIN Batch 13/10800 loss 9.834602 loss_att 14.188482 loss_ctc 14.034719 loss_rnnt 8.272302 hw_loss 0.246576 lr 0.00046156 rank 5
2023-02-21 11:23:10,698 DEBUG TRAIN Batch 13/10800 loss 10.169069 loss_att 13.264309 loss_ctc 7.753690 loss_rnnt 9.871145 hw_loss 0.001735 lr 0.00046156 rank 4
2023-02-21 11:23:10,701 DEBUG TRAIN Batch 13/10800 loss 29.283176 loss_att 29.138168 loss_ctc 46.414967 loss_rnnt 26.881626 hw_loss 0.274335 lr 0.00046156 rank 6
2023-02-21 11:23:10,749 DEBUG TRAIN Batch 13/10800 loss 2.181921 loss_att 5.474754 loss_ctc 5.167842 loss_rnnt 0.870226 hw_loss 0.478133 lr 0.00046156 rank 1
2023-02-21 11:24:06,545 DEBUG TRAIN Batch 13/10900 loss 23.824963 loss_att 26.079983 loss_ctc 34.610733 loss_rnnt 21.777151 hw_loss 0.297576 lr 0.00046146 rank 0
2023-02-21 11:24:06,548 DEBUG TRAIN Batch 13/10900 loss 17.408388 loss_att 13.600902 loss_ctc 18.911741 loss_rnnt 17.767387 hw_loss 0.378844 lr 0.00046146 rank 4
2023-02-21 11:24:06,549 DEBUG TRAIN Batch 13/10900 loss 4.526924 loss_att 6.753474 loss_ctc 9.483826 loss_rnnt 3.420608 hw_loss 0.000160 lr 0.00046146 rank 5
2023-02-21 11:24:06,549 DEBUG TRAIN Batch 13/10900 loss 9.263130 loss_att 10.437809 loss_ctc 10.942488 loss_rnnt 8.621358 hw_loss 0.342979 lr 0.00046146 rank 3
2023-02-21 11:24:06,554 DEBUG TRAIN Batch 13/10900 loss 2.665406 loss_att 7.565083 loss_ctc 3.536442 loss_rnnt 1.501176 hw_loss 0.127793 lr 0.00046146 rank 2
2023-02-21 11:24:06,559 DEBUG TRAIN Batch 13/10900 loss 62.244339 loss_att 62.766949 loss_ctc 79.539558 loss_rnnt 59.833698 hw_loss 0.000160 lr 0.00046146 rank 7
2023-02-21 11:24:06,559 DEBUG TRAIN Batch 13/10900 loss 11.738442 loss_att 12.135914 loss_ctc 13.549640 loss_rnnt 11.159363 hw_loss 0.483922 lr 0.00046146 rank 1
2023-02-21 11:24:06,559 DEBUG TRAIN Batch 13/10900 loss 17.099371 loss_att 20.254061 loss_ctc 19.893574 loss_rnnt 16.033159 hw_loss 0.117589 lr 0.00046146 rank 6
2023-02-21 11:25:02,408 DEBUG TRAIN Batch 13/11000 loss 13.509607 loss_att 14.250845 loss_ctc 17.906208 loss_rnnt 12.718612 hw_loss 0.106004 lr 0.00046136 rank 2
2023-02-21 11:25:02,409 DEBUG TRAIN Batch 13/11000 loss 4.070143 loss_att 7.063465 loss_ctc 5.624239 loss_rnnt 3.174075 hw_loss 0.169107 lr 0.00046136 rank 6
2023-02-21 11:25:02,411 DEBUG TRAIN Batch 13/11000 loss 21.717783 loss_att 19.408970 loss_ctc 38.676685 loss_rnnt 19.599884 hw_loss 0.597140 lr 0.00046136 rank 5
2023-02-21 11:25:02,412 DEBUG TRAIN Batch 13/11000 loss 8.928335 loss_att 12.649639 loss_ctc 12.302439 loss_rnnt 7.682362 hw_loss 0.097185 lr 0.00046136 rank 4
2023-02-21 11:25:02,413 DEBUG TRAIN Batch 13/11000 loss 12.496009 loss_att 16.054590 loss_ctc 19.189009 loss_rnnt 10.891876 hw_loss 0.000032 lr 0.00046136 rank 7
2023-02-21 11:25:02,418 DEBUG TRAIN Batch 13/11000 loss 8.733256 loss_att 11.370895 loss_ctc 9.333307 loss_rnnt 8.125706 hw_loss 0.000032 lr 0.00046136 rank 0
2023-02-21 11:25:02,419 DEBUG TRAIN Batch 13/11000 loss 26.173906 loss_att 29.966927 loss_ctc 30.510645 loss_rnnt 24.782356 hw_loss 0.102588 lr 0.00046136 rank 3
2023-02-21 11:25:02,481 DEBUG TRAIN Batch 13/11000 loss 15.715421 loss_att 23.025404 loss_ctc 17.929369 loss_rnnt 13.841928 hw_loss 0.218064 lr 0.00046136 rank 1
2023-02-21 11:26:02,036 DEBUG TRAIN Batch 13/11100 loss 15.296730 loss_att 22.841475 loss_ctc 21.286175 loss_rnnt 12.861471 hw_loss 0.239472 lr 0.00046126 rank 7
2023-02-21 11:26:02,040 DEBUG TRAIN Batch 13/11100 loss 38.603294 loss_att 34.686443 loss_ctc 47.449879 loss_rnnt 38.060947 hw_loss 0.274062 lr 0.00046126 rank 5
2023-02-21 11:26:02,041 DEBUG TRAIN Batch 13/11100 loss 11.823005 loss_att 16.184353 loss_ctc 9.937575 loss_rnnt 11.060122 hw_loss 0.266256 lr 0.00046126 rank 0
2023-02-21 11:26:02,043 DEBUG TRAIN Batch 13/11100 loss 15.520403 loss_att 19.939976 loss_ctc 18.211227 loss_rnnt 14.086040 hw_loss 0.359386 lr 0.00046126 rank 6
2023-02-21 11:26:02,049 DEBUG TRAIN Batch 13/11100 loss 33.935379 loss_att 44.743622 loss_ctc 57.715797 loss_rnnt 28.433252 hw_loss 0.318293 lr 0.00046126 rank 2
2023-02-21 11:26:02,055 DEBUG TRAIN Batch 13/11100 loss 9.786510 loss_att 17.806307 loss_ctc 9.537786 loss_rnnt 8.027813 hw_loss 0.352315 lr 0.00046126 rank 3
2023-02-21 11:26:02,071 DEBUG TRAIN Batch 13/11100 loss 33.029064 loss_att 41.605000 loss_ctc 54.047913 loss_rnnt 28.364079 hw_loss 0.276153 lr 0.00046126 rank 1
2023-02-21 11:26:02,101 DEBUG TRAIN Batch 13/11100 loss 7.951863 loss_att 10.090590 loss_ctc 8.386465 loss_rnnt 7.466054 hw_loss 0.000218 lr 0.00046126 rank 4
2023-02-21 11:27:51,484 DEBUG TRAIN Batch 13/11200 loss 17.345707 loss_att 19.783871 loss_ctc 23.874725 loss_rnnt 15.715769 hw_loss 0.509566 lr 0.00046117 rank 4
2023-02-21 11:27:51,488 DEBUG TRAIN Batch 13/11200 loss 11.832405 loss_att 17.097290 loss_ctc 12.115657 loss_rnnt 10.741561 hw_loss 0.000189 lr 0.00046117 rank 0
2023-02-21 11:27:51,493 DEBUG TRAIN Batch 13/11200 loss 18.211498 loss_att 21.665987 loss_ctc 28.559319 loss_rnnt 15.989234 hw_loss 0.284356 lr 0.00046117 rank 2
2023-02-21 11:27:51,493 DEBUG TRAIN Batch 13/11200 loss 12.668912 loss_att 13.592403 loss_ctc 20.608040 loss_rnnt 11.425563 hw_loss 0.000189 lr 0.00046117 rank 3
2023-02-21 11:27:51,495 DEBUG TRAIN Batch 13/11200 loss 32.665272 loss_att 29.202526 loss_ctc 34.895370 loss_rnnt 32.893780 hw_loss 0.312560 lr 0.00046117 rank 5
2023-02-21 11:27:51,496 DEBUG TRAIN Batch 13/11200 loss 8.267733 loss_att 8.221085 loss_ctc 7.614608 loss_rnnt 8.144514 hw_loss 0.411808 lr 0.00046117 rank 6
2023-02-21 11:27:51,495 DEBUG TRAIN Batch 13/11200 loss 5.047896 loss_att 9.640910 loss_ctc 10.960184 loss_rnnt 3.189074 hw_loss 0.284840 lr 0.00046117 rank 7
2023-02-21 11:27:51,501 DEBUG TRAIN Batch 13/11200 loss 16.596697 loss_att 16.861362 loss_ctc 19.859837 loss_rnnt 15.912848 hw_loss 0.367182 lr 0.00046117 rank 1
2023-02-21 11:29:40,633 DEBUG TRAIN Batch 13/11300 loss 27.718004 loss_att 26.676178 loss_ctc 31.736343 loss_rnnt 27.390457 hw_loss 0.000255 lr 0.00046107 rank 0
2023-02-21 11:29:40,633 DEBUG TRAIN Batch 13/11300 loss 5.717772 loss_att 9.560472 loss_ctc 6.449723 loss_rnnt 4.690634 hw_loss 0.301884 lr 0.00046107 rank 3
2023-02-21 11:29:40,635 DEBUG TRAIN Batch 13/11300 loss 14.225190 loss_att 18.928793 loss_ctc 16.353865 loss_rnnt 12.791823 hw_loss 0.391542 lr 0.00046107 rank 2
2023-02-21 11:29:40,639 DEBUG TRAIN Batch 13/11300 loss 10.893097 loss_att 13.120855 loss_ctc 14.602203 loss_rnnt 9.900682 hw_loss 0.098090 lr 0.00046107 rank 5
2023-02-21 11:29:40,640 DEBUG TRAIN Batch 13/11300 loss 11.676573 loss_att 15.463022 loss_ctc 19.294641 loss_rnnt 9.773359 hw_loss 0.244090 lr 0.00046107 rank 4
2023-02-21 11:29:40,642 DEBUG TRAIN Batch 13/11300 loss 9.748457 loss_att 11.406700 loss_ctc 16.095060 loss_rnnt 8.372927 hw_loss 0.370627 lr 0.00046107 rank 7
2023-02-21 11:29:40,649 DEBUG TRAIN Batch 13/11300 loss 16.156857 loss_att 18.234173 loss_ctc 20.013691 loss_rnnt 15.172073 hw_loss 0.103262 lr 0.00046107 rank 6
2023-02-21 11:29:40,650 DEBUG TRAIN Batch 13/11300 loss 34.350227 loss_att 43.462048 loss_ctc 44.990665 loss_rnnt 30.936626 hw_loss 0.323460 lr 0.00046107 rank 1
2023-02-21 11:30:39,052 DEBUG TRAIN Batch 13/11400 loss 4.718308 loss_att 7.396322 loss_ctc 6.079137 loss_rnnt 4.001206 hw_loss 0.000104 lr 0.00046097 rank 2
2023-02-21 11:30:39,055 DEBUG TRAIN Batch 13/11400 loss 6.888009 loss_att 11.584723 loss_ctc 15.585092 loss_rnnt 4.629076 hw_loss 0.299962 lr 0.00046097 rank 4
2023-02-21 11:30:39,055 DEBUG TRAIN Batch 13/11400 loss 18.113226 loss_att 18.515324 loss_ctc 17.364199 loss_rnnt 17.892378 hw_loss 0.450564 lr 0.00046097 rank 5
2023-02-21 11:30:39,057 DEBUG TRAIN Batch 13/11400 loss 23.148096 loss_att 33.762398 loss_ctc 27.015972 loss_rnnt 20.303045 hw_loss 0.387141 lr 0.00046097 rank 0
2023-02-21 11:30:39,060 DEBUG TRAIN Batch 13/11400 loss 57.125359 loss_att 45.906532 loss_ctc 67.251060 loss_rnnt 57.799675 hw_loss 0.411300 lr 0.00046097 rank 3
2023-02-21 11:30:39,060 DEBUG TRAIN Batch 13/11400 loss 3.729522 loss_att 7.114260 loss_ctc 3.474628 loss_rnnt 2.837646 hw_loss 0.466714 lr 0.00046097 rank 7
2023-02-21 11:30:39,066 DEBUG TRAIN Batch 13/11400 loss 19.000229 loss_att 30.017162 loss_ctc 18.025673 loss_rnnt 16.736679 hw_loss 0.356448 lr 0.00046097 rank 1
2023-02-21 11:30:39,069 DEBUG TRAIN Batch 13/11400 loss 3.044241 loss_att 10.987973 loss_ctc 1.165529 loss_rnnt 1.705934 hw_loss 0.000104 lr 0.00046097 rank 6
2023-02-21 11:31:34,430 DEBUG TRAIN Batch 13/11500 loss 7.917950 loss_att 7.555252 loss_ctc 10.364265 loss_rnnt 7.441307 hw_loss 0.418139 lr 0.00046087 rank 4
2023-02-21 11:31:34,430 DEBUG TRAIN Batch 13/11500 loss 15.793223 loss_att 14.053599 loss_ctc 19.207729 loss_rnnt 15.467184 hw_loss 0.410058 lr 0.00046087 rank 0
2023-02-21 11:31:34,431 DEBUG TRAIN Batch 13/11500 loss 1.269929 loss_att 5.511753 loss_ctc 2.007791 loss_rnnt 0.210934 hw_loss 0.210466 lr 0.00046087 rank 2
2023-02-21 11:31:34,432 DEBUG TRAIN Batch 13/11500 loss 10.639779 loss_att 16.879856 loss_ctc 16.869522 loss_rnnt 8.417933 hw_loss 0.268496 lr 0.00046087 rank 5
2023-02-21 11:31:34,433 DEBUG TRAIN Batch 13/11500 loss 3.565429 loss_att 10.069906 loss_ctc 2.290968 loss_rnnt 2.342045 hw_loss 0.173281 lr 0.00046087 rank 7
2023-02-21 11:31:34,434 DEBUG TRAIN Batch 13/11500 loss 14.995785 loss_att 18.282845 loss_ctc 14.419698 loss_rnnt 14.338173 hw_loss 0.144396 lr 0.00046087 rank 6
2023-02-21 11:31:34,441 DEBUG TRAIN Batch 13/11500 loss 5.968300 loss_att 8.533991 loss_ctc 9.528101 loss_rnnt 4.820064 hw_loss 0.300858 lr 0.00046087 rank 1
2023-02-21 11:31:34,442 DEBUG TRAIN Batch 13/11500 loss 3.795696 loss_att 9.385979 loss_ctc 4.994900 loss_rnnt 2.364728 hw_loss 0.286905 lr 0.00046087 rank 3
2023-02-21 11:32:32,023 DEBUG TRAIN Batch 13/11600 loss 11.458742 loss_att 15.207954 loss_ctc 13.023727 loss_rnnt 10.309540 hw_loss 0.357551 lr 0.00046077 rank 4
2023-02-21 11:32:32,024 DEBUG TRAIN Batch 13/11600 loss 11.023564 loss_att 13.005472 loss_ctc 18.097410 loss_rnnt 9.682919 hw_loss 0.002034 lr 0.00046077 rank 3
2023-02-21 11:32:32,024 DEBUG TRAIN Batch 13/11600 loss 8.402635 loss_att 11.059464 loss_ctc 10.829199 loss_rnnt 7.416134 hw_loss 0.246736 lr 0.00046077 rank 2
2023-02-21 11:32:32,028 DEBUG TRAIN Batch 13/11600 loss 7.839787 loss_att 7.978803 loss_ctc 6.132902 loss_rnnt 7.899796 hw_loss 0.262071 lr 0.00046077 rank 5
2023-02-21 11:32:32,033 DEBUG TRAIN Batch 13/11600 loss 23.223682 loss_att 26.120819 loss_ctc 30.383175 loss_rnnt 21.502773 hw_loss 0.350406 lr 0.00046077 rank 0
2023-02-21 11:32:32,035 DEBUG TRAIN Batch 13/11600 loss 18.762470 loss_att 19.665022 loss_ctc 23.530025 loss_rnnt 17.873884 hw_loss 0.135755 lr 0.00046077 rank 6
2023-02-21 11:32:32,038 DEBUG TRAIN Batch 13/11600 loss 11.300132 loss_att 18.616201 loss_ctc 14.504762 loss_rnnt 9.203207 hw_loss 0.387051 lr 0.00046077 rank 7
2023-02-21 11:32:32,042 DEBUG TRAIN Batch 13/11600 loss 17.287821 loss_att 20.571663 loss_ctc 23.162455 loss_rnnt 15.742054 hw_loss 0.198216 lr 0.00046077 rank 1
2023-02-21 11:33:29,440 DEBUG TRAIN Batch 13/11700 loss 1.289785 loss_att 2.800125 loss_ctc 2.002222 loss_rnnt 0.718828 hw_loss 0.326058 lr 0.00046068 rank 5
2023-02-21 11:33:29,442 DEBUG TRAIN Batch 13/11700 loss 13.335533 loss_att 12.351652 loss_ctc 12.474945 loss_rnnt 13.400478 hw_loss 0.462330 lr 0.00046068 rank 2
2023-02-21 11:33:29,444 DEBUG TRAIN Batch 13/11700 loss 9.473304 loss_att 14.783762 loss_ctc 11.352118 loss_rnnt 8.160611 hw_loss 0.000172 lr 0.00046068 rank 4
2023-02-21 11:33:29,444 DEBUG TRAIN Batch 13/11700 loss 45.534481 loss_att 60.554703 loss_ctc 63.807709 loss_rnnt 39.917477 hw_loss 0.330996 lr 0.00046068 rank 3
2023-02-21 11:33:29,450 DEBUG TRAIN Batch 13/11700 loss 18.708916 loss_att 19.588305 loss_ctc 21.393190 loss_rnnt 17.971792 hw_loss 0.381268 lr 0.00046068 rank 6
2023-02-21 11:33:29,449 DEBUG TRAIN Batch 13/11700 loss 2.976477 loss_att 6.390936 loss_ctc 3.362709 loss_rnnt 2.051313 hw_loss 0.357703 lr 0.00046068 rank 0
2023-02-21 11:33:29,451 DEBUG TRAIN Batch 13/11700 loss 21.058163 loss_att 21.310991 loss_ctc 26.900131 loss_rnnt 20.228575 hw_loss 0.000172 lr 0.00046068 rank 1
2023-02-21 11:33:29,453 DEBUG TRAIN Batch 13/11700 loss 5.592451 loss_att 7.197052 loss_ctc 6.511649 loss_rnnt 5.148878 hw_loss 0.000172 lr 0.00046068 rank 7
2023-02-21 11:34:25,483 DEBUG TRAIN Batch 13/11800 loss 1.902714 loss_att 7.738579 loss_ctc 1.810143 loss_rnnt 0.543957 hw_loss 0.382364 lr 0.00046058 rank 0
2023-02-21 11:34:25,483 DEBUG TRAIN Batch 13/11800 loss 11.096232 loss_att 14.105049 loss_ctc 16.056606 loss_rnnt 9.740532 hw_loss 0.173538 lr 0.00046058 rank 2
2023-02-21 11:34:25,488 DEBUG TRAIN Batch 13/11800 loss 6.211935 loss_att 7.723140 loss_ctc 11.831182 loss_rnnt 5.160343 hw_loss 0.000220 lr 0.00046058 rank 5
2023-02-21 11:34:25,489 DEBUG TRAIN Batch 13/11800 loss 9.976329 loss_att 12.410480 loss_ctc 16.284832 loss_rnnt 8.562747 hw_loss 0.160535 lr 0.00046058 rank 3
2023-02-21 11:34:25,491 DEBUG TRAIN Batch 13/11800 loss 17.415932 loss_att 18.971167 loss_ctc 21.073574 loss_rnnt 16.477983 hw_loss 0.261031 lr 0.00046058 rank 7
2023-02-21 11:34:25,492 DEBUG TRAIN Batch 13/11800 loss 19.048073 loss_att 19.890728 loss_ctc 25.070744 loss_rnnt 18.045307 hw_loss 0.058527 lr 0.00046058 rank 4
2023-02-21 11:34:25,497 DEBUG TRAIN Batch 13/11800 loss 18.788044 loss_att 17.073517 loss_ctc 29.484409 loss_rnnt 17.531151 hw_loss 0.325534 lr 0.00046058 rank 1
2023-02-21 11:34:25,553 DEBUG TRAIN Batch 13/11800 loss 11.637421 loss_att 11.290067 loss_ctc 12.089602 loss_rnnt 11.434597 hw_loss 0.397505 lr 0.00046058 rank 6
2023-02-21 11:35:22,530 DEBUG TRAIN Batch 13/11900 loss 15.012587 loss_att 17.734322 loss_ctc 21.024015 loss_rnnt 13.509018 hw_loss 0.295683 lr 0.00046048 rank 0
2023-02-21 11:35:22,540 DEBUG TRAIN Batch 13/11900 loss 14.852604 loss_att 21.720821 loss_ctc 20.946779 loss_rnnt 12.448174 hw_loss 0.409181 lr 0.00046048 rank 2
2023-02-21 11:35:22,543 DEBUG TRAIN Batch 13/11900 loss 12.527009 loss_att 18.240892 loss_ctc 18.069489 loss_rnnt 10.481456 hw_loss 0.307085 lr 0.00046048 rank 7
2023-02-21 11:35:22,544 DEBUG TRAIN Batch 13/11900 loss 7.299275 loss_att 8.359358 loss_ctc 16.579565 loss_rnnt 5.595849 hw_loss 0.476319 lr 0.00046048 rank 3
2023-02-21 11:35:22,544 DEBUG TRAIN Batch 13/11900 loss 3.620521 loss_att 7.343862 loss_ctc 6.700338 loss_rnnt 2.248429 hw_loss 0.406466 lr 0.00046048 rank 5
2023-02-21 11:35:22,548 DEBUG TRAIN Batch 13/11900 loss 9.090045 loss_att 14.929110 loss_ctc 15.284000 loss_rnnt 7.009323 hw_loss 0.163214 lr 0.00046048 rank 1
2023-02-21 11:35:22,550 DEBUG TRAIN Batch 13/11900 loss 43.340183 loss_att 38.118362 loss_ctc 47.295746 loss_rnnt 43.728928 hw_loss 0.240395 lr 0.00046048 rank 6
2023-02-21 11:35:22,598 DEBUG TRAIN Batch 13/11900 loss 17.976223 loss_att 21.750715 loss_ctc 31.569492 loss_rnnt 15.223373 hw_loss 0.347845 lr 0.00046048 rank 4
2023-02-21 11:36:20,898 DEBUG TRAIN Batch 13/12000 loss 4.964256 loss_att 8.848170 loss_ctc 4.558951 loss_rnnt 4.241116 hw_loss 0.000746 lr 0.00046038 rank 2
2023-02-21 11:36:20,906 DEBUG TRAIN Batch 13/12000 loss 3.149540 loss_att 4.796068 loss_ctc 4.890136 loss_rnnt 2.485861 hw_loss 0.191801 lr 0.00046038 rank 5
2023-02-21 11:36:20,906 DEBUG TRAIN Batch 13/12000 loss 22.518150 loss_att 22.005201 loss_ctc 34.942860 loss_rnnt 20.963717 hw_loss 0.000746 lr 0.00046038 rank 0
2023-02-21 11:36:20,909 DEBUG TRAIN Batch 13/12000 loss 16.247730 loss_att 18.540527 loss_ctc 17.456625 loss_rnnt 15.443777 hw_loss 0.345394 lr 0.00046038 rank 3
2023-02-21 11:36:20,908 DEBUG TRAIN Batch 13/12000 loss 21.854156 loss_att 29.357344 loss_ctc 28.042456 loss_rnnt 19.410625 hw_loss 0.220852 lr 0.00046038 rank 4
2023-02-21 11:36:20,911 DEBUG TRAIN Batch 13/12000 loss 19.225912 loss_att 20.684633 loss_ctc 30.883183 loss_rnnt 17.379469 hw_loss 0.000746 lr 0.00046038 rank 6
2023-02-21 11:36:20,912 DEBUG TRAIN Batch 13/12000 loss 12.931053 loss_att 12.504520 loss_ctc 22.213810 loss_rnnt 11.510969 hw_loss 0.501919 lr 0.00046038 rank 7
2023-02-21 11:36:20,918 DEBUG TRAIN Batch 13/12000 loss 6.395787 loss_att 10.053831 loss_ctc 9.266524 loss_rnnt 5.088177 hw_loss 0.362318 lr 0.00046038 rank 1
2023-02-21 11:38:13,962 DEBUG TRAIN Batch 13/12100 loss 15.736690 loss_att 17.475071 loss_ctc 20.418097 loss_rnnt 14.639790 hw_loss 0.234441 lr 0.00046029 rank 2
2023-02-21 11:38:13,967 DEBUG TRAIN Batch 13/12100 loss 21.536596 loss_att 22.135342 loss_ctc 27.629358 loss_rnnt 20.482939 hw_loss 0.227892 lr 0.00046029 rank 4
2023-02-21 11:38:13,968 DEBUG TRAIN Batch 13/12100 loss 15.237769 loss_att 13.657375 loss_ctc 16.872551 loss_rnnt 15.184217 hw_loss 0.284365 lr 0.00046029 rank 0
2023-02-21 11:38:13,972 DEBUG TRAIN Batch 13/12100 loss 12.441294 loss_att 12.932735 loss_ctc 15.987946 loss_rnnt 11.697558 hw_loss 0.323548 lr 0.00046029 rank 6
2023-02-21 11:38:13,973 DEBUG TRAIN Batch 13/12100 loss 9.465186 loss_att 19.914080 loss_ctc 15.366906 loss_rnnt 6.503833 hw_loss 0.158769 lr 0.00046029 rank 5
2023-02-21 11:38:13,975 DEBUG TRAIN Batch 13/12100 loss 18.377220 loss_att 18.004665 loss_ctc 17.399261 loss_rnnt 18.411011 hw_loss 0.320841 lr 0.00046029 rank 7
2023-02-21 11:38:13,986 DEBUG TRAIN Batch 13/12100 loss 13.178295 loss_att 19.775637 loss_ctc 21.405422 loss_rnnt 10.681698 hw_loss 0.150335 lr 0.00046029 rank 3
2023-02-21 11:38:13,990 DEBUG TRAIN Batch 13/12100 loss 3.002878 loss_att 5.086634 loss_ctc 7.637813 loss_rnnt 1.852221 hw_loss 0.217340 lr 0.00046029 rank 1
2023-02-21 11:39:12,144 DEBUG TRAIN Batch 13/12200 loss 7.660799 loss_att 12.714589 loss_ctc 7.866071 loss_rnnt 6.542772 hw_loss 0.149811 lr 0.00046019 rank 0
2023-02-21 11:39:12,149 DEBUG TRAIN Batch 13/12200 loss 14.382493 loss_att 14.757898 loss_ctc 17.479439 loss_rnnt 13.806170 hw_loss 0.165592 lr 0.00046019 rank 6
2023-02-21 11:39:12,150 DEBUG TRAIN Batch 13/12200 loss 15.352190 loss_att 15.963392 loss_ctc 17.919514 loss_rnnt 14.728065 hw_loss 0.299207 lr 0.00046019 rank 5
2023-02-21 11:39:12,151 DEBUG TRAIN Batch 13/12200 loss 4.060894 loss_att 9.100763 loss_ctc 7.047108 loss_rnnt 2.467506 hw_loss 0.351097 lr 0.00046019 rank 4
2023-02-21 11:39:12,151 DEBUG TRAIN Batch 13/12200 loss 7.282672 loss_att 12.567770 loss_ctc 10.651771 loss_rnnt 5.745840 hw_loss 0.057373 lr 0.00046019 rank 2
2023-02-21 11:39:12,155 DEBUG TRAIN Batch 13/12200 loss 6.821759 loss_att 12.029873 loss_ctc 12.671684 loss_rnnt 4.893771 hw_loss 0.199452 lr 0.00046019 rank 3
2023-02-21 11:39:12,160 DEBUG TRAIN Batch 13/12200 loss 15.730322 loss_att 19.934057 loss_ctc 16.962360 loss_rnnt 14.655880 hw_loss 0.130170 lr 0.00046019 rank 7
2023-02-21 11:39:12,162 DEBUG TRAIN Batch 13/12200 loss 8.591923 loss_att 15.163519 loss_ctc 14.787172 loss_rnnt 6.247900 hw_loss 0.381879 lr 0.00046019 rank 1
2023-02-21 11:40:08,484 DEBUG TRAIN Batch 13/12300 loss 24.032000 loss_att 35.236351 loss_ctc 32.770298 loss_rnnt 20.514481 hw_loss 0.209146 lr 0.00046009 rank 2
2023-02-21 11:40:08,485 DEBUG TRAIN Batch 13/12300 loss 14.782228 loss_att 18.317417 loss_ctc 10.295188 loss_rnnt 14.427745 hw_loss 0.460717 lr 0.00046009 rank 7
2023-02-21 11:40:08,486 DEBUG TRAIN Batch 13/12300 loss 18.712652 loss_att 27.184649 loss_ctc 23.613302 loss_rnnt 16.247017 hw_loss 0.220902 lr 0.00046009 rank 5
2023-02-21 11:40:08,487 DEBUG TRAIN Batch 13/12300 loss 4.995754 loss_att 7.123738 loss_ctc 6.290896 loss_rnnt 4.397451 hw_loss 0.000037 lr 0.00046009 rank 0
2023-02-21 11:40:08,488 DEBUG TRAIN Batch 13/12300 loss 14.998643 loss_att 19.909977 loss_ctc 15.532488 loss_rnnt 13.760571 hw_loss 0.346174 lr 0.00046009 rank 3
2023-02-21 11:40:08,494 DEBUG TRAIN Batch 13/12300 loss 5.233315 loss_att 16.418549 loss_ctc 10.862532 loss_rnnt 2.116606 hw_loss 0.242062 lr 0.00046009 rank 4
2023-02-21 11:40:08,499 DEBUG TRAIN Batch 13/12300 loss 3.661540 loss_att 5.482322 loss_ctc 6.267086 loss_rnnt 2.773398 hw_loss 0.331087 lr 0.00046009 rank 6
2023-02-21 11:40:08,503 DEBUG TRAIN Batch 13/12300 loss 14.475790 loss_att 19.535301 loss_ctc 21.244576 loss_rnnt 12.383993 hw_loss 0.332607 lr 0.00046009 rank 1
2023-02-21 11:41:04,897 DEBUG TRAIN Batch 13/12400 loss 23.642727 loss_att 19.764536 loss_ctc 25.660522 loss_rnnt 23.906116 hw_loss 0.456019 lr 0.00045999 rank 0
2023-02-21 11:41:04,898 DEBUG TRAIN Batch 13/12400 loss 5.538802 loss_att 7.585310 loss_ctc 9.896994 loss_rnnt 4.548074 hw_loss 0.000627 lr 0.00045999 rank 5
2023-02-21 11:41:04,900 DEBUG TRAIN Batch 13/12400 loss 6.532605 loss_att 7.404725 loss_ctc 9.149984 loss_rnnt 5.957484 hw_loss 0.096962 lr 0.00045999 rank 7
2023-02-21 11:41:04,900 DEBUG TRAIN Batch 13/12400 loss 27.990673 loss_att 28.846151 loss_ctc 36.356529 loss_rnnt 26.538219 hw_loss 0.311084 lr 0.00045999 rank 2
2023-02-21 11:41:04,902 DEBUG TRAIN Batch 13/12400 loss 2.791257 loss_att 5.145211 loss_ctc 2.212970 loss_rnnt 2.230949 hw_loss 0.312416 lr 0.00045999 rank 4
2023-02-21 11:41:04,903 DEBUG TRAIN Batch 13/12400 loss 12.611021 loss_att 14.010328 loss_ctc 14.010281 loss_rnnt 12.083802 hw_loss 0.113980 lr 0.00045999 rank 3
2023-02-21 11:41:04,905 DEBUG TRAIN Batch 13/12400 loss 13.001565 loss_att 13.215265 loss_ctc 16.538980 loss_rnnt 12.413236 hw_loss 0.138624 lr 0.00045999 rank 6
2023-02-21 11:41:04,909 DEBUG TRAIN Batch 13/12400 loss 13.375572 loss_att 19.316322 loss_ctc 15.733234 loss_rnnt 11.826052 hw_loss 0.088154 lr 0.00045999 rank 1
2023-02-21 11:42:04,656 DEBUG TRAIN Batch 13/12500 loss 18.717478 loss_att 25.840981 loss_ctc 23.365524 loss_rnnt 16.485752 hw_loss 0.351159 lr 0.00045990 rank 4
2023-02-21 11:42:04,662 DEBUG TRAIN Batch 13/12500 loss 12.777022 loss_att 14.535309 loss_ctc 14.349168 loss_rnnt 12.168409 hw_loss 0.088756 lr 0.00045990 rank 0
2023-02-21 11:42:04,662 DEBUG TRAIN Batch 13/12500 loss 1.319348 loss_att 6.581143 loss_ctc 1.000323 loss_rnnt 0.309421 hw_loss 0.000195 lr 0.00045990 rank 3
2023-02-21 11:42:04,663 DEBUG TRAIN Batch 13/12500 loss 17.882290 loss_att 22.284344 loss_ctc 25.902901 loss_rnnt 15.753944 hw_loss 0.334726 lr 0.00045990 rank 5
2023-02-21 11:42:04,668 DEBUG TRAIN Batch 13/12500 loss 12.866473 loss_att 18.503052 loss_ctc 15.638925 loss_rnnt 11.276848 hw_loss 0.173719 lr 0.00045990 rank 6
2023-02-21 11:42:04,669 DEBUG TRAIN Batch 13/12500 loss 6.805564 loss_att 9.333490 loss_ctc 7.533084 loss_rnnt 6.041704 hw_loss 0.302386 lr 0.00045990 rank 7
2023-02-21 11:42:04,677 DEBUG TRAIN Batch 13/12500 loss 3.841974 loss_att 5.856393 loss_ctc 1.907151 loss_rnnt 3.472705 hw_loss 0.420679 lr 0.00045990 rank 1
2023-02-21 11:42:04,715 DEBUG TRAIN Batch 13/12500 loss 14.526053 loss_att 17.908808 loss_ctc 18.974909 loss_rnnt 13.083263 hw_loss 0.324486 lr 0.00045990 rank 2
2023-02-21 11:43:01,625 DEBUG TRAIN Batch 13/12600 loss 15.061305 loss_att 17.725681 loss_ctc 19.736536 loss_rnnt 13.754807 hw_loss 0.281734 lr 0.00045980 rank 5
2023-02-21 11:43:01,628 DEBUG TRAIN Batch 13/12600 loss 7.629959 loss_att 8.999247 loss_ctc 7.459315 loss_rnnt 7.325998 hw_loss 0.099103 lr 0.00045980 rank 0
2023-02-21 11:43:01,634 DEBUG TRAIN Batch 13/12600 loss 5.795224 loss_att 8.905244 loss_ctc 5.318233 loss_rnnt 5.035921 hw_loss 0.376685 lr 0.00045980 rank 4
2023-02-21 11:43:01,634 DEBUG TRAIN Batch 13/12600 loss 7.803204 loss_att 10.744318 loss_ctc 8.973191 loss_rnnt 6.906128 hw_loss 0.286602 lr 0.00045980 rank 1
2023-02-21 11:43:01,637 DEBUG TRAIN Batch 13/12600 loss 4.008486 loss_att 9.682215 loss_ctc 10.432839 loss_rnnt 1.771055 hw_loss 0.461447 lr 0.00045980 rank 3
2023-02-21 11:43:01,637 DEBUG TRAIN Batch 13/12600 loss 6.460690 loss_att 17.567125 loss_ctc 6.103480 loss_rnnt 4.205920 hw_loss 0.152084 lr 0.00045980 rank 7
2023-02-21 11:43:01,642 DEBUG TRAIN Batch 13/12600 loss 4.266560 loss_att 7.145113 loss_ctc 4.265896 loss_rnnt 3.521910 hw_loss 0.316926 lr 0.00045980 rank 6
2023-02-21 11:43:01,690 DEBUG TRAIN Batch 13/12600 loss 18.222054 loss_att 18.919321 loss_ctc 18.045395 loss_rnnt 17.911282 hw_loss 0.365385 lr 0.00045980 rank 2
2023-02-21 11:43:58,059 DEBUG TRAIN Batch 13/12700 loss 13.582806 loss_att 15.538133 loss_ctc 16.501570 loss_rnnt 12.647355 hw_loss 0.291031 lr 0.00045970 rank 2
2023-02-21 11:43:58,063 DEBUG TRAIN Batch 13/12700 loss 9.659072 loss_att 11.460996 loss_ctc 14.571802 loss_rnnt 8.472372 hw_loss 0.321158 lr 0.00045970 rank 5
2023-02-21 11:43:58,064 DEBUG TRAIN Batch 13/12700 loss 15.999276 loss_att 19.557196 loss_ctc 21.771919 loss_rnnt 14.359927 hw_loss 0.296399 lr 0.00045970 rank 4
2023-02-21 11:43:58,065 DEBUG TRAIN Batch 13/12700 loss 7.462769 loss_att 14.348505 loss_ctc 14.268004 loss_rnnt 4.971874 hw_loss 0.386968 lr 0.00045970 rank 3
2023-02-21 11:43:58,068 DEBUG TRAIN Batch 13/12700 loss 6.338081 loss_att 9.171725 loss_ctc 7.390658 loss_rnnt 5.416807 hw_loss 0.401627 lr 0.00045970 rank 0
2023-02-21 11:43:58,068 DEBUG TRAIN Batch 13/12700 loss 3.175336 loss_att 5.526254 loss_ctc 5.769458 loss_rnnt 2.161088 hw_loss 0.371591 lr 0.00045970 rank 6
2023-02-21 11:43:58,069 DEBUG TRAIN Batch 13/12700 loss 5.698559 loss_att 10.543488 loss_ctc 9.952572 loss_rnnt 4.060124 hw_loss 0.191712 lr 0.00045970 rank 7
2023-02-21 11:43:58,074 DEBUG TRAIN Batch 13/12700 loss 12.026899 loss_att 16.934406 loss_ctc 16.886282 loss_rnnt 10.162353 hw_loss 0.440864 lr 0.00045970 rank 1
2023-02-21 11:44:56,974 DEBUG TRAIN Batch 13/12800 loss 21.940868 loss_att 22.208050 loss_ctc 27.751560 loss_rnnt 21.060020 hw_loss 0.098725 lr 0.00045960 rank 2
2023-02-21 11:44:56,978 DEBUG TRAIN Batch 13/12800 loss 10.331676 loss_att 12.404966 loss_ctc 20.525415 loss_rnnt 8.433237 hw_loss 0.233654 lr 0.00045960 rank 5
2023-02-21 11:44:56,979 DEBUG TRAIN Batch 13/12800 loss 4.783456 loss_att 7.204143 loss_ctc 11.638445 loss_rnnt 3.201249 hw_loss 0.345133 lr 0.00045960 rank 0
2023-02-21 11:44:56,984 DEBUG TRAIN Batch 13/12800 loss 9.860710 loss_att 12.729092 loss_ctc 11.630735 loss_rnnt 8.945121 hw_loss 0.198582 lr 0.00045960 rank 3
2023-02-21 11:44:56,985 DEBUG TRAIN Batch 13/12800 loss 22.903650 loss_att 30.439356 loss_ctc 26.293121 loss_rnnt 20.895733 hw_loss 0.091587 lr 0.00045960 rank 6
2023-02-21 11:44:56,986 DEBUG TRAIN Batch 13/12800 loss 16.031597 loss_att 25.200935 loss_ctc 26.526161 loss_rnnt 12.704139 hw_loss 0.176842 lr 0.00045960 rank 7
2023-02-21 11:44:56,992 DEBUG TRAIN Batch 13/12800 loss 20.407736 loss_att 17.240953 loss_ctc 35.235573 loss_rnnt 18.892811 hw_loss 0.321070 lr 0.00045960 rank 1
2023-02-21 11:44:56,992 DEBUG TRAIN Batch 13/12800 loss 9.055934 loss_att 11.768855 loss_ctc 17.112856 loss_rnnt 7.272838 hw_loss 0.311730 lr 0.00045960 rank 4
2023-02-21 11:45:56,731 DEBUG TRAIN Batch 13/12900 loss 12.526965 loss_att 17.923977 loss_ctc 14.309199 loss_rnnt 11.047327 hw_loss 0.304883 lr 0.00045951 rank 2
2023-02-21 11:45:56,739 DEBUG TRAIN Batch 13/12900 loss 9.459346 loss_att 24.468887 loss_ctc 13.378027 loss_rnnt 5.724318 hw_loss 0.394928 lr 0.00045951 rank 3
2023-02-21 11:45:56,742 DEBUG TRAIN Batch 13/12900 loss 11.464206 loss_att 9.765867 loss_ctc 22.698517 loss_rnnt 10.160361 hw_loss 0.273008 lr 0.00045951 rank 4
2023-02-21 11:45:56,743 DEBUG TRAIN Batch 13/12900 loss 17.117483 loss_att 14.261814 loss_ctc 18.148937 loss_rnnt 17.281797 hw_loss 0.504923 lr 0.00045951 rank 6
2023-02-21 11:45:56,748 DEBUG TRAIN Batch 13/12900 loss 43.057579 loss_att 48.903076 loss_ctc 58.915627 loss_rnnt 39.774048 hw_loss 0.000048 lr 0.00045951 rank 5
2023-02-21 11:45:56,749 DEBUG TRAIN Batch 13/12900 loss 41.383377 loss_att 39.436844 loss_ctc 52.415039 loss_rnnt 40.185329 hw_loss 0.218376 lr 0.00045951 rank 7
2023-02-21 11:45:56,749 DEBUG TRAIN Batch 13/12900 loss 28.760683 loss_att 28.704113 loss_ctc 31.998850 loss_rnnt 28.253284 hw_loss 0.163044 lr 0.00045951 rank 0
2023-02-21 11:45:56,759 DEBUG TRAIN Batch 13/12900 loss 7.638683 loss_att 20.271515 loss_ctc 9.802077 loss_rnnt 4.823639 hw_loss 0.000048 lr 0.00045951 rank 1
2023-02-21 11:47:49,417 DEBUG TRAIN Batch 13/13000 loss 10.648858 loss_att 15.036371 loss_ctc 5.409152 loss_rnnt 10.304665 hw_loss 0.309971 lr 0.00045941 rank 3
2023-02-21 11:47:49,419 DEBUG TRAIN Batch 13/13000 loss 12.833941 loss_att 13.670112 loss_ctc 14.702810 loss_rnnt 12.381753 hw_loss 0.067070 lr 0.00045941 rank 0
2023-02-21 11:47:49,421 DEBUG TRAIN Batch 13/13000 loss 14.038126 loss_att 13.376551 loss_ctc 17.183371 loss_rnnt 13.592459 hw_loss 0.297409 lr 0.00045941 rank 5
2023-02-21 11:47:49,421 DEBUG TRAIN Batch 13/13000 loss 19.399464 loss_att 20.937042 loss_ctc 19.861050 loss_rnnt 18.851532 hw_loss 0.335383 lr 0.00045941 rank 7
2023-02-21 11:47:49,423 DEBUG TRAIN Batch 13/13000 loss 15.529857 loss_att 22.802156 loss_ctc 22.437351 loss_rnnt 12.982117 hw_loss 0.323026 lr 0.00045941 rank 4
2023-02-21 11:47:49,424 DEBUG TRAIN Batch 13/13000 loss 26.517191 loss_att 31.406151 loss_ctc 29.687258 loss_rnnt 25.091412 hw_loss 0.047465 lr 0.00045941 rank 6
2023-02-21 11:47:49,427 DEBUG TRAIN Batch 13/13000 loss 8.821879 loss_att 11.127171 loss_ctc 13.138020 loss_rnnt 7.711450 hw_loss 0.138536 lr 0.00045941 rank 1
2023-02-21 11:47:49,475 DEBUG TRAIN Batch 13/13000 loss 26.879982 loss_att 27.320187 loss_ctc 34.073635 loss_rnnt 25.748957 hw_loss 0.157178 lr 0.00045941 rank 2
2023-02-21 11:48:48,186 DEBUG TRAIN Batch 13/13100 loss 10.717062 loss_att 11.547518 loss_ctc 12.196057 loss_rnnt 10.116201 hw_loss 0.445442 lr 0.00045931 rank 2
2023-02-21 11:48:48,188 DEBUG TRAIN Batch 13/13100 loss 5.215107 loss_att 7.648145 loss_ctc 5.375920 loss_rnnt 4.637080 hw_loss 0.131209 lr 0.00045931 rank 5
2023-02-21 11:48:48,193 DEBUG TRAIN Batch 13/13100 loss 14.194533 loss_att 15.391572 loss_ctc 15.002652 loss_rnnt 13.846924 hw_loss 0.000848 lr 0.00045931 rank 0
2023-02-21 11:48:48,200 DEBUG TRAIN Batch 13/13100 loss 14.473208 loss_att 17.696356 loss_ctc 24.722565 loss_rnnt 12.225422 hw_loss 0.443581 lr 0.00045931 rank 3
2023-02-21 11:48:48,201 DEBUG TRAIN Batch 13/13100 loss 7.307648 loss_att 12.033556 loss_ctc 7.950642 loss_rnnt 5.968872 hw_loss 0.577241 lr 0.00045931 rank 6
2023-02-21 11:48:48,202 DEBUG TRAIN Batch 13/13100 loss 25.210705 loss_att 27.872677 loss_ctc 34.923000 loss_rnnt 23.382887 hw_loss 0.000848 lr 0.00045931 rank 7
2023-02-21 11:48:48,203 DEBUG TRAIN Batch 13/13100 loss 8.672776 loss_att 14.288689 loss_ctc 8.853851 loss_rnnt 7.166932 hw_loss 0.672222 lr 0.00045931 rank 4
2023-02-21 11:48:48,209 DEBUG TRAIN Batch 13/13100 loss 13.741899 loss_att 15.435934 loss_ctc 18.485970 loss_rnnt 12.650791 hw_loss 0.224546 lr 0.00045931 rank 1
2023-02-21 11:49:44,152 DEBUG TRAIN Batch 13/13200 loss 6.716342 loss_att 18.916828 loss_ctc 6.850756 loss_rnnt 4.212512 hw_loss 0.085895 lr 0.00045922 rank 2
2023-02-21 11:49:44,153 DEBUG TRAIN Batch 13/13200 loss 21.190233 loss_att 15.159689 loss_ctc 20.930754 loss_rnnt 22.158962 hw_loss 0.509961 lr 0.00045922 rank 4
2023-02-21 11:49:44,155 DEBUG TRAIN Batch 13/13200 loss 13.698676 loss_att 15.909346 loss_ctc 20.223280 loss_rnnt 12.147620 hw_loss 0.448076 lr 0.00045922 rank 3
2023-02-21 11:49:44,155 DEBUG TRAIN Batch 13/13200 loss 4.909855 loss_att 10.478196 loss_ctc 5.096835 loss_rnnt 3.770979 hw_loss 0.000521 lr 0.00045922 rank 0
2023-02-21 11:49:44,157 DEBUG TRAIN Batch 13/13200 loss 30.892357 loss_att 39.506004 loss_ctc 33.738632 loss_rnnt 28.789845 hw_loss 0.000521 lr 0.00045922 rank 5
2023-02-21 11:49:44,160 DEBUG TRAIN Batch 13/13200 loss 25.202206 loss_att 32.581150 loss_ctc 30.819221 loss_rnnt 22.725029 hw_loss 0.473348 lr 0.00045922 rank 7
2023-02-21 11:49:44,167 DEBUG TRAIN Batch 13/13200 loss 9.176658 loss_att 8.184879 loss_ctc 10.570460 loss_rnnt 8.894638 hw_loss 0.552254 lr 0.00045922 rank 1
2023-02-21 11:49:44,169 DEBUG TRAIN Batch 13/13200 loss 7.073333 loss_att 8.627584 loss_ctc 12.000616 loss_rnnt 5.905488 hw_loss 0.375043 lr 0.00045922 rank 6
2023-02-21 11:50:40,357 DEBUG TRAIN Batch 13/13300 loss 28.329865 loss_att 29.243795 loss_ctc 34.729527 loss_rnnt 27.114365 hw_loss 0.336421 lr 0.00045912 rank 0
2023-02-21 11:50:40,364 DEBUG TRAIN Batch 13/13300 loss 7.915385 loss_att 10.511126 loss_ctc 12.338985 loss_rnnt 6.670230 hw_loss 0.255363 lr 0.00045912 rank 7
2023-02-21 11:50:40,364 DEBUG TRAIN Batch 13/13300 loss 15.056685 loss_att 15.207491 loss_ctc 21.692144 loss_rnnt 13.953959 hw_loss 0.352195 lr 0.00045912 rank 5
2023-02-21 11:50:40,366 DEBUG TRAIN Batch 13/13300 loss 8.398640 loss_att 10.746405 loss_ctc 7.879017 loss_rnnt 7.998343 hw_loss 0.000051 lr 0.00045912 rank 4
2023-02-21 11:50:40,366 DEBUG TRAIN Batch 13/13300 loss 13.525461 loss_att 17.760939 loss_ctc 17.792776 loss_rnnt 11.900603 hw_loss 0.391475 lr 0.00045912 rank 3
2023-02-21 11:50:40,367 DEBUG TRAIN Batch 13/13300 loss 21.926468 loss_att 22.078959 loss_ctc 27.196552 loss_rnnt 20.963694 hw_loss 0.430493 lr 0.00045912 rank 2
2023-02-21 11:50:40,367 DEBUG TRAIN Batch 13/13300 loss 12.847753 loss_att 15.589039 loss_ctc 14.293826 loss_rnnt 11.957090 hw_loss 0.280493 lr 0.00045912 rank 1
2023-02-21 11:50:40,372 DEBUG TRAIN Batch 13/13300 loss 19.443605 loss_att 21.511532 loss_ctc 21.979874 loss_rnnt 18.567827 hw_loss 0.232545 lr 0.00045912 rank 6
2023-02-21 11:51:38,814 DEBUG TRAIN Batch 13/13400 loss 29.434841 loss_att 34.198486 loss_ctc 29.257404 loss_rnnt 28.362225 hw_loss 0.269147 lr 0.00045902 rank 0
2023-02-21 11:51:38,815 DEBUG TRAIN Batch 13/13400 loss 13.486252 loss_att 15.078368 loss_ctc 17.384005 loss_rnnt 12.563606 hw_loss 0.158480 lr 0.00045902 rank 2
2023-02-21 11:51:38,818 DEBUG TRAIN Batch 13/13400 loss 8.691893 loss_att 11.722198 loss_ctc 13.794177 loss_rnnt 7.167873 hw_loss 0.445599 lr 0.00045902 rank 3
2023-02-21 11:51:38,819 DEBUG TRAIN Batch 13/13400 loss 6.566715 loss_att 12.675329 loss_ctc 12.087971 loss_rnnt 4.608787 hw_loss 0.000070 lr 0.00045902 rank 7
2023-02-21 11:51:38,821 DEBUG TRAIN Batch 13/13400 loss 23.013821 loss_att 23.182764 loss_ctc 35.618294 loss_rnnt 21.299397 hw_loss 0.000070 lr 0.00045902 rank 4
2023-02-21 11:51:38,826 DEBUG TRAIN Batch 13/13400 loss 18.066380 loss_att 26.065804 loss_ctc 31.723845 loss_rnnt 14.558191 hw_loss 0.163701 lr 0.00045902 rank 5
2023-02-21 11:51:38,835 DEBUG TRAIN Batch 13/13400 loss 8.393893 loss_att 9.686397 loss_ctc 10.410255 loss_rnnt 7.866506 hw_loss 0.000070 lr 0.00045902 rank 6
2023-02-21 11:51:38,873 DEBUG TRAIN Batch 13/13400 loss 34.004311 loss_att 35.605263 loss_ctc 29.109804 loss_rnnt 34.148357 hw_loss 0.353184 lr 0.00045902 rank 1
2023-02-21 11:52:34,718 DEBUG TRAIN Batch 13/13500 loss 57.870724 loss_att 54.087475 loss_ctc 64.692924 loss_rnnt 57.717606 hw_loss 0.000262 lr 0.00045893 rank 2
2023-02-21 11:52:34,719 DEBUG TRAIN Batch 13/13500 loss 5.604846 loss_att 8.891933 loss_ctc 10.158425 loss_rnnt 4.153337 hw_loss 0.350528 lr 0.00045893 rank 4
2023-02-21 11:52:34,722 DEBUG TRAIN Batch 13/13500 loss 22.552299 loss_att 24.748707 loss_ctc 34.584785 loss_rnnt 20.233501 hw_loss 0.515971 lr 0.00045893 rank 5
2023-02-21 11:52:34,723 DEBUG TRAIN Batch 13/13500 loss 24.593077 loss_att 25.714146 loss_ctc 30.195797 loss_rnnt 23.621693 hw_loss 0.000262 lr 0.00045893 rank 6
2023-02-21 11:52:34,725 DEBUG TRAIN Batch 13/13500 loss 21.429491 loss_att 27.336205 loss_ctc 36.907688 loss_rnnt 17.969923 hw_loss 0.402121 lr 0.00045893 rank 7
2023-02-21 11:52:34,726 DEBUG TRAIN Batch 13/13500 loss 16.074131 loss_att 16.962133 loss_ctc 17.788780 loss_rnnt 15.474288 hw_loss 0.363043 lr 0.00045893 rank 3
2023-02-21 11:52:34,728 DEBUG TRAIN Batch 13/13500 loss 7.314787 loss_att 9.823658 loss_ctc 11.972547 loss_rnnt 6.191838 hw_loss 0.000262 lr 0.00045893 rank 0
2023-02-21 11:52:34,798 DEBUG TRAIN Batch 13/13500 loss 6.838320 loss_att 7.683743 loss_ctc 9.317694 loss_rnnt 6.209268 hw_loss 0.242594 lr 0.00045893 rank 1
2023-02-21 11:53:31,010 DEBUG TRAIN Batch 13/13600 loss 19.680191 loss_att 22.240129 loss_ctc 21.863094 loss_rnnt 18.772793 hw_loss 0.195672 lr 0.00045883 rank 7
2023-02-21 11:53:31,012 DEBUG TRAIN Batch 13/13600 loss 10.186274 loss_att 12.147285 loss_ctc 9.467819 loss_rnnt 9.671056 hw_loss 0.410267 lr 0.00045883 rank 2
2023-02-21 11:53:31,013 DEBUG TRAIN Batch 13/13600 loss 12.956685 loss_att 14.642302 loss_ctc 19.309752 loss_rnnt 11.772415 hw_loss 0.000133 lr 0.00045883 rank 4
2023-02-21 11:53:31,016 DEBUG TRAIN Batch 13/13600 loss 5.224647 loss_att 8.268388 loss_ctc 7.073225 loss_rnnt 4.369350 hw_loss 0.000133 lr 0.00045883 rank 6
2023-02-21 11:53:31,019 DEBUG TRAIN Batch 13/13600 loss 22.506048 loss_att 23.831085 loss_ctc 24.049847 loss_rnnt 21.951925 hw_loss 0.156140 lr 0.00045883 rank 0
2023-02-21 11:53:31,020 DEBUG TRAIN Batch 13/13600 loss 14.758362 loss_att 13.964312 loss_ctc 15.145683 loss_rnnt 14.689085 hw_loss 0.330834 lr 0.00045883 rank 3
2023-02-21 11:53:31,027 DEBUG TRAIN Batch 13/13600 loss 15.966554 loss_att 22.775009 loss_ctc 19.805269 loss_rnnt 13.956343 hw_loss 0.256297 lr 0.00045883 rank 1
2023-02-21 11:53:31,032 DEBUG TRAIN Batch 13/13600 loss 6.907040 loss_att 9.839050 loss_ctc 9.701376 loss_rnnt 5.670117 hw_loss 0.521140 lr 0.00045883 rank 5
2023-02-21 11:54:30,507 DEBUG TRAIN Batch 13/13700 loss 7.419781 loss_att 12.600628 loss_ctc 8.014861 loss_rnnt 6.055179 hw_loss 0.467042 lr 0.00045873 rank 0
2023-02-21 11:54:30,510 DEBUG TRAIN Batch 13/13700 loss 11.852249 loss_att 16.234726 loss_ctc 18.111639 loss_rnnt 10.087064 hw_loss 0.101444 lr 0.00045873 rank 2
2023-02-21 11:54:30,511 DEBUG TRAIN Batch 13/13700 loss 15.453278 loss_att 24.172245 loss_ctc 25.865135 loss_rnnt 12.235996 hw_loss 0.159825 lr 0.00045873 rank 6
2023-02-21 11:54:30,511 DEBUG TRAIN Batch 13/13700 loss 13.123199 loss_att 19.527180 loss_ctc 27.145292 loss_rnnt 9.971731 hw_loss 0.001986 lr 0.00045873 rank 4
2023-02-21 11:54:30,515 DEBUG TRAIN Batch 13/13700 loss 16.824795 loss_att 17.901136 loss_ctc 22.809170 loss_rnnt 15.727176 hw_loss 0.158316 lr 0.00045873 rank 5
2023-02-21 11:54:30,517 DEBUG TRAIN Batch 13/13700 loss 22.035461 loss_att 26.062164 loss_ctc 27.036037 loss_rnnt 20.419554 hw_loss 0.269671 lr 0.00045873 rank 3
2023-02-21 11:54:30,529 DEBUG TRAIN Batch 13/13700 loss 19.733507 loss_att 28.631491 loss_ctc 25.113520 loss_rnnt 17.189373 hw_loss 0.088507 lr 0.00045873 rank 7
2023-02-21 11:54:30,539 DEBUG TRAIN Batch 13/13700 loss 5.494964 loss_att 6.162771 loss_ctc 5.075480 loss_rnnt 5.416275 hw_loss 0.001986 lr 0.00045873 rank 1
2023-02-21 11:56:22,916 DEBUG TRAIN Batch 13/13800 loss 9.024020 loss_att 13.830351 loss_ctc 13.219455 loss_rnnt 7.346709 hw_loss 0.293725 lr 0.00045864 rank 2
2023-02-21 11:56:22,923 DEBUG TRAIN Batch 13/13800 loss 4.341498 loss_att 6.529106 loss_ctc 6.300414 loss_rnnt 3.475499 hw_loss 0.313666 lr 0.00045864 rank 3
2023-02-21 11:56:22,923 DEBUG TRAIN Batch 13/13800 loss 21.047995 loss_att 22.695978 loss_ctc 25.802961 loss_rnnt 20.036827 hw_loss 0.089207 lr 0.00045864 rank 0
2023-02-21 11:56:22,927 DEBUG TRAIN Batch 13/13800 loss 3.209221 loss_att 4.934567 loss_ctc 3.177753 loss_rnnt 2.867962 hw_loss 0.000724 lr 0.00045864 rank 5
2023-02-21 11:56:22,928 DEBUG TRAIN Batch 13/13800 loss 9.931904 loss_att 14.835945 loss_ctc 10.311335 loss_rnnt 8.797163 hw_loss 0.193766 lr 0.00045864 rank 4
2023-02-21 11:56:22,929 DEBUG TRAIN Batch 13/13800 loss 14.593772 loss_att 14.526405 loss_ctc 17.480339 loss_rnnt 14.012895 hw_loss 0.392766 lr 0.00045864 rank 6
2023-02-21 11:56:22,957 DEBUG TRAIN Batch 13/13800 loss 4.218508 loss_att 9.893536 loss_ctc 6.721478 loss_rnnt 2.605044 hw_loss 0.271368 lr 0.00045864 rank 7
2023-02-21 11:56:22,999 DEBUG TRAIN Batch 13/13800 loss 16.589607 loss_att 17.779940 loss_ctc 18.501879 loss_rnnt 15.871365 hw_loss 0.422265 lr 0.00045864 rank 1
2023-02-21 11:57:23,066 DEBUG TRAIN Batch 13/13900 loss 14.900855 loss_att 18.324776 loss_ctc 16.938892 loss_rnnt 13.892729 hw_loss 0.096758 lr 0.00045854 rank 0
2023-02-21 11:57:23,068 DEBUG TRAIN Batch 13/13900 loss 18.523136 loss_att 22.979307 loss_ctc 22.855589 loss_rnnt 16.952877 hw_loss 0.190058 lr 0.00045854 rank 6
2023-02-21 11:57:23,068 DEBUG TRAIN Batch 13/13900 loss 15.755868 loss_att 15.980142 loss_ctc 17.392284 loss_rnnt 15.492439 hw_loss 0.000722 lr 0.00045854 rank 5
2023-02-21 11:57:23,070 DEBUG TRAIN Batch 13/13900 loss 58.391903 loss_att 59.515526 loss_ctc 74.424461 loss_rnnt 55.850410 hw_loss 0.335788 lr 0.00045854 rank 4
2023-02-21 11:57:23,070 DEBUG TRAIN Batch 13/13900 loss 19.322075 loss_att 22.948887 loss_ctc 22.665953 loss_rnnt 17.943428 hw_loss 0.388936 lr 0.00045854 rank 2
2023-02-21 11:57:23,072 DEBUG TRAIN Batch 13/13900 loss 15.509246 loss_att 19.590881 loss_ctc 21.340971 loss_rnnt 13.727295 hw_loss 0.352615 lr 0.00045854 rank 7
2023-02-21 11:57:23,072 DEBUG TRAIN Batch 13/13900 loss 20.460440 loss_att 25.279934 loss_ctc 30.833298 loss_rnnt 18.049704 hw_loss 0.119606 lr 0.00045854 rank 3
2023-02-21 11:57:23,076 DEBUG TRAIN Batch 13/13900 loss 4.175739 loss_att 9.501865 loss_ctc 7.996593 loss_rnnt 2.600681 hw_loss 0.000722 lr 0.00045854 rank 1
2023-02-21 11:58:21,063 DEBUG TRAIN Batch 13/14000 loss 1.865366 loss_att 5.810034 loss_ctc 4.592695 loss_rnnt 0.409786 hw_loss 0.568131 lr 0.00045844 rank 2
2023-02-21 11:58:21,064 DEBUG TRAIN Batch 13/14000 loss 21.782364 loss_att 25.598677 loss_ctc 26.753374 loss_rnnt 20.356129 hw_loss 0.000318 lr 0.00045844 rank 4
2023-02-21 11:58:21,069 DEBUG TRAIN Batch 13/14000 loss 25.210993 loss_att 27.320436 loss_ctc 42.169903 loss_rnnt 22.261236 hw_loss 0.500020 lr 0.00045844 rank 0
2023-02-21 11:58:21,069 DEBUG TRAIN Batch 13/14000 loss 5.098973 loss_att 10.174185 loss_ctc 10.603783 loss_rnnt 3.286425 hw_loss 0.119121 lr 0.00045844 rank 7
2023-02-21 11:58:21,074 DEBUG TRAIN Batch 13/14000 loss 10.383279 loss_att 12.140448 loss_ctc 11.178038 loss_rnnt 9.841407 hw_loss 0.158384 lr 0.00045844 rank 5
2023-02-21 11:58:21,078 DEBUG TRAIN Batch 13/14000 loss 16.539877 loss_att 29.376816 loss_ctc 25.185783 loss_rnnt 12.696731 hw_loss 0.230570 lr 0.00045844 rank 6
2023-02-21 11:58:21,087 DEBUG TRAIN Batch 13/14000 loss 30.181770 loss_att 30.237028 loss_ctc 40.879261 loss_rnnt 28.659531 hw_loss 0.159099 lr 0.00045844 rank 1
2023-02-21 11:58:21,130 DEBUG TRAIN Batch 13/14000 loss 58.332603 loss_att 60.659088 loss_ctc 61.354977 loss_rnnt 57.331131 hw_loss 0.249730 lr 0.00045844 rank 3
2023-02-21 11:59:17,024 DEBUG TRAIN Batch 13/14100 loss 12.139372 loss_att 12.605149 loss_ctc 13.419699 loss_rnnt 11.590227 hw_loss 0.534897 lr 0.00045835 rank 2
2023-02-21 11:59:17,027 DEBUG TRAIN Batch 13/14100 loss 13.149745 loss_att 17.391941 loss_ctc 19.266285 loss_rnnt 11.485748 hw_loss 0.000036 lr 0.00045835 rank 4
2023-02-21 11:59:17,029 DEBUG TRAIN Batch 13/14100 loss 13.490829 loss_att 16.653622 loss_ctc 16.978148 loss_rnnt 12.278860 hw_loss 0.214565 lr 0.00045835 rank 5
2023-02-21 11:59:17,030 DEBUG TRAIN Batch 13/14100 loss 6.320777 loss_att 10.547858 loss_ctc 9.896588 loss_rnnt 4.831628 hw_loss 0.313047 lr 0.00045835 rank 0
2023-02-21 11:59:17,033 DEBUG TRAIN Batch 13/14100 loss 12.256817 loss_att 15.919998 loss_ctc 14.277092 loss_rnnt 11.112434 hw_loss 0.266955 lr 0.00045835 rank 6
2023-02-21 11:59:17,041 DEBUG TRAIN Batch 13/14100 loss 5.458464 loss_att 7.037605 loss_ctc 5.286941 loss_rnnt 4.924343 hw_loss 0.452179 lr 0.00045835 rank 3
2023-02-21 11:59:17,048 DEBUG TRAIN Batch 13/14100 loss 7.434119 loss_att 7.910494 loss_ctc 7.939235 loss_rnnt 7.087609 hw_loss 0.344787 lr 0.00045835 rank 7
2023-02-21 11:59:17,050 DEBUG TRAIN Batch 13/14100 loss 15.633340 loss_att 16.194813 loss_ctc 18.050045 loss_rnnt 14.938290 hw_loss 0.488491 lr 0.00045835 rank 1
2023-02-21 12:00:14,858 DEBUG TRAIN Batch 13/14200 loss 9.038196 loss_att 11.678762 loss_ctc 8.898447 loss_rnnt 8.380516 hw_loss 0.277872 lr 0.00045825 rank 4
2023-02-21 12:00:14,858 DEBUG TRAIN Batch 13/14200 loss 10.256363 loss_att 13.911613 loss_ctc 14.158620 loss_rnnt 8.763290 hw_loss 0.453227 lr 0.00045825 rank 0
2023-02-21 12:00:14,865 DEBUG TRAIN Batch 13/14200 loss 10.728588 loss_att 14.787310 loss_ctc 16.332308 loss_rnnt 9.166920 hw_loss 0.005176 lr 0.00045825 rank 3
2023-02-21 12:00:14,865 DEBUG TRAIN Batch 13/14200 loss 5.953730 loss_att 7.870252 loss_ctc 6.414589 loss_rnnt 5.273202 hw_loss 0.442079 lr 0.00045825 rank 5
2023-02-21 12:00:14,867 DEBUG TRAIN Batch 13/14200 loss 10.947755 loss_att 16.186790 loss_ctc 20.749704 loss_rnnt 8.523237 hw_loss 0.130843 lr 0.00045825 rank 1
2023-02-21 12:00:14,867 DEBUG TRAIN Batch 13/14200 loss 9.673226 loss_att 12.676888 loss_ctc 11.729742 loss_rnnt 8.750381 hw_loss 0.089834 lr 0.00045825 rank 6
2023-02-21 12:00:14,875 DEBUG TRAIN Batch 13/14200 loss 12.603461 loss_att 16.082525 loss_ctc 13.725374 loss_rnnt 11.664751 hw_loss 0.174953 lr 0.00045825 rank 7
2023-02-21 12:00:14,920 DEBUG TRAIN Batch 13/14200 loss 7.198360 loss_att 10.700260 loss_ctc 14.117637 loss_rnnt 5.419320 hw_loss 0.292667 lr 0.00045825 rank 2
2023-02-21 12:01:13,581 DEBUG TRAIN Batch 13/14300 loss 33.592815 loss_att 37.331535 loss_ctc 47.341644 loss_rnnt 30.912306 hw_loss 0.186729 lr 0.00045815 rank 2
2023-02-21 12:01:13,588 DEBUG TRAIN Batch 13/14300 loss 8.812024 loss_att 17.225918 loss_ctc 12.993836 loss_rnnt 6.400192 hw_loss 0.321521 lr 0.00045815 rank 5
2023-02-21 12:01:13,590 DEBUG TRAIN Batch 13/14300 loss 33.487923 loss_att 33.733498 loss_ctc 38.902340 loss_rnnt 32.625420 hw_loss 0.171503 lr 0.00045815 rank 0
2023-02-21 12:01:13,594 DEBUG TRAIN Batch 13/14300 loss 17.664276 loss_att 15.403815 loss_ctc 22.759834 loss_rnnt 17.330057 hw_loss 0.200444 lr 0.00045815 rank 7
2023-02-21 12:01:13,595 DEBUG TRAIN Batch 13/14300 loss 35.351505 loss_att 31.959099 loss_ctc 35.480171 loss_rnnt 36.012699 hw_loss 0.000239 lr 0.00045815 rank 3
2023-02-21 12:01:13,600 DEBUG TRAIN Batch 13/14300 loss 29.921461 loss_att 25.733368 loss_ctc 32.695705 loss_rnnt 30.228653 hw_loss 0.300983 lr 0.00045815 rank 1
2023-02-21 12:01:13,601 DEBUG TRAIN Batch 13/14300 loss 11.140129 loss_att 11.829255 loss_ctc 14.779413 loss_rnnt 10.281334 hw_loss 0.441997 lr 0.00045815 rank 4
2023-02-21 12:01:13,602 DEBUG TRAIN Batch 13/14300 loss 21.392511 loss_att 22.220692 loss_ctc 21.587444 loss_rnnt 20.957676 hw_loss 0.456020 lr 0.00045815 rank 6
2023-02-21 12:02:10,054 DEBUG TRAIN Batch 13/14400 loss 10.448762 loss_att 11.631781 loss_ctc 10.462900 loss_rnnt 10.046374 hw_loss 0.307309 lr 0.00045806 rank 3
2023-02-21 12:02:10,055 DEBUG TRAIN Batch 13/14400 loss 5.926515 loss_att 7.603589 loss_ctc 8.909521 loss_rnnt 5.018121 hw_loss 0.328583 lr 0.00045806 rank 2
2023-02-21 12:02:10,058 DEBUG TRAIN Batch 13/14400 loss 11.418313 loss_att 15.654368 loss_ctc 18.062811 loss_rnnt 9.567768 hw_loss 0.220127 lr 0.00045806 rank 0
2023-02-21 12:02:10,058 DEBUG TRAIN Batch 13/14400 loss 2.843869 loss_att 5.960782 loss_ctc 4.118008 loss_rnnt 1.785420 hw_loss 0.497214 lr 0.00045806 rank 5
2023-02-21 12:02:10,059 DEBUG TRAIN Batch 13/14400 loss 19.990374 loss_att 19.828533 loss_ctc 21.870792 loss_rnnt 19.660484 hw_loss 0.209131 lr 0.00045806 rank 6
2023-02-21 12:02:10,060 DEBUG TRAIN Batch 13/14400 loss 9.880694 loss_att 11.708069 loss_ctc 14.564987 loss_rnnt 8.859999 hw_loss 0.057466 lr 0.00045806 rank 7
2023-02-21 12:02:10,063 DEBUG TRAIN Batch 13/14400 loss 22.652376 loss_att 28.176926 loss_ctc 30.140203 loss_rnnt 20.376366 hw_loss 0.323852 lr 0.00045806 rank 4
2023-02-21 12:02:10,076 DEBUG TRAIN Batch 13/14400 loss 24.095522 loss_att 25.452259 loss_ctc 24.497120 loss_rnnt 23.522013 hw_loss 0.466153 lr 0.00045806 rank 1
2023-02-21 12:03:07,752 DEBUG TRAIN Batch 13/14500 loss 13.946262 loss_att 19.419331 loss_ctc 19.810959 loss_rnnt 11.870584 hw_loss 0.373317 lr 0.00045796 rank 2
2023-02-21 12:03:07,753 DEBUG TRAIN Batch 13/14500 loss 13.526855 loss_att 20.942190 loss_ctc 21.259773 loss_rnnt 10.868108 hw_loss 0.271169 lr 0.00045796 rank 0
2023-02-21 12:03:07,754 DEBUG TRAIN Batch 13/14500 loss 2.710828 loss_att 7.563926 loss_ctc 5.422630 loss_rnnt 1.230655 hw_loss 0.277462 lr 0.00045796 rank 6
2023-02-21 12:03:07,754 DEBUG TRAIN Batch 13/14500 loss 9.105193 loss_att 13.870843 loss_ctc 15.365669 loss_rnnt 7.317091 hw_loss 0.000454 lr 0.00045796 rank 5
2023-02-21 12:03:07,755 DEBUG TRAIN Batch 13/14500 loss 12.469351 loss_att 18.174957 loss_ctc 17.487429 loss_rnnt 10.576275 hw_loss 0.155393 lr 0.00045796 rank 4
2023-02-21 12:03:07,756 DEBUG TRAIN Batch 13/14500 loss 5.210162 loss_att 7.783319 loss_ctc 6.128687 loss_rnnt 4.402771 hw_loss 0.319292 lr 0.00045796 rank 7
2023-02-21 12:03:07,758 DEBUG TRAIN Batch 13/14500 loss 29.129965 loss_att 28.028530 loss_ctc 29.009317 loss_rnnt 29.289282 hw_loss 0.144477 lr 0.00045796 rank 3
2023-02-21 12:03:07,777 DEBUG TRAIN Batch 13/14500 loss 21.901581 loss_att 21.768185 loss_ctc 28.715981 loss_rnnt 21.019432 hw_loss 0.000454 lr 0.00045796 rank 1
2023-02-21 12:04:06,045 DEBUG TRAIN Batch 13/14600 loss 1.014818 loss_att 3.969930 loss_ctc 0.423656 loss_rnnt 0.242071 hw_loss 0.488524 lr 0.00045787 rank 2
2023-02-21 12:04:06,051 DEBUG TRAIN Batch 13/14600 loss 16.207306 loss_att 21.132942 loss_ctc 29.973326 loss_rnnt 13.193935 hw_loss 0.361452 lr 0.00045787 rank 5
2023-02-21 12:04:06,055 DEBUG TRAIN Batch 13/14600 loss 16.019157 loss_att 18.778618 loss_ctc 17.323494 loss_rnnt 15.110460 hw_loss 0.342922 lr 0.00045787 rank 4
2023-02-21 12:04:06,054 DEBUG TRAIN Batch 13/14600 loss 3.931004 loss_att 10.884361 loss_ctc 8.157146 loss_rnnt 1.792482 hw_loss 0.345683 lr 0.00045787 rank 3
2023-02-21 12:04:06,057 DEBUG TRAIN Batch 13/14600 loss 4.888031 loss_att 6.036480 loss_ctc 8.996166 loss_rnnt 4.110445 hw_loss 0.000269 lr 0.00045787 rank 7
2023-02-21 12:04:06,059 DEBUG TRAIN Batch 13/14600 loss 9.556288 loss_att 10.952370 loss_ctc 9.864349 loss_rnnt 9.235853 hw_loss 0.000269 lr 0.00045787 rank 0
2023-02-21 12:04:06,076 DEBUG TRAIN Batch 13/14600 loss 11.186313 loss_att 17.967524 loss_ctc 12.412807 loss_rnnt 9.474865 hw_loss 0.359388 lr 0.00045787 rank 6
2023-02-21 12:04:06,083 DEBUG TRAIN Batch 13/14600 loss 25.812159 loss_att 25.598536 loss_ctc 28.531681 loss_rnnt 25.272306 hw_loss 0.412449 lr 0.00045787 rank 1
2023-02-21 12:05:57,270 DEBUG TRAIN Batch 13/14700 loss 18.195948 loss_att 21.502731 loss_ctc 19.664824 loss_rnnt 17.338692 hw_loss 0.000088 lr 0.00045777 rank 2
2023-02-21 12:05:57,270 DEBUG TRAIN Batch 13/14700 loss 4.748382 loss_att 7.755988 loss_ctc 9.622822 loss_rnnt 3.496888 hw_loss 0.000088 lr 0.00045777 rank 5
2023-02-21 12:05:57,271 DEBUG TRAIN Batch 13/14700 loss 15.814483 loss_att 18.612499 loss_ctc 20.110920 loss_rnnt 14.611103 hw_loss 0.132970 lr 0.00045777 rank 4
2023-02-21 12:05:57,274 DEBUG TRAIN Batch 13/14700 loss 5.713982 loss_att 6.119678 loss_ctc 6.118489 loss_rnnt 5.394294 hw_loss 0.346153 lr 0.00045777 rank 6
2023-02-21 12:05:57,275 DEBUG TRAIN Batch 13/14700 loss 11.524195 loss_att 18.566263 loss_ctc 16.700975 loss_rnnt 9.372778 hw_loss 0.098935 lr 0.00045777 rank 3
2023-02-21 12:05:57,278 DEBUG TRAIN Batch 13/14700 loss 6.929542 loss_att 8.880309 loss_ctc 8.342916 loss_rnnt 6.161168 hw_loss 0.355819 lr 0.00045777 rank 1
2023-02-21 12:05:57,281 DEBUG TRAIN Batch 13/14700 loss 21.242918 loss_att 20.124670 loss_ctc 29.232149 loss_rnnt 20.340313 hw_loss 0.114419 lr 0.00045777 rank 7
2023-02-21 12:05:57,344 DEBUG TRAIN Batch 13/14700 loss 18.014051 loss_att 22.609608 loss_ctc 22.699677 loss_rnnt 16.307823 hw_loss 0.304435 lr 0.00045777 rank 0
2023-02-21 12:07:44,544 DEBUG TRAIN Batch 13/14800 loss 18.284346 loss_att 24.006205 loss_ctc 22.436970 loss_rnnt 16.397764 hw_loss 0.353488 lr 0.00045767 rank 2
2023-02-21 12:07:44,549 DEBUG TRAIN Batch 13/14800 loss 12.485084 loss_att 15.278406 loss_ctc 15.508562 loss_rnnt 11.412328 hw_loss 0.208051 lr 0.00045767 rank 7
2023-02-21 12:07:44,549 DEBUG TRAIN Batch 13/14800 loss 14.865639 loss_att 16.244202 loss_ctc 16.887831 loss_rnnt 14.126596 hw_loss 0.363195 lr 0.00045767 rank 5
2023-02-21 12:07:44,553 DEBUG TRAIN Batch 13/14800 loss 8.986917 loss_att 14.387959 loss_ctc 12.867660 loss_rnnt 7.314306 hw_loss 0.140569 lr 0.00045767 rank 4
2023-02-21 12:07:44,554 DEBUG TRAIN Batch 13/14800 loss 14.441854 loss_att 16.914154 loss_ctc 18.047539 loss_rnnt 13.342533 hw_loss 0.232690 lr 0.00045767 rank 0
2023-02-21 12:07:44,555 DEBUG TRAIN Batch 13/14800 loss 37.477310 loss_att 37.448265 loss_ctc 45.156693 loss_rnnt 36.342537 hw_loss 0.218748 lr 0.00045767 rank 6
2023-02-21 12:07:44,566 DEBUG TRAIN Batch 13/14800 loss 19.364424 loss_att 17.444696 loss_ctc 16.093834 loss_rnnt 20.183924 hw_loss 0.000981 lr 0.00045767 rank 1
2023-02-21 12:07:44,609 DEBUG TRAIN Batch 13/14800 loss 12.015530 loss_att 12.030104 loss_ctc 11.931610 loss_rnnt 11.836672 hw_loss 0.350872 lr 0.00045767 rank 3
2023-02-21 12:08:42,360 DEBUG TRAIN Batch 13/14900 loss 6.899434 loss_att 12.785337 loss_ctc 7.989015 loss_rnnt 5.360909 hw_loss 0.405126 lr 0.00045758 rank 7
2023-02-21 12:08:42,361 DEBUG TRAIN Batch 13/14900 loss 6.111704 loss_att 9.743783 loss_ctc 5.229794 loss_rnnt 5.416029 hw_loss 0.162839 lr 0.00045758 rank 5
2023-02-21 12:08:42,361 DEBUG TRAIN Batch 13/14900 loss 19.500648 loss_att 18.930161 loss_ctc 23.031891 loss_rnnt 18.944538 hw_loss 0.373829 lr 0.00045758 rank 3
2023-02-21 12:08:42,362 DEBUG TRAIN Batch 13/14900 loss 28.363476 loss_att 33.378693 loss_ctc 37.027702 loss_rnnt 26.091345 hw_loss 0.213487 lr 0.00045758 rank 2
2023-02-21 12:08:42,362 DEBUG TRAIN Batch 13/14900 loss 11.283319 loss_att 16.336603 loss_ctc 15.985914 loss_rnnt 9.359456 hw_loss 0.536613 lr 0.00045758 rank 0
2023-02-21 12:08:42,366 DEBUG TRAIN Batch 13/14900 loss 16.801025 loss_att 18.290066 loss_ctc 26.528456 loss_rnnt 15.045033 hw_loss 0.302238 lr 0.00045758 rank 4
2023-02-21 12:08:42,371 DEBUG TRAIN Batch 13/14900 loss 44.114605 loss_att 49.845699 loss_ctc 45.194286 loss_rnnt 42.688271 hw_loss 0.255289 lr 0.00045758 rank 1
2023-02-21 12:08:42,426 DEBUG TRAIN Batch 13/14900 loss 5.882891 loss_att 9.245896 loss_ctc 5.893344 loss_rnnt 5.043300 hw_loss 0.310492 lr 0.00045758 rank 6
2023-02-21 12:09:39,185 DEBUG TRAIN Batch 13/15000 loss 2.441103 loss_att 4.949384 loss_ctc 2.828525 loss_rnnt 1.752139 hw_loss 0.254347 lr 0.00045748 rank 2
2023-02-21 12:09:39,195 DEBUG TRAIN Batch 13/15000 loss 3.710410 loss_att 5.574716 loss_ctc 3.621504 loss_rnnt 3.117731 hw_loss 0.434386 lr 0.00045748 rank 4
2023-02-21 12:09:39,197 DEBUG TRAIN Batch 13/15000 loss 6.700940 loss_att 10.975081 loss_ctc 7.935015 loss_rnnt 5.554883 hw_loss 0.237536 lr 0.00045748 rank 3
2023-02-21 12:09:39,201 DEBUG TRAIN Batch 13/15000 loss 37.828438 loss_att 40.936478 loss_ctc 55.213188 loss_rnnt 34.640511 hw_loss 0.465661 lr 0.00045748 rank 5
2023-02-21 12:09:39,202 DEBUG TRAIN Batch 13/15000 loss 9.081717 loss_att 11.402578 loss_ctc 11.213707 loss_rnnt 8.092775 hw_loss 0.450944 lr 0.00045748 rank 6
2023-02-21 12:09:39,203 DEBUG TRAIN Batch 13/15000 loss 6.313377 loss_att 11.183180 loss_ctc 8.768861 loss_rnnt 4.842405 hw_loss 0.318027 lr 0.00045748 rank 0
2023-02-21 12:09:39,203 DEBUG TRAIN Batch 13/15000 loss 21.178820 loss_att 28.570581 loss_ctc 36.255508 loss_rnnt 17.606213 hw_loss 0.157557 lr 0.00045748 rank 1
2023-02-21 12:09:39,206 DEBUG TRAIN Batch 13/15000 loss 12.069898 loss_att 12.174100 loss_ctc 11.061357 loss_rnnt 12.017308 hw_loss 0.311666 lr 0.00045748 rank 7
2023-02-21 12:10:38,387 DEBUG TRAIN Batch 13/15100 loss 13.206491 loss_att 18.052729 loss_ctc 16.646786 loss_rnnt 11.750591 hw_loss 0.052398 lr 0.00045739 rank 5
2023-02-21 12:10:38,393 DEBUG TRAIN Batch 13/15100 loss 8.014540 loss_att 12.595299 loss_ctc 12.335657 loss_rnnt 6.414817 hw_loss 0.201415 lr 0.00045739 rank 0
2023-02-21 12:10:38,394 DEBUG TRAIN Batch 13/15100 loss 5.964553 loss_att 6.898937 loss_ctc 6.354350 loss_rnnt 5.519372 hw_loss 0.386872 lr 0.00045739 rank 6
2023-02-21 12:10:38,396 DEBUG TRAIN Batch 13/15100 loss 14.300286 loss_att 15.942778 loss_ctc 15.352594 loss_rnnt 13.747023 hw_loss 0.158360 lr 0.00045739 rank 4
2023-02-21 12:10:38,397 DEBUG TRAIN Batch 13/15100 loss 13.635880 loss_att 20.925474 loss_ctc 15.085155 loss_rnnt 11.862868 hw_loss 0.228479 lr 0.00045739 rank 2
2023-02-21 12:10:38,402 DEBUG TRAIN Batch 13/15100 loss 1.134069 loss_att 3.561676 loss_ctc 2.507677 loss_rnnt 0.408537 hw_loss 0.106617 lr 0.00045739 rank 3
2023-02-21 12:10:38,402 DEBUG TRAIN Batch 13/15100 loss 18.999897 loss_att 26.210270 loss_ctc 22.782673 loss_rnnt 16.848471 hw_loss 0.384342 lr 0.00045739 rank 1
2023-02-21 12:10:38,403 DEBUG TRAIN Batch 13/15100 loss 3.379439 loss_att 8.104089 loss_ctc 4.057779 loss_rnnt 2.218015 hw_loss 0.236342 lr 0.00045739 rank 7
2023-02-21 12:11:35,627 DEBUG TRAIN Batch 13/15200 loss 12.535215 loss_att 15.753954 loss_ctc 14.209116 loss_rnnt 11.667500 hw_loss 0.001461 lr 0.00045729 rank 2
2023-02-21 12:11:35,629 DEBUG TRAIN Batch 13/15200 loss 16.385639 loss_att 18.372135 loss_ctc 24.060299 loss_rnnt 14.734928 hw_loss 0.431481 lr 0.00045729 rank 6
2023-02-21 12:11:35,630 DEBUG TRAIN Batch 13/15200 loss 13.182968 loss_att 23.237217 loss_ctc 28.322235 loss_rnnt 8.999329 hw_loss 0.289164 lr 0.00045729 rank 4
2023-02-21 12:11:35,631 DEBUG TRAIN Batch 13/15200 loss 9.308594 loss_att 13.197335 loss_ctc 17.497730 loss_rnnt 7.103789 hw_loss 0.628447 lr 0.00045729 rank 0
2023-02-21 12:11:35,632 DEBUG TRAIN Batch 13/15200 loss 25.640963 loss_att 27.982988 loss_ctc 30.777882 loss_rnnt 24.230627 hw_loss 0.481891 lr 0.00045729 rank 3
2023-02-21 12:11:35,639 DEBUG TRAIN Batch 13/15200 loss 12.119460 loss_att 12.382538 loss_ctc 14.234590 loss_rnnt 11.590143 hw_loss 0.365030 lr 0.00045729 rank 7
2023-02-21 12:11:35,652 DEBUG TRAIN Batch 13/15200 loss 4.412557 loss_att 8.641439 loss_ctc 5.203256 loss_rnnt 3.189661 hw_loss 0.509423 lr 0.00045729 rank 5
2023-02-21 12:11:35,670 DEBUG TRAIN Batch 13/15200 loss 1.161965 loss_att 3.593344 loss_ctc 1.079777 loss_rnnt 0.551572 hw_loss 0.253268 lr 0.00045729 rank 1
2023-02-21 12:12:32,000 DEBUG TRAIN Batch 13/15300 loss 13.365437 loss_att 15.340332 loss_ctc 15.868084 loss_rnnt 12.380594 hw_loss 0.480331 lr 0.00045720 rank 2
2023-02-21 12:12:32,005 DEBUG TRAIN Batch 13/15300 loss 18.384914 loss_att 29.059597 loss_ctc 22.746456 loss_rnnt 15.591590 hw_loss 0.144091 lr 0.00045720 rank 1
2023-02-21 12:12:32,008 DEBUG TRAIN Batch 13/15300 loss 20.412737 loss_att 21.772875 loss_ctc 24.436069 loss_rnnt 19.497686 hw_loss 0.199835 lr 0.00045720 rank 3
2023-02-21 12:12:32,008 DEBUG TRAIN Batch 13/15300 loss 11.834336 loss_att 17.617516 loss_ctc 15.068989 loss_rnnt 10.152851 hw_loss 0.175428 lr 0.00045720 rank 7
2023-02-21 12:12:32,013 DEBUG TRAIN Batch 13/15300 loss 44.058250 loss_att 50.792057 loss_ctc 53.841705 loss_rnnt 41.334007 hw_loss 0.136922 lr 0.00045720 rank 6
2023-02-21 12:12:32,040 DEBUG TRAIN Batch 13/15300 loss 5.593553 loss_att 13.567855 loss_ctc 5.335971 loss_rnnt 3.894020 hw_loss 0.260655 lr 0.00045720 rank 5
2023-02-21 12:12:32,043 DEBUG TRAIN Batch 13/15300 loss 2.845508 loss_att 5.657737 loss_ctc 7.196761 loss_rnnt 1.665494 hw_loss 0.070126 lr 0.00045720 rank 0
2023-02-21 12:12:32,063 DEBUG TRAIN Batch 13/15300 loss 13.796762 loss_att 16.281349 loss_ctc 17.415209 loss_rnnt 12.592503 hw_loss 0.421655 lr 0.00045720 rank 4
2023-02-21 12:13:31,260 DEBUG TRAIN Batch 13/15400 loss 5.730866 loss_att 12.459297 loss_ctc 5.748464 loss_rnnt 4.137994 hw_loss 0.459073 lr 0.00045710 rank 3
2023-02-21 12:13:31,260 DEBUG TRAIN Batch 13/15400 loss 21.376755 loss_att 17.785650 loss_ctc 16.526989 loss_rnnt 22.544941 hw_loss 0.368758 lr 0.00045710 rank 2
2023-02-21 12:13:31,266 DEBUG TRAIN Batch 13/15400 loss 8.082678 loss_att 11.095299 loss_ctc 8.413497 loss_rnnt 7.241876 hw_loss 0.364066 lr 0.00045710 rank 4
2023-02-21 12:13:31,267 DEBUG TRAIN Batch 13/15400 loss 6.335936 loss_att 6.685214 loss_ctc 8.818511 loss_rnnt 5.834640 hw_loss 0.188308 lr 0.00045710 rank 0
2023-02-21 12:13:31,268 DEBUG TRAIN Batch 13/15400 loss 11.422118 loss_att 20.291147 loss_ctc 15.402159 loss_rnnt 8.969503 hw_loss 0.277755 lr 0.00045710 rank 1
2023-02-21 12:13:31,268 DEBUG TRAIN Batch 13/15400 loss 18.447399 loss_att 21.061161 loss_ctc 22.546146 loss_rnnt 17.214386 hw_loss 0.307049 lr 0.00045710 rank 5
2023-02-21 12:13:31,273 DEBUG TRAIN Batch 13/15400 loss 15.421326 loss_att 18.346189 loss_ctc 14.031906 loss_rnnt 14.744140 hw_loss 0.520256 lr 0.00045710 rank 7
2023-02-21 12:13:31,277 DEBUG TRAIN Batch 13/15400 loss 14.139696 loss_att 18.258833 loss_ctc 15.547417 loss_rnnt 13.043305 hw_loss 0.159125 lr 0.00045710 rank 6
2023-02-21 12:15:20,697 DEBUG TRAIN Batch 13/15500 loss 18.279587 loss_att 21.143551 loss_ctc 17.139805 loss_rnnt 17.628525 hw_loss 0.431699 lr 0.00045701 rank 2
2023-02-21 12:15:20,704 DEBUG TRAIN Batch 13/15500 loss 13.376833 loss_att 12.953594 loss_ctc 17.669003 loss_rnnt 12.785095 hw_loss 0.195182 lr 0.00045701 rank 3
2023-02-21 12:15:20,704 DEBUG TRAIN Batch 13/15500 loss 8.309655 loss_att 10.988911 loss_ctc 11.093469 loss_rnnt 7.402461 hw_loss 0.000314 lr 0.00045701 rank 7
2023-02-21 12:15:20,714 DEBUG TRAIN Batch 13/15500 loss 1.083154 loss_att 4.482270 loss_ctc 0.552888 loss_rnnt 0.422380 hw_loss 0.096848 lr 0.00045701 rank 6
2023-02-21 12:15:20,718 DEBUG TRAIN Batch 13/15500 loss 27.334551 loss_att 26.003632 loss_ctc 30.979528 loss_rnnt 27.114567 hw_loss 0.000314 lr 0.00045701 rank 0
2023-02-21 12:15:20,781 DEBUG TRAIN Batch 13/15500 loss 9.344939 loss_att 10.368271 loss_ctc 14.526175 loss_rnnt 8.449273 hw_loss 0.000314 lr 0.00045701 rank 4
2023-02-21 12:15:20,781 DEBUG TRAIN Batch 13/15500 loss 45.605251 loss_att 46.217083 loss_ctc 73.202934 loss_rnnt 41.623329 hw_loss 0.337246 lr 0.00045701 rank 5
2023-02-21 12:15:20,851 DEBUG TRAIN Batch 13/15500 loss 9.326631 loss_att 10.509189 loss_ctc 10.149821 loss_rnnt 8.625633 hw_loss 0.665113 lr 0.00045701 rank 1
2023-02-21 12:17:11,748 DEBUG TRAIN Batch 13/15600 loss 21.305714 loss_att 22.550755 loss_ctc 24.978466 loss_rnnt 20.426609 hw_loss 0.263242 lr 0.00045691 rank 3
2023-02-21 12:17:11,748 DEBUG TRAIN Batch 13/15600 loss 22.261219 loss_att 19.999695 loss_ctc 24.153709 loss_rnnt 22.304665 hw_loss 0.293490 lr 0.00045691 rank 2
2023-02-21 12:17:11,751 DEBUG TRAIN Batch 13/15600 loss 6.896780 loss_att 10.327779 loss_ctc 6.502853 loss_rnnt 6.050165 hw_loss 0.399260 lr 0.00045691 rank 6
2023-02-21 12:17:11,754 DEBUG TRAIN Batch 13/15600 loss 11.457115 loss_att 13.625072 loss_ctc 14.016542 loss_rnnt 10.577100 hw_loss 0.197186 lr 0.00045691 rank 0
2023-02-21 12:17:11,761 DEBUG TRAIN Batch 13/15600 loss 4.299871 loss_att 8.631250 loss_ctc 4.101163 loss_rnnt 3.307086 hw_loss 0.286882 lr 0.00045691 rank 7
2023-02-21 12:17:11,772 DEBUG TRAIN Batch 13/15600 loss 8.493331 loss_att 16.409328 loss_ctc 11.909462 loss_rnnt 6.292868 hw_loss 0.303338 lr 0.00045691 rank 1
2023-02-21 12:17:11,814 DEBUG TRAIN Batch 13/15600 loss 17.537640 loss_att 18.968245 loss_ctc 27.596983 loss_rnnt 15.815861 hw_loss 0.177023 lr 0.00045691 rank 4
2023-02-21 12:17:11,884 DEBUG TRAIN Batch 13/15600 loss 11.729217 loss_att 13.635614 loss_ctc 16.230587 loss_rnnt 10.581676 hw_loss 0.311398 lr 0.00045691 rank 5
2023-02-21 12:18:10,203 DEBUG TRAIN Batch 13/15700 loss 17.788631 loss_att 21.623135 loss_ctc 23.779072 loss_rnnt 15.998253 hw_loss 0.421411 lr 0.00045681 rank 5
2023-02-21 12:18:10,204 DEBUG TRAIN Batch 13/15700 loss 13.950266 loss_att 17.409595 loss_ctc 17.172335 loss_rnnt 12.647860 hw_loss 0.339244 lr 0.00045681 rank 2
2023-02-21 12:18:10,205 DEBUG TRAIN Batch 13/15700 loss 14.196254 loss_att 16.005806 loss_ctc 18.767511 loss_rnnt 13.224703 hw_loss 0.000260 lr 0.00045681 rank 0
2023-02-21 12:18:10,208 DEBUG TRAIN Batch 13/15700 loss 3.549803 loss_att 6.709834 loss_ctc 3.855546 loss_rnnt 2.612507 hw_loss 0.495984 lr 0.00045681 rank 4
2023-02-21 12:18:10,208 DEBUG TRAIN Batch 13/15700 loss 8.595829 loss_att 9.180767 loss_ctc 16.787289 loss_rnnt 7.386508 hw_loss 0.000260 lr 0.00045681 rank 7
2023-02-21 12:18:10,209 DEBUG TRAIN Batch 13/15700 loss 19.336460 loss_att 20.097343 loss_ctc 17.874859 loss_rnnt 19.264816 hw_loss 0.214405 lr 0.00045681 rank 3
2023-02-21 12:18:10,210 DEBUG TRAIN Batch 13/15700 loss 5.904723 loss_att 9.152292 loss_ctc 5.672880 loss_rnnt 5.199976 hw_loss 0.161523 lr 0.00045681 rank 6
2023-02-21 12:18:10,223 DEBUG TRAIN Batch 13/15700 loss 12.322431 loss_att 19.843542 loss_ctc 14.879878 loss_rnnt 10.361162 hw_loss 0.217596 lr 0.00045681 rank 1
2023-02-21 12:19:06,547 DEBUG TRAIN Batch 13/15800 loss 2.433590 loss_att 5.374030 loss_ctc 4.069901 loss_rnnt 1.514303 hw_loss 0.211920 lr 0.00045672 rank 2
2023-02-21 12:19:06,550 DEBUG TRAIN Batch 13/15800 loss 6.652645 loss_att 13.961267 loss_ctc 10.414833 loss_rnnt 4.483195 hw_loss 0.386438 lr 0.00045672 rank 0
2023-02-21 12:19:06,554 DEBUG TRAIN Batch 13/15800 loss 12.710288 loss_att 14.252323 loss_ctc 15.748091 loss_rnnt 11.834362 hw_loss 0.304645 lr 0.00045672 rank 5
2023-02-21 12:19:06,556 DEBUG TRAIN Batch 13/15800 loss 3.859853 loss_att 11.638287 loss_ctc 3.278716 loss_rnnt 2.333413 hw_loss 0.090448 lr 0.00045672 rank 4
2023-02-21 12:19:06,558 DEBUG TRAIN Batch 13/15800 loss 27.099409 loss_att 38.319065 loss_ctc 26.355913 loss_rnnt 24.775982 hw_loss 0.334930 lr 0.00045672 rank 6
2023-02-21 12:19:06,562 DEBUG TRAIN Batch 13/15800 loss 24.539244 loss_att 23.710379 loss_ctc 29.221836 loss_rnnt 23.907200 hw_loss 0.325257 lr 0.00045672 rank 1
2023-02-21 12:19:06,561 DEBUG TRAIN Batch 13/15800 loss 7.754991 loss_att 10.115167 loss_ctc 19.763363 loss_rnnt 5.681802 hw_loss 0.000070 lr 0.00045672 rank 7
2023-02-21 12:19:06,618 DEBUG TRAIN Batch 13/15800 loss 14.387958 loss_att 16.550848 loss_ctc 18.883007 loss_rnnt 13.356003 hw_loss 0.000070 lr 0.00045672 rank 3
2023-02-21 12:20:01,955 DEBUG TRAIN Batch 13/15900 loss 18.174837 loss_att 24.879658 loss_ctc 21.644842 loss_rnnt 16.079948 hw_loss 0.546105 lr 0.00045662 rank 7
2023-02-21 12:20:01,956 DEBUG TRAIN Batch 13/15900 loss 10.692297 loss_att 14.115894 loss_ctc 13.516191 loss_rnnt 9.513220 hw_loss 0.220947 lr 0.00045662 rank 0
2023-02-21 12:20:01,955 DEBUG TRAIN Batch 13/15900 loss 11.226554 loss_att 15.614497 loss_ctc 13.297311 loss_rnnt 9.910398 hw_loss 0.304623 lr 0.00045662 rank 4
2023-02-21 12:20:01,958 DEBUG TRAIN Batch 13/15900 loss 22.401819 loss_att 29.485701 loss_ctc 24.115376 loss_rnnt 20.595676 hw_loss 0.301672 lr 0.00045662 rank 6
2023-02-21 12:20:01,963 DEBUG TRAIN Batch 13/15900 loss 19.040716 loss_att 21.541153 loss_ctc 18.872393 loss_rnnt 18.563036 hw_loss 0.000066 lr 0.00045662 rank 3
2023-02-21 12:20:01,963 DEBUG TRAIN Batch 13/15900 loss 33.948166 loss_att 39.617382 loss_ctc 41.053467 loss_rnnt 31.781193 hw_loss 0.160790 lr 0.00045662 rank 2
2023-02-21 12:20:01,997 DEBUG TRAIN Batch 13/15900 loss 4.565735 loss_att 6.142152 loss_ctc 6.065917 loss_rnnt 3.884674 hw_loss 0.310788 lr 0.00045662 rank 5
2023-02-21 12:20:02,019 DEBUG TRAIN Batch 13/15900 loss 5.686519 loss_att 11.368950 loss_ctc 5.488825 loss_rnnt 4.544466 hw_loss 0.059861 lr 0.00045662 rank 1
2023-02-21 12:21:00,468 DEBUG TRAIN Batch 13/16000 loss 1.656226 loss_att 5.311892 loss_ctc 3.545390 loss_rnnt 0.548138 hw_loss 0.234498 lr 0.00045653 rank 6
2023-02-21 12:21:00,469 DEBUG TRAIN Batch 13/16000 loss 5.368409 loss_att 7.795285 loss_ctc 5.391429 loss_rnnt 4.704063 hw_loss 0.329813 lr 0.00045653 rank 0
2023-02-21 12:21:00,472 DEBUG TRAIN Batch 13/16000 loss 12.705705 loss_att 17.866428 loss_ctc 26.760630 loss_rnnt 9.799489 hw_loss 0.000150 lr 0.00045653 rank 2
2023-02-21 12:21:00,472 DEBUG TRAIN Batch 13/16000 loss 3.797740 loss_att 4.679793 loss_ctc 1.364272 loss_rnnt 3.733888 hw_loss 0.397318 lr 0.00045653 rank 4
2023-02-21 12:21:00,474 DEBUG TRAIN Batch 13/16000 loss 14.817507 loss_att 20.697834 loss_ctc 22.184162 loss_rnnt 12.463980 hw_loss 0.366076 lr 0.00045653 rank 5
2023-02-21 12:21:00,479 DEBUG TRAIN Batch 13/16000 loss 37.438194 loss_att 43.323845 loss_ctc 63.638119 loss_rnnt 32.767658 hw_loss 0.000150 lr 0.00045653 rank 3
2023-02-21 12:21:00,483 DEBUG TRAIN Batch 13/16000 loss 1.436150 loss_att 6.602414 loss_ctc 2.314520 loss_rnnt 0.285701 hw_loss 0.000150 lr 0.00045653 rank 7
2023-02-21 12:21:00,488 DEBUG TRAIN Batch 13/16000 loss 9.826101 loss_att 13.565042 loss_ctc 18.505226 loss_rnnt 7.921017 hw_loss 0.000150 lr 0.00045653 rank 1
2023-02-21 12:21:57,325 DEBUG TRAIN Batch 13/16100 loss 15.630839 loss_att 15.578725 loss_ctc 16.164450 loss_rnnt 15.376926 hw_loss 0.362227 lr 0.00045643 rank 2
2023-02-21 12:21:57,326 DEBUG TRAIN Batch 13/16100 loss 25.440393 loss_att 25.634233 loss_ctc 37.088856 loss_rnnt 23.848473 hw_loss 0.000049 lr 0.00045643 rank 5
2023-02-21 12:21:57,328 DEBUG TRAIN Batch 13/16100 loss 19.387648 loss_att 19.792891 loss_ctc 18.464046 loss_rnnt 19.337839 hw_loss 0.172326 lr 0.00045643 rank 3
2023-02-21 12:21:57,330 DEBUG TRAIN Batch 13/16100 loss 6.333955 loss_att 10.833412 loss_ctc 8.060073 loss_rnnt 5.055232 hw_loss 0.278780 lr 0.00045643 rank 4
2023-02-21 12:21:57,337 DEBUG TRAIN Batch 13/16100 loss 9.314750 loss_att 11.121202 loss_ctc 12.471336 loss_rnnt 8.446341 hw_loss 0.161701 lr 0.00045643 rank 6
2023-02-21 12:21:57,338 DEBUG TRAIN Batch 13/16100 loss 12.472999 loss_att 13.024551 loss_ctc 25.812469 loss_rnnt 10.386381 hw_loss 0.370709 lr 0.00045643 rank 1
2023-02-21 12:21:57,345 DEBUG TRAIN Batch 13/16100 loss 29.659697 loss_att 38.478905 loss_ctc 54.055305 loss_rnnt 24.335787 hw_loss 0.576226 lr 0.00045643 rank 7
2023-02-21 12:21:57,393 DEBUG TRAIN Batch 13/16100 loss 16.774525 loss_att 36.786938 loss_ctc 16.728937 loss_rnnt 12.654175 hw_loss 0.232402 lr 0.00045643 rank 0
2023-02-21 12:22:54,239 DEBUG TRAIN Batch 13/16200 loss 12.597198 loss_att 16.206972 loss_ctc 15.865313 loss_rnnt 11.279663 hw_loss 0.299680 lr 0.00045634 rank 2
2023-02-21 12:22:54,242 DEBUG TRAIN Batch 13/16200 loss 17.498398 loss_att 18.211454 loss_ctc 22.125156 loss_rnnt 16.584545 hw_loss 0.289386 lr 0.00045634 rank 3
2023-02-21 12:22:54,243 DEBUG TRAIN Batch 13/16200 loss 7.312243 loss_att 8.725751 loss_ctc 7.229013 loss_rnnt 6.883295 hw_loss 0.295021 lr 0.00045634 rank 0
2023-02-21 12:22:54,245 DEBUG TRAIN Batch 13/16200 loss 24.185101 loss_att 31.560242 loss_ctc 29.197264 loss_rnnt 21.942572 hw_loss 0.186022 lr 0.00045634 rank 6
2023-02-21 12:22:54,246 DEBUG TRAIN Batch 13/16200 loss 6.371732 loss_att 9.076910 loss_ctc 7.727288 loss_rnnt 5.384305 hw_loss 0.498096 lr 0.00045634 rank 5
2023-02-21 12:22:54,251 DEBUG TRAIN Batch 13/16200 loss 27.626387 loss_att 36.568974 loss_ctc 42.029884 loss_rnnt 23.820602 hw_loss 0.181501 lr 0.00045634 rank 7
2023-02-21 12:22:54,252 DEBUG TRAIN Batch 13/16200 loss 15.481956 loss_att 18.899349 loss_ctc 22.001711 loss_rnnt 13.929061 hw_loss 0.000214 lr 0.00045634 rank 1
2023-02-21 12:22:54,298 DEBUG TRAIN Batch 13/16200 loss 17.087229 loss_att 21.009962 loss_ctc 19.349602 loss_rnnt 15.911947 hw_loss 0.167034 lr 0.00045634 rank 4
2023-02-21 12:23:53,240 DEBUG TRAIN Batch 13/16300 loss 11.323904 loss_att 14.391836 loss_ctc 17.280243 loss_rnnt 9.864150 hw_loss 0.097479 lr 0.00045624 rank 5
2023-02-21 12:23:53,243 DEBUG TRAIN Batch 13/16300 loss 9.379366 loss_att 12.859635 loss_ctc 7.601255 loss_rnnt 8.721505 hw_loss 0.372916 lr 0.00045624 rank 6
2023-02-21 12:23:53,246 DEBUG TRAIN Batch 13/16300 loss 0.942397 loss_att 4.328375 loss_ctc 1.157929 loss_rnnt 0.236314 hw_loss 0.000282 lr 0.00045624 rank 3
2023-02-21 12:23:53,247 DEBUG TRAIN Batch 13/16300 loss 11.016417 loss_att 18.986616 loss_ctc 16.506704 loss_rnnt 8.690189 hw_loss 0.000282 lr 0.00045624 rank 7
2023-02-21 12:23:53,253 DEBUG TRAIN Batch 13/16300 loss 38.061607 loss_att 41.908520 loss_ctc 41.563297 loss_rnnt 36.711269 hw_loss 0.213866 lr 0.00045624 rank 1
2023-02-21 12:23:53,262 DEBUG TRAIN Batch 13/16300 loss 3.457249 loss_att 4.607361 loss_ctc 2.831124 loss_rnnt 3.014127 hw_loss 0.556092 lr 0.00045624 rank 2
2023-02-21 12:23:53,283 DEBUG TRAIN Batch 13/16300 loss 19.510492 loss_att 20.519732 loss_ctc 20.670490 loss_rnnt 19.153830 hw_loss 0.000282 lr 0.00045624 rank 0
2023-02-21 12:23:53,290 DEBUG TRAIN Batch 13/16300 loss 23.513678 loss_att 28.176266 loss_ctc 41.765125 loss_rnnt 20.147482 hw_loss 0.000282 lr 0.00045624 rank 4
2023-02-21 12:25:45,043 DEBUG TRAIN Batch 13/16400 loss 7.881270 loss_att 15.466366 loss_ctc 9.456116 loss_rnnt 5.906631 hw_loss 0.464326 lr 0.00045615 rank 5
2023-02-21 12:25:45,048 DEBUG TRAIN Batch 13/16400 loss 3.090930 loss_att 3.871899 loss_ctc 5.140913 loss_rnnt 2.444349 hw_loss 0.406980 lr 0.00045615 rank 2
2023-02-21 12:25:45,050 DEBUG TRAIN Batch 13/16400 loss 8.603399 loss_att 12.598186 loss_ctc 8.132580 loss_rnnt 7.867094 hw_loss 0.000234 lr 0.00045615 rank 3
2023-02-21 12:25:45,051 DEBUG TRAIN Batch 13/16400 loss 7.169122 loss_att 9.489173 loss_ctc 7.574154 loss_rnnt 6.486237 hw_loss 0.309131 lr 0.00045615 rank 4
2023-02-21 12:25:45,052 DEBUG TRAIN Batch 13/16400 loss 15.978375 loss_att 17.983795 loss_ctc 20.354019 loss_rnnt 14.833769 hw_loss 0.300195 lr 0.00045615 rank 6
2023-02-21 12:25:45,055 DEBUG TRAIN Batch 13/16400 loss 10.017651 loss_att 13.774281 loss_ctc 13.142889 loss_rnnt 8.609225 hw_loss 0.450752 lr 0.00045615 rank 0
2023-02-21 12:25:45,060 DEBUG TRAIN Batch 13/16400 loss 17.276875 loss_att 15.760093 loss_ctc 24.563635 loss_rnnt 16.500389 hw_loss 0.203015 lr 0.00045615 rank 7
2023-02-21 12:25:45,063 DEBUG TRAIN Batch 13/16400 loss 16.412832 loss_att 19.344574 loss_ctc 24.993275 loss_rnnt 14.476992 hw_loss 0.385184 lr 0.00045615 rank 1
2023-02-21 12:27:36,331 DEBUG TRAIN Batch 13/16500 loss 16.271650 loss_att 17.797888 loss_ctc 21.137743 loss_rnnt 15.252399 hw_loss 0.122234 lr 0.00045605 rank 0
2023-02-21 12:27:36,335 DEBUG TRAIN Batch 13/16500 loss 5.795507 loss_att 8.623520 loss_ctc 5.730956 loss_rnnt 5.083364 hw_loss 0.290900 lr 0.00045605 rank 5
2023-02-21 12:27:36,338 DEBUG TRAIN Batch 13/16500 loss 7.684871 loss_att 14.395426 loss_ctc 8.360125 loss_rnnt 6.131486 hw_loss 0.227326 lr 0.00045605 rank 7
2023-02-21 12:27:36,340 DEBUG TRAIN Batch 13/16500 loss 7.728087 loss_att 10.691054 loss_ctc 9.455072 loss_rnnt 6.621819 hw_loss 0.531394 lr 0.00045605 rank 3
2023-02-21 12:27:36,341 DEBUG TRAIN Batch 13/16500 loss 25.617081 loss_att 26.405735 loss_ctc 28.208668 loss_rnnt 25.005230 hw_loss 0.203577 lr 0.00045605 rank 6
2023-02-21 12:27:36,342 DEBUG TRAIN Batch 13/16500 loss 25.852901 loss_att 31.304810 loss_ctc 37.171623 loss_rnnt 23.090521 hw_loss 0.305320 lr 0.00045605 rank 4
2023-02-21 12:27:36,347 DEBUG TRAIN Batch 13/16500 loss 6.040413 loss_att 10.340367 loss_ctc 7.754245 loss_rnnt 4.838667 hw_loss 0.212333 lr 0.00045605 rank 1
2023-02-21 12:27:36,393 DEBUG TRAIN Batch 13/16500 loss 12.474304 loss_att 16.159088 loss_ctc 13.686065 loss_rnnt 11.479639 hw_loss 0.180262 lr 0.00045605 rank 2
2023-02-21 12:28:33,890 DEBUG TRAIN Batch 13/16600 loss 29.084387 loss_att 31.922527 loss_ctc 42.520966 loss_rnnt 26.605122 hw_loss 0.225172 lr 0.00045596 rank 5
2023-02-21 12:28:33,892 DEBUG TRAIN Batch 13/16600 loss 53.500870 loss_att 47.650558 loss_ctc 63.596642 loss_rnnt 53.251358 hw_loss 0.137761 lr 0.00045596 rank 0
2023-02-21 12:28:33,894 DEBUG TRAIN Batch 13/16600 loss 11.394203 loss_att 17.418131 loss_ctc 12.349833 loss_rnnt 10.012787 hw_loss 0.092275 lr 0.00045596 rank 2
2023-02-21 12:28:33,896 DEBUG TRAIN Batch 13/16600 loss 11.789711 loss_att 10.769428 loss_ctc 14.676893 loss_rnnt 11.448446 hw_loss 0.300680 lr 0.00045596 rank 6
2023-02-21 12:28:33,903 DEBUG TRAIN Batch 13/16600 loss 2.517870 loss_att 5.214691 loss_ctc 3.425105 loss_rnnt 1.726700 hw_loss 0.245327 lr 0.00045596 rank 4
2023-02-21 12:28:33,904 DEBUG TRAIN Batch 13/16600 loss 19.338491 loss_att 28.425262 loss_ctc 29.181929 loss_rnnt 16.208427 hw_loss 0.000470 lr 0.00045596 rank 3
2023-02-21 12:28:33,905 DEBUG TRAIN Batch 13/16600 loss 1.067464 loss_att 5.637933 loss_ctc 0.397797 loss_rnnt 0.242409 hw_loss 0.000470 lr 0.00045596 rank 1
2023-02-21 12:28:33,906 DEBUG TRAIN Batch 13/16600 loss 3.556865 loss_att 8.478836 loss_ctc 6.644114 loss_rnnt 2.160587 hw_loss 0.000470 lr 0.00045596 rank 7
2023-02-21 12:29:31,167 DEBUG TRAIN Batch 13/16700 loss 26.651037 loss_att 30.698742 loss_ctc 26.543560 loss_rnnt 25.710686 hw_loss 0.272137 lr 0.00045586 rank 0
2023-02-21 12:29:31,169 DEBUG TRAIN Batch 13/16700 loss 8.888944 loss_att 10.987966 loss_ctc 12.882765 loss_rnnt 7.885105 hw_loss 0.096610 lr 0.00045586 rank 5
2023-02-21 12:29:31,169 DEBUG TRAIN Batch 13/16700 loss 52.415298 loss_att 64.580978 loss_ctc 60.736900 loss_rnnt 48.739510 hw_loss 0.249575 lr 0.00045586 rank 2
2023-02-21 12:29:31,171 DEBUG TRAIN Batch 13/16700 loss 2.995722 loss_att 8.123882 loss_ctc 3.638227 loss_rnnt 1.617259 hw_loss 0.500930 lr 0.00045586 rank 3
2023-02-21 12:29:31,173 DEBUG TRAIN Batch 13/16700 loss 24.195801 loss_att 23.123970 loss_ctc 27.994911 loss_rnnt 23.714365 hw_loss 0.354850 lr 0.00045586 rank 4
2023-02-21 12:29:31,177 DEBUG TRAIN Batch 13/16700 loss 10.107602 loss_att 10.049689 loss_ctc 14.623733 loss_rnnt 9.381983 hw_loss 0.253223 lr 0.00045586 rank 7
2023-02-21 12:29:31,185 DEBUG TRAIN Batch 13/16700 loss 17.597427 loss_att 20.033373 loss_ctc 25.745842 loss_rnnt 15.777475 hw_loss 0.461827 lr 0.00045586 rank 1
2023-02-21 12:29:31,186 DEBUG TRAIN Batch 13/16700 loss 3.702619 loss_att 4.789594 loss_ctc 5.667807 loss_rnnt 3.061957 hw_loss 0.302327 lr 0.00045586 rank 6
2023-02-21 12:30:29,459 DEBUG TRAIN Batch 13/16800 loss 4.791523 loss_att 8.488627 loss_ctc 5.759040 loss_rnnt 3.743409 hw_loss 0.336919 lr 0.00045577 rank 2
2023-02-21 12:30:29,460 DEBUG TRAIN Batch 13/16800 loss 24.518808 loss_att 29.793848 loss_ctc 31.052965 loss_rnnt 22.468594 hw_loss 0.232473 lr 0.00045577 rank 0
2023-02-21 12:30:29,464 DEBUG TRAIN Batch 13/16800 loss 9.052407 loss_att 10.469385 loss_ctc 9.976673 loss_rnnt 8.500284 hw_loss 0.272795 lr 0.00045577 rank 5
2023-02-21 12:30:29,470 DEBUG TRAIN Batch 13/16800 loss 12.829376 loss_att 16.777208 loss_ctc 14.641301 loss_rnnt 11.685359 hw_loss 0.211614 lr 0.00045577 rank 6
2023-02-21 12:30:29,471 DEBUG TRAIN Batch 13/16800 loss 20.319134 loss_att 25.146637 loss_ctc 22.954109 loss_rnnt 18.934582 hw_loss 0.126975 lr 0.00045577 rank 7
2023-02-21 12:30:29,472 DEBUG TRAIN Batch 13/16800 loss 21.125607 loss_att 23.034056 loss_ctc 23.753952 loss_rnnt 20.393406 hw_loss 0.000117 lr 0.00045577 rank 3
2023-02-21 12:30:29,473 DEBUG TRAIN Batch 13/16800 loss 13.441829 loss_att 14.706035 loss_ctc 17.688448 loss_rnnt 12.467501 hw_loss 0.291133 lr 0.00045577 rank 4
2023-02-21 12:30:29,476 DEBUG TRAIN Batch 13/16800 loss 1.108962 loss_att 4.213520 loss_ctc 1.055997 loss_rnnt 0.495050 hw_loss 0.000117 lr 0.00045577 rank 1
2023-02-21 12:31:26,125 DEBUG TRAIN Batch 13/16900 loss 19.875099 loss_att 19.802521 loss_ctc 21.538784 loss_rnnt 19.362020 hw_loss 0.573316 lr 0.00045567 rank 2
2023-02-21 12:31:26,128 DEBUG TRAIN Batch 13/16900 loss 2.646093 loss_att 5.984626 loss_ctc 3.267287 loss_rnnt 1.804128 hw_loss 0.171436 lr 0.00045567 rank 3
2023-02-21 12:31:26,127 DEBUG TRAIN Batch 13/16900 loss 21.644402 loss_att 26.096714 loss_ctc 41.438068 loss_rnnt 17.982269 hw_loss 0.248466 lr 0.00045567 rank 0
2023-02-21 12:31:26,127 DEBUG TRAIN Batch 13/16900 loss 11.564688 loss_att 15.803639 loss_ctc 12.037586 loss_rnnt 10.651137 hw_loss 0.005076 lr 0.00045567 rank 6
2023-02-21 12:31:26,131 DEBUG TRAIN Batch 13/16900 loss 11.601308 loss_att 12.445552 loss_ctc 21.060713 loss_rnnt 10.028128 hw_loss 0.268271 lr 0.00045567 rank 4
2023-02-21 12:31:26,130 DEBUG TRAIN Batch 13/16900 loss 13.097984 loss_att 14.578859 loss_ctc 19.176168 loss_rnnt 11.838488 hw_loss 0.286678 lr 0.00045567 rank 5
2023-02-21 12:31:26,136 DEBUG TRAIN Batch 13/16900 loss 12.439977 loss_att 11.881765 loss_ctc 12.270216 loss_rnnt 12.512655 hw_loss 0.115500 lr 0.00045567 rank 7
2023-02-21 12:31:26,142 DEBUG TRAIN Batch 13/16900 loss 10.274058 loss_att 14.452963 loss_ctc 12.157398 loss_rnnt 9.039837 hw_loss 0.276242 lr 0.00045567 rank 1
2023-02-21 12:32:21,661 DEBUG TRAIN Batch 13/17000 loss 8.309815 loss_att 12.650485 loss_ctc 8.082245 loss_rnnt 7.268348 hw_loss 0.381895 lr 0.00045558 rank 2
2023-02-21 12:32:21,664 DEBUG TRAIN Batch 13/17000 loss 4.136332 loss_att 10.583755 loss_ctc 4.630546 loss_rnnt 2.684088 hw_loss 0.181619 lr 0.00045558 rank 3
2023-02-21 12:32:21,665 DEBUG TRAIN Batch 13/17000 loss 12.249592 loss_att 15.524013 loss_ctc 14.393550 loss_rnnt 11.142768 hw_loss 0.311400 lr 0.00045558 rank 0
2023-02-21 12:32:21,667 DEBUG TRAIN Batch 13/17000 loss 15.733120 loss_att 20.137747 loss_ctc 26.510803 loss_rnnt 13.308414 hw_loss 0.200166 lr 0.00045558 rank 5
2023-02-21 12:32:21,670 DEBUG TRAIN Batch 13/17000 loss 8.291675 loss_att 12.436611 loss_ctc 10.960793 loss_rnnt 6.901897 hw_loss 0.384203 lr 0.00045558 rank 4
2023-02-21 12:32:21,673 DEBUG TRAIN Batch 13/17000 loss 31.123032 loss_att 31.998825 loss_ctc 39.284500 loss_rnnt 29.858141 hw_loss 0.002880 lr 0.00045558 rank 6
2023-02-21 12:32:21,674 DEBUG TRAIN Batch 13/17000 loss 9.808995 loss_att 9.688753 loss_ctc 9.126726 loss_rnnt 9.686353 hw_loss 0.445612 lr 0.00045558 rank 7
2023-02-21 12:32:21,676 DEBUG TRAIN Batch 13/17000 loss 20.203905 loss_att 21.249224 loss_ctc 29.433693 loss_rnnt 18.621262 hw_loss 0.268015 lr 0.00045558 rank 1
2023-02-21 12:33:19,688 DEBUG TRAIN Batch 13/17100 loss 23.111725 loss_att 25.620743 loss_ctc 33.444256 loss_rnnt 21.134520 hw_loss 0.183250 lr 0.00045549 rank 0
2023-02-21 12:33:19,691 DEBUG TRAIN Batch 13/17100 loss 14.115493 loss_att 18.468376 loss_ctc 15.152023 loss_rnnt 12.868615 hw_loss 0.446432 lr 0.00045549 rank 4
2023-02-21 12:33:19,691 DEBUG TRAIN Batch 13/17100 loss 9.910171 loss_att 13.917755 loss_ctc 12.371819 loss_rnnt 8.638165 hw_loss 0.266753 lr 0.00045549 rank 5
2023-02-21 12:33:19,692 DEBUG TRAIN Batch 13/17100 loss 25.374605 loss_att 28.450134 loss_ctc 40.093479 loss_rnnt 22.690277 hw_loss 0.200071 lr 0.00045549 rank 7
2023-02-21 12:33:19,692 DEBUG TRAIN Batch 13/17100 loss 11.152615 loss_att 14.295429 loss_ctc 17.537323 loss_rnnt 9.476381 hw_loss 0.368205 lr 0.00045549 rank 3
2023-02-21 12:33:19,698 DEBUG TRAIN Batch 13/17100 loss 4.843460 loss_att 8.215334 loss_ctc 6.537906 loss_rnnt 3.797653 hw_loss 0.272824 lr 0.00045549 rank 6
2023-02-21 12:33:19,701 DEBUG TRAIN Batch 13/17100 loss 16.882441 loss_att 27.406483 loss_ctc 29.520586 loss_rnnt 12.987955 hw_loss 0.196111 lr 0.00045549 rank 1
2023-02-21 12:33:19,713 DEBUG TRAIN Batch 13/17100 loss 6.893367 loss_att 10.765299 loss_ctc 7.837039 loss_rnnt 5.962109 hw_loss 0.058217 lr 0.00045549 rank 2
2023-02-21 12:35:10,766 DEBUG TRAIN Batch 13/17200 loss 8.430025 loss_att 10.676581 loss_ctc 12.222198 loss_rnnt 7.475044 hw_loss 0.000086 lr 0.00045539 rank 2
2023-02-21 12:35:10,768 DEBUG TRAIN Batch 13/17200 loss 11.741782 loss_att 16.348120 loss_ctc 17.114082 loss_rnnt 9.973562 hw_loss 0.244960 lr 0.00045539 rank 4
2023-02-21 12:35:10,769 DEBUG TRAIN Batch 13/17200 loss 13.003225 loss_att 16.854284 loss_ctc 13.920305 loss_rnnt 11.955323 hw_loss 0.291399 lr 0.00045539 rank 5
2023-02-21 12:35:10,773 DEBUG TRAIN Batch 13/17200 loss 11.367939 loss_att 18.513500 loss_ctc 18.222311 loss_rnnt 9.024864 hw_loss 0.000086 lr 0.00045539 rank 0
2023-02-21 12:35:10,774 DEBUG TRAIN Batch 13/17200 loss 14.624048 loss_att 13.795793 loss_ctc 18.527580 loss_rnnt 14.073269 hw_loss 0.367421 lr 0.00045539 rank 3
2023-02-21 12:35:10,780 DEBUG TRAIN Batch 13/17200 loss 37.348957 loss_att 35.495617 loss_ctc 33.423138 loss_rnnt 38.018658 hw_loss 0.420772 lr 0.00045539 rank 6
2023-02-21 12:35:10,784 DEBUG TRAIN Batch 13/17200 loss 8.877749 loss_att 10.927649 loss_ctc 10.404164 loss_rnnt 7.881979 hw_loss 0.716752 lr 0.00045539 rank 7
2023-02-21 12:35:10,785 DEBUG TRAIN Batch 13/17200 loss 8.398211 loss_att 20.858335 loss_ctc 17.207201 loss_rnnt 4.731608 hw_loss 0.000086 lr 0.00045539 rank 1
2023-02-21 12:36:08,736 DEBUG TRAIN Batch 13/17300 loss 30.903788 loss_att 38.862335 loss_ctc 56.555031 loss_rnnt 25.891491 hw_loss 0.000795 lr 0.00045530 rank 5
2023-02-21 12:36:08,743 DEBUG TRAIN Batch 13/17300 loss 8.582546 loss_att 9.519263 loss_ctc 10.648284 loss_rnnt 7.908717 hw_loss 0.395728 lr 0.00045530 rank 2
2023-02-21 12:36:08,745 DEBUG TRAIN Batch 13/17300 loss 17.768156 loss_att 17.657871 loss_ctc 22.202360 loss_rnnt 17.029902 hw_loss 0.317034 lr 0.00045530 rank 0
2023-02-21 12:36:08,749 DEBUG TRAIN Batch 13/17300 loss 2.822742 loss_att 6.478819 loss_ctc 5.485062 loss_rnnt 1.555494 hw_loss 0.339481 lr 0.00045530 rank 6
2023-02-21 12:36:08,750 DEBUG TRAIN Batch 13/17300 loss 21.674387 loss_att 19.435505 loss_ctc 24.820843 loss_rnnt 21.601665 hw_loss 0.189324 lr 0.00045530 rank 4
2023-02-21 12:36:08,757 DEBUG TRAIN Batch 13/17300 loss 23.938601 loss_att 21.245945 loss_ctc 29.123915 loss_rnnt 23.677410 hw_loss 0.203151 lr 0.00045530 rank 3
2023-02-21 12:36:08,758 DEBUG TRAIN Batch 13/17300 loss 7.394994 loss_att 8.066940 loss_ctc 9.533918 loss_rnnt 6.850857 hw_loss 0.233546 lr 0.00045530 rank 1
2023-02-21 12:36:08,760 DEBUG TRAIN Batch 13/17300 loss 24.759747 loss_att 21.081999 loss_ctc 26.086529 loss_rnnt 25.262852 hw_loss 0.104135 lr 0.00045530 rank 7
2023-02-21 12:38:00,922 DEBUG TRAIN Batch 13/17400 loss 12.123705 loss_att 13.654970 loss_ctc 18.063503 loss_rnnt 10.829071 hw_loss 0.368263 lr 0.00045520 rank 2
2023-02-21 12:38:00,924 DEBUG TRAIN Batch 13/17400 loss 7.536463 loss_att 9.862101 loss_ctc 6.459321 loss_rnnt 7.066193 hw_loss 0.278927 lr 0.00045520 rank 3
2023-02-21 12:38:00,924 DEBUG TRAIN Batch 13/17400 loss 19.237888 loss_att 25.015541 loss_ctc 23.242630 loss_rnnt 17.308559 hw_loss 0.449688 lr 0.00045520 rank 0
2023-02-21 12:38:00,925 DEBUG TRAIN Batch 13/17400 loss 11.866829 loss_att 13.307470 loss_ctc 16.802181 loss_rnnt 10.790008 hw_loss 0.244963 lr 0.00045520 rank 5
2023-02-21 12:38:00,926 DEBUG TRAIN Batch 13/17400 loss 4.590549 loss_att 8.997735 loss_ctc 7.634508 loss_rnnt 3.157266 hw_loss 0.273721 lr 0.00045520 rank 7
2023-02-21 12:38:00,927 DEBUG TRAIN Batch 13/17400 loss 10.796095 loss_att 10.414265 loss_ctc 13.026992 loss_rnnt 10.405433 hw_loss 0.317956 lr 0.00045520 rank 4
2023-02-21 12:38:00,930 DEBUG TRAIN Batch 13/17400 loss 5.679115 loss_att 8.190477 loss_ctc 8.272388 loss_rnnt 4.578841 hw_loss 0.472937 lr 0.00045520 rank 1
2023-02-21 12:38:00,930 DEBUG TRAIN Batch 13/17400 loss 10.324897 loss_att 12.864176 loss_ctc 11.766523 loss_rnnt 9.624527 hw_loss 0.000557 lr 0.00045520 rank 6
2023-02-21 12:38:57,827 DEBUG TRAIN Batch 13/17500 loss 14.808402 loss_att 14.512881 loss_ctc 17.868053 loss_rnnt 14.311505 hw_loss 0.277589 lr 0.00045511 rank 5
2023-02-21 12:38:57,827 DEBUG TRAIN Batch 13/17500 loss 26.221876 loss_att 22.657549 loss_ctc 22.608423 loss_rnnt 27.259867 hw_loss 0.293757 lr 0.00045511 rank 7
2023-02-21 12:38:57,827 DEBUG TRAIN Batch 13/17500 loss 9.102147 loss_att 14.672851 loss_ctc 8.395398 loss_rnnt 8.082207 hw_loss 0.000059 lr 0.00045511 rank 0
2023-02-21 12:38:57,828 DEBUG TRAIN Batch 13/17500 loss 23.410053 loss_att 23.255348 loss_ctc 31.869081 loss_rnnt 22.256542 hw_loss 0.106089 lr 0.00045511 rank 2
2023-02-21 12:38:57,830 DEBUG TRAIN Batch 13/17500 loss 4.229042 loss_att 5.293283 loss_ctc 5.207617 loss_rnnt 3.615035 hw_loss 0.507528 lr 0.00045511 rank 4
2023-02-21 12:38:57,830 DEBUG TRAIN Batch 13/17500 loss 17.927475 loss_att 18.653099 loss_ctc 25.484455 loss_rnnt 16.631428 hw_loss 0.268735 lr 0.00045511 rank 3
2023-02-21 12:38:57,832 DEBUG TRAIN Batch 13/17500 loss 4.114454 loss_att 13.759346 loss_ctc 2.576966 loss_rnnt 2.303788 hw_loss 0.162537 lr 0.00045511 rank 6
2023-02-21 12:38:57,836 DEBUG TRAIN Batch 13/17500 loss 4.242686 loss_att 13.132000 loss_ctc 4.292017 loss_rnnt 2.279821 hw_loss 0.334547 lr 0.00045511 rank 1
2023-02-21 12:39:54,353 DEBUG TRAIN Batch 13/17600 loss 21.089346 loss_att 18.139095 loss_ctc 41.705185 loss_rnnt 18.930592 hw_loss 0.000052 lr 0.00045501 rank 5
2023-02-21 12:39:54,353 DEBUG TRAIN Batch 13/17600 loss 5.939861 loss_att 7.511570 loss_ctc 6.511189 loss_rnnt 5.512161 hw_loss 0.069715 lr 0.00045501 rank 0
2023-02-21 12:39:54,354 DEBUG TRAIN Batch 13/17600 loss 28.107447 loss_att 25.407589 loss_ctc 32.818497 loss_rnnt 27.839680 hw_loss 0.336747 lr 0.00045501 rank 3
2023-02-21 12:39:54,361 DEBUG TRAIN Batch 13/17600 loss 19.118567 loss_att 17.975792 loss_ctc 23.450457 loss_rnnt 18.672901 hw_loss 0.181189 lr 0.00045501 rank 1
2023-02-21 12:39:54,362 DEBUG TRAIN Batch 13/17600 loss 27.533756 loss_att 30.030174 loss_ctc 36.460365 loss_rnnt 25.689495 hw_loss 0.290182 lr 0.00045501 rank 7
2023-02-21 12:39:54,363 DEBUG TRAIN Batch 13/17600 loss 8.055079 loss_att 11.161763 loss_ctc 12.494030 loss_rnnt 6.647134 hw_loss 0.365150 lr 0.00045501 rank 2
2023-02-21 12:39:54,372 DEBUG TRAIN Batch 13/17600 loss 36.693699 loss_att 39.190536 loss_ctc 43.835648 loss_rnnt 35.132084 hw_loss 0.206223 lr 0.00045501 rank 6
2023-02-21 12:39:54,377 DEBUG TRAIN Batch 13/17600 loss 5.775743 loss_att 8.622931 loss_ctc 6.905256 loss_rnnt 5.009455 hw_loss 0.086716 lr 0.00045501 rank 4
2023-02-21 12:40:52,669 DEBUG TRAIN Batch 13/17700 loss 13.634025 loss_att 14.517841 loss_ctc 18.963291 loss_rnnt 12.630711 hw_loss 0.217467 lr 0.00045492 rank 5
2023-02-21 12:40:52,675 DEBUG TRAIN Batch 13/17700 loss 10.940530 loss_att 16.435379 loss_ctc 14.479875 loss_rnnt 9.179884 hw_loss 0.355805 lr 0.00045492 rank 0
2023-02-21 12:40:52,676 DEBUG TRAIN Batch 13/17700 loss 2.478040 loss_att 5.881370 loss_ctc 6.408055 loss_rnnt 0.872452 hw_loss 0.751724 lr 0.00045492 rank 4
2023-02-21 12:40:52,677 DEBUG TRAIN Batch 13/17700 loss 4.079818 loss_att 8.783304 loss_ctc 5.786264 loss_rnnt 2.847575 hw_loss 0.120036 lr 0.00045492 rank 2
2023-02-21 12:40:52,682 DEBUG TRAIN Batch 13/17700 loss 4.889030 loss_att 10.155170 loss_ctc 7.727445 loss_rnnt 3.315528 hw_loss 0.265911 lr 0.00045492 rank 6
2023-02-21 12:40:52,682 DEBUG TRAIN Batch 13/17700 loss 8.279266 loss_att 10.031460 loss_ctc 8.764629 loss_rnnt 7.807399 hw_loss 0.106339 lr 0.00045492 rank 1
2023-02-21 12:40:52,685 DEBUG TRAIN Batch 13/17700 loss 4.334083 loss_att 8.365252 loss_ctc 4.302374 loss_rnnt 3.531975 hw_loss 0.000192 lr 0.00045492 rank 7
2023-02-21 12:40:52,687 DEBUG TRAIN Batch 13/17700 loss 5.606408 loss_att 7.774196 loss_ctc 12.938306 loss_rnnt 4.086945 hw_loss 0.203099 lr 0.00045492 rank 3
2023-02-21 12:41:49,049 DEBUG TRAIN Batch 13/17800 loss 2.185491 loss_att 4.712142 loss_ctc 1.460212 loss_rnnt 1.512560 hw_loss 0.495571 lr 0.00045483 rank 5
2023-02-21 12:41:49,050 DEBUG TRAIN Batch 13/17800 loss 33.391033 loss_att 28.976578 loss_ctc 38.693439 loss_rnnt 33.424828 hw_loss 0.266449 lr 0.00045483 rank 3
2023-02-21 12:41:49,052 DEBUG TRAIN Batch 13/17800 loss 3.698152 loss_att 6.956274 loss_ctc 3.614583 loss_rnnt 2.766524 hw_loss 0.545899 lr 0.00045483 rank 6
2023-02-21 12:41:49,053 DEBUG TRAIN Batch 13/17800 loss 4.709897 loss_att 9.667026 loss_ctc 4.398579 loss_rnnt 3.759325 hw_loss 0.001226 lr 0.00045483 rank 7
2023-02-21 12:41:49,054 DEBUG TRAIN Batch 13/17800 loss 7.412549 loss_att 7.836833 loss_ctc 9.698818 loss_rnnt 6.782692 hw_loss 0.450304 lr 0.00045483 rank 1
2023-02-21 12:41:49,057 DEBUG TRAIN Batch 13/17800 loss 8.956201 loss_att 17.386471 loss_ctc 14.363947 loss_rnnt 6.221713 hw_loss 0.613879 lr 0.00045483 rank 4
2023-02-21 12:41:49,062 DEBUG TRAIN Batch 13/17800 loss 27.537748 loss_att 29.176586 loss_ctc 32.407009 loss_rnnt 26.560091 hw_loss 0.001226 lr 0.00045483 rank 0
2023-02-21 12:41:49,104 DEBUG TRAIN Batch 13/17800 loss 4.556207 loss_att 9.457615 loss_ctc 3.954639 loss_rnnt 3.655480 hw_loss 0.001226 lr 0.00045483 rank 2
2023-02-21 12:42:45,582 DEBUG TRAIN Batch 13/17900 loss 5.378736 loss_att 8.630631 loss_ctc 8.751312 loss_rnnt 4.056638 hw_loss 0.416328 lr 0.00045473 rank 6
2023-02-21 12:42:45,586 DEBUG TRAIN Batch 13/17900 loss 14.068284 loss_att 21.733330 loss_ctc 19.148903 loss_rnnt 11.655527 hw_loss 0.379373 lr 0.00045473 rank 5
2023-02-21 12:42:45,589 DEBUG TRAIN Batch 13/17900 loss 6.601154 loss_att 10.265100 loss_ctc 8.296732 loss_rnnt 5.456105 hw_loss 0.349093 lr 0.00045473 rank 2
2023-02-21 12:42:45,589 DEBUG TRAIN Batch 13/17900 loss 22.988089 loss_att 25.945591 loss_ctc 32.004608 loss_rnnt 21.045095 hw_loss 0.279923 lr 0.00045473 rank 7
2023-02-21 12:42:45,591 DEBUG TRAIN Batch 13/17900 loss 24.292522 loss_att 26.041035 loss_ctc 32.602608 loss_rnnt 22.583191 hw_loss 0.471781 lr 0.00045473 rank 0
2023-02-21 12:42:45,593 DEBUG TRAIN Batch 13/17900 loss 13.708073 loss_att 15.579828 loss_ctc 18.030098 loss_rnnt 12.573396 hw_loss 0.345103 lr 0.00045473 rank 3
2023-02-21 12:42:45,593 DEBUG TRAIN Batch 13/17900 loss 10.990148 loss_att 12.481297 loss_ctc 12.751945 loss_rnnt 10.351166 hw_loss 0.198462 lr 0.00045473 rank 4
2023-02-21 12:42:45,598 DEBUG TRAIN Batch 13/17900 loss 11.013153 loss_att 10.395612 loss_ctc 14.911971 loss_rnnt 10.493237 hw_loss 0.231714 lr 0.00045473 rank 1
2023-02-21 12:43:44,120 DEBUG TRAIN Batch 13/18000 loss 15.434238 loss_att 19.278477 loss_ctc 12.910376 loss_rnnt 14.999994 hw_loss 0.003584 lr 0.00045464 rank 7
2023-02-21 12:43:44,123 DEBUG TRAIN Batch 13/18000 loss 5.193758 loss_att 6.828221 loss_ctc 4.353079 loss_rnnt 4.977044 hw_loss 0.003584 lr 0.00045464 rank 4
2023-02-21 12:43:44,123 DEBUG TRAIN Batch 13/18000 loss 9.933622 loss_att 12.314782 loss_ctc 14.732178 loss_rnnt 8.488319 hw_loss 0.617369 lr 0.00045464 rank 6
2023-02-21 12:43:44,124 DEBUG TRAIN Batch 13/18000 loss 2.432455 loss_att 5.265418 loss_ctc 5.318817 loss_rnnt 1.336877 hw_loss 0.270258 lr 0.00045464 rank 0
2023-02-21 12:43:44,127 DEBUG TRAIN Batch 13/18000 loss 5.239863 loss_att 8.102670 loss_ctc 6.483608 loss_rnnt 4.290247 hw_loss 0.396042 lr 0.00045464 rank 1
2023-02-21 12:43:44,127 DEBUG TRAIN Batch 13/18000 loss 2.406125 loss_att 8.031765 loss_ctc 2.696720 loss_rnnt 1.088592 hw_loss 0.288110 lr 0.00045464 rank 3
2023-02-21 12:43:44,151 DEBUG TRAIN Batch 13/18000 loss 11.354198 loss_att 14.222894 loss_ctc 21.131065 loss_rnnt 9.372311 hw_loss 0.196060 lr 0.00045464 rank 5
2023-02-21 12:43:44,174 DEBUG TRAIN Batch 13/18000 loss 10.421521 loss_att 10.970270 loss_ctc 7.238465 loss_rnnt 10.734268 hw_loss 0.003584 lr 0.00045464 rank 2
2023-02-21 12:45:33,189 DEBUG TRAIN Batch 13/18100 loss 9.161115 loss_att 10.429790 loss_ctc 7.919518 loss_rnnt 8.986562 hw_loss 0.161931 lr 0.00045454 rank 3
2023-02-21 12:45:33,190 DEBUG TRAIN Batch 13/18100 loss 34.167435 loss_att 39.320660 loss_ctc 47.478310 loss_rnnt 31.288532 hw_loss 0.137759 lr 0.00045454 rank 6
2023-02-21 12:45:33,190 DEBUG TRAIN Batch 13/18100 loss 10.543485 loss_att 10.662219 loss_ctc 8.585782 loss_rnnt 10.513061 hw_loss 0.501946 lr 0.00045454 rank 2
2023-02-21 12:45:33,190 DEBUG TRAIN Batch 13/18100 loss 11.807097 loss_att 14.633748 loss_ctc 13.759913 loss_rnnt 10.764081 hw_loss 0.407454 lr 0.00045454 rank 7
2023-02-21 12:45:33,192 DEBUG TRAIN Batch 13/18100 loss 6.857029 loss_att 12.786696 loss_ctc 14.597133 loss_rnnt 4.379220 hw_loss 0.487242 lr 0.00045454 rank 5
2023-02-21 12:45:33,201 DEBUG TRAIN Batch 13/18100 loss 6.202336 loss_att 5.432056 loss_ctc 3.377210 loss_rnnt 6.592166 hw_loss 0.264206 lr 0.00045454 rank 0
2023-02-21 12:45:33,203 DEBUG TRAIN Batch 13/18100 loss 14.620541 loss_att 15.374342 loss_ctc 23.367506 loss_rnnt 13.139478 hw_loss 0.307575 lr 0.00045454 rank 4
2023-02-21 12:45:33,222 DEBUG TRAIN Batch 13/18100 loss 38.106739 loss_att 37.243813 loss_ctc 43.017090 loss_rnnt 37.435905 hw_loss 0.353824 lr 0.00045454 rank 1
2023-02-21 12:46:33,903 DEBUG TRAIN Batch 13/18200 loss 24.839256 loss_att 23.760536 loss_ctc 27.520981 loss_rnnt 24.564672 hw_loss 0.248934 lr 0.00045445 rank 2
2023-02-21 12:46:33,914 DEBUG TRAIN Batch 13/18200 loss 19.480865 loss_att 24.185921 loss_ctc 21.650484 loss_rnnt 18.209019 hw_loss 0.077909 lr 0.00045445 rank 4
2023-02-21 12:46:33,917 DEBUG TRAIN Batch 13/18200 loss 22.338215 loss_att 26.675230 loss_ctc 24.224510 loss_rnnt 21.119854 hw_loss 0.186474 lr 0.00045445 rank 3
2023-02-21 12:46:33,918 DEBUG TRAIN Batch 13/18200 loss 15.455323 loss_att 21.508539 loss_ctc 15.784882 loss_rnnt 14.200685 hw_loss 0.000101 lr 0.00045445 rank 6
2023-02-21 12:46:33,920 DEBUG TRAIN Batch 13/18200 loss 3.731098 loss_att 6.701478 loss_ctc 3.168369 loss_rnnt 2.993205 hw_loss 0.410338 lr 0.00045445 rank 1
2023-02-21 12:46:33,921 DEBUG TRAIN Batch 13/18200 loss 40.421116 loss_att 42.948048 loss_ctc 53.239441 loss_rnnt 38.020351 hw_loss 0.349250 lr 0.00045445 rank 0
2023-02-21 12:46:33,923 DEBUG TRAIN Batch 13/18200 loss 2.906543 loss_att 6.330455 loss_ctc 2.214684 loss_rnnt 2.095747 hw_loss 0.409241 lr 0.00045445 rank 5
2023-02-21 12:46:33,928 DEBUG TRAIN Batch 13/18200 loss 10.439311 loss_att 13.536688 loss_ctc 12.266813 loss_rnnt 9.480678 hw_loss 0.179046 lr 0.00045445 rank 7
2023-02-21 12:48:24,358 DEBUG TRAIN Batch 13/18300 loss 8.811668 loss_att 8.146489 loss_ctc 9.812684 loss_rnnt 8.618822 hw_loss 0.360776 lr 0.00045436 rank 5
2023-02-21 12:48:24,368 DEBUG TRAIN Batch 13/18300 loss 1.610912 loss_att 5.145474 loss_ctc 4.753504 loss_rnnt 0.319214 hw_loss 0.310825 lr 0.00045436 rank 7
2023-02-21 12:48:24,368 DEBUG TRAIN Batch 13/18300 loss 4.517069 loss_att 11.474122 loss_ctc 5.672316 loss_rnnt 2.971277 hw_loss 0.000653 lr 0.00045436 rank 0
2023-02-21 12:48:24,371 DEBUG TRAIN Batch 13/18300 loss 8.945912 loss_att 12.830471 loss_ctc 19.605522 loss_rnnt 6.476715 hw_loss 0.508132 lr 0.00045436 rank 6
2023-02-21 12:48:24,386 DEBUG TRAIN Batch 13/18300 loss 5.035755 loss_att 8.577403 loss_ctc 6.161757 loss_rnnt 4.176943 hw_loss 0.000653 lr 0.00045436 rank 1
2023-02-21 12:48:24,388 DEBUG TRAIN Batch 13/18300 loss 6.754561 loss_att 8.309562 loss_ctc 6.282613 loss_rnnt 6.313046 hw_loss 0.362704 lr 0.00045436 rank 4
2023-02-21 12:48:24,395 DEBUG TRAIN Batch 13/18300 loss 22.454060 loss_att 27.263729 loss_ctc 25.986143 loss_rnnt 20.832371 hw_loss 0.354017 lr 0.00045436 rank 2
2023-02-21 12:48:24,430 DEBUG TRAIN Batch 13/18300 loss 12.839140 loss_att 17.241398 loss_ctc 18.246088 loss_rnnt 11.126585 hw_loss 0.208459 lr 0.00045436 rank 3
2023-02-21 12:49:21,362 DEBUG TRAIN Batch 13/18400 loss 12.346040 loss_att 16.591309 loss_ctc 15.158324 loss_rnnt 11.120068 hw_loss 0.003653 lr 0.00045426 rank 2
2023-02-21 12:49:21,364 DEBUG TRAIN Batch 13/18400 loss 27.420033 loss_att 28.745491 loss_ctc 27.633699 loss_rnnt 27.124504 hw_loss 0.003654 lr 0.00045426 rank 0
2023-02-21 12:49:21,370 DEBUG TRAIN Batch 13/18400 loss 20.776419 loss_att 22.343775 loss_ctc 28.159920 loss_rnnt 19.244333 hw_loss 0.439023 lr 0.00045426 rank 3
2023-02-21 12:49:21,371 DEBUG TRAIN Batch 13/18400 loss 8.113234 loss_att 10.451043 loss_ctc 8.439951 loss_rnnt 7.541051 hw_loss 0.114483 lr 0.00045426 rank 4
2023-02-21 12:49:21,379 DEBUG TRAIN Batch 13/18400 loss 17.460012 loss_att 18.537052 loss_ctc 24.441505 loss_rnnt 16.126980 hw_loss 0.350169 lr 0.00045426 rank 7
2023-02-21 12:49:21,384 DEBUG TRAIN Batch 13/18400 loss 7.640086 loss_att 8.436181 loss_ctc 10.142712 loss_rnnt 6.961931 hw_loss 0.347348 lr 0.00045426 rank 6
2023-02-21 12:49:21,391 DEBUG TRAIN Batch 13/18400 loss 12.301516 loss_att 11.704130 loss_ctc 19.282854 loss_rnnt 11.247946 hw_loss 0.454127 lr 0.00045426 rank 5
2023-02-21 12:49:21,407 DEBUG TRAIN Batch 13/18400 loss 12.342463 loss_att 12.870578 loss_ctc 13.284889 loss_rnnt 11.964913 hw_loss 0.274256 lr 0.00045426 rank 1
2023-02-21 12:50:18,412 DEBUG TRAIN Batch 13/18500 loss 16.912230 loss_att 20.532267 loss_ctc 23.780815 loss_rnnt 15.037894 hw_loss 0.439719 lr 0.00045417 rank 2
2023-02-21 12:50:18,418 DEBUG TRAIN Batch 13/18500 loss 3.679628 loss_att 6.926772 loss_ctc 2.901099 loss_rnnt 2.928767 hw_loss 0.384817 lr 0.00045417 rank 3
2023-02-21 12:50:18,421 DEBUG TRAIN Batch 13/18500 loss 34.443069 loss_att 38.911324 loss_ctc 34.029686 loss_rnnt 33.466507 hw_loss 0.258805 lr 0.00045417 rank 0
2023-02-21 12:50:18,423 DEBUG TRAIN Batch 13/18500 loss 19.791294 loss_att 23.248911 loss_ctc 32.053314 loss_rnnt 17.366203 hw_loss 0.184934 lr 0.00045417 rank 5
2023-02-21 12:50:18,424 DEBUG TRAIN Batch 13/18500 loss 3.418891 loss_att 6.547312 loss_ctc 4.800659 loss_rnnt 2.530594 hw_loss 0.146958 lr 0.00045417 rank 6
2023-02-21 12:50:18,425 DEBUG TRAIN Batch 13/18500 loss 19.906458 loss_att 22.863415 loss_ctc 30.865238 loss_rnnt 17.700123 hw_loss 0.288325 lr 0.00045417 rank 4
2023-02-21 12:50:18,432 DEBUG TRAIN Batch 13/18500 loss 22.251968 loss_att 24.279829 loss_ctc 22.516836 loss_rnnt 21.770905 hw_loss 0.075330 lr 0.00045417 rank 7
2023-02-21 12:50:18,441 DEBUG TRAIN Batch 13/18500 loss 7.229078 loss_att 7.742608 loss_ctc 6.073121 loss_rnnt 7.139579 hw_loss 0.264226 lr 0.00045417 rank 1
2023-02-21 12:51:16,331 DEBUG TRAIN Batch 13/18600 loss 32.122990 loss_att 34.978359 loss_ctc 35.653954 loss_rnnt 31.000801 hw_loss 0.150590 lr 0.00045407 rank 5
2023-02-21 12:51:16,332 DEBUG TRAIN Batch 13/18600 loss 2.926611 loss_att 4.631274 loss_ctc 7.093518 loss_rnnt 1.896942 hw_loss 0.249653 lr 0.00045407 rank 0
2023-02-21 12:51:16,333 DEBUG TRAIN Batch 13/18600 loss 25.252113 loss_att 31.890411 loss_ctc 32.015968 loss_rnnt 23.022562 hw_loss 0.000082 lr 0.00045407 rank 6
2023-02-21 12:51:16,337 DEBUG TRAIN Batch 13/18600 loss 4.739059 loss_att 10.416594 loss_ctc 9.622253 loss_rnnt 2.902634 hw_loss 0.093423 lr 0.00045407 rank 7
2023-02-21 12:51:16,341 DEBUG TRAIN Batch 13/18600 loss 20.997698 loss_att 25.392620 loss_ctc 26.895063 loss_rnnt 19.135500 hw_loss 0.369182 lr 0.00045407 rank 4
2023-02-21 12:51:16,343 DEBUG TRAIN Batch 13/18600 loss 4.034521 loss_att 6.516997 loss_ctc 5.557152 loss_rnnt 3.334964 hw_loss 0.000082 lr 0.00045407 rank 3
2023-02-21 12:51:16,344 DEBUG TRAIN Batch 13/18600 loss 9.992296 loss_att 13.646563 loss_ctc 16.836319 loss_rnnt 8.220268 hw_loss 0.241197 lr 0.00045407 rank 2
2023-02-21 12:51:16,344 DEBUG TRAIN Batch 13/18600 loss 18.844702 loss_att 18.263142 loss_ctc 23.955944 loss_rnnt 18.279472 hw_loss 0.000082 lr 0.00045407 rank 1
2023-02-21 12:52:13,109 DEBUG TRAIN Batch 13/18700 loss 10.912363 loss_att 12.101753 loss_ctc 13.038438 loss_rnnt 10.073463 hw_loss 0.595398 lr 0.00045398 rank 0
2023-02-21 12:52:13,113 DEBUG TRAIN Batch 13/18700 loss 14.699962 loss_att 15.520583 loss_ctc 18.900654 loss_rnnt 13.801613 hw_loss 0.326498 lr 0.00045398 rank 2
2023-02-21 12:52:13,114 DEBUG TRAIN Batch 13/18700 loss 10.588970 loss_att 14.659552 loss_ctc 21.054743 loss_rnnt 8.377604 hw_loss 0.003403 lr 0.00045398 rank 7
2023-02-21 12:52:13,116 DEBUG TRAIN Batch 13/18700 loss 13.991109 loss_att 14.730001 loss_ctc 17.039043 loss_rnnt 13.314016 hw_loss 0.230482 lr 0.00045398 rank 6
2023-02-21 12:52:13,116 DEBUG TRAIN Batch 13/18700 loss 17.507641 loss_att 23.962524 loss_ctc 24.002363 loss_rnnt 15.170868 hw_loss 0.337185 lr 0.00045398 rank 5
2023-02-21 12:52:13,120 DEBUG TRAIN Batch 13/18700 loss 18.981016 loss_att 19.108978 loss_ctc 23.323017 loss_rnnt 18.188980 hw_loss 0.351584 lr 0.00045398 rank 4
2023-02-21 12:52:13,129 DEBUG TRAIN Batch 13/18700 loss 18.929688 loss_att 23.741190 loss_ctc 26.684553 loss_rnnt 16.843155 hw_loss 0.169221 lr 0.00045398 rank 1
2023-02-21 12:52:13,184 DEBUG TRAIN Batch 13/18700 loss 11.423879 loss_att 13.651207 loss_ctc 16.291403 loss_rnnt 10.182301 hw_loss 0.275830 lr 0.00045398 rank 3
2023-02-21 12:53:11,463 DEBUG TRAIN Batch 13/18800 loss 15.523373 loss_att 17.731672 loss_ctc 16.513973 loss_rnnt 14.824917 hw_loss 0.233841 lr 0.00045389 rank 3
2023-02-21 12:53:11,464 DEBUG TRAIN Batch 13/18800 loss 10.476184 loss_att 12.641087 loss_ctc 11.849745 loss_rnnt 9.804751 hw_loss 0.103707 lr 0.00045389 rank 2
2023-02-21 12:53:11,465 DEBUG TRAIN Batch 13/18800 loss 15.769385 loss_att 18.045803 loss_ctc 15.650557 loss_rnnt 15.177343 hw_loss 0.286129 lr 0.00045389 rank 0
2023-02-21 12:53:11,466 DEBUG TRAIN Batch 13/18800 loss 27.211092 loss_att 24.067499 loss_ctc 34.337105 loss_rnnt 26.749317 hw_loss 0.263170 lr 0.00045389 rank 5
2023-02-21 12:53:11,467 DEBUG TRAIN Batch 13/18800 loss 2.817012 loss_att 6.870833 loss_ctc 2.916669 loss_rnnt 1.817199 hw_loss 0.329553 lr 0.00045389 rank 7
2023-02-21 12:53:11,473 DEBUG TRAIN Batch 13/18800 loss 1.167745 loss_att 3.312217 loss_ctc 1.812240 loss_rnnt 0.652819 hw_loss 0.000186 lr 0.00045389 rank 6
2023-02-21 12:53:11,478 DEBUG TRAIN Batch 13/18800 loss 11.498542 loss_att 14.335708 loss_ctc 13.541937 loss_rnnt 10.658557 hw_loss 0.000186 lr 0.00045389 rank 4
2023-02-21 12:53:11,479 DEBUG TRAIN Batch 13/18800 loss 17.617064 loss_att 19.932220 loss_ctc 13.940845 loss_rnnt 17.513470 hw_loss 0.245110 lr 0.00045389 rank 1
2023-02-21 12:54:09,768 DEBUG TRAIN Batch 13/18900 loss 7.444632 loss_att 9.709129 loss_ctc 11.027103 loss_rnnt 6.327635 hw_loss 0.349564 lr 0.00045379 rank 5
2023-02-21 12:54:09,769 DEBUG TRAIN Batch 13/18900 loss 10.420742 loss_att 20.144701 loss_ctc 15.661434 loss_rnnt 7.662488 hw_loss 0.215068 lr 0.00045379 rank 2
2023-02-21 12:54:09,773 DEBUG TRAIN Batch 13/18900 loss 8.765894 loss_att 14.090613 loss_ctc 11.258197 loss_rnnt 7.195203 hw_loss 0.325199 lr 0.00045379 rank 7
2023-02-21 12:54:09,777 DEBUG TRAIN Batch 13/18900 loss 6.307747 loss_att 12.742216 loss_ctc 10.785160 loss_rnnt 4.311959 hw_loss 0.209822 lr 0.00045379 rank 3
2023-02-21 12:54:09,781 DEBUG TRAIN Batch 13/18900 loss 9.183049 loss_att 14.306929 loss_ctc 5.900412 loss_rnnt 8.454403 hw_loss 0.265415 lr 0.00045379 rank 6
2023-02-21 12:54:09,784 DEBUG TRAIN Batch 13/18900 loss 39.714874 loss_att 37.766834 loss_ctc 44.913456 loss_rnnt 39.217979 hw_loss 0.362551 lr 0.00045379 rank 4
2023-02-21 12:54:09,786 DEBUG TRAIN Batch 13/18900 loss 20.712584 loss_att 23.217060 loss_ctc 21.010727 loss_rnnt 20.052860 hw_loss 0.223263 lr 0.00045379 rank 0
2023-02-21 12:54:09,789 DEBUG TRAIN Batch 13/18900 loss 1.335800 loss_att 4.727945 loss_ctc 0.327063 loss_rnnt 0.648333 hw_loss 0.269132 lr 0.00045379 rank 1
2023-02-21 12:56:03,153 DEBUG TRAIN Batch 13/19000 loss 17.782631 loss_att 18.462074 loss_ctc 30.906948 loss_rnnt 15.719318 hw_loss 0.332837 lr 0.00045370 rank 3
2023-02-21 12:56:03,153 DEBUG TRAIN Batch 13/19000 loss 15.854905 loss_att 16.459328 loss_ctc 24.069525 loss_rnnt 14.483829 hw_loss 0.290453 lr 0.00045370 rank 2
2023-02-21 12:56:03,154 DEBUG TRAIN Batch 13/19000 loss 20.493401 loss_att 19.417763 loss_ctc 28.380472 loss_rnnt 19.656900 hw_loss 0.000035 lr 0.00045370 rank 5
2023-02-21 12:56:03,160 DEBUG TRAIN Batch 13/19000 loss 14.323331 loss_att 15.775405 loss_ctc 14.557608 loss_rnnt 13.827331 hw_loss 0.326904 lr 0.00045370 rank 0
2023-02-21 12:56:03,164 DEBUG TRAIN Batch 13/19000 loss 2.849836 loss_att 4.337423 loss_ctc 4.045641 loss_rnnt 2.311044 hw_loss 0.153437 lr 0.00045370 rank 6
2023-02-21 12:56:03,166 DEBUG TRAIN Batch 13/19000 loss 12.620668 loss_att 13.526487 loss_ctc 13.075812 loss_rnnt 12.346934 hw_loss 0.059786 lr 0.00045370 rank 4
2023-02-21 12:56:03,167 DEBUG TRAIN Batch 13/19000 loss 14.310732 loss_att 13.718180 loss_ctc 15.565096 loss_rnnt 14.050360 hw_loss 0.396815 lr 0.00045370 rank 7
2023-02-21 12:56:03,168 DEBUG TRAIN Batch 13/19000 loss 10.315987 loss_att 10.429986 loss_ctc 13.523502 loss_rnnt 9.794375 hw_loss 0.133394 lr 0.00045370 rank 1
2023-02-21 12:57:57,276 DEBUG TRAIN Batch 13/19100 loss 15.004866 loss_att 16.753632 loss_ctc 21.383951 loss_rnnt 13.599148 hw_loss 0.385161 lr 0.00045361 rank 2
2023-02-21 12:57:57,283 DEBUG TRAIN Batch 13/19100 loss 10.368307 loss_att 17.337666 loss_ctc 14.906319 loss_rnnt 8.243091 hw_loss 0.236768 lr 0.00045361 rank 7
2023-02-21 12:57:57,283 DEBUG TRAIN Batch 13/19100 loss 36.629166 loss_att 42.376549 loss_ctc 35.173866 loss_rnnt 35.590061 hw_loss 0.156873 lr 0.00045361 rank 3
2023-02-21 12:57:57,284 DEBUG TRAIN Batch 13/19100 loss 13.219059 loss_att 13.542925 loss_ctc 14.383525 loss_rnnt 12.906017 hw_loss 0.174385 lr 0.00045361 rank 5
2023-02-21 12:57:57,285 DEBUG TRAIN Batch 13/19100 loss 1.824868 loss_att 5.724017 loss_ctc 2.721348 loss_rnnt 0.925031 hw_loss 0.000893 lr 0.00045361 rank 4
2023-02-21 12:57:57,289 DEBUG TRAIN Batch 13/19100 loss 1.499723 loss_att 4.809538 loss_ctc 3.179450 loss_rnnt 0.549488 hw_loss 0.120578 lr 0.00045361 rank 0
2023-02-21 12:57:57,293 DEBUG TRAIN Batch 13/19100 loss 13.680811 loss_att 19.671303 loss_ctc 16.760271 loss_rnnt 11.884295 hw_loss 0.352167 lr 0.00045361 rank 6
2023-02-21 12:57:57,353 DEBUG TRAIN Batch 13/19100 loss 17.002832 loss_att 24.481901 loss_ctc 19.855070 loss_rnnt 15.126244 hw_loss 0.000893 lr 0.00045361 rank 1
2023-02-21 12:58:54,556 DEBUG TRAIN Batch 13/19200 loss 5.495816 loss_att 8.489618 loss_ctc 11.190134 loss_rnnt 4.136787 hw_loss 0.001925 lr 0.00045351 rank 5
2023-02-21 12:58:54,559 DEBUG TRAIN Batch 13/19200 loss 23.521540 loss_att 25.845602 loss_ctc 26.238125 loss_rnnt 22.498497 hw_loss 0.367535 lr 0.00045351 rank 0
2023-02-21 12:58:54,559 DEBUG TRAIN Batch 13/19200 loss 12.180963 loss_att 16.584953 loss_ctc 16.271185 loss_rnnt 10.753775 hw_loss 0.001925 lr 0.00045351 rank 2
2023-02-21 12:58:54,559 DEBUG TRAIN Batch 13/19200 loss 12.422445 loss_att 19.899250 loss_ctc 17.246260 loss_rnnt 10.282881 hw_loss 0.001925 lr 0.00045351 rank 3
2023-02-21 12:58:54,561 DEBUG TRAIN Batch 13/19200 loss 15.583775 loss_att 19.973202 loss_ctc 18.899950 loss_rnnt 14.122751 hw_loss 0.264340 lr 0.00045351 rank 4
2023-02-21 12:58:54,568 DEBUG TRAIN Batch 13/19200 loss 2.146770 loss_att 3.464990 loss_ctc 2.191451 loss_rnnt 1.876142 hw_loss 0.001925 lr 0.00045351 rank 7
2023-02-21 12:58:54,569 DEBUG TRAIN Batch 13/19200 loss 20.832567 loss_att 19.559437 loss_ctc 23.611519 loss_rnnt 20.627947 hw_loss 0.166347 lr 0.00045351 rank 6
2023-02-21 12:58:54,578 DEBUG TRAIN Batch 13/19200 loss 5.070288 loss_att 8.640906 loss_ctc 5.317906 loss_rnnt 4.199967 hw_loss 0.230965 lr 0.00045351 rank 1
2023-02-21 12:59:51,191 DEBUG TRAIN Batch 13/19300 loss 19.628624 loss_att 21.389957 loss_ctc 24.166378 loss_rnnt 18.421383 hw_loss 0.468637 lr 0.00045342 rank 2
2023-02-21 12:59:51,200 DEBUG TRAIN Batch 13/19300 loss 15.143246 loss_att 16.218979 loss_ctc 17.047522 loss_rnnt 14.460208 hw_loss 0.401227 lr 0.00045342 rank 4
2023-02-21 12:59:51,200 DEBUG TRAIN Batch 13/19300 loss 17.873392 loss_att 21.493645 loss_ctc 25.339392 loss_rnnt 16.080446 hw_loss 0.137676 lr 0.00045342 rank 3
2023-02-21 12:59:51,202 DEBUG TRAIN Batch 13/19300 loss 15.215392 loss_att 15.793216 loss_ctc 18.398773 loss_rnnt 14.568962 hw_loss 0.199528 lr 0.00045342 rank 0
2023-02-21 12:59:51,203 DEBUG TRAIN Batch 13/19300 loss 3.206921 loss_att 6.669567 loss_ctc 6.171309 loss_rnnt 1.980572 hw_loss 0.259815 lr 0.00045342 rank 6
2023-02-21 12:59:51,206 DEBUG TRAIN Batch 13/19300 loss 10.094541 loss_att 11.360922 loss_ctc 14.404575 loss_rnnt 9.098120 hw_loss 0.315887 lr 0.00045342 rank 7
2023-02-21 12:59:51,208 DEBUG TRAIN Batch 13/19300 loss 4.266757 loss_att 12.883252 loss_ctc 4.975446 loss_rnnt 2.448757 hw_loss 0.000394 lr 0.00045342 rank 5
2023-02-21 12:59:51,219 DEBUG TRAIN Batch 13/19300 loss 8.726741 loss_att 13.430104 loss_ctc 12.667076 loss_rnnt 7.082302 hw_loss 0.334476 lr 0.00045342 rank 1
2023-02-21 13:00:50,243 DEBUG TRAIN Batch 13/19400 loss 18.924339 loss_att 22.068039 loss_ctc 29.647812 loss_rnnt 16.723839 hw_loss 0.266183 lr 0.00045333 rank 2
2023-02-21 13:00:50,245 DEBUG TRAIN Batch 13/19400 loss 10.523449 loss_att 14.616064 loss_ctc 15.975424 loss_rnnt 8.841418 hw_loss 0.256082 lr 0.00045333 rank 0
2023-02-21 13:00:50,246 DEBUG TRAIN Batch 13/19400 loss 3.712414 loss_att 6.994291 loss_ctc 5.751921 loss_rnnt 2.645576 hw_loss 0.259741 lr 0.00045333 rank 6
2023-02-21 13:00:50,246 DEBUG TRAIN Batch 13/19400 loss 18.662184 loss_att 20.647350 loss_ctc 21.870178 loss_rnnt 17.618666 hw_loss 0.410162 lr 0.00045333 rank 3
2023-02-21 13:00:50,252 DEBUG TRAIN Batch 13/19400 loss 3.255491 loss_att 4.575205 loss_ctc 4.853759 loss_rnnt 2.634610 hw_loss 0.269691 lr 0.00045333 rank 5
2023-02-21 13:00:50,255 DEBUG TRAIN Batch 13/19400 loss 26.224049 loss_att 32.093040 loss_ctc 35.126930 loss_rnnt 23.820904 hw_loss 0.079308 lr 0.00045333 rank 4
2023-02-21 13:00:50,255 DEBUG TRAIN Batch 13/19400 loss 4.539320 loss_att 8.839234 loss_ctc 7.607312 loss_rnnt 3.084350 hw_loss 0.348604 lr 0.00045333 rank 7
2023-02-21 13:00:50,259 DEBUG TRAIN Batch 13/19400 loss 40.731926 loss_att 42.794712 loss_ctc 43.549606 loss_rnnt 39.943508 hw_loss 0.000313 lr 0.00045333 rank 1
2023-02-21 13:01:47,940 DEBUG TRAIN Batch 13/19500 loss 7.348246 loss_att 10.939233 loss_ctc 8.182972 loss_rnnt 6.304293 hw_loss 0.402109 lr 0.00045323 rank 5
2023-02-21 13:01:47,940 DEBUG TRAIN Batch 13/19500 loss 4.114861 loss_att 5.951329 loss_ctc 6.328259 loss_rnnt 3.340209 hw_loss 0.210447 lr 0.00045323 rank 2
2023-02-21 13:01:47,945 DEBUG TRAIN Batch 13/19500 loss 5.057689 loss_att 10.212099 loss_ctc 8.558735 loss_rnnt 3.310740 hw_loss 0.467363 lr 0.00045323 rank 6
2023-02-21 13:01:47,947 DEBUG TRAIN Batch 13/19500 loss 19.597483 loss_att 19.287256 loss_ctc 26.726532 loss_rnnt 18.708935 hw_loss 0.000100 lr 0.00045323 rank 0
2023-02-21 13:01:47,948 DEBUG TRAIN Batch 13/19500 loss 3.953499 loss_att 6.993554 loss_ctc 5.222789 loss_rnnt 2.910427 hw_loss 0.498418 lr 0.00045323 rank 4
2023-02-21 13:01:47,949 DEBUG TRAIN Batch 13/19500 loss 7.411155 loss_att 10.950041 loss_ctc 11.970233 loss_rnnt 6.095448 hw_loss 0.000100 lr 0.00045323 rank 3
2023-02-21 13:01:47,949 DEBUG TRAIN Batch 13/19500 loss 2.714306 loss_att 7.569389 loss_ctc 4.232837 loss_rnnt 1.449790 hw_loss 0.170680 lr 0.00045323 rank 7
2023-02-21 13:01:47,963 DEBUG TRAIN Batch 13/19500 loss 2.111436 loss_att 5.499923 loss_ctc 2.070814 loss_rnnt 1.252385 hw_loss 0.350194 lr 0.00045323 rank 1
2023-02-21 13:02:44,789 DEBUG TRAIN Batch 13/19600 loss 9.114384 loss_att 15.087765 loss_ctc 11.704037 loss_rnnt 7.327946 hw_loss 0.462139 lr 0.00045314 rank 2
2023-02-21 13:02:44,799 DEBUG TRAIN Batch 13/19600 loss 18.311323 loss_att 17.788383 loss_ctc 21.923672 loss_rnnt 17.791714 hw_loss 0.267282 lr 0.00045314 rank 0
2023-02-21 13:02:44,800 DEBUG TRAIN Batch 13/19600 loss 7.559974 loss_att 8.388872 loss_ctc 7.798773 loss_rnnt 7.360455 hw_loss 0.003563 lr 0.00045314 rank 4
2023-02-21 13:02:44,802 DEBUG TRAIN Batch 13/19600 loss 14.250706 loss_att 18.642128 loss_ctc 13.923407 loss_rnnt 13.371045 hw_loss 0.084406 lr 0.00045314 rank 5
2023-02-21 13:02:44,803 DEBUG TRAIN Batch 13/19600 loss 12.203358 loss_att 11.677233 loss_ctc 16.851131 loss_rnnt 11.551947 hw_loss 0.256752 lr 0.00045314 rank 7
2023-02-21 13:02:44,810 DEBUG TRAIN Batch 13/19600 loss 4.388986 loss_att 7.721542 loss_ctc 5.708477 loss_rnnt 3.414611 hw_loss 0.247370 lr 0.00045314 rank 1
2023-02-21 13:02:44,811 DEBUG TRAIN Batch 13/19600 loss 12.949986 loss_att 12.510206 loss_ctc 15.460199 loss_rnnt 12.492887 hw_loss 0.394427 lr 0.00045314 rank 6
2023-02-21 13:02:44,814 DEBUG TRAIN Batch 13/19600 loss 8.219948 loss_att 10.402962 loss_ctc 7.909153 loss_rnnt 7.711611 hw_loss 0.212201 lr 0.00045314 rank 3
2023-02-21 13:03:44,167 DEBUG TRAIN Batch 13/19700 loss 12.382326 loss_att 15.559269 loss_ctc 16.954155 loss_rnnt 10.965857 hw_loss 0.321569 lr 0.00045305 rank 2
2023-02-21 13:03:44,172 DEBUG TRAIN Batch 13/19700 loss 3.494313 loss_att 7.602961 loss_ctc 6.041690 loss_rnnt 2.059303 hw_loss 0.513057 lr 0.00045305 rank 5
2023-02-21 13:03:44,176 DEBUG TRAIN Batch 13/19700 loss 6.718718 loss_att 7.475063 loss_ctc 8.644280 loss_rnnt 6.215866 hw_loss 0.177826 lr 0.00045305 rank 0
2023-02-21 13:03:44,177 DEBUG TRAIN Batch 13/19700 loss 5.455863 loss_att 8.118771 loss_ctc 7.305804 loss_rnnt 4.461880 hw_loss 0.402642 lr 0.00045305 rank 3
2023-02-21 13:03:44,180 DEBUG TRAIN Batch 13/19700 loss 17.751991 loss_att 20.047573 loss_ctc 21.588108 loss_rnnt 16.622786 hw_loss 0.297390 lr 0.00045305 rank 6
2023-02-21 13:03:44,184 DEBUG TRAIN Batch 13/19700 loss 4.732139 loss_att 7.967202 loss_ctc 6.242702 loss_rnnt 3.702065 hw_loss 0.340598 lr 0.00045305 rank 7
2023-02-21 13:03:44,185 DEBUG TRAIN Batch 13/19700 loss 4.084542 loss_att 4.043496 loss_ctc 3.075067 loss_rnnt 4.038393 hw_loss 0.354290 lr 0.00045305 rank 4
2023-02-21 13:03:44,190 DEBUG TRAIN Batch 13/19700 loss 27.984819 loss_att 31.931566 loss_ctc 26.484646 loss_rnnt 27.218718 hw_loss 0.331456 lr 0.00045305 rank 1
2023-02-21 13:04:41,885 DEBUG TRAIN Batch 13/19800 loss 10.798484 loss_att 11.853132 loss_ctc 10.278797 loss_rnnt 10.656780 hw_loss 0.000121 lr 0.00045296 rank 2
2023-02-21 13:04:41,889 DEBUG TRAIN Batch 13/19800 loss 11.025828 loss_att 18.924637 loss_ctc 15.494026 loss_rnnt 8.850243 hw_loss 0.000121 lr 0.00045296 rank 4
2023-02-21 13:04:41,891 DEBUG TRAIN Batch 13/19800 loss 15.476395 loss_att 19.227047 loss_ctc 21.031981 loss_rnnt 13.898659 hw_loss 0.162864 lr 0.00045296 rank 0
2023-02-21 13:04:41,891 DEBUG TRAIN Batch 13/19800 loss 5.820093 loss_att 6.632779 loss_ctc 7.670402 loss_rnnt 5.120510 hw_loss 0.544382 lr 0.00045296 rank 3
2023-02-21 13:04:41,894 DEBUG TRAIN Batch 13/19800 loss 12.948166 loss_att 16.562695 loss_ctc 14.694530 loss_rnnt 11.817860 hw_loss 0.327285 lr 0.00045296 rank 5
2023-02-21 13:04:41,898 DEBUG TRAIN Batch 13/19800 loss 1.436303 loss_att 5.715112 loss_ctc 1.112978 loss_rnnt 0.396787 hw_loss 0.425370 lr 0.00045296 rank 6
2023-02-21 13:04:41,900 DEBUG TRAIN Batch 13/19800 loss 8.618567 loss_att 10.737524 loss_ctc 17.459827 loss_rnnt 7.015876 hw_loss 0.000121 lr 0.00045296 rank 1
2023-02-21 13:04:41,963 DEBUG TRAIN Batch 13/19800 loss 13.430907 loss_att 15.525261 loss_ctc 19.716618 loss_rnnt 12.173878 hw_loss 0.000121 lr 0.00045296 rank 7
2023-02-21 13:05:39,714 DEBUG TRAIN Batch 13/19900 loss 16.315126 loss_att 17.207632 loss_ctc 15.868116 loss_rnnt 16.043770 hw_loss 0.285855 lr 0.00045286 rank 2
2023-02-21 13:05:39,719 DEBUG TRAIN Batch 13/19900 loss 13.204054 loss_att 16.421122 loss_ctc 14.270949 loss_rnnt 12.161104 hw_loss 0.482403 lr 0.00045286 rank 6
2023-02-21 13:05:39,719 DEBUG TRAIN Batch 13/19900 loss 25.002577 loss_att 27.431776 loss_ctc 40.541771 loss_rnnt 22.160595 hw_loss 0.532968 lr 0.00045286 rank 5
2023-02-21 13:05:39,720 DEBUG TRAIN Batch 13/19900 loss 2.235512 loss_att 6.582106 loss_ctc 2.015201 loss_rnnt 1.179046 hw_loss 0.405978 lr 0.00045286 rank 0
2023-02-21 13:05:39,721 DEBUG TRAIN Batch 13/19900 loss 5.677123 loss_att 8.335072 loss_ctc 7.142925 loss_rnnt 4.747088 hw_loss 0.380634 lr 0.00045286 rank 4
2023-02-21 13:05:39,723 DEBUG TRAIN Batch 13/19900 loss 10.294353 loss_att 10.220717 loss_ctc 12.366161 loss_rnnt 10.032810 hw_loss 0.000055 lr 0.00045286 rank 3
2023-02-21 13:05:39,725 DEBUG TRAIN Batch 13/19900 loss 9.057358 loss_att 8.758834 loss_ctc 9.972515 loss_rnnt 8.925169 hw_loss 0.131011 lr 0.00045286 rank 7
2023-02-21 13:05:39,730 DEBUG TRAIN Batch 13/19900 loss 25.264381 loss_att 28.763987 loss_ctc 35.910576 loss_rnnt 22.956814 hw_loss 0.352794 lr 0.00045286 rank 1
2023-02-21 13:06:38,412 DEBUG TRAIN Batch 13/20000 loss 9.340569 loss_att 15.599323 loss_ctc 10.405003 loss_rnnt 7.603171 hw_loss 0.644478 lr 0.00045277 rank 2
2023-02-21 13:06:38,413 DEBUG TRAIN Batch 13/20000 loss 26.536356 loss_att 30.576931 loss_ctc 38.381062 loss_rnnt 23.995457 hw_loss 0.287791 lr 0.00045277 rank 5
2023-02-21 13:06:38,416 DEBUG TRAIN Batch 13/20000 loss 9.722657 loss_att 15.650703 loss_ctc 14.366823 loss_rnnt 7.744436 hw_loss 0.325106 lr 0.00045277 rank 4
2023-02-21 13:06:38,418 DEBUG TRAIN Batch 13/20000 loss 6.608847 loss_att 16.104679 loss_ctc 9.327157 loss_rnnt 4.347152 hw_loss 0.000164 lr 0.00045277 rank 0
2023-02-21 13:06:38,420 DEBUG TRAIN Batch 13/20000 loss 20.782598 loss_att 20.375454 loss_ctc 24.185493 loss_rnnt 20.410221 hw_loss 0.000164 lr 0.00045277 rank 6
2023-02-21 13:06:38,421 DEBUG TRAIN Batch 13/20000 loss 27.456125 loss_att 33.127232 loss_ctc 36.204250 loss_rnnt 24.922659 hw_loss 0.436560 lr 0.00045277 rank 3
2023-02-21 13:06:38,421 DEBUG TRAIN Batch 13/20000 loss 14.774486 loss_att 15.268534 loss_ctc 17.891014 loss_rnnt 14.104883 hw_loss 0.291102 lr 0.00045277 rank 7
2023-02-21 13:06:38,424 DEBUG TRAIN Batch 13/20000 loss 18.637552 loss_att 14.759996 loss_ctc 15.566817 loss_rnnt 19.653240 hw_loss 0.317353 lr 0.00045277 rank 1
2023-02-21 13:07:34,710 DEBUG TRAIN Batch 13/20100 loss 25.114456 loss_att 20.560284 loss_ctc 38.312477 loss_rnnt 24.069357 hw_loss 0.367873 lr 0.00045268 rank 2
2023-02-21 13:07:34,710 DEBUG TRAIN Batch 13/20100 loss 13.315328 loss_att 12.449701 loss_ctc 16.580116 loss_rnnt 12.801270 hw_loss 0.472272 lr 0.00045268 rank 4
2023-02-21 13:07:34,716 DEBUG TRAIN Batch 13/20100 loss 15.504334 loss_att 17.486423 loss_ctc 23.142973 loss_rnnt 13.931578 hw_loss 0.295975 lr 0.00045268 rank 7
2023-02-21 13:07:34,716 DEBUG TRAIN Batch 13/20100 loss 18.055338 loss_att 18.617392 loss_ctc 20.212963 loss_rnnt 17.524075 hw_loss 0.245942 lr 0.00045268 rank 3
2023-02-21 13:07:34,717 DEBUG TRAIN Batch 13/20100 loss 12.637815 loss_att 13.047328 loss_ctc 19.066416 loss_rnnt 11.598556 hw_loss 0.187892 lr 0.00045268 rank 5
2023-02-21 13:07:34,719 DEBUG TRAIN Batch 13/20100 loss 19.001175 loss_att 17.598740 loss_ctc 20.875454 loss_rnnt 18.801741 hw_loss 0.431285 lr 0.00045268 rank 0
2023-02-21 13:07:34,726 DEBUG TRAIN Batch 13/20100 loss 10.829366 loss_att 12.554808 loss_ctc 17.534937 loss_rnnt 9.590101 hw_loss 0.000187 lr 0.00045268 rank 6
2023-02-21 13:07:34,737 DEBUG TRAIN Batch 13/20100 loss 31.456301 loss_att 40.566879 loss_ctc 43.149006 loss_rnnt 27.860628 hw_loss 0.402239 lr 0.00045268 rank 1
2023-02-21 13:08:32,092 DEBUG TRAIN Batch 13/20200 loss 5.199451 loss_att 7.476814 loss_ctc 9.471458 loss_rnnt 3.996874 hw_loss 0.332819 lr 0.00045258 rank 2
2023-02-21 13:08:32,099 DEBUG TRAIN Batch 13/20200 loss 1.708226 loss_att 4.649689 loss_ctc 3.442227 loss_rnnt 0.680499 hw_loss 0.390440 lr 0.00045258 rank 5
2023-02-21 13:08:32,100 DEBUG TRAIN Batch 13/20200 loss 1.707400 loss_att 5.158913 loss_ctc 3.364052 loss_rnnt 0.796172 hw_loss 0.000072 lr 0.00045258 rank 7
2023-02-21 13:08:32,102 DEBUG TRAIN Batch 13/20200 loss 20.838989 loss_att 19.369989 loss_ctc 29.311884 loss_rnnt 19.844141 hw_loss 0.297989 lr 0.00045258 rank 0
2023-02-21 13:08:32,104 DEBUG TRAIN Batch 13/20200 loss 3.588545 loss_att 5.476974 loss_ctc 3.773204 loss_rnnt 2.957668 hw_loss 0.428569 lr 0.00045258 rank 6
2023-02-21 13:09:23,933 DEBUG TRAIN Batch 13/20300 loss 30.715879 loss_att 37.445858 loss_ctc 36.779953 loss_rnnt 28.434383 hw_loss 0.238045 lr 0.00045249 rank 5
2023-02-21 13:09:42,268 DEBUG CV Batch 13/0 loss 2.669090 loss_att 2.733985 loss_ctc 3.692766 loss_rnnt 2.238449 hw_loss 0.527196 history loss 2.491151 rank 5
2023-02-21 13:09:42,274 DEBUG CV Batch 13/0 loss 2.669090 loss_att 2.733985 loss_ctc 3.692766 loss_rnnt 2.238449 hw_loss 0.527196 history loss 2.491151 rank 6
2023-02-21 13:09:42,275 DEBUG CV Batch 13/0 loss 2.669090 loss_att 2.733985 loss_ctc 3.692766 loss_rnnt 2.238449 hw_loss 0.527196 history loss 2.491151 rank 2
2023-02-21 13:09:42,275 DEBUG CV Batch 13/0 loss 2.669090 loss_att 2.733985 loss_ctc 3.692766 loss_rnnt 2.238449 hw_loss 0.527196 history loss 2.491151 rank 3
2023-02-21 13:09:42,280 DEBUG CV Batch 13/0 loss 2.669090 loss_att 2.733985 loss_ctc 3.692766 loss_rnnt 2.238449 hw_loss 0.527196 history loss 2.491151 rank 4
2023-02-21 13:09:42,285 DEBUG CV Batch 13/0 loss 2.669090 loss_att 2.733985 loss_ctc 3.692766 loss_rnnt 2.238449 hw_loss 0.527196 history loss 2.491151 rank 1
2023-02-21 13:09:42,287 DEBUG CV Batch 13/0 loss 2.669090 loss_att 2.733985 loss_ctc 3.692766 loss_rnnt 2.238449 hw_loss 0.527196 history loss 2.491151 rank 0
2023-02-21 13:09:42,297 DEBUG CV Batch 13/0 loss 2.669090 loss_att 2.733985 loss_ctc 3.692766 loss_rnnt 2.238449 hw_loss 0.527196 history loss 2.491151 rank 7
2023-02-21 13:09:52,674 DEBUG CV Batch 13/100 loss 8.125383 loss_att 12.860764 loss_ctc 10.078533 loss_rnnt 6.772601 hw_loss 0.272413 history loss 5.122343 rank 2
2023-02-21 13:09:52,694 DEBUG CV Batch 13/100 loss 8.125383 loss_att 12.860764 loss_ctc 10.078533 loss_rnnt 6.772601 hw_loss 0.272413 history loss 5.122343 rank 3
2023-02-21 13:09:52,704 DEBUG CV Batch 13/100 loss 8.125383 loss_att 12.860764 loss_ctc 10.078533 loss_rnnt 6.772601 hw_loss 0.272413 history loss 5.122343 rank 5
2023-02-21 13:09:52,746 DEBUG CV Batch 13/100 loss 8.125383 loss_att 12.860764 loss_ctc 10.078533 loss_rnnt 6.772601 hw_loss 0.272413 history loss 5.122343 rank 4
2023-02-21 13:09:52,766 DEBUG CV Batch 13/100 loss 8.125383 loss_att 12.860764 loss_ctc 10.078533 loss_rnnt 6.772601 hw_loss 0.272413 history loss 5.122343 rank 6
2023-02-21 13:09:52,858 DEBUG CV Batch 13/100 loss 8.125383 loss_att 12.860764 loss_ctc 10.078533 loss_rnnt 6.772601 hw_loss 0.272413 history loss 5.122343 rank 0
2023-02-21 13:09:53,104 DEBUG CV Batch 13/100 loss 8.125383 loss_att 12.860764 loss_ctc 10.078533 loss_rnnt 6.772601 hw_loss 0.272413 history loss 5.122343 rank 1
2023-02-21 13:09:53,198 DEBUG CV Batch 13/100 loss 8.125383 loss_att 12.860764 loss_ctc 10.078533 loss_rnnt 6.772601 hw_loss 0.272413 history loss 5.122343 rank 7
2023-02-21 13:10:05,232 DEBUG CV Batch 13/200 loss 4.426262 loss_att 4.788349 loss_ctc 4.237833 loss_rnnt 4.164824 hw_loss 0.401523 history loss 4.727362 rank 2
2023-02-21 13:10:05,241 DEBUG CV Batch 13/200 loss 4.426262 loss_att 4.788349 loss_ctc 4.237833 loss_rnnt 4.164824 hw_loss 0.401523 history loss 4.727362 rank 6
2023-02-21 13:10:05,241 DEBUG CV Batch 13/200 loss 4.426262 loss_att 4.788349 loss_ctc 4.237833 loss_rnnt 4.164824 hw_loss 0.401523 history loss 4.727362 rank 3
2023-02-21 13:10:05,264 DEBUG CV Batch 13/200 loss 4.426262 loss_att 4.788349 loss_ctc 4.237833 loss_rnnt 4.164824 hw_loss 0.401523 history loss 4.727362 rank 5
2023-02-21 13:10:05,391 DEBUG CV Batch 13/200 loss 4.426262 loss_att 4.788349 loss_ctc 4.237833 loss_rnnt 4.164824 hw_loss 0.401523 history loss 4.727362 rank 4
2023-02-21 13:10:05,480 DEBUG CV Batch 13/200 loss 4.426262 loss_att 4.788349 loss_ctc 4.237833 loss_rnnt 4.164824 hw_loss 0.401523 history loss 4.727362 rank 0
2023-02-21 13:10:05,937 DEBUG CV Batch 13/200 loss 4.426262 loss_att 4.788349 loss_ctc 4.237833 loss_rnnt 4.164824 hw_loss 0.401523 history loss 4.727362 rank 1
2023-02-21 13:10:06,460 DEBUG CV Batch 13/200 loss 4.426262 loss_att 4.788349 loss_ctc 4.237833 loss_rnnt 4.164824 hw_loss 0.401523 history loss 4.727362 rank 7
2023-02-21 13:10:18,717 DEBUG CV Batch 13/300 loss 5.437639 loss_att 4.768131 loss_ctc 7.084625 loss_rnnt 5.191494 hw_loss 0.300841 history loss 5.275623 rank 2
2023-02-21 13:10:18,808 DEBUG CV Batch 13/300 loss 5.437639 loss_att 4.768131 loss_ctc 7.084625 loss_rnnt 5.191494 hw_loss 0.300841 history loss 5.275623 rank 5
2023-02-21 13:10:18,970 DEBUG CV Batch 13/300 loss 5.437639 loss_att 4.768131 loss_ctc 7.084625 loss_rnnt 5.191494 hw_loss 0.300841 history loss 5.275623 rank 6
2023-02-21 13:10:18,983 DEBUG CV Batch 13/300 loss 5.437639 loss_att 4.768131 loss_ctc 7.084625 loss_rnnt 5.191494 hw_loss 0.300841 history loss 5.275623 rank 4
2023-02-21 13:10:19,045 DEBUG CV Batch 13/300 loss 5.437639 loss_att 4.768131 loss_ctc 7.084625 loss_rnnt 5.191494 hw_loss 0.300841 history loss 5.275623 rank 3
2023-02-21 13:10:19,107 DEBUG CV Batch 13/300 loss 5.437639 loss_att 4.768131 loss_ctc 7.084625 loss_rnnt 5.191494 hw_loss 0.300841 history loss 5.275623 rank 0
2023-02-21 13:10:19,746 DEBUG CV Batch 13/300 loss 5.437639 loss_att 4.768131 loss_ctc 7.084625 loss_rnnt 5.191494 hw_loss 0.300841 history loss 5.275623 rank 1
2023-02-21 13:10:20,283 DEBUG CV Batch 13/300 loss 5.437639 loss_att 4.768131 loss_ctc 7.084625 loss_rnnt 5.191494 hw_loss 0.300841 history loss 5.275623 rank 7
2023-02-21 13:10:28,958 DEBUG CV Batch 13/400 loss 16.831938 loss_att 16.447651 loss_ctc 24.514359 loss_rnnt 15.678408 hw_loss 0.386366 history loss 5.333664 rank 2
2023-02-21 13:10:29,268 DEBUG CV Batch 13/400 loss 16.831938 loss_att 16.447651 loss_ctc 24.514359 loss_rnnt 15.678408 hw_loss 0.386366 history loss 5.333664 rank 5
2023-02-21 13:10:29,568 DEBUG CV Batch 13/400 loss 16.831938 loss_att 16.447651 loss_ctc 24.514359 loss_rnnt 15.678408 hw_loss 0.386366 history loss 5.333664 rank 6
2023-02-21 13:10:29,606 DEBUG CV Batch 13/400 loss 16.831938 loss_att 16.447651 loss_ctc 24.514359 loss_rnnt 15.678408 hw_loss 0.386366 history loss 5.333664 rank 4
2023-02-21 13:10:29,650 DEBUG CV Batch 13/400 loss 16.831938 loss_att 16.447651 loss_ctc 24.514359 loss_rnnt 15.678408 hw_loss 0.386366 history loss 5.333664 rank 0
2023-02-21 13:10:29,651 DEBUG CV Batch 13/400 loss 16.831938 loss_att 16.447651 loss_ctc 24.514359 loss_rnnt 15.678408 hw_loss 0.386366 history loss 5.333664 rank 3
2023-02-21 13:10:30,367 DEBUG CV Batch 13/400 loss 16.831938 loss_att 16.447651 loss_ctc 24.514359 loss_rnnt 15.678408 hw_loss 0.386366 history loss 5.333664 rank 1
2023-02-21 13:10:30,906 DEBUG CV Batch 13/400 loss 16.831938 loss_att 16.447651 loss_ctc 24.514359 loss_rnnt 15.678408 hw_loss 0.386366 history loss 5.333664 rank 7
2023-02-21 13:10:44,228 DEBUG CV Batch 13/500 loss 4.411334 loss_att 5.447711 loss_ctc 5.210969 loss_rnnt 3.895954 hw_loss 0.377788 history loss 5.395923 rank 2
2023-02-21 13:10:44,913 DEBUG CV Batch 13/500 loss 4.411334 loss_att 5.447711 loss_ctc 5.210969 loss_rnnt 3.895954 hw_loss 0.377788 history loss 5.395923 rank 5
2023-02-21 13:10:45,075 DEBUG CV Batch 13/500 loss 4.411334 loss_att 5.447711 loss_ctc 5.210969 loss_rnnt 3.895954 hw_loss 0.377788 history loss 5.395923 rank 4
2023-02-21 13:10:45,256 DEBUG CV Batch 13/500 loss 4.411334 loss_att 5.447711 loss_ctc 5.210969 loss_rnnt 3.895954 hw_loss 0.377788 history loss 5.395923 rank 6
2023-02-21 13:10:45,371 DEBUG CV Batch 13/500 loss 4.411334 loss_att 5.447711 loss_ctc 5.210969 loss_rnnt 3.895954 hw_loss 0.377788 history loss 5.395923 rank 3
2023-02-21 13:10:45,471 DEBUG CV Batch 13/500 loss 4.411334 loss_att 5.447711 loss_ctc 5.210969 loss_rnnt 3.895954 hw_loss 0.377788 history loss 5.395923 rank 0
2023-02-21 13:10:46,182 DEBUG CV Batch 13/500 loss 4.411334 loss_att 5.447711 loss_ctc 5.210969 loss_rnnt 3.895954 hw_loss 0.377788 history loss 5.395923 rank 1
2023-02-21 13:10:46,939 DEBUG CV Batch 13/500 loss 4.411334 loss_att 5.447711 loss_ctc 5.210969 loss_rnnt 3.895954 hw_loss 0.377788 history loss 5.395923 rank 7
2023-02-21 13:10:57,077 DEBUG CV Batch 13/600 loss 13.956492 loss_att 34.648190 loss_ctc 12.809357 loss_rnnt 9.879251 hw_loss 0.172224 history loss 5.665416 rank 2
2023-02-21 13:10:57,896 DEBUG CV Batch 13/600 loss 13.956492 loss_att 34.648190 loss_ctc 12.809357 loss_rnnt 9.879251 hw_loss 0.172224 history loss 5.665416 rank 5
2023-02-21 13:10:58,033 DEBUG CV Batch 13/600 loss 13.956492 loss_att 34.648190 loss_ctc 12.809357 loss_rnnt 9.879251 hw_loss 0.172224 history loss 5.665416 rank 4
2023-02-21 13:10:58,223 DEBUG CV Batch 13/600 loss 13.956492 loss_att 34.648190 loss_ctc 12.809357 loss_rnnt 9.879251 hw_loss 0.172224 history loss 5.665416 rank 6
2023-02-21 13:10:58,287 DEBUG CV Batch 13/600 loss 13.956492 loss_att 34.648190 loss_ctc 12.809357 loss_rnnt 9.879251 hw_loss 0.172224 history loss 5.665416 rank 3
2023-02-21 13:10:58,668 DEBUG CV Batch 13/600 loss 13.956492 loss_att 34.648190 loss_ctc 12.809357 loss_rnnt 9.879251 hw_loss 0.172224 history loss 5.665416 rank 0
2023-02-21 13:10:59,352 DEBUG CV Batch 13/600 loss 13.956492 loss_att 34.648190 loss_ctc 12.809357 loss_rnnt 9.879251 hw_loss 0.172224 history loss 5.665416 rank 1
2023-02-21 13:11:00,220 DEBUG CV Batch 13/600 loss 13.956492 loss_att 34.648190 loss_ctc 12.809357 loss_rnnt 9.879251 hw_loss 0.172224 history loss 5.665416 rank 7
2023-02-21 13:11:08,035 DEBUG CV Batch 13/700 loss 27.713518 loss_att 21.738129 loss_ctc 34.943085 loss_rnnt 27.906588 hw_loss 0.071372 history loss 5.790939 rank 2
2023-02-21 13:11:09,214 DEBUG CV Batch 13/700 loss 27.713518 loss_att 21.738129 loss_ctc 34.943085 loss_rnnt 27.906588 hw_loss 0.071372 history loss 5.790939 rank 5
2023-02-21 13:11:09,474 DEBUG CV Batch 13/700 loss 27.713518 loss_att 21.738129 loss_ctc 34.943085 loss_rnnt 27.906588 hw_loss 0.071372 history loss 5.790939 rank 4
2023-02-21 13:11:09,503 DEBUG CV Batch 13/700 loss 27.713518 loss_att 21.738129 loss_ctc 34.943085 loss_rnnt 27.906588 hw_loss 0.071372 history loss 5.790939 rank 6
2023-02-21 13:11:09,664 DEBUG CV Batch 13/700 loss 27.713518 loss_att 21.738129 loss_ctc 34.943085 loss_rnnt 27.906588 hw_loss 0.071372 history loss 5.790939 rank 3
2023-02-21 13:11:10,295 DEBUG CV Batch 13/700 loss 27.713518 loss_att 21.738129 loss_ctc 34.943085 loss_rnnt 27.906588 hw_loss 0.071372 history loss 5.790939 rank 0
2023-02-21 13:11:10,762 DEBUG CV Batch 13/700 loss 27.713518 loss_att 21.738129 loss_ctc 34.943085 loss_rnnt 27.906588 hw_loss 0.071372 history loss 5.790939 rank 1
2023-02-21 13:11:11,978 DEBUG CV Batch 13/700 loss 27.713518 loss_att 21.738129 loss_ctc 34.943085 loss_rnnt 27.906588 hw_loss 0.071372 history loss 5.790939 rank 7
2023-02-21 13:11:19,299 DEBUG CV Batch 13/800 loss 6.377342 loss_att 5.476920 loss_ctc 11.729506 loss_rnnt 5.652316 hw_loss 0.359040 history loss 6.185510 rank 2
2023-02-21 13:11:20,579 DEBUG CV Batch 13/800 loss 6.377342 loss_att 5.476920 loss_ctc 11.729506 loss_rnnt 5.652316 hw_loss 0.359040 history loss 6.185510 rank 5
2023-02-21 13:11:21,011 DEBUG CV Batch 13/800 loss 6.377342 loss_att 5.476920 loss_ctc 11.729506 loss_rnnt 5.652316 hw_loss 0.359040 history loss 6.185510 rank 4
2023-02-21 13:11:21,143 DEBUG CV Batch 13/800 loss 6.377342 loss_att 5.476920 loss_ctc 11.729506 loss_rnnt 5.652316 hw_loss 0.359040 history loss 6.185510 rank 6
2023-02-21 13:11:21,251 DEBUG CV Batch 13/800 loss 6.377342 loss_att 5.476920 loss_ctc 11.729506 loss_rnnt 5.652316 hw_loss 0.359040 history loss 6.185510 rank 3
2023-02-21 13:11:21,769 DEBUG CV Batch 13/800 loss 6.377342 loss_att 5.476920 loss_ctc 11.729506 loss_rnnt 5.652316 hw_loss 0.359040 history loss 6.185510 rank 0
2023-02-21 13:11:22,526 DEBUG CV Batch 13/800 loss 6.377342 loss_att 5.476920 loss_ctc 11.729506 loss_rnnt 5.652316 hw_loss 0.359040 history loss 6.185510 rank 1
2023-02-21 13:11:23,587 DEBUG CV Batch 13/800 loss 6.377342 loss_att 5.476920 loss_ctc 11.729506 loss_rnnt 5.652316 hw_loss 0.359040 history loss 6.185510 rank 7
2023-02-21 13:11:31,511 DEBUG CV Batch 13/900 loss 9.601983 loss_att 9.735012 loss_ctc 12.533201 loss_rnnt 8.933502 hw_loss 0.470713 history loss 6.976779 rank 2
2023-02-21 13:11:33,184 DEBUG CV Batch 13/900 loss 9.601983 loss_att 9.735012 loss_ctc 12.533201 loss_rnnt 8.933502 hw_loss 0.470713 history loss 6.976779 rank 5
2023-02-21 13:11:33,459 DEBUG CV Batch 13/900 loss 9.601983 loss_att 9.735012 loss_ctc 12.533201 loss_rnnt 8.933502 hw_loss 0.470713 history loss 6.976779 rank 4
2023-02-21 13:11:33,657 DEBUG CV Batch 13/900 loss 9.601983 loss_att 9.735012 loss_ctc 12.533201 loss_rnnt 8.933502 hw_loss 0.470713 history loss 6.976779 rank 6
2023-02-21 13:11:33,658 DEBUG CV Batch 13/900 loss 9.601983 loss_att 9.735012 loss_ctc 12.533201 loss_rnnt 8.933502 hw_loss 0.470713 history loss 6.976779 rank 3
2023-02-21 13:11:34,506 DEBUG CV Batch 13/900 loss 9.601983 loss_att 9.735012 loss_ctc 12.533201 loss_rnnt 8.933502 hw_loss 0.470713 history loss 6.976779 rank 0
2023-02-21 13:11:35,046 DEBUG CV Batch 13/900 loss 9.601983 loss_att 9.735012 loss_ctc 12.533201 loss_rnnt 8.933502 hw_loss 0.470713 history loss 6.976779 rank 1
2023-02-21 13:11:36,187 DEBUG CV Batch 13/900 loss 9.601983 loss_att 9.735012 loss_ctc 12.533201 loss_rnnt 8.933502 hw_loss 0.470713 history loss 6.976779 rank 7
2023-02-21 13:11:42,264 DEBUG CV Batch 13/1000 loss 66.232086 loss_att 58.020882 loss_ctc 85.259216 loss_rnnt 65.337265 hw_loss 0.000207 history loss 7.912579 rank 2
2023-02-21 13:11:44,101 DEBUG CV Batch 13/1000 loss 66.232086 loss_att 58.020882 loss_ctc 85.259216 loss_rnnt 65.337265 hw_loss 0.000207 history loss 7.912579 rank 5
2023-02-21 13:11:44,590 DEBUG CV Batch 13/1000 loss 66.232086 loss_att 58.020882 loss_ctc 85.259216 loss_rnnt 65.337265 hw_loss 0.000207 history loss 7.912579 rank 6
2023-02-21 13:11:44,998 DEBUG CV Batch 13/1000 loss 66.232086 loss_att 58.020882 loss_ctc 85.259216 loss_rnnt 65.337265 hw_loss 0.000207 history loss 7.912579 rank 3
2023-02-21 13:11:45,517 DEBUG CV Batch 13/1000 loss 66.232086 loss_att 58.020882 loss_ctc 85.259216 loss_rnnt 65.337265 hw_loss 0.000207 history loss 7.912579 rank 0
2023-02-21 13:11:45,653 DEBUG CV Batch 13/1000 loss 66.232086 loss_att 58.020882 loss_ctc 85.259216 loss_rnnt 65.337265 hw_loss 0.000207 history loss 7.912579 rank 4
2023-02-21 13:11:45,997 DEBUG CV Batch 13/1000 loss 66.232086 loss_att 58.020882 loss_ctc 85.259216 loss_rnnt 65.337265 hw_loss 0.000207 history loss 7.912579 rank 1
2023-02-21 13:11:47,334 DEBUG CV Batch 13/1000 loss 66.232086 loss_att 58.020882 loss_ctc 85.259216 loss_rnnt 65.337265 hw_loss 0.000207 history loss 7.912579 rank 7
2023-02-21 13:11:53,274 DEBUG CV Batch 13/1100 loss 0.890013 loss_att 3.135740 loss_ctc 0.739425 loss_rnnt 0.227314 hw_loss 0.438059 history loss 7.832301 rank 2
2023-02-21 13:11:55,114 DEBUG CV Batch 13/1100 loss 0.890013 loss_att 3.135740 loss_ctc 0.739425 loss_rnnt 0.227314 hw_loss 0.438059 history loss 7.832301 rank 5
2023-02-21 13:11:55,567 DEBUG CV Batch 13/1100 loss 0.890013 loss_att 3.135740 loss_ctc 0.739425 loss_rnnt 0.227314 hw_loss 0.438059 history loss 7.832301 rank 6
2023-02-21 13:11:56,876 DEBUG CV Batch 13/1100 loss 0.890013 loss_att 3.135740 loss_ctc 0.739425 loss_rnnt 0.227314 hw_loss 0.438059 history loss 7.832301 rank 4
2023-02-21 13:11:57,006 DEBUG CV Batch 13/1100 loss 0.890013 loss_att 3.135740 loss_ctc 0.739425 loss_rnnt 0.227314 hw_loss 0.438059 history loss 7.832301 rank 0
2023-02-21 13:11:57,104 DEBUG CV Batch 13/1100 loss 0.890013 loss_att 3.135740 loss_ctc 0.739425 loss_rnnt 0.227314 hw_loss 0.438059 history loss 7.832301 rank 3
2023-02-21 13:11:57,384 DEBUG CV Batch 13/1100 loss 0.890013 loss_att 3.135740 loss_ctc 0.739425 loss_rnnt 0.227314 hw_loss 0.438059 history loss 7.832301 rank 1
2023-02-21 13:11:58,829 DEBUG CV Batch 13/1100 loss 0.890013 loss_att 3.135740 loss_ctc 0.739425 loss_rnnt 0.227314 hw_loss 0.438059 history loss 7.832301 rank 7
2023-02-21 13:12:05,169 DEBUG CV Batch 13/1200 loss 13.719561 loss_att 12.400213 loss_ctc 15.681845 loss_rnnt 13.529306 hw_loss 0.360909 history loss 8.207481 rank 2
2023-02-21 13:12:07,137 DEBUG CV Batch 13/1200 loss 13.719561 loss_att 12.400213 loss_ctc 15.681845 loss_rnnt 13.529306 hw_loss 0.360909 history loss 8.207481 rank 5
2023-02-21 13:12:07,752 DEBUG CV Batch 13/1200 loss 13.719561 loss_att 12.400213 loss_ctc 15.681845 loss_rnnt 13.529306 hw_loss 0.360909 history loss 8.207481 rank 6
2023-02-21 13:12:09,011 DEBUG CV Batch 13/1200 loss 13.719561 loss_att 12.400213 loss_ctc 15.681845 loss_rnnt 13.529306 hw_loss 0.360909 history loss 8.207481 rank 0
2023-02-21 13:12:09,019 DEBUG CV Batch 13/1200 loss 13.719561 loss_att 12.400213 loss_ctc 15.681845 loss_rnnt 13.529306 hw_loss 0.360909 history loss 8.207481 rank 4
2023-02-21 13:12:09,241 DEBUG CV Batch 13/1200 loss 13.719561 loss_att 12.400213 loss_ctc 15.681845 loss_rnnt 13.529306 hw_loss 0.360909 history loss 8.207481 rank 3
2023-02-21 13:12:09,675 DEBUG CV Batch 13/1200 loss 13.719561 loss_att 12.400213 loss_ctc 15.681845 loss_rnnt 13.529306 hw_loss 0.360909 history loss 8.207481 rank 1
2023-02-21 13:12:11,205 DEBUG CV Batch 13/1200 loss 13.719561 loss_att 12.400213 loss_ctc 15.681845 loss_rnnt 13.529306 hw_loss 0.360909 history loss 8.207481 rank 7
2023-02-21 13:12:18,451 DEBUG CV Batch 13/1300 loss 7.233016 loss_att 6.573739 loss_ctc 9.445662 loss_rnnt 6.873892 hw_loss 0.367422 history loss 8.844672 rank 2
2023-02-21 13:12:20,250 DEBUG CV Batch 13/1300 loss 7.233016 loss_att 6.573739 loss_ctc 9.445662 loss_rnnt 6.873892 hw_loss 0.367422 history loss 8.844672 rank 5
2023-02-21 13:12:20,967 DEBUG CV Batch 13/1300 loss 7.233016 loss_att 6.573739 loss_ctc 9.445662 loss_rnnt 6.873892 hw_loss 0.367422 history loss 8.844672 rank 6
2023-02-21 13:12:22,000 DEBUG CV Batch 13/1300 loss 7.233016 loss_att 6.573739 loss_ctc 9.445662 loss_rnnt 6.873892 hw_loss 0.367422 history loss 8.844672 rank 4
2023-02-21 13:12:22,030 DEBUG CV Batch 13/1300 loss 7.233016 loss_att 6.573739 loss_ctc 9.445662 loss_rnnt 6.873892 hw_loss 0.367422 history loss 8.844672 rank 0
2023-02-21 13:12:22,257 DEBUG CV Batch 13/1300 loss 7.233016 loss_att 6.573739 loss_ctc 9.445662 loss_rnnt 6.873892 hw_loss 0.367422 history loss 8.844672 rank 3
2023-02-21 13:12:23,000 DEBUG CV Batch 13/1300 loss 7.233016 loss_att 6.573739 loss_ctc 9.445662 loss_rnnt 6.873892 hw_loss 0.367422 history loss 8.844672 rank 1
2023-02-21 13:12:24,332 DEBUG CV Batch 13/1300 loss 7.233016 loss_att 6.573739 loss_ctc 9.445662 loss_rnnt 6.873892 hw_loss 0.367422 history loss 8.844672 rank 7
2023-02-21 13:12:29,169 DEBUG CV Batch 13/1400 loss 150.453217 loss_att 149.967453 loss_ctc 211.439896 loss_rnnt 142.305557 hw_loss 0.212319 history loss 9.408010 rank 2
2023-02-21 13:12:31,055 DEBUG CV Batch 13/1400 loss 150.453217 loss_att 149.967453 loss_ctc 211.439896 loss_rnnt 142.305557 hw_loss 0.212319 history loss 9.408010 rank 5
2023-02-21 13:12:31,894 DEBUG CV Batch 13/1400 loss 150.453217 loss_att 149.967453 loss_ctc 211.439896 loss_rnnt 142.305557 hw_loss 0.212319 history loss 9.408010 rank 6
2023-02-21 13:12:32,831 DEBUG CV Batch 13/1400 loss 150.453217 loss_att 149.967453 loss_ctc 211.439896 loss_rnnt 142.305557 hw_loss 0.212319 history loss 9.408010 rank 3
2023-02-21 13:12:32,868 DEBUG CV Batch 13/1400 loss 150.453217 loss_att 149.967453 loss_ctc 211.439896 loss_rnnt 142.305557 hw_loss 0.212319 history loss 9.408010 rank 4
2023-02-21 13:12:32,900 DEBUG CV Batch 13/1400 loss 150.453217 loss_att 149.967453 loss_ctc 211.439896 loss_rnnt 142.305557 hw_loss 0.212319 history loss 9.408010 rank 0
2023-02-21 13:12:34,105 DEBUG CV Batch 13/1400 loss 150.453217 loss_att 149.967453 loss_ctc 211.439896 loss_rnnt 142.305557 hw_loss 0.212319 history loss 9.408010 rank 1
2023-02-21 13:12:35,024 DEBUG CV Batch 13/1400 loss 150.453217 loss_att 149.967453 loss_ctc 211.439896 loss_rnnt 142.305557 hw_loss 0.212319 history loss 9.408010 rank 7
2023-02-21 13:12:39,099 DEBUG CV Batch 13/1500 loss 5.332277 loss_att 7.251776 loss_ctc 7.269988 loss_rnnt 4.525287 hw_loss 0.308865 history loss 9.605016 rank 2
2023-02-21 13:12:41,004 DEBUG CV Batch 13/1500 loss 5.332277 loss_att 7.251776 loss_ctc 7.269988 loss_rnnt 4.525287 hw_loss 0.308865 history loss 9.605016 rank 5
2023-02-21 13:12:42,270 DEBUG CV Batch 13/1500 loss 5.332277 loss_att 7.251776 loss_ctc 7.269988 loss_rnnt 4.525287 hw_loss 0.308865 history loss 9.605016 rank 6
2023-02-21 13:12:42,830 DEBUG CV Batch 13/1500 loss 5.332277 loss_att 7.251776 loss_ctc 7.269988 loss_rnnt 4.525287 hw_loss 0.308865 history loss 9.605016 rank 3
2023-02-21 13:12:43,027 DEBUG CV Batch 13/1500 loss 5.332277 loss_att 7.251776 loss_ctc 7.269988 loss_rnnt 4.525287 hw_loss 0.308865 history loss 9.605016 rank 4
2023-02-21 13:12:43,257 DEBUG CV Batch 13/1500 loss 5.332277 loss_att 7.251776 loss_ctc 7.269988 loss_rnnt 4.525287 hw_loss 0.308865 history loss 9.605016 rank 0
2023-02-21 13:12:44,409 DEBUG CV Batch 13/1500 loss 5.332277 loss_att 7.251776 loss_ctc 7.269988 loss_rnnt 4.525287 hw_loss 0.308865 history loss 9.605016 rank 1
2023-02-21 13:12:45,521 DEBUG CV Batch 13/1500 loss 5.332277 loss_att 7.251776 loss_ctc 7.269988 loss_rnnt 4.525287 hw_loss 0.308865 history loss 9.605016 rank 7
2023-02-21 13:12:51,120 DEBUG CV Batch 13/1600 loss 15.671362 loss_att 13.996126 loss_ctc 21.609409 loss_rnnt 15.063712 hw_loss 0.283047 history loss 9.352486 rank 2
2023-02-21 13:12:52,922 DEBUG CV Batch 13/1600 loss 15.671362 loss_att 13.996126 loss_ctc 21.609409 loss_rnnt 15.063712 hw_loss 0.283047 history loss 9.352486 rank 5
2023-02-21 13:12:54,215 DEBUG CV Batch 13/1600 loss 15.671362 loss_att 13.996126 loss_ctc 21.609409 loss_rnnt 15.063712 hw_loss 0.283047 history loss 9.352486 rank 6
2023-02-21 13:12:55,023 DEBUG CV Batch 13/1600 loss 15.671362 loss_att 13.996126 loss_ctc 21.609409 loss_rnnt 15.063712 hw_loss 0.283047 history loss 9.352486 rank 3
2023-02-21 13:12:55,264 DEBUG CV Batch 13/1600 loss 15.671362 loss_att 13.996126 loss_ctc 21.609409 loss_rnnt 15.063712 hw_loss 0.283047 history loss 9.352486 rank 4
2023-02-21 13:12:55,427 DEBUG CV Batch 13/1600 loss 15.671362 loss_att 13.996126 loss_ctc 21.609409 loss_rnnt 15.063712 hw_loss 0.283047 history loss 9.352486 rank 0
2023-02-21 13:12:56,678 DEBUG CV Batch 13/1600 loss 15.671362 loss_att 13.996126 loss_ctc 21.609409 loss_rnnt 15.063712 hw_loss 0.283047 history loss 9.352486 rank 1
2023-02-21 13:12:58,135 DEBUG CV Batch 13/1600 loss 15.671362 loss_att 13.996126 loss_ctc 21.609409 loss_rnnt 15.063712 hw_loss 0.283047 history loss 9.352486 rank 7
2023-02-21 13:13:04,002 DEBUG CV Batch 13/1700 loss 4.402485 loss_att 6.351445 loss_ctc 4.173708 loss_rnnt 3.817314 hw_loss 0.423529 history loss 9.113826 rank 2
2023-02-21 13:13:06,053 DEBUG CV Batch 13/1700 loss 4.402485 loss_att 6.351445 loss_ctc 4.173708 loss_rnnt 3.817314 hw_loss 0.423529 history loss 9.113826 rank 5
2023-02-21 13:13:07,211 DEBUG CV Batch 13/1700 loss 4.402485 loss_att 6.351445 loss_ctc 4.173708 loss_rnnt 3.817314 hw_loss 0.423529 history loss 9.113826 rank 6
2023-02-21 13:13:08,269 DEBUG CV Batch 13/1700 loss 4.402485 loss_att 6.351445 loss_ctc 4.173708 loss_rnnt 3.817314 hw_loss 0.423529 history loss 9.113826 rank 4
2023-02-21 13:13:08,437 DEBUG CV Batch 13/1700 loss 4.402485 loss_att 6.351445 loss_ctc 4.173708 loss_rnnt 3.817314 hw_loss 0.423529 history loss 9.113826 rank 0
2023-02-21 13:13:09,047 DEBUG CV Batch 13/1700 loss 4.402485 loss_att 6.351445 loss_ctc 4.173708 loss_rnnt 3.817314 hw_loss 0.423529 history loss 9.113826 rank 3
2023-02-21 13:13:09,667 DEBUG CV Batch 13/1700 loss 4.402485 loss_att 6.351445 loss_ctc 4.173708 loss_rnnt 3.817314 hw_loss 0.423529 history loss 9.113826 rank 1
2023-02-21 13:13:11,373 DEBUG CV Batch 13/1700 loss 4.402485 loss_att 6.351445 loss_ctc 4.173708 loss_rnnt 3.817314 hw_loss 0.423529 history loss 9.113826 rank 7
2023-02-21 13:13:15,405 DEBUG CV Batch 13/1800 loss 17.812923 loss_att 24.172558 loss_ctc 31.637640 loss_rnnt 14.697592 hw_loss 0.000207 history loss 9.032298 rank 2
2023-02-21 13:13:17,888 DEBUG CV Batch 13/1800 loss 17.812923 loss_att 24.172558 loss_ctc 31.637640 loss_rnnt 14.697592 hw_loss 0.000207 history loss 9.032298 rank 5
2023-02-21 13:13:18,911 DEBUG CV Batch 13/1800 loss 17.812923 loss_att 24.172558 loss_ctc 31.637640 loss_rnnt 14.697592 hw_loss 0.000207 history loss 9.032298 rank 6
2023-02-21 13:13:20,113 DEBUG CV Batch 13/1800 loss 17.812923 loss_att 24.172558 loss_ctc 31.637640 loss_rnnt 14.697592 hw_loss 0.000207 history loss 9.032298 rank 4
2023-02-21 13:13:20,396 DEBUG CV Batch 13/1800 loss 17.812923 loss_att 24.172558 loss_ctc 31.637640 loss_rnnt 14.697592 hw_loss 0.000207 history loss 9.032298 rank 0
2023-02-21 13:13:21,423 DEBUG CV Batch 13/1800 loss 17.812923 loss_att 24.172558 loss_ctc 31.637640 loss_rnnt 14.697592 hw_loss 0.000207 history loss 9.032298 rank 1
2023-02-21 13:13:21,838 DEBUG CV Batch 13/1800 loss 17.812923 loss_att 24.172558 loss_ctc 31.637640 loss_rnnt 14.697592 hw_loss 0.000207 history loss 9.032298 rank 3
2023-02-21 13:13:23,448 DEBUG CV Batch 13/1800 loss 17.812923 loss_att 24.172558 loss_ctc 31.637640 loss_rnnt 14.697592 hw_loss 0.000207 history loss 9.032298 rank 7
2023-02-21 13:13:26,593 DEBUG CV Batch 13/1900 loss 6.127034 loss_att 4.889955 loss_ctc 7.028710 loss_rnnt 6.089957 hw_loss 0.308005 history loss 8.769159 rank 2
2023-02-21 13:13:29,284 DEBUG CV Batch 13/1900 loss 6.127034 loss_att 4.889955 loss_ctc 7.028710 loss_rnnt 6.089957 hw_loss 0.308005 history loss 8.769159 rank 5
2023-02-21 13:13:30,199 DEBUG CV Batch 13/1900 loss 6.127034 loss_att 4.889955 loss_ctc 7.028710 loss_rnnt 6.089957 hw_loss 0.308005 history loss 8.769159 rank 6
2023-02-21 13:13:31,651 DEBUG CV Batch 13/1900 loss 6.127034 loss_att 4.889955 loss_ctc 7.028710 loss_rnnt 6.089957 hw_loss 0.308005 history loss 8.769159 rank 4
2023-02-21 13:13:32,047 DEBUG CV Batch 13/1900 loss 6.127034 loss_att 4.889955 loss_ctc 7.028710 loss_rnnt 6.089957 hw_loss 0.308005 history loss 8.769159 rank 0
2023-02-21 13:13:32,969 DEBUG CV Batch 13/1900 loss 6.127034 loss_att 4.889955 loss_ctc 7.028710 loss_rnnt 6.089957 hw_loss 0.308005 history loss 8.769159 rank 1
2023-02-21 13:13:33,026 DEBUG CV Batch 13/1900 loss 6.127034 loss_att 4.889955 loss_ctc 7.028710 loss_rnnt 6.089957 hw_loss 0.308005 history loss 8.769159 rank 3
2023-02-21 13:13:35,398 DEBUG CV Batch 13/1900 loss 6.127034 loss_att 4.889955 loss_ctc 7.028710 loss_rnnt 6.089957 hw_loss 0.308005 history loss 8.769159 rank 7
2023-02-21 13:13:42,275 DEBUG CV Batch 13/2000 loss 10.247582 loss_att 8.743807 loss_ctc 12.308636 loss_rnnt 10.077542 hw_loss 0.367478 history loss 8.660908 rank 2
2023-02-21 13:13:45,350 DEBUG CV Batch 13/2000 loss 10.247582 loss_att 8.743807 loss_ctc 12.308636 loss_rnnt 10.077542 hw_loss 0.367478 history loss 8.660908 rank 5
2023-02-21 13:13:46,070 DEBUG CV Batch 13/2000 loss 10.247582 loss_att 8.743807 loss_ctc 12.308636 loss_rnnt 10.077542 hw_loss 0.367478 history loss 8.660908 rank 6
2023-02-21 13:13:47,546 DEBUG CV Batch 13/2000 loss 10.247582 loss_att 8.743807 loss_ctc 12.308636 loss_rnnt 10.077542 hw_loss 0.367478 history loss 8.660908 rank 4
2023-02-21 13:13:48,112 DEBUG CV Batch 13/2000 loss 10.247582 loss_att 8.743807 loss_ctc 12.308636 loss_rnnt 10.077542 hw_loss 0.367478 history loss 8.660908 rank 0
2023-02-21 13:13:48,729 DEBUG CV Batch 13/2000 loss 10.247582 loss_att 8.743807 loss_ctc 12.308636 loss_rnnt 10.077542 hw_loss 0.367478 history loss 8.660908 rank 1
2023-02-21 13:13:48,830 DEBUG CV Batch 13/2000 loss 10.247582 loss_att 8.743807 loss_ctc 12.308636 loss_rnnt 10.077542 hw_loss 0.367478 history loss 8.660908 rank 3
2023-02-21 13:13:51,492 DEBUG CV Batch 13/2000 loss 10.247582 loss_att 8.743807 loss_ctc 12.308636 loss_rnnt 10.077542 hw_loss 0.367478 history loss 8.660908 rank 7
2023-02-21 13:13:52,722 DEBUG CV Batch 13/2100 loss 3.327566 loss_att 9.471648 loss_ctc 5.330823 loss_rnnt 1.656162 hw_loss 0.329039 history loss 8.607035 rank 2
2023-02-21 13:13:56,274 DEBUG CV Batch 13/2100 loss 3.327566 loss_att 9.471648 loss_ctc 5.330823 loss_rnnt 1.656162 hw_loss 0.329039 history loss 8.607035 rank 5
2023-02-21 13:13:57,165 DEBUG CV Batch 13/2100 loss 3.327566 loss_att 9.471648 loss_ctc 5.330823 loss_rnnt 1.656162 hw_loss 0.329039 history loss 8.607035 rank 6
2023-02-21 13:13:58,361 DEBUG CV Batch 13/2100 loss 3.327566 loss_att 9.471648 loss_ctc 5.330823 loss_rnnt 1.656162 hw_loss 0.329039 history loss 8.607035 rank 4
2023-02-21 13:13:59,250 DEBUG CV Batch 13/2100 loss 3.327566 loss_att 9.471648 loss_ctc 5.330823 loss_rnnt 1.656162 hw_loss 0.329039 history loss 8.607035 rank 0
2023-02-21 13:13:59,607 DEBUG CV Batch 13/2100 loss 3.327566 loss_att 9.471648 loss_ctc 5.330823 loss_rnnt 1.656162 hw_loss 0.329039 history loss 8.607035 rank 1
2023-02-21 13:13:59,708 DEBUG CV Batch 13/2100 loss 3.327566 loss_att 9.471648 loss_ctc 5.330823 loss_rnnt 1.656162 hw_loss 0.329039 history loss 8.607035 rank 3
2023-02-21 13:14:02,593 DEBUG CV Batch 13/2100 loss 3.327566 loss_att 9.471648 loss_ctc 5.330823 loss_rnnt 1.656162 hw_loss 0.329039 history loss 8.607035 rank 7
2023-02-21 13:14:05,728 DEBUG CV Batch 13/2200 loss 2.782408 loss_att 5.275426 loss_ctc 6.390052 loss_rnnt 1.412763 hw_loss 0.731292 history loss 8.481074 rank 2
2023-02-21 13:14:09,208 DEBUG CV Batch 13/2200 loss 2.782408 loss_att 5.275426 loss_ctc 6.390052 loss_rnnt 1.412763 hw_loss 0.731292 history loss 8.481074 rank 5
2023-02-21 13:14:10,446 DEBUG CV Batch 13/2200 loss 2.782408 loss_att 5.275426 loss_ctc 6.390052 loss_rnnt 1.412763 hw_loss 0.731292 history loss 8.481074 rank 6
2023-02-21 13:14:11,691 DEBUG CV Batch 13/2200 loss 2.782408 loss_att 5.275426 loss_ctc 6.390052 loss_rnnt 1.412763 hw_loss 0.731292 history loss 8.481074 rank 4
2023-02-21 13:14:12,693 DEBUG CV Batch 13/2200 loss 2.782408 loss_att 5.275426 loss_ctc 6.390052 loss_rnnt 1.412763 hw_loss 0.731292 history loss 8.481074 rank 0
2023-02-21 13:14:12,746 DEBUG CV Batch 13/2200 loss 2.782408 loss_att 5.275426 loss_ctc 6.390052 loss_rnnt 1.412763 hw_loss 0.731292 history loss 8.481074 rank 1
2023-02-21 13:14:12,875 DEBUG CV Batch 13/2200 loss 2.782408 loss_att 5.275426 loss_ctc 6.390052 loss_rnnt 1.412763 hw_loss 0.731292 history loss 8.481074 rank 3
2023-02-21 13:14:16,071 DEBUG CV Batch 13/2200 loss 2.782408 loss_att 5.275426 loss_ctc 6.390052 loss_rnnt 1.412763 hw_loss 0.731292 history loss 8.481074 rank 7
2023-02-21 13:14:17,379 DEBUG CV Batch 13/2300 loss 2.578443 loss_att 3.012790 loss_ctc 2.231318 loss_rnnt 2.299873 hw_loss 0.446220 history loss 8.402008 rank 2
2023-02-21 13:14:20,818 DEBUG CV Batch 13/2300 loss 2.578443 loss_att 3.012790 loss_ctc 2.231318 loss_rnnt 2.299873 hw_loss 0.446220 history loss 8.402008 rank 5
2023-02-21 13:14:22,218 DEBUG CV Batch 13/2300 loss 2.578443 loss_att 3.012790 loss_ctc 2.231318 loss_rnnt 2.299873 hw_loss 0.446220 history loss 8.402008 rank 6
2023-02-21 13:14:23,745 DEBUG CV Batch 13/2300 loss 2.578443 loss_att 3.012790 loss_ctc 2.231318 loss_rnnt 2.299873 hw_loss 0.446220 history loss 8.402008 rank 4
2023-02-21 13:14:24,414 DEBUG CV Batch 13/2300 loss 2.578443 loss_att 3.012790 loss_ctc 2.231318 loss_rnnt 2.299873 hw_loss 0.446220 history loss 8.402008 rank 3
2023-02-21 13:14:24,553 DEBUG CV Batch 13/2300 loss 2.578443 loss_att 3.012790 loss_ctc 2.231318 loss_rnnt 2.299873 hw_loss 0.446220 history loss 8.402008 rank 0
2023-02-21 13:14:24,710 DEBUG CV Batch 13/2300 loss 2.578443 loss_att 3.012790 loss_ctc 2.231318 loss_rnnt 2.299873 hw_loss 0.446220 history loss 8.402008 rank 1
2023-02-21 13:14:27,985 DEBUG CV Batch 13/2300 loss 2.578443 loss_att 3.012790 loss_ctc 2.231318 loss_rnnt 2.299873 hw_loss 0.446220 history loss 8.402008 rank 7
2023-02-21 13:14:29,458 DEBUG CV Batch 13/2400 loss 6.501006 loss_att 6.022370 loss_ctc 9.533541 loss_rnnt 5.881794 hw_loss 0.582377 history loss 8.486528 rank 2
2023-02-21 13:14:33,113 DEBUG CV Batch 13/2400 loss 6.501006 loss_att 6.022370 loss_ctc 9.533541 loss_rnnt 5.881794 hw_loss 0.582377 history loss 8.486528 rank 5
2023-02-21 13:14:34,624 DEBUG CV Batch 13/2400 loss 6.501006 loss_att 6.022370 loss_ctc 9.533541 loss_rnnt 5.881794 hw_loss 0.582377 history loss 8.486528 rank 6
2023-02-21 13:14:36,086 DEBUG CV Batch 13/2400 loss 6.501006 loss_att 6.022370 loss_ctc 9.533541 loss_rnnt 5.881794 hw_loss 0.582377 history loss 8.486528 rank 4
2023-02-21 13:14:36,675 DEBUG CV Batch 13/2400 loss 6.501006 loss_att 6.022370 loss_ctc 9.533541 loss_rnnt 5.881794 hw_loss 0.582377 history loss 8.486528 rank 3
2023-02-21 13:14:36,915 DEBUG CV Batch 13/2400 loss 6.501006 loss_att 6.022370 loss_ctc 9.533541 loss_rnnt 5.881794 hw_loss 0.582377 history loss 8.486528 rank 0
2023-02-21 13:14:37,157 DEBUG CV Batch 13/2400 loss 6.501006 loss_att 6.022370 loss_ctc 9.533541 loss_rnnt 5.881794 hw_loss 0.582377 history loss 8.486528 rank 1
2023-02-21 13:14:39,369 DEBUG CV Batch 13/2500 loss 31.142580 loss_att 33.304157 loss_ctc 51.714634 loss_rnnt 27.967216 hw_loss 0.000207 history loss 8.859119 rank 2
2023-02-21 13:14:40,492 DEBUG CV Batch 13/2400 loss 6.501006 loss_att 6.022370 loss_ctc 9.533541 loss_rnnt 5.881794 hw_loss 0.582377 history loss 8.486528 rank 7
2023-02-21 13:14:43,211 DEBUG CV Batch 13/2500 loss 31.142580 loss_att 33.304157 loss_ctc 51.714634 loss_rnnt 27.967216 hw_loss 0.000207 history loss 8.859119 rank 5
2023-02-21 13:14:44,851 DEBUG CV Batch 13/2500 loss 31.142580 loss_att 33.304157 loss_ctc 51.714634 loss_rnnt 27.967216 hw_loss 0.000207 history loss 8.859119 rank 6
2023-02-21 13:14:46,400 DEBUG CV Batch 13/2500 loss 31.142580 loss_att 33.304157 loss_ctc 51.714634 loss_rnnt 27.967216 hw_loss 0.000207 history loss 8.859119 rank 4
2023-02-21 13:14:47,394 DEBUG CV Batch 13/2500 loss 31.142580 loss_att 33.304157 loss_ctc 51.714634 loss_rnnt 27.967216 hw_loss 0.000207 history loss 8.859119 rank 0
2023-02-21 13:14:47,416 DEBUG CV Batch 13/2500 loss 31.142580 loss_att 33.304157 loss_ctc 51.714634 loss_rnnt 27.967216 hw_loss 0.000207 history loss 8.859119 rank 1
2023-02-21 13:14:47,487 DEBUG CV Batch 13/2500 loss 31.142580 loss_att 33.304157 loss_ctc 51.714634 loss_rnnt 27.967216 hw_loss 0.000207 history loss 8.859119 rank 3
2023-02-21 13:14:50,654 DEBUG CV Batch 13/2500 loss 31.142580 loss_att 33.304157 loss_ctc 51.714634 loss_rnnt 27.967216 hw_loss 0.000207 history loss 8.859119 rank 7
2023-02-21 13:14:50,824 DEBUG CV Batch 13/2600 loss 2.334250 loss_att 4.990288 loss_ctc 3.854910 loss_rnnt 1.347990 hw_loss 0.473060 history loss 8.900402 rank 2
2023-02-21 13:14:54,585 DEBUG CV Batch 13/2600 loss 2.334250 loss_att 4.990288 loss_ctc 3.854910 loss_rnnt 1.347990 hw_loss 0.473060 history loss 8.900402 rank 5
2023-02-21 13:14:56,066 DEBUG CV Batch 13/2600 loss 2.334250 loss_att 4.990288 loss_ctc 3.854910 loss_rnnt 1.347990 hw_loss 0.473060 history loss 8.900402 rank 6
2023-02-21 13:14:57,684 DEBUG CV Batch 13/2600 loss 2.334250 loss_att 4.990288 loss_ctc 3.854910 loss_rnnt 1.347990 hw_loss 0.473060 history loss 8.900402 rank 4
2023-02-21 13:14:58,810 DEBUG CV Batch 13/2600 loss 2.334250 loss_att 4.990288 loss_ctc 3.854910 loss_rnnt 1.347990 hw_loss 0.473060 history loss 8.900402 rank 0
2023-02-21 13:14:58,825 DEBUG CV Batch 13/2600 loss 2.334250 loss_att 4.990288 loss_ctc 3.854910 loss_rnnt 1.347990 hw_loss 0.473060 history loss 8.900402 rank 3
2023-02-21 13:14:58,885 DEBUG CV Batch 13/2600 loss 2.334250 loss_att 4.990288 loss_ctc 3.854910 loss_rnnt 1.347990 hw_loss 0.473060 history loss 8.900402 rank 1
2023-02-21 13:15:02,278 DEBUG CV Batch 13/2600 loss 2.334250 loss_att 4.990288 loss_ctc 3.854910 loss_rnnt 1.347990 hw_loss 0.473060 history loss 8.900402 rank 7
2023-02-21 13:15:03,464 DEBUG CV Batch 13/2700 loss 16.093931 loss_att 13.880821 loss_ctc 20.665567 loss_rnnt 15.685184 hw_loss 0.453411 history loss 8.940057 rank 2
2023-02-21 13:15:07,215 DEBUG CV Batch 13/2700 loss 16.093931 loss_att 13.880821 loss_ctc 20.665567 loss_rnnt 15.685184 hw_loss 0.453411 history loss 8.940057 rank 5
2023-02-21 13:15:08,710 DEBUG CV Batch 13/2700 loss 16.093931 loss_att 13.880821 loss_ctc 20.665567 loss_rnnt 15.685184 hw_loss 0.453411 history loss 8.940057 rank 6
2023-02-21 13:15:10,564 DEBUG CV Batch 13/2700 loss 16.093931 loss_att 13.880821 loss_ctc 20.665567 loss_rnnt 15.685184 hw_loss 0.453411 history loss 8.940057 rank 4
2023-02-21 13:15:11,346 DEBUG CV Batch 13/2700 loss 16.093931 loss_att 13.880821 loss_ctc 20.665567 loss_rnnt 15.685184 hw_loss 0.453411 history loss 8.940057 rank 3
2023-02-21 13:15:11,510 DEBUG CV Batch 13/2700 loss 16.093931 loss_att 13.880821 loss_ctc 20.665567 loss_rnnt 15.685184 hw_loss 0.453411 history loss 8.940057 rank 0
2023-02-21 13:15:11,575 DEBUG CV Batch 13/2700 loss 16.093931 loss_att 13.880821 loss_ctc 20.665567 loss_rnnt 15.685184 hw_loss 0.453411 history loss 8.940057 rank 1
2023-02-21 13:15:14,195 DEBUG CV Batch 13/2800 loss 15.167926 loss_att 12.646801 loss_ctc 10.816082 loss_rnnt 16.095362 hw_loss 0.294439 history loss 9.178802 rank 2
2023-02-21 13:15:15,135 DEBUG CV Batch 13/2700 loss 16.093931 loss_att 13.880821 loss_ctc 20.665567 loss_rnnt 15.685184 hw_loss 0.453411 history loss 8.940057 rank 7
2023-02-21 13:15:17,532 DEBUG CV Batch 13/2800 loss 15.167926 loss_att 12.646801 loss_ctc 10.816082 loss_rnnt 16.095362 hw_loss 0.294439 history loss 9.178802 rank 5
2023-02-21 13:15:19,181 DEBUG CV Batch 13/2800 loss 15.167926 loss_att 12.646801 loss_ctc 10.816082 loss_rnnt 16.095362 hw_loss 0.294439 history loss 9.178802 rank 6
2023-02-21 13:15:21,220 DEBUG CV Batch 13/2800 loss 15.167926 loss_att 12.646801 loss_ctc 10.816082 loss_rnnt 16.095362 hw_loss 0.294439 history loss 9.178802 rank 4
2023-02-21 13:15:21,816 DEBUG CV Batch 13/2800 loss 15.167926 loss_att 12.646801 loss_ctc 10.816082 loss_rnnt 16.095362 hw_loss 0.294439 history loss 9.178802 rank 3
2023-02-21 13:15:22,035 DEBUG CV Batch 13/2800 loss 15.167926 loss_att 12.646801 loss_ctc 10.816082 loss_rnnt 16.095362 hw_loss 0.294439 history loss 9.178802 rank 1
2023-02-21 13:15:22,112 DEBUG CV Batch 13/2800 loss 15.167926 loss_att 12.646801 loss_ctc 10.816082 loss_rnnt 16.095362 hw_loss 0.294439 history loss 9.178802 rank 0
2023-02-21 13:15:25,220 DEBUG CV Batch 13/2900 loss 25.412123 loss_att 21.761879 loss_ctc 35.274979 loss_rnnt 24.719671 hw_loss 0.201474 history loss 9.402328 rank 2
2023-02-21 13:15:25,927 DEBUG CV Batch 13/2800 loss 15.167926 loss_att 12.646801 loss_ctc 10.816082 loss_rnnt 16.095362 hw_loss 0.294439 history loss 9.178802 rank 7
2023-02-21 13:15:28,813 DEBUG CV Batch 13/2900 loss 25.412123 loss_att 21.761879 loss_ctc 35.274979 loss_rnnt 24.719671 hw_loss 0.201474 history loss 9.402328 rank 5
2023-02-21 13:15:30,502 DEBUG CV Batch 13/2900 loss 25.412123 loss_att 21.761879 loss_ctc 35.274979 loss_rnnt 24.719671 hw_loss 0.201474 history loss 9.402328 rank 6
2023-02-21 13:15:32,296 DEBUG CV Batch 13/2900 loss 25.412123 loss_att 21.761879 loss_ctc 35.274979 loss_rnnt 24.719671 hw_loss 0.201474 history loss 9.402328 rank 4
2023-02-21 13:15:33,174 DEBUG CV Batch 13/2900 loss 25.412123 loss_att 21.761879 loss_ctc 35.274979 loss_rnnt 24.719671 hw_loss 0.201474 history loss 9.402328 rank 1
2023-02-21 13:15:33,199 DEBUG CV Batch 13/2900 loss 25.412123 loss_att 21.761879 loss_ctc 35.274979 loss_rnnt 24.719671 hw_loss 0.201474 history loss 9.402328 rank 3
2023-02-21 13:15:33,221 DEBUG CV Batch 13/2900 loss 25.412123 loss_att 21.761879 loss_ctc 35.274979 loss_rnnt 24.719671 hw_loss 0.201474 history loss 9.402328 rank 0
2023-02-21 13:15:36,471 DEBUG CV Batch 13/3000 loss 3.533975 loss_att 5.117764 loss_ctc 5.640003 loss_rnnt 2.713304 hw_loss 0.418329 history loss 9.538565 rank 2
2023-02-21 13:15:37,291 DEBUG CV Batch 13/2900 loss 25.412123 loss_att 21.761879 loss_ctc 35.274979 loss_rnnt 24.719671 hw_loss 0.201474 history loss 9.402328 rank 7
2023-02-21 13:15:40,386 DEBUG CV Batch 13/3000 loss 3.533975 loss_att 5.117764 loss_ctc 5.640003 loss_rnnt 2.713304 hw_loss 0.418329 history loss 9.538565 rank 5
2023-02-21 13:15:41,635 DEBUG CV Batch 13/3000 loss 3.533975 loss_att 5.117764 loss_ctc 5.640003 loss_rnnt 2.713304 hw_loss 0.418329 history loss 9.538565 rank 6
2023-02-21 13:15:43,620 DEBUG CV Batch 13/3000 loss 3.533975 loss_att 5.117764 loss_ctc 5.640003 loss_rnnt 2.713304 hw_loss 0.418329 history loss 9.538565 rank 4
2023-02-21 13:15:44,594 DEBUG CV Batch 13/3000 loss 3.533975 loss_att 5.117764 loss_ctc 5.640003 loss_rnnt 2.713304 hw_loss 0.418329 history loss 9.538565 rank 0
2023-02-21 13:15:44,608 DEBUG CV Batch 13/3000 loss 3.533975 loss_att 5.117764 loss_ctc 5.640003 loss_rnnt 2.713304 hw_loss 0.418329 history loss 9.538565 rank 1
2023-02-21 13:15:44,915 DEBUG CV Batch 13/3000 loss 3.533975 loss_att 5.117764 loss_ctc 5.640003 loss_rnnt 2.713304 hw_loss 0.418329 history loss 9.538565 rank 3
2023-02-21 13:15:48,609 DEBUG CV Batch 13/3000 loss 3.533975 loss_att 5.117764 loss_ctc 5.640003 loss_rnnt 2.713304 hw_loss 0.418329 history loss 9.538565 rank 7
2023-02-21 13:15:48,734 DEBUG CV Batch 13/3100 loss 8.999243 loss_att 8.252452 loss_ctc 14.603973 loss_rnnt 8.164432 hw_loss 0.444137 history loss 9.525356 rank 2
2023-02-21 13:15:52,854 DEBUG CV Batch 13/3100 loss 8.999243 loss_att 8.252452 loss_ctc 14.603973 loss_rnnt 8.164432 hw_loss 0.444137 history loss 9.525356 rank 5
2023-02-21 13:15:53,918 DEBUG CV Batch 13/3100 loss 8.999243 loss_att 8.252452 loss_ctc 14.603973 loss_rnnt 8.164432 hw_loss 0.444137 history loss 9.525356 rank 6
2023-02-21 13:15:55,947 DEBUG CV Batch 13/3100 loss 8.999243 loss_att 8.252452 loss_ctc 14.603973 loss_rnnt 8.164432 hw_loss 0.444137 history loss 9.525356 rank 4
2023-02-21 13:15:57,116 DEBUG CV Batch 13/3100 loss 8.999243 loss_att 8.252452 loss_ctc 14.603973 loss_rnnt 8.164432 hw_loss 0.444137 history loss 9.525356 rank 1
2023-02-21 13:15:57,157 DEBUG CV Batch 13/3100 loss 8.999243 loss_att 8.252452 loss_ctc 14.603973 loss_rnnt 8.164432 hw_loss 0.444137 history loss 9.525356 rank 0
2023-02-21 13:15:57,750 DEBUG CV Batch 13/3100 loss 8.999243 loss_att 8.252452 loss_ctc 14.603973 loss_rnnt 8.164432 hw_loss 0.444137 history loss 9.525356 rank 3
2023-02-21 13:16:01,050 DEBUG CV Batch 13/3100 loss 8.999243 loss_att 8.252452 loss_ctc 14.603973 loss_rnnt 8.164432 hw_loss 0.444137 history loss 9.525356 rank 7
2023-02-21 13:16:01,341 DEBUG CV Batch 13/3200 loss 2.674983 loss_att 2.027375 loss_ctc 1.520239 loss_rnnt 2.710243 hw_loss 0.465426 history loss 9.472286 rank 2
2023-02-21 13:16:05,601 DEBUG CV Batch 13/3200 loss 2.674983 loss_att 2.027375 loss_ctc 1.520239 loss_rnnt 2.710243 hw_loss 0.465426 history loss 9.472286 rank 5
2023-02-21 13:16:06,453 DEBUG CV Batch 13/3200 loss 2.674983 loss_att 2.027375 loss_ctc 1.520239 loss_rnnt 2.710243 hw_loss 0.465426 history loss 9.472286 rank 6
2023-02-21 13:16:09,708 DEBUG CV Batch 13/3200 loss 2.674983 loss_att 2.027375 loss_ctc 1.520239 loss_rnnt 2.710243 hw_loss 0.465426 history loss 9.472286 rank 4
2023-02-21 13:16:09,780 DEBUG CV Batch 13/3200 loss 2.674983 loss_att 2.027375 loss_ctc 1.520239 loss_rnnt 2.710243 hw_loss 0.465426 history loss 9.472286 rank 0
2023-02-21 13:16:09,911 DEBUG CV Batch 13/3200 loss 2.674983 loss_att 2.027375 loss_ctc 1.520239 loss_rnnt 2.710243 hw_loss 0.465426 history loss 9.472286 rank 1
2023-02-21 13:16:10,452 DEBUG CV Batch 13/3200 loss 2.674983 loss_att 2.027375 loss_ctc 1.520239 loss_rnnt 2.710243 hw_loss 0.465426 history loss 9.472286 rank 3
2023-02-21 13:16:11,627 DEBUG CV Batch 13/3300 loss 4.049827 loss_att 9.961823 loss_ctc 3.384500 loss_rnnt 2.869193 hw_loss 0.163021 history loss 9.389738 rank 2
2023-02-21 13:16:13,950 DEBUG CV Batch 13/3200 loss 2.674983 loss_att 2.027375 loss_ctc 1.520239 loss_rnnt 2.710243 hw_loss 0.465426 history loss 9.472286 rank 7
2023-02-21 13:16:16,200 DEBUG CV Batch 13/3300 loss 4.049827 loss_att 9.961823 loss_ctc 3.384500 loss_rnnt 2.869193 hw_loss 0.163021 history loss 9.389738 rank 5
2023-02-21 13:16:17,143 DEBUG CV Batch 13/3300 loss 4.049827 loss_att 9.961823 loss_ctc 3.384500 loss_rnnt 2.869193 hw_loss 0.163021 history loss 9.389738 rank 6
2023-02-21 13:16:20,294 DEBUG CV Batch 13/3300 loss 4.049827 loss_att 9.961823 loss_ctc 3.384500 loss_rnnt 2.869193 hw_loss 0.163021 history loss 9.389738 rank 0
2023-02-21 13:16:20,611 DEBUG CV Batch 13/3300 loss 4.049827 loss_att 9.961823 loss_ctc 3.384500 loss_rnnt 2.869193 hw_loss 0.163021 history loss 9.389738 rank 4
2023-02-21 13:16:20,719 DEBUG CV Batch 13/3300 loss 4.049827 loss_att 9.961823 loss_ctc 3.384500 loss_rnnt 2.869193 hw_loss 0.163021 history loss 9.389738 rank 1
2023-02-21 13:16:21,053 DEBUG CV Batch 13/3300 loss 4.049827 loss_att 9.961823 loss_ctc 3.384500 loss_rnnt 2.869193 hw_loss 0.163021 history loss 9.389738 rank 3
2023-02-21 13:16:24,337 DEBUG CV Batch 13/3400 loss 10.105983 loss_att 8.635328 loss_ctc 11.159472 loss_rnnt 10.026496 hw_loss 0.437161 history loss 9.289472 rank 2
2023-02-21 13:16:24,564 DEBUG CV Batch 13/3300 loss 4.049827 loss_att 9.961823 loss_ctc 3.384500 loss_rnnt 2.869193 hw_loss 0.163021 history loss 9.389738 rank 7
2023-02-21 13:16:29,012 DEBUG CV Batch 13/3400 loss 10.105983 loss_att 8.635328 loss_ctc 11.159472 loss_rnnt 10.026496 hw_loss 0.437161 history loss 9.289472 rank 5
2023-02-21 13:16:29,943 DEBUG CV Batch 13/3400 loss 10.105983 loss_att 8.635328 loss_ctc 11.159472 loss_rnnt 10.026496 hw_loss 0.437161 history loss 9.289472 rank 6
2023-02-21 13:16:32,931 DEBUG CV Batch 13/3400 loss 10.105983 loss_att 8.635328 loss_ctc 11.159472 loss_rnnt 10.026496 hw_loss 0.437161 history loss 9.289472 rank 0
2023-02-21 13:16:33,254 DEBUG CV Batch 13/3400 loss 10.105983 loss_att 8.635328 loss_ctc 11.159472 loss_rnnt 10.026496 hw_loss 0.437161 history loss 9.289472 rank 4
2023-02-21 13:16:33,558 DEBUG CV Batch 13/3400 loss 10.105983 loss_att 8.635328 loss_ctc 11.159472 loss_rnnt 10.026496 hw_loss 0.437161 history loss 9.289472 rank 1
2023-02-21 13:16:33,647 DEBUG CV Batch 13/3400 loss 10.105983 loss_att 8.635328 loss_ctc 11.159472 loss_rnnt 10.026496 hw_loss 0.437161 history loss 9.289472 rank 3
2023-02-21 13:16:37,500 DEBUG CV Batch 13/3400 loss 10.105983 loss_att 8.635328 loss_ctc 11.159472 loss_rnnt 10.026496 hw_loss 0.437161 history loss 9.289472 rank 7
2023-02-21 13:16:39,822 DEBUG CV Batch 13/3500 loss 106.405327 loss_att 135.728958 loss_ctc 97.649979 loss_rnnt 101.581261 hw_loss 0.237605 history loss 9.308048 rank 2
2023-02-21 13:16:44,902 DEBUG CV Batch 13/3500 loss 106.405327 loss_att 135.728958 loss_ctc 97.649979 loss_rnnt 101.581261 hw_loss 0.237605 history loss 9.308048 rank 5
2023-02-21 13:16:45,578 DEBUG CV Batch 13/3500 loss 106.405327 loss_att 135.728958 loss_ctc 97.649979 loss_rnnt 101.581261 hw_loss 0.237605 history loss 9.308048 rank 6
2023-02-21 13:16:48,656 DEBUG CV Batch 13/3500 loss 106.405327 loss_att 135.728958 loss_ctc 97.649979 loss_rnnt 101.581261 hw_loss 0.237605 history loss 9.308048 rank 0
2023-02-21 13:16:49,145 DEBUG CV Batch 13/3500 loss 106.405327 loss_att 135.728958 loss_ctc 97.649979 loss_rnnt 101.581261 hw_loss 0.237605 history loss 9.308048 rank 4
2023-02-21 13:16:49,241 DEBUG CV Batch 13/3500 loss 106.405327 loss_att 135.728958 loss_ctc 97.649979 loss_rnnt 101.581261 hw_loss 0.237605 history loss 9.308048 rank 1
2023-02-21 13:16:49,270 DEBUG CV Batch 13/3500 loss 106.405327 loss_att 135.728958 loss_ctc 97.649979 loss_rnnt 101.581261 hw_loss 0.237605 history loss 9.308048 rank 3
2023-02-21 13:16:49,941 DEBUG CV Batch 13/3600 loss 7.647610 loss_att 12.007902 loss_ctc 7.213080 loss_rnnt 6.774047 hw_loss 0.111453 history loss 9.195545 rank 2
2023-02-21 13:16:53,257 DEBUG CV Batch 13/3500 loss 106.405327 loss_att 135.728958 loss_ctc 97.649979 loss_rnnt 101.581261 hw_loss 0.237605 history loss 9.308048 rank 7
2023-02-21 13:16:55,413 DEBUG CV Batch 13/3600 loss 7.647610 loss_att 12.007902 loss_ctc 7.213080 loss_rnnt 6.774047 hw_loss 0.111453 history loss 9.195545 rank 5
2023-02-21 13:16:56,011 DEBUG CV Batch 13/3600 loss 7.647610 loss_att 12.007902 loss_ctc 7.213080 loss_rnnt 6.774047 hw_loss 0.111453 history loss 9.195545 rank 6
2023-02-21 13:16:59,293 DEBUG CV Batch 13/3600 loss 7.647610 loss_att 12.007902 loss_ctc 7.213080 loss_rnnt 6.774047 hw_loss 0.111453 history loss 9.195545 rank 0
2023-02-21 13:16:59,782 DEBUG CV Batch 13/3600 loss 7.647610 loss_att 12.007902 loss_ctc 7.213080 loss_rnnt 6.774047 hw_loss 0.111453 history loss 9.195545 rank 4
2023-02-21 13:16:59,814 DEBUG CV Batch 13/3600 loss 7.647610 loss_att 12.007902 loss_ctc 7.213080 loss_rnnt 6.774047 hw_loss 0.111453 history loss 9.195545 rank 3
2023-02-21 13:16:59,927 DEBUG CV Batch 13/3600 loss 7.647610 loss_att 12.007902 loss_ctc 7.213080 loss_rnnt 6.774047 hw_loss 0.111453 history loss 9.195545 rank 1
2023-02-21 13:17:03,776 DEBUG CV Batch 13/3700 loss 7.804625 loss_att 7.292311 loss_ctc 8.497045 loss_rnnt 7.653931 hw_loss 0.301563 history loss 9.142392 rank 2
2023-02-21 13:17:04,102 DEBUG CV Batch 13/3600 loss 7.647610 loss_att 12.007902 loss_ctc 7.213080 loss_rnnt 6.774047 hw_loss 0.111453 history loss 9.195545 rank 7
2023-02-21 13:17:09,207 DEBUG CV Batch 13/3700 loss 7.804625 loss_att 7.292311 loss_ctc 8.497045 loss_rnnt 7.653931 hw_loss 0.301563 history loss 9.142392 rank 5
2023-02-21 13:17:09,854 DEBUG CV Batch 13/3700 loss 7.804625 loss_att 7.292311 loss_ctc 8.497045 loss_rnnt 7.653931 hw_loss 0.301563 history loss 9.142392 rank 6
2023-02-21 13:17:13,201 DEBUG CV Batch 13/3700 loss 7.804625 loss_att 7.292311 loss_ctc 8.497045 loss_rnnt 7.653931 hw_loss 0.301563 history loss 9.142392 rank 0
2023-02-21 13:17:13,438 DEBUG CV Batch 13/3700 loss 7.804625 loss_att 7.292311 loss_ctc 8.497045 loss_rnnt 7.653931 hw_loss 0.301563 history loss 9.142392 rank 3
2023-02-21 13:17:13,442 DEBUG CV Batch 13/3700 loss 7.804625 loss_att 7.292311 loss_ctc 8.497045 loss_rnnt 7.653931 hw_loss 0.301563 history loss 9.142392 rank 4
2023-02-21 13:17:13,888 DEBUG CV Batch 13/3700 loss 7.804625 loss_att 7.292311 loss_ctc 8.497045 loss_rnnt 7.653931 hw_loss 0.301563 history loss 9.142392 rank 1
2023-02-21 13:17:15,135 DEBUG CV Batch 13/3800 loss 2.862325 loss_att 3.254988 loss_ctc 2.817212 loss_rnnt 2.612564 hw_loss 0.332330 history loss 9.103800 rank 2
2023-02-21 13:17:17,963 DEBUG CV Batch 13/3700 loss 7.804625 loss_att 7.292311 loss_ctc 8.497045 loss_rnnt 7.653931 hw_loss 0.301563 history loss 9.142392 rank 7
2023-02-21 13:17:20,735 DEBUG CV Batch 13/3800 loss 2.862325 loss_att 3.254988 loss_ctc 2.817212 loss_rnnt 2.612564 hw_loss 0.332330 history loss 9.103800 rank 5
2023-02-21 13:17:21,368 DEBUG CV Batch 13/3800 loss 2.862325 loss_att 3.254988 loss_ctc 2.817212 loss_rnnt 2.612564 hw_loss 0.332330 history loss 9.103800 rank 6
2023-02-21 13:17:23,807 INFO Epoch 13 CV info cv_loss 9.089381077966605
2023-02-21 13:17:23,807 INFO Epoch 14 TRAIN info lr 0.00045255262655817064
2023-02-21 13:17:23,811 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 13:17:24,447 DEBUG CV Batch 13/3800 loss 2.862325 loss_att 3.254988 loss_ctc 2.817212 loss_rnnt 2.612564 hw_loss 0.332330 history loss 9.103800 rank 3
2023-02-21 13:17:24,747 DEBUG CV Batch 13/3800 loss 2.862325 loss_att 3.254988 loss_ctc 2.817212 loss_rnnt 2.612564 hw_loss 0.332330 history loss 9.103800 rank 0
2023-02-21 13:17:25,054 DEBUG CV Batch 13/3800 loss 2.862325 loss_att 3.254988 loss_ctc 2.817212 loss_rnnt 2.612564 hw_loss 0.332330 history loss 9.103800 rank 4
2023-02-21 13:17:25,116 DEBUG CV Batch 13/3800 loss 2.862325 loss_att 3.254988 loss_ctc 2.817212 loss_rnnt 2.612564 hw_loss 0.332330 history loss 9.103800 rank 1
2023-02-21 13:17:29,290 DEBUG CV Batch 13/3800 loss 2.862325 loss_att 3.254988 loss_ctc 2.817212 loss_rnnt 2.612564 hw_loss 0.332330 history loss 9.103800 rank 7
2023-02-21 13:17:29,429 INFO Epoch 13 CV info cv_loss 9.089381078237967
2023-02-21 13:17:29,429 INFO Epoch 14 TRAIN info lr 0.0004524747916347843
2023-02-21 13:17:29,432 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 13:17:30,095 INFO Epoch 13 CV info cv_loss 9.089381078797917
2023-02-21 13:17:30,096 INFO Epoch 14 TRAIN info lr 0.00045256931069683214
2023-02-21 13:17:30,101 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 13:17:33,063 INFO Epoch 13 CV info cv_loss 9.08938107723436
2023-02-21 13:17:33,063 INFO Epoch 14 TRAIN info lr 0.0004526026845107115
2023-02-21 13:17:33,066 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 13:17:33,559 INFO Epoch 13 CV info cv_loss 9.089381076988845
2023-02-21 13:17:33,560 INFO Checkpoint: save to checkpoint exp/2_20_rnnt_bias_loss_2_class_0-3word_22/13.pt
2023-02-21 13:17:33,859 INFO Epoch 13 CV info cv_loss 9.089381077578947
2023-02-21 13:17:33,859 INFO Epoch 14 TRAIN info lr 0.0004525878507930574
2023-02-21 13:17:33,863 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 13:17:33,953 INFO Epoch 13 CV info cv_loss 9.089381077497109
2023-02-21 13:17:33,953 INFO Epoch 14 TRAIN info lr 0.0004525952674695699
2023-02-21 13:17:33,958 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 13:17:35,365 INFO Epoch 14 TRAIN info lr 0.00045254706558864804
2023-02-21 13:17:35,370 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 13:17:38,270 INFO Epoch 13 CV info cv_loss 9.089381077566024
2023-02-21 13:17:38,270 INFO Epoch 14 TRAIN info lr 0.00045255262655817064
2023-02-21 13:17:38,274 INFO using accumulate grad, new batch size is 2 times larger than before
2023-02-21 13:18:55,093 DEBUG TRAIN Batch 14/0 loss 13.510922 loss_att 12.808125 loss_ctc 14.263896 loss_rnnt 13.286357 hw_loss 0.496366 lr 0.00045260 rank 3
2023-02-21 13:18:55,098 DEBUG TRAIN Batch 14/0 loss 8.925528 loss_att 7.674452 loss_ctc 9.756574 loss_rnnt 8.670062 hw_loss 0.740390 lr 0.00045255 rank 2
2023-02-21 13:18:55,100 DEBUG TRAIN Batch 14/0 loss 14.871017 loss_att 13.023260 loss_ctc 17.267860 loss_rnnt 14.637507 hw_loss 0.531530 lr 0.00045259 rank 4
2023-02-21 13:18:55,104 DEBUG TRAIN Batch 14/0 loss 12.840294 loss_att 10.624808 loss_ctc 12.825751 loss_rnnt 12.954010 hw_loss 0.621223 lr 0.00045257 rank 6
2023-02-21 13:18:55,105 DEBUG TRAIN Batch 14/0 loss 16.849176 loss_att 13.778247 loss_ctc 21.105793 loss_rnnt 16.508194 hw_loss 0.726785 lr 0.00045259 rank 1
2023-02-21 13:18:55,106 DEBUG TRAIN Batch 14/0 loss 5.752778 loss_att 6.323730 loss_ctc 7.114869 loss_rnnt 5.248583 hw_loss 0.390736 lr 0.00045255 rank 0
2023-02-21 13:18:55,106 DEBUG TRAIN Batch 14/0 loss 7.405317 loss_att 6.930858 loss_ctc 9.551982 loss_rnnt 6.950497 hw_loss 0.494043 lr 0.00045247 rank 5
2023-02-21 13:18:55,239 DEBUG TRAIN Batch 14/0 loss 9.109065 loss_att 9.315852 loss_ctc 11.745877 loss_rnnt 8.386184 hw_loss 0.618655 lr 0.00045255 rank 7
2023-02-21 13:19:50,275 DEBUG TRAIN Batch 14/100 loss 7.592796 loss_att 11.242750 loss_ctc 9.990282 loss_rnnt 6.395928 hw_loss 0.276020 lr 0.00045238 rank 5
2023-02-21 13:19:50,276 DEBUG TRAIN Batch 14/100 loss 13.481030 loss_att 21.399582 loss_ctc 12.338877 loss_rnnt 12.003181 hw_loss 0.087049 lr 0.00045250 rank 4
2023-02-21 13:19:50,278 DEBUG TRAIN Batch 14/100 loss 25.028200 loss_att 29.515976 loss_ctc 29.664043 loss_rnnt 23.365938 hw_loss 0.274862 lr 0.00045251 rank 3
2023-02-21 13:19:50,281 DEBUG TRAIN Batch 14/100 loss 4.819288 loss_att 8.898804 loss_ctc 7.125869 loss_rnnt 3.494277 hw_loss 0.377931 lr 0.00045246 rank 2
2023-02-21 13:19:50,282 DEBUG TRAIN Batch 14/100 loss 13.134666 loss_att 16.592901 loss_ctc 14.100821 loss_rnnt 12.175748 hw_loss 0.259594 lr 0.00045246 rank 7
2023-02-21 13:19:50,283 DEBUG TRAIN Batch 14/100 loss 35.350655 loss_att 39.303371 loss_ctc 45.432594 loss_rnnt 33.192619 hw_loss 0.043567 lr 0.00045247 rank 6
2023-02-21 13:19:50,288 DEBUG TRAIN Batch 14/100 loss 13.521685 loss_att 17.551401 loss_ctc 19.506025 loss_rnnt 11.727498 hw_loss 0.356869 lr 0.00045249 rank 1
2023-02-21 13:19:50,340 DEBUG TRAIN Batch 14/100 loss 5.233839 loss_att 7.538260 loss_ctc 6.334813 loss_rnnt 4.451337 hw_loss 0.327790 lr 0.00045245 rank 0
2023-02-21 13:20:46,805 DEBUG TRAIN Batch 14/200 loss 1.637034 loss_att 4.742003 loss_ctc 1.661649 loss_rnnt 0.749760 hw_loss 0.493122 lr 0.00045229 rank 5
2023-02-21 13:20:46,805 DEBUG TRAIN Batch 14/200 loss 3.106051 loss_att 4.612580 loss_ctc 1.238298 loss_rnnt 3.053591 hw_loss 0.000351 lr 0.00045237 rank 2
2023-02-21 13:20:46,805 DEBUG TRAIN Batch 14/200 loss 10.024211 loss_att 7.616933 loss_ctc 7.534020 loss_rnnt 10.565779 hw_loss 0.509837 lr 0.00045242 rank 3
2023-02-21 13:20:46,812 DEBUG TRAIN Batch 14/200 loss 4.578208 loss_att 8.863468 loss_ctc 6.182868 loss_rnnt 3.507014 hw_loss 0.000351 lr 0.00045237 rank 7
2023-02-21 13:20:46,813 DEBUG TRAIN Batch 14/200 loss 17.659306 loss_att 20.306978 loss_ctc 25.748077 loss_rnnt 15.850151 hw_loss 0.377092 lr 0.00045236 rank 0
2023-02-21 13:20:46,814 DEBUG TRAIN Batch 14/200 loss 10.141037 loss_att 15.475821 loss_ctc 12.181077 loss_rnnt 8.588347 hw_loss 0.400738 lr 0.00045240 rank 1
2023-02-21 13:20:46,815 DEBUG TRAIN Batch 14/200 loss 7.622383 loss_att 11.619665 loss_ctc 14.878406 loss_rnnt 5.852963 hw_loss 0.004674 lr 0.00045241 rank 4
2023-02-21 13:20:46,816 DEBUG TRAIN Batch 14/200 loss 17.802999 loss_att 19.585289 loss_ctc 17.760998 loss_rnnt 17.397038 hw_loss 0.103319 lr 0.00045238 rank 6
2023-02-21 13:21:42,190 DEBUG TRAIN Batch 14/300 loss 10.012710 loss_att 9.068451 loss_ctc 11.523787 loss_rnnt 9.686233 hw_loss 0.588473 lr 0.00045227 rank 2
2023-02-21 13:21:42,190 DEBUG TRAIN Batch 14/300 loss 3.340877 loss_att 4.859590 loss_ctc 4.991473 loss_rnnt 2.552483 hw_loss 0.496072 lr 0.00045227 rank 0
2023-02-21 13:21:42,190 DEBUG TRAIN Batch 14/300 loss 9.605444 loss_att 9.817842 loss_ctc 13.597135 loss_rnnt 8.892873 hw_loss 0.258499 lr 0.00045220 rank 5
2023-02-21 13:21:42,192 DEBUG TRAIN Batch 14/300 loss 7.731590 loss_att 8.487956 loss_ctc 10.167600 loss_rnnt 7.115482 hw_loss 0.262563 lr 0.00045232 rank 4
2023-02-21 13:21:42,194 DEBUG TRAIN Batch 14/300 loss 20.698927 loss_att 23.196793 loss_ctc 24.145489 loss_rnnt 19.447372 hw_loss 0.548324 lr 0.00045227 rank 7
2023-02-21 13:21:42,195 DEBUG TRAIN Batch 14/300 loss 14.122065 loss_att 13.916440 loss_ctc 16.355049 loss_rnnt 13.687662 hw_loss 0.333367 lr 0.00045229 rank 6
2023-02-21 13:21:42,199 DEBUG TRAIN Batch 14/300 loss 29.481501 loss_att 28.303272 loss_ctc 38.638599 loss_rnnt 28.270229 hw_loss 0.423699 lr 0.00045232 rank 3
2023-02-21 13:21:42,201 DEBUG TRAIN Batch 14/300 loss 6.473796 loss_att 6.639347 loss_ctc 8.944195 loss_rnnt 5.954339 hw_loss 0.294299 lr 0.00045231 rank 1
2023-02-21 13:22:37,239 DEBUG TRAIN Batch 14/400 loss 3.769901 loss_att 10.061977 loss_ctc 7.352385 loss_rnnt 1.853066 hw_loss 0.338915 lr 0.00045218 rank 2
2023-02-21 13:22:37,240 DEBUG TRAIN Batch 14/400 loss 8.115700 loss_att 10.544209 loss_ctc 6.952874 loss_rnnt 7.651948 hw_loss 0.249551 lr 0.00045222 rank 4
2023-02-21 13:22:37,243 DEBUG TRAIN Batch 14/400 loss 14.029842 loss_att 16.181635 loss_ctc 16.597816 loss_rnnt 13.112961 hw_loss 0.270237 lr 0.00045217 rank 0
2023-02-21 13:22:37,243 DEBUG TRAIN Batch 14/400 loss 24.543770 loss_att 25.860191 loss_ctc 28.888031 loss_rnnt 23.557955 hw_loss 0.268679 lr 0.00045223 rank 3
2023-02-21 13:22:37,262 DEBUG TRAIN Batch 14/400 loss 7.127889 loss_att 8.722793 loss_ctc 7.127134 loss_rnnt 6.613667 hw_loss 0.366264 lr 0.00045218 rank 7
2023-02-21 13:22:37,263 DEBUG TRAIN Batch 14/400 loss 11.757996 loss_att 12.063065 loss_ctc 13.296640 loss_rnnt 11.424417 hw_loss 0.126398 lr 0.00045222 rank 1
2023-02-21 13:22:37,272 DEBUG TRAIN Batch 14/400 loss 7.861098 loss_att 16.087505 loss_ctc 8.744558 loss_rnnt 5.897947 hw_loss 0.375141 lr 0.00045210 rank 5
2023-02-21 13:22:37,281 DEBUG TRAIN Batch 14/400 loss 13.236898 loss_att 17.157314 loss_ctc 19.731384 loss_rnnt 11.444096 hw_loss 0.267726 lr 0.00045220 rank 6
2023-02-21 13:23:32,791 DEBUG TRAIN Batch 14/500 loss 12.231779 loss_att 15.403643 loss_ctc 26.708712 loss_rnnt 9.421589 hw_loss 0.460424 lr 0.00045213 rank 4
2023-02-21 13:23:32,792 DEBUG TRAIN Batch 14/500 loss 21.830624 loss_att 27.149055 loss_ctc 29.210426 loss_rnnt 19.782822 hw_loss 0.000260 lr 0.00045209 rank 2
2023-02-21 13:23:32,793 DEBUG TRAIN Batch 14/500 loss 23.288551 loss_att 24.972153 loss_ctc 28.973791 loss_rnnt 22.193613 hw_loss 0.000350 lr 0.00045214 rank 3
2023-02-21 13:23:32,794 DEBUG TRAIN Batch 14/500 loss 21.119297 loss_att 20.186317 loss_ctc 21.467966 loss_rnnt 21.259262 hw_loss 0.000264 lr 0.00045208 rank 0
2023-02-21 13:23:32,796 DEBUG TRAIN Batch 14/500 loss 36.892784 loss_att 35.778557 loss_ctc 46.257782 loss_rnnt 35.678410 hw_loss 0.353533 lr 0.00045210 rank 6
2023-02-21 13:23:32,797 DEBUG TRAIN Batch 14/500 loss 7.753345 loss_att 10.311179 loss_ctc 4.013960 loss_rnnt 7.740301 hw_loss 0.000114 lr 0.00045201 rank 5
2023-02-21 13:23:32,797 DEBUG TRAIN Batch 14/500 loss 1.732222 loss_att 5.387814 loss_ctc 4.058759 loss_rnnt 0.690760 hw_loss 0.000260 lr 0.00045209 rank 7
2023-02-21 13:23:32,811 DEBUG TRAIN Batch 14/500 loss 34.832138 loss_att 34.596081 loss_ctc 42.471844 loss_rnnt 33.733170 hw_loss 0.239161 lr 0.00045212 rank 1
2023-02-21 13:24:27,465 DEBUG TRAIN Batch 14/600 loss 13.240670 loss_att 13.394299 loss_ctc 17.252674 loss_rnnt 12.347910 hw_loss 0.613312 lr 0.00045200 rank 2
2023-02-21 13:24:27,465 DEBUG TRAIN Batch 14/600 loss 5.329766 loss_att 6.905762 loss_ctc 9.123663 loss_rnnt 4.370259 hw_loss 0.259602 lr 0.00045199 rank 0
2023-02-21 13:24:27,470 DEBUG TRAIN Batch 14/600 loss 18.798357 loss_att 20.170034 loss_ctc 22.702156 loss_rnnt 17.686523 hw_loss 0.594359 lr 0.00045192 rank 5
2023-02-21 13:24:27,471 DEBUG TRAIN Batch 14/600 loss 19.985758 loss_att 16.843870 loss_ctc 18.971970 loss_rnnt 20.683662 hw_loss 0.123080 lr 0.00045205 rank 3
2023-02-21 13:24:27,475 DEBUG TRAIN Batch 14/600 loss 10.566881 loss_att 10.060180 loss_ctc 9.653985 loss_rnnt 10.622124 hw_loss 0.314655 lr 0.00045201 rank 6
2023-02-21 13:24:27,475 DEBUG TRAIN Batch 14/600 loss 8.906113 loss_att 12.345706 loss_ctc 13.877241 loss_rnnt 7.356718 hw_loss 0.372485 lr 0.00045200 rank 7
2023-02-21 13:24:27,477 DEBUG TRAIN Batch 14/600 loss 18.723162 loss_att 18.750484 loss_ctc 22.401730 loss_rnnt 17.998020 hw_loss 0.429748 lr 0.00045204 rank 4
2023-02-21 13:24:27,479 DEBUG TRAIN Batch 14/600 loss 9.008162 loss_att 9.264978 loss_ctc 10.952370 loss_rnnt 8.695776 hw_loss 0.003366 lr 0.00045203 rank 1
2023-02-21 13:25:23,507 DEBUG TRAIN Batch 14/700 loss 18.034073 loss_att 20.107342 loss_ctc 25.583387 loss_rnnt 16.399086 hw_loss 0.400796 lr 0.00045190 rank 2
2023-02-21 13:25:23,511 DEBUG TRAIN Batch 14/700 loss 8.085775 loss_att 11.071335 loss_ctc 8.310237 loss_rnnt 7.327900 hw_loss 0.245317 lr 0.00045183 rank 5
2023-02-21 13:25:23,513 DEBUG TRAIN Batch 14/700 loss 8.642193 loss_att 11.816551 loss_ctc 10.518933 loss_rnnt 7.644269 hw_loss 0.211536 lr 0.00045195 rank 4
2023-02-21 13:25:23,516 DEBUG TRAIN Batch 14/700 loss 11.854082 loss_att 12.112151 loss_ctc 13.322499 loss_rnnt 11.428060 hw_loss 0.334911 lr 0.00045190 rank 0
2023-02-21 13:25:23,522 DEBUG TRAIN Batch 14/700 loss 11.075835 loss_att 15.554654 loss_ctc 11.188790 loss_rnnt 10.036446 hw_loss 0.241057 lr 0.00045192 rank 6
2023-02-21 13:25:23,525 DEBUG TRAIN Batch 14/700 loss 6.919787 loss_att 8.972059 loss_ctc 10.132192 loss_rnnt 5.905568 hw_loss 0.328957 lr 0.00045194 rank 1
2023-02-21 13:25:23,525 DEBUG TRAIN Batch 14/700 loss 18.532660 loss_att 24.265413 loss_ctc 23.816118 loss_rnnt 16.506435 hw_loss 0.328521 lr 0.00045190 rank 7
2023-02-21 13:25:23,549 DEBUG TRAIN Batch 14/700 loss 19.191000 loss_att 25.938856 loss_ctc 25.443720 loss_rnnt 16.774351 hw_loss 0.437590 lr 0.00045195 rank 3
2023-02-21 13:26:20,303 DEBUG TRAIN Batch 14/800 loss 18.747240 loss_att 18.657162 loss_ctc 12.539183 loss_rnnt 19.389225 hw_loss 0.382070 lr 0.00045186 rank 3
2023-02-21 13:26:20,305 DEBUG TRAIN Batch 14/800 loss 7.593562 loss_att 16.510939 loss_ctc 13.948651 loss_rnnt 4.680146 hw_loss 0.529866 lr 0.00045181 rank 2
2023-02-21 13:26:20,307 DEBUG TRAIN Batch 14/800 loss 15.159979 loss_att 14.299945 loss_ctc 15.711724 loss_rnnt 15.050884 hw_loss 0.389128 lr 0.00045181 rank 7
2023-02-21 13:26:20,307 DEBUG TRAIN Batch 14/800 loss 4.931104 loss_att 9.425733 loss_ctc 6.985035 loss_rnnt 3.758271 hw_loss 0.000095 lr 0.00045173 rank 5
2023-02-21 13:26:20,309 DEBUG TRAIN Batch 14/800 loss 33.574013 loss_att 35.397297 loss_ctc 46.168079 loss_rnnt 31.381872 hw_loss 0.278017 lr 0.00045185 rank 1
2023-02-21 13:26:20,313 DEBUG TRAIN Batch 14/800 loss 6.432887 loss_att 11.572683 loss_ctc 9.005134 loss_rnnt 4.835384 hw_loss 0.424832 lr 0.00045181 rank 0
2023-02-21 13:26:20,318 DEBUG TRAIN Batch 14/800 loss 9.702682 loss_att 13.533902 loss_ctc 18.182297 loss_rnnt 7.805685 hw_loss 0.000259 lr 0.00045185 rank 4
2023-02-21 13:26:20,318 DEBUG TRAIN Batch 14/800 loss 3.360127 loss_att 11.189648 loss_ctc 2.379883 loss_rnnt 1.650449 hw_loss 0.514637 lr 0.00045183 rank 6
2023-02-21 13:27:40,945 DEBUG TRAIN Batch 14/900 loss 10.084569 loss_att 10.780366 loss_ctc 12.550483 loss_rnnt 9.442897 hw_loss 0.325732 lr 0.00045164 rank 5
2023-02-21 13:27:40,953 DEBUG TRAIN Batch 14/900 loss 10.571081 loss_att 11.585016 loss_ctc 12.861061 loss_rnnt 9.911417 hw_loss 0.284148 lr 0.00045172 rank 2
2023-02-21 13:27:40,953 DEBUG TRAIN Batch 14/900 loss 6.067322 loss_att 9.745521 loss_ctc 8.603458 loss_rnnt 4.708750 hw_loss 0.533963 lr 0.00045171 rank 0
2023-02-21 13:27:40,954 DEBUG TRAIN Batch 14/900 loss 13.295908 loss_att 17.715544 loss_ctc 17.224117 loss_rnnt 11.757541 hw_loss 0.245022 lr 0.00045177 rank 3
2023-02-21 13:27:40,955 DEBUG TRAIN Batch 14/900 loss 16.667093 loss_att 19.767300 loss_ctc 21.735214 loss_rnnt 15.212992 hw_loss 0.296832 lr 0.00045174 rank 6
2023-02-21 13:27:40,958 DEBUG TRAIN Batch 14/900 loss 12.327825 loss_att 13.476501 loss_ctc 13.820537 loss_rnnt 11.656939 hw_loss 0.453979 lr 0.00045176 rank 4
2023-02-21 13:27:40,961 DEBUG TRAIN Batch 14/900 loss 2.964545 loss_att 5.365094 loss_ctc 2.833693 loss_rnnt 2.379389 hw_loss 0.229675 lr 0.00045172 rank 7
2023-02-21 13:27:40,962 DEBUG TRAIN Batch 14/900 loss 17.165314 loss_att 18.992567 loss_ctc 20.101566 loss_rnnt 16.286592 hw_loss 0.228319 lr 0.00045175 rank 1
2023-02-21 13:28:37,388 DEBUG TRAIN Batch 14/1000 loss 2.971948 loss_att 4.882339 loss_ctc 1.560897 loss_rnnt 2.655169 hw_loss 0.230327 lr 0.00045163 rank 2
2023-02-21 13:28:37,392 DEBUG TRAIN Batch 14/1000 loss 1.960139 loss_att 6.485167 loss_ctc 1.686226 loss_rnnt 0.861109 hw_loss 0.432275 lr 0.00045168 rank 3
2023-02-21 13:28:37,394 DEBUG TRAIN Batch 14/1000 loss 5.793696 loss_att 7.828035 loss_ctc 5.731012 loss_rnnt 5.267381 hw_loss 0.239634 lr 0.00045163 rank 7
2023-02-21 13:28:37,394 DEBUG TRAIN Batch 14/1000 loss 10.059719 loss_att 13.517321 loss_ctc 13.930099 loss_rnnt 8.852053 hw_loss 0.000179 lr 0.00045164 rank 6
2023-02-21 13:28:37,395 DEBUG TRAIN Batch 14/1000 loss 16.612663 loss_att 17.689648 loss_ctc 23.948811 loss_rnnt 15.343539 hw_loss 0.141704 lr 0.00045167 rank 4
2023-02-21 13:28:37,405 DEBUG TRAIN Batch 14/1000 loss 8.507539 loss_att 11.443261 loss_ctc 7.288168 loss_rnnt 7.952726 hw_loss 0.244220 lr 0.00045166 rank 1
2023-02-21 13:28:37,428 DEBUG TRAIN Batch 14/1000 loss 38.667252 loss_att 44.593624 loss_ctc 45.268421 loss_rnnt 36.496773 hw_loss 0.196963 lr 0.00045162 rank 0
2023-02-21 13:28:37,430 DEBUG TRAIN Batch 14/1000 loss 10.970765 loss_att 13.281427 loss_ctc 14.421682 loss_rnnt 9.936667 hw_loss 0.209706 lr 0.00045155 rank 5
2023-02-21 13:29:33,478 DEBUG TRAIN Batch 14/1100 loss 7.608035 loss_att 18.948063 loss_ctc 14.770532 loss_rnnt 4.165207 hw_loss 0.412166 lr 0.00045153 rank 2
2023-02-21 13:29:33,482 DEBUG TRAIN Batch 14/1100 loss 0.737981 loss_att 2.916183 loss_ctc 0.242987 loss_rnnt 0.136508 hw_loss 0.434685 lr 0.00045153 rank 0
2023-02-21 13:29:33,483 DEBUG TRAIN Batch 14/1100 loss 24.276222 loss_att 23.758085 loss_ctc 31.391365 loss_rnnt 23.259733 hw_loss 0.321430 lr 0.00045146 rank 5
2023-02-21 13:29:33,485 DEBUG TRAIN Batch 14/1100 loss 21.487247 loss_att 22.541405 loss_ctc 32.418514 loss_rnnt 19.818888 hw_loss 0.000046 lr 0.00045158 rank 4
2023-02-21 13:29:33,485 DEBUG TRAIN Batch 14/1100 loss 5.539532 loss_att 16.353155 loss_ctc 9.100142 loss_rnnt 2.901732 hw_loss 0.000614 lr 0.00045155 rank 6
2023-02-21 13:29:33,489 DEBUG TRAIN Batch 14/1100 loss 2.402563 loss_att 5.352816 loss_ctc 7.732268 loss_rnnt 1.101789 hw_loss 0.000180 lr 0.00045158 rank 3
2023-02-21 13:29:33,490 DEBUG TRAIN Batch 14/1100 loss 28.407385 loss_att 33.165974 loss_ctc 38.271530 loss_rnnt 26.025728 hw_loss 0.215095 lr 0.00045157 rank 1
2023-02-21 13:29:33,496 DEBUG TRAIN Batch 14/1100 loss 7.992603 loss_att 12.581068 loss_ctc 9.189908 loss_rnnt 6.639078 hw_loss 0.517860 lr 0.00045153 rank 7
2023-02-21 13:30:28,946 DEBUG TRAIN Batch 14/1200 loss 10.858390 loss_att 11.433746 loss_ctc 13.396879 loss_rnnt 10.277171 hw_loss 0.239402 lr 0.00045144 rank 2
2023-02-21 13:30:28,948 DEBUG TRAIN Batch 14/1200 loss 11.359501 loss_att 12.406379 loss_ctc 15.402477 loss_rnnt 10.457268 hw_loss 0.288363 lr 0.00045144 rank 7
2023-02-21 13:30:28,949 DEBUG TRAIN Batch 14/1200 loss 11.942883 loss_att 12.829830 loss_ctc 14.384218 loss_rnnt 11.348791 hw_loss 0.170983 lr 0.00045146 rank 6
2023-02-21 13:30:28,950 DEBUG TRAIN Batch 14/1200 loss 23.219007 loss_att 21.008022 loss_ctc 29.459236 loss_rnnt 22.706314 hw_loss 0.230366 lr 0.00045148 rank 4
2023-02-21 13:30:28,954 DEBUG TRAIN Batch 14/1200 loss 7.396040 loss_att 7.421655 loss_ctc 10.434900 loss_rnnt 6.719632 hw_loss 0.498944 lr 0.00045148 rank 1
2023-02-21 13:30:28,975 DEBUG TRAIN Batch 14/1200 loss 5.027533 loss_att 7.269404 loss_ctc 5.829199 loss_rnnt 4.471972 hw_loss 0.000558 lr 0.00045144 rank 0
2023-02-21 13:30:28,987 DEBUG TRAIN Batch 14/1200 loss 10.935308 loss_att 13.927621 loss_ctc 14.607590 loss_rnnt 9.628546 hw_loss 0.409995 lr 0.00045137 rank 5
2023-02-21 13:30:29,013 DEBUG TRAIN Batch 14/1200 loss 6.371735 loss_att 11.253670 loss_ctc 10.271502 loss_rnnt 4.633435 hw_loss 0.453645 lr 0.00045149 rank 3
2023-02-21 13:31:26,649 DEBUG TRAIN Batch 14/1300 loss 14.666901 loss_att 18.235573 loss_ctc 19.318693 loss_rnnt 13.176265 hw_loss 0.293741 lr 0.00045135 rank 2
2023-02-21 13:31:26,651 DEBUG TRAIN Batch 14/1300 loss 2.153389 loss_att 4.994255 loss_ctc 1.343516 loss_rnnt 1.605958 hw_loss 0.163578 lr 0.00045135 rank 0
2023-02-21 13:31:26,655 DEBUG TRAIN Batch 14/1300 loss 5.730700 loss_att 9.777565 loss_ctc 6.886489 loss_rnnt 4.767026 hw_loss 0.000369 lr 0.00045140 rank 3
2023-02-21 13:31:26,657 DEBUG TRAIN Batch 14/1300 loss 5.913858 loss_att 10.442049 loss_ctc 9.808723 loss_rnnt 4.175946 hw_loss 0.586796 lr 0.00045137 rank 6
2023-02-21 13:31:26,657 DEBUG TRAIN Batch 14/1300 loss 10.435075 loss_att 10.377313 loss_ctc 11.550084 loss_rnnt 10.251604 hw_loss 0.086915 lr 0.00045139 rank 4
2023-02-21 13:31:26,660 DEBUG TRAIN Batch 14/1300 loss 10.443768 loss_att 13.837503 loss_ctc 15.840961 loss_rnnt 8.868248 hw_loss 0.332150 lr 0.00045135 rank 7
2023-02-21 13:31:26,660 DEBUG TRAIN Batch 14/1300 loss 15.708143 loss_att 19.533976 loss_ctc 22.248817 loss_rnnt 14.070736 hw_loss 0.000281 lr 0.00045127 rank 5
2023-02-21 13:31:26,665 DEBUG TRAIN Batch 14/1300 loss 14.718070 loss_att 14.704229 loss_ctc 20.303917 loss_rnnt 13.778645 hw_loss 0.370152 lr 0.00045139 rank 1
2023-02-21 13:32:21,239 DEBUG TRAIN Batch 14/1400 loss 3.261143 loss_att 7.726426 loss_ctc 6.388357 loss_rnnt 1.840839 hw_loss 0.206787 lr 0.00045125 rank 0
2023-02-21 13:32:21,241 DEBUG TRAIN Batch 14/1400 loss 4.981825 loss_att 11.835944 loss_ctc 8.101459 loss_rnnt 2.965970 hw_loss 0.429525 lr 0.00045126 rank 2
2023-02-21 13:32:21,243 DEBUG TRAIN Batch 14/1400 loss 12.110509 loss_att 18.355833 loss_ctc 10.148817 loss_rnnt 10.801595 hw_loss 0.602639 lr 0.00045118 rank 5
2023-02-21 13:32:21,245 DEBUG TRAIN Batch 14/1400 loss 24.295942 loss_att 22.154999 loss_ctc 30.019894 loss_rnnt 23.797878 hw_loss 0.305737 lr 0.00045130 rank 4
2023-02-21 13:32:21,247 DEBUG TRAIN Batch 14/1400 loss 30.109171 loss_att 33.107414 loss_ctc 49.484413 loss_rnnt 26.842787 hw_loss 0.156317 lr 0.00045131 rank 3
2023-02-21 13:32:21,254 DEBUG TRAIN Batch 14/1400 loss 5.863554 loss_att 7.124780 loss_ctc 8.778491 loss_rnnt 5.062001 hw_loss 0.301219 lr 0.00045129 rank 1
2023-02-21 13:32:21,259 DEBUG TRAIN Batch 14/1400 loss 4.123614 loss_att 6.436697 loss_ctc 5.985143 loss_rnnt 3.412432 hw_loss 0.000677 lr 0.00045128 rank 6
2023-02-21 13:32:21,262 DEBUG TRAIN Batch 14/1400 loss 39.857464 loss_att 38.260693 loss_ctc 42.889809 loss_rnnt 39.771793 hw_loss 0.001321 lr 0.00045126 rank 7
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 1.80 GiB (GPU 0; 10.76 GiB total capacity; 5.96 GiB already allocated; 1.63 GiB free; 7.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:44441
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:53924
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:49541
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:8601
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:36236
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:35505
Traceback (most recent call last):
  File "wenet/bin/train.py", line 297, in <module>
    main()
  File "wenet/bin/train.py", line 269, in main
    executor.train(model, optimizer, scheduler, train_data_loader, device,
  File "/home/work_nfs6/tyxu/workspace/wenet-bias-celoss/wenet/utils/executor.py", line 103, in train
    loss.backward()
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/environment/tyxu/anaconda3/envs/wenet/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: [/opt/conda/conda-bld/pytorch_1634272172048/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [192.168.0.33]:13229
