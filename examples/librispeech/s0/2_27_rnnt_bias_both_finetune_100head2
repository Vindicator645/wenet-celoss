/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_27_rnnt_bias_both_finetune_100head.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_27_rnnt_bias_loss_2_class_both_finetune_100head/ddp_init
2023-03-01 17:37:47,501 INFO training on multiple gpus, this gpu 1
2023-03-01 17:37:47,505 INFO training on multiple gpus, this gpu 0
2023-03-01 17:37:47,526 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-03-01 17:37:47,527 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-03-01 17:37:47,527 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2023-03-01 17:37:47,528 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2023-03-01 17:37:48,990 INFO Checkpoint: loading from checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune_100head/54.pt for GPU
2023-03-01 17:37:50,262 INFO Checkpoint: loading from checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune_100head/54.pt for GPU
2023-03-01 17:37:52,039 INFO Epoch 55 TRAIN info lr 4e-08
2023-03-01 17:37:52,040 INFO using accumulate grad, new batch size is 4 times larger than before
2023-03-01 17:37:52,041 INFO Epoch 55 TRAIN info lr 4e-08
2023-03-01 17:37:52,041 INFO using accumulate grad, new batch size is 4 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-03-01 17:38:24,693 DEBUG TRAIN Batch 55/0 loss 6.212205 loss_att 6.842720 loss_ctc 10.076370 loss_rnnt 5.408747 hw_loss 0.303998 lr 0.00025951 rank 0
2023-03-01 17:38:24,700 DEBUG TRAIN Batch 55/0 loss 8.384593 loss_att 8.171894 loss_ctc 10.463362 loss_rnnt 7.961700 hw_loss 0.352993 lr 0.00025951 rank 1
2023-03-01 17:38:46,490 DEBUG TRAIN Batch 55/100 loss 5.812404 loss_att 6.946801 loss_ctc 7.852709 loss_rnnt 5.205415 hw_loss 0.202630 lr 0.00025950 rank 0
2023-03-01 17:38:46,493 DEBUG TRAIN Batch 55/100 loss 4.470262 loss_att 8.175776 loss_ctc 6.492737 loss_rnnt 3.322713 hw_loss 0.256469 lr 0.00025950 rank 1
2023-03-01 17:39:07,944 DEBUG TRAIN Batch 55/200 loss 3.245102 loss_att 5.124983 loss_ctc 3.762371 loss_rnnt 2.688677 hw_loss 0.209022 lr 0.00025949 rank 0
2023-03-01 17:39:07,947 DEBUG TRAIN Batch 55/200 loss 2.981566 loss_att 6.974463 loss_ctc 11.386478 loss_rnnt 0.957888 hw_loss 0.195833 lr 0.00025949 rank 1
2023-03-01 17:39:30,879 DEBUG TRAIN Batch 55/300 loss 3.103263 loss_att 4.993778 loss_ctc 6.095925 loss_rnnt 2.240492 hw_loss 0.160587 lr 0.00025948 rank 1
2023-03-01 17:39:30,881 DEBUG TRAIN Batch 55/300 loss 5.744346 loss_att 8.735968 loss_ctc 10.021667 loss_rnnt 4.459849 hw_loss 0.217243 lr 0.00025948 rank 0
2023-03-01 17:40:02,400 DEBUG TRAIN Batch 55/400 loss 6.774378 loss_att 10.653656 loss_ctc 10.360336 loss_rnnt 5.392590 hw_loss 0.239636 lr 0.00025948 rank 1
2023-03-01 17:40:02,401 DEBUG TRAIN Batch 55/400 loss 4.521272 loss_att 7.561553 loss_ctc 8.083326 loss_rnnt 3.341702 hw_loss 0.181074 lr 0.00025948 rank 0
2023-03-01 17:40:25,362 DEBUG TRAIN Batch 55/500 loss 4.314774 loss_att 6.629830 loss_ctc 6.165016 loss_rnnt 3.500746 hw_loss 0.195596 lr 0.00025947 rank 1
2023-03-01 17:40:25,364 DEBUG TRAIN Batch 55/500 loss 7.614709 loss_att 8.837889 loss_ctc 15.935946 loss_rnnt 6.103811 hw_loss 0.293932 lr 0.00025947 rank 0
2023-03-01 17:40:48,582 DEBUG TRAIN Batch 55/600 loss 4.834550 loss_att 7.929031 loss_ctc 8.412224 loss_rnnt 3.672328 hw_loss 0.124319 lr 0.00025946 rank 0
2023-03-01 17:40:48,583 DEBUG TRAIN Batch 55/600 loss 6.044159 loss_att 5.875196 loss_ctc 9.013199 loss_rnnt 5.536152 hw_loss 0.273613 lr 0.00025946 rank 1
2023-03-01 17:41:10,663 DEBUG TRAIN Batch 55/700 loss 4.356284 loss_att 6.959218 loss_ctc 3.819519 loss_rnnt 3.773583 hw_loss 0.250656 lr 0.00025945 rank 1
2023-03-01 17:41:10,667 DEBUG TRAIN Batch 55/700 loss 2.032729 loss_att 4.401347 loss_ctc 2.785789 loss_rnnt 1.347260 hw_loss 0.208758 lr 0.00025945 rank 0
2023-03-01 17:41:44,416 DEBUG TRAIN Batch 55/800 loss 4.676372 loss_att 7.534342 loss_ctc 6.371336 loss_rnnt 3.851847 hw_loss 0.050503 lr 0.00025944 rank 1
2023-03-01 17:41:44,419 DEBUG TRAIN Batch 55/800 loss 6.273899 loss_att 9.310640 loss_ctc 8.140386 loss_rnnt 5.379845 hw_loss 0.070952 lr 0.00025944 rank 0
2023-03-01 17:42:06,593 DEBUG TRAIN Batch 55/900 loss 11.221375 loss_att 13.213739 loss_ctc 14.390606 loss_rnnt 10.320246 hw_loss 0.150174 lr 0.00025943 rank 1
2023-03-01 17:42:06,595 DEBUG TRAIN Batch 55/900 loss 7.323185 loss_att 8.912275 loss_ctc 10.343143 loss_rnnt 6.424500 hw_loss 0.334136 lr 0.00025943 rank 0
2023-03-01 17:42:28,172 DEBUG TRAIN Batch 55/1000 loss 7.388011 loss_att 11.443130 loss_ctc 12.129202 loss_rnnt 5.803877 hw_loss 0.264283 lr 0.00025942 rank 1
2023-03-01 17:42:28,175 DEBUG TRAIN Batch 55/1000 loss 6.716238 loss_att 9.997075 loss_ctc 11.424707 loss_rnnt 5.323205 hw_loss 0.204508 lr 0.00025942 rank 0
2023-03-01 17:43:00,723 DEBUG TRAIN Batch 55/1100 loss 5.722415 loss_att 8.018153 loss_ctc 8.806152 loss_rnnt 4.747074 hw_loss 0.196928 lr 0.00025942 rank 1
2023-03-01 17:43:00,726 DEBUG TRAIN Batch 55/1100 loss 4.518573 loss_att 6.396924 loss_ctc 5.907338 loss_rnnt 3.892648 hw_loss 0.122037 lr 0.00025942 rank 0
2023-03-01 17:43:22,234 DEBUG TRAIN Batch 55/1200 loss 7.469284 loss_att 12.082119 loss_ctc 17.086462 loss_rnnt 5.098699 hw_loss 0.310739 lr 0.00025941 rank 1
2023-03-01 17:43:22,236 DEBUG TRAIN Batch 55/1200 loss 8.816730 loss_att 11.802216 loss_ctc 14.931213 loss_rnnt 7.307578 hw_loss 0.181481 lr 0.00025941 rank 0
2023-03-01 17:43:44,607 DEBUG TRAIN Batch 55/1300 loss 5.145538 loss_att 5.450588 loss_ctc 8.367141 loss_rnnt 4.474247 hw_loss 0.338876 lr 0.00025940 rank 1
2023-03-01 17:43:44,608 DEBUG TRAIN Batch 55/1300 loss 5.352703 loss_att 9.918169 loss_ctc 9.825202 loss_rnnt 3.688927 hw_loss 0.289405 lr 0.00025940 rank 0
2023-03-01 17:44:06,112 DEBUG TRAIN Batch 55/1400 loss 8.420770 loss_att 11.885680 loss_ctc 16.477743 loss_rnnt 6.549530 hw_loss 0.194991 lr 0.00025939 rank 1
2023-03-01 17:44:06,113 DEBUG TRAIN Batch 55/1400 loss 2.436862 loss_att 4.468784 loss_ctc 5.390778 loss_rnnt 1.501889 hw_loss 0.252625 lr 0.00025939 rank 0
2023-03-01 17:44:39,723 DEBUG TRAIN Batch 55/1500 loss 4.528359 loss_att 7.365290 loss_ctc 6.810379 loss_rnnt 3.577620 hw_loss 0.148284 lr 0.00025938 rank 1
2023-03-01 17:44:39,726 DEBUG TRAIN Batch 55/1500 loss 5.167469 loss_att 7.759507 loss_ctc 8.442057 loss_rnnt 4.055877 hw_loss 0.293573 lr 0.00025938 rank 0
2023-03-01 17:45:01,341 DEBUG TRAIN Batch 55/1600 loss 5.431140 loss_att 7.655508 loss_ctc 9.297417 loss_rnnt 4.385550 hw_loss 0.159774 lr 0.00025937 rank 1
2023-03-01 17:45:01,342 DEBUG TRAIN Batch 55/1600 loss 7.327059 loss_att 11.364907 loss_ctc 16.896976 loss_rnnt 5.136980 hw_loss 0.199725 lr 0.00025937 rank 0
2023-03-01 17:45:22,730 DEBUG TRAIN Batch 55/1700 loss 9.358573 loss_att 10.608085 loss_ctc 16.383766 loss_rnnt 8.042700 hw_loss 0.242399 lr 0.00025936 rank 1
2023-03-01 17:45:22,732 DEBUG TRAIN Batch 55/1700 loss 8.816200 loss_att 11.256786 loss_ctc 18.372822 loss_rnnt 6.970718 hw_loss 0.155901 lr 0.00025936 rank 0
2023-03-01 17:45:55,551 DEBUG TRAIN Batch 55/1800 loss 5.284464 loss_att 7.705381 loss_ctc 11.444654 loss_rnnt 3.828915 hw_loss 0.281262 lr 0.00025935 rank 1
2023-03-01 17:45:55,553 DEBUG TRAIN Batch 55/1800 loss 7.671941 loss_att 8.044310 loss_ctc 13.002695 loss_rnnt 6.741429 hw_loss 0.272382 lr 0.00025935 rank 0
2023-03-01 17:46:16,995 DEBUG TRAIN Batch 55/1900 loss 7.827276 loss_att 8.140948 loss_ctc 13.000092 loss_rnnt 6.897153 hw_loss 0.333150 lr 0.00025935 rank 1
2023-03-01 17:46:16,999 DEBUG TRAIN Batch 55/1900 loss 4.930164 loss_att 6.796989 loss_ctc 9.570494 loss_rnnt 3.753394 hw_loss 0.346302 lr 0.00025935 rank 0
2023-03-01 17:46:38,024 DEBUG TRAIN Batch 55/2000 loss 3.444944 loss_att 5.541266 loss_ctc 8.325243 loss_rnnt 2.297107 hw_loss 0.146000 lr 0.00025934 rank 1
2023-03-01 17:46:38,028 DEBUG TRAIN Batch 55/2000 loss 5.363532 loss_att 8.325146 loss_ctc 7.837314 loss_rnnt 4.288954 hw_loss 0.285782 lr 0.00025934 rank 0
2023-03-01 17:46:59,433 DEBUG TRAIN Batch 55/2100 loss 10.735136 loss_att 13.555489 loss_ctc 15.134048 loss_rnnt 9.558680 hw_loss 0.048496 lr 0.00025933 rank 1
2023-03-01 17:46:59,435 DEBUG TRAIN Batch 55/2100 loss 3.728224 loss_att 8.176961 loss_ctc 7.356441 loss_rnnt 2.255606 hw_loss 0.185829 lr 0.00025933 rank 0
2023-03-01 17:47:31,450 DEBUG TRAIN Batch 55/2200 loss 7.827856 loss_att 12.699447 loss_ctc 13.497395 loss_rnnt 5.980460 hw_loss 0.219636 lr 0.00025932 rank 1
2023-03-01 17:47:31,453 DEBUG TRAIN Batch 55/2200 loss 3.125734 loss_att 9.153063 loss_ctc 8.260836 loss_rnnt 1.165914 hw_loss 0.130639 lr 0.00025932 rank 0
2023-03-01 17:47:52,181 DEBUG TRAIN Batch 55/2300 loss 4.911443 loss_att 7.775300 loss_ctc 7.701227 loss_rnnt 3.903445 hw_loss 0.118603 lr 0.00025931 rank 1
2023-03-01 17:47:52,183 DEBUG TRAIN Batch 55/2300 loss 1.207319 loss_att 4.085908 loss_ctc 2.667586 loss_rnnt 0.299231 hw_loss 0.258127 lr 0.00025931 rank 0
2023-03-01 17:48:13,818 DEBUG TRAIN Batch 55/2400 loss 8.893119 loss_att 10.109614 loss_ctc 11.819190 loss_rnnt 8.146200 hw_loss 0.212769 lr 0.00025930 rank 1
2023-03-01 17:48:13,821 DEBUG TRAIN Batch 55/2400 loss 9.363497 loss_att 13.427255 loss_ctc 16.840069 loss_rnnt 7.505698 hw_loss 0.090321 lr 0.00025930 rank 0
2023-03-01 17:48:47,583 DEBUG TRAIN Batch 55/2500 loss 7.055486 loss_att 9.401140 loss_ctc 10.673125 loss_rnnt 5.975326 hw_loss 0.241269 lr 0.00025929 rank 1
2023-03-01 17:48:47,584 DEBUG TRAIN Batch 55/2500 loss 4.775603 loss_att 7.278191 loss_ctc 9.179422 loss_rnnt 3.527026 hw_loss 0.301657 lr 0.00025929 rank 0
2023-03-01 17:49:09,557 DEBUG TRAIN Batch 55/2600 loss 4.422149 loss_att 6.370416 loss_ctc 6.961500 loss_rnnt 3.623706 hw_loss 0.131643 lr 0.00025928 rank 1
2023-03-01 17:49:09,559 DEBUG TRAIN Batch 55/2600 loss 3.548233 loss_att 7.077713 loss_ctc 5.943927 loss_rnnt 2.432423 hw_loss 0.169665 lr 0.00025928 rank 0
2023-03-01 17:49:30,658 DEBUG TRAIN Batch 55/2700 loss 3.933278 loss_att 7.589875 loss_ctc 8.529360 loss_rnnt 2.566188 hw_loss 0.043049 lr 0.00025928 rank 1
2023-03-01 17:49:30,659 DEBUG TRAIN Batch 55/2700 loss 4.266745 loss_att 7.673340 loss_ctc 9.618042 loss_rnnt 2.782975 hw_loss 0.166771 lr 0.00025928 rank 0
2023-03-01 17:49:52,153 DEBUG TRAIN Batch 55/2800 loss 12.654605 loss_att 14.643991 loss_ctc 16.716660 loss_rnnt 11.583602 hw_loss 0.246597 lr 0.00025927 rank 1
2023-03-01 17:49:52,154 DEBUG TRAIN Batch 55/2800 loss 8.091211 loss_att 11.985939 loss_ctc 19.472679 loss_rnnt 5.731270 hw_loss 0.119000 lr 0.00025927 rank 0
2023-03-01 17:50:24,350 DEBUG TRAIN Batch 55/2900 loss 7.737895 loss_att 10.448298 loss_ctc 8.657946 loss_rnnt 6.958070 hw_loss 0.215757 lr 0.00025926 rank 0
2023-03-01 17:50:24,350 DEBUG TRAIN Batch 55/2900 loss 7.206246 loss_att 9.725230 loss_ctc 13.243183 loss_rnnt 5.798057 hw_loss 0.186503 lr 0.00025926 rank 1
2023-03-01 17:50:45,526 DEBUG TRAIN Batch 55/3000 loss 4.032393 loss_att 6.257899 loss_ctc 7.888902 loss_rnnt 2.934535 hw_loss 0.259792 lr 0.00025925 rank 1
2023-03-01 17:50:45,528 DEBUG TRAIN Batch 55/3000 loss 6.083596 loss_att 11.769117 loss_ctc 13.425266 loss_rnnt 3.897478 hw_loss 0.131482 lr 0.00025925 rank 0
2023-03-01 17:51:07,373 DEBUG TRAIN Batch 55/3100 loss 12.627836 loss_att 12.739618 loss_ctc 12.902305 loss_rnnt 12.517062 hw_loss 0.097165 lr 0.00025924 rank 1
2023-03-01 17:51:07,375 DEBUG TRAIN Batch 55/3100 loss 4.898942 loss_att 7.547633 loss_ctc 10.121440 loss_rnnt 3.585097 hw_loss 0.164576 lr 0.00025924 rank 0
2023-03-01 17:51:43,020 DEBUG TRAIN Batch 55/3200 loss 11.860765 loss_att 14.835329 loss_ctc 19.722918 loss_rnnt 10.098092 hw_loss 0.224013 lr 0.00025923 rank 0
2023-03-01 17:51:43,024 DEBUG TRAIN Batch 55/3200 loss 6.292511 loss_att 7.408256 loss_ctc 10.173024 loss_rnnt 5.410169 hw_loss 0.265859 lr 0.00025923 rank 1
2023-03-01 17:52:04,155 DEBUG TRAIN Batch 55/3300 loss 7.884394 loss_att 10.375757 loss_ctc 15.867541 loss_rnnt 6.223735 hw_loss 0.183689 lr 0.00025922 rank 1
2023-03-01 17:52:04,157 DEBUG TRAIN Batch 55/3300 loss 8.004900 loss_att 10.984053 loss_ctc 15.543407 loss_rnnt 6.330029 hw_loss 0.138572 lr 0.00025922 rank 0
2023-03-01 17:52:25,672 DEBUG TRAIN Batch 55/3400 loss 8.970553 loss_att 12.172897 loss_ctc 15.443985 loss_rnnt 7.414989 hw_loss 0.097449 lr 0.00025921 rank 1
2023-03-01 17:52:25,674 DEBUG TRAIN Batch 55/3400 loss 8.154652 loss_att 10.199210 loss_ctc 14.631674 loss_rnnt 6.773302 hw_loss 0.204068 lr 0.00025921 rank 0
2023-03-01 17:52:46,916 DEBUG TRAIN Batch 55/3500 loss 9.071242 loss_att 9.348160 loss_ctc 11.337170 loss_rnnt 8.588408 hw_loss 0.234988 lr 0.00025921 rank 1
2023-03-01 17:52:46,925 DEBUG TRAIN Batch 55/3500 loss 5.056564 loss_att 6.309882 loss_ctc 5.739785 loss_rnnt 4.600192 hw_loss 0.214899 lr 0.00025921 rank 0
2023-03-01 17:53:20,136 DEBUG TRAIN Batch 55/3600 loss 8.870096 loss_att 10.027758 loss_ctc 16.984383 loss_rnnt 7.469452 hw_loss 0.163511 lr 0.00025920 rank 1
2023-03-01 17:53:20,138 DEBUG TRAIN Batch 55/3600 loss 8.860132 loss_att 11.027420 loss_ctc 15.111994 loss_rnnt 7.462278 hw_loss 0.245278 lr 0.00025920 rank 0
2023-03-01 17:53:42,129 DEBUG TRAIN Batch 55/3700 loss 14.759557 loss_att 15.973604 loss_ctc 23.143450 loss_rnnt 13.227803 hw_loss 0.320794 lr 0.00025919 rank 1
2023-03-01 17:53:42,130 DEBUG TRAIN Batch 55/3700 loss 8.839651 loss_att 11.210135 loss_ctc 15.793816 loss_rnnt 7.371809 hw_loss 0.124731 lr 0.00025919 rank 0
2023-03-01 17:54:04,797 DEBUG TRAIN Batch 55/3800 loss 5.018178 loss_att 5.854109 loss_ctc 6.820325 loss_rnnt 4.491083 hw_loss 0.224294 lr 0.00025918 rank 1
2023-03-01 17:54:04,799 DEBUG TRAIN Batch 55/3800 loss 7.388883 loss_att 10.141313 loss_ctc 10.861094 loss_rnnt 6.249713 hw_loss 0.235728 lr 0.00025918 rank 0
2023-03-01 17:54:27,393 DEBUG TRAIN Batch 55/3900 loss 7.257971 loss_att 11.424921 loss_ctc 12.614861 loss_rnnt 5.609103 hw_loss 0.189798 lr 0.00025917 rank 1
2023-03-01 17:54:27,395 DEBUG TRAIN Batch 55/3900 loss 5.779848 loss_att 8.587558 loss_ctc 9.975713 loss_rnnt 4.517182 hw_loss 0.265642 lr 0.00025917 rank 0
2023-03-01 17:54:59,901 DEBUG TRAIN Batch 55/4000 loss 5.544887 loss_att 8.152199 loss_ctc 12.688373 loss_rnnt 3.988012 hw_loss 0.155525 lr 0.00025916 rank 1
2023-03-01 17:54:59,904 DEBUG TRAIN Batch 55/4000 loss 3.849013 loss_att 5.639096 loss_ctc 7.660400 loss_rnnt 2.826676 hw_loss 0.292753 lr 0.00025916 rank 0
2023-03-01 17:55:21,928 DEBUG TRAIN Batch 55/4100 loss 5.286407 loss_att 10.653918 loss_ctc 12.796122 loss_rnnt 3.133873 hw_loss 0.145757 lr 0.00025915 rank 1
2023-03-01 17:55:21,930 DEBUG TRAIN Batch 55/4100 loss 10.948749 loss_att 15.071774 loss_ctc 16.443104 loss_rnnt 9.242659 hw_loss 0.279196 lr 0.00025915 rank 0
2023-03-01 17:55:43,161 DEBUG TRAIN Batch 55/4200 loss 6.451719 loss_att 9.675216 loss_ctc 11.618003 loss_rnnt 5.000286 hw_loss 0.221053 lr 0.00025914 rank 1
2023-03-01 17:55:43,163 DEBUG TRAIN Batch 55/4200 loss 7.945103 loss_att 13.036034 loss_ctc 15.653588 loss_rnnt 5.731749 hw_loss 0.313817 lr 0.00025914 rank 0
2023-03-01 17:56:16,897 DEBUG TRAIN Batch 55/4300 loss 10.270410 loss_att 13.918082 loss_ctc 14.690097 loss_rnnt 8.844745 hw_loss 0.200323 lr 0.00025914 rank 1
2023-03-01 17:56:16,897 DEBUG TRAIN Batch 55/4300 loss 8.137255 loss_att 10.139688 loss_ctc 13.791477 loss_rnnt 6.886933 hw_loss 0.179885 lr 0.00025914 rank 0
2023-03-01 17:56:39,111 DEBUG TRAIN Batch 55/4400 loss 2.356555 loss_att 4.400992 loss_ctc 3.366282 loss_rnnt 1.682339 hw_loss 0.245059 lr 0.00025913 rank 0
2023-03-01 17:56:39,111 DEBUG TRAIN Batch 55/4400 loss 4.790408 loss_att 8.331219 loss_ctc 8.025352 loss_rnnt 3.541477 hw_loss 0.205205 lr 0.00025913 rank 1
2023-03-01 17:57:01,325 DEBUG TRAIN Batch 55/4500 loss 5.252533 loss_att 7.985035 loss_ctc 12.111977 loss_rnnt 3.712491 hw_loss 0.148029 lr 0.00025912 rank 1
2023-03-01 17:57:01,326 DEBUG TRAIN Batch 55/4500 loss 5.255449 loss_att 6.984175 loss_ctc 8.379251 loss_rnnt 4.381896 hw_loss 0.208689 lr 0.00025912 rank 0
2023-03-01 17:57:23,118 DEBUG TRAIN Batch 55/4600 loss 2.710346 loss_att 7.020238 loss_ctc 6.717681 loss_rnnt 1.272361 hw_loss 0.078179 lr 0.00025911 rank 1
2023-03-01 17:57:23,119 DEBUG TRAIN Batch 55/4600 loss 1.986031 loss_att 4.481042 loss_ctc 4.435016 loss_rnnt 1.134584 hw_loss 0.048587 lr 0.00025911 rank 0
2023-03-01 17:57:56,004 DEBUG TRAIN Batch 55/4700 loss 6.933690 loss_att 9.940587 loss_ctc 12.184951 loss_rnnt 5.515945 hw_loss 0.217872 lr 0.00025910 rank 1
2023-03-01 17:57:56,006 DEBUG TRAIN Batch 55/4700 loss 4.095597 loss_att 9.943542 loss_ctc 9.125179 loss_rnnt 2.174457 hw_loss 0.151763 lr 0.00025910 rank 0
2023-03-01 17:58:18,303 DEBUG TRAIN Batch 55/4800 loss 7.327274 loss_att 11.382425 loss_ctc 13.131031 loss_rnnt 5.658435 hw_loss 0.157454 lr 0.00025909 rank 1
2023-03-01 17:58:18,305 DEBUG TRAIN Batch 55/4800 loss 5.564888 loss_att 7.214915 loss_ctc 8.445171 loss_rnnt 4.758377 hw_loss 0.173377 lr 0.00025909 rank 0
2023-03-01 17:58:40,491 DEBUG TRAIN Batch 55/4900 loss 9.904890 loss_att 14.306787 loss_ctc 17.555061 loss_rnnt 7.868783 hw_loss 0.254446 lr 0.00025908 rank 1
2023-03-01 17:58:40,494 DEBUG TRAIN Batch 55/4900 loss 11.530155 loss_att 11.774898 loss_ctc 12.827038 loss_rnnt 11.250526 hw_loss 0.108305 lr 0.00025908 rank 0
2023-03-01 17:59:14,490 DEBUG TRAIN Batch 55/5000 loss 4.954496 loss_att 6.622856 loss_ctc 8.004952 loss_rnnt 4.090564 hw_loss 0.231625 lr 0.00025908 rank 1
2023-03-01 17:59:14,491 DEBUG TRAIN Batch 55/5000 loss 5.772592 loss_att 8.704090 loss_ctc 10.463191 loss_rnnt 4.514311 hw_loss 0.087314 lr 0.00025908 rank 0
2023-03-01 17:59:37,282 DEBUG TRAIN Batch 55/5100 loss 9.516474 loss_att 14.431250 loss_ctc 13.674473 loss_rnnt 7.868076 hw_loss 0.208207 lr 0.00025907 rank 1
2023-03-01 17:59:37,282 DEBUG TRAIN Batch 55/5100 loss 6.953599 loss_att 9.271249 loss_ctc 10.894826 loss_rnnt 5.852051 hw_loss 0.210977 lr 0.00025907 rank 0
2023-03-01 17:59:59,539 DEBUG TRAIN Batch 55/5200 loss 5.736096 loss_att 9.182018 loss_ctc 7.637622 loss_rnnt 4.649790 hw_loss 0.269220 lr 0.00025906 rank 1
2023-03-01 17:59:59,540 DEBUG TRAIN Batch 55/5200 loss 11.664692 loss_att 12.273462 loss_ctc 19.656527 loss_rnnt 10.311396 hw_loss 0.311181 lr 0.00025906 rank 0
2023-03-01 18:00:22,556 DEBUG TRAIN Batch 55/5300 loss 5.075986 loss_att 8.671714 loss_ctc 10.373046 loss_rnnt 3.568443 hw_loss 0.153981 lr 0.00025905 rank 0
2023-03-01 18:00:22,556 DEBUG TRAIN Batch 55/5300 loss 7.532633 loss_att 10.855154 loss_ctc 13.101062 loss_rnnt 5.997903 hw_loss 0.239565 lr 0.00025905 rank 1
2023-03-01 18:00:56,241 DEBUG TRAIN Batch 55/5400 loss 2.821904 loss_att 6.699562 loss_ctc 5.064432 loss_rnnt 1.577505 hw_loss 0.318495 lr 0.00025904 rank 1
2023-03-01 18:00:56,242 DEBUG TRAIN Batch 55/5400 loss 8.663707 loss_att 11.117303 loss_ctc 16.727743 loss_rnnt 7.013248 hw_loss 0.158500 lr 0.00025904 rank 0
2023-03-01 18:01:18,454 DEBUG TRAIN Batch 55/5500 loss 11.177846 loss_att 11.937140 loss_ctc 13.347797 loss_rnnt 10.559830 hw_loss 0.331558 lr 0.00025903 rank 1
2023-03-01 18:01:18,456 DEBUG TRAIN Batch 55/5500 loss 4.786900 loss_att 6.129851 loss_ctc 7.027413 loss_rnnt 4.116133 hw_loss 0.193952 lr 0.00025903 rank 0
2023-03-01 18:01:40,816 DEBUG TRAIN Batch 55/5600 loss 7.158442 loss_att 13.452623 loss_ctc 14.472271 loss_rnnt 4.755450 hw_loss 0.316834 lr 0.00025902 rank 1
2023-03-01 18:01:40,818 DEBUG TRAIN Batch 55/5600 loss 6.226679 loss_att 8.911270 loss_ctc 13.038104 loss_rnnt 4.708419 hw_loss 0.137159 lr 0.00025902 rank 0
2023-03-01 18:02:16,202 DEBUG TRAIN Batch 55/5700 loss 6.018240 loss_att 11.529447 loss_ctc 14.489161 loss_rnnt 3.737324 hw_loss 0.092285 lr 0.00025901 rank 1
2023-03-01 18:02:16,204 DEBUG TRAIN Batch 55/5700 loss 8.442768 loss_att 10.258725 loss_ctc 13.280998 loss_rnnt 7.351372 hw_loss 0.155825 lr 0.00025901 rank 0
2023-03-01 18:02:38,708 DEBUG TRAIN Batch 55/5800 loss 4.256916 loss_att 6.521426 loss_ctc 11.080432 loss_rnnt 2.716131 hw_loss 0.333902 lr 0.00025901 rank 1
2023-03-01 18:02:38,709 DEBUG TRAIN Batch 55/5800 loss 7.780303 loss_att 9.996649 loss_ctc 13.514675 loss_rnnt 6.439084 hw_loss 0.250060 lr 0.00025901 rank 0
2023-03-01 18:03:01,016 DEBUG TRAIN Batch 55/5900 loss 7.161010 loss_att 8.697345 loss_ctc 10.427206 loss_rnnt 6.228873 hw_loss 0.355081 lr 0.00025900 rank 1
2023-03-01 18:03:01,018 DEBUG TRAIN Batch 55/5900 loss 4.272183 loss_att 6.135867 loss_ctc 6.026647 loss_rnnt 3.604957 hw_loss 0.113552 lr 0.00025900 rank 0
2023-03-01 18:03:23,515 DEBUG TRAIN Batch 55/6000 loss 4.229574 loss_att 5.975094 loss_ctc 3.619322 loss_rnnt 3.834600 hw_loss 0.238570 lr 0.00025899 rank 0
2023-03-01 18:03:23,517 DEBUG TRAIN Batch 55/6000 loss 4.011302 loss_att 6.889651 loss_ctc 6.302454 loss_rnnt 3.020745 hw_loss 0.205125 lr 0.00025899 rank 1
2023-03-01 18:04:00,637 DEBUG TRAIN Batch 55/6100 loss 4.392833 loss_att 8.218767 loss_ctc 7.373256 loss_rnnt 3.109297 hw_loss 0.226800 lr 0.00025898 rank 1
2023-03-01 18:04:00,639 DEBUG TRAIN Batch 55/6100 loss 10.013186 loss_att 14.098832 loss_ctc 21.296295 loss_rnnt 7.562803 hw_loss 0.241572 lr 0.00025898 rank 0
2023-03-01 18:04:22,779 DEBUG TRAIN Batch 55/6200 loss 9.762435 loss_att 9.191334 loss_ctc 20.867905 loss_rnnt 8.337889 hw_loss 0.108819 lr 0.00025897 rank 1
2023-03-01 18:04:22,781 DEBUG TRAIN Batch 55/6200 loss 3.447146 loss_att 6.373534 loss_ctc 6.652183 loss_rnnt 2.346781 hw_loss 0.164530 lr 0.00025897 rank 0
2023-03-01 18:04:45,536 DEBUG TRAIN Batch 55/6300 loss 4.116724 loss_att 7.561374 loss_ctc 9.700819 loss_rnnt 2.545371 hw_loss 0.258519 lr 0.00025896 rank 1
2023-03-01 18:04:45,536 DEBUG TRAIN Batch 55/6300 loss 10.985116 loss_att 13.051451 loss_ctc 15.721446 loss_rnnt 9.815796 hw_loss 0.233516 lr 0.00025896 rank 0
2023-03-01 18:05:21,762 DEBUG TRAIN Batch 55/6400 loss 3.562774 loss_att 5.877674 loss_ctc 3.852674 loss_rnnt 2.988703 hw_loss 0.135820 lr 0.00025895 rank 1
2023-03-01 18:05:21,765 DEBUG TRAIN Batch 55/6400 loss 6.961031 loss_att 8.037861 loss_ctc 8.850126 loss_rnnt 6.348282 hw_loss 0.272820 lr 0.00025895 rank 0
2023-03-01 18:05:44,137 DEBUG TRAIN Batch 55/6500 loss 7.005194 loss_att 7.153474 loss_ctc 11.667334 loss_rnnt 6.272069 hw_loss 0.153467 lr 0.00025895 rank 1
2023-03-01 18:05:44,139 DEBUG TRAIN Batch 55/6500 loss 11.445312 loss_att 16.033531 loss_ctc 18.476177 loss_rnnt 9.533343 hw_loss 0.106643 lr 0.00025895 rank 0
2023-03-01 18:06:06,607 DEBUG TRAIN Batch 55/6600 loss 8.838107 loss_att 10.107444 loss_ctc 11.963579 loss_rnnt 8.078188 hw_loss 0.167477 lr 0.00025894 rank 1
2023-03-01 18:06:06,607 DEBUG TRAIN Batch 55/6600 loss 7.762166 loss_att 9.946823 loss_ctc 10.828888 loss_rnnt 6.805429 hw_loss 0.207954 lr 0.00025894 rank 0
2023-03-01 18:06:28,449 DEBUG TRAIN Batch 55/6700 loss 6.735168 loss_att 7.051458 loss_ctc 9.109417 loss_rnnt 6.204596 hw_loss 0.282651 lr 0.00025893 rank 1
2023-03-01 18:06:28,450 DEBUG TRAIN Batch 55/6700 loss 9.879062 loss_att 15.634428 loss_ctc 13.866947 loss_rnnt 8.051573 hw_loss 0.271309 lr 0.00025893 rank 0
2023-03-01 18:07:04,264 DEBUG TRAIN Batch 55/6800 loss 3.782547 loss_att 6.063255 loss_ctc 5.600764 loss_rnnt 2.951912 hw_loss 0.247620 lr 0.00025892 rank 1
2023-03-01 18:07:04,267 DEBUG TRAIN Batch 55/6800 loss 9.432655 loss_att 12.365261 loss_ctc 17.443327 loss_rnnt 7.649765 hw_loss 0.240525 lr 0.00025892 rank 0
2023-03-01 18:07:26,627 DEBUG TRAIN Batch 55/6900 loss 13.517756 loss_att 16.278589 loss_ctc 23.234207 loss_rnnt 11.583261 hw_loss 0.162750 lr 0.00025891 rank 1
2023-03-01 18:07:26,629 DEBUG TRAIN Batch 55/6900 loss 6.532081 loss_att 8.555407 loss_ctc 6.747149 loss_rnnt 6.022370 hw_loss 0.143193 lr 0.00025891 rank 0
2023-03-01 18:07:49,911 DEBUG TRAIN Batch 55/7000 loss 6.931491 loss_att 12.514277 loss_ctc 10.752478 loss_rnnt 5.231529 hw_loss 0.138638 lr 0.00025890 rank 1
2023-03-01 18:07:49,913 DEBUG TRAIN Batch 55/7000 loss 6.462278 loss_att 8.471377 loss_ctc 10.116690 loss_rnnt 5.419401 hw_loss 0.288380 lr 0.00025890 rank 0
2023-03-01 18:08:26,392 DEBUG TRAIN Batch 55/7100 loss 9.307040 loss_att 15.401569 loss_ctc 25.605659 loss_rnnt 5.863059 hw_loss 0.097364 lr 0.00025889 rank 1
2023-03-01 18:08:26,392 DEBUG TRAIN Batch 55/7100 loss 5.299397 loss_att 6.216488 loss_ctc 8.753682 loss_rnnt 4.498325 hw_loss 0.294529 lr 0.00025889 rank 0
2023-03-01 18:08:49,278 DEBUG TRAIN Batch 55/7200 loss 7.967340 loss_att 8.781288 loss_ctc 9.791705 loss_rnnt 7.475168 hw_loss 0.161501 lr 0.00025888 rank 1
2023-03-01 18:08:49,282 DEBUG TRAIN Batch 55/7200 loss 5.252759 loss_att 7.391948 loss_ctc 10.004112 loss_rnnt 4.151349 hw_loss 0.075111 lr 0.00025888 rank 0
2023-03-01 18:09:11,675 DEBUG TRAIN Batch 55/7300 loss 6.390443 loss_att 9.792459 loss_ctc 8.712628 loss_rnnt 5.265730 hw_loss 0.252534 lr 0.00025888 rank 1
2023-03-01 18:09:11,675 DEBUG TRAIN Batch 55/7300 loss 13.566426 loss_att 21.802898 loss_ctc 28.544630 loss_rnnt 9.783381 hw_loss 0.259980 lr 0.00025888 rank 0
2023-03-01 18:09:34,464 DEBUG TRAIN Batch 55/7400 loss 2.851575 loss_att 5.784272 loss_ctc 3.635798 loss_rnnt 2.084234 hw_loss 0.142947 lr 0.00025887 rank 1
2023-03-01 18:09:34,466 DEBUG TRAIN Batch 55/7400 loss 13.348180 loss_att 14.137225 loss_ctc 24.392258 loss_rnnt 11.605930 hw_loss 0.209807 lr 0.00025887 rank 0
2023-03-01 18:10:11,152 DEBUG TRAIN Batch 55/7500 loss 2.940697 loss_att 5.439007 loss_ctc 6.997044 loss_rnnt 1.802168 hw_loss 0.183788 lr 0.00025886 rank 1
2023-03-01 18:10:11,155 DEBUG TRAIN Batch 55/7500 loss 9.258377 loss_att 11.774458 loss_ctc 16.902733 loss_rnnt 7.693894 hw_loss 0.078785 lr 0.00025886 rank 0
2023-03-01 18:10:34,435 DEBUG TRAIN Batch 55/7600 loss 6.250638 loss_att 6.750488 loss_ctc 9.410084 loss_rnnt 5.619314 hw_loss 0.206426 lr 0.00025885 rank 1
2023-03-01 18:10:34,438 DEBUG TRAIN Batch 55/7600 loss 6.615701 loss_att 8.962096 loss_ctc 9.762102 loss_rnnt 5.647456 hw_loss 0.148962 lr 0.00025885 rank 0
2023-03-01 18:10:57,197 DEBUG TRAIN Batch 55/7700 loss 3.552537 loss_att 8.053843 loss_ctc 5.681191 loss_rnnt 2.268290 hw_loss 0.187811 lr 0.00025884 rank 1
2023-03-01 18:10:57,199 DEBUG TRAIN Batch 55/7700 loss 7.986484 loss_att 10.040140 loss_ctc 12.592389 loss_rnnt 6.854520 hw_loss 0.200834 lr 0.00025884 rank 0
2023-03-01 18:11:19,592 DEBUG TRAIN Batch 55/7800 loss 5.530881 loss_att 11.486595 loss_ctc 10.699485 loss_rnnt 3.544773 hw_loss 0.198409 lr 0.00025883 rank 1
2023-03-01 18:11:19,594 DEBUG TRAIN Batch 55/7800 loss 7.434305 loss_att 10.168421 loss_ctc 17.209244 loss_rnnt 5.396648 hw_loss 0.351577 lr 0.00025883 rank 0
2023-03-01 18:11:56,235 DEBUG TRAIN Batch 55/7900 loss 5.671747 loss_att 8.311689 loss_ctc 6.900304 loss_rnnt 4.861317 hw_loss 0.222438 lr 0.00025882 rank 1
2023-03-01 18:11:56,237 DEBUG TRAIN Batch 55/7900 loss 7.018721 loss_att 9.604839 loss_ctc 9.918714 loss_rnnt 5.993669 hw_loss 0.227179 lr 0.00025882 rank 0
2023-03-01 18:12:18,390 DEBUG TRAIN Batch 55/8000 loss 1.348923 loss_att 4.008703 loss_ctc 3.254821 loss_rnnt 0.397898 hw_loss 0.309281 lr 0.00025881 rank 1
2023-03-01 18:12:18,392 DEBUG TRAIN Batch 55/8000 loss 9.623640 loss_att 13.430727 loss_ctc 18.056778 loss_rnnt 7.556395 hw_loss 0.340143 lr 0.00025881 rank 0
2023-03-01 18:12:40,649 DEBUG TRAIN Batch 55/8100 loss 11.243675 loss_att 12.161774 loss_ctc 18.670828 loss_rnnt 9.936390 hw_loss 0.250085 lr 0.00025881 rank 1
2023-03-01 18:12:40,652 DEBUG TRAIN Batch 55/8100 loss 9.004035 loss_att 9.607372 loss_ctc 12.243863 loss_rnnt 8.331467 hw_loss 0.224858 lr 0.00025881 rank 0
2023-03-01 18:13:18,033 DEBUG TRAIN Batch 55/8200 loss 4.250255 loss_att 6.563853 loss_ctc 9.757749 loss_rnnt 2.842090 hw_loss 0.395835 lr 0.00025880 rank 1
2023-03-01 18:13:18,034 DEBUG TRAIN Batch 55/8200 loss 11.665871 loss_att 13.385212 loss_ctc 16.395071 loss_rnnt 10.523267 hw_loss 0.315327 lr 0.00025880 rank 0
2023-03-01 18:13:40,821 DEBUG TRAIN Batch 55/8300 loss 2.659626 loss_att 5.902261 loss_ctc 6.216562 loss_rnnt 1.443712 hw_loss 0.174618 lr 0.00025879 rank 1
2023-03-01 18:13:40,823 DEBUG TRAIN Batch 55/8300 loss 9.664653 loss_att 10.888891 loss_ctc 17.702827 loss_rnnt 8.267559 hw_loss 0.150916 lr 0.00025879 rank 0
2023-03-01 18:14:02,756 DEBUG TRAIN Batch 55/8400 loss 10.204536 loss_att 13.124198 loss_ctc 18.007156 loss_rnnt 8.458733 hw_loss 0.227854 lr 0.00025878 rank 1
2023-03-01 18:14:02,758 DEBUG TRAIN Batch 55/8400 loss 6.985382 loss_att 9.419362 loss_ctc 10.843399 loss_rnnt 5.956375 hw_loss 0.052141 lr 0.00025878 rank 0
2023-03-01 18:14:25,871 DEBUG TRAIN Batch 55/8500 loss 9.227974 loss_att 12.323198 loss_ctc 11.117338 loss_rnnt 8.207373 hw_loss 0.280575 lr 0.00025877 rank 1
2023-03-01 18:14:25,876 DEBUG TRAIN Batch 55/8500 loss 4.040887 loss_att 9.629613 loss_ctc 8.243220 loss_rnnt 2.331489 hw_loss 0.058766 lr 0.00025877 rank 0
2023-03-01 18:15:02,853 DEBUG TRAIN Batch 55/8600 loss 5.961643 loss_att 11.739821 loss_ctc 11.424290 loss_rnnt 3.989819 hw_loss 0.164690 lr 0.00025876 rank 1
2023-03-01 18:15:02,856 DEBUG TRAIN Batch 55/8600 loss 9.205399 loss_att 9.743547 loss_ctc 14.141294 loss_rnnt 8.387012 hw_loss 0.098697 lr 0.00025876 rank 0
2023-03-01 18:15:24,994 DEBUG TRAIN Batch 55/8700 loss 14.059274 loss_att 15.678461 loss_ctc 19.869110 loss_rnnt 12.913808 hw_loss 0.088092 lr 0.00025875 rank 1
2023-03-01 18:15:24,996 DEBUG TRAIN Batch 55/8700 loss 3.211717 loss_att 6.617047 loss_ctc 8.501706 loss_rnnt 1.788722 hw_loss 0.068618 lr 0.00025875 rank 0
2023-03-01 18:15:48,054 DEBUG TRAIN Batch 55/8800 loss 7.291507 loss_att 9.637301 loss_ctc 13.619077 loss_rnnt 5.896964 hw_loss 0.153202 lr 0.00025875 rank 0
2023-03-01 18:15:48,057 DEBUG TRAIN Batch 55/8800 loss 4.943440 loss_att 7.171279 loss_ctc 8.158470 loss_rnnt 3.936217 hw_loss 0.249346 lr 0.00025875 rank 1
2023-03-01 18:16:24,577 DEBUG TRAIN Batch 55/8900 loss 12.487719 loss_att 13.697804 loss_ctc 19.169609 loss_rnnt 11.254035 hw_loss 0.188903 lr 0.00025874 rank 1
2023-03-01 18:16:24,577 DEBUG TRAIN Batch 55/8900 loss 8.079947 loss_att 9.322514 loss_ctc 10.603324 loss_rnnt 7.361929 hw_loss 0.249479 lr 0.00025874 rank 0
2023-03-01 18:16:47,322 DEBUG TRAIN Batch 55/9000 loss 4.146606 loss_att 6.018192 loss_ctc 10.420641 loss_rnnt 2.782843 hw_loss 0.286701 lr 0.00025873 rank 1
2023-03-01 18:16:47,324 DEBUG TRAIN Batch 55/9000 loss 9.687590 loss_att 11.964554 loss_ctc 10.558455 loss_rnnt 8.998224 hw_loss 0.220980 lr 0.00025873 rank 0
2023-03-01 18:17:09,756 DEBUG TRAIN Batch 55/9100 loss 3.718685 loss_att 5.920878 loss_ctc 6.744815 loss_rnnt 2.715577 hw_loss 0.298472 lr 0.00025872 rank 1
2023-03-01 18:17:09,759 DEBUG TRAIN Batch 55/9100 loss 0.945939 loss_att 3.084671 loss_ctc 1.727597 loss_rnnt 0.294141 hw_loss 0.224682 lr 0.00025872 rank 0
2023-03-01 18:17:32,469 DEBUG TRAIN Batch 55/9200 loss 8.480515 loss_att 13.956056 loss_ctc 14.453866 loss_rnnt 6.464068 hw_loss 0.234171 lr 0.00025871 rank 1
2023-03-01 18:17:32,470 DEBUG TRAIN Batch 55/9200 loss 2.531218 loss_att 5.735086 loss_ctc 3.289165 loss_rnnt 1.613124 hw_loss 0.330489 lr 0.00025871 rank 0
2023-03-01 18:18:08,560 DEBUG TRAIN Batch 55/9300 loss 7.745102 loss_att 9.441133 loss_ctc 13.357796 loss_rnnt 6.561198 hw_loss 0.180634 lr 0.00025870 rank 1
2023-03-01 18:18:08,562 DEBUG TRAIN Batch 55/9300 loss 5.400879 loss_att 9.253658 loss_ctc 7.168242 loss_rnnt 4.339865 hw_loss 0.102768 lr 0.00025870 rank 0
2023-03-01 18:18:31,134 DEBUG TRAIN Batch 55/9400 loss 10.299191 loss_att 12.632436 loss_ctc 22.986885 loss_rnnt 7.989722 hw_loss 0.283363 lr 0.00025869 rank 1
2023-03-01 18:18:31,134 DEBUG TRAIN Batch 55/9400 loss 8.444343 loss_att 10.304668 loss_ctc 12.097180 loss_rnnt 7.459392 hw_loss 0.235951 lr 0.00025869 rank 0
2023-03-01 18:18:54,027 DEBUG TRAIN Batch 55/9500 loss 10.086385 loss_att 10.091228 loss_ctc 13.084636 loss_rnnt 9.529999 hw_loss 0.291844 lr 0.00025868 rank 1
2023-03-01 18:18:54,031 DEBUG TRAIN Batch 55/9500 loss 12.884478 loss_att 16.755070 loss_ctc 19.611492 loss_rnnt 11.123791 hw_loss 0.168062 lr 0.00025868 rank 0
2023-03-01 18:19:28,785 DEBUG TRAIN Batch 55/9600 loss 2.300056 loss_att 5.974775 loss_ctc 2.815807 loss_rnnt 1.364548 hw_loss 0.247121 lr 0.00025868 rank 1
2023-03-01 18:19:28,788 DEBUG TRAIN Batch 55/9600 loss 5.127081 loss_att 5.901192 loss_ctc 9.078323 loss_rnnt 4.306273 hw_loss 0.260914 lr 0.00025868 rank 0
2023-03-01 18:19:51,427 DEBUG TRAIN Batch 55/9700 loss 5.911395 loss_att 8.634626 loss_ctc 8.586781 loss_rnnt 4.997087 hw_loss 0.024267 lr 0.00025867 rank 1
2023-03-01 18:19:51,430 DEBUG TRAIN Batch 55/9700 loss 9.379019 loss_att 13.317570 loss_ctc 14.373465 loss_rnnt 7.804676 hw_loss 0.226324 lr 0.00025867 rank 0
2023-03-01 18:20:13,857 DEBUG TRAIN Batch 55/9800 loss 5.970641 loss_att 8.967354 loss_ctc 10.292870 loss_rnnt 4.740345 hw_loss 0.102481 lr 0.00025866 rank 1
2023-03-01 18:20:13,857 DEBUG TRAIN Batch 55/9800 loss 4.910520 loss_att 7.992229 loss_ctc 8.691833 loss_rnnt 3.654227 hw_loss 0.254580 lr 0.00025866 rank 0
2023-03-01 18:20:35,711 DEBUG TRAIN Batch 55/9900 loss 4.189214 loss_att 7.990676 loss_ctc 10.820612 loss_rnnt 2.404644 hw_loss 0.262672 lr 0.00025865 rank 1
2023-03-01 18:20:35,713 DEBUG TRAIN Batch 55/9900 loss 7.934793 loss_att 10.665503 loss_ctc 11.397808 loss_rnnt 6.799157 hw_loss 0.239548 lr 0.00025865 rank 0
2023-03-01 18:21:11,012 DEBUG TRAIN Batch 55/10000 loss 7.899587 loss_att 11.241297 loss_ctc 12.866560 loss_rnnt 6.423941 hw_loss 0.271953 lr 0.00025864 rank 1
2023-03-01 18:21:11,015 DEBUG TRAIN Batch 55/10000 loss 8.000908 loss_att 10.099633 loss_ctc 11.438153 loss_rnnt 7.058930 hw_loss 0.119873 lr 0.00025864 rank 0
2023-03-01 18:21:33,450 DEBUG TRAIN Batch 55/10100 loss 4.537113 loss_att 6.599951 loss_ctc 6.573349 loss_rnnt 3.756803 hw_loss 0.180456 lr 0.00025863 rank 1
2023-03-01 18:21:33,453 DEBUG TRAIN Batch 55/10100 loss 5.909647 loss_att 8.110710 loss_ctc 9.677908 loss_rnnt 4.833303 hw_loss 0.250682 lr 0.00025863 rank 0
2023-03-01 18:21:56,411 DEBUG TRAIN Batch 55/10200 loss 5.139890 loss_att 5.557751 loss_ctc 8.516611 loss_rnnt 4.456429 hw_loss 0.280613 lr 0.00025862 rank 1
2023-03-01 18:21:56,414 DEBUG TRAIN Batch 55/10200 loss 7.450167 loss_att 10.025465 loss_ctc 12.746155 loss_rnnt 6.077548 hw_loss 0.283927 lr 0.00025862 rank 0
2023-03-01 18:22:33,599 DEBUG TRAIN Batch 55/10300 loss 7.807456 loss_att 9.525848 loss_ctc 11.208788 loss_rnnt 6.840814 hw_loss 0.317725 lr 0.00025862 rank 1
2023-03-01 18:22:33,599 DEBUG TRAIN Batch 55/10300 loss 5.677173 loss_att 6.780635 loss_ctc 11.892321 loss_rnnt 4.480625 hw_loss 0.275941 lr 0.00025862 rank 0
2023-03-01 18:22:56,008 DEBUG TRAIN Batch 55/10400 loss 9.957818 loss_att 13.196409 loss_ctc 13.316122 loss_rnnt 8.848230 hw_loss 0.026430 lr 0.00025861 rank 1
2023-03-01 18:22:56,010 DEBUG TRAIN Batch 55/10400 loss 9.341501 loss_att 12.737172 loss_ctc 16.586754 loss_rnnt 7.582990 hw_loss 0.212518 lr 0.00025861 rank 0
2023-03-01 18:23:18,775 DEBUG TRAIN Batch 55/10500 loss 4.077331 loss_att 6.826514 loss_ctc 6.767065 loss_rnnt 3.031404 hw_loss 0.257737 lr 0.00025860 rank 1
2023-03-01 18:23:18,777 DEBUG TRAIN Batch 55/10500 loss 2.290764 loss_att 5.416743 loss_ctc 5.017781 loss_rnnt 1.221297 hw_loss 0.151255 lr 0.00025860 rank 0
2023-03-01 18:23:41,054 DEBUG TRAIN Batch 55/10600 loss 3.790005 loss_att 7.443264 loss_ctc 9.031578 loss_rnnt 2.274653 hw_loss 0.160920 lr 0.00025859 rank 1
2023-03-01 18:23:41,056 DEBUG TRAIN Batch 55/10600 loss 10.006920 loss_att 14.130973 loss_ctc 16.707945 loss_rnnt 8.213381 hw_loss 0.141110 lr 0.00025859 rank 0
2023-03-01 18:24:17,340 DEBUG TRAIN Batch 55/10700 loss 9.713498 loss_att 12.001852 loss_ctc 16.048704 loss_rnnt 8.272590 hw_loss 0.259769 lr 0.00025858 rank 1
2023-03-01 18:24:17,342 DEBUG TRAIN Batch 55/10700 loss 7.175604 loss_att 11.376429 loss_ctc 12.261847 loss_rnnt 5.537138 hw_loss 0.225253 lr 0.00025858 rank 0
2023-03-01 18:24:40,183 DEBUG TRAIN Batch 55/10800 loss 5.608300 loss_att 5.967952 loss_ctc 8.833132 loss_rnnt 4.953596 hw_loss 0.286493 lr 0.00025857 rank 1
2023-03-01 18:24:40,183 DEBUG TRAIN Batch 55/10800 loss 5.044042 loss_att 6.727852 loss_ctc 7.498927 loss_rnnt 4.327116 hw_loss 0.099087 lr 0.00025857 rank 0
2023-03-01 18:25:02,630 DEBUG TRAIN Batch 55/10900 loss 3.656497 loss_att 6.827282 loss_ctc 5.555121 loss_rnnt 2.689710 hw_loss 0.149024 lr 0.00025856 rank 1
2023-03-01 18:25:02,633 DEBUG TRAIN Batch 55/10900 loss 7.230358 loss_att 10.888144 loss_ctc 13.389495 loss_rnnt 5.604081 hw_loss 0.137813 lr 0.00025856 rank 0
2023-03-01 18:25:25,701 DEBUG TRAIN Batch 55/11000 loss 3.530089 loss_att 5.613072 loss_ctc 6.380642 loss_rnnt 2.635230 hw_loss 0.184103 lr 0.00025856 rank 1
2023-03-01 18:25:25,704 DEBUG TRAIN Batch 55/11000 loss 2.401090 loss_att 5.308666 loss_ctc 4.203642 loss_rnnt 1.495182 hw_loss 0.157597 lr 0.00025856 rank 0
2023-03-01 18:26:01,895 DEBUG TRAIN Batch 55/11100 loss 2.473758 loss_att 5.819480 loss_ctc 4.498703 loss_rnnt 1.393255 hw_loss 0.265063 lr 0.00025855 rank 1
2023-03-01 18:26:01,896 DEBUG TRAIN Batch 55/11100 loss 11.981767 loss_att 13.745355 loss_ctc 16.820065 loss_rnnt 10.915667 hw_loss 0.128015 lr 0.00025855 rank 0
2023-03-01 18:26:24,714 DEBUG TRAIN Batch 55/11200 loss 9.917870 loss_att 11.803508 loss_ctc 14.330263 loss_rnnt 8.873184 hw_loss 0.148572 lr 0.00025854 rank 1
2023-03-01 18:26:24,715 DEBUG TRAIN Batch 55/11200 loss 13.451835 loss_att 15.540298 loss_ctc 23.292875 loss_rnnt 11.654449 hw_loss 0.126664 lr 0.00025854 rank 0
2023-03-01 18:26:47,225 DEBUG TRAIN Batch 55/11300 loss 3.885285 loss_att 5.681295 loss_ctc 6.476838 loss_rnnt 3.052120 hw_loss 0.240793 lr 0.00025853 rank 1
2023-03-01 18:26:47,226 DEBUG TRAIN Batch 55/11300 loss 2.951938 loss_att 5.951934 loss_ctc 6.516060 loss_rnnt 1.753611 hw_loss 0.230833 lr 0.00025853 rank 0
2023-03-01 18:27:23,279 DEBUG TRAIN Batch 55/11400 loss 4.733388 loss_att 5.948914 loss_ctc 7.879171 loss_rnnt 3.879929 hw_loss 0.357967 lr 0.00025852 rank 1
2023-03-01 18:27:23,280 DEBUG TRAIN Batch 55/11400 loss 9.315957 loss_att 11.013793 loss_ctc 12.626949 loss_rnnt 8.408774 hw_loss 0.236533 lr 0.00025852 rank 0
2023-03-01 18:27:45,750 DEBUG TRAIN Batch 55/11500 loss 7.381579 loss_att 9.859686 loss_ctc 8.453184 loss_rnnt 6.573519 hw_loss 0.317923 lr 0.00025851 rank 1
2023-03-01 18:27:45,751 DEBUG TRAIN Batch 55/11500 loss 5.866591 loss_att 8.147547 loss_ctc 8.838851 loss_rnnt 4.942504 hw_loss 0.134240 lr 0.00025851 rank 0
2023-03-01 18:28:08,443 DEBUG TRAIN Batch 55/11600 loss 9.669191 loss_att 12.579600 loss_ctc 16.533224 loss_rnnt 8.134121 hw_loss 0.070847 lr 0.00025850 rank 1
2023-03-01 18:28:08,444 DEBUG TRAIN Batch 55/11600 loss 4.136162 loss_att 6.042580 loss_ctc 4.726244 loss_rnnt 3.532947 hw_loss 0.268601 lr 0.00025850 rank 0
2023-03-01 18:28:30,825 DEBUG TRAIN Batch 55/11700 loss 13.162513 loss_att 17.456600 loss_ctc 22.772293 loss_rnnt 10.935720 hw_loss 0.162508 lr 0.00025849 rank 1
2023-03-01 18:28:30,827 DEBUG TRAIN Batch 55/11700 loss 6.009756 loss_att 8.827154 loss_ctc 8.921106 loss_rnnt 5.003259 hw_loss 0.102820 lr 0.00025849 rank 0
2023-03-01 18:29:07,350 DEBUG TRAIN Batch 55/11800 loss 13.090261 loss_att 19.160824 loss_ctc 27.551773 loss_rnnt 9.896864 hw_loss 0.095783 lr 0.00025849 rank 1
2023-03-01 18:29:07,353 DEBUG TRAIN Batch 55/11800 loss 7.130795 loss_att 10.223979 loss_ctc 9.684902 loss_rnnt 6.110719 hw_loss 0.114170 lr 0.00025849 rank 0
2023-03-01 18:29:29,700 DEBUG TRAIN Batch 55/11900 loss 2.500233 loss_att 4.197052 loss_ctc 3.650984 loss_rnnt 1.893815 hw_loss 0.213039 lr 0.00025848 rank 1
2023-03-01 18:29:29,702 DEBUG TRAIN Batch 55/11900 loss 13.218324 loss_att 12.697318 loss_ctc 17.158442 loss_rnnt 12.661766 hw_loss 0.253893 lr 0.00025848 rank 0
2023-03-01 18:29:52,574 DEBUG TRAIN Batch 55/12000 loss 3.940350 loss_att 5.311061 loss_ctc 6.278905 loss_rnnt 3.271080 hw_loss 0.156223 lr 0.00025847 rank 1
2023-03-01 18:29:52,575 DEBUG TRAIN Batch 55/12000 loss 6.085631 loss_att 9.885301 loss_ctc 10.991704 loss_rnnt 4.555250 hw_loss 0.218070 lr 0.00025847 rank 0
2023-03-01 18:30:28,903 DEBUG TRAIN Batch 55/12100 loss 8.237174 loss_att 11.655263 loss_ctc 11.927955 loss_rnnt 6.974995 hw_loss 0.162109 lr 0.00025846 rank 1
2023-03-01 18:30:28,904 DEBUG TRAIN Batch 55/12100 loss 11.416746 loss_att 16.271883 loss_ctc 27.782326 loss_rnnt 8.164883 hw_loss 0.185173 lr 0.00025846 rank 0
2023-03-01 18:30:51,573 DEBUG TRAIN Batch 55/12200 loss 7.614869 loss_att 13.184916 loss_ctc 15.036685 loss_rnnt 5.344993 hw_loss 0.311797 lr 0.00025845 rank 1
2023-03-01 18:30:51,575 DEBUG TRAIN Batch 55/12200 loss 7.560385 loss_att 10.058883 loss_ctc 9.852221 loss_rnnt 6.622909 hw_loss 0.247872 lr 0.00025845 rank 0
2023-03-01 18:31:14,290 DEBUG TRAIN Batch 55/12300 loss 1.749716 loss_att 4.932805 loss_ctc 4.367640 loss_rnnt 0.708975 hw_loss 0.103250 lr 0.00025844 rank 1
2023-03-01 18:31:14,290 DEBUG TRAIN Batch 55/12300 loss 9.836348 loss_att 11.665801 loss_ctc 12.498562 loss_rnnt 9.030570 hw_loss 0.159235 lr 0.00025844 rank 0
2023-03-01 18:31:36,222 DEBUG TRAIN Batch 55/12400 loss 4.177604 loss_att 7.704967 loss_ctc 6.428819 loss_rnnt 3.076376 hw_loss 0.179237 lr 0.00025843 rank 1
2023-03-01 18:31:36,223 DEBUG TRAIN Batch 55/12400 loss 7.167809 loss_att 9.773834 loss_ctc 10.852783 loss_rnnt 6.036083 hw_loss 0.223482 lr 0.00025843 rank 0
2023-03-01 18:32:14,017 DEBUG TRAIN Batch 55/12500 loss 2.598197 loss_att 4.894574 loss_ctc 4.758952 loss_rnnt 1.740097 hw_loss 0.207607 lr 0.00025843 rank 1
2023-03-01 18:32:14,017 DEBUG TRAIN Batch 55/12500 loss 2.779246 loss_att 8.089415 loss_ctc 9.011404 loss_rnnt 0.836795 hw_loss 0.092743 lr 0.00025843 rank 0
2023-03-01 18:32:35,740 DEBUG TRAIN Batch 55/12600 loss 4.021902 loss_att 5.837477 loss_ctc 7.452963 loss_rnnt 3.174210 hw_loss 0.050816 lr 0.00025842 rank 1
2023-03-01 18:32:35,741 DEBUG TRAIN Batch 55/12600 loss 9.154403 loss_att 13.688811 loss_ctc 16.601292 loss_rnnt 7.205986 hw_loss 0.091155 lr 0.00025842 rank 0
2023-03-01 18:32:58,373 DEBUG TRAIN Batch 55/12700 loss 5.339029 loss_att 9.359563 loss_ctc 7.618491 loss_rnnt 4.194247 hw_loss 0.068900 lr 0.00025841 rank 1
2023-03-01 18:32:58,375 DEBUG TRAIN Batch 55/12700 loss 5.485709 loss_att 6.521175 loss_ctc 6.963076 loss_rnnt 4.932962 hw_loss 0.278758 lr 0.00025841 rank 0
2023-03-01 18:33:34,238 DEBUG TRAIN Batch 55/12800 loss 11.995368 loss_att 14.627487 loss_ctc 18.027159 loss_rnnt 10.542413 hw_loss 0.229297 lr 0.00025840 rank 1
2023-03-01 18:33:34,240 DEBUG TRAIN Batch 55/12800 loss 6.023440 loss_att 8.804731 loss_ctc 11.159273 loss_rnnt 4.623931 hw_loss 0.297137 lr 0.00025840 rank 0
2023-03-01 18:33:56,582 DEBUG TRAIN Batch 55/12900 loss 8.161467 loss_att 8.273215 loss_ctc 10.996560 loss_rnnt 7.624704 hw_loss 0.255751 lr 0.00025839 rank 0
2023-03-01 18:33:56,583 DEBUG TRAIN Batch 55/12900 loss 7.324517 loss_att 10.085601 loss_ctc 14.391363 loss_rnnt 5.723742 hw_loss 0.199334 lr 0.00025839 rank 1
2023-03-01 18:34:18,711 DEBUG TRAIN Batch 55/13000 loss 18.533569 loss_att 21.028360 loss_ctc 24.712276 loss_rnnt 17.089201 hw_loss 0.227966 lr 0.00025838 rank 1
2023-03-01 18:34:18,712 DEBUG TRAIN Batch 55/13000 loss 6.850095 loss_att 11.275050 loss_ctc 11.579047 loss_rnnt 5.315601 hw_loss 0.035579 lr 0.00025838 rank 0
2023-03-01 18:34:40,835 DEBUG TRAIN Batch 55/13100 loss 3.657048 loss_att 6.465921 loss_ctc 7.030519 loss_rnnt 2.516739 hw_loss 0.241384 lr 0.00025837 rank 0
2023-03-01 18:34:40,849 DEBUG TRAIN Batch 55/13100 loss 7.447533 loss_att 8.433638 loss_ctc 9.140287 loss_rnnt 6.961498 hw_loss 0.118337 lr 0.00025837 rank 1
2023-03-01 18:35:16,560 DEBUG TRAIN Batch 55/13200 loss 11.614196 loss_att 15.526226 loss_ctc 18.376633 loss_rnnt 9.850159 hw_loss 0.149948 lr 0.00025837 rank 0
2023-03-01 18:35:16,560 DEBUG TRAIN Batch 55/13200 loss 8.431163 loss_att 11.473073 loss_ctc 13.113228 loss_rnnt 7.136909 hw_loss 0.115496 lr 0.00025837 rank 1
2023-03-01 18:35:38,494 DEBUG TRAIN Batch 55/13300 loss 10.435089 loss_att 12.448620 loss_ctc 18.770618 loss_rnnt 8.790236 hw_loss 0.245144 lr 0.00025836 rank 1
2023-03-01 18:35:38,495 DEBUG TRAIN Batch 55/13300 loss 9.746387 loss_att 10.553593 loss_ctc 15.165230 loss_rnnt 8.814161 hw_loss 0.090510 lr 0.00025836 rank 0
2023-03-01 18:36:01,102 DEBUG TRAIN Batch 55/13400 loss 4.284122 loss_att 6.678992 loss_ctc 5.896050 loss_rnnt 3.447509 hw_loss 0.267590 lr 0.00025835 rank 1
2023-03-01 18:36:01,103 DEBUG TRAIN Batch 55/13400 loss 5.742644 loss_att 10.615702 loss_ctc 9.245442 loss_rnnt 4.204319 hw_loss 0.181263 lr 0.00025835 rank 0
2023-03-01 18:36:37,633 DEBUG TRAIN Batch 55/13500 loss 2.248917 loss_att 5.522638 loss_ctc 4.472098 loss_rnnt 1.218156 hw_loss 0.149235 lr 0.00025834 rank 1
2023-03-01 18:36:37,679 DEBUG TRAIN Batch 55/13500 loss 5.990400 loss_att 6.505402 loss_ctc 8.963809 loss_rnnt 5.358251 hw_loss 0.248802 lr 0.00025834 rank 0
2023-03-01 18:37:00,089 DEBUG TRAIN Batch 55/13600 loss 1.968321 loss_att 5.007806 loss_ctc 4.397041 loss_rnnt 0.941014 hw_loss 0.179215 lr 0.00025833 rank 1
2023-03-01 18:37:00,091 DEBUG TRAIN Batch 55/13600 loss 5.007727 loss_att 10.437790 loss_ctc 8.085232 loss_rnnt 3.404917 hw_loss 0.199618 lr 0.00025833 rank 0
2023-03-01 18:37:23,018 DEBUG TRAIN Batch 55/13700 loss 6.272636 loss_att 9.287397 loss_ctc 12.734345 loss_rnnt 4.745429 hw_loss 0.117550 lr 0.00025832 rank 1
2023-03-01 18:37:23,019 DEBUG TRAIN Batch 55/13700 loss 4.622387 loss_att 8.054398 loss_ctc 7.792544 loss_rnnt 3.398712 hw_loss 0.214848 lr 0.00025832 rank 0
2023-03-01 18:37:46,084 DEBUG TRAIN Batch 55/13800 loss 4.954234 loss_att 8.489210 loss_ctc 7.622796 loss_rnnt 3.777901 hw_loss 0.212868 lr 0.00025831 rank 1
2023-03-01 18:37:46,086 DEBUG TRAIN Batch 55/13800 loss 1.810132 loss_att 4.609166 loss_ctc 3.018969 loss_rnnt 1.067240 hw_loss 0.041074 lr 0.00025831 rank 0
2023-03-01 18:38:22,726 DEBUG TRAIN Batch 55/13900 loss 6.569551 loss_att 7.963477 loss_ctc 8.616585 loss_rnnt 5.906423 hw_loss 0.208885 lr 0.00025830 rank 1
2023-03-01 18:38:22,729 DEBUG TRAIN Batch 55/13900 loss 6.150130 loss_att 7.482398 loss_ctc 7.697401 loss_rnnt 5.587329 hw_loss 0.168834 lr 0.00025830 rank 0
2023-03-01 18:38:45,546 DEBUG TRAIN Batch 55/14000 loss 5.169289 loss_att 5.959637 loss_ctc 8.260534 loss_rnnt 4.507894 hw_loss 0.170925 lr 0.00025830 rank 1
2023-03-01 18:38:45,548 DEBUG TRAIN Batch 55/14000 loss 4.562555 loss_att 6.506489 loss_ctc 8.805361 loss_rnnt 3.520835 hw_loss 0.163548 lr 0.00025830 rank 0
2023-03-01 18:39:08,208 DEBUG TRAIN Batch 55/14100 loss 4.770486 loss_att 5.881037 loss_ctc 8.802476 loss_rnnt 3.884709 hw_loss 0.236378 lr 0.00025829 rank 0
2023-03-01 18:39:08,208 DEBUG TRAIN Batch 55/14100 loss 7.010522 loss_att 9.871563 loss_ctc 12.753019 loss_rnnt 5.565705 hw_loss 0.200516 lr 0.00025829 rank 1
2023-03-01 18:39:30,813 DEBUG TRAIN Batch 55/14200 loss 16.423340 loss_att 19.343143 loss_ctc 22.047611 loss_rnnt 15.052741 hw_loss 0.068881 lr 0.00025828 rank 1
2023-03-01 18:39:30,815 DEBUG TRAIN Batch 55/14200 loss 7.630401 loss_att 13.546980 loss_ctc 14.790379 loss_rnnt 5.435366 hw_loss 0.106980 lr 0.00025828 rank 0
2023-03-01 18:40:06,193 DEBUG TRAIN Batch 55/14300 loss 6.275977 loss_att 9.791958 loss_ctc 5.955296 loss_rnnt 5.528186 hw_loss 0.163786 lr 0.00025827 rank 1
2023-03-01 18:40:06,193 DEBUG TRAIN Batch 55/14300 loss 11.374408 loss_att 11.043476 loss_ctc 15.193186 loss_rnnt 10.828751 hw_loss 0.192510 lr 0.00025827 rank 0
2023-03-01 18:40:28,717 DEBUG TRAIN Batch 55/14400 loss 4.791336 loss_att 9.076441 loss_ctc 9.256055 loss_rnnt 3.277695 hw_loss 0.114984 lr 0.00025826 rank 1
2023-03-01 18:40:28,719 DEBUG TRAIN Batch 55/14400 loss 3.390460 loss_att 4.912797 loss_ctc 5.615555 loss_rnnt 2.623921 hw_loss 0.310109 lr 0.00025826 rank 0
2023-03-01 18:40:51,815 DEBUG TRAIN Batch 55/14500 loss 2.936042 loss_att 7.146584 loss_ctc 7.310279 loss_rnnt 1.356363 hw_loss 0.289386 lr 0.00025825 rank 1
2023-03-01 18:40:51,817 DEBUG TRAIN Batch 55/14500 loss 7.683922 loss_att 11.113695 loss_ctc 12.808338 loss_rnnt 6.194932 hw_loss 0.224588 lr 0.00025825 rank 0
2023-03-01 18:41:27,838 DEBUG TRAIN Batch 55/14600 loss 13.211545 loss_att 16.165623 loss_ctc 23.232635 loss_rnnt 11.152197 hw_loss 0.248226 lr 0.00025824 rank 1
2023-03-01 18:41:27,839 DEBUG TRAIN Batch 55/14600 loss 8.045048 loss_att 10.477798 loss_ctc 15.360703 loss_rnnt 6.480396 hw_loss 0.192524 lr 0.00025824 rank 0
2023-03-01 18:41:50,453 DEBUG TRAIN Batch 55/14700 loss 3.844905 loss_att 8.070201 loss_ctc 11.271349 loss_rnnt 1.904683 hw_loss 0.196820 lr 0.00025824 rank 1
2023-03-01 18:41:50,455 DEBUG TRAIN Batch 55/14700 loss 8.060136 loss_att 11.146520 loss_ctc 14.702161 loss_rnnt 6.513213 hw_loss 0.082581 lr 0.00025824 rank 0
2023-03-01 18:42:13,337 DEBUG TRAIN Batch 55/14800 loss 2.149261 loss_att 5.092343 loss_ctc 5.806659 loss_rnnt 0.943456 hw_loss 0.242879 lr 0.00025823 rank 1
2023-03-01 18:42:13,339 DEBUG TRAIN Batch 55/14800 loss 7.830298 loss_att 12.702112 loss_ctc 13.109346 loss_rnnt 6.028347 hw_loss 0.231966 lr 0.00025823 rank 0
2023-03-01 18:42:36,149 DEBUG TRAIN Batch 55/14900 loss 3.660449 loss_att 6.338360 loss_ctc 5.649729 loss_rnnt 2.732896 hw_loss 0.237624 lr 0.00025822 rank 1
2023-03-01 18:42:36,149 DEBUG TRAIN Batch 55/14900 loss 4.298605 loss_att 6.189802 loss_ctc 9.662275 loss_rnnt 3.038961 hw_loss 0.311716 lr 0.00025822 rank 0
2023-03-01 18:43:10,582 DEBUG TRAIN Batch 55/15000 loss 5.035426 loss_att 7.425965 loss_ctc 9.198435 loss_rnnt 3.907881 hw_loss 0.176941 lr 0.00025821 rank 1
2023-03-01 18:43:10,584 DEBUG TRAIN Batch 55/15000 loss 4.739508 loss_att 6.532889 loss_ctc 7.480871 loss_rnnt 3.946496 hw_loss 0.129038 lr 0.00025821 rank 0
2023-03-01 18:43:33,262 DEBUG TRAIN Batch 55/15100 loss 14.666220 loss_att 15.847837 loss_ctc 21.122990 loss_rnnt 13.485286 hw_loss 0.156950 lr 0.00025820 rank 1
2023-03-01 18:43:33,263 DEBUG TRAIN Batch 55/15100 loss 7.997932 loss_att 11.999506 loss_ctc 12.004400 loss_rnnt 6.566036 hw_loss 0.182598 lr 0.00025820 rank 0
2023-03-01 18:43:56,251 DEBUG TRAIN Batch 55/15200 loss 5.700891 loss_att 7.570581 loss_ctc 10.426316 loss_rnnt 4.622947 hw_loss 0.138653 lr 0.00025819 rank 1
2023-03-01 18:43:56,253 DEBUG TRAIN Batch 55/15200 loss 8.306371 loss_att 10.767670 loss_ctc 12.690163 loss_rnnt 7.084507 hw_loss 0.272059 lr 0.00025819 rank 0
2023-03-01 18:44:31,987 DEBUG TRAIN Batch 55/15300 loss 2.976832 loss_att 8.180010 loss_ctc 6.190038 loss_rnnt 1.406503 hw_loss 0.189872 lr 0.00025818 rank 1
2023-03-01 18:44:31,989 DEBUG TRAIN Batch 55/15300 loss 7.264570 loss_att 8.661081 loss_ctc 12.708008 loss_rnnt 6.157262 hw_loss 0.191650 lr 0.00025818 rank 0
2023-03-01 18:44:54,383 DEBUG TRAIN Batch 55/15400 loss 15.932076 loss_att 20.144985 loss_ctc 27.201162 loss_rnnt 13.514033 hw_loss 0.136717 lr 0.00025818 rank 1
2023-03-01 18:44:54,385 DEBUG TRAIN Batch 55/15400 loss 5.695767 loss_att 7.617506 loss_ctc 8.761389 loss_rnnt 4.795367 hw_loss 0.201193 lr 0.00025818 rank 0
2023-03-01 18:45:16,806 DEBUG TRAIN Batch 55/15500 loss 8.801975 loss_att 11.934971 loss_ctc 13.430911 loss_rnnt 7.422788 hw_loss 0.253868 lr 0.00025817 rank 1
2023-03-01 18:45:16,806 DEBUG TRAIN Batch 55/15500 loss 8.346437 loss_att 10.359401 loss_ctc 11.308899 loss_rnnt 7.487542 hw_loss 0.114949 lr 0.00025817 rank 0
2023-03-01 18:45:39,655 DEBUG TRAIN Batch 55/15600 loss 4.733801 loss_att 8.699809 loss_ctc 10.344316 loss_rnnt 3.065210 hw_loss 0.238726 lr 0.00025816 rank 1
2023-03-01 18:45:39,655 DEBUG TRAIN Batch 55/15600 loss 7.449759 loss_att 12.067215 loss_ctc 11.750135 loss_rnnt 5.878399 hw_loss 0.139660 lr 0.00025816 rank 0
2023-03-01 18:46:14,220 DEBUG TRAIN Batch 55/15700 loss 6.620725 loss_att 9.261452 loss_ctc 8.426067 loss_rnnt 5.716728 hw_loss 0.253388 lr 0.00025815 rank 1
2023-03-01 18:46:14,222 DEBUG TRAIN Batch 55/15700 loss 8.255583 loss_att 13.503336 loss_ctc 14.406858 loss_rnnt 6.352523 hw_loss 0.062511 lr 0.00025815 rank 0
2023-03-01 18:46:36,550 DEBUG TRAIN Batch 55/15800 loss 10.114697 loss_att 13.637529 loss_ctc 15.423775 loss_rnnt 8.598647 hw_loss 0.194262 lr 0.00025814 rank 1
2023-03-01 18:46:36,553 DEBUG TRAIN Batch 55/15800 loss 2.698964 loss_att 7.171446 loss_ctc 4.449764 loss_rnnt 1.498041 hw_loss 0.136851 lr 0.00025814 rank 0
2023-03-01 18:46:59,272 DEBUG TRAIN Batch 55/15900 loss 3.346742 loss_att 6.806450 loss_ctc 9.028322 loss_rnnt 1.788733 hw_loss 0.203483 lr 0.00025813 rank 1
2023-03-01 18:46:59,274 DEBUG TRAIN Batch 55/15900 loss 10.102101 loss_att 11.534516 loss_ctc 15.581473 loss_rnnt 8.988091 hw_loss 0.181771 lr 0.00025813 rank 0
2023-03-01 18:47:35,130 DEBUG TRAIN Batch 55/16000 loss 10.830371 loss_att 15.497751 loss_ctc 13.666212 loss_rnnt 9.396822 hw_loss 0.228674 lr 0.00025812 rank 1
2023-03-01 18:47:35,131 DEBUG TRAIN Batch 55/16000 loss 8.917186 loss_att 11.479244 loss_ctc 13.674173 loss_rnnt 7.604885 hw_loss 0.310544 lr 0.00025812 rank 0
2023-03-01 18:47:57,729 DEBUG TRAIN Batch 55/16100 loss 3.896428 loss_att 8.210306 loss_ctc 8.662417 loss_rnnt 2.315672 hw_loss 0.154716 lr 0.00025812 rank 1
2023-03-01 18:47:57,731 DEBUG TRAIN Batch 55/16100 loss 9.453200 loss_att 15.691730 loss_ctc 18.425625 loss_rnnt 6.838116 hw_loss 0.320727 lr 0.00025812 rank 0
2023-03-01 18:48:20,391 DEBUG TRAIN Batch 55/16200 loss 9.028425 loss_att 11.547284 loss_ctc 14.606087 loss_rnnt 7.667718 hw_loss 0.212339 lr 0.00025811 rank 1
2023-03-01 18:48:20,391 DEBUG TRAIN Batch 55/16200 loss 3.322571 loss_att 5.724726 loss_ctc 6.188090 loss_rnnt 2.359655 hw_loss 0.188280 lr 0.00025811 rank 0
2023-03-01 18:48:43,054 DEBUG TRAIN Batch 55/16300 loss 3.549576 loss_att 6.216196 loss_ctc 7.757130 loss_rnnt 2.288018 hw_loss 0.313550 lr 0.00025810 rank 1
2023-03-01 18:48:43,056 DEBUG TRAIN Batch 55/16300 loss 2.840510 loss_att 5.276800 loss_ctc 4.056048 loss_rnnt 2.099818 hw_loss 0.171304 lr 0.00025810 rank 0
2023-03-01 18:49:18,035 DEBUG TRAIN Batch 55/16400 loss 10.609843 loss_att 12.412694 loss_ctc 18.904211 loss_rnnt 9.081325 hw_loss 0.116311 lr 0.00025809 rank 1
2023-03-01 18:49:18,037 DEBUG TRAIN Batch 55/16400 loss 3.899373 loss_att 6.145500 loss_ctc 8.758097 loss_rnnt 2.707211 hw_loss 0.178325 lr 0.00025809 rank 0
2023-03-01 18:49:40,833 DEBUG TRAIN Batch 55/16500 loss 4.797906 loss_att 5.534821 loss_ctc 8.343184 loss_rnnt 4.035787 hw_loss 0.266311 lr 0.00025808 rank 1
2023-03-01 18:49:40,836 DEBUG TRAIN Batch 55/16500 loss 5.697246 loss_att 7.844773 loss_ctc 8.381244 loss_rnnt 4.856662 hw_loss 0.099772 lr 0.00025808 rank 0
2023-03-01 18:50:03,311 DEBUG TRAIN Batch 55/16600 loss 9.729420 loss_att 10.613619 loss_ctc 15.266711 loss_rnnt 8.755719 hw_loss 0.109789 lr 0.00025807 rank 0
2023-03-01 18:50:03,313 DEBUG TRAIN Batch 55/16600 loss 8.955036 loss_att 12.786549 loss_ctc 13.485558 loss_rnnt 7.477788 hw_loss 0.200393 lr 0.00025807 rank 1
2023-03-01 18:50:38,355 DEBUG TRAIN Batch 55/16700 loss 5.259315 loss_att 7.323020 loss_ctc 7.189545 loss_rnnt 4.542956 hw_loss 0.086725 lr 0.00025806 rank 1
2023-03-01 18:50:38,401 DEBUG TRAIN Batch 55/16700 loss 7.884769 loss_att 8.349383 loss_ctc 11.467155 loss_rnnt 7.147719 hw_loss 0.312142 lr 0.00025806 rank 0
2023-03-01 18:51:01,417 DEBUG TRAIN Batch 55/16800 loss 4.730052 loss_att 7.083966 loss_ctc 6.370436 loss_rnnt 3.920209 hw_loss 0.225640 lr 0.00025806 rank 1
2023-03-01 18:51:01,417 DEBUG TRAIN Batch 55/16800 loss 3.882289 loss_att 7.103159 loss_ctc 5.264419 loss_rnnt 2.927953 hw_loss 0.236021 lr 0.00025806 rank 0
2023-03-01 18:51:24,020 DEBUG TRAIN Batch 55/16900 loss 2.707192 loss_att 6.643019 loss_ctc 5.229896 loss_rnnt 1.471002 hw_loss 0.211245 lr 0.00025805 rank 1
2023-03-01 18:51:24,021 DEBUG TRAIN Batch 55/16900 loss 8.820384 loss_att 12.503393 loss_ctc 15.227332 loss_rnnt 7.109372 hw_loss 0.225281 lr 0.00025805 rank 0
2023-03-01 18:51:46,701 DEBUG TRAIN Batch 55/17000 loss 5.382975 loss_att 8.032943 loss_ctc 8.388094 loss_rnnt 4.349419 hw_loss 0.192897 lr 0.00025804 rank 1
2023-03-01 18:51:46,702 DEBUG TRAIN Batch 55/17000 loss 13.139059 loss_att 13.848372 loss_ctc 21.358873 loss_rnnt 11.789215 hw_loss 0.210014 lr 0.00025804 rank 0
2023-03-01 18:52:23,312 DEBUG TRAIN Batch 55/17100 loss 5.718973 loss_att 7.345358 loss_ctc 8.262846 loss_rnnt 4.931005 hw_loss 0.231577 lr 0.00025803 rank 1
2023-03-01 18:52:23,314 DEBUG TRAIN Batch 55/17100 loss 6.288982 loss_att 9.661683 loss_ctc 9.309599 loss_rnnt 5.144599 hw_loss 0.125802 lr 0.00025803 rank 0
2023-03-01 18:52:45,239 DEBUG TRAIN Batch 55/17200 loss 4.812906 loss_att 8.159877 loss_ctc 9.821590 loss_rnnt 3.430714 hw_loss 0.084326 lr 0.00025802 rank 1
2023-03-01 18:52:45,240 DEBUG TRAIN Batch 55/17200 loss 8.632674 loss_att 11.004951 loss_ctc 20.671925 loss_rnnt 6.527457 hw_loss 0.047865 lr 0.00025802 rank 0
2023-03-01 18:53:07,821 DEBUG TRAIN Batch 55/17300 loss 5.568601 loss_att 10.364641 loss_ctc 11.838696 loss_rnnt 3.731660 hw_loss 0.078225 lr 0.00025801 rank 1
2023-03-01 18:53:07,821 DEBUG TRAIN Batch 55/17300 loss 4.621968 loss_att 5.829836 loss_ctc 6.558454 loss_rnnt 3.998737 hw_loss 0.231487 lr 0.00025801 rank 0
2023-03-01 18:53:30,567 DEBUG TRAIN Batch 55/17400 loss 7.995959 loss_att 7.978972 loss_ctc 11.363365 loss_rnnt 7.511482 hw_loss 0.072913 lr 0.00025800 rank 1
2023-03-01 18:53:30,570 DEBUG TRAIN Batch 55/17400 loss 7.599995 loss_att 10.000032 loss_ctc 9.387524 loss_rnnt 6.749519 hw_loss 0.247746 lr 0.00025800 rank 0
2023-03-01 18:54:06,108 DEBUG TRAIN Batch 55/17500 loss 2.821876 loss_att 5.347903 loss_ctc 4.370302 loss_rnnt 1.986401 hw_loss 0.232148 lr 0.00025800 rank 1
2023-03-01 18:54:06,110 DEBUG TRAIN Batch 55/17500 loss 4.713367 loss_att 10.451609 loss_ctc 11.340010 loss_rnnt 2.627797 hw_loss 0.101942 lr 0.00025800 rank 0
2023-03-01 18:54:28,679 DEBUG TRAIN Batch 55/17600 loss 7.552413 loss_att 11.303994 loss_ctc 9.475491 loss_rnnt 6.484087 hw_loss 0.115499 lr 0.00025799 rank 1
2023-03-01 18:54:28,681 DEBUG TRAIN Batch 55/17600 loss 2.580183 loss_att 4.272424 loss_ctc 3.860774 loss_rnnt 1.993898 hw_loss 0.144545 lr 0.00025799 rank 0
2023-03-01 18:54:50,520 DEBUG TRAIN Batch 55/17700 loss 8.550613 loss_att 10.317670 loss_ctc 14.645239 loss_rnnt 7.284820 hw_loss 0.187060 lr 0.00025798 rank 1
2023-03-01 18:54:50,520 DEBUG TRAIN Batch 55/17700 loss 7.523894 loss_att 9.157841 loss_ctc 16.342018 loss_rnnt 5.883390 hw_loss 0.258683 lr 0.00025798 rank 0
2023-03-01 18:55:25,862 DEBUG TRAIN Batch 55/17800 loss 9.575608 loss_att 11.849346 loss_ctc 14.941202 loss_rnnt 8.293322 hw_loss 0.210237 lr 0.00025797 rank 1
2023-03-01 18:55:25,864 DEBUG TRAIN Batch 55/17800 loss 11.367351 loss_att 13.791840 loss_ctc 18.080410 loss_rnnt 9.929024 hw_loss 0.109415 lr 0.00025797 rank 0
2023-03-01 18:55:48,120 DEBUG TRAIN Batch 55/17900 loss 6.773419 loss_att 10.100187 loss_ctc 8.717405 loss_rnnt 5.771821 hw_loss 0.144462 lr 0.00025796 rank 1
2023-03-01 18:55:48,120 DEBUG TRAIN Batch 55/17900 loss 10.080878 loss_att 12.950109 loss_ctc 18.702452 loss_rnnt 8.232553 hw_loss 0.234254 lr 0.00025796 rank 0
2023-03-01 18:56:10,892 DEBUG TRAIN Batch 55/18000 loss 7.193593 loss_att 9.767550 loss_ctc 10.268814 loss_rnnt 6.156236 hw_loss 0.211005 lr 0.00025795 rank 1
2023-03-01 18:56:10,894 DEBUG TRAIN Batch 55/18000 loss 6.429378 loss_att 9.196847 loss_ctc 12.877678 loss_rnnt 4.927700 hw_loss 0.165770 lr 0.00025795 rank 0
2023-03-01 18:56:33,148 DEBUG TRAIN Batch 55/18100 loss 1.963082 loss_att 4.921503 loss_ctc 2.966620 loss_rnnt 1.167619 hw_loss 0.131202 lr 0.00025794 rank 1
2023-03-01 18:56:33,149 DEBUG TRAIN Batch 55/18100 loss 4.155536 loss_att 8.708138 loss_ctc 8.792057 loss_rnnt 2.510062 hw_loss 0.218907 lr 0.00025794 rank 0
2023-03-01 18:57:08,791 DEBUG TRAIN Batch 55/18200 loss 12.518305 loss_att 17.762495 loss_ctc 25.285364 loss_rnnt 9.679347 hw_loss 0.164710 lr 0.00025794 rank 1
2023-03-01 18:57:08,792 DEBUG TRAIN Batch 55/18200 loss 8.739198 loss_att 12.402525 loss_ctc 20.493374 loss_rnnt 6.389847 hw_loss 0.092741 lr 0.00025794 rank 0
2023-03-01 18:57:31,325 DEBUG TRAIN Batch 55/18300 loss 4.422570 loss_att 6.179503 loss_ctc 9.260389 loss_rnnt 3.337981 hw_loss 0.165300 lr 0.00025793 rank 1
2023-03-01 18:57:31,327 DEBUG TRAIN Batch 55/18300 loss 7.716971 loss_att 11.242941 loss_ctc 14.504014 loss_rnnt 5.993369 hw_loss 0.212754 lr 0.00025793 rank 0
2023-03-01 18:57:53,659 DEBUG TRAIN Batch 55/18400 loss 6.667338 loss_att 9.525521 loss_ctc 11.314180 loss_rnnt 5.368297 hw_loss 0.202173 lr 0.00025792 rank 1
2023-03-01 18:57:53,660 DEBUG TRAIN Batch 55/18400 loss 3.934673 loss_att 5.815211 loss_ctc 5.719200 loss_rnnt 3.148592 hw_loss 0.322566 lr 0.00025792 rank 0
2023-03-01 18:58:27,742 DEBUG TRAIN Batch 55/18500 loss 10.419254 loss_att 14.319784 loss_ctc 17.595825 loss_rnnt 8.600536 hw_loss 0.153255 lr 0.00025791 rank 1
2023-03-01 18:58:27,744 DEBUG TRAIN Batch 55/18500 loss 7.579947 loss_att 9.051577 loss_ctc 12.708787 loss_rnnt 6.437737 hw_loss 0.307573 lr 0.00025791 rank 0
2023-03-01 18:58:51,572 DEBUG TRAIN Batch 55/18600 loss 5.437360 loss_att 7.346187 loss_ctc 8.216354 loss_rnnt 4.551702 hw_loss 0.250052 lr 0.00025790 rank 0
2023-03-01 18:58:51,572 DEBUG TRAIN Batch 55/18600 loss 6.242767 loss_att 9.039811 loss_ctc 8.508990 loss_rnnt 5.306444 hw_loss 0.140160 lr 0.00025790 rank 1
2023-03-01 18:59:13,711 DEBUG TRAIN Batch 55/18700 loss 9.135736 loss_att 11.356791 loss_ctc 15.803039 loss_rnnt 7.699205 hw_loss 0.193775 lr 0.00025789 rank 1
2023-03-01 18:59:13,711 DEBUG TRAIN Batch 55/18700 loss 6.541280 loss_att 8.058634 loss_ctc 11.542463 loss_rnnt 5.417926 hw_loss 0.286985 lr 0.00025789 rank 0
2023-03-01 18:59:36,113 DEBUG TRAIN Batch 55/18800 loss 6.616365 loss_att 7.428293 loss_ctc 8.721406 loss_rnnt 6.117928 hw_loss 0.103838 lr 0.00025788 rank 1
2023-03-01 18:59:36,117 DEBUG TRAIN Batch 55/18800 loss 2.633423 loss_att 4.193488 loss_ctc 5.362118 loss_rnnt 1.848356 hw_loss 0.204801 lr 0.00025788 rank 0
2023-03-01 19:00:12,395 DEBUG TRAIN Batch 55/18900 loss 3.407070 loss_att 5.959175 loss_ctc 7.156214 loss_rnnt 2.300597 hw_loss 0.180311 lr 0.00025788 rank 1
2023-03-01 19:00:12,397 DEBUG TRAIN Batch 55/18900 loss 1.721592 loss_att 4.373370 loss_ctc 3.083796 loss_rnnt 0.893860 hw_loss 0.217029 lr 0.00025788 rank 0
2023-03-01 19:00:34,758 DEBUG TRAIN Batch 55/19000 loss 9.408276 loss_att 10.631682 loss_ctc 17.331722 loss_rnnt 7.980694 hw_loss 0.237074 lr 0.00025787 rank 1
2023-03-01 19:00:34,759 DEBUG TRAIN Batch 55/19000 loss 12.181553 loss_att 11.778128 loss_ctc 15.324751 loss_rnnt 11.777143 hw_loss 0.123753 lr 0.00025787 rank 0
2023-03-01 19:00:57,683 DEBUG TRAIN Batch 55/19100 loss 2.338995 loss_att 5.208616 loss_ctc 2.524318 loss_rnnt 1.628607 hw_loss 0.209539 lr 0.00025786 rank 1
2023-03-01 19:00:57,684 DEBUG TRAIN Batch 55/19100 loss 13.308823 loss_att 15.470310 loss_ctc 19.834724 loss_rnnt 11.855095 hw_loss 0.283703 lr 0.00025786 rank 0
2023-03-01 19:01:33,181 DEBUG TRAIN Batch 55/19200 loss 8.042287 loss_att 11.892414 loss_ctc 10.071012 loss_rnnt 6.921718 hw_loss 0.150088 lr 0.00025785 rank 1
2023-03-01 19:01:33,183 DEBUG TRAIN Batch 55/19200 loss 5.725392 loss_att 7.885626 loss_ctc 8.942397 loss_rnnt 4.731094 hw_loss 0.249969 lr 0.00025785 rank 0
2023-03-01 19:01:55,924 DEBUG TRAIN Batch 55/19300 loss 11.504845 loss_att 13.591330 loss_ctc 21.185492 loss_rnnt 9.720556 hw_loss 0.142946 lr 0.00025784 rank 1
2023-03-01 19:01:55,926 DEBUG TRAIN Batch 55/19300 loss 5.997216 loss_att 7.360339 loss_ctc 8.070405 loss_rnnt 5.307364 hw_loss 0.264003 lr 0.00025784 rank 0
2023-03-01 19:02:18,101 DEBUG TRAIN Batch 55/19400 loss 7.039588 loss_att 9.474477 loss_ctc 13.065938 loss_rnnt 5.662959 hw_loss 0.161510 lr 0.00025783 rank 1
2023-03-01 19:02:18,102 DEBUG TRAIN Batch 55/19400 loss 7.673642 loss_att 10.929278 loss_ctc 13.464633 loss_rnnt 6.194379 hw_loss 0.105007 lr 0.00025783 rank 0
2023-03-01 19:02:40,680 DEBUG TRAIN Batch 55/19500 loss 4.810828 loss_att 10.139665 loss_ctc 10.833145 loss_rnnt 2.810892 hw_loss 0.245989 lr 0.00025782 rank 1
2023-03-01 19:02:40,682 DEBUG TRAIN Batch 55/19500 loss 4.627608 loss_att 6.636377 loss_ctc 8.792629 loss_rnnt 3.610761 hw_loss 0.112045 lr 0.00025782 rank 0
2023-03-01 19:03:15,462 DEBUG TRAIN Batch 55/19600 loss 9.883516 loss_att 11.806950 loss_ctc 15.928116 loss_rnnt 8.554076 hw_loss 0.260264 lr 0.00025782 rank 1
2023-03-01 19:03:15,464 DEBUG TRAIN Batch 55/19600 loss 5.218454 loss_att 8.509310 loss_ctc 10.619526 loss_rnnt 3.751555 hw_loss 0.166096 lr 0.00025782 rank 0
2023-03-01 19:03:37,227 DEBUG TRAIN Batch 55/19700 loss 3.029412 loss_att 6.478771 loss_ctc 4.784193 loss_rnnt 2.031274 hw_loss 0.139305 lr 0.00025781 rank 0
2023-03-01 19:03:37,227 DEBUG TRAIN Batch 55/19700 loss 2.723240 loss_att 5.452976 loss_ctc 3.094474 loss_rnnt 2.077604 hw_loss 0.094108 lr 0.00025781 rank 1
2023-03-01 19:03:59,912 DEBUG TRAIN Batch 55/19800 loss 3.179005 loss_att 5.083422 loss_ctc 5.172997 loss_rnnt 2.449761 hw_loss 0.154677 lr 0.00025780 rank 1
2023-03-01 19:03:59,914 DEBUG TRAIN Batch 55/19800 loss 7.474670 loss_att 9.531487 loss_ctc 13.047762 loss_rnnt 6.260239 hw_loss 0.112481 lr 0.00025780 rank 0
2023-03-01 19:04:34,825 DEBUG TRAIN Batch 55/19900 loss 7.751387 loss_att 9.014610 loss_ctc 16.702168 loss_rnnt 6.213458 hw_loss 0.172214 lr 0.00025779 rank 1
2023-03-01 19:04:34,826 DEBUG TRAIN Batch 55/19900 loss 7.165203 loss_att 8.205332 loss_ctc 8.410144 loss_rnnt 6.605728 hw_loss 0.347731 lr 0.00025779 rank 0
2023-03-01 19:04:57,420 DEBUG TRAIN Batch 55/20000 loss 5.056357 loss_att 6.990913 loss_ctc 6.431804 loss_rnnt 4.385391 hw_loss 0.188740 lr 0.00025778 rank 1
2023-03-01 19:04:57,423 DEBUG TRAIN Batch 55/20000 loss 8.262808 loss_att 12.049583 loss_ctc 17.416039 loss_rnnt 6.253247 hw_loss 0.059576 lr 0.00025778 rank 0
2023-03-01 19:05:19,534 DEBUG TRAIN Batch 55/20100 loss 6.613059 loss_att 10.006435 loss_ctc 10.638206 loss_rnnt 5.326155 hw_loss 0.134140 lr 0.00025777 rank 1
2023-03-01 19:05:19,537 DEBUG TRAIN Batch 55/20100 loss 6.737165 loss_att 9.746139 loss_ctc 11.906088 loss_rnnt 5.393142 hw_loss 0.099448 lr 0.00025777 rank 0
2023-03-01 19:05:41,570 DEBUG TRAIN Batch 55/20200 loss 6.942379 loss_att 9.522685 loss_ctc 13.766068 loss_rnnt 5.409584 hw_loss 0.200454 lr 0.00025776 rank 1
2023-03-01 19:05:41,570 DEBUG TRAIN Batch 55/20200 loss 6.059690 loss_att 7.831064 loss_ctc 8.587580 loss_rnnt 5.207715 hw_loss 0.301215 lr 0.00025776 rank 0
2023-03-01 19:06:17,036 DEBUG TRAIN Batch 55/20300 loss 13.428627 loss_att 17.179981 loss_ctc 23.627996 loss_rnnt 11.193295 hw_loss 0.234647 lr 0.00025776 rank 1
2023-03-01 19:06:17,038 DEBUG TRAIN Batch 55/20300 loss 4.563427 loss_att 7.232325 loss_ctc 8.105642 loss_rnnt 3.475293 hw_loss 0.153863 lr 0.00025776 rank 0
2023-03-01 19:06:39,440 DEBUG TRAIN Batch 55/20400 loss 5.541673 loss_att 9.374031 loss_ctc 9.115008 loss_rnnt 4.201071 hw_loss 0.183160 lr 0.00025775 rank 1
2023-03-01 19:06:39,442 DEBUG TRAIN Batch 55/20400 loss 5.919334 loss_att 8.800584 loss_ctc 8.016385 loss_rnnt 4.952172 hw_loss 0.208697 lr 0.00025775 rank 0
2023-03-01 19:07:02,183 DEBUG TRAIN Batch 55/20500 loss 1.924927 loss_att 5.921441 loss_ctc 4.199871 loss_rnnt 0.790654 hw_loss 0.059333 lr 0.00025774 rank 1
2023-03-01 19:07:02,184 DEBUG TRAIN Batch 55/20500 loss 3.562143 loss_att 7.179695 loss_ctc 7.541916 loss_rnnt 2.165621 hw_loss 0.266953 lr 0.00025774 rank 0
2023-03-01 19:07:35,316 DEBUG TRAIN Batch 55/20600 loss 7.836186 loss_att 9.683274 loss_ctc 12.234638 loss_rnnt 6.746482 hw_loss 0.250923 lr 0.00025773 rank 1
2023-03-01 19:07:35,362 DEBUG TRAIN Batch 55/20600 loss 5.845673 loss_att 6.370193 loss_ctc 8.620327 loss_rnnt 5.236887 hw_loss 0.251114 lr 0.00025773 rank 0
2023-03-01 19:07:57,654 DEBUG TRAIN Batch 55/20700 loss 8.769545 loss_att 11.287002 loss_ctc 15.886736 loss_rnnt 7.232689 hw_loss 0.158259 lr 0.00025772 rank 1
2023-03-01 19:07:57,655 DEBUG TRAIN Batch 55/20700 loss 8.428926 loss_att 9.343414 loss_ctc 10.365523 loss_rnnt 7.900452 hw_loss 0.163807 lr 0.00025772 rank 0
2023-03-01 19:08:20,597 DEBUG TRAIN Batch 55/20800 loss 5.803444 loss_att 8.731168 loss_ctc 7.561629 loss_rnnt 4.909994 hw_loss 0.137775 lr 0.00025771 rank 1
2023-03-01 19:08:20,600 DEBUG TRAIN Batch 55/20800 loss 2.204316 loss_att 6.942046 loss_ctc 3.229080 loss_rnnt 1.081438 hw_loss 0.072558 lr 0.00025771 rank 0
2023-03-01 19:08:43,023 DEBUG TRAIN Batch 55/20900 loss 6.544216 loss_att 8.320789 loss_ctc 9.250706 loss_rnnt 5.704125 hw_loss 0.232332 lr 0.00025770 rank 0
2023-03-01 19:08:43,024 DEBUG TRAIN Batch 55/20900 loss 9.778892 loss_att 10.987555 loss_ctc 15.320233 loss_rnnt 8.635796 hw_loss 0.304719 lr 0.00025770 rank 1
2023-03-01 19:09:15,987 DEBUG TRAIN Batch 55/21000 loss 1.422747 loss_att 3.202572 loss_ctc 2.783794 loss_rnnt 0.788870 hw_loss 0.180823 lr 0.00025770 rank 1
2023-03-01 19:09:15,988 DEBUG TRAIN Batch 55/21000 loss 4.755271 loss_att 8.184250 loss_ctc 8.666158 loss_rnnt 3.437284 hw_loss 0.207638 lr 0.00025770 rank 0
2023-03-01 19:09:39,100 DEBUG TRAIN Batch 55/21100 loss 5.806086 loss_att 8.619762 loss_ctc 11.459384 loss_rnnt 4.310430 hw_loss 0.335903 lr 0.00025769 rank 1
2023-03-01 19:09:39,102 DEBUG TRAIN Batch 55/21100 loss 9.932267 loss_att 11.960536 loss_ctc 17.411251 loss_rnnt 8.429188 hw_loss 0.187926 lr 0.00025769 rank 0
2023-03-01 19:10:01,976 DEBUG TRAIN Batch 55/21200 loss 12.038603 loss_att 12.814578 loss_ctc 16.570793 loss_rnnt 11.194962 hw_loss 0.157789 lr 0.00025768 rank 1
2023-03-01 19:10:01,978 DEBUG TRAIN Batch 55/21200 loss 5.929235 loss_att 8.008802 loss_ctc 11.540869 loss_rnnt 4.667580 hw_loss 0.182856 lr 0.00025768 rank 0
2023-03-01 19:10:24,463 DEBUG TRAIN Batch 55/21300 loss 5.924676 loss_att 9.910962 loss_ctc 13.402900 loss_rnnt 4.020669 hw_loss 0.205600 lr 0.00025767 rank 1
2023-03-01 19:10:24,466 DEBUG TRAIN Batch 55/21300 loss 3.134031 loss_att 6.610863 loss_ctc 3.496336 loss_rnnt 2.265193 hw_loss 0.234683 lr 0.00025767 rank 0
2023-03-01 19:10:57,946 DEBUG TRAIN Batch 55/21400 loss 7.026893 loss_att 9.537724 loss_ctc 13.882523 loss_rnnt 5.491663 hw_loss 0.223087 lr 0.00025766 rank 1
2023-03-01 19:10:57,947 DEBUG TRAIN Batch 55/21400 loss 5.954930 loss_att 10.372513 loss_ctc 14.869009 loss_rnnt 3.843573 hw_loss 0.073680 lr 0.00025766 rank 0
2023-03-01 19:11:20,601 DEBUG TRAIN Batch 55/21500 loss 4.543256 loss_att 6.673906 loss_ctc 7.460124 loss_rnnt 3.553224 hw_loss 0.328098 lr 0.00025765 rank 1
2023-03-01 19:11:20,602 DEBUG TRAIN Batch 55/21500 loss 13.350247 loss_att 15.108264 loss_ctc 19.781788 loss_rnnt 12.005060 hw_loss 0.255082 lr 0.00025765 rank 0
2023-03-01 19:11:42,490 DEBUG TRAIN Batch 55/21600 loss 6.419520 loss_att 11.170611 loss_ctc 13.631404 loss_rnnt 4.446917 hw_loss 0.114001 lr 0.00025764 rank 1
2023-03-01 19:11:42,493 DEBUG TRAIN Batch 55/21600 loss 11.276554 loss_att 11.684320 loss_ctc 16.250565 loss_rnnt 10.465126 hw_loss 0.125012 lr 0.00025764 rank 0
2023-03-01 19:12:15,977 DEBUG TRAIN Batch 55/21700 loss 9.282890 loss_att 15.930250 loss_ctc 17.437716 loss_rnnt 6.762586 hw_loss 0.194104 lr 0.00025764 rank 1
2023-03-01 19:12:15,979 DEBUG TRAIN Batch 55/21700 loss 3.271711 loss_att 5.981363 loss_ctc 5.081796 loss_rnnt 2.406703 hw_loss 0.153249 lr 0.00025764 rank 0
2023-03-01 19:12:38,109 DEBUG TRAIN Batch 55/21800 loss 10.935443 loss_att 12.282822 loss_ctc 21.432964 loss_rnnt 9.137938 hw_loss 0.240674 lr 0.00025763 rank 1
2023-03-01 19:12:38,109 DEBUG TRAIN Batch 55/21800 loss 17.126490 loss_att 20.761093 loss_ctc 33.207481 loss_rnnt 14.174100 hw_loss 0.152507 lr 0.00025763 rank 0
2023-03-01 19:13:00,137 DEBUG TRAIN Batch 55/21900 loss 2.345865 loss_att 4.322918 loss_ctc 5.620979 loss_rnnt 1.419270 hw_loss 0.177191 lr 0.00025762 rank 1
2023-03-01 19:13:00,139 DEBUG TRAIN Batch 55/21900 loss 3.923755 loss_att 9.881783 loss_ctc 7.052156 loss_rnnt 2.258147 hw_loss 0.106655 lr 0.00025762 rank 0
2023-03-01 19:13:22,609 DEBUG TRAIN Batch 55/22000 loss 7.110396 loss_att 8.240509 loss_ctc 9.179729 loss_rnnt 6.428499 hw_loss 0.337430 lr 0.00025761 rank 1
2023-03-01 19:13:22,610 DEBUG TRAIN Batch 55/22000 loss 4.321270 loss_att 7.503680 loss_ctc 7.433639 loss_rnnt 3.193830 hw_loss 0.142453 lr 0.00025761 rank 0
2023-03-01 19:13:55,414 DEBUG TRAIN Batch 55/22100 loss 5.966193 loss_att 8.360825 loss_ctc 14.520908 loss_rnnt 4.316669 hw_loss 0.056193 lr 0.00025760 rank 0
2023-03-01 19:13:55,415 DEBUG TRAIN Batch 55/22100 loss 8.332450 loss_att 10.415083 loss_ctc 15.116627 loss_rnnt 6.835220 hw_loss 0.330275 lr 0.00025760 rank 1
2023-03-01 19:14:17,577 DEBUG TRAIN Batch 55/22200 loss 10.433534 loss_att 15.766077 loss_ctc 11.795391 loss_rnnt 9.017252 hw_loss 0.315358 lr 0.00025759 rank 1
2023-03-01 19:14:17,578 DEBUG TRAIN Batch 55/22200 loss 1.412664 loss_att 4.364983 loss_ctc 2.742067 loss_rnnt 0.486525 hw_loss 0.297042 lr 0.00025759 rank 0
2023-03-01 19:14:40,033 DEBUG TRAIN Batch 55/22300 loss 2.755514 loss_att 5.489295 loss_ctc 5.447926 loss_rnnt 1.778947 hw_loss 0.132792 lr 0.00025758 rank 1
2023-03-01 19:14:40,035 DEBUG TRAIN Batch 55/22300 loss 5.293909 loss_att 9.336490 loss_ctc 13.071925 loss_rnnt 3.356243 hw_loss 0.172652 lr 0.00025758 rank 0
2023-03-01 19:15:13,063 DEBUG TRAIN Batch 55/22400 loss 6.692626 loss_att 12.387255 loss_ctc 14.213114 loss_rnnt 4.430732 hw_loss 0.225443 lr 0.00025758 rank 1
2023-03-01 19:15:13,065 DEBUG TRAIN Batch 55/22400 loss 8.656277 loss_att 11.120128 loss_ctc 16.367102 loss_rnnt 7.041917 hw_loss 0.175274 lr 0.00025758 rank 0
2023-03-01 19:15:35,361 DEBUG TRAIN Batch 55/22500 loss 10.255271 loss_att 13.448952 loss_ctc 15.661885 loss_rnnt 8.806576 hw_loss 0.167020 lr 0.00025757 rank 1
2023-03-01 19:15:35,361 DEBUG TRAIN Batch 55/22500 loss 3.918744 loss_att 6.223660 loss_ctc 6.155821 loss_rnnt 3.063047 hw_loss 0.180818 lr 0.00025757 rank 0
2023-03-01 19:15:58,041 DEBUG TRAIN Batch 55/22600 loss 7.123330 loss_att 9.603031 loss_ctc 10.487687 loss_rnnt 6.061987 hw_loss 0.219041 lr 0.00025756 rank 1
2023-03-01 19:15:58,044 DEBUG TRAIN Batch 55/22600 loss 4.677355 loss_att 9.244669 loss_ctc 9.175888 loss_rnnt 3.072573 hw_loss 0.171589 lr 0.00025756 rank 0
2023-03-01 19:16:20,716 DEBUG TRAIN Batch 55/22700 loss 17.295479 loss_att 20.524706 loss_ctc 24.100197 loss_rnnt 15.608109 hw_loss 0.251678 lr 0.00025755 rank 1
2023-03-01 19:16:20,718 DEBUG TRAIN Batch 55/22700 loss 5.664664 loss_att 7.185456 loss_ctc 8.332588 loss_rnnt 4.887146 hw_loss 0.220570 lr 0.00025755 rank 0
2023-03-01 19:16:54,319 DEBUG TRAIN Batch 55/22800 loss 5.952645 loss_att 8.347607 loss_ctc 7.534795 loss_rnnt 5.168524 hw_loss 0.176578 lr 0.00025754 rank 0
2023-03-01 19:16:54,321 DEBUG TRAIN Batch 55/22800 loss 10.006019 loss_att 12.217454 loss_ctc 12.824733 loss_rnnt 9.141193 hw_loss 0.087581 lr 0.00025754 rank 1
2023-03-01 19:17:16,779 DEBUG TRAIN Batch 55/22900 loss 7.463151 loss_att 9.059855 loss_ctc 8.152849 loss_rnnt 6.978845 hw_loss 0.136885 lr 0.00025753 rank 1
2023-03-01 19:17:16,779 DEBUG TRAIN Batch 55/22900 loss 8.601418 loss_att 11.844997 loss_ctc 13.111939 loss_rnnt 7.316317 hw_loss 0.065593 lr 0.00025753 rank 0
2023-03-01 19:17:39,310 DEBUG TRAIN Batch 55/23000 loss 10.580528 loss_att 12.998299 loss_ctc 18.389992 loss_rnnt 9.003117 hw_loss 0.098616 lr 0.00025752 rank 1
2023-03-01 19:17:39,314 DEBUG TRAIN Batch 55/23000 loss 7.951334 loss_att 10.261223 loss_ctc 15.152588 loss_rnnt 6.418478 hw_loss 0.207584 lr 0.00025752 rank 0
2023-03-01 19:18:12,610 DEBUG TRAIN Batch 55/23100 loss 5.099801 loss_att 8.562435 loss_ctc 7.864537 loss_rnnt 3.899197 hw_loss 0.261461 lr 0.00025752 rank 1
2023-03-01 19:18:12,612 DEBUG TRAIN Batch 55/23100 loss 5.488830 loss_att 7.809930 loss_ctc 10.743888 loss_rnnt 4.243009 hw_loss 0.151738 lr 0.00025752 rank 0
2023-03-01 19:18:35,640 DEBUG TRAIN Batch 55/23200 loss 4.394523 loss_att 7.522295 loss_ctc 7.547379 loss_rnnt 3.253158 hw_loss 0.178932 lr 0.00025751 rank 1
2023-03-01 19:18:35,641 DEBUG TRAIN Batch 55/23200 loss 8.382115 loss_att 8.138528 loss_ctc 14.028419 loss_rnnt 7.483603 hw_loss 0.364480 lr 0.00025751 rank 0
2023-03-01 19:18:57,947 DEBUG TRAIN Batch 55/23300 loss 12.089449 loss_att 16.352188 loss_ctc 24.865026 loss_rnnt 9.471494 hw_loss 0.116244 lr 0.00025750 rank 1
2023-03-01 19:18:57,949 DEBUG TRAIN Batch 55/23300 loss 2.134613 loss_att 6.202881 loss_ctc 4.132105 loss_rnnt 0.873612 hw_loss 0.339402 lr 0.00025750 rank 0
2023-03-01 19:19:21,291 DEBUG TRAIN Batch 55/23400 loss 7.438593 loss_att 8.117928 loss_ctc 11.451821 loss_rnnt 6.650201 hw_loss 0.220179 lr 0.00025749 rank 1
2023-03-01 19:19:21,293 DEBUG TRAIN Batch 55/23400 loss 7.198495 loss_att 11.368088 loss_ctc 15.455200 loss_rnnt 5.168232 hw_loss 0.178970 lr 0.00025749 rank 0
2023-03-01 19:19:53,583 DEBUG TRAIN Batch 55/23500 loss 7.693001 loss_att 10.225994 loss_ctc 9.609751 loss_rnnt 6.840374 hw_loss 0.169616 lr 0.00025748 rank 1
2023-03-01 19:19:53,586 DEBUG TRAIN Batch 55/23500 loss 3.963177 loss_att 7.675443 loss_ctc 9.595943 loss_rnnt 2.399951 hw_loss 0.130759 lr 0.00025748 rank 0
2023-03-01 19:20:15,744 DEBUG TRAIN Batch 55/23600 loss 8.785513 loss_att 18.094400 loss_ctc 24.638083 loss_rnnt 4.697034 hw_loss 0.211924 lr 0.00025747 rank 1
2023-03-01 19:20:15,745 DEBUG TRAIN Batch 55/23600 loss 7.549988 loss_att 10.560869 loss_ctc 16.714634 loss_rnnt 5.520802 hw_loss 0.384479 lr 0.00025747 rank 0
2023-03-01 19:20:38,161 DEBUG TRAIN Batch 55/23700 loss 4.880748 loss_att 6.918146 loss_ctc 6.868500 loss_rnnt 4.128202 hw_loss 0.150060 lr 0.00025746 rank 1
2023-03-01 19:20:38,163 DEBUG TRAIN Batch 55/23700 loss 7.859921 loss_att 9.020608 loss_ctc 15.376956 loss_rnnt 6.511631 hw_loss 0.213527 lr 0.00025746 rank 0
2023-03-01 19:21:00,758 DEBUG TRAIN Batch 55/23800 loss 4.202711 loss_att 7.093091 loss_ctc 6.053679 loss_rnnt 3.247367 hw_loss 0.244636 lr 0.00025746 rank 1
2023-03-01 19:21:00,761 DEBUG TRAIN Batch 55/23800 loss 5.177613 loss_att 7.044983 loss_ctc 6.150211 loss_rnnt 4.531580 hw_loss 0.267898 lr 0.00025746 rank 0
2023-03-01 19:21:34,094 DEBUG TRAIN Batch 55/23900 loss 9.023995 loss_att 12.741146 loss_ctc 15.532513 loss_rnnt 7.345479 hw_loss 0.126156 lr 0.00025745 rank 1
2023-03-01 19:21:34,096 DEBUG TRAIN Batch 55/23900 loss 2.722135 loss_att 4.473115 loss_ctc 3.827346 loss_rnnt 2.144639 hw_loss 0.149886 lr 0.00025745 rank 0
2023-03-01 19:21:56,632 DEBUG TRAIN Batch 55/24000 loss 6.661932 loss_att 9.239424 loss_ctc 11.313811 loss_rnnt 5.389873 hw_loss 0.255581 lr 0.00025744 rank 1
2023-03-01 19:21:56,634 DEBUG TRAIN Batch 55/24000 loss 8.947163 loss_att 9.922653 loss_ctc 18.558475 loss_rnnt 7.395241 hw_loss 0.141215 lr 0.00025744 rank 0
2023-03-01 19:22:18,973 DEBUG TRAIN Batch 55/24100 loss 4.069854 loss_att 7.801552 loss_ctc 8.119331 loss_rnnt 2.674670 hw_loss 0.204215 lr 0.00025743 rank 1
2023-03-01 19:22:18,975 DEBUG TRAIN Batch 55/24100 loss 5.714109 loss_att 9.576587 loss_ctc 16.004961 loss_rnnt 3.472569 hw_loss 0.181745 lr 0.00025743 rank 0
2023-03-01 19:22:51,563 DEBUG TRAIN Batch 55/24200 loss 4.196016 loss_att 7.009777 loss_ctc 8.995773 loss_rnnt 2.900490 hw_loss 0.174011 lr 0.00025742 rank 1
2023-03-01 19:22:51,566 DEBUG TRAIN Batch 55/24200 loss 8.607308 loss_att 10.040981 loss_ctc 12.602175 loss_rnnt 7.672193 hw_loss 0.216998 lr 0.00025742 rank 0
2023-03-01 19:23:14,069 DEBUG TRAIN Batch 55/24300 loss 9.858178 loss_att 13.611720 loss_ctc 13.708296 loss_rnnt 8.420287 hw_loss 0.325939 lr 0.00025741 rank 1
2023-03-01 19:23:14,071 DEBUG TRAIN Batch 55/24300 loss 12.467144 loss_att 16.968655 loss_ctc 22.194952 loss_rnnt 10.174998 hw_loss 0.177753 lr 0.00025741 rank 0
2023-03-01 19:23:35,967 DEBUG TRAIN Batch 55/24400 loss 6.903373 loss_att 8.101796 loss_ctc 11.857795 loss_rnnt 5.902846 hw_loss 0.187972 lr 0.00025740 rank 1
2023-03-01 19:23:35,967 DEBUG TRAIN Batch 55/24400 loss 6.610538 loss_att 8.049306 loss_ctc 12.260208 loss_rnnt 5.452177 hw_loss 0.219971 lr 0.00025740 rank 0
2023-03-01 19:23:58,195 DEBUG TRAIN Batch 55/24500 loss 9.391416 loss_att 13.211231 loss_ctc 15.288451 loss_rnnt 7.796512 hw_loss 0.083754 lr 0.00025740 rank 1
2023-03-01 19:23:58,196 DEBUG TRAIN Batch 55/24500 loss 4.366273 loss_att 7.511539 loss_ctc 10.034482 loss_rnnt 2.844818 hw_loss 0.256201 lr 0.00025740 rank 0
2023-03-01 19:24:31,391 DEBUG TRAIN Batch 55/24600 loss 1.402999 loss_att 4.125630 loss_ctc 3.033714 loss_rnnt 0.512011 hw_loss 0.241938 lr 0.00025739 rank 0
2023-03-01 19:24:31,391 DEBUG TRAIN Batch 55/24600 loss 5.654757 loss_att 7.779700 loss_ctc 8.959574 loss_rnnt 4.666985 hw_loss 0.229013 lr 0.00025739 rank 1
2023-03-01 19:24:53,586 DEBUG TRAIN Batch 55/24700 loss 4.319501 loss_att 7.300130 loss_ctc 7.807944 loss_rnnt 3.153738 hw_loss 0.195959 lr 0.00025738 rank 0
2023-03-01 19:24:53,586 DEBUG TRAIN Batch 55/24700 loss 9.615419 loss_att 10.215839 loss_ctc 15.600429 loss_rnnt 8.525096 hw_loss 0.322947 lr 0.00025738 rank 1
2023-03-01 19:25:15,372 DEBUG TRAIN Batch 55/24800 loss 5.299866 loss_att 8.532269 loss_ctc 9.871356 loss_rnnt 3.910889 hw_loss 0.249309 lr 0.00025737 rank 1
2023-03-01 19:25:15,373 DEBUG TRAIN Batch 55/24800 loss 3.530817 loss_att 5.700802 loss_ctc 6.138308 loss_rnnt 2.667536 hw_loss 0.153034 lr 0.00025737 rank 0
2023-03-01 19:25:48,433 DEBUG TRAIN Batch 55/24900 loss 1.912170 loss_att 4.709719 loss_ctc 7.369508 loss_rnnt 0.547849 hw_loss 0.144687 lr 0.00025736 rank 1
2023-03-01 19:25:48,433 DEBUG TRAIN Batch 55/24900 loss 3.025437 loss_att 5.842345 loss_ctc 6.460519 loss_rnnt 1.880181 hw_loss 0.232245 lr 0.00025736 rank 0
2023-03-01 19:26:10,597 DEBUG TRAIN Batch 55/25000 loss 14.599907 loss_att 16.910349 loss_ctc 25.506750 loss_rnnt 12.592818 hw_loss 0.170163 lr 0.00025735 rank 1
2023-03-01 19:26:10,599 DEBUG TRAIN Batch 55/25000 loss 3.780105 loss_att 5.901488 loss_ctc 6.091722 loss_rnnt 2.893813 hw_loss 0.288375 lr 0.00025735 rank 0
2023-03-01 19:26:32,446 DEBUG TRAIN Batch 55/25100 loss 5.420467 loss_att 8.304959 loss_ctc 6.873357 loss_rnnt 4.553520 hw_loss 0.180621 lr 0.00025735 rank 1
2023-03-01 19:26:32,448 DEBUG TRAIN Batch 55/25100 loss 3.894139 loss_att 8.104784 loss_ctc 6.331993 loss_rnnt 2.595854 hw_loss 0.245827 lr 0.00025735 rank 0
2023-03-01 19:26:54,772 DEBUG TRAIN Batch 55/25200 loss 3.842475 loss_att 5.945172 loss_ctc 7.620059 loss_rnnt 2.775982 hw_loss 0.266768 lr 0.00025734 rank 1
2023-03-01 19:26:54,773 DEBUG TRAIN Batch 55/25200 loss 4.978483 loss_att 9.363710 loss_ctc 11.947447 loss_rnnt 3.125696 hw_loss 0.087276 lr 0.00025734 rank 0
2023-03-01 19:27:28,743 DEBUG TRAIN Batch 55/25300 loss 5.470988 loss_att 7.257524 loss_ctc 11.339019 loss_rnnt 4.108275 hw_loss 0.418127 lr 0.00025733 rank 1
2023-03-01 19:27:28,744 DEBUG TRAIN Batch 55/25300 loss 9.748036 loss_att 12.111058 loss_ctc 12.299240 loss_rnnt 8.877775 hw_loss 0.107806 lr 0.00025733 rank 0
2023-03-01 19:27:50,584 DEBUG TRAIN Batch 55/25400 loss 4.740590 loss_att 6.967149 loss_ctc 6.666766 loss_rnnt 3.944839 hw_loss 0.175527 lr 0.00025732 rank 1
2023-03-01 19:27:50,586 DEBUG TRAIN Batch 55/25400 loss 5.814662 loss_att 6.858070 loss_ctc 5.577427 loss_rnnt 5.506701 hw_loss 0.245456 lr 0.00025732 rank 0
2023-03-01 19:28:12,814 DEBUG TRAIN Batch 55/25500 loss 12.671704 loss_att 17.698866 loss_ctc 22.187847 loss_rnnt 10.249786 hw_loss 0.276876 lr 0.00025731 rank 1
2023-03-01 19:28:12,818 DEBUG TRAIN Batch 55/25500 loss 6.797455 loss_att 9.238050 loss_ctc 8.851274 loss_rnnt 5.951755 hw_loss 0.157011 lr 0.00025731 rank 0
2023-03-01 19:28:46,382 DEBUG TRAIN Batch 55/25600 loss 11.298892 loss_att 17.909260 loss_ctc 24.606949 loss_rnnt 7.930643 hw_loss 0.509564 lr 0.00025730 rank 1
2023-03-01 19:28:46,385 DEBUG TRAIN Batch 55/25600 loss 4.869264 loss_att 6.508540 loss_ctc 8.194440 loss_rnnt 3.964240 hw_loss 0.250897 lr 0.00025730 rank 0
2023-03-01 19:29:08,444 DEBUG TRAIN Batch 55/25700 loss 8.300387 loss_att 10.399005 loss_ctc 16.170702 loss_rnnt 6.692208 hw_loss 0.260776 lr 0.00025729 rank 1
2023-03-01 19:29:08,447 DEBUG TRAIN Batch 55/25700 loss 4.478479 loss_att 6.597130 loss_ctc 7.116939 loss_rnnt 3.618539 hw_loss 0.158278 lr 0.00025729 rank 0
2023-03-01 19:29:30,889 DEBUG TRAIN Batch 55/25800 loss 12.614446 loss_att 14.167994 loss_ctc 16.583900 loss_rnnt 11.663589 hw_loss 0.207911 lr 0.00025729 rank 1
2023-03-01 19:29:30,892 DEBUG TRAIN Batch 55/25800 loss 11.148074 loss_att 13.963985 loss_ctc 21.503044 loss_rnnt 9.124284 hw_loss 0.149897 lr 0.00025729 rank 0
2023-03-01 19:29:54,037 DEBUG TRAIN Batch 55/25900 loss 7.593335 loss_att 10.920939 loss_ctc 13.145261 loss_rnnt 6.083474 hw_loss 0.195156 lr 0.00025728 rank 1
2023-03-01 19:29:54,038 DEBUG TRAIN Batch 55/25900 loss 4.165637 loss_att 5.770773 loss_ctc 6.081749 loss_rnnt 3.468553 hw_loss 0.226077 lr 0.00025728 rank 0
2023-03-01 19:30:27,176 DEBUG TRAIN Batch 55/26000 loss 9.624933 loss_att 12.303799 loss_ctc 11.156125 loss_rnnt 8.832687 hw_loss 0.098088 lr 0.00025727 rank 0
2023-03-01 19:30:27,187 DEBUG TRAIN Batch 55/26000 loss 11.106394 loss_att 14.436496 loss_ctc 12.869665 loss_rnnt 10.139788 hw_loss 0.122782 lr 0.00025727 rank 1
2023-03-01 19:30:49,385 DEBUG TRAIN Batch 55/26100 loss 3.916682 loss_att 6.108314 loss_ctc 5.131578 loss_rnnt 3.181267 hw_loss 0.253319 lr 0.00025726 rank 1
2023-03-01 19:30:49,387 DEBUG TRAIN Batch 55/26100 loss 10.157353 loss_att 11.802906 loss_ctc 17.471859 loss_rnnt 8.747823 hw_loss 0.197161 lr 0.00025726 rank 0
2023-03-01 19:31:12,027 DEBUG TRAIN Batch 55/26200 loss 10.201404 loss_att 12.127637 loss_ctc 13.947969 loss_rnnt 9.287907 hw_loss 0.053829 lr 0.00025725 rank 1
2023-03-01 19:31:12,030 DEBUG TRAIN Batch 55/26200 loss 4.660517 loss_att 7.436680 loss_ctc 6.221426 loss_rnnt 3.787913 hw_loss 0.204845 lr 0.00025725 rank 0
2023-03-01 19:31:46,042 DEBUG TRAIN Batch 55/26300 loss 3.997714 loss_att 9.300310 loss_ctc 6.843865 loss_rnnt 2.493032 hw_loss 0.121265 lr 0.00025724 rank 1
2023-03-01 19:31:46,043 DEBUG TRAIN Batch 55/26300 loss 5.653579 loss_att 7.743813 loss_ctc 10.093501 loss_rnnt 4.559977 hw_loss 0.156686 lr 0.00025724 rank 0
2023-03-01 19:32:07,727 DEBUG TRAIN Batch 55/26400 loss 3.747312 loss_att 8.091384 loss_ctc 9.503197 loss_rnnt 1.987667 hw_loss 0.231336 lr 0.00025723 rank 1
2023-03-01 19:32:07,728 DEBUG TRAIN Batch 55/26400 loss 4.339564 loss_att 9.426684 loss_ctc 9.390723 loss_rnnt 2.596138 hw_loss 0.098463 lr 0.00025723 rank 0
2023-03-01 19:32:29,831 DEBUG TRAIN Batch 55/26500 loss 7.847187 loss_att 11.563128 loss_ctc 12.239771 loss_rnnt 6.358762 hw_loss 0.299172 lr 0.00025723 rank 1
2023-03-01 19:32:29,833 DEBUG TRAIN Batch 55/26500 loss 4.260929 loss_att 6.065906 loss_ctc 10.223023 loss_rnnt 3.016201 hw_loss 0.166475 lr 0.00025723 rank 0
2023-03-01 19:32:52,223 DEBUG TRAIN Batch 55/26600 loss 17.509075 loss_att 19.357450 loss_ctc 22.386780 loss_rnnt 16.376259 hw_loss 0.211461 lr 0.00025722 rank 1
2023-03-01 19:32:52,226 DEBUG TRAIN Batch 55/26600 loss 4.001052 loss_att 6.548004 loss_ctc 7.042209 loss_rnnt 2.964828 hw_loss 0.227525 lr 0.00025722 rank 0
2023-03-01 19:33:24,516 DEBUG TRAIN Batch 55/26700 loss 8.230107 loss_att 11.839213 loss_ctc 13.239014 loss_rnnt 6.756744 hw_loss 0.156916 lr 0.00025721 rank 1
2023-03-01 19:33:24,517 DEBUG TRAIN Batch 55/26700 loss 5.088097 loss_att 7.046227 loss_ctc 8.630993 loss_rnnt 4.114621 hw_loss 0.205244 lr 0.00025721 rank 0
2023-03-01 19:33:47,360 DEBUG TRAIN Batch 55/26800 loss 2.231631 loss_att 6.214866 loss_ctc 6.490333 loss_rnnt 0.710517 hw_loss 0.293700 lr 0.00025720 rank 1
2023-03-01 19:33:47,361 DEBUG TRAIN Batch 55/26800 loss 3.335224 loss_att 4.978944 loss_ctc 6.657571 loss_rnnt 2.417355 hw_loss 0.274022 lr 0.00025720 rank 0
2023-03-01 19:34:10,332 DEBUG TRAIN Batch 55/26900 loss 6.420848 loss_att 11.503910 loss_ctc 11.124592 loss_rnnt 4.723486 hw_loss 0.100471 lr 0.00025719 rank 1
2023-03-01 19:34:10,333 DEBUG TRAIN Batch 55/26900 loss 4.859418 loss_att 8.434443 loss_ctc 9.547529 loss_rnnt 3.465292 hw_loss 0.101323 lr 0.00025719 rank 0
2023-03-01 19:34:32,558 DEBUG TRAIN Batch 55/27000 loss 5.996322 loss_att 9.836126 loss_ctc 8.554741 loss_rnnt 4.726439 hw_loss 0.301498 lr 0.00025718 rank 1
2023-03-01 19:34:32,558 DEBUG TRAIN Batch 55/27000 loss 5.661894 loss_att 8.904943 loss_ctc 9.696170 loss_rnnt 4.311858 hw_loss 0.306604 lr 0.00025718 rank 0
2023-03-01 19:35:04,769 DEBUG TRAIN Batch 55/27100 loss 2.018533 loss_att 4.097919 loss_ctc 3.645256 loss_rnnt 1.338495 hw_loss 0.088622 lr 0.00025717 rank 1
2023-03-01 19:35:04,771 DEBUG TRAIN Batch 55/27100 loss 6.959811 loss_att 12.174119 loss_ctc 10.912291 loss_rnnt 5.324973 hw_loss 0.121835 lr 0.00025717 rank 0
2023-03-01 19:35:27,228 DEBUG TRAIN Batch 55/27200 loss 6.858636 loss_att 9.491844 loss_ctc 11.579023 loss_rnnt 5.603327 hw_loss 0.186155 lr 0.00025717 rank 1
2023-03-01 19:35:27,230 DEBUG TRAIN Batch 55/27200 loss 10.724465 loss_att 17.262686 loss_ctc 22.072384 loss_rnnt 7.873922 hw_loss 0.055956 lr 0.00025717 rank 0
2023-03-01 19:35:49,640 DEBUG TRAIN Batch 55/27300 loss 7.421964 loss_att 7.712126 loss_ctc 11.449738 loss_rnnt 6.696418 hw_loss 0.244646 lr 0.00025716 rank 1
2023-03-01 19:35:49,643 DEBUG TRAIN Batch 55/27300 loss 13.040631 loss_att 14.299394 loss_ctc 16.281466 loss_rnnt 12.262867 hw_loss 0.176065 lr 0.00025716 rank 0
2023-03-01 19:36:22,982 DEBUG TRAIN Batch 55/27400 loss 7.331225 loss_att 15.575701 loss_ctc 13.716326 loss_rnnt 4.744917 hw_loss 0.161375 lr 0.00025715 rank 1
2023-03-01 19:36:22,985 DEBUG TRAIN Batch 55/27400 loss 4.882740 loss_att 9.107950 loss_ctc 8.480892 loss_rnnt 3.476617 hw_loss 0.152487 lr 0.00025715 rank 0
2023-03-01 19:36:45,526 DEBUG TRAIN Batch 55/27500 loss 13.024977 loss_att 15.558197 loss_ctc 17.497684 loss_rnnt 11.782269 hw_loss 0.261943 lr 0.00025714 rank 1
2023-03-01 19:36:45,527 DEBUG TRAIN Batch 55/27500 loss 6.347482 loss_att 9.066339 loss_ctc 9.270290 loss_rnnt 5.278183 hw_loss 0.254662 lr 0.00025714 rank 0
2023-03-01 19:37:08,231 DEBUG TRAIN Batch 55/27600 loss 11.192897 loss_att 15.271145 loss_ctc 18.774521 loss_rnnt 9.232782 hw_loss 0.250467 lr 0.00025713 rank 1
2023-03-01 19:37:08,233 DEBUG TRAIN Batch 55/27600 loss 9.233665 loss_att 10.018387 loss_ctc 14.409322 loss_rnnt 8.211377 hw_loss 0.328603 lr 0.00025713 rank 0
2023-03-01 19:37:30,750 DEBUG TRAIN Batch 55/27700 loss 7.965449 loss_att 11.407168 loss_ctc 15.302132 loss_rnnt 6.279166 hw_loss 0.036966 lr 0.00025712 rank 1
2023-03-01 19:37:30,751 DEBUG TRAIN Batch 55/27700 loss 2.927849 loss_att 6.620818 loss_ctc 7.154644 loss_rnnt 1.515068 hw_loss 0.207402 lr 0.00025712 rank 0
2023-03-01 19:38:04,051 DEBUG TRAIN Batch 55/27800 loss 4.700139 loss_att 7.621361 loss_ctc 8.420594 loss_rnnt 3.492636 hw_loss 0.238498 lr 0.00025712 rank 1
2023-03-01 19:38:04,052 DEBUG TRAIN Batch 55/27800 loss 4.884265 loss_att 6.323107 loss_ctc 10.610698 loss_rnnt 3.733432 hw_loss 0.186640 lr 0.00025712 rank 0
2023-03-01 19:38:26,371 DEBUG TRAIN Batch 55/27900 loss 8.795345 loss_att 11.447046 loss_ctc 15.385587 loss_rnnt 7.321800 hw_loss 0.120947 lr 0.00025711 rank 0
2023-03-01 19:38:26,373 DEBUG TRAIN Batch 55/27900 loss 8.341152 loss_att 9.700212 loss_ctc 16.060768 loss_rnnt 6.869394 hw_loss 0.319996 lr 0.00025711 rank 1
2023-03-01 19:38:48,371 DEBUG TRAIN Batch 55/28000 loss 5.794400 loss_att 10.374398 loss_ctc 12.229533 loss_rnnt 3.972248 hw_loss 0.090253 lr 0.00025710 rank 1
2023-03-01 19:38:48,375 DEBUG TRAIN Batch 55/28000 loss 10.309302 loss_att 14.965446 loss_ctc 11.026342 loss_rnnt 9.160959 hw_loss 0.227827 lr 0.00025710 rank 0
2023-03-01 19:39:21,474 DEBUG TRAIN Batch 55/28100 loss 3.640696 loss_att 5.539022 loss_ctc 6.849746 loss_rnnt 2.721533 hw_loss 0.209294 lr 0.00025709 rank 1
2023-03-01 19:39:21,478 DEBUG TRAIN Batch 55/28100 loss 9.598664 loss_att 12.231676 loss_ctc 15.868427 loss_rnnt 8.123924 hw_loss 0.210319 lr 0.00025709 rank 0
2023-03-01 19:39:44,532 DEBUG TRAIN Batch 55/28200 loss 3.612798 loss_att 6.261171 loss_ctc 8.139701 loss_rnnt 2.389747 hw_loss 0.168356 lr 0.00025708 rank 1
2023-03-01 19:39:44,534 DEBUG TRAIN Batch 55/28200 loss 7.052822 loss_att 8.249806 loss_ctc 16.351633 loss_rnnt 5.432054 hw_loss 0.265367 lr 0.00025708 rank 0
2023-03-01 19:40:06,673 DEBUG TRAIN Batch 55/28300 loss 6.502405 loss_att 11.225266 loss_ctc 12.169204 loss_rnnt 4.644915 hw_loss 0.295022 lr 0.00025707 rank 1
2023-03-01 19:40:06,673 DEBUG TRAIN Batch 55/28300 loss 5.601424 loss_att 8.956492 loss_ctc 10.642959 loss_rnnt 4.176147 hw_loss 0.153861 lr 0.00025707 rank 0
2023-03-01 19:40:29,072 DEBUG TRAIN Batch 55/28400 loss 6.011326 loss_att 9.002205 loss_ctc 8.436029 loss_rnnt 4.963813 hw_loss 0.236331 lr 0.00025706 rank 1
2023-03-01 19:40:29,074 DEBUG TRAIN Batch 55/28400 loss 5.232522 loss_att 6.852662 loss_ctc 8.174442 loss_rnnt 4.472744 hw_loss 0.081550 lr 0.00025706 rank 0
2023-03-01 19:41:03,300 DEBUG TRAIN Batch 55/28500 loss 11.306908 loss_att 13.280169 loss_ctc 16.302362 loss_rnnt 10.148668 hw_loss 0.182862 lr 0.00025706 rank 1
2023-03-01 19:41:03,302 DEBUG TRAIN Batch 55/28500 loss 3.788326 loss_att 6.255541 loss_ctc 6.801888 loss_rnnt 2.735826 hw_loss 0.294841 lr 0.00025706 rank 0
2023-03-01 19:41:25,332 DEBUG TRAIN Batch 55/28600 loss 3.926747 loss_att 7.426769 loss_ctc 5.688263 loss_rnnt 2.947749 hw_loss 0.082734 lr 0.00025705 rank 1
2023-03-01 19:41:25,332 DEBUG TRAIN Batch 55/28600 loss 7.517911 loss_att 11.122910 loss_ctc 17.120375 loss_rnnt 5.401412 hw_loss 0.215945 lr 0.00025705 rank 0
2023-03-01 19:41:47,467 DEBUG TRAIN Batch 55/28700 loss 7.658970 loss_att 8.709180 loss_ctc 10.669531 loss_rnnt 7.027208 hw_loss 0.038085 lr 0.00025704 rank 1
2023-03-01 19:41:47,469 DEBUG TRAIN Batch 55/28700 loss 5.957634 loss_att 9.338357 loss_ctc 14.637684 loss_rnnt 4.028615 hw_loss 0.179127 lr 0.00025704 rank 0
2023-03-01 19:42:21,613 DEBUG TRAIN Batch 55/28800 loss 14.099769 loss_att 14.432922 loss_ctc 21.580433 loss_rnnt 12.901337 hw_loss 0.251963 lr 0.00025703 rank 1
2023-03-01 19:42:21,614 DEBUG TRAIN Batch 55/28800 loss 9.037378 loss_att 11.179189 loss_ctc 12.809469 loss_rnnt 7.986938 hw_loss 0.223377 lr 0.00025703 rank 0
2023-03-01 19:42:43,885 DEBUG TRAIN Batch 55/28900 loss 10.816245 loss_att 11.996350 loss_ctc 15.733040 loss_rnnt 9.852036 hw_loss 0.136153 lr 0.00025702 rank 1
2023-03-01 19:42:43,887 DEBUG TRAIN Batch 55/28900 loss 7.247246 loss_att 8.822542 loss_ctc 7.940399 loss_rnnt 6.711398 hw_loss 0.240692 lr 0.00025702 rank 0
2023-03-01 19:43:06,224 DEBUG TRAIN Batch 55/29000 loss 8.282401 loss_att 9.190261 loss_ctc 14.931713 loss_rnnt 7.076231 hw_loss 0.258795 lr 0.00025701 rank 1
2023-03-01 19:43:06,225 DEBUG TRAIN Batch 55/29000 loss 8.731009 loss_att 11.502554 loss_ctc 9.708858 loss_rnnt 7.939526 hw_loss 0.200241 lr 0.00025701 rank 0
2023-03-01 19:43:28,526 DEBUG TRAIN Batch 55/29100 loss 5.700626 loss_att 8.192945 loss_ctc 10.267801 loss_rnnt 4.514903 hw_loss 0.146817 lr 0.00025701 rank 1
2023-03-01 19:43:28,528 DEBUG TRAIN Batch 55/29100 loss 2.540345 loss_att 4.815374 loss_ctc 6.049469 loss_rnnt 1.513197 hw_loss 0.195485 lr 0.00025701 rank 0
2023-03-01 19:44:01,455 DEBUG TRAIN Batch 55/29200 loss 8.199322 loss_att 10.223272 loss_ctc 14.264539 loss_rnnt 6.927691 hw_loss 0.109021 lr 0.00025700 rank 0
2023-03-01 19:44:01,464 DEBUG TRAIN Batch 55/29200 loss 6.945181 loss_att 7.057184 loss_ctc 10.417807 loss_rnnt 6.310764 hw_loss 0.279375 lr 0.00025700 rank 1
2023-03-01 19:44:23,924 DEBUG TRAIN Batch 55/29300 loss 6.737444 loss_att 11.197739 loss_ctc 16.203133 loss_rnnt 4.523951 hw_loss 0.111267 lr 0.00025699 rank 1
2023-03-01 19:44:23,928 DEBUG TRAIN Batch 55/29300 loss 6.168676 loss_att 7.419178 loss_ctc 10.561580 loss_rnnt 5.189549 hw_loss 0.268698 lr 0.00025699 rank 0
2023-03-01 19:44:46,700 DEBUG TRAIN Batch 55/29400 loss 6.540648 loss_att 9.885620 loss_ctc 6.844184 loss_rnnt 5.812540 hw_loss 0.034953 lr 0.00025698 rank 1
2023-03-01 19:44:46,704 DEBUG TRAIN Batch 55/29400 loss 2.573222 loss_att 4.873905 loss_ctc 4.685573 loss_rnnt 1.733982 hw_loss 0.182733 lr 0.00025698 rank 0
2023-03-01 19:45:20,690 DEBUG TRAIN Batch 55/29500 loss 7.657840 loss_att 9.861661 loss_ctc 11.467676 loss_rnnt 6.569243 hw_loss 0.262228 lr 0.00025697 rank 1
2023-03-01 19:45:20,692 DEBUG TRAIN Batch 55/29500 loss 6.138496 loss_att 7.849476 loss_ctc 10.685626 loss_rnnt 5.004423 hw_loss 0.347987 lr 0.00025697 rank 0
2023-03-01 19:45:43,268 DEBUG TRAIN Batch 55/29600 loss 4.443190 loss_att 6.783601 loss_ctc 5.757362 loss_rnnt 3.725940 hw_loss 0.138645 lr 0.00025696 rank 1
2023-03-01 19:45:43,271 DEBUG TRAIN Batch 55/29600 loss 3.111882 loss_att 7.897034 loss_ctc 4.660911 loss_rnnt 1.893717 hw_loss 0.102370 lr 0.00025696 rank 0
2023-03-01 19:46:05,694 DEBUG TRAIN Batch 55/29700 loss 6.514232 loss_att 11.667359 loss_ctc 20.957998 loss_rnnt 3.411142 hw_loss 0.274930 lr 0.00025695 rank 1
2023-03-01 19:46:05,697 DEBUG TRAIN Batch 55/29700 loss 7.219179 loss_att 9.284125 loss_ctc 9.268963 loss_rnnt 6.451328 hw_loss 0.152918 lr 0.00025695 rank 0
2023-03-01 19:46:28,623 DEBUG TRAIN Batch 55/29800 loss 9.260771 loss_att 11.950394 loss_ctc 15.676482 loss_rnnt 7.778922 hw_loss 0.165930 lr 0.00025695 rank 1
2023-03-01 19:46:28,624 DEBUG TRAIN Batch 55/29800 loss 8.683762 loss_att 11.241647 loss_ctc 14.138836 loss_rnnt 7.326444 hw_loss 0.221995 lr 0.00025695 rank 0
2023-03-01 19:47:01,154 DEBUG TRAIN Batch 55/29900 loss 7.018200 loss_att 11.595390 loss_ctc 12.170792 loss_rnnt 5.383560 hw_loss 0.060357 lr 0.00025694 rank 1
2023-03-01 19:47:01,157 DEBUG TRAIN Batch 55/29900 loss 3.326000 loss_att 5.966637 loss_ctc 7.731250 loss_rnnt 2.115621 hw_loss 0.177911 lr 0.00025694 rank 0
2023-03-01 19:47:24,396 DEBUG TRAIN Batch 55/30000 loss 4.405478 loss_att 5.465298 loss_ctc 5.522479 loss_rnnt 3.951342 hw_loss 0.174823 lr 0.00025693 rank 1
2023-03-01 19:47:24,398 DEBUG TRAIN Batch 55/30000 loss 2.348069 loss_att 6.521571 loss_ctc 3.971838 loss_rnnt 1.144847 hw_loss 0.285035 lr 0.00025693 rank 0
2023-03-01 19:47:47,164 DEBUG TRAIN Batch 55/30100 loss 2.669844 loss_att 6.567940 loss_ctc 6.991022 loss_rnnt 1.208052 hw_loss 0.198779 lr 0.00025692 rank 1
2023-03-01 19:47:47,165 DEBUG TRAIN Batch 55/30100 loss 6.682940 loss_att 8.198409 loss_ctc 10.213360 loss_rnnt 5.773663 hw_loss 0.253988 lr 0.00025692 rank 0
2023-03-01 19:48:09,399 DEBUG TRAIN Batch 55/30200 loss 8.554430 loss_att 10.168713 loss_ctc 12.457821 loss_rnnt 7.637512 hw_loss 0.138019 lr 0.00025691 rank 1
2023-03-01 19:48:09,401 DEBUG TRAIN Batch 55/30200 loss 5.505931 loss_att 8.996295 loss_ctc 8.760386 loss_rnnt 4.260160 hw_loss 0.213319 lr 0.00025691 rank 0
2023-03-01 19:48:42,381 DEBUG TRAIN Batch 55/30300 loss 4.970285 loss_att 8.515209 loss_ctc 10.432479 loss_rnnt 3.434548 hw_loss 0.184611 lr 0.00025690 rank 1
2023-03-01 19:48:42,383 DEBUG TRAIN Batch 55/30300 loss 3.021057 loss_att 6.851332 loss_ctc 5.299937 loss_rnnt 1.836602 hw_loss 0.214779 lr 0.00025690 rank 0
2023-03-01 19:49:05,442 DEBUG TRAIN Batch 55/30400 loss 8.774996 loss_att 11.117237 loss_ctc 12.675952 loss_rnnt 7.655463 hw_loss 0.245544 lr 0.00025689 rank 1
2023-03-01 19:49:05,443 DEBUG TRAIN Batch 55/30400 loss 10.284971 loss_att 12.462870 loss_ctc 21.803900 loss_rnnt 8.176630 hw_loss 0.256695 lr 0.00025689 rank 0
2023-03-01 19:49:27,659 DEBUG TRAIN Batch 55/30500 loss 12.816450 loss_att 15.122438 loss_ctc 16.046934 loss_rnnt 11.835541 hw_loss 0.166838 lr 0.00025689 rank 1
2023-03-01 19:49:27,661 DEBUG TRAIN Batch 55/30500 loss 7.227834 loss_att 9.443282 loss_ctc 13.676643 loss_rnnt 5.821471 hw_loss 0.193935 lr 0.00025689 rank 0
2023-03-01 19:50:00,311 DEBUG TRAIN Batch 55/30600 loss 8.381155 loss_att 13.212312 loss_ctc 14.431173 loss_rnnt 6.495360 hw_loss 0.211679 lr 0.00025688 rank 1
2023-03-01 19:50:00,313 DEBUG TRAIN Batch 55/30600 loss 8.361255 loss_att 10.000068 loss_ctc 12.792866 loss_rnnt 7.341960 hw_loss 0.188721 lr 0.00025688 rank 0
2023-03-01 19:50:22,651 DEBUG TRAIN Batch 55/30700 loss 1.862085 loss_att 5.642553 loss_ctc 4.388073 loss_rnnt 0.585324 hw_loss 0.344753 lr 0.00025687 rank 1
2023-03-01 19:50:22,653 DEBUG TRAIN Batch 55/30700 loss 17.786194 loss_att 18.745430 loss_ctc 20.399948 loss_rnnt 17.180592 hw_loss 0.122353 lr 0.00025687 rank 0
2023-03-01 19:50:44,863 DEBUG TRAIN Batch 55/30800 loss 5.933827 loss_att 8.957993 loss_ctc 9.002186 loss_rnnt 4.791315 hw_loss 0.241059 lr 0.00025686 rank 1
2023-03-01 19:50:44,865 DEBUG TRAIN Batch 55/30800 loss 5.733956 loss_att 6.900111 loss_ctc 8.251895 loss_rnnt 4.951166 hw_loss 0.400939 lr 0.00025686 rank 0
2023-03-01 19:51:07,291 DEBUG TRAIN Batch 55/30900 loss 7.948956 loss_att 11.464117 loss_ctc 12.549631 loss_rnnt 6.549568 hw_loss 0.155498 lr 0.00025685 rank 1
2023-03-01 19:51:07,293 DEBUG TRAIN Batch 55/30900 loss 5.162757 loss_att 10.255877 loss_ctc 17.367302 loss_rnnt 2.451503 hw_loss 0.122543 lr 0.00025685 rank 0
2023-03-01 19:51:40,728 DEBUG TRAIN Batch 55/31000 loss 3.781499 loss_att 6.232472 loss_ctc 10.161260 loss_rnnt 2.328447 hw_loss 0.210419 lr 0.00025684 rank 1
2023-03-01 19:51:40,729 DEBUG TRAIN Batch 55/31000 loss 6.255985 loss_att 9.177711 loss_ctc 14.839523 loss_rnnt 4.415969 hw_loss 0.208497 lr 0.00025684 rank 0
2023-03-01 19:52:03,189 DEBUG TRAIN Batch 55/31100 loss 5.235586 loss_att 6.577371 loss_ctc 7.262097 loss_rnnt 4.557905 hw_loss 0.260856 lr 0.00025684 rank 1
2023-03-01 19:52:03,191 DEBUG TRAIN Batch 55/31100 loss 11.538156 loss_att 14.035380 loss_ctc 22.628414 loss_rnnt 9.445269 hw_loss 0.215137 lr 0.00025684 rank 0
2023-03-01 19:52:25,531 DEBUG TRAIN Batch 55/31200 loss 4.885405 loss_att 7.710995 loss_ctc 9.568064 loss_rnnt 3.527175 hw_loss 0.316419 lr 0.00025683 rank 1
2023-03-01 19:52:25,534 DEBUG TRAIN Batch 55/31200 loss 10.426734 loss_att 15.270844 loss_ctc 13.660616 loss_rnnt 8.919241 hw_loss 0.201538 lr 0.00025683 rank 0
2023-03-01 19:52:58,742 DEBUG TRAIN Batch 55/31300 loss 6.171324 loss_att 9.707804 loss_ctc 8.456814 loss_rnnt 5.088932 hw_loss 0.131935 lr 0.00025682 rank 1
2023-03-01 19:52:58,744 DEBUG TRAIN Batch 55/31300 loss 4.562928 loss_att 8.792763 loss_ctc 8.304910 loss_rnnt 3.127659 hw_loss 0.169445 lr 0.00025682 rank 0
2023-03-01 19:53:21,446 DEBUG TRAIN Batch 55/31400 loss 7.839131 loss_att 12.256853 loss_ctc 10.163733 loss_rnnt 6.603347 hw_loss 0.079301 lr 0.00025681 rank 1
2023-03-01 19:53:21,447 DEBUG TRAIN Batch 55/31400 loss 7.355882 loss_att 8.227248 loss_ctc 10.733096 loss_rnnt 6.585935 hw_loss 0.272586 lr 0.00025681 rank 0
2023-03-01 19:53:43,668 DEBUG TRAIN Batch 55/31500 loss 3.272065 loss_att 4.779369 loss_ctc 3.955318 loss_rnnt 2.719770 hw_loss 0.299500 lr 0.00025680 rank 1
2023-03-01 19:53:43,672 DEBUG TRAIN Batch 55/31500 loss 14.809754 loss_att 18.749434 loss_ctc 27.771505 loss_rnnt 12.158694 hw_loss 0.252919 lr 0.00025680 rank 0
2023-03-01 19:54:06,434 DEBUG TRAIN Batch 55/31600 loss 13.087223 loss_att 10.951069 loss_ctc 25.469055 loss_rnnt 11.761182 hw_loss 0.191928 lr 0.00025679 rank 1
2023-03-01 19:54:06,436 DEBUG TRAIN Batch 55/31600 loss 5.297686 loss_att 10.604034 loss_ctc 8.503263 loss_rnnt 3.720996 hw_loss 0.165017 lr 0.00025679 rank 0
2023-03-01 19:54:40,485 DEBUG TRAIN Batch 55/31700 loss 12.723082 loss_att 13.684062 loss_ctc 24.982643 loss_rnnt 10.814253 hw_loss 0.153797 lr 0.00025678 rank 1
2023-03-01 19:54:40,487 DEBUG TRAIN Batch 55/31700 loss 2.369160 loss_att 4.286049 loss_ctc 6.252227 loss_rnnt 1.325932 hw_loss 0.266454 lr 0.00025678 rank 0
2023-03-01 19:55:02,756 DEBUG TRAIN Batch 55/31800 loss 2.466196 loss_att 5.590022 loss_ctc 7.199975 loss_rnnt 1.084611 hw_loss 0.235593 lr 0.00025678 rank 1
2023-03-01 19:55:02,757 DEBUG TRAIN Batch 55/31800 loss 9.298435 loss_att 11.020731 loss_ctc 14.453669 loss_rnnt 8.170444 hw_loss 0.180315 lr 0.00025678 rank 0
2023-03-01 19:55:24,989 DEBUG TRAIN Batch 55/31900 loss 5.472759 loss_att 7.773335 loss_ctc 8.959139 loss_rnnt 4.459129 hw_loss 0.166244 lr 0.00025677 rank 1
2023-03-01 19:55:24,989 DEBUG TRAIN Batch 55/31900 loss 3.589989 loss_att 6.362599 loss_ctc 7.677148 loss_rnnt 2.426556 hw_loss 0.119916 lr 0.00025677 rank 0
2023-03-01 19:56:00,226 DEBUG TRAIN Batch 55/32000 loss 8.939884 loss_att 12.114378 loss_ctc 17.509394 loss_rnnt 7.069816 hw_loss 0.173563 lr 0.00025676 rank 1
2023-03-01 19:56:00,228 DEBUG TRAIN Batch 55/32000 loss 5.936595 loss_att 8.351265 loss_ctc 12.297536 loss_rnnt 4.478733 hw_loss 0.237757 lr 0.00025676 rank 0
2023-03-01 19:56:22,611 DEBUG TRAIN Batch 55/32100 loss 9.341222 loss_att 11.935018 loss_ctc 13.118206 loss_rnnt 8.153343 hw_loss 0.310352 lr 0.00025675 rank 1
2023-03-01 19:56:22,612 DEBUG TRAIN Batch 55/32100 loss 6.367736 loss_att 7.457038 loss_ctc 10.156863 loss_rnnt 5.573003 hw_loss 0.134355 lr 0.00025675 rank 0
2023-03-01 19:56:44,843 DEBUG TRAIN Batch 55/32200 loss 14.189877 loss_att 15.424976 loss_ctc 23.648077 loss_rnnt 12.586944 hw_loss 0.177785 lr 0.00025674 rank 1
2023-03-01 19:56:44,844 DEBUG TRAIN Batch 55/32200 loss 3.025594 loss_att 7.460757 loss_ctc 7.487785 loss_rnnt 1.393210 hw_loss 0.281986 lr 0.00025674 rank 0
2023-03-01 19:57:07,655 DEBUG TRAIN Batch 55/32300 loss 7.164379 loss_att 10.836551 loss_ctc 9.758598 loss_rnnt 5.944597 hw_loss 0.261471 lr 0.00025673 rank 1
2023-03-01 19:57:07,655 DEBUG TRAIN Batch 55/32300 loss 7.169264 loss_att 10.669494 loss_ctc 13.567088 loss_rnnt 5.455502 hw_loss 0.301262 lr 0.00025673 rank 0
2023-03-01 19:57:42,556 DEBUG TRAIN Batch 55/32400 loss 10.869225 loss_att 11.199645 loss_ctc 16.855665 loss_rnnt 9.800632 hw_loss 0.383094 lr 0.00025673 rank 1
2023-03-01 19:57:42,560 DEBUG TRAIN Batch 55/32400 loss 10.382269 loss_att 12.892321 loss_ctc 15.154564 loss_rnnt 9.133656 hw_loss 0.206806 lr 0.00025673 rank 0
2023-03-01 19:58:04,734 DEBUG TRAIN Batch 55/32500 loss 6.453118 loss_att 9.017359 loss_ctc 7.500165 loss_rnnt 5.729986 hw_loss 0.132520 lr 0.00025672 rank 1
2023-03-01 19:58:04,736 DEBUG TRAIN Batch 55/32500 loss 4.492176 loss_att 8.933602 loss_ctc 6.575493 loss_rnnt 3.265635 hw_loss 0.113399 lr 0.00025672 rank 0
2023-03-01 19:58:27,353 DEBUG TRAIN Batch 55/32600 loss 11.847368 loss_att 15.042131 loss_ctc 18.202114 loss_rnnt 10.336548 hw_loss 0.046066 lr 0.00025671 rank 1
2023-03-01 19:58:27,356 DEBUG TRAIN Batch 55/32600 loss 6.388343 loss_att 8.922739 loss_ctc 9.178035 loss_rnnt 5.375755 hw_loss 0.250780 lr 0.00025671 rank 0
2023-03-01 19:59:01,809 DEBUG TRAIN Batch 55/32700 loss 6.684723 loss_att 9.614031 loss_ctc 9.273984 loss_rnnt 5.619440 hw_loss 0.251600 lr 0.00025670 rank 1
2023-03-01 19:59:01,834 DEBUG TRAIN Batch 55/32700 loss 9.300074 loss_att 8.943377 loss_ctc 14.101048 loss_rnnt 8.501664 hw_loss 0.430533 lr 0.00025670 rank 0
2023-03-01 19:59:24,037 DEBUG TRAIN Batch 55/32800 loss 5.618458 loss_att 7.613164 loss_ctc 8.410225 loss_rnnt 4.757754 hw_loss 0.167864 lr 0.00025669 rank 1
2023-03-01 19:59:24,038 DEBUG TRAIN Batch 55/32800 loss 2.325733 loss_att 6.888991 loss_ctc 4.048464 loss_rnnt 1.082613 hw_loss 0.188944 lr 0.00025669 rank 0
2023-03-01 19:59:46,722 DEBUG TRAIN Batch 55/32900 loss 11.907351 loss_att 15.568921 loss_ctc 22.671837 loss_rnnt 9.619162 hw_loss 0.226143 lr 0.00025668 rank 1
2023-03-01 19:59:46,724 DEBUG TRAIN Batch 55/32900 loss 5.389160 loss_att 10.158292 loss_ctc 6.501542 loss_rnnt 4.205327 hw_loss 0.153167 lr 0.00025668 rank 0
2023-03-01 20:00:09,405 DEBUG TRAIN Batch 55/33000 loss 8.025133 loss_att 8.396763 loss_ctc 13.040937 loss_rnnt 7.136431 hw_loss 0.273004 lr 0.00025667 rank 1
2023-03-01 20:00:09,407 DEBUG TRAIN Batch 55/33000 loss 6.027450 loss_att 12.368596 loss_ctc 12.136661 loss_rnnt 3.770488 hw_loss 0.326570 lr 0.00025667 rank 0
2023-03-01 20:00:33,441 DEBUG TRAIN Batch 55/33100 loss 2.318773 loss_att 6.217812 loss_ctc 5.714725 loss_rnnt 0.985248 hw_loss 0.189231 lr 0.00025667 rank 1
2023-03-01 20:00:33,442 DEBUG TRAIN Batch 55/33100 loss 2.430606 loss_att 4.849670 loss_ctc 2.975986 loss_rnnt 1.719297 hw_loss 0.290211 lr 0.00025667 rank 0
2023-03-01 20:00:57,201 DEBUG TRAIN Batch 55/33200 loss 6.206439 loss_att 7.814540 loss_ctc 10.614784 loss_rnnt 5.144357 hw_loss 0.286280 lr 0.00025666 rank 0
2023-03-01 20:00:57,202 DEBUG TRAIN Batch 55/33200 loss 11.460243 loss_att 15.199464 loss_ctc 22.346060 loss_rnnt 9.164667 hw_loss 0.180542 lr 0.00025666 rank 1
2023-03-01 20:01:19,554 DEBUG TRAIN Batch 55/33300 loss 5.850532 loss_att 9.661146 loss_ctc 13.371815 loss_rnnt 3.990245 hw_loss 0.178735 lr 0.00025665 rank 1
2023-03-01 20:01:19,554 DEBUG TRAIN Batch 55/33300 loss 15.875284 loss_att 18.630392 loss_ctc 31.107412 loss_rnnt 13.195356 hw_loss 0.183669 lr 0.00025665 rank 0
2023-03-01 20:01:38,334 DEBUG CV Batch 55/0 loss 1.302819 loss_att 1.165421 loss_ctc 1.955021 loss_rnnt 1.089366 hw_loss 0.288698 history loss 1.254566 rank 1
2023-03-01 20:01:38,334 DEBUG CV Batch 55/0 loss 1.302819 loss_att 1.165421 loss_ctc 1.955021 loss_rnnt 1.089366 hw_loss 0.288698 history loss 1.254566 rank 0
2023-03-01 20:01:45,851 DEBUG CV Batch 55/100 loss 3.191813 loss_att 4.286662 loss_ctc 6.589993 loss_rnnt 2.413540 hw_loss 0.199148 history loss 2.999064 rank 0
2023-03-01 20:01:45,860 DEBUG CV Batch 55/100 loss 3.191813 loss_att 4.286662 loss_ctc 6.589993 loss_rnnt 2.413540 hw_loss 0.199148 history loss 2.999064 rank 1
2023-03-01 20:01:55,581 DEBUG CV Batch 55/200 loss 6.491848 loss_att 8.504442 loss_ctc 8.243221 loss_rnnt 5.817140 hw_loss 0.072510 history loss 3.551062 rank 1
2023-03-01 20:01:55,591 DEBUG CV Batch 55/200 loss 6.491848 loss_att 8.504442 loss_ctc 8.243221 loss_rnnt 5.817140 hw_loss 0.072510 history loss 3.551062 rank 0
2023-03-01 20:02:04,035 DEBUG CV Batch 55/300 loss 2.655238 loss_att 3.331206 loss_ctc 5.428164 loss_rnnt 2.008935 hw_loss 0.265100 history loss 3.699343 rank 0
2023-03-01 20:02:04,243 DEBUG CV Batch 55/300 loss 2.655238 loss_att 3.331206 loss_ctc 5.428164 loss_rnnt 2.008935 hw_loss 0.265100 history loss 3.699343 rank 1
2023-03-01 20:02:12,498 DEBUG CV Batch 55/400 loss 20.521675 loss_att 71.478867 loss_ctc 18.106350 loss_rnnt 10.603687 hw_loss 0.091114 history loss 4.517171 rank 0
2023-03-01 20:02:12,642 DEBUG CV Batch 55/400 loss 20.521675 loss_att 71.478867 loss_ctc 18.106350 loss_rnnt 10.603687 hw_loss 0.091114 history loss 4.517171 rank 1
2023-03-01 20:02:19,366 DEBUG CV Batch 55/500 loss 3.269717 loss_att 4.023876 loss_ctc 6.251220 loss_rnnt 2.605164 hw_loss 0.217852 history loss 5.112939 rank 0
2023-03-01 20:02:19,488 DEBUG CV Batch 55/500 loss 3.269717 loss_att 4.023876 loss_ctc 6.251220 loss_rnnt 2.605164 hw_loss 0.217852 history loss 5.112939 rank 1
2023-03-01 20:02:28,066 DEBUG CV Batch 55/600 loss 7.239055 loss_att 6.524756 loss_ctc 9.682654 loss_rnnt 6.839884 hw_loss 0.405407 history loss 6.001005 rank 1
2023-03-01 20:02:28,233 DEBUG CV Batch 55/600 loss 7.239055 loss_att 6.524756 loss_ctc 9.682654 loss_rnnt 6.839884 hw_loss 0.405407 history loss 6.001005 rank 0
2023-03-01 20:02:35,776 DEBUG CV Batch 55/700 loss 14.511363 loss_att 30.512121 loss_ctc 11.958199 loss_rnnt 11.651245 hw_loss 0.000728 history loss 6.546844 rank 1
2023-03-01 20:02:35,989 DEBUG CV Batch 55/700 loss 14.511363 loss_att 30.512121 loss_ctc 11.958199 loss_rnnt 11.651245 hw_loss 0.000728 history loss 6.546844 rank 0
2023-03-01 20:02:43,240 DEBUG CV Batch 55/800 loss 4.959366 loss_att 6.649698 loss_ctc 10.788702 loss_rnnt 3.717976 hw_loss 0.236398 history loss 6.073080 rank 1
2023-03-01 20:02:43,466 DEBUG CV Batch 55/800 loss 4.959366 loss_att 6.649698 loss_ctc 10.788702 loss_rnnt 3.717976 hw_loss 0.236398 history loss 6.073080 rank 0
2023-03-01 20:02:52,965 DEBUG CV Batch 55/900 loss 11.128971 loss_att 11.912008 loss_ctc 18.767656 loss_rnnt 9.826099 hw_loss 0.239574 history loss 5.904503 rank 1
2023-03-01 20:02:53,241 DEBUG CV Batch 55/900 loss 11.128971 loss_att 11.912008 loss_ctc 18.767656 loss_rnnt 9.826099 hw_loss 0.239574 history loss 5.904503 rank 0
2023-03-01 20:03:01,556 DEBUG CV Batch 55/1000 loss 4.719969 loss_att 4.806995 loss_ctc 5.542332 loss_rnnt 4.465506 hw_loss 0.238894 history loss 5.715361 rank 1
2023-03-01 20:03:01,811 DEBUG CV Batch 55/1000 loss 4.719969 loss_att 4.806995 loss_ctc 5.542332 loss_rnnt 4.465506 hw_loss 0.238894 history loss 5.715361 rank 0
2023-03-01 20:03:09,854 DEBUG CV Batch 55/1100 loss 4.561465 loss_att 4.454979 loss_ctc 8.105479 loss_rnnt 3.956955 hw_loss 0.287385 history loss 5.680541 rank 1
2023-03-01 20:03:10,133 DEBUG CV Batch 55/1100 loss 4.561465 loss_att 4.454979 loss_ctc 8.105479 loss_rnnt 3.956955 hw_loss 0.287385 history loss 5.680541 rank 0
2023-03-01 20:03:16,678 DEBUG CV Batch 55/1200 loss 5.793016 loss_att 6.087034 loss_ctc 7.397195 loss_rnnt 5.427906 hw_loss 0.173282 history loss 5.948765 rank 1
2023-03-01 20:03:17,000 DEBUG CV Batch 55/1200 loss 5.793016 loss_att 6.087034 loss_ctc 7.397195 loss_rnnt 5.427906 hw_loss 0.173282 history loss 5.948765 rank 0
2023-03-01 20:03:24,992 DEBUG CV Batch 55/1300 loss 5.004058 loss_att 4.468395 loss_ctc 7.443187 loss_rnnt 4.643723 hw_loss 0.266719 history loss 6.244308 rank 1
2023-03-01 20:03:25,334 DEBUG CV Batch 55/1300 loss 5.004058 loss_att 4.468395 loss_ctc 7.443187 loss_rnnt 4.643723 hw_loss 0.266719 history loss 6.244308 rank 0
2023-03-01 20:03:32,546 DEBUG CV Batch 55/1400 loss 3.966552 loss_att 9.088393 loss_ctc 6.833052 loss_rnnt 2.559595 hw_loss 0.000728 history loss 6.517642 rank 1
2023-03-01 20:03:32,936 DEBUG CV Batch 55/1400 loss 3.966552 loss_att 9.088393 loss_ctc 6.833052 loss_rnnt 2.559595 hw_loss 0.000728 history loss 6.517642 rank 0
2023-03-01 20:03:40,197 DEBUG CV Batch 55/1500 loss 6.972997 loss_att 7.539084 loss_ctc 7.890727 loss_rnnt 6.604222 hw_loss 0.249738 history loss 6.377194 rank 1
2023-03-01 20:03:40,611 DEBUG CV Batch 55/1500 loss 6.972997 loss_att 7.539084 loss_ctc 7.890727 loss_rnnt 6.604222 hw_loss 0.249738 history loss 6.377194 rank 0
2023-03-01 20:03:49,643 DEBUG CV Batch 55/1600 loss 11.020288 loss_att 12.560082 loss_ctc 11.590351 loss_rnnt 10.521615 hw_loss 0.215076 history loss 6.332343 rank 1
2023-03-01 20:03:50,102 DEBUG CV Batch 55/1600 loss 11.020288 loss_att 12.560082 loss_ctc 11.590351 loss_rnnt 10.521615 hw_loss 0.215076 history loss 6.332343 rank 0
2023-03-01 20:03:58,521 DEBUG CV Batch 55/1700 loss 7.438795 loss_att 6.825509 loss_ctc 13.582736 loss_rnnt 6.534747 hw_loss 0.389087 history loss 6.264017 rank 1
2023-03-01 20:03:59,012 DEBUG CV Batch 55/1700 loss 7.438795 loss_att 6.825509 loss_ctc 13.582736 loss_rnnt 6.534747 hw_loss 0.389087 history loss 6.264017 rank 0
2023-03-01 20:04:05,166 INFO Epoch 55 CV info cv_loss 6.242165262694992
2023-03-01 20:04:05,167 INFO Epoch 56 TRAIN info lr 0.0002566428758911699
2023-03-01 20:04:05,168 INFO using accumulate grad, new batch size is 4 times larger than before
2023-03-01 20:04:05,681 INFO Epoch 55 CV info cv_loss 6.2421652637976655
2023-03-01 20:04:05,681 INFO Checkpoint: save to checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune_100head/55.pt
2023-03-01 20:04:06,104 INFO Epoch 56 TRAIN info lr 0.00025664693293014066
2023-03-01 20:04:06,106 INFO using accumulate grad, new batch size is 4 times larger than before
2023-03-01 20:04:39,025 DEBUG TRAIN Batch 56/0 loss 4.312840 loss_att 4.771679 loss_ctc 6.964856 loss_rnnt 3.648582 hw_loss 0.410414 lr 0.00025664 rank 1
2023-03-01 20:04:39,035 DEBUG TRAIN Batch 56/0 loss 7.354834 loss_att 7.004553 loss_ctc 9.231241 loss_rnnt 7.077973 hw_loss 0.181366 lr 0.00025665 rank 0
2023-03-01 20:05:00,322 DEBUG TRAIN Batch 56/100 loss 6.223193 loss_att 8.761362 loss_ctc 15.612906 loss_rnnt 4.405041 hw_loss 0.109793 lr 0.00025663 rank 1
2023-03-01 20:05:00,324 DEBUG TRAIN Batch 56/100 loss 3.817541 loss_att 5.952134 loss_ctc 4.691524 loss_rnnt 3.164824 hw_loss 0.204876 lr 0.00025664 rank 0
2023-03-01 20:05:22,017 DEBUG TRAIN Batch 56/200 loss 3.949849 loss_att 6.716497 loss_ctc 6.754113 loss_rnnt 2.875669 hw_loss 0.275526 lr 0.00025663 rank 1
2023-03-01 20:05:22,019 DEBUG TRAIN Batch 56/200 loss 8.897477 loss_att 9.014422 loss_ctc 10.618793 loss_rnnt 8.570201 hw_loss 0.139461 lr 0.00025663 rank 0
2023-03-01 20:05:43,665 DEBUG TRAIN Batch 56/300 loss 4.602071 loss_att 7.823310 loss_ctc 8.191080 loss_rnnt 3.447805 hw_loss 0.059032 lr 0.00025662 rank 1
2023-03-01 20:05:43,667 DEBUG TRAIN Batch 56/300 loss 4.746897 loss_att 6.674792 loss_ctc 5.735892 loss_rnnt 4.161256 hw_loss 0.127866 lr 0.00025662 rank 0
2023-03-01 20:06:15,873 DEBUG TRAIN Batch 56/400 loss 6.930034 loss_att 8.426237 loss_ctc 13.886522 loss_rnnt 5.640960 hw_loss 0.116816 lr 0.00025661 rank 1
2023-03-01 20:06:15,875 DEBUG TRAIN Batch 56/400 loss 10.217858 loss_att 12.411664 loss_ctc 15.255579 loss_rnnt 8.995352 hw_loss 0.210093 lr 0.00025661 rank 0
2023-03-01 20:06:37,884 DEBUG TRAIN Batch 56/500 loss 10.583053 loss_att 15.521914 loss_ctc 15.429733 loss_rnnt 8.897834 hw_loss 0.096042 lr 0.00025660 rank 1
2023-03-01 20:06:37,884 DEBUG TRAIN Batch 56/500 loss 4.279433 loss_att 7.627677 loss_ctc 8.364562 loss_rnnt 3.023487 hw_loss 0.078026 lr 0.00025660 rank 0
2023-03-01 20:07:00,985 DEBUG TRAIN Batch 56/600 loss 4.579859 loss_att 6.300588 loss_ctc 7.755447 loss_rnnt 3.680133 hw_loss 0.247816 lr 0.00025659 rank 1
2023-03-01 20:07:00,987 DEBUG TRAIN Batch 56/600 loss 2.545785 loss_att 5.239909 loss_ctc 4.723689 loss_rnnt 1.604106 hw_loss 0.210874 lr 0.00025660 rank 0
2023-03-01 20:07:23,010 DEBUG TRAIN Batch 56/700 loss 2.324349 loss_att 4.859631 loss_ctc 5.960931 loss_rnnt 1.190654 hw_loss 0.265802 lr 0.00025658 rank 1
2023-03-01 20:07:23,013 DEBUG TRAIN Batch 56/700 loss 4.896931 loss_att 8.070568 loss_ctc 8.005061 loss_rnnt 3.760099 hw_loss 0.164412 lr 0.00025659 rank 0
2023-03-01 20:07:54,339 DEBUG TRAIN Batch 56/800 loss 11.069115 loss_att 13.408009 loss_ctc 12.991237 loss_rnnt 10.266665 hw_loss 0.146981 lr 0.00025657 rank 1
2023-03-01 20:07:54,340 DEBUG TRAIN Batch 56/800 loss 3.133686 loss_att 4.828007 loss_ctc 4.432252 loss_rnnt 2.463726 hw_loss 0.296163 lr 0.00025658 rank 0
2023-03-01 20:08:16,443 DEBUG TRAIN Batch 56/900 loss 2.859713 loss_att 5.947702 loss_ctc 7.392747 loss_rnnt 1.596532 hw_loss 0.077211 lr 0.00025657 rank 1
2023-03-01 20:08:16,444 DEBUG TRAIN Batch 56/900 loss 3.898430 loss_att 7.203775 loss_ctc 6.826544 loss_rnnt 2.809346 hw_loss 0.070498 lr 0.00025657 rank 0
2023-03-01 20:08:37,923 DEBUG TRAIN Batch 56/1000 loss 7.130886 loss_att 9.480765 loss_ctc 13.213992 loss_rnnt 5.794934 hw_loss 0.102927 lr 0.00025656 rank 1
2023-03-01 20:08:37,925 DEBUG TRAIN Batch 56/1000 loss 8.725575 loss_att 12.045373 loss_ctc 11.553566 loss_rnnt 7.555991 hw_loss 0.241052 lr 0.00025656 rank 0
2023-03-01 20:09:09,692 DEBUG TRAIN Batch 56/1100 loss 7.659062 loss_att 10.255553 loss_ctc 13.234659 loss_rnnt 6.267132 hw_loss 0.242285 lr 0.00025655 rank 1
2023-03-01 20:09:09,694 DEBUG TRAIN Batch 56/1100 loss 5.814127 loss_att 7.345655 loss_ctc 10.009365 loss_rnnt 4.853464 hw_loss 0.178112 lr 0.00025655 rank 0
2023-03-01 20:09:31,452 DEBUG TRAIN Batch 56/1200 loss 9.000644 loss_att 9.345454 loss_ctc 13.797183 loss_rnnt 8.188899 hw_loss 0.193583 lr 0.00025654 rank 1
2023-03-01 20:09:31,454 DEBUG TRAIN Batch 56/1200 loss 8.320743 loss_att 10.155049 loss_ctc 12.514879 loss_rnnt 7.355820 hw_loss 0.072832 lr 0.00025655 rank 0
2023-03-01 20:09:53,768 DEBUG TRAIN Batch 56/1300 loss 7.323453 loss_att 8.671916 loss_ctc 7.211902 loss_rnnt 6.982471 hw_loss 0.161558 lr 0.00025653 rank 1
2023-03-01 20:09:53,770 DEBUG TRAIN Batch 56/1300 loss 4.591012 loss_att 9.103836 loss_ctc 12.780905 loss_rnnt 2.536890 hw_loss 0.111695 lr 0.00025654 rank 0
2023-03-01 20:10:15,723 DEBUG TRAIN Batch 56/1400 loss 5.283488 loss_att 8.370858 loss_ctc 9.825068 loss_rnnt 3.966347 hw_loss 0.176481 lr 0.00025652 rank 1
2023-03-01 20:10:15,724 DEBUG TRAIN Batch 56/1400 loss 6.577527 loss_att 9.724694 loss_ctc 10.173102 loss_rnnt 5.403566 hw_loss 0.122094 lr 0.00025653 rank 0
2023-03-01 20:10:47,114 DEBUG TRAIN Batch 56/1500 loss 6.546591 loss_att 11.774390 loss_ctc 14.116491 loss_rnnt 4.321658 hw_loss 0.318850 lr 0.00025652 rank 1
2023-03-01 20:10:47,115 DEBUG TRAIN Batch 56/1500 loss 4.000229 loss_att 8.002951 loss_ctc 8.777210 loss_rnnt 2.421416 hw_loss 0.265008 lr 0.00025652 rank 0
2023-03-01 20:11:08,058 DEBUG TRAIN Batch 56/1600 loss 3.059009 loss_att 6.964295 loss_ctc 5.106947 loss_rnnt 1.837761 hw_loss 0.313372 lr 0.00025651 rank 1
2023-03-01 20:11:08,060 DEBUG TRAIN Batch 56/1600 loss 3.962059 loss_att 6.090315 loss_ctc 7.687088 loss_rnnt 2.898629 hw_loss 0.264578 lr 0.00025651 rank 0
2023-03-01 20:11:29,294 DEBUG TRAIN Batch 56/1700 loss 4.171696 loss_att 6.017327 loss_ctc 6.836121 loss_rnnt 3.306556 hw_loss 0.263917 lr 0.00025650 rank 1
2023-03-01 20:11:29,297 DEBUG TRAIN Batch 56/1700 loss 7.157667 loss_att 8.146683 loss_ctc 8.915800 loss_rnnt 6.635942 hw_loss 0.167819 lr 0.00025650 rank 0
2023-03-01 20:12:01,629 DEBUG TRAIN Batch 56/1800 loss 10.257232 loss_att 13.140467 loss_ctc 14.624367 loss_rnnt 8.969829 hw_loss 0.240884 lr 0.00025649 rank 1
2023-03-01 20:12:01,629 DEBUG TRAIN Batch 56/1800 loss 11.864969 loss_att 15.552813 loss_ctc 22.722836 loss_rnnt 9.580539 hw_loss 0.185900 lr 0.00025649 rank 0
2023-03-01 20:12:23,645 DEBUG TRAIN Batch 56/1900 loss 4.884861 loss_att 5.810098 loss_ctc 8.404117 loss_rnnt 4.081669 hw_loss 0.279207 lr 0.00025648 rank 1
2023-03-01 20:12:23,646 DEBUG TRAIN Batch 56/1900 loss 8.259251 loss_att 9.844239 loss_ctc 12.939579 loss_rnnt 7.205380 hw_loss 0.211554 lr 0.00025649 rank 0
2023-03-01 20:12:44,879 DEBUG TRAIN Batch 56/2000 loss 3.302049 loss_att 8.946550 loss_ctc 6.279294 loss_rnnt 1.690007 hw_loss 0.161578 lr 0.00025647 rank 1
2023-03-01 20:12:44,880 DEBUG TRAIN Batch 56/2000 loss 10.628802 loss_att 10.361707 loss_ctc 11.883872 loss_rnnt 10.404101 hw_loss 0.207708 lr 0.00025648 rank 0
2023-03-01 20:13:06,732 DEBUG TRAIN Batch 56/2100 loss 4.877066 loss_att 8.402687 loss_ctc 7.024818 loss_rnnt 3.810137 hw_loss 0.141445 lr 0.00025647 rank 1
2023-03-01 20:13:06,734 DEBUG TRAIN Batch 56/2100 loss 4.515673 loss_att 8.471960 loss_ctc 8.203447 loss_rnnt 3.142650 hw_loss 0.168867 lr 0.00025647 rank 0
2023-03-01 20:13:38,483 DEBUG TRAIN Batch 56/2200 loss 6.945717 loss_att 9.236853 loss_ctc 10.740671 loss_rnnt 5.891095 hw_loss 0.169502 lr 0.00025646 rank 1
2023-03-01 20:13:38,484 DEBUG TRAIN Batch 56/2200 loss 4.849266 loss_att 7.087857 loss_ctc 7.677362 loss_rnnt 3.920468 hw_loss 0.195000 lr 0.00025646 rank 0
2023-03-01 20:13:59,634 DEBUG TRAIN Batch 56/2300 loss 5.060034 loss_att 7.555149 loss_ctc 7.851299 loss_rnnt 4.137696 hw_loss 0.095899 lr 0.00025645 rank 1
2023-03-01 20:13:59,635 DEBUG TRAIN Batch 56/2300 loss 8.543934 loss_att 13.047108 loss_ctc 22.402504 loss_rnnt 5.719791 hw_loss 0.141934 lr 0.00025645 rank 0
2023-03-01 20:14:21,170 DEBUG TRAIN Batch 56/2400 loss 2.753189 loss_att 4.366199 loss_ctc 2.939240 loss_rnnt 2.327797 hw_loss 0.146220 lr 0.00025644 rank 1
2023-03-01 20:14:21,172 DEBUG TRAIN Batch 56/2400 loss 6.918507 loss_att 10.118197 loss_ctc 14.386652 loss_rnnt 5.183887 hw_loss 0.185494 lr 0.00025644 rank 0
2023-03-01 20:14:53,745 DEBUG TRAIN Batch 56/2500 loss 5.251950 loss_att 6.966174 loss_ctc 7.204768 loss_rnnt 4.520814 hw_loss 0.239842 lr 0.00025643 rank 1
2023-03-01 20:14:53,747 DEBUG TRAIN Batch 56/2500 loss 4.649308 loss_att 5.853612 loss_ctc 9.185576 loss_rnnt 3.714198 hw_loss 0.167651 lr 0.00025644 rank 0
2023-03-01 20:15:15,197 DEBUG TRAIN Batch 56/2600 loss 5.388381 loss_att 6.467893 loss_ctc 7.674434 loss_rnnt 4.744615 hw_loss 0.230731 lr 0.00025642 rank 1
2023-03-01 20:15:15,200 DEBUG TRAIN Batch 56/2600 loss 4.758227 loss_att 7.724460 loss_ctc 6.633930 loss_rnnt 3.795783 hw_loss 0.223321 lr 0.00025643 rank 0
2023-03-01 20:15:36,751 DEBUG TRAIN Batch 56/2700 loss 6.040586 loss_att 9.501614 loss_ctc 12.946203 loss_rnnt 4.381197 hw_loss 0.087063 lr 0.00025641 rank 1
2023-03-01 20:15:36,753 DEBUG TRAIN Batch 56/2700 loss 5.514756 loss_att 6.721832 loss_ctc 5.510732 loss_rnnt 5.196010 hw_loss 0.146001 lr 0.00025642 rank 0
2023-03-01 20:15:58,275 DEBUG TRAIN Batch 56/2800 loss 3.294354 loss_att 5.276578 loss_ctc 4.542912 loss_rnnt 2.613372 hw_loss 0.221370 lr 0.00025641 rank 1
2023-03-01 20:15:58,276 DEBUG TRAIN Batch 56/2800 loss 2.943358 loss_att 6.620876 loss_ctc 6.102989 loss_rnnt 1.708691 hw_loss 0.146024 lr 0.00025641 rank 0
2023-03-01 20:16:30,025 DEBUG TRAIN Batch 56/2900 loss 7.443854 loss_att 12.750051 loss_ctc 14.945398 loss_rnnt 5.335151 hw_loss 0.088608 lr 0.00025640 rank 1
2023-03-01 20:16:30,027 DEBUG TRAIN Batch 56/2900 loss 10.294670 loss_att 11.664880 loss_ctc 14.513294 loss_rnnt 9.378621 hw_loss 0.149108 lr 0.00025640 rank 0
2023-03-01 20:16:51,363 DEBUG TRAIN Batch 56/3000 loss 3.139215 loss_att 5.675042 loss_ctc 5.627507 loss_rnnt 2.155559 hw_loss 0.271348 lr 0.00025639 rank 1
2023-03-01 20:16:51,364 DEBUG TRAIN Batch 56/3000 loss 9.797118 loss_att 12.105570 loss_ctc 19.220242 loss_rnnt 7.917955 hw_loss 0.301978 lr 0.00025639 rank 0
2023-03-01 20:17:13,260 DEBUG TRAIN Batch 56/3100 loss 7.044553 loss_att 7.836786 loss_ctc 9.982525 loss_rnnt 6.386688 hw_loss 0.201918 lr 0.00025638 rank 1
2023-03-01 20:17:13,261 DEBUG TRAIN Batch 56/3100 loss 9.278991 loss_att 9.582379 loss_ctc 12.699583 loss_rnnt 8.678595 hw_loss 0.156825 lr 0.00025638 rank 0
2023-03-01 20:17:35,441 DEBUG TRAIN Batch 56/3200 loss 8.267223 loss_att 11.194994 loss_ctc 12.976919 loss_rnnt 6.887693 hw_loss 0.311283 lr 0.00025637 rank 1
2023-03-01 20:17:35,444 DEBUG TRAIN Batch 56/3200 loss 4.932409 loss_att 10.835217 loss_ctc 9.277715 loss_rnnt 3.116149 hw_loss 0.105608 lr 0.00025638 rank 0
2023-03-01 20:18:07,485 DEBUG TRAIN Batch 56/3300 loss 11.174555 loss_att 14.898645 loss_ctc 17.780336 loss_rnnt 9.446730 hw_loss 0.191692 lr 0.00025636 rank 1
2023-03-01 20:18:07,488 DEBUG TRAIN Batch 56/3300 loss 4.997937 loss_att 10.026562 loss_ctc 12.660637 loss_rnnt 2.888031 hw_loss 0.154666 lr 0.00025637 rank 0
2023-03-01 20:18:29,014 DEBUG TRAIN Batch 56/3400 loss 6.575488 loss_att 9.211723 loss_ctc 11.803475 loss_rnnt 5.232435 hw_loss 0.222637 lr 0.00025636 rank 1
2023-03-01 20:18:29,016 DEBUG TRAIN Batch 56/3400 loss 7.481009 loss_att 8.614072 loss_ctc 6.406255 loss_rnnt 7.255821 hw_loss 0.266018 lr 0.00025636 rank 0
2023-03-01 20:18:50,376 DEBUG TRAIN Batch 56/3500 loss 2.658609 loss_att 5.626883 loss_ctc 5.110644 loss_rnnt 1.615762 hw_loss 0.229228 lr 0.00025635 rank 1
2023-03-01 20:18:50,378 DEBUG TRAIN Batch 56/3500 loss 5.977798 loss_att 7.698991 loss_ctc 9.112405 loss_rnnt 5.077928 hw_loss 0.258158 lr 0.00025635 rank 0
2023-03-01 20:19:23,323 DEBUG TRAIN Batch 56/3600 loss 4.582458 loss_att 8.573450 loss_ctc 9.514060 loss_rnnt 3.040479 hw_loss 0.161690 lr 0.00025634 rank 1
2023-03-01 20:19:23,324 DEBUG TRAIN Batch 56/3600 loss 7.257908 loss_att 8.766777 loss_ctc 7.535686 loss_rnnt 6.783465 hw_loss 0.254309 lr 0.00025634 rank 0
2023-03-01 20:19:45,120 DEBUG TRAIN Batch 56/3700 loss 6.297792 loss_att 7.862082 loss_ctc 10.644633 loss_rnnt 5.262066 hw_loss 0.268665 lr 0.00025633 rank 1
2023-03-01 20:19:45,121 DEBUG TRAIN Batch 56/3700 loss 11.746839 loss_att 13.405186 loss_ctc 22.096428 loss_rnnt 9.956169 hw_loss 0.148227 lr 0.00025633 rank 0
2023-03-01 20:20:07,304 DEBUG TRAIN Batch 56/3800 loss 4.668784 loss_att 7.427228 loss_ctc 11.447844 loss_rnnt 3.019463 hw_loss 0.363295 lr 0.00025632 rank 1
2023-03-01 20:20:07,307 DEBUG TRAIN Batch 56/3800 loss 8.765045 loss_att 10.216629 loss_ctc 19.192238 loss_rnnt 6.964855 hw_loss 0.224214 lr 0.00025633 rank 0
2023-03-01 20:20:28,663 DEBUG TRAIN Batch 56/3900 loss 15.080044 loss_att 18.227543 loss_ctc 21.559631 loss_rnnt 13.523794 hw_loss 0.117759 lr 0.00025631 rank 1
2023-03-01 20:20:28,662 DEBUG TRAIN Batch 56/3900 loss 7.872196 loss_att 12.076888 loss_ctc 7.188511 loss_rnnt 7.017503 hw_loss 0.196711 lr 0.00025632 rank 0
2023-03-01 20:21:00,278 DEBUG TRAIN Batch 56/4000 loss 3.244081 loss_att 7.331009 loss_ctc 6.495473 loss_rnnt 1.886370 hw_loss 0.200260 lr 0.00025631 rank 1
2023-03-01 20:21:00,280 DEBUG TRAIN Batch 56/4000 loss 4.152265 loss_att 6.414002 loss_ctc 8.504089 loss_rnnt 3.049500 hw_loss 0.131575 lr 0.00025631 rank 0
2023-03-01 20:21:21,132 DEBUG TRAIN Batch 56/4100 loss 7.018893 loss_att 9.348450 loss_ctc 13.414949 loss_rnnt 5.612724 hw_loss 0.163969 lr 0.00025630 rank 1
2023-03-01 20:21:21,133 DEBUG TRAIN Batch 56/4100 loss 6.127545 loss_att 10.343204 loss_ctc 13.317900 loss_rnnt 4.234323 hw_loss 0.171331 lr 0.00025630 rank 0
2023-03-01 20:21:42,710 DEBUG TRAIN Batch 56/4200 loss 4.031032 loss_att 6.067348 loss_ctc 7.444419 loss_rnnt 3.082992 hw_loss 0.160608 lr 0.00025629 rank 1
2023-03-01 20:21:42,713 DEBUG TRAIN Batch 56/4200 loss 3.198194 loss_att 6.204597 loss_ctc 5.638213 loss_rnnt 2.139010 hw_loss 0.248564 lr 0.00025629 rank 0
2023-03-01 20:22:15,155 DEBUG TRAIN Batch 56/4300 loss 12.136135 loss_att 18.055418 loss_ctc 18.628284 loss_rnnt 9.996431 hw_loss 0.169174 lr 0.00025628 rank 1
2023-03-01 20:22:15,157 DEBUG TRAIN Batch 56/4300 loss 4.397357 loss_att 7.402454 loss_ctc 7.323046 loss_rnnt 3.302494 hw_loss 0.194536 lr 0.00025628 rank 0
2023-03-01 20:22:37,950 DEBUG TRAIN Batch 56/4400 loss 7.193784 loss_att 9.841858 loss_ctc 13.676661 loss_rnnt 5.676157 hw_loss 0.231803 lr 0.00025627 rank 1
2023-03-01 20:22:37,951 DEBUG TRAIN Batch 56/4400 loss 5.332837 loss_att 6.558523 loss_ctc 9.526651 loss_rnnt 4.337392 hw_loss 0.358374 lr 0.00025628 rank 0
2023-03-01 20:23:00,103 DEBUG TRAIN Batch 56/4500 loss 2.771685 loss_att 7.899068 loss_ctc 6.791885 loss_rnnt 1.141930 hw_loss 0.127971 lr 0.00025626 rank 1
2023-03-01 20:23:00,104 DEBUG TRAIN Batch 56/4500 loss 5.411429 loss_att 8.667573 loss_ctc 6.460377 loss_rnnt 4.538696 hw_loss 0.153082 lr 0.00025627 rank 0
2023-03-01 20:23:22,220 DEBUG TRAIN Batch 56/4600 loss 3.246327 loss_att 6.800573 loss_ctc 8.173653 loss_rnnt 1.787599 hw_loss 0.170440 lr 0.00025625 rank 1
2023-03-01 20:23:22,221 DEBUG TRAIN Batch 56/4600 loss 10.966459 loss_att 13.670218 loss_ctc 18.859930 loss_rnnt 9.255665 hw_loss 0.220461 lr 0.00025626 rank 0
2023-03-01 20:23:54,229 DEBUG TRAIN Batch 56/4700 loss 9.159288 loss_att 11.287098 loss_ctc 11.183314 loss_rnnt 8.370429 hw_loss 0.175175 lr 0.00025625 rank 1
2023-03-01 20:23:54,231 DEBUG TRAIN Batch 56/4700 loss 10.164872 loss_att 16.355232 loss_ctc 24.304804 loss_rnnt 6.908977 hw_loss 0.248438 lr 0.00025625 rank 0
2023-03-01 20:24:16,080 DEBUG TRAIN Batch 56/4800 loss 4.994583 loss_att 7.609376 loss_ctc 7.099023 loss_rnnt 4.109432 hw_loss 0.153000 lr 0.00025624 rank 1
2023-03-01 20:24:16,081 DEBUG TRAIN Batch 56/4800 loss 11.227150 loss_att 16.608509 loss_ctc 14.870952 loss_rnnt 9.557286 hw_loss 0.202034 lr 0.00025624 rank 0
2023-03-01 20:24:38,557 DEBUG TRAIN Batch 56/4900 loss 8.495817 loss_att 12.201481 loss_ctc 15.426305 loss_rnnt 6.731311 hw_loss 0.186202 lr 0.00025623 rank 1
2023-03-01 20:24:38,558 DEBUG TRAIN Batch 56/4900 loss 6.253466 loss_att 7.246394 loss_ctc 8.761964 loss_rnnt 5.633837 hw_loss 0.162332 lr 0.00025623 rank 0
2023-03-01 20:25:11,918 DEBUG TRAIN Batch 56/5000 loss 12.083387 loss_att 12.795262 loss_ctc 22.072235 loss_rnnt 10.532106 hw_loss 0.144486 lr 0.00025622 rank 1
2023-03-01 20:25:11,920 DEBUG TRAIN Batch 56/5000 loss 10.183962 loss_att 10.562511 loss_ctc 17.279902 loss_rnnt 9.074858 hw_loss 0.163626 lr 0.00025623 rank 0
2023-03-01 20:25:34,153 DEBUG TRAIN Batch 56/5100 loss 3.539066 loss_att 5.792416 loss_ctc 8.119014 loss_rnnt 2.305492 hw_loss 0.322959 lr 0.00025621 rank 1
2023-03-01 20:25:34,155 DEBUG TRAIN Batch 56/5100 loss 6.498815 loss_att 10.251511 loss_ctc 8.100813 loss_rnnt 5.471336 hw_loss 0.118762 lr 0.00025622 rank 0
2023-03-01 20:25:56,639 DEBUG TRAIN Batch 56/5200 loss 1.966753 loss_att 4.256706 loss_ctc 7.390248 loss_rnnt 0.710169 hw_loss 0.141488 lr 0.00025620 rank 1
2023-03-01 20:25:56,640 DEBUG TRAIN Batch 56/5200 loss 7.976151 loss_att 14.380627 loss_ctc 15.927696 loss_rnnt 5.572505 hw_loss 0.117271 lr 0.00025621 rank 0
2023-03-01 20:26:19,546 DEBUG TRAIN Batch 56/5300 loss 5.257964 loss_att 8.136080 loss_ctc 11.102936 loss_rnnt 3.799938 hw_loss 0.193261 lr 0.00025620 rank 1
2023-03-01 20:26:19,549 DEBUG TRAIN Batch 56/5300 loss 9.046173 loss_att 10.748266 loss_ctc 15.039173 loss_rnnt 7.796950 hw_loss 0.205759 lr 0.00025620 rank 0
2023-03-01 20:26:51,441 DEBUG TRAIN Batch 56/5400 loss 7.460106 loss_att 8.643903 loss_ctc 9.007584 loss_rnnt 6.848171 hw_loss 0.316586 lr 0.00025619 rank 1
2023-03-01 20:26:51,444 DEBUG TRAIN Batch 56/5400 loss 8.787088 loss_att 10.022466 loss_ctc 11.870129 loss_rnnt 8.057373 hw_loss 0.134189 lr 0.00025619 rank 0
2023-03-01 20:27:14,185 DEBUG TRAIN Batch 56/5500 loss 11.216870 loss_att 12.559278 loss_ctc 18.897717 loss_rnnt 9.765937 hw_loss 0.296886 lr 0.00025618 rank 1
2023-03-01 20:27:14,187 DEBUG TRAIN Batch 56/5500 loss 4.048851 loss_att 7.982822 loss_ctc 6.953148 loss_rnnt 2.735233 hw_loss 0.261718 lr 0.00025618 rank 0
2023-03-01 20:27:37,167 DEBUG TRAIN Batch 56/5600 loss 6.431494 loss_att 8.079041 loss_ctc 11.357744 loss_rnnt 5.325094 hw_loss 0.225106 lr 0.00025617 rank 0
2023-03-01 20:27:37,167 DEBUG TRAIN Batch 56/5600 loss 3.552629 loss_att 4.377528 loss_ctc 5.494616 loss_rnnt 3.026639 hw_loss 0.191397 lr 0.00025617 rank 1
2023-03-01 20:28:09,122 DEBUG TRAIN Batch 56/5700 loss 9.776943 loss_att 11.590563 loss_ctc 15.323474 loss_rnnt 8.505760 hw_loss 0.316727 lr 0.00025616 rank 1
2023-03-01 20:28:09,124 DEBUG TRAIN Batch 56/5700 loss 6.487970 loss_att 9.111752 loss_ctc 11.854281 loss_rnnt 5.213871 hw_loss 0.063439 lr 0.00025617 rank 0
2023-03-01 20:28:31,752 DEBUG TRAIN Batch 56/5800 loss 6.638815 loss_att 8.179686 loss_ctc 10.476236 loss_rnnt 5.777277 hw_loss 0.078203 lr 0.00025615 rank 1
2023-03-01 20:28:31,753 DEBUG TRAIN Batch 56/5800 loss 4.059440 loss_att 8.499013 loss_ctc 8.461677 loss_rnnt 2.509360 hw_loss 0.141000 lr 0.00025616 rank 0
2023-03-01 20:28:54,250 DEBUG TRAIN Batch 56/5900 loss 6.724164 loss_att 9.086083 loss_ctc 10.456424 loss_rnnt 5.627056 hw_loss 0.238294 lr 0.00025615 rank 1
2023-03-01 20:28:54,252 DEBUG TRAIN Batch 56/5900 loss 7.179771 loss_att 9.776424 loss_ctc 10.260051 loss_rnnt 6.155622 hw_loss 0.176466 lr 0.00025615 rank 0
2023-03-01 20:29:17,021 DEBUG TRAIN Batch 56/6000 loss 2.471807 loss_att 6.169281 loss_ctc 7.380779 loss_rnnt 0.904569 hw_loss 0.324774 lr 0.00025614 rank 1
2023-03-01 20:29:17,023 DEBUG TRAIN Batch 56/6000 loss 12.470724 loss_att 15.623075 loss_ctc 17.157621 loss_rnnt 11.150396 hw_loss 0.121760 lr 0.00025614 rank 0
2023-03-01 20:29:49,705 DEBUG TRAIN Batch 56/6100 loss 10.424420 loss_att 12.508703 loss_ctc 15.402857 loss_rnnt 9.202228 hw_loss 0.265396 lr 0.00025613 rank 1
2023-03-01 20:29:49,707 DEBUG TRAIN Batch 56/6100 loss 4.045492 loss_att 8.225443 loss_ctc 8.291525 loss_rnnt 2.530645 hw_loss 0.211348 lr 0.00025613 rank 0
2023-03-01 20:30:12,211 DEBUG TRAIN Batch 56/6200 loss 7.769265 loss_att 10.094210 loss_ctc 11.787578 loss_rnnt 6.651140 hw_loss 0.220051 lr 0.00025612 rank 1
2023-03-01 20:30:12,212 DEBUG TRAIN Batch 56/6200 loss 11.976750 loss_att 14.067786 loss_ctc 16.788778 loss_rnnt 10.795934 hw_loss 0.226887 lr 0.00025612 rank 0
2023-03-01 20:30:35,345 DEBUG TRAIN Batch 56/6300 loss 2.187346 loss_att 4.428683 loss_ctc 3.382879 loss_rnnt 1.433955 hw_loss 0.273225 lr 0.00025611 rank 1
2023-03-01 20:30:35,346 DEBUG TRAIN Batch 56/6300 loss 6.145585 loss_att 11.705226 loss_ctc 13.231753 loss_rnnt 3.995131 hw_loss 0.175693 lr 0.00025612 rank 0
2023-03-01 20:31:08,192 DEBUG TRAIN Batch 56/6400 loss 8.064095 loss_att 9.183699 loss_ctc 11.681408 loss_rnnt 7.173246 hw_loss 0.346162 lr 0.00025610 rank 1
2023-03-01 20:31:08,195 DEBUG TRAIN Batch 56/6400 loss 13.257113 loss_att 15.973586 loss_ctc 24.035803 loss_rnnt 11.166113 hw_loss 0.207275 lr 0.00025611 rank 0
2023-03-01 20:31:31,047 DEBUG TRAIN Batch 56/6500 loss 4.972686 loss_att 7.386250 loss_ctc 9.151394 loss_rnnt 3.872241 hw_loss 0.113571 lr 0.00025609 rank 1
2023-03-01 20:31:31,047 DEBUG TRAIN Batch 56/6500 loss 5.700357 loss_att 7.065300 loss_ctc 8.781012 loss_rnnt 4.944285 hw_loss 0.135619 lr 0.00025610 rank 0
2023-03-01 20:31:53,638 DEBUG TRAIN Batch 56/6600 loss 11.905542 loss_att 16.671215 loss_ctc 16.024357 loss_rnnt 10.215937 hw_loss 0.351181 lr 0.00025609 rank 1
2023-03-01 20:31:53,640 DEBUG TRAIN Batch 56/6600 loss 6.384976 loss_att 8.652433 loss_ctc 8.967753 loss_rnnt 5.489618 hw_loss 0.182807 lr 0.00025609 rank 0
2023-03-01 20:32:16,136 DEBUG TRAIN Batch 56/6700 loss 6.218544 loss_att 11.128201 loss_ctc 13.339201 loss_rnnt 4.246632 hw_loss 0.076052 lr 0.00025608 rank 0
2023-03-01 20:32:16,136 DEBUG TRAIN Batch 56/6700 loss 8.534535 loss_att 12.636518 loss_ctc 12.452709 loss_rnnt 7.083597 hw_loss 0.202719 lr 0.00025608 rank 1
2023-03-01 20:32:48,840 DEBUG TRAIN Batch 56/6800 loss 8.210495 loss_att 11.877693 loss_ctc 13.592974 loss_rnnt 6.726970 hw_loss 0.060793 lr 0.00025607 rank 1
2023-03-01 20:32:48,842 DEBUG TRAIN Batch 56/6800 loss 6.413778 loss_att 9.662597 loss_ctc 9.663145 loss_rnnt 5.262507 hw_loss 0.127983 lr 0.00025607 rank 0
2023-03-01 20:33:12,010 DEBUG TRAIN Batch 56/6900 loss 6.810837 loss_att 9.997591 loss_ctc 12.343056 loss_rnnt 5.274210 hw_loss 0.303089 lr 0.00025606 rank 1
2023-03-01 20:33:12,011 DEBUG TRAIN Batch 56/6900 loss 6.743028 loss_att 8.249985 loss_ctc 13.482718 loss_rnnt 5.373041 hw_loss 0.318694 lr 0.00025607 rank 0
2023-03-01 20:33:34,717 DEBUG TRAIN Batch 56/7000 loss 7.540738 loss_att 8.318675 loss_ctc 11.419116 loss_rnnt 6.738838 hw_loss 0.242241 lr 0.00025605 rank 1
2023-03-01 20:33:34,717 DEBUG TRAIN Batch 56/7000 loss 4.757026 loss_att 8.536131 loss_ctc 8.505730 loss_rnnt 3.382937 hw_loss 0.222076 lr 0.00025606 rank 0
2023-03-01 20:33:57,870 DEBUG TRAIN Batch 56/7100 loss 3.452548 loss_att 5.778773 loss_ctc 4.687759 loss_rnnt 2.791701 hw_loss 0.057949 lr 0.00025604 rank 1
2023-03-01 20:33:57,870 DEBUG TRAIN Batch 56/7100 loss 6.317933 loss_att 9.096590 loss_ctc 12.278419 loss_rnnt 4.898272 hw_loss 0.129746 lr 0.00025605 rank 0
2023-03-01 20:34:30,778 DEBUG TRAIN Batch 56/7200 loss 7.819901 loss_att 10.624359 loss_ctc 9.584693 loss_rnnt 6.874207 hw_loss 0.280307 lr 0.00025604 rank 1
2023-03-01 20:34:30,781 DEBUG TRAIN Batch 56/7200 loss 7.689381 loss_att 11.567067 loss_ctc 15.396100 loss_rnnt 5.797210 hw_loss 0.167007 lr 0.00025604 rank 0
2023-03-01 20:34:53,041 DEBUG TRAIN Batch 56/7300 loss 5.680521 loss_att 7.727400 loss_ctc 11.233217 loss_rnnt 4.474993 hw_loss 0.104611 lr 0.00025603 rank 1
2023-03-01 20:34:53,043 DEBUG TRAIN Batch 56/7300 loss 6.789684 loss_att 9.157582 loss_ctc 10.611868 loss_rnnt 5.806241 hw_loss 0.000447 lr 0.00025603 rank 0
2023-03-01 20:35:15,609 DEBUG TRAIN Batch 56/7400 loss 1.330378 loss_att 3.810946 loss_ctc 2.409035 loss_rnnt 0.585946 hw_loss 0.195932 lr 0.00025602 rank 1
2023-03-01 20:35:15,610 DEBUG TRAIN Batch 56/7400 loss 5.215666 loss_att 7.838546 loss_ctc 10.300945 loss_rnnt 3.856713 hw_loss 0.293138 lr 0.00025602 rank 0
2023-03-01 20:35:48,051 DEBUG TRAIN Batch 56/7500 loss 4.176363 loss_att 6.480699 loss_ctc 4.234293 loss_rnnt 3.617349 hw_loss 0.169542 lr 0.00025601 rank 1
2023-03-01 20:35:48,052 DEBUG TRAIN Batch 56/7500 loss 7.060213 loss_att 7.170380 loss_ctc 9.009051 loss_rnnt 6.729267 hw_loss 0.092001 lr 0.00025602 rank 0
2023-03-01 20:36:10,945 DEBUG TRAIN Batch 56/7600 loss 8.891730 loss_att 9.584965 loss_ctc 11.291937 loss_rnnt 8.334569 hw_loss 0.184661 lr 0.00025600 rank 1
2023-03-01 20:36:10,948 DEBUG TRAIN Batch 56/7600 loss 6.690470 loss_att 9.155174 loss_ctc 8.625303 loss_rnnt 5.823555 hw_loss 0.217494 lr 0.00025601 rank 0
2023-03-01 20:36:33,691 DEBUG TRAIN Batch 56/7700 loss 3.895935 loss_att 6.931714 loss_ctc 7.528348 loss_rnnt 2.726695 hw_loss 0.145805 lr 0.00025599 rank 1
2023-03-01 20:36:33,691 DEBUG TRAIN Batch 56/7700 loss 5.603919 loss_att 7.254115 loss_ctc 8.600101 loss_rnnt 4.700050 hw_loss 0.326886 lr 0.00025600 rank 0
2023-03-01 20:36:56,356 DEBUG TRAIN Batch 56/7800 loss 3.736432 loss_att 7.439843 loss_ctc 7.401621 loss_rnnt 2.358239 hw_loss 0.279035 lr 0.00025599 rank 1
2023-03-01 20:36:56,357 DEBUG TRAIN Batch 56/7800 loss 8.946350 loss_att 12.962749 loss_ctc 13.007347 loss_rnnt 7.518363 hw_loss 0.156079 lr 0.00025599 rank 0
2023-03-01 20:37:28,916 DEBUG TRAIN Batch 56/7900 loss 5.986887 loss_att 14.387654 loss_ctc 13.804573 loss_rnnt 3.173740 hw_loss 0.169940 lr 0.00025598 rank 1
2023-03-01 20:37:28,917 DEBUG TRAIN Batch 56/7900 loss 8.525648 loss_att 11.746450 loss_ctc 10.980944 loss_rnnt 7.501184 hw_loss 0.099245 lr 0.00025598 rank 0
2023-03-01 20:37:51,124 DEBUG TRAIN Batch 56/8000 loss 5.140068 loss_att 8.188339 loss_ctc 7.246675 loss_rnnt 4.195636 hw_loss 0.101055 lr 0.00025597 rank 1
2023-03-01 20:37:51,125 DEBUG TRAIN Batch 56/8000 loss 3.746761 loss_att 6.255721 loss_ctc 4.229715 loss_rnnt 3.160421 hw_loss 0.037788 lr 0.00025597 rank 0
2023-03-01 20:38:14,503 DEBUG TRAIN Batch 56/8100 loss 6.783309 loss_att 10.673203 loss_ctc 12.032267 loss_rnnt 5.202159 hw_loss 0.193708 lr 0.00025596 rank 1
2023-03-01 20:38:14,504 DEBUG TRAIN Batch 56/8100 loss 7.354884 loss_att 8.153783 loss_ctc 10.565860 loss_rnnt 6.587764 hw_loss 0.336020 lr 0.00025596 rank 0
2023-03-01 20:38:47,615 DEBUG TRAIN Batch 56/8200 loss 9.998253 loss_att 10.102203 loss_ctc 14.589429 loss_rnnt 9.252021 hw_loss 0.212408 lr 0.00025595 rank 1
2023-03-01 20:38:47,618 DEBUG TRAIN Batch 56/8200 loss 4.876688 loss_att 7.676654 loss_ctc 6.870535 loss_rnnt 3.898335 hw_loss 0.285961 lr 0.00025596 rank 0
2023-03-01 20:39:10,488 DEBUG TRAIN Batch 56/8300 loss 3.311862 loss_att 5.163075 loss_ctc 4.525566 loss_rnnt 2.746773 hw_loss 0.061909 lr 0.00025595 rank 0
2023-03-01 20:39:10,489 DEBUG TRAIN Batch 56/8300 loss 8.133365 loss_att 9.276601 loss_ctc 11.408643 loss_rnnt 7.361206 hw_loss 0.200264 lr 0.00025594 rank 1
2023-03-01 20:39:32,918 DEBUG TRAIN Batch 56/8400 loss 10.888349 loss_att 14.727581 loss_ctc 16.720352 loss_rnnt 9.169453 hw_loss 0.325218 lr 0.00025594 rank 1
2023-03-01 20:39:32,918 DEBUG TRAIN Batch 56/8400 loss 6.623055 loss_att 10.560180 loss_ctc 9.671758 loss_rnnt 5.346931 hw_loss 0.154134 lr 0.00025594 rank 0
2023-03-01 20:39:55,810 DEBUG TRAIN Batch 56/8500 loss 6.663896 loss_att 9.786826 loss_ctc 10.390059 loss_rnnt 5.482394 hw_loss 0.112678 lr 0.00025593 rank 1
2023-03-01 20:39:55,812 DEBUG TRAIN Batch 56/8500 loss 5.404248 loss_att 8.462124 loss_ctc 7.389127 loss_rnnt 4.451945 hw_loss 0.142644 lr 0.00025593 rank 0
2023-03-01 20:40:27,612 DEBUG TRAIN Batch 56/8600 loss 11.269190 loss_att 13.839201 loss_ctc 19.137331 loss_rnnt 9.587502 hw_loss 0.222375 lr 0.00025592 rank 1
2023-03-01 20:40:27,616 DEBUG TRAIN Batch 56/8600 loss 5.214846 loss_att 7.584170 loss_ctc 8.846590 loss_rnnt 4.135526 hw_loss 0.227292 lr 0.00025592 rank 0
2023-03-01 20:40:49,819 DEBUG TRAIN Batch 56/8700 loss 5.986339 loss_att 10.080870 loss_ctc 8.925549 loss_rnnt 4.701188 hw_loss 0.139406 lr 0.00025591 rank 1
2023-03-01 20:40:49,821 DEBUG TRAIN Batch 56/8700 loss 5.079256 loss_att 7.929259 loss_ctc 11.879885 loss_rnnt 3.518510 hw_loss 0.157490 lr 0.00025591 rank 0
2023-03-01 20:41:12,371 DEBUG TRAIN Batch 56/8800 loss 5.338223 loss_att 9.364630 loss_ctc 9.505278 loss_rnnt 3.886324 hw_loss 0.170644 lr 0.00025590 rank 1
2023-03-01 20:41:12,371 DEBUG TRAIN Batch 56/8800 loss 3.188108 loss_att 4.196927 loss_ctc 5.690033 loss_rnnt 2.520830 hw_loss 0.247357 lr 0.00025591 rank 0
2023-03-01 20:41:45,296 DEBUG TRAIN Batch 56/8900 loss 5.528069 loss_att 7.438611 loss_ctc 9.705665 loss_rnnt 4.532304 hw_loss 0.106208 lr 0.00025589 rank 1
2023-03-01 20:41:45,297 DEBUG TRAIN Batch 56/8900 loss 8.858779 loss_att 13.525898 loss_ctc 15.515079 loss_rnnt 6.956033 hw_loss 0.153404 lr 0.00025590 rank 0
2023-03-01 20:42:07,835 DEBUG TRAIN Batch 56/9000 loss 4.548015 loss_att 7.333855 loss_ctc 9.294143 loss_rnnt 3.270035 hw_loss 0.164991 lr 0.00025589 rank 1
2023-03-01 20:42:07,835 DEBUG TRAIN Batch 56/9000 loss 1.917952 loss_att 3.990093 loss_ctc 4.525196 loss_rnnt 0.970363 hw_loss 0.347865 lr 0.00025589 rank 0
2023-03-01 20:42:30,387 DEBUG TRAIN Batch 56/9100 loss 3.594599 loss_att 5.332052 loss_ctc 3.998859 loss_rnnt 3.117515 hw_loss 0.141925 lr 0.00025588 rank 1
2023-03-01 20:42:30,389 DEBUG TRAIN Batch 56/9100 loss 1.994451 loss_att 4.086528 loss_ctc 5.088414 loss_rnnt 0.997883 hw_loss 0.310544 lr 0.00025588 rank 0
2023-03-01 20:42:53,044 DEBUG TRAIN Batch 56/9200 loss 17.067678 loss_att 19.114544 loss_ctc 25.785019 loss_rnnt 15.495800 hw_loss 0.000361 lr 0.00025587 rank 1
2023-03-01 20:42:53,046 DEBUG TRAIN Batch 56/9200 loss 9.544626 loss_att 12.645083 loss_ctc 16.029701 loss_rnnt 7.995052 hw_loss 0.121511 lr 0.00025587 rank 0
2023-03-01 20:43:26,407 DEBUG TRAIN Batch 56/9300 loss 4.251770 loss_att 7.161136 loss_ctc 9.303432 loss_rnnt 2.911129 hw_loss 0.159775 lr 0.00025586 rank 1
2023-03-01 20:43:26,408 DEBUG TRAIN Batch 56/9300 loss 7.991456 loss_att 11.771322 loss_ctc 14.491669 loss_rnnt 6.209208 hw_loss 0.299212 lr 0.00025586 rank 0
2023-03-01 20:43:49,403 DEBUG TRAIN Batch 56/9400 loss 4.950698 loss_att 7.257665 loss_ctc 8.404566 loss_rnnt 3.949039 hw_loss 0.149531 lr 0.00025585 rank 1
2023-03-01 20:43:49,404 DEBUG TRAIN Batch 56/9400 loss 5.505712 loss_att 8.072826 loss_ctc 10.653704 loss_rnnt 4.206466 hw_loss 0.186421 lr 0.00025586 rank 0
2023-03-01 20:44:12,778 DEBUG TRAIN Batch 56/9500 loss 8.258443 loss_att 10.636548 loss_ctc 14.674479 loss_rnnt 6.783107 hw_loss 0.270456 lr 0.00025584 rank 1
2023-03-01 20:44:12,779 DEBUG TRAIN Batch 56/9500 loss 6.248883 loss_att 9.815605 loss_ctc 11.486967 loss_rnnt 4.666163 hw_loss 0.320558 lr 0.00025585 rank 0
2023-03-01 20:44:45,534 DEBUG TRAIN Batch 56/9600 loss 4.644324 loss_att 8.403046 loss_ctc 9.499438 loss_rnnt 3.126408 hw_loss 0.222795 lr 0.00025584 rank 0
2023-03-01 20:44:45,538 DEBUG TRAIN Batch 56/9600 loss 5.402064 loss_att 6.139654 loss_ctc 8.619114 loss_rnnt 4.702085 hw_loss 0.231604 lr 0.00025583 rank 1
2023-03-01 20:45:08,202 DEBUG TRAIN Batch 56/9700 loss 2.207283 loss_att 6.887612 loss_ctc 5.898414 loss_rnnt 0.710101 hw_loss 0.129310 lr 0.00025583 rank 1
2023-03-01 20:45:08,204 DEBUG TRAIN Batch 56/9700 loss 2.356243 loss_att 7.027232 loss_ctc 7.313959 loss_rnnt 0.760848 hw_loss 0.000318 lr 0.00025583 rank 0
2023-03-01 20:45:30,597 DEBUG TRAIN Batch 56/9800 loss 8.966228 loss_att 16.599018 loss_ctc 21.020336 loss_rnnt 5.668170 hw_loss 0.308033 lr 0.00025582 rank 1
2023-03-01 20:45:30,600 DEBUG TRAIN Batch 56/9800 loss 10.636979 loss_att 14.141096 loss_ctc 17.534786 loss_rnnt 8.983109 hw_loss 0.062512 lr 0.00025582 rank 0
2023-03-01 20:45:53,228 DEBUG TRAIN Batch 56/9900 loss 6.153300 loss_att 13.099020 loss_ctc 14.864952 loss_rnnt 3.526350 hw_loss 0.142971 lr 0.00025581 rank 1
2023-03-01 20:45:53,230 DEBUG TRAIN Batch 56/9900 loss 7.501754 loss_att 10.343590 loss_ctc 12.857277 loss_rnnt 6.173144 hw_loss 0.086575 lr 0.00025581 rank 0
2023-03-01 20:46:25,318 DEBUG TRAIN Batch 56/10000 loss 6.762559 loss_att 10.957584 loss_ctc 11.971109 loss_rnnt 5.066932 hw_loss 0.304031 lr 0.00025580 rank 1
2023-03-01 20:46:25,320 DEBUG TRAIN Batch 56/10000 loss 5.335715 loss_att 7.694873 loss_ctc 8.625038 loss_rnnt 4.358365 hw_loss 0.125516 lr 0.00025581 rank 0
2023-03-01 20:46:48,022 DEBUG TRAIN Batch 56/10100 loss 11.603426 loss_att 14.334396 loss_ctc 23.598217 loss_rnnt 9.406453 hw_loss 0.096513 lr 0.00025579 rank 1
2023-03-01 20:46:48,023 DEBUG TRAIN Batch 56/10100 loss 9.011655 loss_att 9.437288 loss_ctc 12.486663 loss_rnnt 8.429368 hw_loss 0.063422 lr 0.00025580 rank 0
2023-03-01 20:47:10,747 DEBUG TRAIN Batch 56/10200 loss 3.801462 loss_att 5.080086 loss_ctc 7.262340 loss_rnnt 3.001455 hw_loss 0.155308 lr 0.00025578 rank 1
2023-03-01 20:47:10,749 DEBUG TRAIN Batch 56/10200 loss 6.172417 loss_att 7.770991 loss_ctc 7.698727 loss_rnnt 5.554131 hw_loss 0.178245 lr 0.00025579 rank 0
2023-03-01 20:47:33,427 DEBUG TRAIN Batch 56/10300 loss 5.294345 loss_att 8.148179 loss_ctc 8.469154 loss_rnnt 4.230108 hw_loss 0.131555 lr 0.00025578 rank 1
2023-03-01 20:47:33,427 DEBUG TRAIN Batch 56/10300 loss 9.383630 loss_att 11.948009 loss_ctc 14.882467 loss_rnnt 8.056289 hw_loss 0.152414 lr 0.00025578 rank 0
2023-03-01 20:48:05,766 DEBUG TRAIN Batch 56/10400 loss 3.959301 loss_att 7.147070 loss_ctc 9.482989 loss_rnnt 2.486976 hw_loss 0.184274 lr 0.00025577 rank 1
2023-03-01 20:48:05,768 DEBUG TRAIN Batch 56/10400 loss 3.315982 loss_att 5.604317 loss_ctc 4.533216 loss_rnnt 2.612275 hw_loss 0.157017 lr 0.00025577 rank 0
2023-03-01 20:48:28,158 DEBUG TRAIN Batch 56/10500 loss 6.952533 loss_att 8.552471 loss_ctc 14.144470 loss_rnnt 5.555969 hw_loss 0.220595 lr 0.00025576 rank 1
2023-03-01 20:48:28,160 DEBUG TRAIN Batch 56/10500 loss 5.861368 loss_att 6.868141 loss_ctc 14.485317 loss_rnnt 4.421638 hw_loss 0.165966 lr 0.00025576 rank 0
2023-03-01 20:48:50,681 DEBUG TRAIN Batch 56/10600 loss 4.658636 loss_att 8.332936 loss_ctc 9.497801 loss_rnnt 3.161571 hw_loss 0.219341 lr 0.00025575 rank 1
2023-03-01 20:48:50,681 DEBUG TRAIN Batch 56/10600 loss 10.840833 loss_att 13.866671 loss_ctc 16.085938 loss_rnnt 9.357752 hw_loss 0.334811 lr 0.00025576 rank 0
2023-03-01 20:49:23,595 DEBUG TRAIN Batch 56/10700 loss 5.246725 loss_att 7.944631 loss_ctc 9.113686 loss_rnnt 4.120476 hw_loss 0.133262 lr 0.00025574 rank 1
2023-03-01 20:49:23,596 DEBUG TRAIN Batch 56/10700 loss 3.964760 loss_att 5.377740 loss_ctc 7.082122 loss_rnnt 3.118769 hw_loss 0.277024 lr 0.00025575 rank 0
2023-03-01 20:49:46,799 DEBUG TRAIN Batch 56/10800 loss 5.286064 loss_att 6.469406 loss_ctc 8.646921 loss_rnnt 4.499512 hw_loss 0.190817 lr 0.00025573 rank 1
2023-03-01 20:49:46,800 DEBUG TRAIN Batch 56/10800 loss 10.862068 loss_att 15.477411 loss_ctc 20.591290 loss_rnnt 8.609524 hw_loss 0.060463 lr 0.00025574 rank 0
2023-03-01 20:50:09,583 DEBUG TRAIN Batch 56/10900 loss 4.095665 loss_att 4.737207 loss_ctc 4.807184 loss_rnnt 3.705274 hw_loss 0.313525 lr 0.00025573 rank 1
2023-03-01 20:50:09,584 DEBUG TRAIN Batch 56/10900 loss 5.922107 loss_att 8.610351 loss_ctc 9.411882 loss_rnnt 4.809201 hw_loss 0.206164 lr 0.00025573 rank 0
2023-03-01 20:50:32,759 DEBUG TRAIN Batch 56/11000 loss 3.709148 loss_att 6.130067 loss_ctc 8.893229 loss_rnnt 2.415960 hw_loss 0.220863 lr 0.00025572 rank 1
2023-03-01 20:50:32,760 DEBUG TRAIN Batch 56/11000 loss 8.195909 loss_att 11.900432 loss_ctc 11.692748 loss_rnnt 6.868319 hw_loss 0.225824 lr 0.00025572 rank 0
2023-03-01 20:51:04,231 DEBUG TRAIN Batch 56/11100 loss 11.966231 loss_att 15.706719 loss_ctc 22.815952 loss_rnnt 9.632236 hw_loss 0.261128 lr 0.00025571 rank 0
2023-03-01 20:51:04,232 DEBUG TRAIN Batch 56/11100 loss 3.036612 loss_att 5.256536 loss_ctc 3.862895 loss_rnnt 2.340063 hw_loss 0.266988 lr 0.00025571 rank 1
2023-03-01 20:51:26,628 DEBUG TRAIN Batch 56/11200 loss 5.048194 loss_att 8.238619 loss_ctc 13.461741 loss_rnnt 3.140133 hw_loss 0.277817 lr 0.00025570 rank 1
2023-03-01 20:51:26,629 DEBUG TRAIN Batch 56/11200 loss 6.152778 loss_att 10.094034 loss_ctc 11.241653 loss_rnnt 4.532557 hw_loss 0.287722 lr 0.00025571 rank 0
2023-03-01 20:51:48,974 DEBUG TRAIN Batch 56/11300 loss 11.821207 loss_att 17.510878 loss_ctc 19.673908 loss_rnnt 9.556538 hw_loss 0.149453 lr 0.00025569 rank 1
2023-03-01 20:51:48,974 DEBUG TRAIN Batch 56/11300 loss 9.986569 loss_att 11.556846 loss_ctc 15.231174 loss_rnnt 8.855380 hw_loss 0.220975 lr 0.00025570 rank 0
2023-03-01 20:52:22,446 DEBUG TRAIN Batch 56/11400 loss 3.750563 loss_att 5.730288 loss_ctc 3.978242 loss_rnnt 3.172530 hw_loss 0.284496 lr 0.00025568 rank 1
2023-03-01 20:52:22,448 DEBUG TRAIN Batch 56/11400 loss 5.910105 loss_att 10.457495 loss_ctc 12.152519 loss_rnnt 4.026506 hw_loss 0.265873 lr 0.00025569 rank 0
2023-03-01 20:52:45,124 DEBUG TRAIN Batch 56/11500 loss 4.983754 loss_att 7.119153 loss_ctc 11.953068 loss_rnnt 3.516895 hw_loss 0.207258 lr 0.00025568 rank 1
2023-03-01 20:52:45,128 DEBUG TRAIN Batch 56/11500 loss 8.598313 loss_att 12.274730 loss_ctc 11.697603 loss_rnnt 7.360549 hw_loss 0.167329 lr 0.00025568 rank 0
2023-03-01 20:53:07,914 DEBUG TRAIN Batch 56/11600 loss 6.080392 loss_att 10.760906 loss_ctc 11.541849 loss_rnnt 4.367442 hw_loss 0.091224 lr 0.00025567 rank 1
2023-03-01 20:53:07,915 DEBUG TRAIN Batch 56/11600 loss 4.260418 loss_att 5.996801 loss_ctc 6.797521 loss_rnnt 3.470569 hw_loss 0.195548 lr 0.00025567 rank 0
2023-03-01 20:53:30,355 DEBUG TRAIN Batch 56/11700 loss 3.050504 loss_att 6.839381 loss_ctc 4.253682 loss_rnnt 2.046955 hw_loss 0.160031 lr 0.00025566 rank 1
2023-03-01 20:53:30,358 DEBUG TRAIN Batch 56/11700 loss 5.485839 loss_att 9.209484 loss_ctc 9.017142 loss_rnnt 4.184080 hw_loss 0.161606 lr 0.00025566 rank 0
2023-03-01 20:54:02,944 DEBUG TRAIN Batch 56/11800 loss 4.020051 loss_att 6.993583 loss_ctc 14.305626 loss_rnnt 1.954754 hw_loss 0.185962 lr 0.00025565 rank 1
2023-03-01 20:54:02,947 DEBUG TRAIN Batch 56/11800 loss 7.084609 loss_att 10.028786 loss_ctc 14.139399 loss_rnnt 5.466100 hw_loss 0.166940 lr 0.00025565 rank 0
2023-03-01 20:54:25,726 DEBUG TRAIN Batch 56/11900 loss 6.458163 loss_att 9.444511 loss_ctc 12.678306 loss_rnnt 4.872887 hw_loss 0.297476 lr 0.00025565 rank 0
2023-03-01 20:54:25,726 DEBUG TRAIN Batch 56/11900 loss 7.662560 loss_att 14.163536 loss_ctc 13.789934 loss_rnnt 5.476444 hw_loss 0.129259 lr 0.00025564 rank 1
2023-03-01 20:54:48,532 DEBUG TRAIN Batch 56/12000 loss 12.896853 loss_att 14.001097 loss_ctc 16.903320 loss_rnnt 12.008465 hw_loss 0.250020 lr 0.00025563 rank 1
2023-03-01 20:54:48,533 DEBUG TRAIN Batch 56/12000 loss 4.667388 loss_att 6.258800 loss_ctc 7.554074 loss_rnnt 3.766780 hw_loss 0.370188 lr 0.00025564 rank 0
2023-03-01 20:55:21,458 DEBUG TRAIN Batch 56/12100 loss 6.828728 loss_att 7.086098 loss_ctc 8.577529 loss_rnnt 6.432467 hw_loss 0.209275 lr 0.00025563 rank 1
2023-03-01 20:55:21,460 DEBUG TRAIN Batch 56/12100 loss 4.803595 loss_att 7.239224 loss_ctc 6.530656 loss_rnnt 3.971442 hw_loss 0.215160 lr 0.00025563 rank 0
2023-03-01 20:55:44,465 DEBUG TRAIN Batch 56/12200 loss 5.035942 loss_att 6.256111 loss_ctc 6.884241 loss_rnnt 4.421487 hw_loss 0.232464 lr 0.00025562 rank 1
2023-03-01 20:55:44,465 DEBUG TRAIN Batch 56/12200 loss 7.572092 loss_att 13.585441 loss_ctc 13.781432 loss_rnnt 5.455488 hw_loss 0.161290 lr 0.00025562 rank 0
2023-03-01 20:56:06,966 DEBUG TRAIN Batch 56/12300 loss 3.871941 loss_att 7.242798 loss_ctc 6.657103 loss_rnnt 2.713115 hw_loss 0.212437 lr 0.00025561 rank 1
2023-03-01 20:56:06,968 DEBUG TRAIN Batch 56/12300 loss 10.043401 loss_att 14.541677 loss_ctc 12.230762 loss_rnnt 8.685269 hw_loss 0.312801 lr 0.00025561 rank 0
2023-03-01 20:56:29,625 DEBUG TRAIN Batch 56/12400 loss 7.286036 loss_att 13.594080 loss_ctc 13.938072 loss_rnnt 5.136871 hw_loss 0.001157 lr 0.00025560 rank 1
2023-03-01 20:56:29,627 DEBUG TRAIN Batch 56/12400 loss 8.391428 loss_att 10.774396 loss_ctc 14.464741 loss_rnnt 7.026263 hw_loss 0.147743 lr 0.00025560 rank 0
2023-03-01 20:57:02,105 DEBUG TRAIN Batch 56/12500 loss 3.332539 loss_att 5.377022 loss_ctc 8.254847 loss_rnnt 2.200683 hw_loss 0.124973 lr 0.00025559 rank 1
2023-03-01 20:57:02,107 DEBUG TRAIN Batch 56/12500 loss 9.501157 loss_att 10.355234 loss_ctc 13.564583 loss_rnnt 8.645996 hw_loss 0.267291 lr 0.00025560 rank 0
2023-03-01 20:57:24,874 DEBUG TRAIN Batch 56/12600 loss 7.413719 loss_att 10.706472 loss_ctc 12.947573 loss_rnnt 5.920565 hw_loss 0.181417 lr 0.00025558 rank 1
2023-03-01 20:57:24,875 DEBUG TRAIN Batch 56/12600 loss 3.663626 loss_att 6.010854 loss_ctc 7.009011 loss_rnnt 2.620547 hw_loss 0.239218 lr 0.00025559 rank 0
2023-03-01 20:57:48,219 DEBUG TRAIN Batch 56/12700 loss 7.900877 loss_att 9.580729 loss_ctc 13.095737 loss_rnnt 6.820375 hw_loss 0.097283 lr 0.00025558 rank 1
2023-03-01 20:57:48,221 DEBUG TRAIN Batch 56/12700 loss 3.867645 loss_att 7.813987 loss_ctc 9.130111 loss_rnnt 2.249996 hw_loss 0.237597 lr 0.00025558 rank 0
2023-03-01 20:58:19,180 DEBUG TRAIN Batch 56/12800 loss 2.398928 loss_att 6.567264 loss_ctc 7.440502 loss_rnnt 0.758666 hw_loss 0.251973 lr 0.00025557 rank 0
2023-03-01 20:58:19,211 DEBUG TRAIN Batch 56/12800 loss 5.785461 loss_att 5.998904 loss_ctc 9.279043 loss_rnnt 5.108556 hw_loss 0.315760 lr 0.00025557 rank 1
2023-03-01 20:58:42,941 DEBUG TRAIN Batch 56/12900 loss 3.447312 loss_att 6.868976 loss_ctc 6.436134 loss_rnnt 2.269683 hw_loss 0.177723 lr 0.00025556 rank 1
2023-03-01 20:58:42,943 DEBUG TRAIN Batch 56/12900 loss 15.055297 loss_att 16.582811 loss_ctc 21.379768 loss_rnnt 13.864994 hw_loss 0.077882 lr 0.00025556 rank 0
2023-03-01 20:59:05,520 DEBUG TRAIN Batch 56/13000 loss 15.613397 loss_att 17.789442 loss_ctc 24.570166 loss_rnnt 13.893360 hw_loss 0.169855 lr 0.00025555 rank 1
2023-03-01 20:59:05,524 DEBUG TRAIN Batch 56/13000 loss 9.331188 loss_att 14.327258 loss_ctc 18.684776 loss_rnnt 6.979850 hw_loss 0.196835 lr 0.00025555 rank 0
2023-03-01 20:59:28,255 DEBUG TRAIN Batch 56/13100 loss 3.271626 loss_att 5.572344 loss_ctc 6.519468 loss_rnnt 2.262311 hw_loss 0.217735 lr 0.00025554 rank 1
2023-03-01 20:59:28,257 DEBUG TRAIN Batch 56/13100 loss 8.520634 loss_att 13.009944 loss_ctc 15.855228 loss_rnnt 6.475466 hw_loss 0.317549 lr 0.00025555 rank 0
2023-03-01 21:00:00,875 DEBUG TRAIN Batch 56/13200 loss 8.215245 loss_att 11.266808 loss_ctc 12.900042 loss_rnnt 6.864284 hw_loss 0.217519 lr 0.00025553 rank 1
2023-03-01 21:00:00,877 DEBUG TRAIN Batch 56/13200 loss 4.459793 loss_att 6.087296 loss_ctc 5.799500 loss_rnnt 3.850640 hw_loss 0.196921 lr 0.00025554 rank 0
2023-03-01 21:00:23,779 DEBUG TRAIN Batch 56/13300 loss 7.805722 loss_att 11.028258 loss_ctc 14.595980 loss_rnnt 6.149860 hw_loss 0.198726 lr 0.00025553 rank 1
2023-03-01 21:00:23,781 DEBUG TRAIN Batch 56/13300 loss 3.951112 loss_att 7.770103 loss_ctc 5.548893 loss_rnnt 2.796873 hw_loss 0.332632 lr 0.00025553 rank 0
2023-03-01 21:00:46,811 DEBUG TRAIN Batch 56/13400 loss 6.733480 loss_att 10.827429 loss_ctc 14.961951 loss_rnnt 4.695474 hw_loss 0.228912 lr 0.00025552 rank 1
2023-03-01 21:00:46,812 DEBUG TRAIN Batch 56/13400 loss 4.205679 loss_att 8.791377 loss_ctc 7.402507 loss_rnnt 2.798918 hw_loss 0.118832 lr 0.00025552 rank 0
2023-03-01 21:01:18,511 DEBUG TRAIN Batch 56/13500 loss 4.614480 loss_att 9.300286 loss_ctc 6.418519 loss_rnnt 3.355745 hw_loss 0.151941 lr 0.00025551 rank 0
2023-03-01 21:01:18,522 DEBUG TRAIN Batch 56/13500 loss 3.866324 loss_att 5.517234 loss_ctc 4.746338 loss_rnnt 3.291095 hw_loss 0.239460 lr 0.00025551 rank 1
2023-03-01 21:01:40,945 DEBUG TRAIN Batch 56/13600 loss 5.160514 loss_att 8.188692 loss_ctc 10.110453 loss_rnnt 3.808726 hw_loss 0.161551 lr 0.00025550 rank 1
2023-03-01 21:01:40,947 DEBUG TRAIN Batch 56/13600 loss 5.911592 loss_att 10.007619 loss_ctc 11.841163 loss_rnnt 4.236310 hw_loss 0.122751 lr 0.00025550 rank 0
2023-03-01 21:02:03,363 DEBUG TRAIN Batch 56/13700 loss 5.998205 loss_att 10.279344 loss_ctc 12.677738 loss_rnnt 4.186523 hw_loss 0.121593 lr 0.00025549 rank 1
2023-03-01 21:02:03,364 DEBUG TRAIN Batch 56/13700 loss 2.802839 loss_att 6.332404 loss_ctc 6.277701 loss_rnnt 1.577804 hw_loss 0.104637 lr 0.00025550 rank 0
2023-03-01 21:02:25,835 DEBUG TRAIN Batch 56/13800 loss 3.926534 loss_att 5.615720 loss_ctc 5.954347 loss_rnnt 3.166145 hw_loss 0.285332 lr 0.00025548 rank 1
2023-03-01 21:02:25,848 DEBUG TRAIN Batch 56/13800 loss 6.795212 loss_att 10.084660 loss_ctc 10.875891 loss_rnnt 5.544254 hw_loss 0.091833 lr 0.00025549 rank 0
2023-03-01 21:02:58,082 DEBUG TRAIN Batch 56/13900 loss 10.503795 loss_att 15.515823 loss_ctc 19.839600 loss_rnnt 8.168539 hw_loss 0.165143 lr 0.00025548 rank 1
2023-03-01 21:02:58,083 DEBUG TRAIN Batch 56/13900 loss 9.974528 loss_att 11.978477 loss_ctc 14.655606 loss_rnnt 8.902936 hw_loss 0.087484 lr 0.00025548 rank 0
2023-03-01 21:03:20,704 DEBUG TRAIN Batch 56/14000 loss 14.679195 loss_att 16.197208 loss_ctc 23.587116 loss_rnnt 13.091209 hw_loss 0.181238 lr 0.00025547 rank 1
2023-03-01 21:03:20,707 DEBUG TRAIN Batch 56/14000 loss 6.509137 loss_att 8.539614 loss_ctc 7.180616 loss_rnnt 5.916043 hw_loss 0.182751 lr 0.00025547 rank 0
2023-03-01 21:03:43,417 DEBUG TRAIN Batch 56/14100 loss 4.112327 loss_att 6.388797 loss_ctc 5.787145 loss_rnnt 3.255770 hw_loss 0.333664 lr 0.00025546 rank 1
2023-03-01 21:03:43,419 DEBUG TRAIN Batch 56/14100 loss 13.121113 loss_att 18.218727 loss_ctc 21.271070 loss_rnnt 10.899540 hw_loss 0.216355 lr 0.00025546 rank 0
2023-03-01 21:04:06,616 DEBUG TRAIN Batch 56/14200 loss 3.456154 loss_att 6.414328 loss_ctc 5.758508 loss_rnnt 2.502413 hw_loss 0.103363 lr 0.00025545 rank 1
2023-03-01 21:04:06,617 DEBUG TRAIN Batch 56/14200 loss 4.298784 loss_att 7.732268 loss_ctc 7.195154 loss_rnnt 3.144046 hw_loss 0.153484 lr 0.00025545 rank 0
2023-03-01 21:04:38,659 DEBUG TRAIN Batch 56/14300 loss 8.095908 loss_att 8.840743 loss_ctc 15.096335 loss_rnnt 6.945498 hw_loss 0.127598 lr 0.00025544 rank 1
2023-03-01 21:04:38,662 DEBUG TRAIN Batch 56/14300 loss 6.544236 loss_att 8.524873 loss_ctc 10.450901 loss_rnnt 5.535450 hw_loss 0.172067 lr 0.00025545 rank 0
2023-03-01 21:05:01,494 DEBUG TRAIN Batch 56/14400 loss 10.133605 loss_att 12.700200 loss_ctc 12.343025 loss_rnnt 9.223322 hw_loss 0.191952 lr 0.00025543 rank 1
2023-03-01 21:05:01,496 DEBUG TRAIN Batch 56/14400 loss 9.148789 loss_att 12.815134 loss_ctc 17.009750 loss_rnnt 7.230206 hw_loss 0.257221 lr 0.00025544 rank 0
2023-03-01 21:05:24,065 DEBUG TRAIN Batch 56/14500 loss 15.218935 loss_att 21.179558 loss_ctc 29.078545 loss_rnnt 12.027008 hw_loss 0.284729 lr 0.00025543 rank 1
2023-03-01 21:05:24,065 DEBUG TRAIN Batch 56/14500 loss 7.414335 loss_att 10.387609 loss_ctc 12.606967 loss_rnnt 5.992006 hw_loss 0.253730 lr 0.00025543 rank 0
2023-03-01 21:05:57,232 DEBUG TRAIN Batch 56/14600 loss 9.059149 loss_att 11.299435 loss_ctc 11.131201 loss_rnnt 8.173339 hw_loss 0.302771 lr 0.00025542 rank 1
2023-03-01 21:05:57,235 DEBUG TRAIN Batch 56/14600 loss 4.440609 loss_att 8.237820 loss_ctc 9.459200 loss_rnnt 2.909032 hw_loss 0.193104 lr 0.00025542 rank 0
2023-03-01 21:06:19,839 DEBUG TRAIN Batch 56/14700 loss 6.040853 loss_att 7.811847 loss_ctc 9.855401 loss_rnnt 5.062220 hw_loss 0.217176 lr 0.00025541 rank 1
2023-03-01 21:06:19,843 DEBUG TRAIN Batch 56/14700 loss 7.975952 loss_att 11.572550 loss_ctc 11.683500 loss_rnnt 6.652825 hw_loss 0.205249 lr 0.00025541 rank 0
2023-03-01 21:06:42,227 DEBUG TRAIN Batch 56/14800 loss 5.790411 loss_att 7.041329 loss_ctc 10.365485 loss_rnnt 4.740551 hw_loss 0.355626 lr 0.00025540 rank 1
2023-03-01 21:06:42,229 DEBUG TRAIN Batch 56/14800 loss 2.795151 loss_att 4.609900 loss_ctc 5.600201 loss_rnnt 1.995419 hw_loss 0.117703 lr 0.00025540 rank 0
2023-03-01 21:07:05,238 DEBUG TRAIN Batch 56/14900 loss 3.531180 loss_att 6.225402 loss_ctc 8.008383 loss_rnnt 2.339233 hw_loss 0.105267 lr 0.00025539 rank 1
2023-03-01 21:07:05,241 DEBUG TRAIN Batch 56/14900 loss 2.803392 loss_att 5.568188 loss_ctc 6.566110 loss_rnnt 1.649668 hw_loss 0.185754 lr 0.00025540 rank 0
2023-03-01 21:07:36,922 DEBUG TRAIN Batch 56/15000 loss 25.100643 loss_att 35.759422 loss_ctc 43.698936 loss_rnnt 20.367588 hw_loss 0.227862 lr 0.00025538 rank 1
2023-03-01 21:07:36,924 DEBUG TRAIN Batch 56/15000 loss 3.206788 loss_att 5.298233 loss_ctc 7.296520 loss_rnnt 2.168725 hw_loss 0.139642 lr 0.00025539 rank 0
2023-03-01 21:07:59,866 DEBUG TRAIN Batch 56/15100 loss 3.726869 loss_att 6.749213 loss_ctc 6.163836 loss_rnnt 2.709909 hw_loss 0.164179 lr 0.00025538 rank 1
2023-03-01 21:07:59,868 DEBUG TRAIN Batch 56/15100 loss 13.821522 loss_att 19.475714 loss_ctc 21.301334 loss_rnnt 11.586354 hw_loss 0.200662 lr 0.00025538 rank 0
2023-03-01 21:08:22,123 DEBUG TRAIN Batch 56/15200 loss 9.575192 loss_att 10.700830 loss_ctc 14.300737 loss_rnnt 8.543070 hw_loss 0.331730 lr 0.00025537 rank 0
2023-03-01 21:08:22,123 DEBUG TRAIN Batch 56/15200 loss 14.951618 loss_att 14.842093 loss_ctc 25.420559 loss_rnnt 13.375694 hw_loss 0.378695 lr 0.00025537 rank 1
2023-03-01 21:08:54,365 DEBUG TRAIN Batch 56/15300 loss 8.873040 loss_att 11.321245 loss_ctc 13.085960 loss_rnnt 7.742806 hw_loss 0.147878 lr 0.00025536 rank 1
2023-03-01 21:08:54,367 DEBUG TRAIN Batch 56/15300 loss 5.302501 loss_att 9.497545 loss_ctc 13.714321 loss_rnnt 3.175772 hw_loss 0.311521 lr 0.00025536 rank 0
2023-03-01 21:09:17,162 DEBUG TRAIN Batch 56/15400 loss 3.292559 loss_att 4.451425 loss_ctc 4.370788 loss_rnnt 2.782438 hw_loss 0.252346 lr 0.00025535 rank 1
2023-03-01 21:09:17,165 DEBUG TRAIN Batch 56/15400 loss 7.325025 loss_att 9.977134 loss_ctc 12.943701 loss_rnnt 5.993768 hw_loss 0.096897 lr 0.00025535 rank 0
2023-03-01 21:09:39,796 DEBUG TRAIN Batch 56/15500 loss 10.536398 loss_att 11.626741 loss_ctc 15.729786 loss_rnnt 9.574636 hw_loss 0.096079 lr 0.00025534 rank 1
2023-03-01 21:09:39,796 DEBUG TRAIN Batch 56/15500 loss 3.693743 loss_att 9.881465 loss_ctc 7.616183 loss_rnnt 1.878491 hw_loss 0.102594 lr 0.00025535 rank 0
2023-03-01 21:10:02,251 DEBUG TRAIN Batch 56/15600 loss 7.223638 loss_att 11.071808 loss_ctc 11.160640 loss_rnnt 5.811665 hw_loss 0.220135 lr 0.00025533 rank 1
2023-03-01 21:10:02,253 DEBUG TRAIN Batch 56/15600 loss 8.233629 loss_att 10.527091 loss_ctc 16.513914 loss_rnnt 6.572255 hw_loss 0.184956 lr 0.00025534 rank 0
2023-03-01 21:10:34,756 DEBUG TRAIN Batch 56/15700 loss 4.485771 loss_att 7.529908 loss_ctc 6.865449 loss_rnnt 3.500369 hw_loss 0.111157 lr 0.00025533 rank 1
2023-03-01 21:10:34,757 DEBUG TRAIN Batch 56/15700 loss 11.118988 loss_att 13.212360 loss_ctc 15.577701 loss_rnnt 10.021132 hw_loss 0.158787 lr 0.00025533 rank 0
2023-03-01 21:10:57,128 DEBUG TRAIN Batch 56/15800 loss 7.777712 loss_att 9.155603 loss_ctc 9.968769 loss_rnnt 7.068920 hw_loss 0.264511 lr 0.00025532 rank 1
2023-03-01 21:10:57,129 DEBUG TRAIN Batch 56/15800 loss 13.237883 loss_att 14.502214 loss_ctc 24.544765 loss_rnnt 11.302351 hw_loss 0.328279 lr 0.00025532 rank 0
2023-03-01 21:11:19,689 DEBUG TRAIN Batch 56/15900 loss 6.442391 loss_att 11.346072 loss_ctc 8.860309 loss_rnnt 5.019831 hw_loss 0.223939 lr 0.00025531 rank 1
2023-03-01 21:11:19,690 DEBUG TRAIN Batch 56/15900 loss 4.473319 loss_att 8.458488 loss_ctc 9.362593 loss_rnnt 2.855050 hw_loss 0.317498 lr 0.00025531 rank 0
2023-03-01 21:11:52,244 DEBUG TRAIN Batch 56/16000 loss 5.045013 loss_att 8.451533 loss_ctc 13.293640 loss_rnnt 3.165946 hw_loss 0.183650 lr 0.00025530 rank 1
2023-03-01 21:11:52,246 DEBUG TRAIN Batch 56/16000 loss 3.955172 loss_att 8.237073 loss_ctc 9.217749 loss_rnnt 2.222653 hw_loss 0.327115 lr 0.00025530 rank 0
2023-03-01 21:12:14,844 DEBUG TRAIN Batch 56/16100 loss 7.239992 loss_att 7.995660 loss_ctc 10.511565 loss_rnnt 6.492671 hw_loss 0.299958 lr 0.00025529 rank 1
2023-03-01 21:12:14,847 DEBUG TRAIN Batch 56/16100 loss 8.421977 loss_att 11.246074 loss_ctc 11.331737 loss_rnnt 7.335790 hw_loss 0.250123 lr 0.00025530 rank 0
2023-03-01 21:12:36,979 DEBUG TRAIN Batch 56/16200 loss 3.909106 loss_att 7.762887 loss_ctc 5.032542 loss_rnnt 2.865401 hw_loss 0.230919 lr 0.00025528 rank 1
2023-03-01 21:12:36,981 DEBUG TRAIN Batch 56/16200 loss 8.750595 loss_att 11.586859 loss_ctc 14.214615 loss_rnnt 7.396900 hw_loss 0.108576 lr 0.00025529 rank 0
2023-03-01 21:12:59,825 DEBUG TRAIN Batch 56/16300 loss 8.620927 loss_att 12.641690 loss_ctc 16.965706 loss_rnnt 6.526538 hw_loss 0.332998 lr 0.00025528 rank 1
2023-03-01 21:12:59,825 DEBUG TRAIN Batch 56/16300 loss 5.014415 loss_att 6.297281 loss_ctc 6.676301 loss_rnnt 4.379056 hw_loss 0.294753 lr 0.00025528 rank 0
2023-03-01 21:13:32,782 DEBUG TRAIN Batch 56/16400 loss 4.273703 loss_att 6.439227 loss_ctc 7.914315 loss_rnnt 3.261678 hw_loss 0.175321 lr 0.00025527 rank 1
2023-03-01 21:13:32,783 DEBUG TRAIN Batch 56/16400 loss 3.829302 loss_att 6.883565 loss_ctc 6.350858 loss_rnnt 2.764868 hw_loss 0.220077 lr 0.00025527 rank 0
2023-03-01 21:13:55,296 DEBUG TRAIN Batch 56/16500 loss 7.275904 loss_att 9.262006 loss_ctc 12.804325 loss_rnnt 6.103885 hw_loss 0.070643 lr 0.00025526 rank 1
2023-03-01 21:13:55,299 DEBUG TRAIN Batch 56/16500 loss 1.871926 loss_att 5.550208 loss_ctc 3.067870 loss_rnnt 0.900408 hw_loss 0.143256 lr 0.00025526 rank 0
2023-03-01 21:14:17,808 DEBUG TRAIN Batch 56/16600 loss 4.989757 loss_att 6.065923 loss_ctc 6.384295 loss_rnnt 4.492943 hw_loss 0.179329 lr 0.00025525 rank 1
2023-03-01 21:14:17,809 DEBUG TRAIN Batch 56/16600 loss 7.544692 loss_att 9.456699 loss_ctc 13.437651 loss_rnnt 6.220323 hw_loss 0.292949 lr 0.00025525 rank 0
2023-03-01 21:14:51,241 DEBUG TRAIN Batch 56/16700 loss 10.295660 loss_att 12.050114 loss_ctc 14.062472 loss_rnnt 9.292815 hw_loss 0.280712 lr 0.00025525 rank 0
2023-03-01 21:14:51,241 DEBUG TRAIN Batch 56/16700 loss 6.632516 loss_att 9.766949 loss_ctc 9.867090 loss_rnnt 5.445344 hw_loss 0.241890 lr 0.00025524 rank 1
2023-03-01 21:15:13,820 DEBUG TRAIN Batch 56/16800 loss 4.852602 loss_att 9.107639 loss_ctc 11.523303 loss_rnnt 3.015190 hw_loss 0.181834 lr 0.00025523 rank 1
2023-03-01 21:15:13,822 DEBUG TRAIN Batch 56/16800 loss 11.326806 loss_att 15.536134 loss_ctc 15.923998 loss_rnnt 9.725613 hw_loss 0.274442 lr 0.00025524 rank 0
2023-03-01 21:15:36,035 DEBUG TRAIN Batch 56/16900 loss 8.131100 loss_att 9.314285 loss_ctc 14.918369 loss_rnnt 6.794630 hw_loss 0.365368 lr 0.00025523 rank 1
2023-03-01 21:15:36,036 DEBUG TRAIN Batch 56/16900 loss 6.671611 loss_att 9.189192 loss_ctc 9.128508 loss_rnnt 5.840083 hw_loss 0.000798 lr 0.00025523 rank 0
2023-03-01 21:15:58,840 DEBUG TRAIN Batch 56/17000 loss 6.704556 loss_att 9.271393 loss_ctc 10.973259 loss_rnnt 5.460580 hw_loss 0.302716 lr 0.00025522 rank 1
2023-03-01 21:15:58,842 DEBUG TRAIN Batch 56/17000 loss 4.266594 loss_att 5.715635 loss_ctc 7.116123 loss_rnnt 3.471322 hw_loss 0.235363 lr 0.00025522 rank 0
2023-03-01 21:16:31,391 DEBUG TRAIN Batch 56/17100 loss 2.647985 loss_att 5.230021 loss_ctc 2.711748 loss_rnnt 2.021483 hw_loss 0.190487 lr 0.00025521 rank 1
2023-03-01 21:16:31,404 DEBUG TRAIN Batch 56/17100 loss 4.830348 loss_att 5.713401 loss_ctc 7.814920 loss_rnnt 4.112501 hw_loss 0.268678 lr 0.00025521 rank 0
2023-03-01 21:16:53,694 DEBUG TRAIN Batch 56/17200 loss 9.930266 loss_att 16.789663 loss_ctc 24.831974 loss_rnnt 6.416281 hw_loss 0.291022 lr 0.00025520 rank 1
2023-03-01 21:16:53,697 DEBUG TRAIN Batch 56/17200 loss 6.842976 loss_att 10.254781 loss_ctc 17.658783 loss_rnnt 4.689251 hw_loss 0.054855 lr 0.00025521 rank 0
2023-03-01 21:17:16,000 DEBUG TRAIN Batch 56/17300 loss 2.800608 loss_att 5.349465 loss_ctc 5.770757 loss_rnnt 1.795648 hw_loss 0.185940 lr 0.00025519 rank 1
2023-03-01 21:17:16,001 DEBUG TRAIN Batch 56/17300 loss 4.461520 loss_att 8.615802 loss_ctc 9.637798 loss_rnnt 2.832300 hw_loss 0.202861 lr 0.00025520 rank 0
2023-03-01 21:17:48,922 DEBUG TRAIN Batch 56/17400 loss 5.212124 loss_att 5.345788 loss_ctc 6.734767 loss_rnnt 4.836803 hw_loss 0.272942 lr 0.00025518 rank 1
2023-03-01 21:17:48,925 DEBUG TRAIN Batch 56/17400 loss 15.270978 loss_att 15.486135 loss_ctc 24.435684 loss_rnnt 13.903646 hw_loss 0.191886 lr 0.00025519 rank 0
2023-03-01 21:18:11,056 DEBUG TRAIN Batch 56/17500 loss 5.159380 loss_att 6.999216 loss_ctc 4.602504 loss_rnnt 4.724910 hw_loss 0.263912 lr 0.00025518 rank 1
2023-03-01 21:18:11,059 DEBUG TRAIN Batch 56/17500 loss 2.276455 loss_att 5.208209 loss_ctc 5.761611 loss_rnnt 1.123968 hw_loss 0.190218 lr 0.00025518 rank 0
2023-03-01 21:18:33,410 DEBUG TRAIN Batch 56/17600 loss 2.953228 loss_att 5.441986 loss_ctc 6.630795 loss_rnnt 1.834868 hw_loss 0.244249 lr 0.00025517 rank 1
2023-03-01 21:18:33,413 DEBUG TRAIN Batch 56/17600 loss 1.913655 loss_att 4.713337 loss_ctc 3.130314 loss_rnnt 1.081183 hw_loss 0.206838 lr 0.00025517 rank 0
2023-03-01 21:18:55,999 DEBUG TRAIN Batch 56/17700 loss 12.034839 loss_att 14.200823 loss_ctc 21.494114 loss_rnnt 10.252824 hw_loss 0.164214 lr 0.00025516 rank 1
2023-03-01 21:18:56,001 DEBUG TRAIN Batch 56/17700 loss 8.017136 loss_att 9.679743 loss_ctc 10.274979 loss_rnnt 7.288229 hw_loss 0.178764 lr 0.00025516 rank 0
2023-03-01 21:19:27,828 DEBUG TRAIN Batch 56/17800 loss 8.322114 loss_att 12.321619 loss_ctc 17.173885 loss_rnnt 6.216194 hw_loss 0.235844 lr 0.00025515 rank 1
2023-03-01 21:19:27,830 DEBUG TRAIN Batch 56/17800 loss 1.334462 loss_att 4.275915 loss_ctc 2.896709 loss_rnnt 0.464723 hw_loss 0.137156 lr 0.00025516 rank 0
2023-03-01 21:19:51,028 DEBUG TRAIN Batch 56/17900 loss 3.550092 loss_att 5.747893 loss_ctc 5.689261 loss_rnnt 2.734121 hw_loss 0.170978 lr 0.00025514 rank 1
2023-03-01 21:19:51,030 DEBUG TRAIN Batch 56/17900 loss 8.396939 loss_att 10.792936 loss_ctc 13.075175 loss_rnnt 7.162747 hw_loss 0.246050 lr 0.00025515 rank 0
2023-03-01 21:20:13,669 DEBUG TRAIN Batch 56/18000 loss 7.566756 loss_att 10.716455 loss_ctc 10.725749 loss_rnnt 6.413925 hw_loss 0.190673 lr 0.00025513 rank 1
2023-03-01 21:20:13,671 DEBUG TRAIN Batch 56/18000 loss 3.402879 loss_att 6.397981 loss_ctc 5.970324 loss_rnnt 2.351771 hw_loss 0.205803 lr 0.00025514 rank 0
2023-03-01 21:20:35,979 DEBUG TRAIN Batch 56/18100 loss 6.671698 loss_att 10.404119 loss_ctc 13.384338 loss_rnnt 4.899102 hw_loss 0.245799 lr 0.00025513 rank 0
2023-03-01 21:20:35,980 DEBUG TRAIN Batch 56/18100 loss 3.648696 loss_att 7.394911 loss_ctc 7.466398 loss_rnnt 2.306068 hw_loss 0.158172 lr 0.00025513 rank 1
2023-03-01 21:21:07,913 DEBUG TRAIN Batch 56/18200 loss 6.585126 loss_att 12.030136 loss_ctc 10.068735 loss_rnnt 4.957691 hw_loss 0.138661 lr 0.00025512 rank 1
2023-03-01 21:21:07,915 DEBUG TRAIN Batch 56/18200 loss 4.805525 loss_att 7.132549 loss_ctc 5.828385 loss_rnnt 4.098482 hw_loss 0.197355 lr 0.00025512 rank 0
2023-03-01 21:21:30,697 DEBUG TRAIN Batch 56/18300 loss 2.372045 loss_att 5.564685 loss_ctc 6.969254 loss_rnnt 1.016177 hw_loss 0.195710 lr 0.00025511 rank 1
2023-03-01 21:21:30,699 DEBUG TRAIN Batch 56/18300 loss 2.629740 loss_att 4.172584 loss_ctc 3.797055 loss_rnnt 2.108122 hw_loss 0.107638 lr 0.00025511 rank 0
2023-03-01 21:21:53,124 DEBUG TRAIN Batch 56/18400 loss 9.035847 loss_att 12.537416 loss_ctc 11.848905 loss_rnnt 7.917616 hw_loss 0.080328 lr 0.00025510 rank 1
2023-03-01 21:21:53,128 DEBUG TRAIN Batch 56/18400 loss 3.729448 loss_att 5.313887 loss_ctc 4.004499 loss_rnnt 3.233837 hw_loss 0.266342 lr 0.00025511 rank 0
2023-03-01 21:22:25,896 DEBUG TRAIN Batch 56/18500 loss 6.591014 loss_att 7.740157 loss_ctc 10.548383 loss_rnnt 5.728704 hw_loss 0.196561 lr 0.00025509 rank 1
2023-03-01 21:22:25,897 DEBUG TRAIN Batch 56/18500 loss 7.526709 loss_att 11.913490 loss_ctc 13.522079 loss_rnnt 5.687784 hw_loss 0.304099 lr 0.00025510 rank 0
2023-03-01 21:22:48,242 DEBUG TRAIN Batch 56/18600 loss 4.326942 loss_att 7.489875 loss_ctc 7.455049 loss_rnnt 3.185217 hw_loss 0.172608 lr 0.00025508 rank 1
2023-03-01 21:22:48,243 DEBUG TRAIN Batch 56/18600 loss 4.788235 loss_att 8.668255 loss_ctc 10.592301 loss_rnnt 3.180813 hw_loss 0.107892 lr 0.00025509 rank 0
2023-03-01 21:23:10,365 DEBUG TRAIN Batch 56/18700 loss 5.523541 loss_att 7.781396 loss_ctc 7.641950 loss_rnnt 4.708224 hw_loss 0.152420 lr 0.00025508 rank 1
2023-03-01 21:23:10,368 DEBUG TRAIN Batch 56/18700 loss 5.984837 loss_att 9.183424 loss_ctc 7.586404 loss_rnnt 4.998928 hw_loss 0.248717 lr 0.00025508 rank 0
2023-03-01 21:23:32,875 DEBUG TRAIN Batch 56/18800 loss 6.477208 loss_att 10.542295 loss_ctc 8.559391 loss_rnnt 5.290395 hw_loss 0.180321 lr 0.00025507 rank 1
2023-03-01 21:23:32,877 DEBUG TRAIN Batch 56/18800 loss 6.208821 loss_att 6.419764 loss_ctc 8.293394 loss_rnnt 5.826418 hw_loss 0.116757 lr 0.00025507 rank 0
2023-03-01 21:24:05,442 DEBUG TRAIN Batch 56/18900 loss 5.044357 loss_att 8.440840 loss_ctc 9.825435 loss_rnnt 3.625970 hw_loss 0.190525 lr 0.00025506 rank 1
2023-03-01 21:24:05,444 DEBUG TRAIN Batch 56/18900 loss 7.302996 loss_att 11.087233 loss_ctc 12.467758 loss_rnnt 5.792639 hw_loss 0.121640 lr 0.00025506 rank 0
2023-03-01 21:24:27,819 DEBUG TRAIN Batch 56/19000 loss 7.731529 loss_att 7.459608 loss_ctc 8.271685 loss_rnnt 7.613138 hw_loss 0.188913 lr 0.00025505 rank 1
2023-03-01 21:24:27,819 DEBUG TRAIN Batch 56/19000 loss 4.782959 loss_att 5.164958 loss_ctc 6.724947 loss_rnnt 4.278086 hw_loss 0.317889 lr 0.00025506 rank 0
2023-03-01 21:24:50,078 DEBUG TRAIN Batch 56/19100 loss 6.555939 loss_att 10.334159 loss_ctc 11.743296 loss_rnnt 5.003790 hw_loss 0.196606 lr 0.00025504 rank 1
2023-03-01 21:24:50,080 DEBUG TRAIN Batch 56/19100 loss 4.246310 loss_att 8.658972 loss_ctc 9.657592 loss_rnnt 2.574579 hw_loss 0.126926 lr 0.00025505 rank 0
2023-03-01 21:25:23,080 DEBUG TRAIN Batch 56/19200 loss 3.081812 loss_att 4.561353 loss_ctc 5.907600 loss_rnnt 2.344505 hw_loss 0.121174 lr 0.00025503 rank 1
2023-03-01 21:25:23,082 DEBUG TRAIN Batch 56/19200 loss 10.855627 loss_att 14.071709 loss_ctc 19.635260 loss_rnnt 8.983189 hw_loss 0.109882 lr 0.00025504 rank 0
2023-03-01 21:25:45,315 DEBUG TRAIN Batch 56/19300 loss 4.753077 loss_att 7.258626 loss_ctc 8.489669 loss_rnnt 3.662642 hw_loss 0.170838 lr 0.00025503 rank 1
2023-03-01 21:25:45,316 DEBUG TRAIN Batch 56/19300 loss 8.166237 loss_att 10.089069 loss_ctc 10.507895 loss_rnnt 7.331208 hw_loss 0.259201 lr 0.00025503 rank 0
2023-03-01 21:26:07,610 DEBUG TRAIN Batch 56/19400 loss 5.570048 loss_att 7.372206 loss_ctc 9.493431 loss_rnnt 4.528943 hw_loss 0.295418 lr 0.00025502 rank 1
2023-03-01 21:26:07,610 DEBUG TRAIN Batch 56/19400 loss 11.225163 loss_att 12.796171 loss_ctc 14.680933 loss_rnnt 10.360518 hw_loss 0.168140 lr 0.00025502 rank 0
2023-03-01 21:26:30,414 DEBUG TRAIN Batch 56/19500 loss 4.882409 loss_att 7.575240 loss_ctc 8.488532 loss_rnnt 3.812686 hw_loss 0.094387 lr 0.00025501 rank 1
2023-03-01 21:26:30,415 DEBUG TRAIN Batch 56/19500 loss 9.937703 loss_att 13.883043 loss_ctc 19.404024 loss_rnnt 7.840165 hw_loss 0.086802 lr 0.00025501 rank 0
2023-03-01 21:27:02,991 DEBUG TRAIN Batch 56/19600 loss 10.042084 loss_att 14.397148 loss_ctc 16.208426 loss_rnnt 8.264463 hw_loss 0.158303 lr 0.00025500 rank 1
2023-03-01 21:27:02,994 DEBUG TRAIN Batch 56/19600 loss 9.768030 loss_att 12.951257 loss_ctc 15.892698 loss_rnnt 8.172806 hw_loss 0.266168 lr 0.00025501 rank 0
2023-03-01 21:27:25,330 DEBUG TRAIN Batch 56/19700 loss 5.611264 loss_att 8.261606 loss_ctc 8.532740 loss_rnnt 4.593331 hw_loss 0.184376 lr 0.00025499 rank 1
2023-03-01 21:27:25,332 DEBUG TRAIN Batch 56/19700 loss 7.765483 loss_att 13.802723 loss_ctc 12.503304 loss_rnnt 5.886925 hw_loss 0.073877 lr 0.00025500 rank 0
2023-03-01 21:27:47,214 DEBUG TRAIN Batch 56/19800 loss 2.160689 loss_att 4.880309 loss_ctc 5.245316 loss_rnnt 1.163373 hw_loss 0.078953 lr 0.00025499 rank 0
2023-03-01 21:27:47,214 DEBUG TRAIN Batch 56/19800 loss 7.341854 loss_att 10.111058 loss_ctc 9.795647 loss_rnnt 6.379691 hw_loss 0.152156 lr 0.00025499 rank 1
2023-03-01 21:28:20,046 DEBUG TRAIN Batch 56/19900 loss 4.428799 loss_att 6.446747 loss_ctc 6.763301 loss_rnnt 3.567917 hw_loss 0.273796 lr 0.00025498 rank 1
2023-03-01 21:28:20,048 DEBUG TRAIN Batch 56/19900 loss 3.565569 loss_att 6.333529 loss_ctc 5.625863 loss_rnnt 2.606970 hw_loss 0.244316 lr 0.00025498 rank 0
2023-03-01 21:28:42,403 DEBUG TRAIN Batch 56/20000 loss 9.568861 loss_att 9.081169 loss_ctc 13.295998 loss_rnnt 9.076982 hw_loss 0.173373 lr 0.00025497 rank 1
2023-03-01 21:28:42,404 DEBUG TRAIN Batch 56/20000 loss 7.599620 loss_att 9.449543 loss_ctc 12.515582 loss_rnnt 6.455309 hw_loss 0.222873 lr 0.00025497 rank 0
2023-03-01 21:29:04,493 DEBUG TRAIN Batch 56/20100 loss 5.122503 loss_att 8.508831 loss_ctc 11.517038 loss_rnnt 3.533150 hw_loss 0.111529 lr 0.00025496 rank 1
2023-03-01 21:29:04,494 DEBUG TRAIN Batch 56/20100 loss 6.607872 loss_att 8.797097 loss_ctc 12.890818 loss_rnnt 5.238514 hw_loss 0.175847 lr 0.00025496 rank 0
2023-03-01 21:29:26,981 DEBUG TRAIN Batch 56/20200 loss 12.358712 loss_att 14.665949 loss_ctc 18.480547 loss_rnnt 10.927447 hw_loss 0.287950 lr 0.00025495 rank 1
2023-03-01 21:29:26,982 DEBUG TRAIN Batch 56/20200 loss 5.468621 loss_att 7.202880 loss_ctc 7.172171 loss_rnnt 4.816699 hw_loss 0.146119 lr 0.00025496 rank 0
2023-03-01 21:29:59,144 DEBUG TRAIN Batch 56/20300 loss 6.675970 loss_att 9.982859 loss_ctc 13.111233 loss_rnnt 5.145945 hw_loss 0.019898 lr 0.00025494 rank 1
2023-03-01 21:29:59,152 DEBUG TRAIN Batch 56/20300 loss 9.293333 loss_att 9.505379 loss_ctc 19.756714 loss_rnnt 7.703297 hw_loss 0.285957 lr 0.00025495 rank 0
2023-03-01 21:30:21,141 DEBUG TRAIN Batch 56/20400 loss 8.556965 loss_att 12.325193 loss_ctc 15.197049 loss_rnnt 6.855629 hw_loss 0.116899 lr 0.00025494 rank 1
2023-03-01 21:30:21,144 DEBUG TRAIN Batch 56/20400 loss 6.850255 loss_att 8.618594 loss_ctc 8.749809 loss_rnnt 6.136799 hw_loss 0.199712 lr 0.00025494 rank 0
2023-03-01 21:30:43,571 DEBUG TRAIN Batch 56/20500 loss 4.798112 loss_att 7.996302 loss_ctc 9.448219 loss_rnnt 3.466457 hw_loss 0.135006 lr 0.00025493 rank 1
2023-03-01 21:30:43,575 DEBUG TRAIN Batch 56/20500 loss 1.673629 loss_att 4.372674 loss_ctc 3.529959 loss_rnnt 0.733475 hw_loss 0.286565 lr 0.00025493 rank 0
2023-03-01 21:31:16,293 DEBUG TRAIN Batch 56/20600 loss 5.981957 loss_att 7.596425 loss_ctc 9.406672 loss_rnnt 5.087968 hw_loss 0.214625 lr 0.00025492 rank 1
2023-03-01 21:31:16,295 DEBUG TRAIN Batch 56/20600 loss 3.010763 loss_att 6.352724 loss_ctc 6.445298 loss_rnnt 1.837930 hw_loss 0.087194 lr 0.00025492 rank 0
2023-03-01 21:31:38,692 DEBUG TRAIN Batch 56/20700 loss 4.555557 loss_att 6.437893 loss_ctc 7.185474 loss_rnnt 3.714805 hw_loss 0.213055 lr 0.00025491 rank 1
2023-03-01 21:31:38,692 DEBUG TRAIN Batch 56/20700 loss 7.541954 loss_att 9.486954 loss_ctc 11.188368 loss_rnnt 6.537820 hw_loss 0.241772 lr 0.00025491 rank 0
2023-03-01 21:32:01,232 DEBUG TRAIN Batch 56/20800 loss 11.850396 loss_att 16.641785 loss_ctc 15.487742 loss_rnnt 10.292519 hw_loss 0.214913 lr 0.00025490 rank 1
2023-03-01 21:32:01,233 DEBUG TRAIN Batch 56/20800 loss 5.294868 loss_att 8.449743 loss_ctc 13.432720 loss_rnnt 3.496576 hw_loss 0.154256 lr 0.00025491 rank 0
2023-03-01 21:32:24,451 DEBUG TRAIN Batch 56/20900 loss 9.667852 loss_att 12.476410 loss_ctc 17.820036 loss_rnnt 7.899500 hw_loss 0.224405 lr 0.00025489 rank 1
2023-03-01 21:32:24,452 DEBUG TRAIN Batch 56/20900 loss 9.316428 loss_att 11.293264 loss_ctc 12.881018 loss_rnnt 8.308571 hw_loss 0.257270 lr 0.00025490 rank 0
2023-03-01 21:32:55,715 DEBUG TRAIN Batch 56/21000 loss 5.764827 loss_att 9.743252 loss_ctc 10.429386 loss_rnnt 4.230619 hw_loss 0.218591 lr 0.00025489 rank 1
2023-03-01 21:32:55,716 DEBUG TRAIN Batch 56/21000 loss 6.324254 loss_att 8.969624 loss_ctc 11.125132 loss_rnnt 5.079005 hw_loss 0.142607 lr 0.00025489 rank 0
2023-03-01 21:33:19,076 DEBUG TRAIN Batch 56/21100 loss 7.275051 loss_att 13.815599 loss_ctc 12.247122 loss_rnnt 5.179187 hw_loss 0.234019 lr 0.00025488 rank 1
2023-03-01 21:33:19,079 DEBUG TRAIN Batch 56/21100 loss 5.086169 loss_att 7.894406 loss_ctc 8.973942 loss_rnnt 3.887739 hw_loss 0.222024 lr 0.00025488 rank 0
2023-03-01 21:33:41,919 DEBUG TRAIN Batch 56/21200 loss 6.822633 loss_att 8.887820 loss_ctc 11.829346 loss_rnnt 5.698943 hw_loss 0.080796 lr 0.00025487 rank 1
2023-03-01 21:33:41,921 DEBUG TRAIN Batch 56/21200 loss 5.634015 loss_att 9.006904 loss_ctc 8.973672 loss_rnnt 4.424314 hw_loss 0.168442 lr 0.00025487 rank 0
2023-03-01 21:34:14,948 DEBUG TRAIN Batch 56/21300 loss 5.295725 loss_att 7.243923 loss_ctc 11.840745 loss_rnnt 3.906113 hw_loss 0.238695 lr 0.00025486 rank 1
2023-03-01 21:34:14,950 DEBUG TRAIN Batch 56/21300 loss 10.542404 loss_att 11.467325 loss_ctc 16.119282 loss_rnnt 9.513052 hw_loss 0.188971 lr 0.00025486 rank 0
2023-03-01 21:34:37,381 DEBUG TRAIN Batch 56/21400 loss 6.500981 loss_att 8.140952 loss_ctc 8.276211 loss_rnnt 5.803931 hw_loss 0.248172 lr 0.00025485 rank 1
2023-03-01 21:34:37,382 DEBUG TRAIN Batch 56/21400 loss 10.272792 loss_att 11.690628 loss_ctc 15.195900 loss_rnnt 9.291433 hw_loss 0.077581 lr 0.00025486 rank 0
2023-03-01 21:35:00,128 DEBUG TRAIN Batch 56/21500 loss 10.432568 loss_att 13.710436 loss_ctc 17.544460 loss_rnnt 8.742752 hw_loss 0.161231 lr 0.00025484 rank 1
2023-03-01 21:35:00,129 DEBUG TRAIN Batch 56/21500 loss 13.983301 loss_att 14.949848 loss_ctc 23.133453 loss_rnnt 12.444809 hw_loss 0.234681 lr 0.00025485 rank 0
2023-03-01 21:35:22,860 DEBUG TRAIN Batch 56/21600 loss 5.715436 loss_att 8.083002 loss_ctc 5.841642 loss_rnnt 5.172899 hw_loss 0.097867 lr 0.00025484 rank 1
2023-03-01 21:35:22,862 DEBUG TRAIN Batch 56/21600 loss 8.617332 loss_att 11.872221 loss_ctc 14.336878 loss_rnnt 7.123071 hw_loss 0.151269 lr 0.00025484 rank 0
2023-03-01 21:35:55,282 DEBUG TRAIN Batch 56/21700 loss 5.686141 loss_att 9.045189 loss_ctc 10.606824 loss_rnnt 4.241961 hw_loss 0.218022 lr 0.00025483 rank 1
2023-03-01 21:35:55,284 DEBUG TRAIN Batch 56/21700 loss 3.903736 loss_att 6.987645 loss_ctc 6.536302 loss_rnnt 2.822235 hw_loss 0.213207 lr 0.00025483 rank 0
2023-03-01 21:36:17,834 DEBUG TRAIN Batch 56/21800 loss 4.285187 loss_att 6.453372 loss_ctc 8.470448 loss_rnnt 3.139933 hw_loss 0.287966 lr 0.00025482 rank 1
2023-03-01 21:36:17,836 DEBUG TRAIN Batch 56/21800 loss 3.789307 loss_att 7.208525 loss_ctc 9.669962 loss_rnnt 2.305068 hw_loss 0.030577 lr 0.00025482 rank 0
2023-03-01 21:36:39,683 DEBUG TRAIN Batch 56/21900 loss 11.506260 loss_att 12.551366 loss_ctc 17.396275 loss_rnnt 10.351148 hw_loss 0.301415 lr 0.00025481 rank 1
2023-03-01 21:36:39,684 DEBUG TRAIN Batch 56/21900 loss 2.162193 loss_att 5.174522 loss_ctc 5.594893 loss_rnnt 0.914151 hw_loss 0.352280 lr 0.00025482 rank 0
2023-03-01 21:37:01,828 DEBUG TRAIN Batch 56/22000 loss 5.137286 loss_att 7.052273 loss_ctc 15.217861 loss_rnnt 3.263020 hw_loss 0.275987 lr 0.00025480 rank 1
2023-03-01 21:37:01,830 DEBUG TRAIN Batch 56/22000 loss 5.777300 loss_att 8.252110 loss_ctc 7.362470 loss_rnnt 5.044778 hw_loss 0.049131 lr 0.00025481 rank 0
2023-03-01 21:37:35,466 DEBUG TRAIN Batch 56/22100 loss 4.833284 loss_att 6.867638 loss_ctc 10.141474 loss_rnnt 3.612216 hw_loss 0.199573 lr 0.00025479 rank 1
2023-03-01 21:37:35,470 DEBUG TRAIN Batch 56/22100 loss 5.834242 loss_att 7.687529 loss_ctc 7.567734 loss_rnnt 5.150967 hw_loss 0.152785 lr 0.00025480 rank 0
2023-03-01 21:37:58,309 DEBUG TRAIN Batch 56/22200 loss 1.444709 loss_att 4.379685 loss_ctc 3.585151 loss_rnnt 0.446831 hw_loss 0.235294 lr 0.00025479 rank 1
2023-03-01 21:37:58,311 DEBUG TRAIN Batch 56/22200 loss 8.607149 loss_att 8.903499 loss_ctc 11.797719 loss_rnnt 7.977837 hw_loss 0.271188 lr 0.00025479 rank 0
2023-03-01 21:38:20,178 DEBUG TRAIN Batch 56/22300 loss 4.953238 loss_att 8.906103 loss_ctc 9.631321 loss_rnnt 3.369039 hw_loss 0.318529 lr 0.00025478 rank 1
2023-03-01 21:38:20,178 DEBUG TRAIN Batch 56/22300 loss 6.942103 loss_att 8.591437 loss_ctc 8.079571 loss_rnnt 6.312269 hw_loss 0.278071 lr 0.00025478 rank 0
2023-03-01 21:38:53,847 DEBUG TRAIN Batch 56/22400 loss 8.246531 loss_att 9.633574 loss_ctc 10.841952 loss_rnnt 7.528639 hw_loss 0.177050 lr 0.00025477 rank 0
2023-03-01 21:38:53,849 DEBUG TRAIN Batch 56/22400 loss 3.185542 loss_att 5.192552 loss_ctc 5.083936 loss_rnnt 2.396747 hw_loss 0.251763 lr 0.00025477 rank 1
2023-03-01 21:39:16,379 DEBUG TRAIN Batch 56/22500 loss 8.946534 loss_att 11.627824 loss_ctc 13.664392 loss_rnnt 7.655525 hw_loss 0.235694 lr 0.00025476 rank 1
2023-03-01 21:39:16,380 DEBUG TRAIN Batch 56/22500 loss 7.272874 loss_att 10.806376 loss_ctc 16.228168 loss_rnnt 5.282799 hw_loss 0.167504 lr 0.00025477 rank 0
2023-03-01 21:39:38,274 DEBUG TRAIN Batch 56/22600 loss 2.723290 loss_att 3.947712 loss_ctc 2.313263 loss_rnnt 2.370623 hw_loss 0.304599 lr 0.00025475 rank 1
2023-03-01 21:39:38,275 DEBUG TRAIN Batch 56/22600 loss 7.132048 loss_att 8.498494 loss_ctc 8.462392 loss_rnnt 6.541393 hw_loss 0.262476 lr 0.00025476 rank 0
2023-03-01 21:40:01,474 DEBUG TRAIN Batch 56/22700 loss 7.056418 loss_att 9.469292 loss_ctc 6.956202 loss_rnnt 6.528842 hw_loss 0.109432 lr 0.00025475 rank 1
2023-03-01 21:40:01,477 DEBUG TRAIN Batch 56/22700 loss 5.150698 loss_att 6.479605 loss_ctc 9.350613 loss_rnnt 4.225017 hw_loss 0.187332 lr 0.00025475 rank 0
2023-03-01 21:40:37,195 DEBUG TRAIN Batch 56/22800 loss 7.548717 loss_att 11.126283 loss_ctc 11.924598 loss_rnnt 6.169659 hw_loss 0.150175 lr 0.00025474 rank 1
2023-03-01 21:40:37,197 DEBUG TRAIN Batch 56/22800 loss 3.474270 loss_att 4.247756 loss_ctc 5.737783 loss_rnnt 2.900808 hw_loss 0.219304 lr 0.00025474 rank 0
2023-03-01 21:40:59,299 DEBUG TRAIN Batch 56/22900 loss 5.098285 loss_att 7.576931 loss_ctc 6.942931 loss_rnnt 4.245286 hw_loss 0.208719 lr 0.00025473 rank 1
2023-03-01 21:40:59,301 DEBUG TRAIN Batch 56/22900 loss 12.607372 loss_att 17.954025 loss_ctc 23.737677 loss_rnnt 9.949638 hw_loss 0.195680 lr 0.00025473 rank 0
2023-03-01 21:41:22,111 DEBUG TRAIN Batch 56/23000 loss 7.208182 loss_att 9.495493 loss_ctc 12.986933 loss_rnnt 5.840582 hw_loss 0.261821 lr 0.00025472 rank 1
2023-03-01 21:41:22,114 DEBUG TRAIN Batch 56/23000 loss 2.149610 loss_att 4.840045 loss_ctc 5.185241 loss_rnnt 1.124496 hw_loss 0.154268 lr 0.00025472 rank 0
2023-03-01 21:41:54,578 DEBUG TRAIN Batch 56/23100 loss 13.472011 loss_att 16.523653 loss_ctc 23.006344 loss_rnnt 11.535151 hw_loss 0.103663 lr 0.00025471 rank 1
2023-03-01 21:41:54,581 DEBUG TRAIN Batch 56/23100 loss 3.261266 loss_att 6.459562 loss_ctc 9.355099 loss_rnnt 1.655074 hw_loss 0.288790 lr 0.00025472 rank 0
2023-03-01 21:42:16,848 DEBUG TRAIN Batch 56/23200 loss 3.611158 loss_att 6.914960 loss_ctc 7.172941 loss_rnnt 2.385528 hw_loss 0.168684 lr 0.00025470 rank 1
2023-03-01 21:42:16,849 DEBUG TRAIN Batch 56/23200 loss 4.662396 loss_att 8.017241 loss_ctc 9.177143 loss_rnnt 3.304657 hw_loss 0.159007 lr 0.00025471 rank 0
2023-03-01 21:42:39,477 DEBUG TRAIN Batch 56/23300 loss 11.325077 loss_att 15.107384 loss_ctc 21.391293 loss_rnnt 9.154502 hw_loss 0.134910 lr 0.00025470 rank 1
2023-03-01 21:42:39,479 DEBUG TRAIN Batch 56/23300 loss 5.450645 loss_att 7.693029 loss_ctc 8.322473 loss_rnnt 4.549788 hw_loss 0.130255 lr 0.00025470 rank 0
2023-03-01 21:43:01,986 DEBUG TRAIN Batch 56/23400 loss 6.950911 loss_att 9.781454 loss_ctc 5.654784 loss_rnnt 6.508302 hw_loss 0.092468 lr 0.00025469 rank 1
2023-03-01 21:43:01,989 DEBUG TRAIN Batch 56/23400 loss 7.074008 loss_att 8.340109 loss_ctc 10.962106 loss_rnnt 6.213668 hw_loss 0.166324 lr 0.00025469 rank 0
2023-03-01 21:43:34,087 DEBUG TRAIN Batch 56/23500 loss 3.344284 loss_att 5.662095 loss_ctc 4.787358 loss_rnnt 2.635795 hw_loss 0.098468 lr 0.00025468 rank 1
2023-03-01 21:43:34,089 DEBUG TRAIN Batch 56/23500 loss 3.729301 loss_att 6.119232 loss_ctc 6.230543 loss_rnnt 2.816516 hw_loss 0.189937 lr 0.00025468 rank 0
2023-03-01 21:43:57,458 DEBUG TRAIN Batch 56/23600 loss 4.639670 loss_att 6.981432 loss_ctc 5.773183 loss_rnnt 3.871951 hw_loss 0.277934 lr 0.00025467 rank 1
2023-03-01 21:43:57,460 DEBUG TRAIN Batch 56/23600 loss 2.978994 loss_att 5.954950 loss_ctc 6.640646 loss_rnnt 1.810804 hw_loss 0.158960 lr 0.00025467 rank 0
2023-03-01 21:44:20,247 DEBUG TRAIN Batch 56/23700 loss 5.461704 loss_att 7.721912 loss_ctc 8.206144 loss_rnnt 4.581802 hw_loss 0.116128 lr 0.00025466 rank 1
2023-03-01 21:44:20,249 DEBUG TRAIN Batch 56/23700 loss 6.892869 loss_att 8.501171 loss_ctc 11.553276 loss_rnnt 5.786146 hw_loss 0.306889 lr 0.00025467 rank 0
2023-03-01 21:44:52,096 DEBUG TRAIN Batch 56/23800 loss 7.344360 loss_att 8.509613 loss_ctc 11.052284 loss_rnnt 6.430091 hw_loss 0.350305 lr 0.00025465 rank 1
2023-03-01 21:44:52,097 DEBUG TRAIN Batch 56/23800 loss 7.473063 loss_att 9.718697 loss_ctc 10.608280 loss_rnnt 6.539659 hw_loss 0.124214 lr 0.00025466 rank 0
2023-03-01 21:45:15,607 DEBUG TRAIN Batch 56/23900 loss 4.567461 loss_att 7.478323 loss_ctc 10.526744 loss_rnnt 3.054353 hw_loss 0.255684 lr 0.00025465 rank 1
2023-03-01 21:45:15,609 DEBUG TRAIN Batch 56/23900 loss 8.652151 loss_att 11.073811 loss_ctc 13.258984 loss_rnnt 7.461651 hw_loss 0.172355 lr 0.00025465 rank 0
2023-03-01 21:45:38,697 DEBUG TRAIN Batch 56/24000 loss 3.624928 loss_att 8.275042 loss_ctc 7.274990 loss_rnnt 2.121293 hw_loss 0.163010 lr 0.00025464 rank 1
2023-03-01 21:45:38,701 DEBUG TRAIN Batch 56/24000 loss 4.045592 loss_att 4.570117 loss_ctc 6.108561 loss_rnnt 3.520135 hw_loss 0.272794 lr 0.00025464 rank 0
2023-03-01 21:46:01,198 DEBUG TRAIN Batch 56/24100 loss 3.407473 loss_att 5.589253 loss_ctc 4.683656 loss_rnnt 2.746955 hw_loss 0.101258 lr 0.00025463 rank 1
2023-03-01 21:46:01,199 DEBUG TRAIN Batch 56/24100 loss 2.538885 loss_att 6.341097 loss_ctc 3.260806 loss_rnnt 1.553893 hw_loss 0.240549 lr 0.00025463 rank 0
2023-03-01 21:46:33,483 DEBUG TRAIN Batch 56/24200 loss 4.861392 loss_att 7.495376 loss_ctc 6.606489 loss_rnnt 3.952425 hw_loss 0.280293 lr 0.00025462 rank 1
2023-03-01 21:46:33,485 DEBUG TRAIN Batch 56/24200 loss 3.608039 loss_att 5.692665 loss_ctc 4.283851 loss_rnnt 3.024782 hw_loss 0.142919 lr 0.00025463 rank 0
2023-03-01 21:46:56,258 DEBUG TRAIN Batch 56/24300 loss 3.803369 loss_att 6.995351 loss_ctc 6.481065 loss_rnnt 2.716733 hw_loss 0.171026 lr 0.00025461 rank 1
2023-03-01 21:46:56,260 DEBUG TRAIN Batch 56/24300 loss 5.357894 loss_att 7.667482 loss_ctc 11.103991 loss_rnnt 4.022117 hw_loss 0.201963 lr 0.00025462 rank 0
2023-03-01 21:47:18,930 DEBUG TRAIN Batch 56/24400 loss 7.246163 loss_att 8.304359 loss_ctc 10.590094 loss_rnnt 6.459688 hw_loss 0.241834 lr 0.00025460 rank 1
2023-03-01 21:47:18,932 DEBUG TRAIN Batch 56/24400 loss 6.841639 loss_att 10.892554 loss_ctc 14.492413 loss_rnnt 4.837148 hw_loss 0.326635 lr 0.00025461 rank 0
2023-03-01 21:47:41,871 DEBUG TRAIN Batch 56/24500 loss 2.893804 loss_att 5.916139 loss_ctc 8.377064 loss_rnnt 1.463537 hw_loss 0.177561 lr 0.00025460 rank 1
2023-03-01 21:47:41,873 DEBUG TRAIN Batch 56/24500 loss 6.422690 loss_att 8.968776 loss_ctc 11.933339 loss_rnnt 5.124383 hw_loss 0.101880 lr 0.00025460 rank 0
2023-03-01 21:48:14,403 DEBUG TRAIN Batch 56/24600 loss 2.681313 loss_att 5.886636 loss_ctc 6.481467 loss_rnnt 1.429984 hw_loss 0.194207 lr 0.00025459 rank 1
2023-03-01 21:48:14,431 DEBUG TRAIN Batch 56/24600 loss 5.709508 loss_att 5.869081 loss_ctc 8.316124 loss_rnnt 5.089992 hw_loss 0.450098 lr 0.00025459 rank 0
2023-03-01 21:48:36,355 DEBUG TRAIN Batch 56/24700 loss 5.488092 loss_att 8.109745 loss_ctc 8.459835 loss_rnnt 4.518914 hw_loss 0.091153 lr 0.00025458 rank 1
2023-03-01 21:48:36,356 DEBUG TRAIN Batch 56/24700 loss 3.979180 loss_att 6.180326 loss_ctc 4.113497 loss_rnnt 3.432842 hw_loss 0.165373 lr 0.00025458 rank 0
2023-03-01 21:48:58,514 DEBUG TRAIN Batch 56/24800 loss 3.398647 loss_att 6.832730 loss_ctc 7.449564 loss_rnnt 2.133447 hw_loss 0.071739 lr 0.00025457 rank 1
2023-03-01 21:48:58,516 DEBUG TRAIN Batch 56/24800 loss 5.277786 loss_att 10.072557 loss_ctc 11.505039 loss_rnnt 3.356989 hw_loss 0.246641 lr 0.00025458 rank 0
2023-03-01 21:49:30,225 DEBUG TRAIN Batch 56/24900 loss 7.961556 loss_att 10.201887 loss_ctc 11.584606 loss_rnnt 6.945213 hw_loss 0.159758 lr 0.00025456 rank 1
2023-03-01 21:49:30,227 DEBUG TRAIN Batch 56/24900 loss 6.364304 loss_att 10.472133 loss_ctc 11.941401 loss_rnnt 4.711643 hw_loss 0.164028 lr 0.00025457 rank 0
2023-03-01 21:49:53,537 DEBUG TRAIN Batch 56/25000 loss 7.941327 loss_att 8.632486 loss_ctc 13.639905 loss_rnnt 6.986502 hw_loss 0.106464 lr 0.00025456 rank 1
2023-03-01 21:49:53,539 DEBUG TRAIN Batch 56/25000 loss 2.578058 loss_att 6.318389 loss_ctc 7.932662 loss_rnnt 1.080819 hw_loss 0.066048 lr 0.00025456 rank 0
2023-03-01 21:50:17,002 DEBUG TRAIN Batch 56/25100 loss 9.131889 loss_att 9.438059 loss_ctc 14.196943 loss_rnnt 8.218624 hw_loss 0.331293 lr 0.00025455 rank 1
2023-03-01 21:50:17,003 DEBUG TRAIN Batch 56/25100 loss 8.643042 loss_att 11.536897 loss_ctc 15.249915 loss_rnnt 7.125728 hw_loss 0.108049 lr 0.00025455 rank 0
2023-03-01 21:50:39,238 DEBUG TRAIN Batch 56/25200 loss 6.445457 loss_att 9.673907 loss_ctc 12.910505 loss_rnnt 4.784513 hw_loss 0.287340 lr 0.00025454 rank 1
2023-03-01 21:50:39,239 DEBUG TRAIN Batch 56/25200 loss 4.605416 loss_att 6.783164 loss_ctc 9.442608 loss_rnnt 3.380752 hw_loss 0.270291 lr 0.00025454 rank 0
2023-03-01 21:51:11,907 DEBUG TRAIN Batch 56/25300 loss 11.034383 loss_att 14.126360 loss_ctc 13.969852 loss_rnnt 9.937880 hw_loss 0.162586 lr 0.00025453 rank 1
2023-03-01 21:51:11,909 DEBUG TRAIN Batch 56/25300 loss 7.105912 loss_att 8.333410 loss_ctc 10.375854 loss_rnnt 6.306907 hw_loss 0.220337 lr 0.00025453 rank 0
2023-03-01 21:51:34,235 DEBUG TRAIN Batch 56/25400 loss 4.889636 loss_att 9.236603 loss_ctc 8.390831 loss_rnnt 3.425394 hw_loss 0.240040 lr 0.00025452 rank 1
2023-03-01 21:51:34,236 DEBUG TRAIN Batch 56/25400 loss 8.037493 loss_att 12.308359 loss_ctc 17.199362 loss_rnnt 5.935329 hw_loss 0.049512 lr 0.00025453 rank 0
2023-03-01 21:51:56,715 DEBUG TRAIN Batch 56/25500 loss 4.004334 loss_att 6.793962 loss_ctc 6.553662 loss_rnnt 2.977466 hw_loss 0.241934 lr 0.00025451 rank 1
2023-03-01 21:51:56,717 DEBUG TRAIN Batch 56/25500 loss 6.795780 loss_att 11.491199 loss_ctc 10.856333 loss_rnnt 5.222435 hw_loss 0.174102 lr 0.00025452 rank 0
2023-03-01 21:52:28,598 DEBUG TRAIN Batch 56/25600 loss 7.589493 loss_att 9.053165 loss_ctc 12.844405 loss_rnnt 6.503342 hw_loss 0.173927 lr 0.00025451 rank 1
2023-03-01 21:52:28,601 DEBUG TRAIN Batch 56/25600 loss 7.226628 loss_att 9.191507 loss_ctc 10.388863 loss_rnnt 6.311772 hw_loss 0.187965 lr 0.00025451 rank 0
2023-03-01 21:52:51,726 DEBUG TRAIN Batch 56/25700 loss 11.192056 loss_att 11.702875 loss_ctc 18.035063 loss_rnnt 10.061267 hw_loss 0.217923 lr 0.00025450 rank 1
2023-03-01 21:52:51,729 DEBUG TRAIN Batch 56/25700 loss 10.737727 loss_att 13.172417 loss_ctc 17.538944 loss_rnnt 9.218891 hw_loss 0.234503 lr 0.00025450 rank 0
2023-03-01 21:53:14,514 DEBUG TRAIN Batch 56/25800 loss 5.561918 loss_att 6.720841 loss_ctc 7.661918 loss_rnnt 4.918878 hw_loss 0.246105 lr 0.00025449 rank 1
2023-03-01 21:53:14,515 DEBUG TRAIN Batch 56/25800 loss 5.220756 loss_att 8.719874 loss_ctc 12.075696 loss_rnnt 3.473902 hw_loss 0.249447 lr 0.00025449 rank 0
2023-03-01 21:53:37,510 DEBUG TRAIN Batch 56/25900 loss 7.853594 loss_att 13.891708 loss_ctc 15.404831 loss_rnnt 5.573722 hw_loss 0.122659 lr 0.00025448 rank 1
2023-03-01 21:53:37,515 DEBUG TRAIN Batch 56/25900 loss 7.875394 loss_att 10.753662 loss_ctc 10.791220 loss_rnnt 6.864236 hw_loss 0.087614 lr 0.00025449 rank 0
2023-03-01 21:54:08,983 DEBUG TRAIN Batch 56/26000 loss 4.814653 loss_att 6.770963 loss_ctc 6.266132 loss_rnnt 4.143854 hw_loss 0.161263 lr 0.00025447 rank 1
2023-03-01 21:54:08,985 DEBUG TRAIN Batch 56/26000 loss 7.358747 loss_att 11.351900 loss_ctc 13.837152 loss_rnnt 5.603806 hw_loss 0.173480 lr 0.00025448 rank 0
2023-03-01 21:54:32,476 DEBUG TRAIN Batch 56/26100 loss 1.754917 loss_att 4.178443 loss_ctc 4.793299 loss_rnnt 0.738257 hw_loss 0.237819 lr 0.00025446 rank 1
2023-03-01 21:54:32,478 DEBUG TRAIN Batch 56/26100 loss 2.023498 loss_att 4.289398 loss_ctc 3.743491 loss_rnnt 1.220818 hw_loss 0.225314 lr 0.00025447 rank 0
2023-03-01 21:54:54,898 DEBUG TRAIN Batch 56/26200 loss 13.902688 loss_att 18.841665 loss_ctc 25.526459 loss_rnnt 11.250942 hw_loss 0.213963 lr 0.00025446 rank 1
2023-03-01 21:54:54,899 DEBUG TRAIN Batch 56/26200 loss 12.966581 loss_att 18.283947 loss_ctc 22.285423 loss_rnnt 10.555010 hw_loss 0.197973 lr 0.00025446 rank 0
2023-03-01 21:55:26,672 DEBUG TRAIN Batch 56/26300 loss 13.880041 loss_att 20.224577 loss_ctc 25.697599 loss_rnnt 10.905051 hw_loss 0.244514 lr 0.00025445 rank 1
2023-03-01 21:55:26,674 DEBUG TRAIN Batch 56/26300 loss 6.135955 loss_att 10.299979 loss_ctc 11.198966 loss_rnnt 4.511689 hw_loss 0.218236 lr 0.00025445 rank 0
2023-03-01 21:55:50,762 DEBUG TRAIN Batch 56/26400 loss 3.730339 loss_att 7.671208 loss_ctc 11.669065 loss_rnnt 1.790911 hw_loss 0.173920 lr 0.00025444 rank 1
2023-03-01 21:55:50,764 DEBUG TRAIN Batch 56/26400 loss 17.454245 loss_att 18.141609 loss_ctc 26.583761 loss_rnnt 16.029509 hw_loss 0.131241 lr 0.00025444 rank 0
2023-03-01 21:56:13,713 DEBUG TRAIN Batch 56/26500 loss 5.130563 loss_att 11.023285 loss_ctc 12.291342 loss_rnnt 2.843986 hw_loss 0.287366 lr 0.00025443 rank 1
2023-03-01 21:56:13,716 DEBUG TRAIN Batch 56/26500 loss 4.984431 loss_att 6.187491 loss_ctc 9.220701 loss_rnnt 3.989420 hw_loss 0.355428 lr 0.00025444 rank 0
2023-03-01 21:56:36,283 DEBUG TRAIN Batch 56/26600 loss 3.764646 loss_att 7.874723 loss_ctc 7.357594 loss_rnnt 2.413695 hw_loss 0.093517 lr 0.00025442 rank 1
2023-03-01 21:56:36,285 DEBUG TRAIN Batch 56/26600 loss 2.898853 loss_att 6.070325 loss_ctc 6.269119 loss_rnnt 1.728915 hw_loss 0.161765 lr 0.00025443 rank 0
2023-03-01 21:57:08,489 DEBUG TRAIN Batch 56/26700 loss 15.285439 loss_att 15.865404 loss_ctc 18.252056 loss_rnnt 14.648433 hw_loss 0.235246 lr 0.00025442 rank 0
2023-03-01 21:57:08,491 DEBUG TRAIN Batch 56/26700 loss 3.017098 loss_att 6.466333 loss_ctc 8.604180 loss_rnnt 1.465468 hw_loss 0.219072 lr 0.00025442 rank 1
2023-03-01 21:57:32,257 DEBUG TRAIN Batch 56/26800 loss 6.090331 loss_att 8.795956 loss_ctc 8.376855 loss_rnnt 5.141078 hw_loss 0.193609 lr 0.00025441 rank 1
2023-03-01 21:57:32,260 DEBUG TRAIN Batch 56/26800 loss 11.054019 loss_att 13.702309 loss_ctc 15.475416 loss_rnnt 9.779177 hw_loss 0.291873 lr 0.00025441 rank 0
2023-03-01 21:57:54,957 DEBUG TRAIN Batch 56/26900 loss 5.589042 loss_att 8.364560 loss_ctc 9.871119 loss_rnnt 4.404348 hw_loss 0.109963 lr 0.00025440 rank 1
2023-03-01 21:57:54,961 DEBUG TRAIN Batch 56/26900 loss 5.753718 loss_att 8.123261 loss_ctc 9.262720 loss_rnnt 4.707353 hw_loss 0.196105 lr 0.00025440 rank 0
2023-03-01 21:58:27,125 DEBUG TRAIN Batch 56/27000 loss 9.504921 loss_att 9.608067 loss_ctc 13.499530 loss_rnnt 8.818258 hw_loss 0.250161 lr 0.00025439 rank 1
2023-03-01 21:58:27,127 DEBUG TRAIN Batch 56/27000 loss 17.986219 loss_att 18.057106 loss_ctc 25.316999 loss_rnnt 16.894012 hw_loss 0.188608 lr 0.00025439 rank 0
2023-03-01 21:58:59,488 DEBUG TRAIN Batch 56/27100 loss 3.486982 loss_att 9.138298 loss_ctc 9.707436 loss_rnnt 1.437702 hw_loss 0.168044 lr 0.00025438 rank 1
2023-03-01 21:58:59,497 DEBUG TRAIN Batch 56/27100 loss 6.142144 loss_att 6.835506 loss_ctc 10.784139 loss_rnnt 5.235415 hw_loss 0.279607 lr 0.00025439 rank 0
2023-03-01 21:59:21,756 DEBUG TRAIN Batch 56/27200 loss 4.379963 loss_att 8.275293 loss_ctc 12.208870 loss_rnnt 2.462230 hw_loss 0.177775 lr 0.00025437 rank 1
2023-03-01 21:59:21,759 DEBUG TRAIN Batch 56/27200 loss 4.555943 loss_att 7.195470 loss_ctc 7.359183 loss_rnnt 3.541451 hw_loss 0.211539 lr 0.00025438 rank 0
2023-03-01 21:59:44,762 DEBUG TRAIN Batch 56/27300 loss 7.977524 loss_att 12.613978 loss_ctc 14.705284 loss_rnnt 6.038357 hw_loss 0.215327 lr 0.00025437 rank 1
2023-03-01 21:59:44,763 DEBUG TRAIN Batch 56/27300 loss 5.042488 loss_att 6.300095 loss_ctc 7.575117 loss_rnnt 4.387523 hw_loss 0.123299 lr 0.00025437 rank 0
2023-03-01 22:00:16,228 DEBUG TRAIN Batch 56/27400 loss 2.904251 loss_att 5.278046 loss_ctc 5.625297 loss_rnnt 1.989475 hw_loss 0.144770 lr 0.00025436 rank 1
2023-03-01 22:00:16,230 DEBUG TRAIN Batch 56/27400 loss 6.649273 loss_att 8.005998 loss_ctc 11.728342 loss_rnnt 5.559597 hw_loss 0.264605 lr 0.00025436 rank 0
2023-03-01 22:00:47,739 DEBUG TRAIN Batch 56/27500 loss 8.092429 loss_att 13.637218 loss_ctc 15.270418 loss_rnnt 5.944603 hw_loss 0.153381 lr 0.00025435 rank 1
2023-03-01 22:00:47,740 DEBUG TRAIN Batch 56/27500 loss 3.700247 loss_att 6.971837 loss_ctc 6.952480 loss_rnnt 2.537440 hw_loss 0.140358 lr 0.00025435 rank 0
2023-03-01 22:01:10,453 DEBUG TRAIN Batch 56/27600 loss 7.140568 loss_att 9.987421 loss_ctc 11.191069 loss_rnnt 5.945997 hw_loss 0.159626 lr 0.00025434 rank 1
2023-03-01 22:01:10,456 DEBUG TRAIN Batch 56/27600 loss 6.229365 loss_att 10.822210 loss_ctc 12.293781 loss_rnnt 4.319710 hw_loss 0.342182 lr 0.00025435 rank 0
2023-03-01 22:01:33,260 DEBUG TRAIN Batch 56/27700 loss 4.460074 loss_att 11.094437 loss_ctc 10.216425 loss_rnnt 2.244293 hw_loss 0.227616 lr 0.00025433 rank 1
2023-03-01 22:01:33,262 DEBUG TRAIN Batch 56/27700 loss 6.081954 loss_att 7.860444 loss_ctc 11.196716 loss_rnnt 4.936662 hw_loss 0.201797 lr 0.00025434 rank 0
2023-03-01 22:02:05,830 DEBUG TRAIN Batch 56/27800 loss 5.224384 loss_att 6.584777 loss_ctc 7.935553 loss_rnnt 4.501220 hw_loss 0.167993 lr 0.00025432 rank 1
2023-03-01 22:02:05,833 DEBUG TRAIN Batch 56/27800 loss 3.709598 loss_att 8.728509 loss_ctc 10.125347 loss_rnnt 1.774297 hw_loss 0.142660 lr 0.00025433 rank 0
2023-03-01 22:02:37,726 DEBUG TRAIN Batch 56/27900 loss 5.325002 loss_att 8.035028 loss_ctc 7.780803 loss_rnnt 4.384333 hw_loss 0.133544 lr 0.00025432 rank 1
2023-03-01 22:02:37,728 DEBUG TRAIN Batch 56/27900 loss 17.945469 loss_att 20.701391 loss_ctc 35.225101 loss_rnnt 14.935524 hw_loss 0.290264 lr 0.00025432 rank 0
2023-03-01 22:03:00,321 DEBUG TRAIN Batch 56/28000 loss 5.028737 loss_att 7.841248 loss_ctc 8.467108 loss_rnnt 3.927748 hw_loss 0.150070 lr 0.00025431 rank 1
2023-03-01 22:03:00,324 DEBUG TRAIN Batch 56/28000 loss 3.812521 loss_att 5.218403 loss_ctc 5.272089 loss_rnnt 3.222421 hw_loss 0.214341 lr 0.00025431 rank 0
2023-03-01 22:03:31,765 DEBUG TRAIN Batch 56/28100 loss 4.664735 loss_att 6.353066 loss_ctc 7.408324 loss_rnnt 3.847575 hw_loss 0.213153 lr 0.00025430 rank 1
2023-03-01 22:03:31,768 DEBUG TRAIN Batch 56/28100 loss 10.352279 loss_att 16.220911 loss_ctc 17.068010 loss_rnnt 8.240133 hw_loss 0.080601 lr 0.00025430 rank 0
2023-03-01 22:04:03,366 DEBUG TRAIN Batch 56/28200 loss 12.875898 loss_att 13.272692 loss_ctc 24.034702 loss_rnnt 11.308195 hw_loss 0.000947 lr 0.00025429 rank 1
2023-03-01 22:04:03,368 DEBUG TRAIN Batch 56/28200 loss 13.312061 loss_att 16.428297 loss_ctc 22.962053 loss_rnnt 11.315432 hw_loss 0.162593 lr 0.00025430 rank 0
2023-03-01 22:04:26,306 DEBUG TRAIN Batch 56/28300 loss 8.335631 loss_att 9.660774 loss_ctc 14.402860 loss_rnnt 7.109016 hw_loss 0.286166 lr 0.00025428 rank 1
2023-03-01 22:04:26,309 DEBUG TRAIN Batch 56/28300 loss 5.270227 loss_att 9.485391 loss_ctc 11.843514 loss_rnnt 3.491955 hw_loss 0.110252 lr 0.00025429 rank 0
2023-03-01 22:04:48,848 DEBUG TRAIN Batch 56/28400 loss 7.546103 loss_att 13.182285 loss_ctc 19.576202 loss_rnnt 4.708992 hw_loss 0.198491 lr 0.00025428 rank 1
2023-03-01 22:04:48,850 DEBUG TRAIN Batch 56/28400 loss 4.326855 loss_att 4.974597 loss_ctc 6.289193 loss_rnnt 3.761439 hw_loss 0.326667 lr 0.00025428 rank 0
2023-03-01 22:05:19,578 DEBUG TRAIN Batch 56/28500 loss 3.863532 loss_att 7.763615 loss_ctc 5.399859 loss_rnnt 2.799283 hw_loss 0.148854 lr 0.00025427 rank 0
2023-03-01 22:05:19,593 DEBUG TRAIN Batch 56/28500 loss 8.820790 loss_att 10.198938 loss_ctc 15.482924 loss_rnnt 7.595267 hw_loss 0.115519 lr 0.00025427 rank 1
2023-03-01 22:05:52,094 DEBUG TRAIN Batch 56/28600 loss 2.038574 loss_att 7.338717 loss_ctc 3.412376 loss_rnnt 0.670439 hw_loss 0.234250 lr 0.00025426 rank 1
2023-03-01 22:05:52,096 DEBUG TRAIN Batch 56/28600 loss 4.119795 loss_att 7.588667 loss_ctc 7.568954 loss_rnnt 2.923069 hw_loss 0.080745 lr 0.00025426 rank 0
2023-03-01 22:06:13,979 DEBUG TRAIN Batch 56/28700 loss 7.778400 loss_att 12.165373 loss_ctc 11.980614 loss_rnnt 6.239020 hw_loss 0.190670 lr 0.00025425 rank 1
2023-03-01 22:06:13,983 DEBUG TRAIN Batch 56/28700 loss 7.386778 loss_att 11.088607 loss_ctc 13.312899 loss_rnnt 5.748896 hw_loss 0.201314 lr 0.00025425 rank 0
2023-03-01 22:06:44,995 DEBUG TRAIN Batch 56/28800 loss 5.909289 loss_att 8.374699 loss_ctc 8.009988 loss_rnnt 5.101186 hw_loss 0.065490 lr 0.00025424 rank 1
2023-03-01 22:06:44,999 DEBUG TRAIN Batch 56/28800 loss 15.078291 loss_att 18.350445 loss_ctc 21.997002 loss_rnnt 13.385201 hw_loss 0.217810 lr 0.00025425 rank 0
2023-03-01 22:07:16,932 DEBUG TRAIN Batch 56/28900 loss 9.180981 loss_att 10.474825 loss_ctc 14.041405 loss_rnnt 8.148630 hw_loss 0.235359 lr 0.00025423 rank 1
2023-03-01 22:07:16,933 DEBUG TRAIN Batch 56/28900 loss 10.334996 loss_att 11.968987 loss_ctc 18.786396 loss_rnnt 8.780638 hw_loss 0.188824 lr 0.00025424 rank 0
2023-03-01 22:07:39,147 DEBUG TRAIN Batch 56/29000 loss 7.887311 loss_att 8.136606 loss_ctc 11.099166 loss_rnnt 7.274307 hw_loss 0.252931 lr 0.00025423 rank 1
2023-03-01 22:07:39,149 DEBUG TRAIN Batch 56/29000 loss 8.258577 loss_att 9.472825 loss_ctc 15.338345 loss_rnnt 6.934825 hw_loss 0.256751 lr 0.00025423 rank 0
2023-03-01 22:08:00,763 DEBUG TRAIN Batch 56/29100 loss 7.026353 loss_att 10.749254 loss_ctc 10.895708 loss_rnnt 5.694653 hw_loss 0.133512 lr 0.00025422 rank 1
2023-03-01 22:08:00,765 DEBUG TRAIN Batch 56/29100 loss 9.346097 loss_att 11.489771 loss_ctc 17.657616 loss_rnnt 7.659232 hw_loss 0.281114 lr 0.00025422 rank 0
2023-03-01 22:08:31,500 DEBUG TRAIN Batch 56/29200 loss 3.942610 loss_att 7.401899 loss_ctc 5.800986 loss_rnnt 2.969237 hw_loss 0.063247 lr 0.00025421 rank 1
2023-03-01 22:08:31,503 DEBUG TRAIN Batch 56/29200 loss 3.193133 loss_att 6.770029 loss_ctc 5.291656 loss_rnnt 2.069821 hw_loss 0.240242 lr 0.00025421 rank 0
2023-03-01 22:09:02,678 DEBUG TRAIN Batch 56/29300 loss 12.622209 loss_att 14.381792 loss_ctc 15.772314 loss_rnnt 11.772974 hw_loss 0.144941 lr 0.00025420 rank 1
2023-03-01 22:09:02,679 DEBUG TRAIN Batch 56/29300 loss 2.178671 loss_att 5.635266 loss_ctc 4.267269 loss_rnnt 1.174237 hw_loss 0.064941 lr 0.00025421 rank 0
2023-03-01 22:09:24,288 DEBUG TRAIN Batch 56/29400 loss 6.042428 loss_att 8.138545 loss_ctc 7.128193 loss_rnnt 5.329877 hw_loss 0.278548 lr 0.00025419 rank 1
2023-03-01 22:09:24,288 DEBUG TRAIN Batch 56/29400 loss 8.916079 loss_att 10.014109 loss_ctc 11.237686 loss_rnnt 8.333770 hw_loss 0.099666 lr 0.00025420 rank 0
2023-03-01 22:09:55,918 DEBUG TRAIN Batch 56/29500 loss 12.887911 loss_att 13.168884 loss_ctc 17.957771 loss_rnnt 12.067917 hw_loss 0.164656 lr 0.00025418 rank 1
2023-03-01 22:09:55,921 DEBUG TRAIN Batch 56/29500 loss 7.642283 loss_att 12.103151 loss_ctc 15.628429 loss_rnnt 5.547686 hw_loss 0.258010 lr 0.00025419 rank 0
2023-03-01 22:10:27,823 DEBUG TRAIN Batch 56/29600 loss 5.774661 loss_att 7.196785 loss_ctc 11.031664 loss_rnnt 4.704525 hw_loss 0.158956 lr 0.00025418 rank 1
2023-03-01 22:10:27,825 DEBUG TRAIN Batch 56/29600 loss 14.633405 loss_att 18.335745 loss_ctc 25.510227 loss_rnnt 12.316154 hw_loss 0.237263 lr 0.00025418 rank 0
2023-03-01 22:10:50,212 DEBUG TRAIN Batch 56/29700 loss 6.134023 loss_att 12.057218 loss_ctc 14.644024 loss_rnnt 3.768885 hw_loss 0.085935 lr 0.00025417 rank 1
2023-03-01 22:10:50,213 DEBUG TRAIN Batch 56/29700 loss 4.119282 loss_att 9.383893 loss_ctc 6.353637 loss_rnnt 2.627243 hw_loss 0.264756 lr 0.00025417 rank 0
2023-03-01 22:11:11,860 DEBUG TRAIN Batch 56/29800 loss 9.266429 loss_att 13.424623 loss_ctc 13.819096 loss_rnnt 7.720413 hw_loss 0.201289 lr 0.00025416 rank 1
2023-03-01 22:11:11,862 DEBUG TRAIN Batch 56/29800 loss 2.722462 loss_att 5.517473 loss_ctc 4.513989 loss_rnnt 1.857681 hw_loss 0.125454 lr 0.00025416 rank 0
2023-03-01 22:11:43,569 DEBUG TRAIN Batch 56/29900 loss 2.677508 loss_att 5.465549 loss_ctc 3.832941 loss_rnnt 1.899158 hw_loss 0.125033 lr 0.00025415 rank 1
2023-03-01 22:11:43,569 DEBUG TRAIN Batch 56/29900 loss 2.245134 loss_att 5.657550 loss_ctc 4.856061 loss_rnnt 1.138205 hw_loss 0.143106 lr 0.00025416 rank 0
2023-03-01 22:12:12,966 DEBUG TRAIN Batch 56/30000 loss 3.133619 loss_att 5.236882 loss_ctc 5.612157 loss_rnnt 2.264607 hw_loss 0.221038 lr 0.00025414 rank 1
2023-03-01 22:12:12,969 DEBUG TRAIN Batch 56/30000 loss 2.105269 loss_att 6.644988 loss_ctc 3.535954 loss_rnnt 0.926093 hw_loss 0.150889 lr 0.00025415 rank 0
2023-03-01 22:12:33,969 DEBUG TRAIN Batch 56/30100 loss 6.001103 loss_att 8.900141 loss_ctc 11.679023 loss_rnnt 4.606410 hw_loss 0.108432 lr 0.00025414 rank 1
2023-03-01 22:12:33,971 DEBUG TRAIN Batch 56/30100 loss 3.749309 loss_att 8.755625 loss_ctc 8.068434 loss_rnnt 2.111218 hw_loss 0.114272 lr 0.00025414 rank 0
2023-03-01 22:13:06,118 DEBUG TRAIN Batch 56/30200 loss 1.848900 loss_att 5.868109 loss_ctc 3.007264 loss_rnnt 0.738233 hw_loss 0.285707 lr 0.00025413 rank 1
2023-03-01 22:13:06,119 DEBUG TRAIN Batch 56/30200 loss 3.382012 loss_att 6.106829 loss_ctc 6.170588 loss_rnnt 2.364148 hw_loss 0.189543 lr 0.00025413 rank 0
2023-03-01 22:13:39,396 DEBUG TRAIN Batch 56/30300 loss 7.879972 loss_att 8.565207 loss_ctc 10.775378 loss_rnnt 7.268929 hw_loss 0.164889 lr 0.00025412 rank 1
2023-03-01 22:13:39,399 DEBUG TRAIN Batch 56/30300 loss 2.929364 loss_att 4.945839 loss_ctc 6.099347 loss_rnnt 2.026376 hw_loss 0.144428 lr 0.00025412 rank 0
2023-03-01 22:14:01,055 DEBUG TRAIN Batch 56/30400 loss 11.078247 loss_att 15.038851 loss_ctc 17.405739 loss_rnnt 9.425980 hw_loss 0.030902 lr 0.00025411 rank 1
2023-03-01 22:14:01,057 DEBUG TRAIN Batch 56/30400 loss 9.174624 loss_att 15.014178 loss_ctc 16.051123 loss_rnnt 6.987941 hw_loss 0.191073 lr 0.00025412 rank 0
2023-03-01 22:14:23,099 DEBUG TRAIN Batch 56/30500 loss 4.627272 loss_att 5.831583 loss_ctc 7.015384 loss_rnnt 3.945892 hw_loss 0.228941 lr 0.00025410 rank 1
2023-03-01 22:14:23,102 DEBUG TRAIN Batch 56/30500 loss 6.689835 loss_att 8.131675 loss_ctc 12.525573 loss_rnnt 5.520061 hw_loss 0.193701 lr 0.00025411 rank 0
2023-03-01 22:14:54,456 DEBUG TRAIN Batch 56/30600 loss 7.666949 loss_att 11.436864 loss_ctc 15.694271 loss_rnnt 5.731790 hw_loss 0.207875 lr 0.00025409 rank 1
2023-03-01 22:14:54,460 DEBUG TRAIN Batch 56/30600 loss 3.950791 loss_att 7.574682 loss_ctc 6.408773 loss_rnnt 2.817955 hw_loss 0.150613 lr 0.00025410 rank 0
2023-03-01 22:15:27,524 DEBUG TRAIN Batch 56/30700 loss 8.666594 loss_att 10.803897 loss_ctc 13.888622 loss_rnnt 7.416693 hw_loss 0.236567 lr 0.00025409 rank 1
2023-03-01 22:15:27,527 DEBUG TRAIN Batch 56/30700 loss 12.144625 loss_att 14.907312 loss_ctc 21.364706 loss_rnnt 10.283810 hw_loss 0.148002 lr 0.00025409 rank 0
2023-03-01 22:15:49,458 DEBUG TRAIN Batch 56/30800 loss 5.361873 loss_att 8.272802 loss_ctc 8.151337 loss_rnnt 4.329962 hw_loss 0.145869 lr 0.00025408 rank 1
2023-03-01 22:15:49,463 DEBUG TRAIN Batch 56/30800 loss 5.151059 loss_att 7.306326 loss_ctc 10.157871 loss_rnnt 3.906266 hw_loss 0.274056 lr 0.00025408 rank 0
2023-03-01 22:16:21,433 DEBUG TRAIN Batch 56/30900 loss 6.357107 loss_att 9.097630 loss_ctc 10.309775 loss_rnnt 5.170280 hw_loss 0.209438 lr 0.00025407 rank 1
2023-03-01 22:16:21,438 DEBUG TRAIN Batch 56/30900 loss 5.191753 loss_att 7.288085 loss_ctc 7.223977 loss_rnnt 4.398845 hw_loss 0.192521 lr 0.00025407 rank 0
2023-03-01 22:16:42,887 DEBUG TRAIN Batch 56/31000 loss 10.501350 loss_att 10.573287 loss_ctc 13.704002 loss_rnnt 9.923118 hw_loss 0.256549 lr 0.00025406 rank 1
2023-03-01 22:16:42,888 DEBUG TRAIN Batch 56/31000 loss 9.708186 loss_att 10.392216 loss_ctc 17.512686 loss_rnnt 8.470077 hw_loss 0.113821 lr 0.00025407 rank 0
2023-03-01 22:17:14,840 DEBUG TRAIN Batch 56/31100 loss 7.169746 loss_att 8.845891 loss_ctc 10.416124 loss_rnnt 6.401289 hw_loss 0.000708 lr 0.00025405 rank 1
2023-03-01 22:17:14,842 DEBUG TRAIN Batch 56/31100 loss 7.431595 loss_att 11.511728 loss_ctc 11.105399 loss_rnnt 6.047210 hw_loss 0.147220 lr 0.00025406 rank 0
2023-03-01 22:17:37,050 DEBUG TRAIN Batch 56/31200 loss 6.018543 loss_att 9.783833 loss_ctc 12.851620 loss_rnnt 4.232439 hw_loss 0.228692 lr 0.00025405 rank 1
2023-03-01 22:17:37,050 DEBUG TRAIN Batch 56/31200 loss 5.067998 loss_att 8.364359 loss_ctc 7.886806 loss_rnnt 3.940506 hw_loss 0.173212 lr 0.00025405 rank 0
2023-03-01 22:18:09,633 DEBUG TRAIN Batch 56/31300 loss 7.471500 loss_att 10.383731 loss_ctc 13.820312 loss_rnnt 5.966292 hw_loss 0.142975 lr 0.00025404 rank 1
2023-03-01 22:18:09,635 DEBUG TRAIN Batch 56/31300 loss 3.645615 loss_att 5.745985 loss_ctc 4.868163 loss_rnnt 2.937134 hw_loss 0.235125 lr 0.00025404 rank 0
2023-03-01 22:18:42,269 DEBUG TRAIN Batch 56/31400 loss 8.481627 loss_att 12.837402 loss_ctc 17.794594 loss_rnnt 6.237662 hw_loss 0.245777 lr 0.00025403 rank 1
2023-03-01 22:18:42,272 DEBUG TRAIN Batch 56/31400 loss 3.226741 loss_att 4.903048 loss_ctc 4.879069 loss_rnnt 2.544260 hw_loss 0.237955 lr 0.00025403 rank 0
2023-03-01 22:19:05,164 DEBUG TRAIN Batch 56/31500 loss 5.860008 loss_att 8.403490 loss_ctc 10.968737 loss_rnnt 4.546648 hw_loss 0.231565 lr 0.00025402 rank 1
2023-03-01 22:19:05,167 DEBUG TRAIN Batch 56/31500 loss 9.333517 loss_att 11.240888 loss_ctc 13.451687 loss_rnnt 8.249237 hw_loss 0.288218 lr 0.00025402 rank 0
2023-03-01 22:19:38,843 DEBUG TRAIN Batch 56/31600 loss 7.200927 loss_att 7.537133 loss_ctc 13.408502 loss_rnnt 6.139375 hw_loss 0.312439 lr 0.00025401 rank 1
2023-03-01 22:19:38,844 DEBUG TRAIN Batch 56/31600 loss 5.123565 loss_att 7.958545 loss_ctc 11.134050 loss_rnnt 3.634387 hw_loss 0.226469 lr 0.00025402 rank 0
2023-03-01 22:20:00,678 DEBUG TRAIN Batch 56/31700 loss 1.431566 loss_att 4.917267 loss_ctc 3.908607 loss_rnnt 0.259027 hw_loss 0.272113 lr 0.00025400 rank 1
2023-03-01 22:20:00,681 DEBUG TRAIN Batch 56/31700 loss 11.266306 loss_att 13.975808 loss_ctc 25.112371 loss_rnnt 8.735814 hw_loss 0.267090 lr 0.00025401 rank 0
2023-03-01 22:20:33,542 DEBUG TRAIN Batch 56/31800 loss 10.177100 loss_att 12.573084 loss_ctc 17.293331 loss_rnnt 8.609854 hw_loss 0.261034 lr 0.00025400 rank 1
2023-03-01 22:20:33,545 DEBUG TRAIN Batch 56/31800 loss 2.227738 loss_att 5.906599 loss_ctc 6.183141 loss_rnnt 0.862098 hw_loss 0.192152 lr 0.00025400 rank 0
2023-03-01 22:20:55,807 DEBUG TRAIN Batch 56/31900 loss 11.879415 loss_att 15.663395 loss_ctc 18.238743 loss_rnnt 10.208818 hw_loss 0.123543 lr 0.00025399 rank 1
2023-03-01 22:20:55,810 DEBUG TRAIN Batch 56/31900 loss 6.003319 loss_att 9.120319 loss_ctc 8.439454 loss_rnnt 4.986177 hw_loss 0.129231 lr 0.00025399 rank 0
2023-03-01 22:21:28,376 DEBUG TRAIN Batch 56/32000 loss 7.933146 loss_att 12.706961 loss_ctc 23.093191 loss_rnnt 4.915227 hw_loss 0.078404 lr 0.00025398 rank 1
2023-03-01 22:21:28,378 DEBUG TRAIN Batch 56/32000 loss 2.735422 loss_att 5.508128 loss_ctc 6.387906 loss_rnnt 1.676512 hw_loss 0.032572 lr 0.00025398 rank 0
2023-03-01 22:22:01,657 DEBUG TRAIN Batch 56/32100 loss 6.895257 loss_att 9.202462 loss_ctc 12.768308 loss_rnnt 5.597744 hw_loss 0.099371 lr 0.00025398 rank 0
2023-03-01 22:22:01,657 DEBUG TRAIN Batch 56/32100 loss 5.083677 loss_att 6.949967 loss_ctc 7.568553 loss_rnnt 4.252408 hw_loss 0.237553 lr 0.00025397 rank 1
2023-03-01 22:22:24,442 DEBUG TRAIN Batch 56/32200 loss 4.800802 loss_att 6.163431 loss_ctc 6.781104 loss_rnnt 4.174256 hw_loss 0.168711 lr 0.00025396 rank 1
2023-03-01 22:22:24,444 DEBUG TRAIN Batch 56/32200 loss 9.778211 loss_att 12.331194 loss_ctc 18.006689 loss_rnnt 8.083168 hw_loss 0.163717 lr 0.00025397 rank 0
2023-03-01 22:22:46,397 DEBUG TRAIN Batch 56/32300 loss 2.811716 loss_att 4.665138 loss_ctc 5.672718 loss_rnnt 1.899983 hw_loss 0.299215 lr 0.00025396 rank 1
2023-03-01 22:22:46,398 DEBUG TRAIN Batch 56/32300 loss 4.214664 loss_att 6.927685 loss_ctc 9.421680 loss_rnnt 2.954949 hw_loss 0.042830 lr 0.00025396 rank 0
2023-03-01 22:23:20,645 DEBUG TRAIN Batch 56/32400 loss 3.248812 loss_att 5.930555 loss_ctc 5.707108 loss_rnnt 2.237745 hw_loss 0.275523 lr 0.00025395 rank 1
2023-03-01 22:23:20,647 DEBUG TRAIN Batch 56/32400 loss 10.250763 loss_att 18.298321 loss_ctc 20.739388 loss_rnnt 7.129036 hw_loss 0.213246 lr 0.00025395 rank 0
2023-03-01 22:23:54,096 DEBUG TRAIN Batch 56/32500 loss 7.580963 loss_att 10.580046 loss_ctc 11.268873 loss_rnnt 6.361196 hw_loss 0.240429 lr 0.00025394 rank 1
2023-03-01 22:23:54,098 DEBUG TRAIN Batch 56/32500 loss 3.786759 loss_att 8.777723 loss_ctc 9.097038 loss_rnnt 1.997811 hw_loss 0.155096 lr 0.00025394 rank 0
2023-03-01 22:24:16,288 DEBUG TRAIN Batch 56/32600 loss 8.250750 loss_att 11.500908 loss_ctc 12.160522 loss_rnnt 6.982300 hw_loss 0.182089 lr 0.00025393 rank 1
2023-03-01 22:24:16,290 DEBUG TRAIN Batch 56/32600 loss 8.826525 loss_att 12.147184 loss_ctc 14.520844 loss_rnnt 7.353487 hw_loss 0.093118 lr 0.00025393 rank 0
2023-03-01 22:24:50,475 DEBUG TRAIN Batch 56/32700 loss 3.270628 loss_att 6.147637 loss_ctc 7.083226 loss_rnnt 2.126334 hw_loss 0.113525 lr 0.00025392 rank 1
2023-03-01 22:24:50,476 DEBUG TRAIN Batch 56/32700 loss 7.016800 loss_att 10.121293 loss_ctc 11.633163 loss_rnnt 5.690801 hw_loss 0.167972 lr 0.00025393 rank 0
2023-03-01 22:25:13,565 DEBUG TRAIN Batch 56/32800 loss 9.768099 loss_att 12.820745 loss_ctc 13.227180 loss_rnnt 8.596614 hw_loss 0.187020 lr 0.00025391 rank 1
2023-03-01 22:25:13,568 DEBUG TRAIN Batch 56/32800 loss 4.299918 loss_att 6.755797 loss_ctc 7.293062 loss_rnnt 3.319497 hw_loss 0.169049 lr 0.00025392 rank 0
2023-03-01 22:25:45,078 DEBUG TRAIN Batch 56/32900 loss 5.620476 loss_att 6.682608 loss_ctc 7.695658 loss_rnnt 5.011771 hw_loss 0.224227 lr 0.00025391 rank 1
2023-03-01 22:25:45,079 DEBUG TRAIN Batch 56/32900 loss 2.739935 loss_att 5.247427 loss_ctc 5.982416 loss_rnnt 1.745543 hw_loss 0.113555 lr 0.00025391 rank 0
2023-03-01 22:26:08,034 DEBUG TRAIN Batch 56/33000 loss 2.131145 loss_att 4.276049 loss_ctc 3.601858 loss_rnnt 1.458454 hw_loss 0.089278 lr 0.00025390 rank 1
2023-03-01 22:26:08,035 DEBUG TRAIN Batch 56/33000 loss 11.187322 loss_att 14.167997 loss_ctc 21.892628 loss_rnnt 9.127845 hw_loss 0.067436 lr 0.00025390 rank 0
2023-03-01 22:26:33,166 DEBUG TRAIN Batch 56/33100 loss 7.666258 loss_att 12.370541 loss_ctc 22.822731 loss_rnnt 4.636235 hw_loss 0.128068 lr 0.00025389 rank 1
2023-03-01 22:26:33,168 DEBUG TRAIN Batch 56/33100 loss 12.705157 loss_att 14.234135 loss_ctc 15.914185 loss_rnnt 11.920780 hw_loss 0.095084 lr 0.00025389 rank 0
2023-03-01 22:26:57,965 DEBUG TRAIN Batch 56/33200 loss 9.759243 loss_att 12.228592 loss_ctc 14.816946 loss_rnnt 8.472182 hw_loss 0.222808 lr 0.00025388 rank 1
2023-03-01 22:26:57,967 DEBUG TRAIN Batch 56/33200 loss 6.521749 loss_att 11.454129 loss_ctc 12.503267 loss_rnnt 4.607850 hw_loss 0.243536 lr 0.00025389 rank 0
2023-03-01 22:27:19,458 DEBUG TRAIN Batch 56/33300 loss 12.929929 loss_att 16.330008 loss_ctc 17.582771 loss_rnnt 11.487128 hw_loss 0.267012 lr 0.00025388 rank 0
2023-03-01 22:27:40,962 DEBUG TRAIN Batch 56/33400 loss 4.264040 loss_att 6.096760 loss_ctc 8.863955 loss_rnnt 3.144520 hw_loss 0.261851 lr 0.00025387 rank 0
2023-03-01 22:27:49,577 DEBUG CV Batch 56/0 loss 0.672067 loss_att 0.827717 loss_ctc 1.181536 loss_rnnt 0.419730 hw_loss 0.287397 history loss 0.647176 rank 1
2023-03-01 22:27:49,582 DEBUG CV Batch 56/0 loss 0.672067 loss_att 0.827717 loss_ctc 1.181536 loss_rnnt 0.419730 hw_loss 0.287397 history loss 0.647176 rank 0
2023-03-01 22:27:57,106 DEBUG CV Batch 56/100 loss 2.815642 loss_att 4.037851 loss_ctc 7.568281 loss_rnnt 1.907537 hw_loss 0.056210 history loss 3.043746 rank 1
2023-03-01 22:27:57,123 DEBUG CV Batch 56/100 loss 2.815642 loss_att 4.037851 loss_ctc 7.568281 loss_rnnt 1.907537 hw_loss 0.056210 history loss 3.043746 rank 0
2023-03-01 22:28:06,830 DEBUG CV Batch 56/200 loss 5.745818 loss_att 9.717093 loss_ctc 7.450084 loss_rnnt 4.586327 hw_loss 0.258750 history loss 3.662358 rank 1
2023-03-01 22:28:06,886 DEBUG CV Batch 56/200 loss 5.745818 loss_att 9.717093 loss_ctc 7.450084 loss_rnnt 4.586327 hw_loss 0.258750 history loss 3.662358 rank 0
2023-03-01 22:28:15,271 DEBUG CV Batch 56/300 loss 2.641147 loss_att 3.565701 loss_ctc 5.847613 loss_rnnt 1.943231 hw_loss 0.160269 history loss 3.795108 rank 1
2023-03-01 22:28:15,357 DEBUG CV Batch 56/300 loss 2.641147 loss_att 3.565701 loss_ctc 5.847613 loss_rnnt 1.943231 hw_loss 0.160269 history loss 3.795108 rank 0
2023-03-01 22:28:23,671 DEBUG CV Batch 56/400 loss 20.819389 loss_att 78.216949 loss_ctc 13.490002 loss_rnnt 10.316716 hw_loss 0.000775 history loss 4.656930 rank 1
2023-03-01 22:28:23,818 DEBUG CV Batch 56/400 loss 20.819389 loss_att 78.216949 loss_ctc 13.490002 loss_rnnt 10.316716 hw_loss 0.000775 history loss 4.656930 rank 0
2023-03-01 22:28:30,489 DEBUG CV Batch 56/500 loss 4.391154 loss_att 4.647658 loss_ctc 6.714842 loss_rnnt 3.918195 hw_loss 0.209688 history loss 5.322062 rank 1
2023-03-01 22:28:30,673 DEBUG CV Batch 56/500 loss 4.391154 loss_att 4.647658 loss_ctc 6.714842 loss_rnnt 3.918195 hw_loss 0.209688 history loss 5.322062 rank 0
2023-03-01 22:28:38,909 DEBUG CV Batch 56/600 loss 8.542531 loss_att 7.336203 loss_ctc 11.258960 loss_rnnt 8.213354 hw_loss 0.390472 history loss 6.236922 rank 1
2023-03-01 22:28:39,149 DEBUG CV Batch 56/600 loss 8.542531 loss_att 7.336203 loss_ctc 11.258960 loss_rnnt 8.213354 hw_loss 0.390472 history loss 6.236922 rank 0
2023-03-01 22:28:46,582 DEBUG CV Batch 56/700 loss 14.423964 loss_att 42.484741 loss_ctc 13.744532 loss_rnnt 8.832027 hw_loss 0.131946 history loss 6.806953 rank 1
2023-03-01 22:28:46,864 DEBUG CV Batch 56/700 loss 14.423964 loss_att 42.484741 loss_ctc 13.744532 loss_rnnt 8.832027 hw_loss 0.131946 history loss 6.806953 rank 0
2023-03-01 22:28:54,032 DEBUG CV Batch 56/800 loss 5.611867 loss_att 6.014339 loss_ctc 11.414821 loss_rnnt 4.679559 hw_loss 0.146411 history loss 6.312316 rank 1
2023-03-01 22:28:54,344 DEBUG CV Batch 56/800 loss 5.611867 loss_att 6.014339 loss_ctc 11.414821 loss_rnnt 4.679559 hw_loss 0.146411 history loss 6.312316 rank 0
2023-03-01 22:29:03,741 DEBUG CV Batch 56/900 loss 7.413257 loss_att 10.439954 loss_ctc 18.093796 loss_rnnt 5.255651 hw_loss 0.240365 history loss 6.138348 rank 1
2023-03-01 22:29:04,100 DEBUG CV Batch 56/900 loss 7.413257 loss_att 10.439954 loss_ctc 18.093796 loss_rnnt 5.255651 hw_loss 0.240365 history loss 6.138348 rank 0
2023-03-01 22:29:12,249 DEBUG CV Batch 56/1000 loss 3.131642 loss_att 4.247913 loss_ctc 4.586530 loss_rnnt 2.589711 hw_loss 0.233796 history loss 5.927589 rank 1
2023-03-01 22:29:12,668 DEBUG CV Batch 56/1000 loss 3.131642 loss_att 4.247913 loss_ctc 4.586530 loss_rnnt 2.589711 hw_loss 0.233796 history loss 5.927589 rank 0
2023-03-01 22:29:20,497 DEBUG CV Batch 56/1100 loss 4.305090 loss_att 4.313951 loss_ctc 7.943043 loss_rnnt 3.625131 hw_loss 0.362112 history loss 5.899115 rank 1
2023-03-01 22:29:20,904 DEBUG CV Batch 56/1100 loss 4.305090 loss_att 4.313951 loss_ctc 7.943043 loss_rnnt 3.625131 hw_loss 0.362112 history loss 5.899115 rank 0
2023-03-01 22:29:27,300 DEBUG CV Batch 56/1200 loss 6.542085 loss_att 6.232675 loss_ctc 8.922997 loss_rnnt 6.161561 hw_loss 0.234282 history loss 6.182958 rank 1
2023-03-01 22:29:27,798 DEBUG CV Batch 56/1200 loss 6.542085 loss_att 6.232675 loss_ctc 8.922997 loss_rnnt 6.161561 hw_loss 0.234282 history loss 6.182958 rank 0
2023-03-01 22:29:35,614 DEBUG CV Batch 56/1300 loss 5.526151 loss_att 4.810305 loss_ctc 8.391147 loss_rnnt 5.138592 hw_loss 0.278867 history loss 6.484960 rank 1
2023-03-01 22:29:36,177 DEBUG CV Batch 56/1300 loss 5.526151 loss_att 4.810305 loss_ctc 8.391147 loss_rnnt 5.138592 hw_loss 0.278867 history loss 6.484960 rank 0
2023-03-01 22:29:43,172 DEBUG CV Batch 56/1400 loss 5.959669 loss_att 15.460317 loss_ctc 7.153880 loss_rnnt 3.829392 hw_loss 0.132973 history loss 6.764483 rank 1
2023-03-01 22:29:43,803 DEBUG CV Batch 56/1400 loss 5.959669 loss_att 15.460317 loss_ctc 7.153880 loss_rnnt 3.829392 hw_loss 0.132973 history loss 6.764483 rank 0
2023-03-01 22:29:50,810 DEBUG CV Batch 56/1500 loss 6.892889 loss_att 7.213173 loss_ctc 7.735316 loss_rnnt 6.608615 hw_loss 0.202303 history loss 6.631587 rank 1
2023-03-01 22:29:51,489 DEBUG CV Batch 56/1500 loss 6.892889 loss_att 7.213173 loss_ctc 7.735316 loss_rnnt 6.608615 hw_loss 0.202303 history loss 6.631587 rank 0
2023-03-01 22:30:00,252 DEBUG CV Batch 56/1600 loss 7.951832 loss_att 12.953903 loss_ctc 8.375340 loss_rnnt 6.858808 hw_loss 0.067767 history loss 6.588471 rank 1
2023-03-01 22:30:01,018 DEBUG CV Batch 56/1600 loss 7.951832 loss_att 12.953903 loss_ctc 8.375340 loss_rnnt 6.858808 hw_loss 0.067767 history loss 6.588471 rank 0
2023-03-01 22:30:09,113 DEBUG CV Batch 56/1700 loss 7.277931 loss_att 6.204847 loss_ctc 14.140714 loss_rnnt 6.445412 hw_loss 0.247685 history loss 6.520236 rank 1
2023-03-01 22:30:09,931 DEBUG CV Batch 56/1700 loss 7.277931 loss_att 6.204847 loss_ctc 14.140714 loss_rnnt 6.445412 hw_loss 0.247685 history loss 6.520236 rank 0
2023-03-01 22:30:15,739 INFO Epoch 56 CV info cv_loss 6.494602964448264
2023-03-01 22:30:15,739 INFO Epoch 57 TRAIN info lr 0.0002538754762041183
2023-03-01 22:30:15,740 INFO using accumulate grad, new batch size is 4 times larger than before
2023-03-01 22:30:16,595 INFO Epoch 56 CV info cv_loss 6.494602965544476
2023-03-01 22:30:16,595 INFO Checkpoint: save to checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune_100head/56.pt
2023-03-01 22:30:17,010 INFO Epoch 57 TRAIN info lr 0.0002538672951123456
2023-03-01 22:30:17,011 INFO using accumulate grad, new batch size is 4 times larger than before
2023-03-01 22:30:50,716 DEBUG TRAIN Batch 57/0 loss 8.080155 loss_att 7.738859 loss_ctc 12.706996 loss_rnnt 7.355361 hw_loss 0.330267 lr 0.00025388 rank 1
2023-03-01 22:30:50,717 DEBUG TRAIN Batch 57/0 loss 5.144414 loss_att 4.895366 loss_ctc 7.378173 loss_rnnt 4.697745 hw_loss 0.372457 lr 0.00025387 rank 0
2023-03-01 22:31:11,639 DEBUG TRAIN Batch 57/100 loss 2.707916 loss_att 5.718328 loss_ctc 3.977202 loss_rnnt 1.789861 hw_loss 0.275127 lr 0.00025387 rank 1
2023-03-01 22:31:11,641 DEBUG TRAIN Batch 57/100 loss 5.944413 loss_att 9.868653 loss_ctc 11.325488 loss_rnnt 4.340803 hw_loss 0.189910 lr 0.00025386 rank 0
2023-03-01 22:31:33,307 DEBUG TRAIN Batch 57/200 loss 6.257798 loss_att 9.990939 loss_ctc 14.109222 loss_rnnt 4.385226 hw_loss 0.148288 lr 0.00025386 rank 1
2023-03-01 22:31:33,309 DEBUG TRAIN Batch 57/200 loss 4.126282 loss_att 9.761765 loss_ctc 5.093687 loss_rnnt 2.808582 hw_loss 0.115531 lr 0.00025385 rank 0
2023-03-01 22:31:54,756 DEBUG TRAIN Batch 57/300 loss 7.464432 loss_att 8.434302 loss_ctc 11.477255 loss_rnnt 6.628414 hw_loss 0.200625 lr 0.00025385 rank 1
2023-03-01 22:31:54,758 DEBUG TRAIN Batch 57/300 loss 8.867066 loss_att 13.022085 loss_ctc 13.243461 loss_rnnt 7.300466 hw_loss 0.285146 lr 0.00025384 rank 0
2023-03-01 22:32:28,493 DEBUG TRAIN Batch 57/400 loss 7.364525 loss_att 9.608395 loss_ctc 12.548360 loss_rnnt 6.143365 hw_loss 0.152264 lr 0.00025383 rank 0
2023-03-01 22:32:28,493 DEBUG TRAIN Batch 57/400 loss 7.788595 loss_att 10.865249 loss_ctc 12.590694 loss_rnnt 6.388383 hw_loss 0.271126 lr 0.00025384 rank 1
2023-03-01 22:32:50,032 DEBUG TRAIN Batch 57/500 loss 7.317679 loss_att 9.414437 loss_ctc 12.933935 loss_rnnt 6.056566 hw_loss 0.174238 lr 0.00025383 rank 0
2023-03-01 22:32:50,033 DEBUG TRAIN Batch 57/500 loss 6.625794 loss_att 10.415021 loss_ctc 13.829699 loss_rnnt 4.817272 hw_loss 0.169042 lr 0.00025383 rank 1
2023-03-01 22:33:12,572 DEBUG TRAIN Batch 57/600 loss 6.541220 loss_att 8.819453 loss_ctc 10.995798 loss_rnnt 5.370513 hw_loss 0.227094 lr 0.00025383 rank 1
2023-03-01 22:33:12,576 DEBUG TRAIN Batch 57/600 loss 4.017108 loss_att 5.535368 loss_ctc 6.253162 loss_rnnt 3.344831 hw_loss 0.132160 lr 0.00025382 rank 0
2023-03-01 22:33:35,045 DEBUG TRAIN Batch 57/700 loss 1.554873 loss_att 3.806982 loss_ctc 2.805653 loss_rnnt 0.805971 hw_loss 0.246955 lr 0.00025382 rank 1
2023-03-01 22:33:35,048 DEBUG TRAIN Batch 57/700 loss 2.234935 loss_att 5.655177 loss_ctc 2.783253 loss_rnnt 1.380486 hw_loss 0.182423 lr 0.00025381 rank 0
2023-03-01 22:34:09,194 DEBUG TRAIN Batch 57/800 loss 14.741920 loss_att 16.370949 loss_ctc 19.109550 loss_rnnt 13.763778 hw_loss 0.131220 lr 0.00025381 rank 1
2023-03-01 22:34:09,196 DEBUG TRAIN Batch 57/800 loss 3.986400 loss_att 5.303644 loss_ctc 3.479993 loss_rnnt 3.623373 hw_loss 0.313311 lr 0.00025380 rank 0
2023-03-01 22:34:31,012 DEBUG TRAIN Batch 57/900 loss 4.210675 loss_att 6.620872 loss_ctc 4.578735 loss_rnnt 3.597188 hw_loss 0.154449 lr 0.00025380 rank 1
2023-03-01 22:34:31,014 DEBUG TRAIN Batch 57/900 loss 7.285902 loss_att 10.413171 loss_ctc 13.821015 loss_rnnt 5.611839 hw_loss 0.332362 lr 0.00025379 rank 0
2023-03-01 22:34:52,563 DEBUG TRAIN Batch 57/1000 loss 4.489478 loss_att 7.029778 loss_ctc 7.433835 loss_rnnt 3.485536 hw_loss 0.193688 lr 0.00025379 rank 1
2023-03-01 22:34:52,566 DEBUG TRAIN Batch 57/1000 loss 4.118906 loss_att 8.843805 loss_ctc 11.179497 loss_rnnt 2.123088 hw_loss 0.205174 lr 0.00025379 rank 0
2023-03-01 22:35:28,185 DEBUG TRAIN Batch 57/1100 loss 7.283967 loss_att 8.879180 loss_ctc 10.222488 loss_rnnt 6.503519 hw_loss 0.130507 lr 0.00025379 rank 1
2023-03-01 22:35:28,187 DEBUG TRAIN Batch 57/1100 loss 14.638287 loss_att 15.937387 loss_ctc 24.036808 loss_rnnt 13.078047 hw_loss 0.088659 lr 0.00025378 rank 0
2023-03-01 22:35:50,702 DEBUG TRAIN Batch 57/1200 loss 7.973347 loss_att 10.889451 loss_ctc 11.300348 loss_rnnt 6.808412 hw_loss 0.258963 lr 0.00025378 rank 1
2023-03-01 22:35:50,703 DEBUG TRAIN Batch 57/1200 loss 9.123812 loss_att 13.486946 loss_ctc 16.255514 loss_rnnt 7.195917 hw_loss 0.195704 lr 0.00025377 rank 0
2023-03-01 22:36:13,073 DEBUG TRAIN Batch 57/1300 loss 3.770093 loss_att 5.734378 loss_ctc 4.808807 loss_rnnt 3.107246 hw_loss 0.246554 lr 0.00025377 rank 1
2023-03-01 22:36:13,074 DEBUG TRAIN Batch 57/1300 loss 3.897663 loss_att 7.520572 loss_ctc 6.057222 loss_rnnt 2.824639 hw_loss 0.113439 lr 0.00025376 rank 0
2023-03-01 22:36:35,369 DEBUG TRAIN Batch 57/1400 loss 3.805615 loss_att 6.708735 loss_ctc 4.559565 loss_rnnt 2.943331 hw_loss 0.339624 lr 0.00025376 rank 1
2023-03-01 22:36:35,372 DEBUG TRAIN Batch 57/1400 loss 6.006972 loss_att 12.285255 loss_ctc 14.004457 loss_rnnt 3.552684 hw_loss 0.248063 lr 0.00025375 rank 0
2023-03-01 22:37:08,842 DEBUG TRAIN Batch 57/1500 loss 3.795076 loss_att 5.516046 loss_ctc 5.196730 loss_rnnt 3.190799 hw_loss 0.137242 lr 0.00025375 rank 1
2023-03-01 22:37:08,845 DEBUG TRAIN Batch 57/1500 loss 10.584667 loss_att 14.766637 loss_ctc 17.792669 loss_rnnt 8.708750 hw_loss 0.147106 lr 0.00025374 rank 0
2023-03-01 22:37:30,374 DEBUG TRAIN Batch 57/1600 loss 7.849755 loss_att 12.514185 loss_ctc 10.245548 loss_rnnt 6.453542 hw_loss 0.269791 lr 0.00025374 rank 1
2023-03-01 22:37:30,375 DEBUG TRAIN Batch 57/1600 loss 6.369676 loss_att 9.176303 loss_ctc 11.612199 loss_rnnt 4.955441 hw_loss 0.288575 lr 0.00025374 rank 0
2023-03-01 22:37:52,128 DEBUG TRAIN Batch 57/1700 loss 3.771350 loss_att 6.418568 loss_ctc 8.152624 loss_rnnt 2.549416 hw_loss 0.203100 lr 0.00025374 rank 1
2023-03-01 22:37:52,128 DEBUG TRAIN Batch 57/1700 loss 3.424072 loss_att 4.959344 loss_ctc 5.235903 loss_rnnt 2.874890 hw_loss 0.001031 lr 0.00025373 rank 0
2023-03-01 22:38:26,115 DEBUG TRAIN Batch 57/1800 loss 7.032558 loss_att 9.980334 loss_ctc 11.344459 loss_rnnt 5.729224 hw_loss 0.260360 lr 0.00025373 rank 1
2023-03-01 22:38:26,116 DEBUG TRAIN Batch 57/1800 loss 9.404133 loss_att 11.470539 loss_ctc 12.874172 loss_rnnt 8.424906 hw_loss 0.193638 lr 0.00025372 rank 0
2023-03-01 22:38:48,715 DEBUG TRAIN Batch 57/1900 loss 6.030293 loss_att 9.491237 loss_ctc 10.182029 loss_rnnt 4.647306 hw_loss 0.257312 lr 0.00025372 rank 1
2023-03-01 22:38:48,716 DEBUG TRAIN Batch 57/1900 loss 5.927816 loss_att 8.629766 loss_ctc 8.224695 loss_rnnt 4.974653 hw_loss 0.199730 lr 0.00025371 rank 0
2023-03-01 22:39:11,255 DEBUG TRAIN Batch 57/2000 loss 9.086342 loss_att 12.262687 loss_ctc 14.670065 loss_rnnt 7.636024 hw_loss 0.132286 lr 0.00025371 rank 1
2023-03-01 22:39:11,257 DEBUG TRAIN Batch 57/2000 loss 9.221164 loss_att 11.829310 loss_ctc 11.966276 loss_rnnt 8.304379 hw_loss 0.054639 lr 0.00025370 rank 0
2023-03-01 22:39:33,620 DEBUG TRAIN Batch 57/2100 loss 7.914031 loss_att 9.648407 loss_ctc 17.914204 loss_rnnt 6.100433 hw_loss 0.250060 lr 0.00025370 rank 1
2023-03-01 22:39:33,621 DEBUG TRAIN Batch 57/2100 loss 3.740013 loss_att 7.656637 loss_ctc 7.430404 loss_rnnt 2.425625 hw_loss 0.073146 lr 0.00025370 rank 0
2023-03-01 22:40:08,320 DEBUG TRAIN Batch 57/2200 loss 12.883965 loss_att 16.020645 loss_ctc 21.934689 loss_rnnt 10.972557 hw_loss 0.144954 lr 0.00025370 rank 1
2023-03-01 22:40:08,324 DEBUG TRAIN Batch 57/2200 loss 4.621363 loss_att 6.682458 loss_ctc 7.589989 loss_rnnt 3.640088 hw_loss 0.324823 lr 0.00025369 rank 0
2023-03-01 22:40:30,921 DEBUG TRAIN Batch 57/2300 loss 12.532582 loss_att 14.551062 loss_ctc 17.057526 loss_rnnt 11.411174 hw_loss 0.214477 lr 0.00025369 rank 1
2023-03-01 22:40:30,923 DEBUG TRAIN Batch 57/2300 loss 4.464771 loss_att 8.218417 loss_ctc 9.940257 loss_rnnt 2.880477 hw_loss 0.194063 lr 0.00025368 rank 0
2023-03-01 22:40:53,440 DEBUG TRAIN Batch 57/2400 loss 6.629215 loss_att 7.799250 loss_ctc 8.339481 loss_rnnt 6.012641 hw_loss 0.289747 lr 0.00025368 rank 1
2023-03-01 22:40:53,443 DEBUG TRAIN Batch 57/2400 loss 7.618443 loss_att 11.998894 loss_ctc 12.840142 loss_rnnt 5.946208 hw_loss 0.187348 lr 0.00025367 rank 0
2023-03-01 22:41:27,465 DEBUG TRAIN Batch 57/2500 loss 3.998270 loss_att 5.714491 loss_ctc 8.744583 loss_rnnt 2.872834 hw_loss 0.280031 lr 0.00025367 rank 1
2023-03-01 22:41:27,468 DEBUG TRAIN Batch 57/2500 loss 14.658074 loss_att 18.719833 loss_ctc 26.197983 loss_rnnt 12.218332 hw_loss 0.166377 lr 0.00025366 rank 0
2023-03-01 22:41:50,163 DEBUG TRAIN Batch 57/2600 loss 3.878061 loss_att 5.078197 loss_ctc 6.892421 loss_rnnt 3.176454 hw_loss 0.111873 lr 0.00025366 rank 1
2023-03-01 22:41:50,164 DEBUG TRAIN Batch 57/2600 loss 4.369254 loss_att 5.923242 loss_ctc 6.779030 loss_rnnt 3.560440 hw_loss 0.331337 lr 0.00025365 rank 0
2023-03-01 22:42:12,182 DEBUG TRAIN Batch 57/2700 loss 2.924612 loss_att 5.506157 loss_ctc 6.252378 loss_rnnt 1.964474 hw_loss 0.000237 lr 0.00025365 rank 0
2023-03-01 22:42:12,182 DEBUG TRAIN Batch 57/2700 loss 5.197204 loss_att 9.127469 loss_ctc 9.902149 loss_rnnt 3.698843 hw_loss 0.159343 lr 0.00025365 rank 1
2023-03-01 22:42:34,687 DEBUG TRAIN Batch 57/2800 loss 6.753268 loss_att 9.131536 loss_ctc 13.339797 loss_rnnt 5.325641 hw_loss 0.138317 lr 0.00025365 rank 1
2023-03-01 22:42:34,689 DEBUG TRAIN Batch 57/2800 loss 4.158022 loss_att 7.114458 loss_ctc 6.603053 loss_rnnt 3.155096 hw_loss 0.160567 lr 0.00025364 rank 0
2023-03-01 22:43:09,510 DEBUG TRAIN Batch 57/2900 loss 5.482833 loss_att 7.663867 loss_ctc 8.718191 loss_rnnt 4.492353 hw_loss 0.230423 lr 0.00025364 rank 1
2023-03-01 22:43:09,513 DEBUG TRAIN Batch 57/2900 loss 11.869700 loss_att 14.221525 loss_ctc 16.278723 loss_rnnt 10.722471 hw_loss 0.166866 lr 0.00025363 rank 0
2023-03-01 22:43:31,131 DEBUG TRAIN Batch 57/3000 loss 7.969287 loss_att 13.622497 loss_ctc 12.378243 loss_rnnt 6.135043 hw_loss 0.217016 lr 0.00025363 rank 1
2023-03-01 22:43:31,131 DEBUG TRAIN Batch 57/3000 loss 3.692691 loss_att 6.217332 loss_ctc 6.682446 loss_rnnt 2.661921 hw_loss 0.238513 lr 0.00025362 rank 0
2023-03-01 22:43:52,639 DEBUG TRAIN Batch 57/3100 loss 3.262402 loss_att 5.404948 loss_ctc 6.056447 loss_rnnt 2.374750 hw_loss 0.162382 lr 0.00025362 rank 1
2023-03-01 22:43:52,642 DEBUG TRAIN Batch 57/3100 loss 7.149772 loss_att 9.526220 loss_ctc 10.346182 loss_rnnt 6.106784 hw_loss 0.265331 lr 0.00025361 rank 0
2023-03-01 22:44:26,412 DEBUG TRAIN Batch 57/3200 loss 9.443769 loss_att 10.603418 loss_ctc 17.918783 loss_rnnt 7.972373 hw_loss 0.205247 lr 0.00025361 rank 1
2023-03-01 22:44:26,422 DEBUG TRAIN Batch 57/3200 loss 4.266551 loss_att 4.543286 loss_ctc 7.226167 loss_rnnt 3.651629 hw_loss 0.309300 lr 0.00025361 rank 0
2023-03-01 22:44:47,694 DEBUG TRAIN Batch 57/3300 loss 10.939646 loss_att 11.193907 loss_ctc 14.864937 loss_rnnt 10.329491 hw_loss 0.067369 lr 0.00025361 rank 1
2023-03-01 22:44:47,694 DEBUG TRAIN Batch 57/3300 loss 2.503502 loss_att 6.926378 loss_ctc 6.227421 loss_rnnt 1.038649 hw_loss 0.157040 lr 0.00025360 rank 0
2023-03-01 22:45:09,516 DEBUG TRAIN Batch 57/3400 loss 9.769600 loss_att 12.575535 loss_ctc 15.484214 loss_rnnt 8.325466 hw_loss 0.226871 lr 0.00025360 rank 1
2023-03-01 22:45:09,518 DEBUG TRAIN Batch 57/3400 loss 9.954095 loss_att 13.848692 loss_ctc 15.496696 loss_rnnt 8.332994 hw_loss 0.193439 lr 0.00025359 rank 0
2023-03-01 22:45:31,380 DEBUG TRAIN Batch 57/3500 loss 5.442020 loss_att 7.855373 loss_ctc 6.209152 loss_rnnt 4.737756 hw_loss 0.223706 lr 0.00025359 rank 1
2023-03-01 22:45:31,381 DEBUG TRAIN Batch 57/3500 loss 5.641547 loss_att 7.135341 loss_ctc 10.644918 loss_rnnt 4.628128 hw_loss 0.089145 lr 0.00025358 rank 0
2023-03-01 22:46:03,908 DEBUG TRAIN Batch 57/3600 loss 4.446563 loss_att 7.299572 loss_ctc 6.659348 loss_rnnt 3.397042 hw_loss 0.344776 lr 0.00025358 rank 1
2023-03-01 22:46:03,911 DEBUG TRAIN Batch 57/3600 loss 7.970232 loss_att 10.229288 loss_ctc 12.742673 loss_rnnt 6.773490 hw_loss 0.203635 lr 0.00025357 rank 0
2023-03-01 22:46:25,535 DEBUG TRAIN Batch 57/3700 loss 4.209165 loss_att 7.552479 loss_ctc 7.682654 loss_rnnt 2.959861 hw_loss 0.220331 lr 0.00025357 rank 1
2023-03-01 22:46:25,537 DEBUG TRAIN Batch 57/3700 loss 5.117765 loss_att 8.099014 loss_ctc 8.039380 loss_rnnt 4.007385 hw_loss 0.233590 lr 0.00025356 rank 0
2023-03-01 22:46:47,318 DEBUG TRAIN Batch 57/3800 loss 6.650901 loss_att 7.089192 loss_ctc 9.295972 loss_rnnt 6.077524 hw_loss 0.249455 lr 0.00025356 rank 1
2023-03-01 22:46:47,320 DEBUG TRAIN Batch 57/3800 loss 4.821412 loss_att 6.624623 loss_ctc 8.529894 loss_rnnt 3.776045 hw_loss 0.356740 lr 0.00025356 rank 0
2023-03-01 22:47:08,716 DEBUG TRAIN Batch 57/3900 loss 5.639589 loss_att 7.412726 loss_ctc 8.467105 loss_rnnt 4.840369 hw_loss 0.126732 lr 0.00025356 rank 1
2023-03-01 22:47:08,718 DEBUG TRAIN Batch 57/3900 loss 11.169842 loss_att 11.909172 loss_ctc 12.502129 loss_rnnt 10.724707 hw_loss 0.224309 lr 0.00025355 rank 0
2023-03-01 22:47:42,015 DEBUG TRAIN Batch 57/4000 loss 4.366838 loss_att 7.329021 loss_ctc 6.239957 loss_rnnt 3.436017 hw_loss 0.166191 lr 0.00025355 rank 1
2023-03-01 22:47:42,017 DEBUG TRAIN Batch 57/4000 loss 11.379097 loss_att 12.514629 loss_ctc 16.112228 loss_rnnt 10.404314 hw_loss 0.218611 lr 0.00025354 rank 0
2023-03-01 22:48:03,504 DEBUG TRAIN Batch 57/4100 loss 4.297377 loss_att 8.133629 loss_ctc 9.557989 loss_rnnt 2.730153 hw_loss 0.184798 lr 0.00025354 rank 1
2023-03-01 22:48:03,506 DEBUG TRAIN Batch 57/4100 loss 1.420940 loss_att 3.820470 loss_ctc 3.360216 loss_rnnt 0.566497 hw_loss 0.217438 lr 0.00025353 rank 0
2023-03-01 22:48:25,004 DEBUG TRAIN Batch 57/4200 loss 5.594571 loss_att 9.795366 loss_ctc 6.733359 loss_rnnt 4.527955 hw_loss 0.139910 lr 0.00025353 rank 1
2023-03-01 22:48:25,005 DEBUG TRAIN Batch 57/4200 loss 2.401798 loss_att 4.290143 loss_ctc 3.548222 loss_rnnt 1.807884 hw_loss 0.118853 lr 0.00025352 rank 0
2023-03-01 22:48:59,716 DEBUG TRAIN Batch 57/4300 loss 6.075476 loss_att 8.127493 loss_ctc 11.961934 loss_rnnt 4.773952 hw_loss 0.199236 lr 0.00025352 rank 1
2023-03-01 22:48:59,718 DEBUG TRAIN Batch 57/4300 loss 8.027621 loss_att 8.172181 loss_ctc 11.144429 loss_rnnt 7.536427 hw_loss 0.087579 lr 0.00025352 rank 0
2023-03-01 22:49:22,180 DEBUG TRAIN Batch 57/4400 loss 6.922835 loss_att 10.203687 loss_ctc 13.044154 loss_rnnt 5.360482 hw_loss 0.168761 lr 0.00025351 rank 0
2023-03-01 22:49:22,181 DEBUG TRAIN Batch 57/4400 loss 3.217308 loss_att 5.209519 loss_ctc 4.402477 loss_rnnt 2.529474 hw_loss 0.246317 lr 0.00025352 rank 1
2023-03-01 22:49:44,420 DEBUG TRAIN Batch 57/4500 loss 9.510939 loss_att 15.197132 loss_ctc 17.954626 loss_rnnt 7.100118 hw_loss 0.277045 lr 0.00025351 rank 1
2023-03-01 22:49:44,421 DEBUG TRAIN Batch 57/4500 loss 7.799052 loss_att 8.461786 loss_ctc 11.081849 loss_rnnt 7.082796 hw_loss 0.273757 lr 0.00025350 rank 0
2023-03-01 22:50:06,119 DEBUG TRAIN Batch 57/4600 loss 2.052710 loss_att 5.032823 loss_ctc 5.144125 loss_rnnt 0.956705 hw_loss 0.164612 lr 0.00025350 rank 1
2023-03-01 22:50:06,121 DEBUG TRAIN Batch 57/4600 loss 10.959722 loss_att 11.616688 loss_ctc 13.806475 loss_rnnt 10.343370 hw_loss 0.197609 lr 0.00025349 rank 0
2023-03-01 22:50:41,158 DEBUG TRAIN Batch 57/4700 loss 7.276663 loss_att 11.990673 loss_ctc 14.221451 loss_rnnt 5.315782 hw_loss 0.172702 lr 0.00025349 rank 1
2023-03-01 22:50:41,160 DEBUG TRAIN Batch 57/4700 loss 4.901875 loss_att 7.854660 loss_ctc 6.967200 loss_rnnt 4.004713 hw_loss 0.058552 lr 0.00025348 rank 0
2023-03-01 22:51:02,853 DEBUG TRAIN Batch 57/4800 loss 4.943875 loss_att 6.749329 loss_ctc 6.860843 loss_rnnt 4.271382 hw_loss 0.104636 lr 0.00025348 rank 1
2023-03-01 22:51:02,853 DEBUG TRAIN Batch 57/4800 loss 9.400032 loss_att 12.283787 loss_ctc 15.299922 loss_rnnt 7.862950 hw_loss 0.325649 lr 0.00025348 rank 0
2023-03-01 22:51:24,547 DEBUG TRAIN Batch 57/4900 loss 12.284822 loss_att 13.834167 loss_ctc 17.274708 loss_rnnt 11.209953 hw_loss 0.186904 lr 0.00025348 rank 1
2023-03-01 22:51:24,549 DEBUG TRAIN Batch 57/4900 loss 10.511666 loss_att 16.218273 loss_ctc 18.563833 loss_rnnt 8.195283 hw_loss 0.190199 lr 0.00025347 rank 0
2023-03-01 22:51:59,160 DEBUG TRAIN Batch 57/5000 loss 7.410359 loss_att 9.251650 loss_ctc 11.479209 loss_rnnt 6.411362 hw_loss 0.165423 lr 0.00025347 rank 1
2023-03-01 22:51:59,160 DEBUG TRAIN Batch 57/5000 loss 5.207523 loss_att 7.617034 loss_ctc 9.028051 loss_rnnt 4.093933 hw_loss 0.229283 lr 0.00025346 rank 0
2023-03-01 22:52:21,567 DEBUG TRAIN Batch 57/5100 loss 10.069554 loss_att 13.302335 loss_ctc 18.299797 loss_rnnt 8.275328 hw_loss 0.094323 lr 0.00025346 rank 1
2023-03-01 22:52:21,569 DEBUG TRAIN Batch 57/5100 loss 6.127517 loss_att 7.193486 loss_ctc 13.132281 loss_rnnt 4.811204 hw_loss 0.317159 lr 0.00025345 rank 0
2023-03-01 22:52:43,412 DEBUG TRAIN Batch 57/5200 loss 4.956054 loss_att 6.333385 loss_ctc 6.482042 loss_rnnt 4.333629 hw_loss 0.269050 lr 0.00025345 rank 1
2023-03-01 22:52:43,414 DEBUG TRAIN Batch 57/5200 loss 7.069195 loss_att 13.826923 loss_ctc 15.729869 loss_rnnt 4.448283 hw_loss 0.214894 lr 0.00025344 rank 0
2023-03-01 22:53:05,843 DEBUG TRAIN Batch 57/5300 loss 2.860134 loss_att 4.539874 loss_ctc 9.946800 loss_rnnt 1.449994 hw_loss 0.242443 lr 0.00025344 rank 1
2023-03-01 22:53:05,844 DEBUG TRAIN Batch 57/5300 loss 3.690332 loss_att 5.419713 loss_ctc 5.759895 loss_rnnt 2.890306 hw_loss 0.334141 lr 0.00025343 rank 0
