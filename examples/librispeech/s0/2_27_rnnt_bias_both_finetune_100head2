/home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000
dictionary: /home/work_nfs5_ssd/kxhuang/wenet-encoder_decoder_bias/examples/librispeech/s0/data/lang_char/train_960_unigram5000_units.txt
run_2_27_rnnt_bias_both_finetune_100head.sh: init method is file:///home/work_nfs6/tyxu/workspace/wenet-bias-celoss/examples/librispeech/s0/exp/2_27_rnnt_bias_loss_2_class_both_finetune_100head/ddp_init
2023-03-01 17:37:47,501 INFO training on multiple gpus, this gpu 1
2023-03-01 17:37:47,505 INFO training on multiple gpus, this gpu 0
2023-03-01 17:37:47,526 INFO Added key: store_based_barrier_key:1 to store for rank: 1
2023-03-01 17:37:47,527 INFO Added key: store_based_barrier_key:1 to store for rank: 0
2023-03-01 17:37:47,527 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2023-03-01 17:37:47,528 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2023-03-01 17:37:48,990 INFO Checkpoint: loading from checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune_100head/54.pt for GPU
2023-03-01 17:37:50,262 INFO Checkpoint: loading from checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune_100head/54.pt for GPU
2023-03-01 17:37:52,039 INFO Epoch 55 TRAIN info lr 4e-08
2023-03-01 17:37:52,040 INFO using accumulate grad, new batch size is 4 times larger than before
2023-03-01 17:37:52,041 INFO Epoch 55 TRAIN info lr 4e-08
2023-03-01 17:37:52,041 INFO using accumulate grad, new batch size is 4 times larger than before
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
Transducer(
  (encoder): ConformerEncoder(
    (global_cmvn): GlobalCMVN()
    (embed): Conv2dSubsampling4(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
      )
      (pos_enc): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoders): ModuleList(
      (0): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (1): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (2): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (3): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (4): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (5): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (6): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (7): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (8): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (9): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (10): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
      (11): ConformerEncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (activation): SiLU()
          (dropout): Dropout(p=0.1, inplace=False)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): SiLU()
        )
        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (concat_linear): Identity()
      )
    )
  )
  (decoder): BiTransformerDecoder(
    (left_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
    (right_decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5002, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5002, bias=True)
      (decoders): ModuleList(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (activation): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (concat_linear1): Identity()
          (concat_linear2): Identity()
        )
      )
    )
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
  (context_bias): ContextBias(
    (context_extractor): BLSTM(
      (word_embedding): Embedding(5002, 256)
      (sen_rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
    )
    (context_encoder): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (predictor_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=256, out_features=256, bias=True)
      (linear_k): Linear(in_features=256, out_features=256, bias=True)
      (linear_v): Linear(in_features=256, out_features=256, bias=True)
      (linear_out): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (hw_bias): MultiHeadedAttention(
      (linear_q): Linear(in_features=100, out_features=100, bias=True)
      (linear_k): Linear(in_features=100, out_features=100, bias=True)
      (linear_v): Linear(in_features=100, out_features=100, bias=True)
      (linear_out): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encoder_ffn): Linear(in_features=512, out_features=256, bias=True)
    (encoder_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (encdoer_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (encdoer_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_combine): Linear(in_features=512, out_features=256, bias=True)
    (predictor_bias_bias_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (predictor_bias_out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (hw_bias_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (hw_output_layer): Linear(in_features=100, out_features=2, bias=True)
    (hw_output_layer_enc): Linear(in_features=256, out_features=100, bias=True)
    (hw_output_layer_dec): Linear(in_features=256, out_features=100, bias=True)
  )
  (predictor): RNNPredictor(
    (embed): Embedding(5002, 256)
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.1)
    (projection): Linear(in_features=256, out_features=256, bias=True)
  )
  (joint): TransducerJoint(
    (activatoin): Tanh()
    (enc_ffn): Linear(in_features=256, out_features=512, bias=True)
    (pred_ffn): Linear(in_features=256, out_features=512, bias=True)
    (ffn_out): Linear(in_features=512, out_features=5002, bias=True)
  )
  (hw_criterion): CrossEntropyLoss()
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
the number of model params: 58195794
2023-03-01 17:38:24,693 DEBUG TRAIN Batch 55/0 loss 6.212205 loss_att 6.842720 loss_ctc 10.076370 loss_rnnt 5.408747 hw_loss 0.303998 lr 0.00025951 rank 0
2023-03-01 17:38:24,700 DEBUG TRAIN Batch 55/0 loss 8.384593 loss_att 8.171894 loss_ctc 10.463362 loss_rnnt 7.961700 hw_loss 0.352993 lr 0.00025951 rank 1
2023-03-01 17:38:46,490 DEBUG TRAIN Batch 55/100 loss 5.812404 loss_att 6.946801 loss_ctc 7.852709 loss_rnnt 5.205415 hw_loss 0.202630 lr 0.00025950 rank 0
2023-03-01 17:38:46,493 DEBUG TRAIN Batch 55/100 loss 4.470262 loss_att 8.175776 loss_ctc 6.492737 loss_rnnt 3.322713 hw_loss 0.256469 lr 0.00025950 rank 1
2023-03-01 17:39:07,944 DEBUG TRAIN Batch 55/200 loss 3.245102 loss_att 5.124983 loss_ctc 3.762371 loss_rnnt 2.688677 hw_loss 0.209022 lr 0.00025949 rank 0
2023-03-01 17:39:07,947 DEBUG TRAIN Batch 55/200 loss 2.981566 loss_att 6.974463 loss_ctc 11.386478 loss_rnnt 0.957888 hw_loss 0.195833 lr 0.00025949 rank 1
2023-03-01 17:39:30,879 DEBUG TRAIN Batch 55/300 loss 3.103263 loss_att 4.993778 loss_ctc 6.095925 loss_rnnt 2.240492 hw_loss 0.160587 lr 0.00025948 rank 1
2023-03-01 17:39:30,881 DEBUG TRAIN Batch 55/300 loss 5.744346 loss_att 8.735968 loss_ctc 10.021667 loss_rnnt 4.459849 hw_loss 0.217243 lr 0.00025948 rank 0
2023-03-01 17:40:02,400 DEBUG TRAIN Batch 55/400 loss 6.774378 loss_att 10.653656 loss_ctc 10.360336 loss_rnnt 5.392590 hw_loss 0.239636 lr 0.00025948 rank 1
2023-03-01 17:40:02,401 DEBUG TRAIN Batch 55/400 loss 4.521272 loss_att 7.561553 loss_ctc 8.083326 loss_rnnt 3.341702 hw_loss 0.181074 lr 0.00025948 rank 0
2023-03-01 17:40:25,362 DEBUG TRAIN Batch 55/500 loss 4.314774 loss_att 6.629830 loss_ctc 6.165016 loss_rnnt 3.500746 hw_loss 0.195596 lr 0.00025947 rank 1
2023-03-01 17:40:25,364 DEBUG TRAIN Batch 55/500 loss 7.614709 loss_att 8.837889 loss_ctc 15.935946 loss_rnnt 6.103811 hw_loss 0.293932 lr 0.00025947 rank 0
2023-03-01 17:40:48,582 DEBUG TRAIN Batch 55/600 loss 4.834550 loss_att 7.929031 loss_ctc 8.412224 loss_rnnt 3.672328 hw_loss 0.124319 lr 0.00025946 rank 0
2023-03-01 17:40:48,583 DEBUG TRAIN Batch 55/600 loss 6.044159 loss_att 5.875196 loss_ctc 9.013199 loss_rnnt 5.536152 hw_loss 0.273613 lr 0.00025946 rank 1
2023-03-01 17:41:10,663 DEBUG TRAIN Batch 55/700 loss 4.356284 loss_att 6.959218 loss_ctc 3.819519 loss_rnnt 3.773583 hw_loss 0.250656 lr 0.00025945 rank 1
2023-03-01 17:41:10,667 DEBUG TRAIN Batch 55/700 loss 2.032729 loss_att 4.401347 loss_ctc 2.785789 loss_rnnt 1.347260 hw_loss 0.208758 lr 0.00025945 rank 0
2023-03-01 17:41:44,416 DEBUG TRAIN Batch 55/800 loss 4.676372 loss_att 7.534342 loss_ctc 6.371336 loss_rnnt 3.851847 hw_loss 0.050503 lr 0.00025944 rank 1
2023-03-01 17:41:44,419 DEBUG TRAIN Batch 55/800 loss 6.273899 loss_att 9.310640 loss_ctc 8.140386 loss_rnnt 5.379845 hw_loss 0.070952 lr 0.00025944 rank 0
2023-03-01 17:42:06,593 DEBUG TRAIN Batch 55/900 loss 11.221375 loss_att 13.213739 loss_ctc 14.390606 loss_rnnt 10.320246 hw_loss 0.150174 lr 0.00025943 rank 1
2023-03-01 17:42:06,595 DEBUG TRAIN Batch 55/900 loss 7.323185 loss_att 8.912275 loss_ctc 10.343143 loss_rnnt 6.424500 hw_loss 0.334136 lr 0.00025943 rank 0
2023-03-01 17:42:28,172 DEBUG TRAIN Batch 55/1000 loss 7.388011 loss_att 11.443130 loss_ctc 12.129202 loss_rnnt 5.803877 hw_loss 0.264283 lr 0.00025942 rank 1
2023-03-01 17:42:28,175 DEBUG TRAIN Batch 55/1000 loss 6.716238 loss_att 9.997075 loss_ctc 11.424707 loss_rnnt 5.323205 hw_loss 0.204508 lr 0.00025942 rank 0
2023-03-01 17:43:00,723 DEBUG TRAIN Batch 55/1100 loss 5.722415 loss_att 8.018153 loss_ctc 8.806152 loss_rnnt 4.747074 hw_loss 0.196928 lr 0.00025942 rank 1
2023-03-01 17:43:00,726 DEBUG TRAIN Batch 55/1100 loss 4.518573 loss_att 6.396924 loss_ctc 5.907338 loss_rnnt 3.892648 hw_loss 0.122037 lr 0.00025942 rank 0
2023-03-01 17:43:22,234 DEBUG TRAIN Batch 55/1200 loss 7.469284 loss_att 12.082119 loss_ctc 17.086462 loss_rnnt 5.098699 hw_loss 0.310739 lr 0.00025941 rank 1
2023-03-01 17:43:22,236 DEBUG TRAIN Batch 55/1200 loss 8.816730 loss_att 11.802216 loss_ctc 14.931213 loss_rnnt 7.307578 hw_loss 0.181481 lr 0.00025941 rank 0
2023-03-01 17:43:44,607 DEBUG TRAIN Batch 55/1300 loss 5.145538 loss_att 5.450588 loss_ctc 8.367141 loss_rnnt 4.474247 hw_loss 0.338876 lr 0.00025940 rank 1
2023-03-01 17:43:44,608 DEBUG TRAIN Batch 55/1300 loss 5.352703 loss_att 9.918169 loss_ctc 9.825202 loss_rnnt 3.688927 hw_loss 0.289405 lr 0.00025940 rank 0
2023-03-01 17:44:06,112 DEBUG TRAIN Batch 55/1400 loss 8.420770 loss_att 11.885680 loss_ctc 16.477743 loss_rnnt 6.549530 hw_loss 0.194991 lr 0.00025939 rank 1
2023-03-01 17:44:06,113 DEBUG TRAIN Batch 55/1400 loss 2.436862 loss_att 4.468784 loss_ctc 5.390778 loss_rnnt 1.501889 hw_loss 0.252625 lr 0.00025939 rank 0
2023-03-01 17:44:39,723 DEBUG TRAIN Batch 55/1500 loss 4.528359 loss_att 7.365290 loss_ctc 6.810379 loss_rnnt 3.577620 hw_loss 0.148284 lr 0.00025938 rank 1
2023-03-01 17:44:39,726 DEBUG TRAIN Batch 55/1500 loss 5.167469 loss_att 7.759507 loss_ctc 8.442057 loss_rnnt 4.055877 hw_loss 0.293573 lr 0.00025938 rank 0
2023-03-01 17:45:01,341 DEBUG TRAIN Batch 55/1600 loss 5.431140 loss_att 7.655508 loss_ctc 9.297417 loss_rnnt 4.385550 hw_loss 0.159774 lr 0.00025937 rank 1
2023-03-01 17:45:01,342 DEBUG TRAIN Batch 55/1600 loss 7.327059 loss_att 11.364907 loss_ctc 16.896976 loss_rnnt 5.136980 hw_loss 0.199725 lr 0.00025937 rank 0
2023-03-01 17:45:22,730 DEBUG TRAIN Batch 55/1700 loss 9.358573 loss_att 10.608085 loss_ctc 16.383766 loss_rnnt 8.042700 hw_loss 0.242399 lr 0.00025936 rank 1
2023-03-01 17:45:22,732 DEBUG TRAIN Batch 55/1700 loss 8.816200 loss_att 11.256786 loss_ctc 18.372822 loss_rnnt 6.970718 hw_loss 0.155901 lr 0.00025936 rank 0
2023-03-01 17:45:55,551 DEBUG TRAIN Batch 55/1800 loss 5.284464 loss_att 7.705381 loss_ctc 11.444654 loss_rnnt 3.828915 hw_loss 0.281262 lr 0.00025935 rank 1
2023-03-01 17:45:55,553 DEBUG TRAIN Batch 55/1800 loss 7.671941 loss_att 8.044310 loss_ctc 13.002695 loss_rnnt 6.741429 hw_loss 0.272382 lr 0.00025935 rank 0
2023-03-01 17:46:16,995 DEBUG TRAIN Batch 55/1900 loss 7.827276 loss_att 8.140948 loss_ctc 13.000092 loss_rnnt 6.897153 hw_loss 0.333150 lr 0.00025935 rank 1
2023-03-01 17:46:16,999 DEBUG TRAIN Batch 55/1900 loss 4.930164 loss_att 6.796989 loss_ctc 9.570494 loss_rnnt 3.753394 hw_loss 0.346302 lr 0.00025935 rank 0
2023-03-01 17:46:38,024 DEBUG TRAIN Batch 55/2000 loss 3.444944 loss_att 5.541266 loss_ctc 8.325243 loss_rnnt 2.297107 hw_loss 0.146000 lr 0.00025934 rank 1
2023-03-01 17:46:38,028 DEBUG TRAIN Batch 55/2000 loss 5.363532 loss_att 8.325146 loss_ctc 7.837314 loss_rnnt 4.288954 hw_loss 0.285782 lr 0.00025934 rank 0
2023-03-01 17:46:59,433 DEBUG TRAIN Batch 55/2100 loss 10.735136 loss_att 13.555489 loss_ctc 15.134048 loss_rnnt 9.558680 hw_loss 0.048496 lr 0.00025933 rank 1
2023-03-01 17:46:59,435 DEBUG TRAIN Batch 55/2100 loss 3.728224 loss_att 8.176961 loss_ctc 7.356441 loss_rnnt 2.255606 hw_loss 0.185829 lr 0.00025933 rank 0
2023-03-01 17:47:31,450 DEBUG TRAIN Batch 55/2200 loss 7.827856 loss_att 12.699447 loss_ctc 13.497395 loss_rnnt 5.980460 hw_loss 0.219636 lr 0.00025932 rank 1
2023-03-01 17:47:31,453 DEBUG TRAIN Batch 55/2200 loss 3.125734 loss_att 9.153063 loss_ctc 8.260836 loss_rnnt 1.165914 hw_loss 0.130639 lr 0.00025932 rank 0
2023-03-01 17:47:52,181 DEBUG TRAIN Batch 55/2300 loss 4.911443 loss_att 7.775300 loss_ctc 7.701227 loss_rnnt 3.903445 hw_loss 0.118603 lr 0.00025931 rank 1
2023-03-01 17:47:52,183 DEBUG TRAIN Batch 55/2300 loss 1.207319 loss_att 4.085908 loss_ctc 2.667586 loss_rnnt 0.299231 hw_loss 0.258127 lr 0.00025931 rank 0
2023-03-01 17:48:13,818 DEBUG TRAIN Batch 55/2400 loss 8.893119 loss_att 10.109614 loss_ctc 11.819190 loss_rnnt 8.146200 hw_loss 0.212769 lr 0.00025930 rank 1
2023-03-01 17:48:13,821 DEBUG TRAIN Batch 55/2400 loss 9.363497 loss_att 13.427255 loss_ctc 16.840069 loss_rnnt 7.505698 hw_loss 0.090321 lr 0.00025930 rank 0
2023-03-01 17:48:47,583 DEBUG TRAIN Batch 55/2500 loss 7.055486 loss_att 9.401140 loss_ctc 10.673125 loss_rnnt 5.975326 hw_loss 0.241269 lr 0.00025929 rank 1
2023-03-01 17:48:47,584 DEBUG TRAIN Batch 55/2500 loss 4.775603 loss_att 7.278191 loss_ctc 9.179422 loss_rnnt 3.527026 hw_loss 0.301657 lr 0.00025929 rank 0
2023-03-01 17:49:09,557 DEBUG TRAIN Batch 55/2600 loss 4.422149 loss_att 6.370416 loss_ctc 6.961500 loss_rnnt 3.623706 hw_loss 0.131643 lr 0.00025928 rank 1
2023-03-01 17:49:09,559 DEBUG TRAIN Batch 55/2600 loss 3.548233 loss_att 7.077713 loss_ctc 5.943927 loss_rnnt 2.432423 hw_loss 0.169665 lr 0.00025928 rank 0
2023-03-01 17:49:30,658 DEBUG TRAIN Batch 55/2700 loss 3.933278 loss_att 7.589875 loss_ctc 8.529360 loss_rnnt 2.566188 hw_loss 0.043049 lr 0.00025928 rank 1
2023-03-01 17:49:30,659 DEBUG TRAIN Batch 55/2700 loss 4.266745 loss_att 7.673340 loss_ctc 9.618042 loss_rnnt 2.782975 hw_loss 0.166771 lr 0.00025928 rank 0
2023-03-01 17:49:52,153 DEBUG TRAIN Batch 55/2800 loss 12.654605 loss_att 14.643991 loss_ctc 16.716660 loss_rnnt 11.583602 hw_loss 0.246597 lr 0.00025927 rank 1
2023-03-01 17:49:52,154 DEBUG TRAIN Batch 55/2800 loss 8.091211 loss_att 11.985939 loss_ctc 19.472679 loss_rnnt 5.731270 hw_loss 0.119000 lr 0.00025927 rank 0
2023-03-01 17:50:24,350 DEBUG TRAIN Batch 55/2900 loss 7.737895 loss_att 10.448298 loss_ctc 8.657946 loss_rnnt 6.958070 hw_loss 0.215757 lr 0.00025926 rank 0
2023-03-01 17:50:24,350 DEBUG TRAIN Batch 55/2900 loss 7.206246 loss_att 9.725230 loss_ctc 13.243183 loss_rnnt 5.798057 hw_loss 0.186503 lr 0.00025926 rank 1
2023-03-01 17:50:45,526 DEBUG TRAIN Batch 55/3000 loss 4.032393 loss_att 6.257899 loss_ctc 7.888902 loss_rnnt 2.934535 hw_loss 0.259792 lr 0.00025925 rank 1
2023-03-01 17:50:45,528 DEBUG TRAIN Batch 55/3000 loss 6.083596 loss_att 11.769117 loss_ctc 13.425266 loss_rnnt 3.897478 hw_loss 0.131482 lr 0.00025925 rank 0
2023-03-01 17:51:07,373 DEBUG TRAIN Batch 55/3100 loss 12.627836 loss_att 12.739618 loss_ctc 12.902305 loss_rnnt 12.517062 hw_loss 0.097165 lr 0.00025924 rank 1
2023-03-01 17:51:07,375 DEBUG TRAIN Batch 55/3100 loss 4.898942 loss_att 7.547633 loss_ctc 10.121440 loss_rnnt 3.585097 hw_loss 0.164576 lr 0.00025924 rank 0
2023-03-01 17:51:43,020 DEBUG TRAIN Batch 55/3200 loss 11.860765 loss_att 14.835329 loss_ctc 19.722918 loss_rnnt 10.098092 hw_loss 0.224013 lr 0.00025923 rank 0
2023-03-01 17:51:43,024 DEBUG TRAIN Batch 55/3200 loss 6.292511 loss_att 7.408256 loss_ctc 10.173024 loss_rnnt 5.410169 hw_loss 0.265859 lr 0.00025923 rank 1
2023-03-01 17:52:04,155 DEBUG TRAIN Batch 55/3300 loss 7.884394 loss_att 10.375757 loss_ctc 15.867541 loss_rnnt 6.223735 hw_loss 0.183689 lr 0.00025922 rank 1
2023-03-01 17:52:04,157 DEBUG TRAIN Batch 55/3300 loss 8.004900 loss_att 10.984053 loss_ctc 15.543407 loss_rnnt 6.330029 hw_loss 0.138572 lr 0.00025922 rank 0
2023-03-01 17:52:25,672 DEBUG TRAIN Batch 55/3400 loss 8.970553 loss_att 12.172897 loss_ctc 15.443985 loss_rnnt 7.414989 hw_loss 0.097449 lr 0.00025921 rank 1
2023-03-01 17:52:25,674 DEBUG TRAIN Batch 55/3400 loss 8.154652 loss_att 10.199210 loss_ctc 14.631674 loss_rnnt 6.773302 hw_loss 0.204068 lr 0.00025921 rank 0
2023-03-01 17:52:46,916 DEBUG TRAIN Batch 55/3500 loss 9.071242 loss_att 9.348160 loss_ctc 11.337170 loss_rnnt 8.588408 hw_loss 0.234988 lr 0.00025921 rank 1
2023-03-01 17:52:46,925 DEBUG TRAIN Batch 55/3500 loss 5.056564 loss_att 6.309882 loss_ctc 5.739785 loss_rnnt 4.600192 hw_loss 0.214899 lr 0.00025921 rank 0
2023-03-01 17:53:20,136 DEBUG TRAIN Batch 55/3600 loss 8.870096 loss_att 10.027758 loss_ctc 16.984383 loss_rnnt 7.469452 hw_loss 0.163511 lr 0.00025920 rank 1
2023-03-01 17:53:20,138 DEBUG TRAIN Batch 55/3600 loss 8.860132 loss_att 11.027420 loss_ctc 15.111994 loss_rnnt 7.462278 hw_loss 0.245278 lr 0.00025920 rank 0
2023-03-01 17:53:42,129 DEBUG TRAIN Batch 55/3700 loss 14.759557 loss_att 15.973604 loss_ctc 23.143450 loss_rnnt 13.227803 hw_loss 0.320794 lr 0.00025919 rank 1
2023-03-01 17:53:42,130 DEBUG TRAIN Batch 55/3700 loss 8.839651 loss_att 11.210135 loss_ctc 15.793816 loss_rnnt 7.371809 hw_loss 0.124731 lr 0.00025919 rank 0
2023-03-01 17:54:04,797 DEBUG TRAIN Batch 55/3800 loss 5.018178 loss_att 5.854109 loss_ctc 6.820325 loss_rnnt 4.491083 hw_loss 0.224294 lr 0.00025918 rank 1
2023-03-01 17:54:04,799 DEBUG TRAIN Batch 55/3800 loss 7.388883 loss_att 10.141313 loss_ctc 10.861094 loss_rnnt 6.249713 hw_loss 0.235728 lr 0.00025918 rank 0
2023-03-01 17:54:27,393 DEBUG TRAIN Batch 55/3900 loss 7.257971 loss_att 11.424921 loss_ctc 12.614861 loss_rnnt 5.609103 hw_loss 0.189798 lr 0.00025917 rank 1
2023-03-01 17:54:27,395 DEBUG TRAIN Batch 55/3900 loss 5.779848 loss_att 8.587558 loss_ctc 9.975713 loss_rnnt 4.517182 hw_loss 0.265642 lr 0.00025917 rank 0
2023-03-01 17:54:59,901 DEBUG TRAIN Batch 55/4000 loss 5.544887 loss_att 8.152199 loss_ctc 12.688373 loss_rnnt 3.988012 hw_loss 0.155525 lr 0.00025916 rank 1
2023-03-01 17:54:59,904 DEBUG TRAIN Batch 55/4000 loss 3.849013 loss_att 5.639096 loss_ctc 7.660400 loss_rnnt 2.826676 hw_loss 0.292753 lr 0.00025916 rank 0
2023-03-01 17:55:21,928 DEBUG TRAIN Batch 55/4100 loss 5.286407 loss_att 10.653918 loss_ctc 12.796122 loss_rnnt 3.133873 hw_loss 0.145757 lr 0.00025915 rank 1
2023-03-01 17:55:21,930 DEBUG TRAIN Batch 55/4100 loss 10.948749 loss_att 15.071774 loss_ctc 16.443104 loss_rnnt 9.242659 hw_loss 0.279196 lr 0.00025915 rank 0
2023-03-01 17:55:43,161 DEBUG TRAIN Batch 55/4200 loss 6.451719 loss_att 9.675216 loss_ctc 11.618003 loss_rnnt 5.000286 hw_loss 0.221053 lr 0.00025914 rank 1
2023-03-01 17:55:43,163 DEBUG TRAIN Batch 55/4200 loss 7.945103 loss_att 13.036034 loss_ctc 15.653588 loss_rnnt 5.731749 hw_loss 0.313817 lr 0.00025914 rank 0
2023-03-01 17:56:16,897 DEBUG TRAIN Batch 55/4300 loss 10.270410 loss_att 13.918082 loss_ctc 14.690097 loss_rnnt 8.844745 hw_loss 0.200323 lr 0.00025914 rank 1
2023-03-01 17:56:16,897 DEBUG TRAIN Batch 55/4300 loss 8.137255 loss_att 10.139688 loss_ctc 13.791477 loss_rnnt 6.886933 hw_loss 0.179885 lr 0.00025914 rank 0
2023-03-01 17:56:39,111 DEBUG TRAIN Batch 55/4400 loss 2.356555 loss_att 4.400992 loss_ctc 3.366282 loss_rnnt 1.682339 hw_loss 0.245059 lr 0.00025913 rank 0
2023-03-01 17:56:39,111 DEBUG TRAIN Batch 55/4400 loss 4.790408 loss_att 8.331219 loss_ctc 8.025352 loss_rnnt 3.541477 hw_loss 0.205205 lr 0.00025913 rank 1
2023-03-01 17:57:01,325 DEBUG TRAIN Batch 55/4500 loss 5.252533 loss_att 7.985035 loss_ctc 12.111977 loss_rnnt 3.712491 hw_loss 0.148029 lr 0.00025912 rank 1
2023-03-01 17:57:01,326 DEBUG TRAIN Batch 55/4500 loss 5.255449 loss_att 6.984175 loss_ctc 8.379251 loss_rnnt 4.381896 hw_loss 0.208689 lr 0.00025912 rank 0
2023-03-01 17:57:23,118 DEBUG TRAIN Batch 55/4600 loss 2.710346 loss_att 7.020238 loss_ctc 6.717681 loss_rnnt 1.272361 hw_loss 0.078179 lr 0.00025911 rank 1
2023-03-01 17:57:23,119 DEBUG TRAIN Batch 55/4600 loss 1.986031 loss_att 4.481042 loss_ctc 4.435016 loss_rnnt 1.134584 hw_loss 0.048587 lr 0.00025911 rank 0
2023-03-01 17:57:56,004 DEBUG TRAIN Batch 55/4700 loss 6.933690 loss_att 9.940587 loss_ctc 12.184951 loss_rnnt 5.515945 hw_loss 0.217872 lr 0.00025910 rank 1
2023-03-01 17:57:56,006 DEBUG TRAIN Batch 55/4700 loss 4.095597 loss_att 9.943542 loss_ctc 9.125179 loss_rnnt 2.174457 hw_loss 0.151763 lr 0.00025910 rank 0
2023-03-01 17:58:18,303 DEBUG TRAIN Batch 55/4800 loss 7.327274 loss_att 11.382425 loss_ctc 13.131031 loss_rnnt 5.658435 hw_loss 0.157454 lr 0.00025909 rank 1
2023-03-01 17:58:18,305 DEBUG TRAIN Batch 55/4800 loss 5.564888 loss_att 7.214915 loss_ctc 8.445171 loss_rnnt 4.758377 hw_loss 0.173377 lr 0.00025909 rank 0
2023-03-01 17:58:40,491 DEBUG TRAIN Batch 55/4900 loss 9.904890 loss_att 14.306787 loss_ctc 17.555061 loss_rnnt 7.868783 hw_loss 0.254446 lr 0.00025908 rank 1
2023-03-01 17:58:40,494 DEBUG TRAIN Batch 55/4900 loss 11.530155 loss_att 11.774898 loss_ctc 12.827038 loss_rnnt 11.250526 hw_loss 0.108305 lr 0.00025908 rank 0
2023-03-01 17:59:14,490 DEBUG TRAIN Batch 55/5000 loss 4.954496 loss_att 6.622856 loss_ctc 8.004952 loss_rnnt 4.090564 hw_loss 0.231625 lr 0.00025908 rank 1
2023-03-01 17:59:14,491 DEBUG TRAIN Batch 55/5000 loss 5.772592 loss_att 8.704090 loss_ctc 10.463191 loss_rnnt 4.514311 hw_loss 0.087314 lr 0.00025908 rank 0
2023-03-01 17:59:37,282 DEBUG TRAIN Batch 55/5100 loss 9.516474 loss_att 14.431250 loss_ctc 13.674473 loss_rnnt 7.868076 hw_loss 0.208207 lr 0.00025907 rank 1
2023-03-01 17:59:37,282 DEBUG TRAIN Batch 55/5100 loss 6.953599 loss_att 9.271249 loss_ctc 10.894826 loss_rnnt 5.852051 hw_loss 0.210977 lr 0.00025907 rank 0
2023-03-01 17:59:59,539 DEBUG TRAIN Batch 55/5200 loss 5.736096 loss_att 9.182018 loss_ctc 7.637622 loss_rnnt 4.649790 hw_loss 0.269220 lr 0.00025906 rank 1
2023-03-01 17:59:59,540 DEBUG TRAIN Batch 55/5200 loss 11.664692 loss_att 12.273462 loss_ctc 19.656527 loss_rnnt 10.311396 hw_loss 0.311181 lr 0.00025906 rank 0
2023-03-01 18:00:22,556 DEBUG TRAIN Batch 55/5300 loss 5.075986 loss_att 8.671714 loss_ctc 10.373046 loss_rnnt 3.568443 hw_loss 0.153981 lr 0.00025905 rank 0
2023-03-01 18:00:22,556 DEBUG TRAIN Batch 55/5300 loss 7.532633 loss_att 10.855154 loss_ctc 13.101062 loss_rnnt 5.997903 hw_loss 0.239565 lr 0.00025905 rank 1
2023-03-01 18:00:56,241 DEBUG TRAIN Batch 55/5400 loss 2.821904 loss_att 6.699562 loss_ctc 5.064432 loss_rnnt 1.577505 hw_loss 0.318495 lr 0.00025904 rank 1
2023-03-01 18:00:56,242 DEBUG TRAIN Batch 55/5400 loss 8.663707 loss_att 11.117303 loss_ctc 16.727743 loss_rnnt 7.013248 hw_loss 0.158500 lr 0.00025904 rank 0
2023-03-01 18:01:18,454 DEBUG TRAIN Batch 55/5500 loss 11.177846 loss_att 11.937140 loss_ctc 13.347797 loss_rnnt 10.559830 hw_loss 0.331558 lr 0.00025903 rank 1
2023-03-01 18:01:18,456 DEBUG TRAIN Batch 55/5500 loss 4.786900 loss_att 6.129851 loss_ctc 7.027413 loss_rnnt 4.116133 hw_loss 0.193952 lr 0.00025903 rank 0
2023-03-01 18:01:40,816 DEBUG TRAIN Batch 55/5600 loss 7.158442 loss_att 13.452623 loss_ctc 14.472271 loss_rnnt 4.755450 hw_loss 0.316834 lr 0.00025902 rank 1
2023-03-01 18:01:40,818 DEBUG TRAIN Batch 55/5600 loss 6.226679 loss_att 8.911270 loss_ctc 13.038104 loss_rnnt 4.708419 hw_loss 0.137159 lr 0.00025902 rank 0
2023-03-01 18:02:16,202 DEBUG TRAIN Batch 55/5700 loss 6.018240 loss_att 11.529447 loss_ctc 14.489161 loss_rnnt 3.737324 hw_loss 0.092285 lr 0.00025901 rank 1
2023-03-01 18:02:16,204 DEBUG TRAIN Batch 55/5700 loss 8.442768 loss_att 10.258725 loss_ctc 13.280998 loss_rnnt 7.351372 hw_loss 0.155825 lr 0.00025901 rank 0
2023-03-01 18:02:38,708 DEBUG TRAIN Batch 55/5800 loss 4.256916 loss_att 6.521426 loss_ctc 11.080432 loss_rnnt 2.716131 hw_loss 0.333902 lr 0.00025901 rank 1
2023-03-01 18:02:38,709 DEBUG TRAIN Batch 55/5800 loss 7.780303 loss_att 9.996649 loss_ctc 13.514675 loss_rnnt 6.439084 hw_loss 0.250060 lr 0.00025901 rank 0
2023-03-01 18:03:01,016 DEBUG TRAIN Batch 55/5900 loss 7.161010 loss_att 8.697345 loss_ctc 10.427206 loss_rnnt 6.228873 hw_loss 0.355081 lr 0.00025900 rank 1
2023-03-01 18:03:01,018 DEBUG TRAIN Batch 55/5900 loss 4.272183 loss_att 6.135867 loss_ctc 6.026647 loss_rnnt 3.604957 hw_loss 0.113552 lr 0.00025900 rank 0
2023-03-01 18:03:23,515 DEBUG TRAIN Batch 55/6000 loss 4.229574 loss_att 5.975094 loss_ctc 3.619322 loss_rnnt 3.834600 hw_loss 0.238570 lr 0.00025899 rank 0
2023-03-01 18:03:23,517 DEBUG TRAIN Batch 55/6000 loss 4.011302 loss_att 6.889651 loss_ctc 6.302454 loss_rnnt 3.020745 hw_loss 0.205125 lr 0.00025899 rank 1
2023-03-01 18:04:00,637 DEBUG TRAIN Batch 55/6100 loss 4.392833 loss_att 8.218767 loss_ctc 7.373256 loss_rnnt 3.109297 hw_loss 0.226800 lr 0.00025898 rank 1
2023-03-01 18:04:00,639 DEBUG TRAIN Batch 55/6100 loss 10.013186 loss_att 14.098832 loss_ctc 21.296295 loss_rnnt 7.562803 hw_loss 0.241572 lr 0.00025898 rank 0
2023-03-01 18:04:22,779 DEBUG TRAIN Batch 55/6200 loss 9.762435 loss_att 9.191334 loss_ctc 20.867905 loss_rnnt 8.337889 hw_loss 0.108819 lr 0.00025897 rank 1
2023-03-01 18:04:22,781 DEBUG TRAIN Batch 55/6200 loss 3.447146 loss_att 6.373534 loss_ctc 6.652183 loss_rnnt 2.346781 hw_loss 0.164530 lr 0.00025897 rank 0
2023-03-01 18:04:45,536 DEBUG TRAIN Batch 55/6300 loss 4.116724 loss_att 7.561374 loss_ctc 9.700819 loss_rnnt 2.545371 hw_loss 0.258519 lr 0.00025896 rank 1
2023-03-01 18:04:45,536 DEBUG TRAIN Batch 55/6300 loss 10.985116 loss_att 13.051451 loss_ctc 15.721446 loss_rnnt 9.815796 hw_loss 0.233516 lr 0.00025896 rank 0
2023-03-01 18:05:21,762 DEBUG TRAIN Batch 55/6400 loss 3.562774 loss_att 5.877674 loss_ctc 3.852674 loss_rnnt 2.988703 hw_loss 0.135820 lr 0.00025895 rank 1
2023-03-01 18:05:21,765 DEBUG TRAIN Batch 55/6400 loss 6.961031 loss_att 8.037861 loss_ctc 8.850126 loss_rnnt 6.348282 hw_loss 0.272820 lr 0.00025895 rank 0
2023-03-01 18:05:44,137 DEBUG TRAIN Batch 55/6500 loss 7.005194 loss_att 7.153474 loss_ctc 11.667334 loss_rnnt 6.272069 hw_loss 0.153467 lr 0.00025895 rank 1
2023-03-01 18:05:44,139 DEBUG TRAIN Batch 55/6500 loss 11.445312 loss_att 16.033531 loss_ctc 18.476177 loss_rnnt 9.533343 hw_loss 0.106643 lr 0.00025895 rank 0
2023-03-01 18:06:06,607 DEBUG TRAIN Batch 55/6600 loss 8.838107 loss_att 10.107444 loss_ctc 11.963579 loss_rnnt 8.078188 hw_loss 0.167477 lr 0.00025894 rank 1
2023-03-01 18:06:06,607 DEBUG TRAIN Batch 55/6600 loss 7.762166 loss_att 9.946823 loss_ctc 10.828888 loss_rnnt 6.805429 hw_loss 0.207954 lr 0.00025894 rank 0
2023-03-01 18:06:28,449 DEBUG TRAIN Batch 55/6700 loss 6.735168 loss_att 7.051458 loss_ctc 9.109417 loss_rnnt 6.204596 hw_loss 0.282651 lr 0.00025893 rank 1
2023-03-01 18:06:28,450 DEBUG TRAIN Batch 55/6700 loss 9.879062 loss_att 15.634428 loss_ctc 13.866947 loss_rnnt 8.051573 hw_loss 0.271309 lr 0.00025893 rank 0
2023-03-01 18:07:04,264 DEBUG TRAIN Batch 55/6800 loss 3.782547 loss_att 6.063255 loss_ctc 5.600764 loss_rnnt 2.951912 hw_loss 0.247620 lr 0.00025892 rank 1
2023-03-01 18:07:04,267 DEBUG TRAIN Batch 55/6800 loss 9.432655 loss_att 12.365261 loss_ctc 17.443327 loss_rnnt 7.649765 hw_loss 0.240525 lr 0.00025892 rank 0
2023-03-01 18:07:26,627 DEBUG TRAIN Batch 55/6900 loss 13.517756 loss_att 16.278589 loss_ctc 23.234207 loss_rnnt 11.583261 hw_loss 0.162750 lr 0.00025891 rank 1
2023-03-01 18:07:26,629 DEBUG TRAIN Batch 55/6900 loss 6.532081 loss_att 8.555407 loss_ctc 6.747149 loss_rnnt 6.022370 hw_loss 0.143193 lr 0.00025891 rank 0
2023-03-01 18:07:49,911 DEBUG TRAIN Batch 55/7000 loss 6.931491 loss_att 12.514277 loss_ctc 10.752478 loss_rnnt 5.231529 hw_loss 0.138638 lr 0.00025890 rank 1
2023-03-01 18:07:49,913 DEBUG TRAIN Batch 55/7000 loss 6.462278 loss_att 8.471377 loss_ctc 10.116690 loss_rnnt 5.419401 hw_loss 0.288380 lr 0.00025890 rank 0
2023-03-01 18:08:26,392 DEBUG TRAIN Batch 55/7100 loss 9.307040 loss_att 15.401569 loss_ctc 25.605659 loss_rnnt 5.863059 hw_loss 0.097364 lr 0.00025889 rank 1
2023-03-01 18:08:26,392 DEBUG TRAIN Batch 55/7100 loss 5.299397 loss_att 6.216488 loss_ctc 8.753682 loss_rnnt 4.498325 hw_loss 0.294529 lr 0.00025889 rank 0
2023-03-01 18:08:49,278 DEBUG TRAIN Batch 55/7200 loss 7.967340 loss_att 8.781288 loss_ctc 9.791705 loss_rnnt 7.475168 hw_loss 0.161501 lr 0.00025888 rank 1
2023-03-01 18:08:49,282 DEBUG TRAIN Batch 55/7200 loss 5.252759 loss_att 7.391948 loss_ctc 10.004112 loss_rnnt 4.151349 hw_loss 0.075111 lr 0.00025888 rank 0
2023-03-01 18:09:11,675 DEBUG TRAIN Batch 55/7300 loss 6.390443 loss_att 9.792459 loss_ctc 8.712628 loss_rnnt 5.265730 hw_loss 0.252534 lr 0.00025888 rank 1
2023-03-01 18:09:11,675 DEBUG TRAIN Batch 55/7300 loss 13.566426 loss_att 21.802898 loss_ctc 28.544630 loss_rnnt 9.783381 hw_loss 0.259980 lr 0.00025888 rank 0
2023-03-01 18:09:34,464 DEBUG TRAIN Batch 55/7400 loss 2.851575 loss_att 5.784272 loss_ctc 3.635798 loss_rnnt 2.084234 hw_loss 0.142947 lr 0.00025887 rank 1
2023-03-01 18:09:34,466 DEBUG TRAIN Batch 55/7400 loss 13.348180 loss_att 14.137225 loss_ctc 24.392258 loss_rnnt 11.605930 hw_loss 0.209807 lr 0.00025887 rank 0
2023-03-01 18:10:11,152 DEBUG TRAIN Batch 55/7500 loss 2.940697 loss_att 5.439007 loss_ctc 6.997044 loss_rnnt 1.802168 hw_loss 0.183788 lr 0.00025886 rank 1
2023-03-01 18:10:11,155 DEBUG TRAIN Batch 55/7500 loss 9.258377 loss_att 11.774458 loss_ctc 16.902733 loss_rnnt 7.693894 hw_loss 0.078785 lr 0.00025886 rank 0
2023-03-01 18:10:34,435 DEBUG TRAIN Batch 55/7600 loss 6.250638 loss_att 6.750488 loss_ctc 9.410084 loss_rnnt 5.619314 hw_loss 0.206426 lr 0.00025885 rank 1
2023-03-01 18:10:34,438 DEBUG TRAIN Batch 55/7600 loss 6.615701 loss_att 8.962096 loss_ctc 9.762102 loss_rnnt 5.647456 hw_loss 0.148962 lr 0.00025885 rank 0
2023-03-01 18:10:57,197 DEBUG TRAIN Batch 55/7700 loss 3.552537 loss_att 8.053843 loss_ctc 5.681191 loss_rnnt 2.268290 hw_loss 0.187811 lr 0.00025884 rank 1
2023-03-01 18:10:57,199 DEBUG TRAIN Batch 55/7700 loss 7.986484 loss_att 10.040140 loss_ctc 12.592389 loss_rnnt 6.854520 hw_loss 0.200834 lr 0.00025884 rank 0
2023-03-01 18:11:19,592 DEBUG TRAIN Batch 55/7800 loss 5.530881 loss_att 11.486595 loss_ctc 10.699485 loss_rnnt 3.544773 hw_loss 0.198409 lr 0.00025883 rank 1
2023-03-01 18:11:19,594 DEBUG TRAIN Batch 55/7800 loss 7.434305 loss_att 10.168421 loss_ctc 17.209244 loss_rnnt 5.396648 hw_loss 0.351577 lr 0.00025883 rank 0
2023-03-01 18:11:56,235 DEBUG TRAIN Batch 55/7900 loss 5.671747 loss_att 8.311689 loss_ctc 6.900304 loss_rnnt 4.861317 hw_loss 0.222438 lr 0.00025882 rank 1
2023-03-01 18:11:56,237 DEBUG TRAIN Batch 55/7900 loss 7.018721 loss_att 9.604839 loss_ctc 9.918714 loss_rnnt 5.993669 hw_loss 0.227179 lr 0.00025882 rank 0
2023-03-01 18:12:18,390 DEBUG TRAIN Batch 55/8000 loss 1.348923 loss_att 4.008703 loss_ctc 3.254821 loss_rnnt 0.397898 hw_loss 0.309281 lr 0.00025881 rank 1
2023-03-01 18:12:18,392 DEBUG TRAIN Batch 55/8000 loss 9.623640 loss_att 13.430727 loss_ctc 18.056778 loss_rnnt 7.556395 hw_loss 0.340143 lr 0.00025881 rank 0
2023-03-01 18:12:40,649 DEBUG TRAIN Batch 55/8100 loss 11.243675 loss_att 12.161774 loss_ctc 18.670828 loss_rnnt 9.936390 hw_loss 0.250085 lr 0.00025881 rank 1
2023-03-01 18:12:40,652 DEBUG TRAIN Batch 55/8100 loss 9.004035 loss_att 9.607372 loss_ctc 12.243863 loss_rnnt 8.331467 hw_loss 0.224858 lr 0.00025881 rank 0
2023-03-01 18:13:18,033 DEBUG TRAIN Batch 55/8200 loss 4.250255 loss_att 6.563853 loss_ctc 9.757749 loss_rnnt 2.842090 hw_loss 0.395835 lr 0.00025880 rank 1
2023-03-01 18:13:18,034 DEBUG TRAIN Batch 55/8200 loss 11.665871 loss_att 13.385212 loss_ctc 16.395071 loss_rnnt 10.523267 hw_loss 0.315327 lr 0.00025880 rank 0
2023-03-01 18:13:40,821 DEBUG TRAIN Batch 55/8300 loss 2.659626 loss_att 5.902261 loss_ctc 6.216562 loss_rnnt 1.443712 hw_loss 0.174618 lr 0.00025879 rank 1
2023-03-01 18:13:40,823 DEBUG TRAIN Batch 55/8300 loss 9.664653 loss_att 10.888891 loss_ctc 17.702827 loss_rnnt 8.267559 hw_loss 0.150916 lr 0.00025879 rank 0
2023-03-01 18:14:02,756 DEBUG TRAIN Batch 55/8400 loss 10.204536 loss_att 13.124198 loss_ctc 18.007156 loss_rnnt 8.458733 hw_loss 0.227854 lr 0.00025878 rank 1
2023-03-01 18:14:02,758 DEBUG TRAIN Batch 55/8400 loss 6.985382 loss_att 9.419362 loss_ctc 10.843399 loss_rnnt 5.956375 hw_loss 0.052141 lr 0.00025878 rank 0
2023-03-01 18:14:25,871 DEBUG TRAIN Batch 55/8500 loss 9.227974 loss_att 12.323198 loss_ctc 11.117338 loss_rnnt 8.207373 hw_loss 0.280575 lr 0.00025877 rank 1
2023-03-01 18:14:25,876 DEBUG TRAIN Batch 55/8500 loss 4.040887 loss_att 9.629613 loss_ctc 8.243220 loss_rnnt 2.331489 hw_loss 0.058766 lr 0.00025877 rank 0
2023-03-01 18:15:02,853 DEBUG TRAIN Batch 55/8600 loss 5.961643 loss_att 11.739821 loss_ctc 11.424290 loss_rnnt 3.989819 hw_loss 0.164690 lr 0.00025876 rank 1
2023-03-01 18:15:02,856 DEBUG TRAIN Batch 55/8600 loss 9.205399 loss_att 9.743547 loss_ctc 14.141294 loss_rnnt 8.387012 hw_loss 0.098697 lr 0.00025876 rank 0
2023-03-01 18:15:24,994 DEBUG TRAIN Batch 55/8700 loss 14.059274 loss_att 15.678461 loss_ctc 19.869110 loss_rnnt 12.913808 hw_loss 0.088092 lr 0.00025875 rank 1
2023-03-01 18:15:24,996 DEBUG TRAIN Batch 55/8700 loss 3.211717 loss_att 6.617047 loss_ctc 8.501706 loss_rnnt 1.788722 hw_loss 0.068618 lr 0.00025875 rank 0
2023-03-01 18:15:48,054 DEBUG TRAIN Batch 55/8800 loss 7.291507 loss_att 9.637301 loss_ctc 13.619077 loss_rnnt 5.896964 hw_loss 0.153202 lr 0.00025875 rank 0
2023-03-01 18:15:48,057 DEBUG TRAIN Batch 55/8800 loss 4.943440 loss_att 7.171279 loss_ctc 8.158470 loss_rnnt 3.936217 hw_loss 0.249346 lr 0.00025875 rank 1
2023-03-01 18:16:24,577 DEBUG TRAIN Batch 55/8900 loss 12.487719 loss_att 13.697804 loss_ctc 19.169609 loss_rnnt 11.254035 hw_loss 0.188903 lr 0.00025874 rank 1
2023-03-01 18:16:24,577 DEBUG TRAIN Batch 55/8900 loss 8.079947 loss_att 9.322514 loss_ctc 10.603324 loss_rnnt 7.361929 hw_loss 0.249479 lr 0.00025874 rank 0
2023-03-01 18:16:47,322 DEBUG TRAIN Batch 55/9000 loss 4.146606 loss_att 6.018192 loss_ctc 10.420641 loss_rnnt 2.782843 hw_loss 0.286701 lr 0.00025873 rank 1
2023-03-01 18:16:47,324 DEBUG TRAIN Batch 55/9000 loss 9.687590 loss_att 11.964554 loss_ctc 10.558455 loss_rnnt 8.998224 hw_loss 0.220980 lr 0.00025873 rank 0
2023-03-01 18:17:09,756 DEBUG TRAIN Batch 55/9100 loss 3.718685 loss_att 5.920878 loss_ctc 6.744815 loss_rnnt 2.715577 hw_loss 0.298472 lr 0.00025872 rank 1
2023-03-01 18:17:09,759 DEBUG TRAIN Batch 55/9100 loss 0.945939 loss_att 3.084671 loss_ctc 1.727597 loss_rnnt 0.294141 hw_loss 0.224682 lr 0.00025872 rank 0
2023-03-01 18:17:32,469 DEBUG TRAIN Batch 55/9200 loss 8.480515 loss_att 13.956056 loss_ctc 14.453866 loss_rnnt 6.464068 hw_loss 0.234171 lr 0.00025871 rank 1
2023-03-01 18:17:32,470 DEBUG TRAIN Batch 55/9200 loss 2.531218 loss_att 5.735086 loss_ctc 3.289165 loss_rnnt 1.613124 hw_loss 0.330489 lr 0.00025871 rank 0
2023-03-01 18:18:08,560 DEBUG TRAIN Batch 55/9300 loss 7.745102 loss_att 9.441133 loss_ctc 13.357796 loss_rnnt 6.561198 hw_loss 0.180634 lr 0.00025870 rank 1
2023-03-01 18:18:08,562 DEBUG TRAIN Batch 55/9300 loss 5.400879 loss_att 9.253658 loss_ctc 7.168242 loss_rnnt 4.339865 hw_loss 0.102768 lr 0.00025870 rank 0
2023-03-01 18:18:31,134 DEBUG TRAIN Batch 55/9400 loss 10.299191 loss_att 12.632436 loss_ctc 22.986885 loss_rnnt 7.989722 hw_loss 0.283363 lr 0.00025869 rank 1
2023-03-01 18:18:31,134 DEBUG TRAIN Batch 55/9400 loss 8.444343 loss_att 10.304668 loss_ctc 12.097180 loss_rnnt 7.459392 hw_loss 0.235951 lr 0.00025869 rank 0
2023-03-01 18:18:54,027 DEBUG TRAIN Batch 55/9500 loss 10.086385 loss_att 10.091228 loss_ctc 13.084636 loss_rnnt 9.529999 hw_loss 0.291844 lr 0.00025868 rank 1
2023-03-01 18:18:54,031 DEBUG TRAIN Batch 55/9500 loss 12.884478 loss_att 16.755070 loss_ctc 19.611492 loss_rnnt 11.123791 hw_loss 0.168062 lr 0.00025868 rank 0
2023-03-01 18:19:28,785 DEBUG TRAIN Batch 55/9600 loss 2.300056 loss_att 5.974775 loss_ctc 2.815807 loss_rnnt 1.364548 hw_loss 0.247121 lr 0.00025868 rank 1
2023-03-01 18:19:28,788 DEBUG TRAIN Batch 55/9600 loss 5.127081 loss_att 5.901192 loss_ctc 9.078323 loss_rnnt 4.306273 hw_loss 0.260914 lr 0.00025868 rank 0
2023-03-01 18:19:51,427 DEBUG TRAIN Batch 55/9700 loss 5.911395 loss_att 8.634626 loss_ctc 8.586781 loss_rnnt 4.997087 hw_loss 0.024267 lr 0.00025867 rank 1
2023-03-01 18:19:51,430 DEBUG TRAIN Batch 55/9700 loss 9.379019 loss_att 13.317570 loss_ctc 14.373465 loss_rnnt 7.804676 hw_loss 0.226324 lr 0.00025867 rank 0
2023-03-01 18:20:13,857 DEBUG TRAIN Batch 55/9800 loss 5.970641 loss_att 8.967354 loss_ctc 10.292870 loss_rnnt 4.740345 hw_loss 0.102481 lr 0.00025866 rank 1
2023-03-01 18:20:13,857 DEBUG TRAIN Batch 55/9800 loss 4.910520 loss_att 7.992229 loss_ctc 8.691833 loss_rnnt 3.654227 hw_loss 0.254580 lr 0.00025866 rank 0
2023-03-01 18:20:35,711 DEBUG TRAIN Batch 55/9900 loss 4.189214 loss_att 7.990676 loss_ctc 10.820612 loss_rnnt 2.404644 hw_loss 0.262672 lr 0.00025865 rank 1
2023-03-01 18:20:35,713 DEBUG TRAIN Batch 55/9900 loss 7.934793 loss_att 10.665503 loss_ctc 11.397808 loss_rnnt 6.799157 hw_loss 0.239548 lr 0.00025865 rank 0
2023-03-01 18:21:11,012 DEBUG TRAIN Batch 55/10000 loss 7.899587 loss_att 11.241297 loss_ctc 12.866560 loss_rnnt 6.423941 hw_loss 0.271953 lr 0.00025864 rank 1
2023-03-01 18:21:11,015 DEBUG TRAIN Batch 55/10000 loss 8.000908 loss_att 10.099633 loss_ctc 11.438153 loss_rnnt 7.058930 hw_loss 0.119873 lr 0.00025864 rank 0
2023-03-01 18:21:33,450 DEBUG TRAIN Batch 55/10100 loss 4.537113 loss_att 6.599951 loss_ctc 6.573349 loss_rnnt 3.756803 hw_loss 0.180456 lr 0.00025863 rank 1
2023-03-01 18:21:33,453 DEBUG TRAIN Batch 55/10100 loss 5.909647 loss_att 8.110710 loss_ctc 9.677908 loss_rnnt 4.833303 hw_loss 0.250682 lr 0.00025863 rank 0
2023-03-01 18:21:56,411 DEBUG TRAIN Batch 55/10200 loss 5.139890 loss_att 5.557751 loss_ctc 8.516611 loss_rnnt 4.456429 hw_loss 0.280613 lr 0.00025862 rank 1
2023-03-01 18:21:56,414 DEBUG TRAIN Batch 55/10200 loss 7.450167 loss_att 10.025465 loss_ctc 12.746155 loss_rnnt 6.077548 hw_loss 0.283927 lr 0.00025862 rank 0
2023-03-01 18:22:33,599 DEBUG TRAIN Batch 55/10300 loss 7.807456 loss_att 9.525848 loss_ctc 11.208788 loss_rnnt 6.840814 hw_loss 0.317725 lr 0.00025862 rank 1
2023-03-01 18:22:33,599 DEBUG TRAIN Batch 55/10300 loss 5.677173 loss_att 6.780635 loss_ctc 11.892321 loss_rnnt 4.480625 hw_loss 0.275941 lr 0.00025862 rank 0
2023-03-01 18:22:56,008 DEBUG TRAIN Batch 55/10400 loss 9.957818 loss_att 13.196409 loss_ctc 13.316122 loss_rnnt 8.848230 hw_loss 0.026430 lr 0.00025861 rank 1
2023-03-01 18:22:56,010 DEBUG TRAIN Batch 55/10400 loss 9.341501 loss_att 12.737172 loss_ctc 16.586754 loss_rnnt 7.582990 hw_loss 0.212518 lr 0.00025861 rank 0
2023-03-01 18:23:18,775 DEBUG TRAIN Batch 55/10500 loss 4.077331 loss_att 6.826514 loss_ctc 6.767065 loss_rnnt 3.031404 hw_loss 0.257737 lr 0.00025860 rank 1
2023-03-01 18:23:18,777 DEBUG TRAIN Batch 55/10500 loss 2.290764 loss_att 5.416743 loss_ctc 5.017781 loss_rnnt 1.221297 hw_loss 0.151255 lr 0.00025860 rank 0
2023-03-01 18:23:41,054 DEBUG TRAIN Batch 55/10600 loss 3.790005 loss_att 7.443264 loss_ctc 9.031578 loss_rnnt 2.274653 hw_loss 0.160920 lr 0.00025859 rank 1
2023-03-01 18:23:41,056 DEBUG TRAIN Batch 55/10600 loss 10.006920 loss_att 14.130973 loss_ctc 16.707945 loss_rnnt 8.213381 hw_loss 0.141110 lr 0.00025859 rank 0
2023-03-01 18:24:17,340 DEBUG TRAIN Batch 55/10700 loss 9.713498 loss_att 12.001852 loss_ctc 16.048704 loss_rnnt 8.272590 hw_loss 0.259769 lr 0.00025858 rank 1
2023-03-01 18:24:17,342 DEBUG TRAIN Batch 55/10700 loss 7.175604 loss_att 11.376429 loss_ctc 12.261847 loss_rnnt 5.537138 hw_loss 0.225253 lr 0.00025858 rank 0
2023-03-01 18:24:40,183 DEBUG TRAIN Batch 55/10800 loss 5.608300 loss_att 5.967952 loss_ctc 8.833132 loss_rnnt 4.953596 hw_loss 0.286493 lr 0.00025857 rank 1
2023-03-01 18:24:40,183 DEBUG TRAIN Batch 55/10800 loss 5.044042 loss_att 6.727852 loss_ctc 7.498927 loss_rnnt 4.327116 hw_loss 0.099087 lr 0.00025857 rank 0
2023-03-01 18:25:02,630 DEBUG TRAIN Batch 55/10900 loss 3.656497 loss_att 6.827282 loss_ctc 5.555121 loss_rnnt 2.689710 hw_loss 0.149024 lr 0.00025856 rank 1
2023-03-01 18:25:02,633 DEBUG TRAIN Batch 55/10900 loss 7.230358 loss_att 10.888144 loss_ctc 13.389495 loss_rnnt 5.604081 hw_loss 0.137813 lr 0.00025856 rank 0
2023-03-01 18:25:25,701 DEBUG TRAIN Batch 55/11000 loss 3.530089 loss_att 5.613072 loss_ctc 6.380642 loss_rnnt 2.635230 hw_loss 0.184103 lr 0.00025856 rank 1
2023-03-01 18:25:25,704 DEBUG TRAIN Batch 55/11000 loss 2.401090 loss_att 5.308666 loss_ctc 4.203642 loss_rnnt 1.495182 hw_loss 0.157597 lr 0.00025856 rank 0
2023-03-01 18:26:01,895 DEBUG TRAIN Batch 55/11100 loss 2.473758 loss_att 5.819480 loss_ctc 4.498703 loss_rnnt 1.393255 hw_loss 0.265063 lr 0.00025855 rank 1
2023-03-01 18:26:01,896 DEBUG TRAIN Batch 55/11100 loss 11.981767 loss_att 13.745355 loss_ctc 16.820065 loss_rnnt 10.915667 hw_loss 0.128015 lr 0.00025855 rank 0
2023-03-01 18:26:24,714 DEBUG TRAIN Batch 55/11200 loss 9.917870 loss_att 11.803508 loss_ctc 14.330263 loss_rnnt 8.873184 hw_loss 0.148572 lr 0.00025854 rank 1
2023-03-01 18:26:24,715 DEBUG TRAIN Batch 55/11200 loss 13.451835 loss_att 15.540298 loss_ctc 23.292875 loss_rnnt 11.654449 hw_loss 0.126664 lr 0.00025854 rank 0
2023-03-01 18:26:47,225 DEBUG TRAIN Batch 55/11300 loss 3.885285 loss_att 5.681295 loss_ctc 6.476838 loss_rnnt 3.052120 hw_loss 0.240793 lr 0.00025853 rank 1
2023-03-01 18:26:47,226 DEBUG TRAIN Batch 55/11300 loss 2.951938 loss_att 5.951934 loss_ctc 6.516060 loss_rnnt 1.753611 hw_loss 0.230833 lr 0.00025853 rank 0
2023-03-01 18:27:23,279 DEBUG TRAIN Batch 55/11400 loss 4.733388 loss_att 5.948914 loss_ctc 7.879171 loss_rnnt 3.879929 hw_loss 0.357967 lr 0.00025852 rank 1
2023-03-01 18:27:23,280 DEBUG TRAIN Batch 55/11400 loss 9.315957 loss_att 11.013793 loss_ctc 12.626949 loss_rnnt 8.408774 hw_loss 0.236533 lr 0.00025852 rank 0
2023-03-01 18:27:45,750 DEBUG TRAIN Batch 55/11500 loss 7.381579 loss_att 9.859686 loss_ctc 8.453184 loss_rnnt 6.573519 hw_loss 0.317923 lr 0.00025851 rank 1
2023-03-01 18:27:45,751 DEBUG TRAIN Batch 55/11500 loss 5.866591 loss_att 8.147547 loss_ctc 8.838851 loss_rnnt 4.942504 hw_loss 0.134240 lr 0.00025851 rank 0
2023-03-01 18:28:08,443 DEBUG TRAIN Batch 55/11600 loss 9.669191 loss_att 12.579600 loss_ctc 16.533224 loss_rnnt 8.134121 hw_loss 0.070847 lr 0.00025850 rank 1
2023-03-01 18:28:08,444 DEBUG TRAIN Batch 55/11600 loss 4.136162 loss_att 6.042580 loss_ctc 4.726244 loss_rnnt 3.532947 hw_loss 0.268601 lr 0.00025850 rank 0
2023-03-01 18:28:30,825 DEBUG TRAIN Batch 55/11700 loss 13.162513 loss_att 17.456600 loss_ctc 22.772293 loss_rnnt 10.935720 hw_loss 0.162508 lr 0.00025849 rank 1
2023-03-01 18:28:30,827 DEBUG TRAIN Batch 55/11700 loss 6.009756 loss_att 8.827154 loss_ctc 8.921106 loss_rnnt 5.003259 hw_loss 0.102820 lr 0.00025849 rank 0
2023-03-01 18:29:07,350 DEBUG TRAIN Batch 55/11800 loss 13.090261 loss_att 19.160824 loss_ctc 27.551773 loss_rnnt 9.896864 hw_loss 0.095783 lr 0.00025849 rank 1
2023-03-01 18:29:07,353 DEBUG TRAIN Batch 55/11800 loss 7.130795 loss_att 10.223979 loss_ctc 9.684902 loss_rnnt 6.110719 hw_loss 0.114170 lr 0.00025849 rank 0
2023-03-01 18:29:29,700 DEBUG TRAIN Batch 55/11900 loss 2.500233 loss_att 4.197052 loss_ctc 3.650984 loss_rnnt 1.893815 hw_loss 0.213039 lr 0.00025848 rank 1
2023-03-01 18:29:29,702 DEBUG TRAIN Batch 55/11900 loss 13.218324 loss_att 12.697318 loss_ctc 17.158442 loss_rnnt 12.661766 hw_loss 0.253893 lr 0.00025848 rank 0
2023-03-01 18:29:52,574 DEBUG TRAIN Batch 55/12000 loss 3.940350 loss_att 5.311061 loss_ctc 6.278905 loss_rnnt 3.271080 hw_loss 0.156223 lr 0.00025847 rank 1
2023-03-01 18:29:52,575 DEBUG TRAIN Batch 55/12000 loss 6.085631 loss_att 9.885301 loss_ctc 10.991704 loss_rnnt 4.555250 hw_loss 0.218070 lr 0.00025847 rank 0
2023-03-01 18:30:28,903 DEBUG TRAIN Batch 55/12100 loss 8.237174 loss_att 11.655263 loss_ctc 11.927955 loss_rnnt 6.974995 hw_loss 0.162109 lr 0.00025846 rank 1
2023-03-01 18:30:28,904 DEBUG TRAIN Batch 55/12100 loss 11.416746 loss_att 16.271883 loss_ctc 27.782326 loss_rnnt 8.164883 hw_loss 0.185173 lr 0.00025846 rank 0
2023-03-01 18:30:51,573 DEBUG TRAIN Batch 55/12200 loss 7.614869 loss_att 13.184916 loss_ctc 15.036685 loss_rnnt 5.344993 hw_loss 0.311797 lr 0.00025845 rank 1
2023-03-01 18:30:51,575 DEBUG TRAIN Batch 55/12200 loss 7.560385 loss_att 10.058883 loss_ctc 9.852221 loss_rnnt 6.622909 hw_loss 0.247872 lr 0.00025845 rank 0
2023-03-01 18:31:14,290 DEBUG TRAIN Batch 55/12300 loss 1.749716 loss_att 4.932805 loss_ctc 4.367640 loss_rnnt 0.708975 hw_loss 0.103250 lr 0.00025844 rank 1
2023-03-01 18:31:14,290 DEBUG TRAIN Batch 55/12300 loss 9.836348 loss_att 11.665801 loss_ctc 12.498562 loss_rnnt 9.030570 hw_loss 0.159235 lr 0.00025844 rank 0
2023-03-01 18:31:36,222 DEBUG TRAIN Batch 55/12400 loss 4.177604 loss_att 7.704967 loss_ctc 6.428819 loss_rnnt 3.076376 hw_loss 0.179237 lr 0.00025843 rank 1
2023-03-01 18:31:36,223 DEBUG TRAIN Batch 55/12400 loss 7.167809 loss_att 9.773834 loss_ctc 10.852783 loss_rnnt 6.036083 hw_loss 0.223482 lr 0.00025843 rank 0
2023-03-01 18:32:14,017 DEBUG TRAIN Batch 55/12500 loss 2.598197 loss_att 4.894574 loss_ctc 4.758952 loss_rnnt 1.740097 hw_loss 0.207607 lr 0.00025843 rank 1
2023-03-01 18:32:14,017 DEBUG TRAIN Batch 55/12500 loss 2.779246 loss_att 8.089415 loss_ctc 9.011404 loss_rnnt 0.836795 hw_loss 0.092743 lr 0.00025843 rank 0
2023-03-01 18:32:35,740 DEBUG TRAIN Batch 55/12600 loss 4.021902 loss_att 5.837477 loss_ctc 7.452963 loss_rnnt 3.174210 hw_loss 0.050816 lr 0.00025842 rank 1
2023-03-01 18:32:35,741 DEBUG TRAIN Batch 55/12600 loss 9.154403 loss_att 13.688811 loss_ctc 16.601292 loss_rnnt 7.205986 hw_loss 0.091155 lr 0.00025842 rank 0
2023-03-01 18:32:58,373 DEBUG TRAIN Batch 55/12700 loss 5.339029 loss_att 9.359563 loss_ctc 7.618491 loss_rnnt 4.194247 hw_loss 0.068900 lr 0.00025841 rank 1
2023-03-01 18:32:58,375 DEBUG TRAIN Batch 55/12700 loss 5.485709 loss_att 6.521175 loss_ctc 6.963076 loss_rnnt 4.932962 hw_loss 0.278758 lr 0.00025841 rank 0
2023-03-01 18:33:34,238 DEBUG TRAIN Batch 55/12800 loss 11.995368 loss_att 14.627487 loss_ctc 18.027159 loss_rnnt 10.542413 hw_loss 0.229297 lr 0.00025840 rank 1
2023-03-01 18:33:34,240 DEBUG TRAIN Batch 55/12800 loss 6.023440 loss_att 8.804731 loss_ctc 11.159273 loss_rnnt 4.623931 hw_loss 0.297137 lr 0.00025840 rank 0
2023-03-01 18:33:56,582 DEBUG TRAIN Batch 55/12900 loss 8.161467 loss_att 8.273215 loss_ctc 10.996560 loss_rnnt 7.624704 hw_loss 0.255751 lr 0.00025839 rank 0
2023-03-01 18:33:56,583 DEBUG TRAIN Batch 55/12900 loss 7.324517 loss_att 10.085601 loss_ctc 14.391363 loss_rnnt 5.723742 hw_loss 0.199334 lr 0.00025839 rank 1
2023-03-01 18:34:18,711 DEBUG TRAIN Batch 55/13000 loss 18.533569 loss_att 21.028360 loss_ctc 24.712276 loss_rnnt 17.089201 hw_loss 0.227966 lr 0.00025838 rank 1
2023-03-01 18:34:18,712 DEBUG TRAIN Batch 55/13000 loss 6.850095 loss_att 11.275050 loss_ctc 11.579047 loss_rnnt 5.315601 hw_loss 0.035579 lr 0.00025838 rank 0
2023-03-01 18:34:40,835 DEBUG TRAIN Batch 55/13100 loss 3.657048 loss_att 6.465921 loss_ctc 7.030519 loss_rnnt 2.516739 hw_loss 0.241384 lr 0.00025837 rank 0
2023-03-01 18:34:40,849 DEBUG TRAIN Batch 55/13100 loss 7.447533 loss_att 8.433638 loss_ctc 9.140287 loss_rnnt 6.961498 hw_loss 0.118337 lr 0.00025837 rank 1
2023-03-01 18:35:16,560 DEBUG TRAIN Batch 55/13200 loss 11.614196 loss_att 15.526226 loss_ctc 18.376633 loss_rnnt 9.850159 hw_loss 0.149948 lr 0.00025837 rank 0
2023-03-01 18:35:16,560 DEBUG TRAIN Batch 55/13200 loss 8.431163 loss_att 11.473073 loss_ctc 13.113228 loss_rnnt 7.136909 hw_loss 0.115496 lr 0.00025837 rank 1
2023-03-01 18:35:38,494 DEBUG TRAIN Batch 55/13300 loss 10.435089 loss_att 12.448620 loss_ctc 18.770618 loss_rnnt 8.790236 hw_loss 0.245144 lr 0.00025836 rank 1
2023-03-01 18:35:38,495 DEBUG TRAIN Batch 55/13300 loss 9.746387 loss_att 10.553593 loss_ctc 15.165230 loss_rnnt 8.814161 hw_loss 0.090510 lr 0.00025836 rank 0
2023-03-01 18:36:01,102 DEBUG TRAIN Batch 55/13400 loss 4.284122 loss_att 6.678992 loss_ctc 5.896050 loss_rnnt 3.447509 hw_loss 0.267590 lr 0.00025835 rank 1
2023-03-01 18:36:01,103 DEBUG TRAIN Batch 55/13400 loss 5.742644 loss_att 10.615702 loss_ctc 9.245442 loss_rnnt 4.204319 hw_loss 0.181263 lr 0.00025835 rank 0
2023-03-01 18:36:37,633 DEBUG TRAIN Batch 55/13500 loss 2.248917 loss_att 5.522638 loss_ctc 4.472098 loss_rnnt 1.218156 hw_loss 0.149235 lr 0.00025834 rank 1
2023-03-01 18:36:37,679 DEBUG TRAIN Batch 55/13500 loss 5.990400 loss_att 6.505402 loss_ctc 8.963809 loss_rnnt 5.358251 hw_loss 0.248802 lr 0.00025834 rank 0
2023-03-01 18:37:00,089 DEBUG TRAIN Batch 55/13600 loss 1.968321 loss_att 5.007806 loss_ctc 4.397041 loss_rnnt 0.941014 hw_loss 0.179215 lr 0.00025833 rank 1
2023-03-01 18:37:00,091 DEBUG TRAIN Batch 55/13600 loss 5.007727 loss_att 10.437790 loss_ctc 8.085232 loss_rnnt 3.404917 hw_loss 0.199618 lr 0.00025833 rank 0
2023-03-01 18:37:23,018 DEBUG TRAIN Batch 55/13700 loss 6.272636 loss_att 9.287397 loss_ctc 12.734345 loss_rnnt 4.745429 hw_loss 0.117550 lr 0.00025832 rank 1
2023-03-01 18:37:23,019 DEBUG TRAIN Batch 55/13700 loss 4.622387 loss_att 8.054398 loss_ctc 7.792544 loss_rnnt 3.398712 hw_loss 0.214848 lr 0.00025832 rank 0
2023-03-01 18:37:46,084 DEBUG TRAIN Batch 55/13800 loss 4.954234 loss_att 8.489210 loss_ctc 7.622796 loss_rnnt 3.777901 hw_loss 0.212868 lr 0.00025831 rank 1
2023-03-01 18:37:46,086 DEBUG TRAIN Batch 55/13800 loss 1.810132 loss_att 4.609166 loss_ctc 3.018969 loss_rnnt 1.067240 hw_loss 0.041074 lr 0.00025831 rank 0
2023-03-01 18:38:22,726 DEBUG TRAIN Batch 55/13900 loss 6.569551 loss_att 7.963477 loss_ctc 8.616585 loss_rnnt 5.906423 hw_loss 0.208885 lr 0.00025830 rank 1
2023-03-01 18:38:22,729 DEBUG TRAIN Batch 55/13900 loss 6.150130 loss_att 7.482398 loss_ctc 7.697401 loss_rnnt 5.587329 hw_loss 0.168834 lr 0.00025830 rank 0
2023-03-01 18:38:45,546 DEBUG TRAIN Batch 55/14000 loss 5.169289 loss_att 5.959637 loss_ctc 8.260534 loss_rnnt 4.507894 hw_loss 0.170925 lr 0.00025830 rank 1
2023-03-01 18:38:45,548 DEBUG TRAIN Batch 55/14000 loss 4.562555 loss_att 6.506489 loss_ctc 8.805361 loss_rnnt 3.520835 hw_loss 0.163548 lr 0.00025830 rank 0
2023-03-01 18:39:08,208 DEBUG TRAIN Batch 55/14100 loss 4.770486 loss_att 5.881037 loss_ctc 8.802476 loss_rnnt 3.884709 hw_loss 0.236378 lr 0.00025829 rank 0
2023-03-01 18:39:08,208 DEBUG TRAIN Batch 55/14100 loss 7.010522 loss_att 9.871563 loss_ctc 12.753019 loss_rnnt 5.565705 hw_loss 0.200516 lr 0.00025829 rank 1
2023-03-01 18:39:30,813 DEBUG TRAIN Batch 55/14200 loss 16.423340 loss_att 19.343143 loss_ctc 22.047611 loss_rnnt 15.052741 hw_loss 0.068881 lr 0.00025828 rank 1
2023-03-01 18:39:30,815 DEBUG TRAIN Batch 55/14200 loss 7.630401 loss_att 13.546980 loss_ctc 14.790379 loss_rnnt 5.435366 hw_loss 0.106980 lr 0.00025828 rank 0
2023-03-01 18:40:06,193 DEBUG TRAIN Batch 55/14300 loss 6.275977 loss_att 9.791958 loss_ctc 5.955296 loss_rnnt 5.528186 hw_loss 0.163786 lr 0.00025827 rank 1
2023-03-01 18:40:06,193 DEBUG TRAIN Batch 55/14300 loss 11.374408 loss_att 11.043476 loss_ctc 15.193186 loss_rnnt 10.828751 hw_loss 0.192510 lr 0.00025827 rank 0
2023-03-01 18:40:28,717 DEBUG TRAIN Batch 55/14400 loss 4.791336 loss_att 9.076441 loss_ctc 9.256055 loss_rnnt 3.277695 hw_loss 0.114984 lr 0.00025826 rank 1
2023-03-01 18:40:28,719 DEBUG TRAIN Batch 55/14400 loss 3.390460 loss_att 4.912797 loss_ctc 5.615555 loss_rnnt 2.623921 hw_loss 0.310109 lr 0.00025826 rank 0
2023-03-01 18:40:51,815 DEBUG TRAIN Batch 55/14500 loss 2.936042 loss_att 7.146584 loss_ctc 7.310279 loss_rnnt 1.356363 hw_loss 0.289386 lr 0.00025825 rank 1
2023-03-01 18:40:51,817 DEBUG TRAIN Batch 55/14500 loss 7.683922 loss_att 11.113695 loss_ctc 12.808338 loss_rnnt 6.194932 hw_loss 0.224588 lr 0.00025825 rank 0
2023-03-01 18:41:27,838 DEBUG TRAIN Batch 55/14600 loss 13.211545 loss_att 16.165623 loss_ctc 23.232635 loss_rnnt 11.152197 hw_loss 0.248226 lr 0.00025824 rank 1
2023-03-01 18:41:27,839 DEBUG TRAIN Batch 55/14600 loss 8.045048 loss_att 10.477798 loss_ctc 15.360703 loss_rnnt 6.480396 hw_loss 0.192524 lr 0.00025824 rank 0
2023-03-01 18:41:50,453 DEBUG TRAIN Batch 55/14700 loss 3.844905 loss_att 8.070201 loss_ctc 11.271349 loss_rnnt 1.904683 hw_loss 0.196820 lr 0.00025824 rank 1
2023-03-01 18:41:50,455 DEBUG TRAIN Batch 55/14700 loss 8.060136 loss_att 11.146520 loss_ctc 14.702161 loss_rnnt 6.513213 hw_loss 0.082581 lr 0.00025824 rank 0
2023-03-01 18:42:13,337 DEBUG TRAIN Batch 55/14800 loss 2.149261 loss_att 5.092343 loss_ctc 5.806659 loss_rnnt 0.943456 hw_loss 0.242879 lr 0.00025823 rank 1
2023-03-01 18:42:13,339 DEBUG TRAIN Batch 55/14800 loss 7.830298 loss_att 12.702112 loss_ctc 13.109346 loss_rnnt 6.028347 hw_loss 0.231966 lr 0.00025823 rank 0
2023-03-01 18:42:36,149 DEBUG TRAIN Batch 55/14900 loss 3.660449 loss_att 6.338360 loss_ctc 5.649729 loss_rnnt 2.732896 hw_loss 0.237624 lr 0.00025822 rank 1
2023-03-01 18:42:36,149 DEBUG TRAIN Batch 55/14900 loss 4.298605 loss_att 6.189802 loss_ctc 9.662275 loss_rnnt 3.038961 hw_loss 0.311716 lr 0.00025822 rank 0
2023-03-01 18:43:10,582 DEBUG TRAIN Batch 55/15000 loss 5.035426 loss_att 7.425965 loss_ctc 9.198435 loss_rnnt 3.907881 hw_loss 0.176941 lr 0.00025821 rank 1
2023-03-01 18:43:10,584 DEBUG TRAIN Batch 55/15000 loss 4.739508 loss_att 6.532889 loss_ctc 7.480871 loss_rnnt 3.946496 hw_loss 0.129038 lr 0.00025821 rank 0
2023-03-01 18:43:33,262 DEBUG TRAIN Batch 55/15100 loss 14.666220 loss_att 15.847837 loss_ctc 21.122990 loss_rnnt 13.485286 hw_loss 0.156950 lr 0.00025820 rank 1
2023-03-01 18:43:33,263 DEBUG TRAIN Batch 55/15100 loss 7.997932 loss_att 11.999506 loss_ctc 12.004400 loss_rnnt 6.566036 hw_loss 0.182598 lr 0.00025820 rank 0
2023-03-01 18:43:56,251 DEBUG TRAIN Batch 55/15200 loss 5.700891 loss_att 7.570581 loss_ctc 10.426316 loss_rnnt 4.622947 hw_loss 0.138653 lr 0.00025819 rank 1
2023-03-01 18:43:56,253 DEBUG TRAIN Batch 55/15200 loss 8.306371 loss_att 10.767670 loss_ctc 12.690163 loss_rnnt 7.084507 hw_loss 0.272059 lr 0.00025819 rank 0
2023-03-01 18:44:31,987 DEBUG TRAIN Batch 55/15300 loss 2.976832 loss_att 8.180010 loss_ctc 6.190038 loss_rnnt 1.406503 hw_loss 0.189872 lr 0.00025818 rank 1
2023-03-01 18:44:31,989 DEBUG TRAIN Batch 55/15300 loss 7.264570 loss_att 8.661081 loss_ctc 12.708008 loss_rnnt 6.157262 hw_loss 0.191650 lr 0.00025818 rank 0
2023-03-01 18:44:54,383 DEBUG TRAIN Batch 55/15400 loss 15.932076 loss_att 20.144985 loss_ctc 27.201162 loss_rnnt 13.514033 hw_loss 0.136717 lr 0.00025818 rank 1
2023-03-01 18:44:54,385 DEBUG TRAIN Batch 55/15400 loss 5.695767 loss_att 7.617506 loss_ctc 8.761389 loss_rnnt 4.795367 hw_loss 0.201193 lr 0.00025818 rank 0
2023-03-01 18:45:16,806 DEBUG TRAIN Batch 55/15500 loss 8.801975 loss_att 11.934971 loss_ctc 13.430911 loss_rnnt 7.422788 hw_loss 0.253868 lr 0.00025817 rank 1
2023-03-01 18:45:16,806 DEBUG TRAIN Batch 55/15500 loss 8.346437 loss_att 10.359401 loss_ctc 11.308899 loss_rnnt 7.487542 hw_loss 0.114949 lr 0.00025817 rank 0
2023-03-01 18:45:39,655 DEBUG TRAIN Batch 55/15600 loss 4.733801 loss_att 8.699809 loss_ctc 10.344316 loss_rnnt 3.065210 hw_loss 0.238726 lr 0.00025816 rank 1
2023-03-01 18:45:39,655 DEBUG TRAIN Batch 55/15600 loss 7.449759 loss_att 12.067215 loss_ctc 11.750135 loss_rnnt 5.878399 hw_loss 0.139660 lr 0.00025816 rank 0
2023-03-01 18:46:14,220 DEBUG TRAIN Batch 55/15700 loss 6.620725 loss_att 9.261452 loss_ctc 8.426067 loss_rnnt 5.716728 hw_loss 0.253388 lr 0.00025815 rank 1
2023-03-01 18:46:14,222 DEBUG TRAIN Batch 55/15700 loss 8.255583 loss_att 13.503336 loss_ctc 14.406858 loss_rnnt 6.352523 hw_loss 0.062511 lr 0.00025815 rank 0
2023-03-01 18:46:36,550 DEBUG TRAIN Batch 55/15800 loss 10.114697 loss_att 13.637529 loss_ctc 15.423775 loss_rnnt 8.598647 hw_loss 0.194262 lr 0.00025814 rank 1
2023-03-01 18:46:36,553 DEBUG TRAIN Batch 55/15800 loss 2.698964 loss_att 7.171446 loss_ctc 4.449764 loss_rnnt 1.498041 hw_loss 0.136851 lr 0.00025814 rank 0
2023-03-01 18:46:59,272 DEBUG TRAIN Batch 55/15900 loss 3.346742 loss_att 6.806450 loss_ctc 9.028322 loss_rnnt 1.788733 hw_loss 0.203483 lr 0.00025813 rank 1
2023-03-01 18:46:59,274 DEBUG TRAIN Batch 55/15900 loss 10.102101 loss_att 11.534516 loss_ctc 15.581473 loss_rnnt 8.988091 hw_loss 0.181771 lr 0.00025813 rank 0
2023-03-01 18:47:35,130 DEBUG TRAIN Batch 55/16000 loss 10.830371 loss_att 15.497751 loss_ctc 13.666212 loss_rnnt 9.396822 hw_loss 0.228674 lr 0.00025812 rank 1
2023-03-01 18:47:35,131 DEBUG TRAIN Batch 55/16000 loss 8.917186 loss_att 11.479244 loss_ctc 13.674173 loss_rnnt 7.604885 hw_loss 0.310544 lr 0.00025812 rank 0
2023-03-01 18:47:57,729 DEBUG TRAIN Batch 55/16100 loss 3.896428 loss_att 8.210306 loss_ctc 8.662417 loss_rnnt 2.315672 hw_loss 0.154716 lr 0.00025812 rank 1
2023-03-01 18:47:57,731 DEBUG TRAIN Batch 55/16100 loss 9.453200 loss_att 15.691730 loss_ctc 18.425625 loss_rnnt 6.838116 hw_loss 0.320727 lr 0.00025812 rank 0
2023-03-01 18:48:20,391 DEBUG TRAIN Batch 55/16200 loss 9.028425 loss_att 11.547284 loss_ctc 14.606087 loss_rnnt 7.667718 hw_loss 0.212339 lr 0.00025811 rank 1
2023-03-01 18:48:20,391 DEBUG TRAIN Batch 55/16200 loss 3.322571 loss_att 5.724726 loss_ctc 6.188090 loss_rnnt 2.359655 hw_loss 0.188280 lr 0.00025811 rank 0
2023-03-01 18:48:43,054 DEBUG TRAIN Batch 55/16300 loss 3.549576 loss_att 6.216196 loss_ctc 7.757130 loss_rnnt 2.288018 hw_loss 0.313550 lr 0.00025810 rank 1
2023-03-01 18:48:43,056 DEBUG TRAIN Batch 55/16300 loss 2.840510 loss_att 5.276800 loss_ctc 4.056048 loss_rnnt 2.099818 hw_loss 0.171304 lr 0.00025810 rank 0
2023-03-01 18:49:18,035 DEBUG TRAIN Batch 55/16400 loss 10.609843 loss_att 12.412694 loss_ctc 18.904211 loss_rnnt 9.081325 hw_loss 0.116311 lr 0.00025809 rank 1
2023-03-01 18:49:18,037 DEBUG TRAIN Batch 55/16400 loss 3.899373 loss_att 6.145500 loss_ctc 8.758097 loss_rnnt 2.707211 hw_loss 0.178325 lr 0.00025809 rank 0
2023-03-01 18:49:40,833 DEBUG TRAIN Batch 55/16500 loss 4.797906 loss_att 5.534821 loss_ctc 8.343184 loss_rnnt 4.035787 hw_loss 0.266311 lr 0.00025808 rank 1
2023-03-01 18:49:40,836 DEBUG TRAIN Batch 55/16500 loss 5.697246 loss_att 7.844773 loss_ctc 8.381244 loss_rnnt 4.856662 hw_loss 0.099772 lr 0.00025808 rank 0
2023-03-01 18:50:03,311 DEBUG TRAIN Batch 55/16600 loss 9.729420 loss_att 10.613619 loss_ctc 15.266711 loss_rnnt 8.755719 hw_loss 0.109789 lr 0.00025807 rank 0
2023-03-01 18:50:03,313 DEBUG TRAIN Batch 55/16600 loss 8.955036 loss_att 12.786549 loss_ctc 13.485558 loss_rnnt 7.477788 hw_loss 0.200393 lr 0.00025807 rank 1
2023-03-01 18:50:38,355 DEBUG TRAIN Batch 55/16700 loss 5.259315 loss_att 7.323020 loss_ctc 7.189545 loss_rnnt 4.542956 hw_loss 0.086725 lr 0.00025806 rank 1
2023-03-01 18:50:38,401 DEBUG TRAIN Batch 55/16700 loss 7.884769 loss_att 8.349383 loss_ctc 11.467155 loss_rnnt 7.147719 hw_loss 0.312142 lr 0.00025806 rank 0
2023-03-01 18:51:01,417 DEBUG TRAIN Batch 55/16800 loss 4.730052 loss_att 7.083966 loss_ctc 6.370436 loss_rnnt 3.920209 hw_loss 0.225640 lr 0.00025806 rank 1
2023-03-01 18:51:01,417 DEBUG TRAIN Batch 55/16800 loss 3.882289 loss_att 7.103159 loss_ctc 5.264419 loss_rnnt 2.927953 hw_loss 0.236021 lr 0.00025806 rank 0
2023-03-01 18:51:24,020 DEBUG TRAIN Batch 55/16900 loss 2.707192 loss_att 6.643019 loss_ctc 5.229896 loss_rnnt 1.471002 hw_loss 0.211245 lr 0.00025805 rank 1
2023-03-01 18:51:24,021 DEBUG TRAIN Batch 55/16900 loss 8.820384 loss_att 12.503393 loss_ctc 15.227332 loss_rnnt 7.109372 hw_loss 0.225281 lr 0.00025805 rank 0
2023-03-01 18:51:46,701 DEBUG TRAIN Batch 55/17000 loss 5.382975 loss_att 8.032943 loss_ctc 8.388094 loss_rnnt 4.349419 hw_loss 0.192897 lr 0.00025804 rank 1
2023-03-01 18:51:46,702 DEBUG TRAIN Batch 55/17000 loss 13.139059 loss_att 13.848372 loss_ctc 21.358873 loss_rnnt 11.789215 hw_loss 0.210014 lr 0.00025804 rank 0
2023-03-01 18:52:23,312 DEBUG TRAIN Batch 55/17100 loss 5.718973 loss_att 7.345358 loss_ctc 8.262846 loss_rnnt 4.931005 hw_loss 0.231577 lr 0.00025803 rank 1
2023-03-01 18:52:23,314 DEBUG TRAIN Batch 55/17100 loss 6.288982 loss_att 9.661683 loss_ctc 9.309599 loss_rnnt 5.144599 hw_loss 0.125802 lr 0.00025803 rank 0
2023-03-01 18:52:45,239 DEBUG TRAIN Batch 55/17200 loss 4.812906 loss_att 8.159877 loss_ctc 9.821590 loss_rnnt 3.430714 hw_loss 0.084326 lr 0.00025802 rank 1
2023-03-01 18:52:45,240 DEBUG TRAIN Batch 55/17200 loss 8.632674 loss_att 11.004951 loss_ctc 20.671925 loss_rnnt 6.527457 hw_loss 0.047865 lr 0.00025802 rank 0
2023-03-01 18:53:07,821 DEBUG TRAIN Batch 55/17300 loss 5.568601 loss_att 10.364641 loss_ctc 11.838696 loss_rnnt 3.731660 hw_loss 0.078225 lr 0.00025801 rank 1
2023-03-01 18:53:07,821 DEBUG TRAIN Batch 55/17300 loss 4.621968 loss_att 5.829836 loss_ctc 6.558454 loss_rnnt 3.998737 hw_loss 0.231487 lr 0.00025801 rank 0
2023-03-01 18:53:30,567 DEBUG TRAIN Batch 55/17400 loss 7.995959 loss_att 7.978972 loss_ctc 11.363365 loss_rnnt 7.511482 hw_loss 0.072913 lr 0.00025800 rank 1
2023-03-01 18:53:30,570 DEBUG TRAIN Batch 55/17400 loss 7.599995 loss_att 10.000032 loss_ctc 9.387524 loss_rnnt 6.749519 hw_loss 0.247746 lr 0.00025800 rank 0
2023-03-01 18:54:06,108 DEBUG TRAIN Batch 55/17500 loss 2.821876 loss_att 5.347903 loss_ctc 4.370302 loss_rnnt 1.986401 hw_loss 0.232148 lr 0.00025800 rank 1
2023-03-01 18:54:06,110 DEBUG TRAIN Batch 55/17500 loss 4.713367 loss_att 10.451609 loss_ctc 11.340010 loss_rnnt 2.627797 hw_loss 0.101942 lr 0.00025800 rank 0
2023-03-01 18:54:28,679 DEBUG TRAIN Batch 55/17600 loss 7.552413 loss_att 11.303994 loss_ctc 9.475491 loss_rnnt 6.484087 hw_loss 0.115499 lr 0.00025799 rank 1
2023-03-01 18:54:28,681 DEBUG TRAIN Batch 55/17600 loss 2.580183 loss_att 4.272424 loss_ctc 3.860774 loss_rnnt 1.993898 hw_loss 0.144545 lr 0.00025799 rank 0
2023-03-01 18:54:50,520 DEBUG TRAIN Batch 55/17700 loss 8.550613 loss_att 10.317670 loss_ctc 14.645239 loss_rnnt 7.284820 hw_loss 0.187060 lr 0.00025798 rank 1
2023-03-01 18:54:50,520 DEBUG TRAIN Batch 55/17700 loss 7.523894 loss_att 9.157841 loss_ctc 16.342018 loss_rnnt 5.883390 hw_loss 0.258683 lr 0.00025798 rank 0
2023-03-01 18:55:25,862 DEBUG TRAIN Batch 55/17800 loss 9.575608 loss_att 11.849346 loss_ctc 14.941202 loss_rnnt 8.293322 hw_loss 0.210237 lr 0.00025797 rank 1
2023-03-01 18:55:25,864 DEBUG TRAIN Batch 55/17800 loss 11.367351 loss_att 13.791840 loss_ctc 18.080410 loss_rnnt 9.929024 hw_loss 0.109415 lr 0.00025797 rank 0
2023-03-01 18:55:48,120 DEBUG TRAIN Batch 55/17900 loss 6.773419 loss_att 10.100187 loss_ctc 8.717405 loss_rnnt 5.771821 hw_loss 0.144462 lr 0.00025796 rank 1
2023-03-01 18:55:48,120 DEBUG TRAIN Batch 55/17900 loss 10.080878 loss_att 12.950109 loss_ctc 18.702452 loss_rnnt 8.232553 hw_loss 0.234254 lr 0.00025796 rank 0
2023-03-01 18:56:10,892 DEBUG TRAIN Batch 55/18000 loss 7.193593 loss_att 9.767550 loss_ctc 10.268814 loss_rnnt 6.156236 hw_loss 0.211005 lr 0.00025795 rank 1
2023-03-01 18:56:10,894 DEBUG TRAIN Batch 55/18000 loss 6.429378 loss_att 9.196847 loss_ctc 12.877678 loss_rnnt 4.927700 hw_loss 0.165770 lr 0.00025795 rank 0
2023-03-01 18:56:33,148 DEBUG TRAIN Batch 55/18100 loss 1.963082 loss_att 4.921503 loss_ctc 2.966620 loss_rnnt 1.167619 hw_loss 0.131202 lr 0.00025794 rank 1
2023-03-01 18:56:33,149 DEBUG TRAIN Batch 55/18100 loss 4.155536 loss_att 8.708138 loss_ctc 8.792057 loss_rnnt 2.510062 hw_loss 0.218907 lr 0.00025794 rank 0
2023-03-01 18:57:08,791 DEBUG TRAIN Batch 55/18200 loss 12.518305 loss_att 17.762495 loss_ctc 25.285364 loss_rnnt 9.679347 hw_loss 0.164710 lr 0.00025794 rank 1
2023-03-01 18:57:08,792 DEBUG TRAIN Batch 55/18200 loss 8.739198 loss_att 12.402525 loss_ctc 20.493374 loss_rnnt 6.389847 hw_loss 0.092741 lr 0.00025794 rank 0
2023-03-01 18:57:31,325 DEBUG TRAIN Batch 55/18300 loss 4.422570 loss_att 6.179503 loss_ctc 9.260389 loss_rnnt 3.337981 hw_loss 0.165300 lr 0.00025793 rank 1
2023-03-01 18:57:31,327 DEBUG TRAIN Batch 55/18300 loss 7.716971 loss_att 11.242941 loss_ctc 14.504014 loss_rnnt 5.993369 hw_loss 0.212754 lr 0.00025793 rank 0
2023-03-01 18:57:53,659 DEBUG TRAIN Batch 55/18400 loss 6.667338 loss_att 9.525521 loss_ctc 11.314180 loss_rnnt 5.368297 hw_loss 0.202173 lr 0.00025792 rank 1
2023-03-01 18:57:53,660 DEBUG TRAIN Batch 55/18400 loss 3.934673 loss_att 5.815211 loss_ctc 5.719200 loss_rnnt 3.148592 hw_loss 0.322566 lr 0.00025792 rank 0
2023-03-01 18:58:27,742 DEBUG TRAIN Batch 55/18500 loss 10.419254 loss_att 14.319784 loss_ctc 17.595825 loss_rnnt 8.600536 hw_loss 0.153255 lr 0.00025791 rank 1
2023-03-01 18:58:27,744 DEBUG TRAIN Batch 55/18500 loss 7.579947 loss_att 9.051577 loss_ctc 12.708787 loss_rnnt 6.437737 hw_loss 0.307573 lr 0.00025791 rank 0
2023-03-01 18:58:51,572 DEBUG TRAIN Batch 55/18600 loss 5.437360 loss_att 7.346187 loss_ctc 8.216354 loss_rnnt 4.551702 hw_loss 0.250052 lr 0.00025790 rank 0
2023-03-01 18:58:51,572 DEBUG TRAIN Batch 55/18600 loss 6.242767 loss_att 9.039811 loss_ctc 8.508990 loss_rnnt 5.306444 hw_loss 0.140160 lr 0.00025790 rank 1
2023-03-01 18:59:13,711 DEBUG TRAIN Batch 55/18700 loss 9.135736 loss_att 11.356791 loss_ctc 15.803039 loss_rnnt 7.699205 hw_loss 0.193775 lr 0.00025789 rank 1
2023-03-01 18:59:13,711 DEBUG TRAIN Batch 55/18700 loss 6.541280 loss_att 8.058634 loss_ctc 11.542463 loss_rnnt 5.417926 hw_loss 0.286985 lr 0.00025789 rank 0
2023-03-01 18:59:36,113 DEBUG TRAIN Batch 55/18800 loss 6.616365 loss_att 7.428293 loss_ctc 8.721406 loss_rnnt 6.117928 hw_loss 0.103838 lr 0.00025788 rank 1
2023-03-01 18:59:36,117 DEBUG TRAIN Batch 55/18800 loss 2.633423 loss_att 4.193488 loss_ctc 5.362118 loss_rnnt 1.848356 hw_loss 0.204801 lr 0.00025788 rank 0
2023-03-01 19:00:12,395 DEBUG TRAIN Batch 55/18900 loss 3.407070 loss_att 5.959175 loss_ctc 7.156214 loss_rnnt 2.300597 hw_loss 0.180311 lr 0.00025788 rank 1
2023-03-01 19:00:12,397 DEBUG TRAIN Batch 55/18900 loss 1.721592 loss_att 4.373370 loss_ctc 3.083796 loss_rnnt 0.893860 hw_loss 0.217029 lr 0.00025788 rank 0
2023-03-01 19:00:34,758 DEBUG TRAIN Batch 55/19000 loss 9.408276 loss_att 10.631682 loss_ctc 17.331722 loss_rnnt 7.980694 hw_loss 0.237074 lr 0.00025787 rank 1
2023-03-01 19:00:34,759 DEBUG TRAIN Batch 55/19000 loss 12.181553 loss_att 11.778128 loss_ctc 15.324751 loss_rnnt 11.777143 hw_loss 0.123753 lr 0.00025787 rank 0
2023-03-01 19:00:57,683 DEBUG TRAIN Batch 55/19100 loss 2.338995 loss_att 5.208616 loss_ctc 2.524318 loss_rnnt 1.628607 hw_loss 0.209539 lr 0.00025786 rank 1
2023-03-01 19:00:57,684 DEBUG TRAIN Batch 55/19100 loss 13.308823 loss_att 15.470310 loss_ctc 19.834724 loss_rnnt 11.855095 hw_loss 0.283703 lr 0.00025786 rank 0
2023-03-01 19:01:33,181 DEBUG TRAIN Batch 55/19200 loss 8.042287 loss_att 11.892414 loss_ctc 10.071012 loss_rnnt 6.921718 hw_loss 0.150088 lr 0.00025785 rank 1
2023-03-01 19:01:33,183 DEBUG TRAIN Batch 55/19200 loss 5.725392 loss_att 7.885626 loss_ctc 8.942397 loss_rnnt 4.731094 hw_loss 0.249969 lr 0.00025785 rank 0
2023-03-01 19:01:55,924 DEBUG TRAIN Batch 55/19300 loss 11.504845 loss_att 13.591330 loss_ctc 21.185492 loss_rnnt 9.720556 hw_loss 0.142946 lr 0.00025784 rank 1
2023-03-01 19:01:55,926 DEBUG TRAIN Batch 55/19300 loss 5.997216 loss_att 7.360339 loss_ctc 8.070405 loss_rnnt 5.307364 hw_loss 0.264003 lr 0.00025784 rank 0
2023-03-01 19:02:18,101 DEBUG TRAIN Batch 55/19400 loss 7.039588 loss_att 9.474477 loss_ctc 13.065938 loss_rnnt 5.662959 hw_loss 0.161510 lr 0.00025783 rank 1
2023-03-01 19:02:18,102 DEBUG TRAIN Batch 55/19400 loss 7.673642 loss_att 10.929278 loss_ctc 13.464633 loss_rnnt 6.194379 hw_loss 0.105007 lr 0.00025783 rank 0
2023-03-01 19:02:40,680 DEBUG TRAIN Batch 55/19500 loss 4.810828 loss_att 10.139665 loss_ctc 10.833145 loss_rnnt 2.810892 hw_loss 0.245989 lr 0.00025782 rank 1
2023-03-01 19:02:40,682 DEBUG TRAIN Batch 55/19500 loss 4.627608 loss_att 6.636377 loss_ctc 8.792629 loss_rnnt 3.610761 hw_loss 0.112045 lr 0.00025782 rank 0
2023-03-01 19:03:15,462 DEBUG TRAIN Batch 55/19600 loss 9.883516 loss_att 11.806950 loss_ctc 15.928116 loss_rnnt 8.554076 hw_loss 0.260264 lr 0.00025782 rank 1
2023-03-01 19:03:15,464 DEBUG TRAIN Batch 55/19600 loss 5.218454 loss_att 8.509310 loss_ctc 10.619526 loss_rnnt 3.751555 hw_loss 0.166096 lr 0.00025782 rank 0
2023-03-01 19:03:37,227 DEBUG TRAIN Batch 55/19700 loss 3.029412 loss_att 6.478771 loss_ctc 4.784193 loss_rnnt 2.031274 hw_loss 0.139305 lr 0.00025781 rank 0
2023-03-01 19:03:37,227 DEBUG TRAIN Batch 55/19700 loss 2.723240 loss_att 5.452976 loss_ctc 3.094474 loss_rnnt 2.077604 hw_loss 0.094108 lr 0.00025781 rank 1
2023-03-01 19:03:59,912 DEBUG TRAIN Batch 55/19800 loss 3.179005 loss_att 5.083422 loss_ctc 5.172997 loss_rnnt 2.449761 hw_loss 0.154677 lr 0.00025780 rank 1
2023-03-01 19:03:59,914 DEBUG TRAIN Batch 55/19800 loss 7.474670 loss_att 9.531487 loss_ctc 13.047762 loss_rnnt 6.260239 hw_loss 0.112481 lr 0.00025780 rank 0
2023-03-01 19:04:34,825 DEBUG TRAIN Batch 55/19900 loss 7.751387 loss_att 9.014610 loss_ctc 16.702168 loss_rnnt 6.213458 hw_loss 0.172214 lr 0.00025779 rank 1
2023-03-01 19:04:34,826 DEBUG TRAIN Batch 55/19900 loss 7.165203 loss_att 8.205332 loss_ctc 8.410144 loss_rnnt 6.605728 hw_loss 0.347731 lr 0.00025779 rank 0
2023-03-01 19:04:57,420 DEBUG TRAIN Batch 55/20000 loss 5.056357 loss_att 6.990913 loss_ctc 6.431804 loss_rnnt 4.385391 hw_loss 0.188740 lr 0.00025778 rank 1
2023-03-01 19:04:57,423 DEBUG TRAIN Batch 55/20000 loss 8.262808 loss_att 12.049583 loss_ctc 17.416039 loss_rnnt 6.253247 hw_loss 0.059576 lr 0.00025778 rank 0
2023-03-01 19:05:19,534 DEBUG TRAIN Batch 55/20100 loss 6.613059 loss_att 10.006435 loss_ctc 10.638206 loss_rnnt 5.326155 hw_loss 0.134140 lr 0.00025777 rank 1
2023-03-01 19:05:19,537 DEBUG TRAIN Batch 55/20100 loss 6.737165 loss_att 9.746139 loss_ctc 11.906088 loss_rnnt 5.393142 hw_loss 0.099448 lr 0.00025777 rank 0
2023-03-01 19:05:41,570 DEBUG TRAIN Batch 55/20200 loss 6.942379 loss_att 9.522685 loss_ctc 13.766068 loss_rnnt 5.409584 hw_loss 0.200454 lr 0.00025776 rank 1
2023-03-01 19:05:41,570 DEBUG TRAIN Batch 55/20200 loss 6.059690 loss_att 7.831064 loss_ctc 8.587580 loss_rnnt 5.207715 hw_loss 0.301215 lr 0.00025776 rank 0
2023-03-01 19:06:17,036 DEBUG TRAIN Batch 55/20300 loss 13.428627 loss_att 17.179981 loss_ctc 23.627996 loss_rnnt 11.193295 hw_loss 0.234647 lr 0.00025776 rank 1
2023-03-01 19:06:17,038 DEBUG TRAIN Batch 55/20300 loss 4.563427 loss_att 7.232325 loss_ctc 8.105642 loss_rnnt 3.475293 hw_loss 0.153863 lr 0.00025776 rank 0
2023-03-01 19:06:39,440 DEBUG TRAIN Batch 55/20400 loss 5.541673 loss_att 9.374031 loss_ctc 9.115008 loss_rnnt 4.201071 hw_loss 0.183160 lr 0.00025775 rank 1
2023-03-01 19:06:39,442 DEBUG TRAIN Batch 55/20400 loss 5.919334 loss_att 8.800584 loss_ctc 8.016385 loss_rnnt 4.952172 hw_loss 0.208697 lr 0.00025775 rank 0
2023-03-01 19:07:02,183 DEBUG TRAIN Batch 55/20500 loss 1.924927 loss_att 5.921441 loss_ctc 4.199871 loss_rnnt 0.790654 hw_loss 0.059333 lr 0.00025774 rank 1
2023-03-01 19:07:02,184 DEBUG TRAIN Batch 55/20500 loss 3.562143 loss_att 7.179695 loss_ctc 7.541916 loss_rnnt 2.165621 hw_loss 0.266953 lr 0.00025774 rank 0
2023-03-01 19:07:35,316 DEBUG TRAIN Batch 55/20600 loss 7.836186 loss_att 9.683274 loss_ctc 12.234638 loss_rnnt 6.746482 hw_loss 0.250923 lr 0.00025773 rank 1
2023-03-01 19:07:35,362 DEBUG TRAIN Batch 55/20600 loss 5.845673 loss_att 6.370193 loss_ctc 8.620327 loss_rnnt 5.236887 hw_loss 0.251114 lr 0.00025773 rank 0
2023-03-01 19:07:57,654 DEBUG TRAIN Batch 55/20700 loss 8.769545 loss_att 11.287002 loss_ctc 15.886736 loss_rnnt 7.232689 hw_loss 0.158259 lr 0.00025772 rank 1
2023-03-01 19:07:57,655 DEBUG TRAIN Batch 55/20700 loss 8.428926 loss_att 9.343414 loss_ctc 10.365523 loss_rnnt 7.900452 hw_loss 0.163807 lr 0.00025772 rank 0
2023-03-01 19:08:20,597 DEBUG TRAIN Batch 55/20800 loss 5.803444 loss_att 8.731168 loss_ctc 7.561629 loss_rnnt 4.909994 hw_loss 0.137775 lr 0.00025771 rank 1
2023-03-01 19:08:20,600 DEBUG TRAIN Batch 55/20800 loss 2.204316 loss_att 6.942046 loss_ctc 3.229080 loss_rnnt 1.081438 hw_loss 0.072558 lr 0.00025771 rank 0
2023-03-01 19:08:43,023 DEBUG TRAIN Batch 55/20900 loss 6.544216 loss_att 8.320789 loss_ctc 9.250706 loss_rnnt 5.704125 hw_loss 0.232332 lr 0.00025770 rank 0
2023-03-01 19:08:43,024 DEBUG TRAIN Batch 55/20900 loss 9.778892 loss_att 10.987555 loss_ctc 15.320233 loss_rnnt 8.635796 hw_loss 0.304719 lr 0.00025770 rank 1
2023-03-01 19:09:15,987 DEBUG TRAIN Batch 55/21000 loss 1.422747 loss_att 3.202572 loss_ctc 2.783794 loss_rnnt 0.788870 hw_loss 0.180823 lr 0.00025770 rank 1
2023-03-01 19:09:15,988 DEBUG TRAIN Batch 55/21000 loss 4.755271 loss_att 8.184250 loss_ctc 8.666158 loss_rnnt 3.437284 hw_loss 0.207638 lr 0.00025770 rank 0
2023-03-01 19:09:39,100 DEBUG TRAIN Batch 55/21100 loss 5.806086 loss_att 8.619762 loss_ctc 11.459384 loss_rnnt 4.310430 hw_loss 0.335903 lr 0.00025769 rank 1
2023-03-01 19:09:39,102 DEBUG TRAIN Batch 55/21100 loss 9.932267 loss_att 11.960536 loss_ctc 17.411251 loss_rnnt 8.429188 hw_loss 0.187926 lr 0.00025769 rank 0
2023-03-01 19:10:01,976 DEBUG TRAIN Batch 55/21200 loss 12.038603 loss_att 12.814578 loss_ctc 16.570793 loss_rnnt 11.194962 hw_loss 0.157789 lr 0.00025768 rank 1
2023-03-01 19:10:01,978 DEBUG TRAIN Batch 55/21200 loss 5.929235 loss_att 8.008802 loss_ctc 11.540869 loss_rnnt 4.667580 hw_loss 0.182856 lr 0.00025768 rank 0
2023-03-01 19:10:24,463 DEBUG TRAIN Batch 55/21300 loss 5.924676 loss_att 9.910962 loss_ctc 13.402900 loss_rnnt 4.020669 hw_loss 0.205600 lr 0.00025767 rank 1
2023-03-01 19:10:24,466 DEBUG TRAIN Batch 55/21300 loss 3.134031 loss_att 6.610863 loss_ctc 3.496336 loss_rnnt 2.265193 hw_loss 0.234683 lr 0.00025767 rank 0
2023-03-01 19:10:57,946 DEBUG TRAIN Batch 55/21400 loss 7.026893 loss_att 9.537724 loss_ctc 13.882523 loss_rnnt 5.491663 hw_loss 0.223087 lr 0.00025766 rank 1
2023-03-01 19:10:57,947 DEBUG TRAIN Batch 55/21400 loss 5.954930 loss_att 10.372513 loss_ctc 14.869009 loss_rnnt 3.843573 hw_loss 0.073680 lr 0.00025766 rank 0
2023-03-01 19:11:20,601 DEBUG TRAIN Batch 55/21500 loss 4.543256 loss_att 6.673906 loss_ctc 7.460124 loss_rnnt 3.553224 hw_loss 0.328098 lr 0.00025765 rank 1
2023-03-01 19:11:20,602 DEBUG TRAIN Batch 55/21500 loss 13.350247 loss_att 15.108264 loss_ctc 19.781788 loss_rnnt 12.005060 hw_loss 0.255082 lr 0.00025765 rank 0
2023-03-01 19:11:42,490 DEBUG TRAIN Batch 55/21600 loss 6.419520 loss_att 11.170611 loss_ctc 13.631404 loss_rnnt 4.446917 hw_loss 0.114001 lr 0.00025764 rank 1
2023-03-01 19:11:42,493 DEBUG TRAIN Batch 55/21600 loss 11.276554 loss_att 11.684320 loss_ctc 16.250565 loss_rnnt 10.465126 hw_loss 0.125012 lr 0.00025764 rank 0
2023-03-01 19:12:15,977 DEBUG TRAIN Batch 55/21700 loss 9.282890 loss_att 15.930250 loss_ctc 17.437716 loss_rnnt 6.762586 hw_loss 0.194104 lr 0.00025764 rank 1
2023-03-01 19:12:15,979 DEBUG TRAIN Batch 55/21700 loss 3.271711 loss_att 5.981363 loss_ctc 5.081796 loss_rnnt 2.406703 hw_loss 0.153249 lr 0.00025764 rank 0
2023-03-01 19:12:38,109 DEBUG TRAIN Batch 55/21800 loss 10.935443 loss_att 12.282822 loss_ctc 21.432964 loss_rnnt 9.137938 hw_loss 0.240674 lr 0.00025763 rank 1
2023-03-01 19:12:38,109 DEBUG TRAIN Batch 55/21800 loss 17.126490 loss_att 20.761093 loss_ctc 33.207481 loss_rnnt 14.174100 hw_loss 0.152507 lr 0.00025763 rank 0
2023-03-01 19:13:00,137 DEBUG TRAIN Batch 55/21900 loss 2.345865 loss_att 4.322918 loss_ctc 5.620979 loss_rnnt 1.419270 hw_loss 0.177191 lr 0.00025762 rank 1
2023-03-01 19:13:00,139 DEBUG TRAIN Batch 55/21900 loss 3.923755 loss_att 9.881783 loss_ctc 7.052156 loss_rnnt 2.258147 hw_loss 0.106655 lr 0.00025762 rank 0
2023-03-01 19:13:22,609 DEBUG TRAIN Batch 55/22000 loss 7.110396 loss_att 8.240509 loss_ctc 9.179729 loss_rnnt 6.428499 hw_loss 0.337430 lr 0.00025761 rank 1
2023-03-01 19:13:22,610 DEBUG TRAIN Batch 55/22000 loss 4.321270 loss_att 7.503680 loss_ctc 7.433639 loss_rnnt 3.193830 hw_loss 0.142453 lr 0.00025761 rank 0
2023-03-01 19:13:55,414 DEBUG TRAIN Batch 55/22100 loss 5.966193 loss_att 8.360825 loss_ctc 14.520908 loss_rnnt 4.316669 hw_loss 0.056193 lr 0.00025760 rank 0
2023-03-01 19:13:55,415 DEBUG TRAIN Batch 55/22100 loss 8.332450 loss_att 10.415083 loss_ctc 15.116627 loss_rnnt 6.835220 hw_loss 0.330275 lr 0.00025760 rank 1
2023-03-01 19:14:17,577 DEBUG TRAIN Batch 55/22200 loss 10.433534 loss_att 15.766077 loss_ctc 11.795391 loss_rnnt 9.017252 hw_loss 0.315358 lr 0.00025759 rank 1
2023-03-01 19:14:17,578 DEBUG TRAIN Batch 55/22200 loss 1.412664 loss_att 4.364983 loss_ctc 2.742067 loss_rnnt 0.486525 hw_loss 0.297042 lr 0.00025759 rank 0
2023-03-01 19:14:40,033 DEBUG TRAIN Batch 55/22300 loss 2.755514 loss_att 5.489295 loss_ctc 5.447926 loss_rnnt 1.778947 hw_loss 0.132792 lr 0.00025758 rank 1
2023-03-01 19:14:40,035 DEBUG TRAIN Batch 55/22300 loss 5.293909 loss_att 9.336490 loss_ctc 13.071925 loss_rnnt 3.356243 hw_loss 0.172652 lr 0.00025758 rank 0
2023-03-01 19:15:13,063 DEBUG TRAIN Batch 55/22400 loss 6.692626 loss_att 12.387255 loss_ctc 14.213114 loss_rnnt 4.430732 hw_loss 0.225443 lr 0.00025758 rank 1
2023-03-01 19:15:13,065 DEBUG TRAIN Batch 55/22400 loss 8.656277 loss_att 11.120128 loss_ctc 16.367102 loss_rnnt 7.041917 hw_loss 0.175274 lr 0.00025758 rank 0
2023-03-01 19:15:35,361 DEBUG TRAIN Batch 55/22500 loss 10.255271 loss_att 13.448952 loss_ctc 15.661885 loss_rnnt 8.806576 hw_loss 0.167020 lr 0.00025757 rank 1
2023-03-01 19:15:35,361 DEBUG TRAIN Batch 55/22500 loss 3.918744 loss_att 6.223660 loss_ctc 6.155821 loss_rnnt 3.063047 hw_loss 0.180818 lr 0.00025757 rank 0
2023-03-01 19:15:58,041 DEBUG TRAIN Batch 55/22600 loss 7.123330 loss_att 9.603031 loss_ctc 10.487687 loss_rnnt 6.061987 hw_loss 0.219041 lr 0.00025756 rank 1
2023-03-01 19:15:58,044 DEBUG TRAIN Batch 55/22600 loss 4.677355 loss_att 9.244669 loss_ctc 9.175888 loss_rnnt 3.072573 hw_loss 0.171589 lr 0.00025756 rank 0
2023-03-01 19:16:20,716 DEBUG TRAIN Batch 55/22700 loss 17.295479 loss_att 20.524706 loss_ctc 24.100197 loss_rnnt 15.608109 hw_loss 0.251678 lr 0.00025755 rank 1
2023-03-01 19:16:20,718 DEBUG TRAIN Batch 55/22700 loss 5.664664 loss_att 7.185456 loss_ctc 8.332588 loss_rnnt 4.887146 hw_loss 0.220570 lr 0.00025755 rank 0
2023-03-01 19:16:54,319 DEBUG TRAIN Batch 55/22800 loss 5.952645 loss_att 8.347607 loss_ctc 7.534795 loss_rnnt 5.168524 hw_loss 0.176578 lr 0.00025754 rank 0
2023-03-01 19:16:54,321 DEBUG TRAIN Batch 55/22800 loss 10.006019 loss_att 12.217454 loss_ctc 12.824733 loss_rnnt 9.141193 hw_loss 0.087581 lr 0.00025754 rank 1
2023-03-01 19:17:16,779 DEBUG TRAIN Batch 55/22900 loss 7.463151 loss_att 9.059855 loss_ctc 8.152849 loss_rnnt 6.978845 hw_loss 0.136885 lr 0.00025753 rank 1
2023-03-01 19:17:16,779 DEBUG TRAIN Batch 55/22900 loss 8.601418 loss_att 11.844997 loss_ctc 13.111939 loss_rnnt 7.316317 hw_loss 0.065593 lr 0.00025753 rank 0
2023-03-01 19:17:39,310 DEBUG TRAIN Batch 55/23000 loss 10.580528 loss_att 12.998299 loss_ctc 18.389992 loss_rnnt 9.003117 hw_loss 0.098616 lr 0.00025752 rank 1
2023-03-01 19:17:39,314 DEBUG TRAIN Batch 55/23000 loss 7.951334 loss_att 10.261223 loss_ctc 15.152588 loss_rnnt 6.418478 hw_loss 0.207584 lr 0.00025752 rank 0
2023-03-01 19:18:12,610 DEBUG TRAIN Batch 55/23100 loss 5.099801 loss_att 8.562435 loss_ctc 7.864537 loss_rnnt 3.899197 hw_loss 0.261461 lr 0.00025752 rank 1
2023-03-01 19:18:12,612 DEBUG TRAIN Batch 55/23100 loss 5.488830 loss_att 7.809930 loss_ctc 10.743888 loss_rnnt 4.243009 hw_loss 0.151738 lr 0.00025752 rank 0
2023-03-01 19:18:35,640 DEBUG TRAIN Batch 55/23200 loss 4.394523 loss_att 7.522295 loss_ctc 7.547379 loss_rnnt 3.253158 hw_loss 0.178932 lr 0.00025751 rank 1
2023-03-01 19:18:35,641 DEBUG TRAIN Batch 55/23200 loss 8.382115 loss_att 8.138528 loss_ctc 14.028419 loss_rnnt 7.483603 hw_loss 0.364480 lr 0.00025751 rank 0
2023-03-01 19:18:57,947 DEBUG TRAIN Batch 55/23300 loss 12.089449 loss_att 16.352188 loss_ctc 24.865026 loss_rnnt 9.471494 hw_loss 0.116244 lr 0.00025750 rank 1
2023-03-01 19:18:57,949 DEBUG TRAIN Batch 55/23300 loss 2.134613 loss_att 6.202881 loss_ctc 4.132105 loss_rnnt 0.873612 hw_loss 0.339402 lr 0.00025750 rank 0
2023-03-01 19:19:21,291 DEBUG TRAIN Batch 55/23400 loss 7.438593 loss_att 8.117928 loss_ctc 11.451821 loss_rnnt 6.650201 hw_loss 0.220179 lr 0.00025749 rank 1
2023-03-01 19:19:21,293 DEBUG TRAIN Batch 55/23400 loss 7.198495 loss_att 11.368088 loss_ctc 15.455200 loss_rnnt 5.168232 hw_loss 0.178970 lr 0.00025749 rank 0
2023-03-01 19:19:53,583 DEBUG TRAIN Batch 55/23500 loss 7.693001 loss_att 10.225994 loss_ctc 9.609751 loss_rnnt 6.840374 hw_loss 0.169616 lr 0.00025748 rank 1
2023-03-01 19:19:53,586 DEBUG TRAIN Batch 55/23500 loss 3.963177 loss_att 7.675443 loss_ctc 9.595943 loss_rnnt 2.399951 hw_loss 0.130759 lr 0.00025748 rank 0
2023-03-01 19:20:15,744 DEBUG TRAIN Batch 55/23600 loss 8.785513 loss_att 18.094400 loss_ctc 24.638083 loss_rnnt 4.697034 hw_loss 0.211924 lr 0.00025747 rank 1
2023-03-01 19:20:15,745 DEBUG TRAIN Batch 55/23600 loss 7.549988 loss_att 10.560869 loss_ctc 16.714634 loss_rnnt 5.520802 hw_loss 0.384479 lr 0.00025747 rank 0
2023-03-01 19:20:38,161 DEBUG TRAIN Batch 55/23700 loss 4.880748 loss_att 6.918146 loss_ctc 6.868500 loss_rnnt 4.128202 hw_loss 0.150060 lr 0.00025746 rank 1
2023-03-01 19:20:38,163 DEBUG TRAIN Batch 55/23700 loss 7.859921 loss_att 9.020608 loss_ctc 15.376956 loss_rnnt 6.511631 hw_loss 0.213527 lr 0.00025746 rank 0
2023-03-01 19:21:00,758 DEBUG TRAIN Batch 55/23800 loss 4.202711 loss_att 7.093091 loss_ctc 6.053679 loss_rnnt 3.247367 hw_loss 0.244636 lr 0.00025746 rank 1
2023-03-01 19:21:00,761 DEBUG TRAIN Batch 55/23800 loss 5.177613 loss_att 7.044983 loss_ctc 6.150211 loss_rnnt 4.531580 hw_loss 0.267898 lr 0.00025746 rank 0
2023-03-01 19:21:34,094 DEBUG TRAIN Batch 55/23900 loss 9.023995 loss_att 12.741146 loss_ctc 15.532513 loss_rnnt 7.345479 hw_loss 0.126156 lr 0.00025745 rank 1
2023-03-01 19:21:34,096 DEBUG TRAIN Batch 55/23900 loss 2.722135 loss_att 4.473115 loss_ctc 3.827346 loss_rnnt 2.144639 hw_loss 0.149886 lr 0.00025745 rank 0
2023-03-01 19:21:56,632 DEBUG TRAIN Batch 55/24000 loss 6.661932 loss_att 9.239424 loss_ctc 11.313811 loss_rnnt 5.389873 hw_loss 0.255581 lr 0.00025744 rank 1
2023-03-01 19:21:56,634 DEBUG TRAIN Batch 55/24000 loss 8.947163 loss_att 9.922653 loss_ctc 18.558475 loss_rnnt 7.395241 hw_loss 0.141215 lr 0.00025744 rank 0
2023-03-01 19:22:18,973 DEBUG TRAIN Batch 55/24100 loss 4.069854 loss_att 7.801552 loss_ctc 8.119331 loss_rnnt 2.674670 hw_loss 0.204215 lr 0.00025743 rank 1
2023-03-01 19:22:18,975 DEBUG TRAIN Batch 55/24100 loss 5.714109 loss_att 9.576587 loss_ctc 16.004961 loss_rnnt 3.472569 hw_loss 0.181745 lr 0.00025743 rank 0
2023-03-01 19:22:51,563 DEBUG TRAIN Batch 55/24200 loss 4.196016 loss_att 7.009777 loss_ctc 8.995773 loss_rnnt 2.900490 hw_loss 0.174011 lr 0.00025742 rank 1
2023-03-01 19:22:51,566 DEBUG TRAIN Batch 55/24200 loss 8.607308 loss_att 10.040981 loss_ctc 12.602175 loss_rnnt 7.672193 hw_loss 0.216998 lr 0.00025742 rank 0
2023-03-01 19:23:14,069 DEBUG TRAIN Batch 55/24300 loss 9.858178 loss_att 13.611720 loss_ctc 13.708296 loss_rnnt 8.420287 hw_loss 0.325939 lr 0.00025741 rank 1
2023-03-01 19:23:14,071 DEBUG TRAIN Batch 55/24300 loss 12.467144 loss_att 16.968655 loss_ctc 22.194952 loss_rnnt 10.174998 hw_loss 0.177753 lr 0.00025741 rank 0
2023-03-01 19:23:35,967 DEBUG TRAIN Batch 55/24400 loss 6.903373 loss_att 8.101796 loss_ctc 11.857795 loss_rnnt 5.902846 hw_loss 0.187972 lr 0.00025740 rank 1
2023-03-01 19:23:35,967 DEBUG TRAIN Batch 55/24400 loss 6.610538 loss_att 8.049306 loss_ctc 12.260208 loss_rnnt 5.452177 hw_loss 0.219971 lr 0.00025740 rank 0
2023-03-01 19:23:58,195 DEBUG TRAIN Batch 55/24500 loss 9.391416 loss_att 13.211231 loss_ctc 15.288451 loss_rnnt 7.796512 hw_loss 0.083754 lr 0.00025740 rank 1
2023-03-01 19:23:58,196 DEBUG TRAIN Batch 55/24500 loss 4.366273 loss_att 7.511539 loss_ctc 10.034482 loss_rnnt 2.844818 hw_loss 0.256201 lr 0.00025740 rank 0
2023-03-01 19:24:31,391 DEBUG TRAIN Batch 55/24600 loss 1.402999 loss_att 4.125630 loss_ctc 3.033714 loss_rnnt 0.512011 hw_loss 0.241938 lr 0.00025739 rank 0
2023-03-01 19:24:31,391 DEBUG TRAIN Batch 55/24600 loss 5.654757 loss_att 7.779700 loss_ctc 8.959574 loss_rnnt 4.666985 hw_loss 0.229013 lr 0.00025739 rank 1
2023-03-01 19:24:53,586 DEBUG TRAIN Batch 55/24700 loss 4.319501 loss_att 7.300130 loss_ctc 7.807944 loss_rnnt 3.153738 hw_loss 0.195959 lr 0.00025738 rank 0
2023-03-01 19:24:53,586 DEBUG TRAIN Batch 55/24700 loss 9.615419 loss_att 10.215839 loss_ctc 15.600429 loss_rnnt 8.525096 hw_loss 0.322947 lr 0.00025738 rank 1
2023-03-01 19:25:15,372 DEBUG TRAIN Batch 55/24800 loss 5.299866 loss_att 8.532269 loss_ctc 9.871356 loss_rnnt 3.910889 hw_loss 0.249309 lr 0.00025737 rank 1
2023-03-01 19:25:15,373 DEBUG TRAIN Batch 55/24800 loss 3.530817 loss_att 5.700802 loss_ctc 6.138308 loss_rnnt 2.667536 hw_loss 0.153034 lr 0.00025737 rank 0
2023-03-01 19:25:48,433 DEBUG TRAIN Batch 55/24900 loss 1.912170 loss_att 4.709719 loss_ctc 7.369508 loss_rnnt 0.547849 hw_loss 0.144687 lr 0.00025736 rank 1
2023-03-01 19:25:48,433 DEBUG TRAIN Batch 55/24900 loss 3.025437 loss_att 5.842345 loss_ctc 6.460519 loss_rnnt 1.880181 hw_loss 0.232245 lr 0.00025736 rank 0
2023-03-01 19:26:10,597 DEBUG TRAIN Batch 55/25000 loss 14.599907 loss_att 16.910349 loss_ctc 25.506750 loss_rnnt 12.592818 hw_loss 0.170163 lr 0.00025735 rank 1
2023-03-01 19:26:10,599 DEBUG TRAIN Batch 55/25000 loss 3.780105 loss_att 5.901488 loss_ctc 6.091722 loss_rnnt 2.893813 hw_loss 0.288375 lr 0.00025735 rank 0
2023-03-01 19:26:32,446 DEBUG TRAIN Batch 55/25100 loss 5.420467 loss_att 8.304959 loss_ctc 6.873357 loss_rnnt 4.553520 hw_loss 0.180621 lr 0.00025735 rank 1
2023-03-01 19:26:32,448 DEBUG TRAIN Batch 55/25100 loss 3.894139 loss_att 8.104784 loss_ctc 6.331993 loss_rnnt 2.595854 hw_loss 0.245827 lr 0.00025735 rank 0
2023-03-01 19:26:54,772 DEBUG TRAIN Batch 55/25200 loss 3.842475 loss_att 5.945172 loss_ctc 7.620059 loss_rnnt 2.775982 hw_loss 0.266768 lr 0.00025734 rank 1
2023-03-01 19:26:54,773 DEBUG TRAIN Batch 55/25200 loss 4.978483 loss_att 9.363710 loss_ctc 11.947447 loss_rnnt 3.125696 hw_loss 0.087276 lr 0.00025734 rank 0
2023-03-01 19:27:28,743 DEBUG TRAIN Batch 55/25300 loss 5.470988 loss_att 7.257524 loss_ctc 11.339019 loss_rnnt 4.108275 hw_loss 0.418127 lr 0.00025733 rank 1
2023-03-01 19:27:28,744 DEBUG TRAIN Batch 55/25300 loss 9.748036 loss_att 12.111058 loss_ctc 12.299240 loss_rnnt 8.877775 hw_loss 0.107806 lr 0.00025733 rank 0
2023-03-01 19:27:50,584 DEBUG TRAIN Batch 55/25400 loss 4.740590 loss_att 6.967149 loss_ctc 6.666766 loss_rnnt 3.944839 hw_loss 0.175527 lr 0.00025732 rank 1
2023-03-01 19:27:50,586 DEBUG TRAIN Batch 55/25400 loss 5.814662 loss_att 6.858070 loss_ctc 5.577427 loss_rnnt 5.506701 hw_loss 0.245456 lr 0.00025732 rank 0
2023-03-01 19:28:12,814 DEBUG TRAIN Batch 55/25500 loss 12.671704 loss_att 17.698866 loss_ctc 22.187847 loss_rnnt 10.249786 hw_loss 0.276876 lr 0.00025731 rank 1
2023-03-01 19:28:12,818 DEBUG TRAIN Batch 55/25500 loss 6.797455 loss_att 9.238050 loss_ctc 8.851274 loss_rnnt 5.951755 hw_loss 0.157011 lr 0.00025731 rank 0
2023-03-01 19:28:46,382 DEBUG TRAIN Batch 55/25600 loss 11.298892 loss_att 17.909260 loss_ctc 24.606949 loss_rnnt 7.930643 hw_loss 0.509564 lr 0.00025730 rank 1
2023-03-01 19:28:46,385 DEBUG TRAIN Batch 55/25600 loss 4.869264 loss_att 6.508540 loss_ctc 8.194440 loss_rnnt 3.964240 hw_loss 0.250897 lr 0.00025730 rank 0
2023-03-01 19:29:08,444 DEBUG TRAIN Batch 55/25700 loss 8.300387 loss_att 10.399005 loss_ctc 16.170702 loss_rnnt 6.692208 hw_loss 0.260776 lr 0.00025729 rank 1
2023-03-01 19:29:08,447 DEBUG TRAIN Batch 55/25700 loss 4.478479 loss_att 6.597130 loss_ctc 7.116939 loss_rnnt 3.618539 hw_loss 0.158278 lr 0.00025729 rank 0
2023-03-01 19:29:30,889 DEBUG TRAIN Batch 55/25800 loss 12.614446 loss_att 14.167994 loss_ctc 16.583900 loss_rnnt 11.663589 hw_loss 0.207911 lr 0.00025729 rank 1
2023-03-01 19:29:30,892 DEBUG TRAIN Batch 55/25800 loss 11.148074 loss_att 13.963985 loss_ctc 21.503044 loss_rnnt 9.124284 hw_loss 0.149897 lr 0.00025729 rank 0
2023-03-01 19:29:54,037 DEBUG TRAIN Batch 55/25900 loss 7.593335 loss_att 10.920939 loss_ctc 13.145261 loss_rnnt 6.083474 hw_loss 0.195156 lr 0.00025728 rank 1
2023-03-01 19:29:54,038 DEBUG TRAIN Batch 55/25900 loss 4.165637 loss_att 5.770773 loss_ctc 6.081749 loss_rnnt 3.468553 hw_loss 0.226077 lr 0.00025728 rank 0
2023-03-01 19:30:27,176 DEBUG TRAIN Batch 55/26000 loss 9.624933 loss_att 12.303799 loss_ctc 11.156125 loss_rnnt 8.832687 hw_loss 0.098088 lr 0.00025727 rank 0
2023-03-01 19:30:27,187 DEBUG TRAIN Batch 55/26000 loss 11.106394 loss_att 14.436496 loss_ctc 12.869665 loss_rnnt 10.139788 hw_loss 0.122782 lr 0.00025727 rank 1
2023-03-01 19:30:49,385 DEBUG TRAIN Batch 55/26100 loss 3.916682 loss_att 6.108314 loss_ctc 5.131578 loss_rnnt 3.181267 hw_loss 0.253319 lr 0.00025726 rank 1
2023-03-01 19:30:49,387 DEBUG TRAIN Batch 55/26100 loss 10.157353 loss_att 11.802906 loss_ctc 17.471859 loss_rnnt 8.747823 hw_loss 0.197161 lr 0.00025726 rank 0
2023-03-01 19:31:12,027 DEBUG TRAIN Batch 55/26200 loss 10.201404 loss_att 12.127637 loss_ctc 13.947969 loss_rnnt 9.287907 hw_loss 0.053829 lr 0.00025725 rank 1
2023-03-01 19:31:12,030 DEBUG TRAIN Batch 55/26200 loss 4.660517 loss_att 7.436680 loss_ctc 6.221426 loss_rnnt 3.787913 hw_loss 0.204845 lr 0.00025725 rank 0
2023-03-01 19:31:46,042 DEBUG TRAIN Batch 55/26300 loss 3.997714 loss_att 9.300310 loss_ctc 6.843865 loss_rnnt 2.493032 hw_loss 0.121265 lr 0.00025724 rank 1
2023-03-01 19:31:46,043 DEBUG TRAIN Batch 55/26300 loss 5.653579 loss_att 7.743813 loss_ctc 10.093501 loss_rnnt 4.559977 hw_loss 0.156686 lr 0.00025724 rank 0
2023-03-01 19:32:07,727 DEBUG TRAIN Batch 55/26400 loss 3.747312 loss_att 8.091384 loss_ctc 9.503197 loss_rnnt 1.987667 hw_loss 0.231336 lr 0.00025723 rank 1
2023-03-01 19:32:07,728 DEBUG TRAIN Batch 55/26400 loss 4.339564 loss_att 9.426684 loss_ctc 9.390723 loss_rnnt 2.596138 hw_loss 0.098463 lr 0.00025723 rank 0
2023-03-01 19:32:29,831 DEBUG TRAIN Batch 55/26500 loss 7.847187 loss_att 11.563128 loss_ctc 12.239771 loss_rnnt 6.358762 hw_loss 0.299172 lr 0.00025723 rank 1
2023-03-01 19:32:29,833 DEBUG TRAIN Batch 55/26500 loss 4.260929 loss_att 6.065906 loss_ctc 10.223023 loss_rnnt 3.016201 hw_loss 0.166475 lr 0.00025723 rank 0
2023-03-01 19:32:52,223 DEBUG TRAIN Batch 55/26600 loss 17.509075 loss_att 19.357450 loss_ctc 22.386780 loss_rnnt 16.376259 hw_loss 0.211461 lr 0.00025722 rank 1
2023-03-01 19:32:52,226 DEBUG TRAIN Batch 55/26600 loss 4.001052 loss_att 6.548004 loss_ctc 7.042209 loss_rnnt 2.964828 hw_loss 0.227525 lr 0.00025722 rank 0
2023-03-01 19:33:24,516 DEBUG TRAIN Batch 55/26700 loss 8.230107 loss_att 11.839213 loss_ctc 13.239014 loss_rnnt 6.756744 hw_loss 0.156916 lr 0.00025721 rank 1
2023-03-01 19:33:24,517 DEBUG TRAIN Batch 55/26700 loss 5.088097 loss_att 7.046227 loss_ctc 8.630993 loss_rnnt 4.114621 hw_loss 0.205244 lr 0.00025721 rank 0
2023-03-01 19:33:47,360 DEBUG TRAIN Batch 55/26800 loss 2.231631 loss_att 6.214866 loss_ctc 6.490333 loss_rnnt 0.710517 hw_loss 0.293700 lr 0.00025720 rank 1
2023-03-01 19:33:47,361 DEBUG TRAIN Batch 55/26800 loss 3.335224 loss_att 4.978944 loss_ctc 6.657571 loss_rnnt 2.417355 hw_loss 0.274022 lr 0.00025720 rank 0
2023-03-01 19:34:10,332 DEBUG TRAIN Batch 55/26900 loss 6.420848 loss_att 11.503910 loss_ctc 11.124592 loss_rnnt 4.723486 hw_loss 0.100471 lr 0.00025719 rank 1
2023-03-01 19:34:10,333 DEBUG TRAIN Batch 55/26900 loss 4.859418 loss_att 8.434443 loss_ctc 9.547529 loss_rnnt 3.465292 hw_loss 0.101323 lr 0.00025719 rank 0
2023-03-01 19:34:32,558 DEBUG TRAIN Batch 55/27000 loss 5.996322 loss_att 9.836126 loss_ctc 8.554741 loss_rnnt 4.726439 hw_loss 0.301498 lr 0.00025718 rank 1
2023-03-01 19:34:32,558 DEBUG TRAIN Batch 55/27000 loss 5.661894 loss_att 8.904943 loss_ctc 9.696170 loss_rnnt 4.311858 hw_loss 0.306604 lr 0.00025718 rank 0
2023-03-01 19:35:04,769 DEBUG TRAIN Batch 55/27100 loss 2.018533 loss_att 4.097919 loss_ctc 3.645256 loss_rnnt 1.338495 hw_loss 0.088622 lr 0.00025717 rank 1
2023-03-01 19:35:04,771 DEBUG TRAIN Batch 55/27100 loss 6.959811 loss_att 12.174119 loss_ctc 10.912291 loss_rnnt 5.324973 hw_loss 0.121835 lr 0.00025717 rank 0
2023-03-01 19:35:27,228 DEBUG TRAIN Batch 55/27200 loss 6.858636 loss_att 9.491844 loss_ctc 11.579023 loss_rnnt 5.603327 hw_loss 0.186155 lr 0.00025717 rank 1
2023-03-01 19:35:27,230 DEBUG TRAIN Batch 55/27200 loss 10.724465 loss_att 17.262686 loss_ctc 22.072384 loss_rnnt 7.873922 hw_loss 0.055956 lr 0.00025717 rank 0
2023-03-01 19:35:49,640 DEBUG TRAIN Batch 55/27300 loss 7.421964 loss_att 7.712126 loss_ctc 11.449738 loss_rnnt 6.696418 hw_loss 0.244646 lr 0.00025716 rank 1
2023-03-01 19:35:49,643 DEBUG TRAIN Batch 55/27300 loss 13.040631 loss_att 14.299394 loss_ctc 16.281466 loss_rnnt 12.262867 hw_loss 0.176065 lr 0.00025716 rank 0
2023-03-01 19:36:22,982 DEBUG TRAIN Batch 55/27400 loss 7.331225 loss_att 15.575701 loss_ctc 13.716326 loss_rnnt 4.744917 hw_loss 0.161375 lr 0.00025715 rank 1
2023-03-01 19:36:22,985 DEBUG TRAIN Batch 55/27400 loss 4.882740 loss_att 9.107950 loss_ctc 8.480892 loss_rnnt 3.476617 hw_loss 0.152487 lr 0.00025715 rank 0
2023-03-01 19:36:45,526 DEBUG TRAIN Batch 55/27500 loss 13.024977 loss_att 15.558197 loss_ctc 17.497684 loss_rnnt 11.782269 hw_loss 0.261943 lr 0.00025714 rank 1
2023-03-01 19:36:45,527 DEBUG TRAIN Batch 55/27500 loss 6.347482 loss_att 9.066339 loss_ctc 9.270290 loss_rnnt 5.278183 hw_loss 0.254662 lr 0.00025714 rank 0
2023-03-01 19:37:08,231 DEBUG TRAIN Batch 55/27600 loss 11.192897 loss_att 15.271145 loss_ctc 18.774521 loss_rnnt 9.232782 hw_loss 0.250467 lr 0.00025713 rank 1
2023-03-01 19:37:08,233 DEBUG TRAIN Batch 55/27600 loss 9.233665 loss_att 10.018387 loss_ctc 14.409322 loss_rnnt 8.211377 hw_loss 0.328603 lr 0.00025713 rank 0
2023-03-01 19:37:30,750 DEBUG TRAIN Batch 55/27700 loss 7.965449 loss_att 11.407168 loss_ctc 15.302132 loss_rnnt 6.279166 hw_loss 0.036966 lr 0.00025712 rank 1
2023-03-01 19:37:30,751 DEBUG TRAIN Batch 55/27700 loss 2.927849 loss_att 6.620818 loss_ctc 7.154644 loss_rnnt 1.515068 hw_loss 0.207402 lr 0.00025712 rank 0
2023-03-01 19:38:04,051 DEBUG TRAIN Batch 55/27800 loss 4.700139 loss_att 7.621361 loss_ctc 8.420594 loss_rnnt 3.492636 hw_loss 0.238498 lr 0.00025712 rank 1
2023-03-01 19:38:04,052 DEBUG TRAIN Batch 55/27800 loss 4.884265 loss_att 6.323107 loss_ctc 10.610698 loss_rnnt 3.733432 hw_loss 0.186640 lr 0.00025712 rank 0
2023-03-01 19:38:26,371 DEBUG TRAIN Batch 55/27900 loss 8.795345 loss_att 11.447046 loss_ctc 15.385587 loss_rnnt 7.321800 hw_loss 0.120947 lr 0.00025711 rank 0
2023-03-01 19:38:26,373 DEBUG TRAIN Batch 55/27900 loss 8.341152 loss_att 9.700212 loss_ctc 16.060768 loss_rnnt 6.869394 hw_loss 0.319996 lr 0.00025711 rank 1
2023-03-01 19:38:48,371 DEBUG TRAIN Batch 55/28000 loss 5.794400 loss_att 10.374398 loss_ctc 12.229533 loss_rnnt 3.972248 hw_loss 0.090253 lr 0.00025710 rank 1
2023-03-01 19:38:48,375 DEBUG TRAIN Batch 55/28000 loss 10.309302 loss_att 14.965446 loss_ctc 11.026342 loss_rnnt 9.160959 hw_loss 0.227827 lr 0.00025710 rank 0
2023-03-01 19:39:21,474 DEBUG TRAIN Batch 55/28100 loss 3.640696 loss_att 5.539022 loss_ctc 6.849746 loss_rnnt 2.721533 hw_loss 0.209294 lr 0.00025709 rank 1
2023-03-01 19:39:21,478 DEBUG TRAIN Batch 55/28100 loss 9.598664 loss_att 12.231676 loss_ctc 15.868427 loss_rnnt 8.123924 hw_loss 0.210319 lr 0.00025709 rank 0
2023-03-01 19:39:44,532 DEBUG TRAIN Batch 55/28200 loss 3.612798 loss_att 6.261171 loss_ctc 8.139701 loss_rnnt 2.389747 hw_loss 0.168356 lr 0.00025708 rank 1
2023-03-01 19:39:44,534 DEBUG TRAIN Batch 55/28200 loss 7.052822 loss_att 8.249806 loss_ctc 16.351633 loss_rnnt 5.432054 hw_loss 0.265367 lr 0.00025708 rank 0
2023-03-01 19:40:06,673 DEBUG TRAIN Batch 55/28300 loss 6.502405 loss_att 11.225266 loss_ctc 12.169204 loss_rnnt 4.644915 hw_loss 0.295022 lr 0.00025707 rank 1
2023-03-01 19:40:06,673 DEBUG TRAIN Batch 55/28300 loss 5.601424 loss_att 8.956492 loss_ctc 10.642959 loss_rnnt 4.176147 hw_loss 0.153861 lr 0.00025707 rank 0
2023-03-01 19:40:29,072 DEBUG TRAIN Batch 55/28400 loss 6.011326 loss_att 9.002205 loss_ctc 8.436029 loss_rnnt 4.963813 hw_loss 0.236331 lr 0.00025706 rank 1
2023-03-01 19:40:29,074 DEBUG TRAIN Batch 55/28400 loss 5.232522 loss_att 6.852662 loss_ctc 8.174442 loss_rnnt 4.472744 hw_loss 0.081550 lr 0.00025706 rank 0
2023-03-01 19:41:03,300 DEBUG TRAIN Batch 55/28500 loss 11.306908 loss_att 13.280169 loss_ctc 16.302362 loss_rnnt 10.148668 hw_loss 0.182862 lr 0.00025706 rank 1
2023-03-01 19:41:03,302 DEBUG TRAIN Batch 55/28500 loss 3.788326 loss_att 6.255541 loss_ctc 6.801888 loss_rnnt 2.735826 hw_loss 0.294841 lr 0.00025706 rank 0
2023-03-01 19:41:25,332 DEBUG TRAIN Batch 55/28600 loss 3.926747 loss_att 7.426769 loss_ctc 5.688263 loss_rnnt 2.947749 hw_loss 0.082734 lr 0.00025705 rank 1
2023-03-01 19:41:25,332 DEBUG TRAIN Batch 55/28600 loss 7.517911 loss_att 11.122910 loss_ctc 17.120375 loss_rnnt 5.401412 hw_loss 0.215945 lr 0.00025705 rank 0
2023-03-01 19:41:47,467 DEBUG TRAIN Batch 55/28700 loss 7.658970 loss_att 8.709180 loss_ctc 10.669531 loss_rnnt 7.027208 hw_loss 0.038085 lr 0.00025704 rank 1
2023-03-01 19:41:47,469 DEBUG TRAIN Batch 55/28700 loss 5.957634 loss_att 9.338357 loss_ctc 14.637684 loss_rnnt 4.028615 hw_loss 0.179127 lr 0.00025704 rank 0
2023-03-01 19:42:21,613 DEBUG TRAIN Batch 55/28800 loss 14.099769 loss_att 14.432922 loss_ctc 21.580433 loss_rnnt 12.901337 hw_loss 0.251963 lr 0.00025703 rank 1
2023-03-01 19:42:21,614 DEBUG TRAIN Batch 55/28800 loss 9.037378 loss_att 11.179189 loss_ctc 12.809469 loss_rnnt 7.986938 hw_loss 0.223377 lr 0.00025703 rank 0
2023-03-01 19:42:43,885 DEBUG TRAIN Batch 55/28900 loss 10.816245 loss_att 11.996350 loss_ctc 15.733040 loss_rnnt 9.852036 hw_loss 0.136153 lr 0.00025702 rank 1
2023-03-01 19:42:43,887 DEBUG TRAIN Batch 55/28900 loss 7.247246 loss_att 8.822542 loss_ctc 7.940399 loss_rnnt 6.711398 hw_loss 0.240692 lr 0.00025702 rank 0
2023-03-01 19:43:06,224 DEBUG TRAIN Batch 55/29000 loss 8.282401 loss_att 9.190261 loss_ctc 14.931713 loss_rnnt 7.076231 hw_loss 0.258795 lr 0.00025701 rank 1
2023-03-01 19:43:06,225 DEBUG TRAIN Batch 55/29000 loss 8.731009 loss_att 11.502554 loss_ctc 9.708858 loss_rnnt 7.939526 hw_loss 0.200241 lr 0.00025701 rank 0
2023-03-01 19:43:28,526 DEBUG TRAIN Batch 55/29100 loss 5.700626 loss_att 8.192945 loss_ctc 10.267801 loss_rnnt 4.514903 hw_loss 0.146817 lr 0.00025701 rank 1
2023-03-01 19:43:28,528 DEBUG TRAIN Batch 55/29100 loss 2.540345 loss_att 4.815374 loss_ctc 6.049469 loss_rnnt 1.513197 hw_loss 0.195485 lr 0.00025701 rank 0
2023-03-01 19:44:01,455 DEBUG TRAIN Batch 55/29200 loss 8.199322 loss_att 10.223272 loss_ctc 14.264539 loss_rnnt 6.927691 hw_loss 0.109021 lr 0.00025700 rank 0
2023-03-01 19:44:01,464 DEBUG TRAIN Batch 55/29200 loss 6.945181 loss_att 7.057184 loss_ctc 10.417807 loss_rnnt 6.310764 hw_loss 0.279375 lr 0.00025700 rank 1
2023-03-01 19:44:23,924 DEBUG TRAIN Batch 55/29300 loss 6.737444 loss_att 11.197739 loss_ctc 16.203133 loss_rnnt 4.523951 hw_loss 0.111267 lr 0.00025699 rank 1
2023-03-01 19:44:23,928 DEBUG TRAIN Batch 55/29300 loss 6.168676 loss_att 7.419178 loss_ctc 10.561580 loss_rnnt 5.189549 hw_loss 0.268698 lr 0.00025699 rank 0
2023-03-01 19:44:46,700 DEBUG TRAIN Batch 55/29400 loss 6.540648 loss_att 9.885620 loss_ctc 6.844184 loss_rnnt 5.812540 hw_loss 0.034953 lr 0.00025698 rank 1
2023-03-01 19:44:46,704 DEBUG TRAIN Batch 55/29400 loss 2.573222 loss_att 4.873905 loss_ctc 4.685573 loss_rnnt 1.733982 hw_loss 0.182733 lr 0.00025698 rank 0
2023-03-01 19:45:20,690 DEBUG TRAIN Batch 55/29500 loss 7.657840 loss_att 9.861661 loss_ctc 11.467676 loss_rnnt 6.569243 hw_loss 0.262228 lr 0.00025697 rank 1
2023-03-01 19:45:20,692 DEBUG TRAIN Batch 55/29500 loss 6.138496 loss_att 7.849476 loss_ctc 10.685626 loss_rnnt 5.004423 hw_loss 0.347987 lr 0.00025697 rank 0
2023-03-01 19:45:43,268 DEBUG TRAIN Batch 55/29600 loss 4.443190 loss_att 6.783601 loss_ctc 5.757362 loss_rnnt 3.725940 hw_loss 0.138645 lr 0.00025696 rank 1
2023-03-01 19:45:43,271 DEBUG TRAIN Batch 55/29600 loss 3.111882 loss_att 7.897034 loss_ctc 4.660911 loss_rnnt 1.893717 hw_loss 0.102370 lr 0.00025696 rank 0
2023-03-01 19:46:05,694 DEBUG TRAIN Batch 55/29700 loss 6.514232 loss_att 11.667359 loss_ctc 20.957998 loss_rnnt 3.411142 hw_loss 0.274930 lr 0.00025695 rank 1
2023-03-01 19:46:05,697 DEBUG TRAIN Batch 55/29700 loss 7.219179 loss_att 9.284125 loss_ctc 9.268963 loss_rnnt 6.451328 hw_loss 0.152918 lr 0.00025695 rank 0
2023-03-01 19:46:28,623 DEBUG TRAIN Batch 55/29800 loss 9.260771 loss_att 11.950394 loss_ctc 15.676482 loss_rnnt 7.778922 hw_loss 0.165930 lr 0.00025695 rank 1
2023-03-01 19:46:28,624 DEBUG TRAIN Batch 55/29800 loss 8.683762 loss_att 11.241647 loss_ctc 14.138836 loss_rnnt 7.326444 hw_loss 0.221995 lr 0.00025695 rank 0
2023-03-01 19:47:01,154 DEBUG TRAIN Batch 55/29900 loss 7.018200 loss_att 11.595390 loss_ctc 12.170792 loss_rnnt 5.383560 hw_loss 0.060357 lr 0.00025694 rank 1
2023-03-01 19:47:01,157 DEBUG TRAIN Batch 55/29900 loss 3.326000 loss_att 5.966637 loss_ctc 7.731250 loss_rnnt 2.115621 hw_loss 0.177911 lr 0.00025694 rank 0
2023-03-01 19:47:24,396 DEBUG TRAIN Batch 55/30000 loss 4.405478 loss_att 5.465298 loss_ctc 5.522479 loss_rnnt 3.951342 hw_loss 0.174823 lr 0.00025693 rank 1
2023-03-01 19:47:24,398 DEBUG TRAIN Batch 55/30000 loss 2.348069 loss_att 6.521571 loss_ctc 3.971838 loss_rnnt 1.144847 hw_loss 0.285035 lr 0.00025693 rank 0
2023-03-01 19:47:47,164 DEBUG TRAIN Batch 55/30100 loss 2.669844 loss_att 6.567940 loss_ctc 6.991022 loss_rnnt 1.208052 hw_loss 0.198779 lr 0.00025692 rank 1
2023-03-01 19:47:47,165 DEBUG TRAIN Batch 55/30100 loss 6.682940 loss_att 8.198409 loss_ctc 10.213360 loss_rnnt 5.773663 hw_loss 0.253988 lr 0.00025692 rank 0
2023-03-01 19:48:09,399 DEBUG TRAIN Batch 55/30200 loss 8.554430 loss_att 10.168713 loss_ctc 12.457821 loss_rnnt 7.637512 hw_loss 0.138019 lr 0.00025691 rank 1
2023-03-01 19:48:09,401 DEBUG TRAIN Batch 55/30200 loss 5.505931 loss_att 8.996295 loss_ctc 8.760386 loss_rnnt 4.260160 hw_loss 0.213319 lr 0.00025691 rank 0
2023-03-01 19:48:42,381 DEBUG TRAIN Batch 55/30300 loss 4.970285 loss_att 8.515209 loss_ctc 10.432479 loss_rnnt 3.434548 hw_loss 0.184611 lr 0.00025690 rank 1
2023-03-01 19:48:42,383 DEBUG TRAIN Batch 55/30300 loss 3.021057 loss_att 6.851332 loss_ctc 5.299937 loss_rnnt 1.836602 hw_loss 0.214779 lr 0.00025690 rank 0
2023-03-01 19:49:05,442 DEBUG TRAIN Batch 55/30400 loss 8.774996 loss_att 11.117237 loss_ctc 12.675952 loss_rnnt 7.655463 hw_loss 0.245544 lr 0.00025689 rank 1
2023-03-01 19:49:05,443 DEBUG TRAIN Batch 55/30400 loss 10.284971 loss_att 12.462870 loss_ctc 21.803900 loss_rnnt 8.176630 hw_loss 0.256695 lr 0.00025689 rank 0
2023-03-01 19:49:27,659 DEBUG TRAIN Batch 55/30500 loss 12.816450 loss_att 15.122438 loss_ctc 16.046934 loss_rnnt 11.835541 hw_loss 0.166838 lr 0.00025689 rank 1
2023-03-01 19:49:27,661 DEBUG TRAIN Batch 55/30500 loss 7.227834 loss_att 9.443282 loss_ctc 13.676643 loss_rnnt 5.821471 hw_loss 0.193935 lr 0.00025689 rank 0
2023-03-01 19:50:00,311 DEBUG TRAIN Batch 55/30600 loss 8.381155 loss_att 13.212312 loss_ctc 14.431173 loss_rnnt 6.495360 hw_loss 0.211679 lr 0.00025688 rank 1
2023-03-01 19:50:00,313 DEBUG TRAIN Batch 55/30600 loss 8.361255 loss_att 10.000068 loss_ctc 12.792866 loss_rnnt 7.341960 hw_loss 0.188721 lr 0.00025688 rank 0
2023-03-01 19:50:22,651 DEBUG TRAIN Batch 55/30700 loss 1.862085 loss_att 5.642553 loss_ctc 4.388073 loss_rnnt 0.585324 hw_loss 0.344753 lr 0.00025687 rank 1
2023-03-01 19:50:22,653 DEBUG TRAIN Batch 55/30700 loss 17.786194 loss_att 18.745430 loss_ctc 20.399948 loss_rnnt 17.180592 hw_loss 0.122353 lr 0.00025687 rank 0
2023-03-01 19:50:44,863 DEBUG TRAIN Batch 55/30800 loss 5.933827 loss_att 8.957993 loss_ctc 9.002186 loss_rnnt 4.791315 hw_loss 0.241059 lr 0.00025686 rank 1
2023-03-01 19:50:44,865 DEBUG TRAIN Batch 55/30800 loss 5.733956 loss_att 6.900111 loss_ctc 8.251895 loss_rnnt 4.951166 hw_loss 0.400939 lr 0.00025686 rank 0
2023-03-01 19:51:07,291 DEBUG TRAIN Batch 55/30900 loss 7.948956 loss_att 11.464117 loss_ctc 12.549631 loss_rnnt 6.549568 hw_loss 0.155498 lr 0.00025685 rank 1
2023-03-01 19:51:07,293 DEBUG TRAIN Batch 55/30900 loss 5.162757 loss_att 10.255877 loss_ctc 17.367302 loss_rnnt 2.451503 hw_loss 0.122543 lr 0.00025685 rank 0
2023-03-01 19:51:40,728 DEBUG TRAIN Batch 55/31000 loss 3.781499 loss_att 6.232472 loss_ctc 10.161260 loss_rnnt 2.328447 hw_loss 0.210419 lr 0.00025684 rank 1
2023-03-01 19:51:40,729 DEBUG TRAIN Batch 55/31000 loss 6.255985 loss_att 9.177711 loss_ctc 14.839523 loss_rnnt 4.415969 hw_loss 0.208497 lr 0.00025684 rank 0
2023-03-01 19:52:03,189 DEBUG TRAIN Batch 55/31100 loss 5.235586 loss_att 6.577371 loss_ctc 7.262097 loss_rnnt 4.557905 hw_loss 0.260856 lr 0.00025684 rank 1
2023-03-01 19:52:03,191 DEBUG TRAIN Batch 55/31100 loss 11.538156 loss_att 14.035380 loss_ctc 22.628414 loss_rnnt 9.445269 hw_loss 0.215137 lr 0.00025684 rank 0
2023-03-01 19:52:25,531 DEBUG TRAIN Batch 55/31200 loss 4.885405 loss_att 7.710995 loss_ctc 9.568064 loss_rnnt 3.527175 hw_loss 0.316419 lr 0.00025683 rank 1
2023-03-01 19:52:25,534 DEBUG TRAIN Batch 55/31200 loss 10.426734 loss_att 15.270844 loss_ctc 13.660616 loss_rnnt 8.919241 hw_loss 0.201538 lr 0.00025683 rank 0
2023-03-01 19:52:58,742 DEBUG TRAIN Batch 55/31300 loss 6.171324 loss_att 9.707804 loss_ctc 8.456814 loss_rnnt 5.088932 hw_loss 0.131935 lr 0.00025682 rank 1
2023-03-01 19:52:58,744 DEBUG TRAIN Batch 55/31300 loss 4.562928 loss_att 8.792763 loss_ctc 8.304910 loss_rnnt 3.127659 hw_loss 0.169445 lr 0.00025682 rank 0
2023-03-01 19:53:21,446 DEBUG TRAIN Batch 55/31400 loss 7.839131 loss_att 12.256853 loss_ctc 10.163733 loss_rnnt 6.603347 hw_loss 0.079301 lr 0.00025681 rank 1
2023-03-01 19:53:21,447 DEBUG TRAIN Batch 55/31400 loss 7.355882 loss_att 8.227248 loss_ctc 10.733096 loss_rnnt 6.585935 hw_loss 0.272586 lr 0.00025681 rank 0
2023-03-01 19:53:43,668 DEBUG TRAIN Batch 55/31500 loss 3.272065 loss_att 4.779369 loss_ctc 3.955318 loss_rnnt 2.719770 hw_loss 0.299500 lr 0.00025680 rank 1
2023-03-01 19:53:43,672 DEBUG TRAIN Batch 55/31500 loss 14.809754 loss_att 18.749434 loss_ctc 27.771505 loss_rnnt 12.158694 hw_loss 0.252919 lr 0.00025680 rank 0
2023-03-01 19:54:06,434 DEBUG TRAIN Batch 55/31600 loss 13.087223 loss_att 10.951069 loss_ctc 25.469055 loss_rnnt 11.761182 hw_loss 0.191928 lr 0.00025679 rank 1
2023-03-01 19:54:06,436 DEBUG TRAIN Batch 55/31600 loss 5.297686 loss_att 10.604034 loss_ctc 8.503263 loss_rnnt 3.720996 hw_loss 0.165017 lr 0.00025679 rank 0
2023-03-01 19:54:40,485 DEBUG TRAIN Batch 55/31700 loss 12.723082 loss_att 13.684062 loss_ctc 24.982643 loss_rnnt 10.814253 hw_loss 0.153797 lr 0.00025678 rank 1
2023-03-01 19:54:40,487 DEBUG TRAIN Batch 55/31700 loss 2.369160 loss_att 4.286049 loss_ctc 6.252227 loss_rnnt 1.325932 hw_loss 0.266454 lr 0.00025678 rank 0
2023-03-01 19:55:02,756 DEBUG TRAIN Batch 55/31800 loss 2.466196 loss_att 5.590022 loss_ctc 7.199975 loss_rnnt 1.084611 hw_loss 0.235593 lr 0.00025678 rank 1
2023-03-01 19:55:02,757 DEBUG TRAIN Batch 55/31800 loss 9.298435 loss_att 11.020731 loss_ctc 14.453669 loss_rnnt 8.170444 hw_loss 0.180315 lr 0.00025678 rank 0
2023-03-01 19:55:24,989 DEBUG TRAIN Batch 55/31900 loss 5.472759 loss_att 7.773335 loss_ctc 8.959139 loss_rnnt 4.459129 hw_loss 0.166244 lr 0.00025677 rank 1
2023-03-01 19:55:24,989 DEBUG TRAIN Batch 55/31900 loss 3.589989 loss_att 6.362599 loss_ctc 7.677148 loss_rnnt 2.426556 hw_loss 0.119916 lr 0.00025677 rank 0
2023-03-01 19:56:00,226 DEBUG TRAIN Batch 55/32000 loss 8.939884 loss_att 12.114378 loss_ctc 17.509394 loss_rnnt 7.069816 hw_loss 0.173563 lr 0.00025676 rank 1
2023-03-01 19:56:00,228 DEBUG TRAIN Batch 55/32000 loss 5.936595 loss_att 8.351265 loss_ctc 12.297536 loss_rnnt 4.478733 hw_loss 0.237757 lr 0.00025676 rank 0
2023-03-01 19:56:22,611 DEBUG TRAIN Batch 55/32100 loss 9.341222 loss_att 11.935018 loss_ctc 13.118206 loss_rnnt 8.153343 hw_loss 0.310352 lr 0.00025675 rank 1
2023-03-01 19:56:22,612 DEBUG TRAIN Batch 55/32100 loss 6.367736 loss_att 7.457038 loss_ctc 10.156863 loss_rnnt 5.573003 hw_loss 0.134355 lr 0.00025675 rank 0
2023-03-01 19:56:44,843 DEBUG TRAIN Batch 55/32200 loss 14.189877 loss_att 15.424976 loss_ctc 23.648077 loss_rnnt 12.586944 hw_loss 0.177785 lr 0.00025674 rank 1
2023-03-01 19:56:44,844 DEBUG TRAIN Batch 55/32200 loss 3.025594 loss_att 7.460757 loss_ctc 7.487785 loss_rnnt 1.393210 hw_loss 0.281986 lr 0.00025674 rank 0
2023-03-01 19:57:07,655 DEBUG TRAIN Batch 55/32300 loss 7.164379 loss_att 10.836551 loss_ctc 9.758598 loss_rnnt 5.944597 hw_loss 0.261471 lr 0.00025673 rank 1
2023-03-01 19:57:07,655 DEBUG TRAIN Batch 55/32300 loss 7.169264 loss_att 10.669494 loss_ctc 13.567088 loss_rnnt 5.455502 hw_loss 0.301262 lr 0.00025673 rank 0
2023-03-01 19:57:42,556 DEBUG TRAIN Batch 55/32400 loss 10.869225 loss_att 11.199645 loss_ctc 16.855665 loss_rnnt 9.800632 hw_loss 0.383094 lr 0.00025673 rank 1
2023-03-01 19:57:42,560 DEBUG TRAIN Batch 55/32400 loss 10.382269 loss_att 12.892321 loss_ctc 15.154564 loss_rnnt 9.133656 hw_loss 0.206806 lr 0.00025673 rank 0
2023-03-01 19:58:04,734 DEBUG TRAIN Batch 55/32500 loss 6.453118 loss_att 9.017359 loss_ctc 7.500165 loss_rnnt 5.729986 hw_loss 0.132520 lr 0.00025672 rank 1
2023-03-01 19:58:04,736 DEBUG TRAIN Batch 55/32500 loss 4.492176 loss_att 8.933602 loss_ctc 6.575493 loss_rnnt 3.265635 hw_loss 0.113399 lr 0.00025672 rank 0
2023-03-01 19:58:27,353 DEBUG TRAIN Batch 55/32600 loss 11.847368 loss_att 15.042131 loss_ctc 18.202114 loss_rnnt 10.336548 hw_loss 0.046066 lr 0.00025671 rank 1
2023-03-01 19:58:27,356 DEBUG TRAIN Batch 55/32600 loss 6.388343 loss_att 8.922739 loss_ctc 9.178035 loss_rnnt 5.375755 hw_loss 0.250780 lr 0.00025671 rank 0
2023-03-01 19:59:01,809 DEBUG TRAIN Batch 55/32700 loss 6.684723 loss_att 9.614031 loss_ctc 9.273984 loss_rnnt 5.619440 hw_loss 0.251600 lr 0.00025670 rank 1
2023-03-01 19:59:01,834 DEBUG TRAIN Batch 55/32700 loss 9.300074 loss_att 8.943377 loss_ctc 14.101048 loss_rnnt 8.501664 hw_loss 0.430533 lr 0.00025670 rank 0
2023-03-01 19:59:24,037 DEBUG TRAIN Batch 55/32800 loss 5.618458 loss_att 7.613164 loss_ctc 8.410225 loss_rnnt 4.757754 hw_loss 0.167864 lr 0.00025669 rank 1
2023-03-01 19:59:24,038 DEBUG TRAIN Batch 55/32800 loss 2.325733 loss_att 6.888991 loss_ctc 4.048464 loss_rnnt 1.082613 hw_loss 0.188944 lr 0.00025669 rank 0
2023-03-01 19:59:46,722 DEBUG TRAIN Batch 55/32900 loss 11.907351 loss_att 15.568921 loss_ctc 22.671837 loss_rnnt 9.619162 hw_loss 0.226143 lr 0.00025668 rank 1
2023-03-01 19:59:46,724 DEBUG TRAIN Batch 55/32900 loss 5.389160 loss_att 10.158292 loss_ctc 6.501542 loss_rnnt 4.205327 hw_loss 0.153167 lr 0.00025668 rank 0
2023-03-01 20:00:09,405 DEBUG TRAIN Batch 55/33000 loss 8.025133 loss_att 8.396763 loss_ctc 13.040937 loss_rnnt 7.136431 hw_loss 0.273004 lr 0.00025667 rank 1
2023-03-01 20:00:09,407 DEBUG TRAIN Batch 55/33000 loss 6.027450 loss_att 12.368596 loss_ctc 12.136661 loss_rnnt 3.770488 hw_loss 0.326570 lr 0.00025667 rank 0
2023-03-01 20:00:33,441 DEBUG TRAIN Batch 55/33100 loss 2.318773 loss_att 6.217812 loss_ctc 5.714725 loss_rnnt 0.985248 hw_loss 0.189231 lr 0.00025667 rank 1
2023-03-01 20:00:33,442 DEBUG TRAIN Batch 55/33100 loss 2.430606 loss_att 4.849670 loss_ctc 2.975986 loss_rnnt 1.719297 hw_loss 0.290211 lr 0.00025667 rank 0
2023-03-01 20:00:57,201 DEBUG TRAIN Batch 55/33200 loss 6.206439 loss_att 7.814540 loss_ctc 10.614784 loss_rnnt 5.144357 hw_loss 0.286280 lr 0.00025666 rank 0
2023-03-01 20:00:57,202 DEBUG TRAIN Batch 55/33200 loss 11.460243 loss_att 15.199464 loss_ctc 22.346060 loss_rnnt 9.164667 hw_loss 0.180542 lr 0.00025666 rank 1
2023-03-01 20:01:19,554 DEBUG TRAIN Batch 55/33300 loss 5.850532 loss_att 9.661146 loss_ctc 13.371815 loss_rnnt 3.990245 hw_loss 0.178735 lr 0.00025665 rank 1
2023-03-01 20:01:19,554 DEBUG TRAIN Batch 55/33300 loss 15.875284 loss_att 18.630392 loss_ctc 31.107412 loss_rnnt 13.195356 hw_loss 0.183669 lr 0.00025665 rank 0
2023-03-01 20:01:38,334 DEBUG CV Batch 55/0 loss 1.302819 loss_att 1.165421 loss_ctc 1.955021 loss_rnnt 1.089366 hw_loss 0.288698 history loss 1.254566 rank 1
2023-03-01 20:01:38,334 DEBUG CV Batch 55/0 loss 1.302819 loss_att 1.165421 loss_ctc 1.955021 loss_rnnt 1.089366 hw_loss 0.288698 history loss 1.254566 rank 0
2023-03-01 20:01:45,851 DEBUG CV Batch 55/100 loss 3.191813 loss_att 4.286662 loss_ctc 6.589993 loss_rnnt 2.413540 hw_loss 0.199148 history loss 2.999064 rank 0
2023-03-01 20:01:45,860 DEBUG CV Batch 55/100 loss 3.191813 loss_att 4.286662 loss_ctc 6.589993 loss_rnnt 2.413540 hw_loss 0.199148 history loss 2.999064 rank 1
2023-03-01 20:01:55,581 DEBUG CV Batch 55/200 loss 6.491848 loss_att 8.504442 loss_ctc 8.243221 loss_rnnt 5.817140 hw_loss 0.072510 history loss 3.551062 rank 1
2023-03-01 20:01:55,591 DEBUG CV Batch 55/200 loss 6.491848 loss_att 8.504442 loss_ctc 8.243221 loss_rnnt 5.817140 hw_loss 0.072510 history loss 3.551062 rank 0
2023-03-01 20:02:04,035 DEBUG CV Batch 55/300 loss 2.655238 loss_att 3.331206 loss_ctc 5.428164 loss_rnnt 2.008935 hw_loss 0.265100 history loss 3.699343 rank 0
2023-03-01 20:02:04,243 DEBUG CV Batch 55/300 loss 2.655238 loss_att 3.331206 loss_ctc 5.428164 loss_rnnt 2.008935 hw_loss 0.265100 history loss 3.699343 rank 1
2023-03-01 20:02:12,498 DEBUG CV Batch 55/400 loss 20.521675 loss_att 71.478867 loss_ctc 18.106350 loss_rnnt 10.603687 hw_loss 0.091114 history loss 4.517171 rank 0
2023-03-01 20:02:12,642 DEBUG CV Batch 55/400 loss 20.521675 loss_att 71.478867 loss_ctc 18.106350 loss_rnnt 10.603687 hw_loss 0.091114 history loss 4.517171 rank 1
2023-03-01 20:02:19,366 DEBUG CV Batch 55/500 loss 3.269717 loss_att 4.023876 loss_ctc 6.251220 loss_rnnt 2.605164 hw_loss 0.217852 history loss 5.112939 rank 0
2023-03-01 20:02:19,488 DEBUG CV Batch 55/500 loss 3.269717 loss_att 4.023876 loss_ctc 6.251220 loss_rnnt 2.605164 hw_loss 0.217852 history loss 5.112939 rank 1
2023-03-01 20:02:28,066 DEBUG CV Batch 55/600 loss 7.239055 loss_att 6.524756 loss_ctc 9.682654 loss_rnnt 6.839884 hw_loss 0.405407 history loss 6.001005 rank 1
2023-03-01 20:02:28,233 DEBUG CV Batch 55/600 loss 7.239055 loss_att 6.524756 loss_ctc 9.682654 loss_rnnt 6.839884 hw_loss 0.405407 history loss 6.001005 rank 0
2023-03-01 20:02:35,776 DEBUG CV Batch 55/700 loss 14.511363 loss_att 30.512121 loss_ctc 11.958199 loss_rnnt 11.651245 hw_loss 0.000728 history loss 6.546844 rank 1
2023-03-01 20:02:35,989 DEBUG CV Batch 55/700 loss 14.511363 loss_att 30.512121 loss_ctc 11.958199 loss_rnnt 11.651245 hw_loss 0.000728 history loss 6.546844 rank 0
2023-03-01 20:02:43,240 DEBUG CV Batch 55/800 loss 4.959366 loss_att 6.649698 loss_ctc 10.788702 loss_rnnt 3.717976 hw_loss 0.236398 history loss 6.073080 rank 1
2023-03-01 20:02:43,466 DEBUG CV Batch 55/800 loss 4.959366 loss_att 6.649698 loss_ctc 10.788702 loss_rnnt 3.717976 hw_loss 0.236398 history loss 6.073080 rank 0
2023-03-01 20:02:52,965 DEBUG CV Batch 55/900 loss 11.128971 loss_att 11.912008 loss_ctc 18.767656 loss_rnnt 9.826099 hw_loss 0.239574 history loss 5.904503 rank 1
2023-03-01 20:02:53,241 DEBUG CV Batch 55/900 loss 11.128971 loss_att 11.912008 loss_ctc 18.767656 loss_rnnt 9.826099 hw_loss 0.239574 history loss 5.904503 rank 0
2023-03-01 20:03:01,556 DEBUG CV Batch 55/1000 loss 4.719969 loss_att 4.806995 loss_ctc 5.542332 loss_rnnt 4.465506 hw_loss 0.238894 history loss 5.715361 rank 1
2023-03-01 20:03:01,811 DEBUG CV Batch 55/1000 loss 4.719969 loss_att 4.806995 loss_ctc 5.542332 loss_rnnt 4.465506 hw_loss 0.238894 history loss 5.715361 rank 0
2023-03-01 20:03:09,854 DEBUG CV Batch 55/1100 loss 4.561465 loss_att 4.454979 loss_ctc 8.105479 loss_rnnt 3.956955 hw_loss 0.287385 history loss 5.680541 rank 1
2023-03-01 20:03:10,133 DEBUG CV Batch 55/1100 loss 4.561465 loss_att 4.454979 loss_ctc 8.105479 loss_rnnt 3.956955 hw_loss 0.287385 history loss 5.680541 rank 0
2023-03-01 20:03:16,678 DEBUG CV Batch 55/1200 loss 5.793016 loss_att 6.087034 loss_ctc 7.397195 loss_rnnt 5.427906 hw_loss 0.173282 history loss 5.948765 rank 1
2023-03-01 20:03:17,000 DEBUG CV Batch 55/1200 loss 5.793016 loss_att 6.087034 loss_ctc 7.397195 loss_rnnt 5.427906 hw_loss 0.173282 history loss 5.948765 rank 0
2023-03-01 20:03:24,992 DEBUG CV Batch 55/1300 loss 5.004058 loss_att 4.468395 loss_ctc 7.443187 loss_rnnt 4.643723 hw_loss 0.266719 history loss 6.244308 rank 1
2023-03-01 20:03:25,334 DEBUG CV Batch 55/1300 loss 5.004058 loss_att 4.468395 loss_ctc 7.443187 loss_rnnt 4.643723 hw_loss 0.266719 history loss 6.244308 rank 0
2023-03-01 20:03:32,546 DEBUG CV Batch 55/1400 loss 3.966552 loss_att 9.088393 loss_ctc 6.833052 loss_rnnt 2.559595 hw_loss 0.000728 history loss 6.517642 rank 1
2023-03-01 20:03:32,936 DEBUG CV Batch 55/1400 loss 3.966552 loss_att 9.088393 loss_ctc 6.833052 loss_rnnt 2.559595 hw_loss 0.000728 history loss 6.517642 rank 0
2023-03-01 20:03:40,197 DEBUG CV Batch 55/1500 loss 6.972997 loss_att 7.539084 loss_ctc 7.890727 loss_rnnt 6.604222 hw_loss 0.249738 history loss 6.377194 rank 1
2023-03-01 20:03:40,611 DEBUG CV Batch 55/1500 loss 6.972997 loss_att 7.539084 loss_ctc 7.890727 loss_rnnt 6.604222 hw_loss 0.249738 history loss 6.377194 rank 0
2023-03-01 20:03:49,643 DEBUG CV Batch 55/1600 loss 11.020288 loss_att 12.560082 loss_ctc 11.590351 loss_rnnt 10.521615 hw_loss 0.215076 history loss 6.332343 rank 1
2023-03-01 20:03:50,102 DEBUG CV Batch 55/1600 loss 11.020288 loss_att 12.560082 loss_ctc 11.590351 loss_rnnt 10.521615 hw_loss 0.215076 history loss 6.332343 rank 0
2023-03-01 20:03:58,521 DEBUG CV Batch 55/1700 loss 7.438795 loss_att 6.825509 loss_ctc 13.582736 loss_rnnt 6.534747 hw_loss 0.389087 history loss 6.264017 rank 1
2023-03-01 20:03:59,012 DEBUG CV Batch 55/1700 loss 7.438795 loss_att 6.825509 loss_ctc 13.582736 loss_rnnt 6.534747 hw_loss 0.389087 history loss 6.264017 rank 0
2023-03-01 20:04:05,166 INFO Epoch 55 CV info cv_loss 6.242165262694992
2023-03-01 20:04:05,167 INFO Epoch 56 TRAIN info lr 0.0002566428758911699
2023-03-01 20:04:05,168 INFO using accumulate grad, new batch size is 4 times larger than before
2023-03-01 20:04:05,681 INFO Epoch 55 CV info cv_loss 6.2421652637976655
2023-03-01 20:04:05,681 INFO Checkpoint: save to checkpoint exp/2_27_rnnt_bias_loss_2_class_both_finetune_100head/55.pt
2023-03-01 20:04:06,104 INFO Epoch 56 TRAIN info lr 0.00025664693293014066
2023-03-01 20:04:06,106 INFO using accumulate grad, new batch size is 4 times larger than before
2023-03-01 20:04:39,025 DEBUG TRAIN Batch 56/0 loss 4.312840 loss_att 4.771679 loss_ctc 6.964856 loss_rnnt 3.648582 hw_loss 0.410414 lr 0.00025664 rank 1
2023-03-01 20:04:39,035 DEBUG TRAIN Batch 56/0 loss 7.354834 loss_att 7.004553 loss_ctc 9.231241 loss_rnnt 7.077973 hw_loss 0.181366 lr 0.00025665 rank 0
2023-03-01 20:05:00,322 DEBUG TRAIN Batch 56/100 loss 6.223193 loss_att 8.761362 loss_ctc 15.612906 loss_rnnt 4.405041 hw_loss 0.109793 lr 0.00025663 rank 1
2023-03-01 20:05:00,324 DEBUG TRAIN Batch 56/100 loss 3.817541 loss_att 5.952134 loss_ctc 4.691524 loss_rnnt 3.164824 hw_loss 0.204876 lr 0.00025664 rank 0
2023-03-01 20:05:22,017 DEBUG TRAIN Batch 56/200 loss 3.949849 loss_att 6.716497 loss_ctc 6.754113 loss_rnnt 2.875669 hw_loss 0.275526 lr 0.00025663 rank 1
2023-03-01 20:05:22,019 DEBUG TRAIN Batch 56/200 loss 8.897477 loss_att 9.014422 loss_ctc 10.618793 loss_rnnt 8.570201 hw_loss 0.139461 lr 0.00025663 rank 0
2023-03-01 20:05:43,665 DEBUG TRAIN Batch 56/300 loss 4.602071 loss_att 7.823310 loss_ctc 8.191080 loss_rnnt 3.447805 hw_loss 0.059032 lr 0.00025662 rank 1
2023-03-01 20:05:43,667 DEBUG TRAIN Batch 56/300 loss 4.746897 loss_att 6.674792 loss_ctc 5.735892 loss_rnnt 4.161256 hw_loss 0.127866 lr 0.00025662 rank 0
2023-03-01 20:06:15,873 DEBUG TRAIN Batch 56/400 loss 6.930034 loss_att 8.426237 loss_ctc 13.886522 loss_rnnt 5.640960 hw_loss 0.116816 lr 0.00025661 rank 1
2023-03-01 20:06:15,875 DEBUG TRAIN Batch 56/400 loss 10.217858 loss_att 12.411664 loss_ctc 15.255579 loss_rnnt 8.995352 hw_loss 0.210093 lr 0.00025661 rank 0
2023-03-01 20:06:37,884 DEBUG TRAIN Batch 56/500 loss 10.583053 loss_att 15.521914 loss_ctc 15.429733 loss_rnnt 8.897834 hw_loss 0.096042 lr 0.00025660 rank 1
2023-03-01 20:06:37,884 DEBUG TRAIN Batch 56/500 loss 4.279433 loss_att 7.627677 loss_ctc 8.364562 loss_rnnt 3.023487 hw_loss 0.078026 lr 0.00025660 rank 0
2023-03-01 20:07:00,985 DEBUG TRAIN Batch 56/600 loss 4.579859 loss_att 6.300588 loss_ctc 7.755447 loss_rnnt 3.680133 hw_loss 0.247816 lr 0.00025659 rank 1
2023-03-01 20:07:00,987 DEBUG TRAIN Batch 56/600 loss 2.545785 loss_att 5.239909 loss_ctc 4.723689 loss_rnnt 1.604106 hw_loss 0.210874 lr 0.00025660 rank 0
2023-03-01 20:07:23,010 DEBUG TRAIN Batch 56/700 loss 2.324349 loss_att 4.859631 loss_ctc 5.960931 loss_rnnt 1.190654 hw_loss 0.265802 lr 0.00025658 rank 1
2023-03-01 20:07:23,013 DEBUG TRAIN Batch 56/700 loss 4.896931 loss_att 8.070568 loss_ctc 8.005061 loss_rnnt 3.760099 hw_loss 0.164412 lr 0.00025659 rank 0
2023-03-01 20:07:54,339 DEBUG TRAIN Batch 56/800 loss 11.069115 loss_att 13.408009 loss_ctc 12.991237 loss_rnnt 10.266665 hw_loss 0.146981 lr 0.00025657 rank 1
2023-03-01 20:07:54,340 DEBUG TRAIN Batch 56/800 loss 3.133686 loss_att 4.828007 loss_ctc 4.432252 loss_rnnt 2.463726 hw_loss 0.296163 lr 0.00025658 rank 0
2023-03-01 20:08:16,443 DEBUG TRAIN Batch 56/900 loss 2.859713 loss_att 5.947702 loss_ctc 7.392747 loss_rnnt 1.596532 hw_loss 0.077211 lr 0.00025657 rank 1
2023-03-01 20:08:16,444 DEBUG TRAIN Batch 56/900 loss 3.898430 loss_att 7.203775 loss_ctc 6.826544 loss_rnnt 2.809346 hw_loss 0.070498 lr 0.00025657 rank 0
2023-03-01 20:08:37,923 DEBUG TRAIN Batch 56/1000 loss 7.130886 loss_att 9.480765 loss_ctc 13.213992 loss_rnnt 5.794934 hw_loss 0.102927 lr 0.00025656 rank 1
2023-03-01 20:08:37,925 DEBUG TRAIN Batch 56/1000 loss 8.725575 loss_att 12.045373 loss_ctc 11.553566 loss_rnnt 7.555991 hw_loss 0.241052 lr 0.00025656 rank 0
2023-03-01 20:09:09,692 DEBUG TRAIN Batch 56/1100 loss 7.659062 loss_att 10.255553 loss_ctc 13.234659 loss_rnnt 6.267132 hw_loss 0.242285 lr 0.00025655 rank 1
2023-03-01 20:09:09,694 DEBUG TRAIN Batch 56/1100 loss 5.814127 loss_att 7.345655 loss_ctc 10.009365 loss_rnnt 4.853464 hw_loss 0.178112 lr 0.00025655 rank 0
2023-03-01 20:09:31,452 DEBUG TRAIN Batch 56/1200 loss 9.000644 loss_att 9.345454 loss_ctc 13.797183 loss_rnnt 8.188899 hw_loss 0.193583 lr 0.00025654 rank 1
2023-03-01 20:09:31,454 DEBUG TRAIN Batch 56/1200 loss 8.320743 loss_att 10.155049 loss_ctc 12.514879 loss_rnnt 7.355820 hw_loss 0.072832 lr 0.00025655 rank 0
2023-03-01 20:09:53,768 DEBUG TRAIN Batch 56/1300 loss 7.323453 loss_att 8.671916 loss_ctc 7.211902 loss_rnnt 6.982471 hw_loss 0.161558 lr 0.00025653 rank 1
2023-03-01 20:09:53,770 DEBUG TRAIN Batch 56/1300 loss 4.591012 loss_att 9.103836 loss_ctc 12.780905 loss_rnnt 2.536890 hw_loss 0.111695 lr 0.00025654 rank 0
2023-03-01 20:10:15,723 DEBUG TRAIN Batch 56/1400 loss 5.283488 loss_att 8.370858 loss_ctc 9.825068 loss_rnnt 3.966347 hw_loss 0.176481 lr 0.00025652 rank 1
2023-03-01 20:10:15,724 DEBUG TRAIN Batch 56/1400 loss 6.577527 loss_att 9.724694 loss_ctc 10.173102 loss_rnnt 5.403566 hw_loss 0.122094 lr 0.00025653 rank 0
2023-03-01 20:10:47,114 DEBUG TRAIN Batch 56/1500 loss 6.546591 loss_att 11.774390 loss_ctc 14.116491 loss_rnnt 4.321658 hw_loss 0.318850 lr 0.00025652 rank 1
2023-03-01 20:10:47,115 DEBUG TRAIN Batch 56/1500 loss 4.000229 loss_att 8.002951 loss_ctc 8.777210 loss_rnnt 2.421416 hw_loss 0.265008 lr 0.00025652 rank 0
2023-03-01 20:11:08,058 DEBUG TRAIN Batch 56/1600 loss 3.059009 loss_att 6.964295 loss_ctc 5.106947 loss_rnnt 1.837761 hw_loss 0.313372 lr 0.00025651 rank 1
2023-03-01 20:11:08,060 DEBUG TRAIN Batch 56/1600 loss 3.962059 loss_att 6.090315 loss_ctc 7.687088 loss_rnnt 2.898629 hw_loss 0.264578 lr 0.00025651 rank 0
2023-03-01 20:11:29,294 DEBUG TRAIN Batch 56/1700 loss 4.171696 loss_att 6.017327 loss_ctc 6.836121 loss_rnnt 3.306556 hw_loss 0.263917 lr 0.00025650 rank 1
2023-03-01 20:11:29,297 DEBUG TRAIN Batch 56/1700 loss 7.157667 loss_att 8.146683 loss_ctc 8.915800 loss_rnnt 6.635942 hw_loss 0.167819 lr 0.00025650 rank 0
2023-03-01 20:12:01,629 DEBUG TRAIN Batch 56/1800 loss 10.257232 loss_att 13.140467 loss_ctc 14.624367 loss_rnnt 8.969829 hw_loss 0.240884 lr 0.00025649 rank 1
2023-03-01 20:12:01,629 DEBUG TRAIN Batch 56/1800 loss 11.864969 loss_att 15.552813 loss_ctc 22.722836 loss_rnnt 9.580539 hw_loss 0.185900 lr 0.00025649 rank 0
2023-03-01 20:12:23,645 DEBUG TRAIN Batch 56/1900 loss 4.884861 loss_att 5.810098 loss_ctc 8.404117 loss_rnnt 4.081669 hw_loss 0.279207 lr 0.00025648 rank 1
2023-03-01 20:12:23,646 DEBUG TRAIN Batch 56/1900 loss 8.259251 loss_att 9.844239 loss_ctc 12.939579 loss_rnnt 7.205380 hw_loss 0.211554 lr 0.00025649 rank 0
2023-03-01 20:12:44,879 DEBUG TRAIN Batch 56/2000 loss 3.302049 loss_att 8.946550 loss_ctc 6.279294 loss_rnnt 1.690007 hw_loss 0.161578 lr 0.00025647 rank 1
2023-03-01 20:12:44,880 DEBUG TRAIN Batch 56/2000 loss 10.628802 loss_att 10.361707 loss_ctc 11.883872 loss_rnnt 10.404101 hw_loss 0.207708 lr 0.00025648 rank 0
2023-03-01 20:13:06,732 DEBUG TRAIN Batch 56/2100 loss 4.877066 loss_att 8.402687 loss_ctc 7.024818 loss_rnnt 3.810137 hw_loss 0.141445 lr 0.00025647 rank 1
2023-03-01 20:13:06,734 DEBUG TRAIN Batch 56/2100 loss 4.515673 loss_att 8.471960 loss_ctc 8.203447 loss_rnnt 3.142650 hw_loss 0.168867 lr 0.00025647 rank 0
2023-03-01 20:13:38,483 DEBUG TRAIN Batch 56/2200 loss 6.945717 loss_att 9.236853 loss_ctc 10.740671 loss_rnnt 5.891095 hw_loss 0.169502 lr 0.00025646 rank 1
2023-03-01 20:13:38,484 DEBUG TRAIN Batch 56/2200 loss 4.849266 loss_att 7.087857 loss_ctc 7.677362 loss_rnnt 3.920468 hw_loss 0.195000 lr 0.00025646 rank 0
2023-03-01 20:13:59,634 DEBUG TRAIN Batch 56/2300 loss 5.060034 loss_att 7.555149 loss_ctc 7.851299 loss_rnnt 4.137696 hw_loss 0.095899 lr 0.00025645 rank 1
2023-03-01 20:13:59,635 DEBUG TRAIN Batch 56/2300 loss 8.543934 loss_att 13.047108 loss_ctc 22.402504 loss_rnnt 5.719791 hw_loss 0.141934 lr 0.00025645 rank 0
2023-03-01 20:14:21,170 DEBUG TRAIN Batch 56/2400 loss 2.753189 loss_att 4.366199 loss_ctc 2.939240 loss_rnnt 2.327797 hw_loss 0.146220 lr 0.00025644 rank 1
2023-03-01 20:14:21,172 DEBUG TRAIN Batch 56/2400 loss 6.918507 loss_att 10.118197 loss_ctc 14.386652 loss_rnnt 5.183887 hw_loss 0.185494 lr 0.00025644 rank 0
2023-03-01 20:14:53,745 DEBUG TRAIN Batch 56/2500 loss 5.251950 loss_att 6.966174 loss_ctc 7.204768 loss_rnnt 4.520814 hw_loss 0.239842 lr 0.00025643 rank 1
2023-03-01 20:14:53,747 DEBUG TRAIN Batch 56/2500 loss 4.649308 loss_att 5.853612 loss_ctc 9.185576 loss_rnnt 3.714198 hw_loss 0.167651 lr 0.00025644 rank 0
2023-03-01 20:15:15,197 DEBUG TRAIN Batch 56/2600 loss 5.388381 loss_att 6.467893 loss_ctc 7.674434 loss_rnnt 4.744615 hw_loss 0.230731 lr 0.00025642 rank 1
2023-03-01 20:15:15,200 DEBUG TRAIN Batch 56/2600 loss 4.758227 loss_att 7.724460 loss_ctc 6.633930 loss_rnnt 3.795783 hw_loss 0.223321 lr 0.00025643 rank 0
2023-03-01 20:15:36,751 DEBUG TRAIN Batch 56/2700 loss 6.040586 loss_att 9.501614 loss_ctc 12.946203 loss_rnnt 4.381197 hw_loss 0.087063 lr 0.00025641 rank 1
2023-03-01 20:15:36,753 DEBUG TRAIN Batch 56/2700 loss 5.514756 loss_att 6.721832 loss_ctc 5.510732 loss_rnnt 5.196010 hw_loss 0.146001 lr 0.00025642 rank 0
2023-03-01 20:15:58,275 DEBUG TRAIN Batch 56/2800 loss 3.294354 loss_att 5.276578 loss_ctc 4.542912 loss_rnnt 2.613372 hw_loss 0.221370 lr 0.00025641 rank 1
2023-03-01 20:15:58,276 DEBUG TRAIN Batch 56/2800 loss 2.943358 loss_att 6.620876 loss_ctc 6.102989 loss_rnnt 1.708691 hw_loss 0.146024 lr 0.00025641 rank 0
2023-03-01 20:16:30,025 DEBUG TRAIN Batch 56/2900 loss 7.443854 loss_att 12.750051 loss_ctc 14.945398 loss_rnnt 5.335151 hw_loss 0.088608 lr 0.00025640 rank 1
2023-03-01 20:16:30,027 DEBUG TRAIN Batch 56/2900 loss 10.294670 loss_att 11.664880 loss_ctc 14.513294 loss_rnnt 9.378621 hw_loss 0.149108 lr 0.00025640 rank 0
2023-03-01 20:16:51,363 DEBUG TRAIN Batch 56/3000 loss 3.139215 loss_att 5.675042 loss_ctc 5.627507 loss_rnnt 2.155559 hw_loss 0.271348 lr 0.00025639 rank 1
2023-03-01 20:16:51,364 DEBUG TRAIN Batch 56/3000 loss 9.797118 loss_att 12.105570 loss_ctc 19.220242 loss_rnnt 7.917955 hw_loss 0.301978 lr 0.00025639 rank 0
2023-03-01 20:17:13,260 DEBUG TRAIN Batch 56/3100 loss 7.044553 loss_att 7.836786 loss_ctc 9.982525 loss_rnnt 6.386688 hw_loss 0.201918 lr 0.00025638 rank 1
2023-03-01 20:17:13,261 DEBUG TRAIN Batch 56/3100 loss 9.278991 loss_att 9.582379 loss_ctc 12.699583 loss_rnnt 8.678595 hw_loss 0.156825 lr 0.00025638 rank 0
2023-03-01 20:17:35,441 DEBUG TRAIN Batch 56/3200 loss 8.267223 loss_att 11.194994 loss_ctc 12.976919 loss_rnnt 6.887693 hw_loss 0.311283 lr 0.00025637 rank 1
2023-03-01 20:17:35,444 DEBUG TRAIN Batch 56/3200 loss 4.932409 loss_att 10.835217 loss_ctc 9.277715 loss_rnnt 3.116149 hw_loss 0.105608 lr 0.00025638 rank 0
2023-03-01 20:18:07,485 DEBUG TRAIN Batch 56/3300 loss 11.174555 loss_att 14.898645 loss_ctc 17.780336 loss_rnnt 9.446730 hw_loss 0.191692 lr 0.00025636 rank 1
2023-03-01 20:18:07,488 DEBUG TRAIN Batch 56/3300 loss 4.997937 loss_att 10.026562 loss_ctc 12.660637 loss_rnnt 2.888031 hw_loss 0.154666 lr 0.00025637 rank 0
2023-03-01 20:18:29,014 DEBUG TRAIN Batch 56/3400 loss 6.575488 loss_att 9.211723 loss_ctc 11.803475 loss_rnnt 5.232435 hw_loss 0.222637 lr 0.00025636 rank 1
2023-03-01 20:18:29,016 DEBUG TRAIN Batch 56/3400 loss 7.481009 loss_att 8.614072 loss_ctc 6.406255 loss_rnnt 7.255821 hw_loss 0.266018 lr 0.00025636 rank 0
2023-03-01 20:18:50,376 DEBUG TRAIN Batch 56/3500 loss 2.658609 loss_att 5.626883 loss_ctc 5.110644 loss_rnnt 1.615762 hw_loss 0.229228 lr 0.00025635 rank 1
2023-03-01 20:18:50,378 DEBUG TRAIN Batch 56/3500 loss 5.977798 loss_att 7.698991 loss_ctc 9.112405 loss_rnnt 5.077928 hw_loss 0.258158 lr 0.00025635 rank 0
2023-03-01 20:19:23,323 DEBUG TRAIN Batch 56/3600 loss 4.582458 loss_att 8.573450 loss_ctc 9.514060 loss_rnnt 3.040479 hw_loss 0.161690 lr 0.00025634 rank 1
2023-03-01 20:19:23,324 DEBUG TRAIN Batch 56/3600 loss 7.257908 loss_att 8.766777 loss_ctc 7.535686 loss_rnnt 6.783465 hw_loss 0.254309 lr 0.00025634 rank 0
2023-03-01 20:19:45,120 DEBUG TRAIN Batch 56/3700 loss 6.297792 loss_att 7.862082 loss_ctc 10.644633 loss_rnnt 5.262066 hw_loss 0.268665 lr 0.00025633 rank 1
2023-03-01 20:19:45,121 DEBUG TRAIN Batch 56/3700 loss 11.746839 loss_att 13.405186 loss_ctc 22.096428 loss_rnnt 9.956169 hw_loss 0.148227 lr 0.00025633 rank 0
2023-03-01 20:20:07,304 DEBUG TRAIN Batch 56/3800 loss 4.668784 loss_att 7.427228 loss_ctc 11.447844 loss_rnnt 3.019463 hw_loss 0.363295 lr 0.00025632 rank 1
2023-03-01 20:20:07,307 DEBUG TRAIN Batch 56/3800 loss 8.765045 loss_att 10.216629 loss_ctc 19.192238 loss_rnnt 6.964855 hw_loss 0.224214 lr 0.00025633 rank 0
2023-03-01 20:20:28,663 DEBUG TRAIN Batch 56/3900 loss 15.080044 loss_att 18.227543 loss_ctc 21.559631 loss_rnnt 13.523794 hw_loss 0.117759 lr 0.00025631 rank 1
2023-03-01 20:20:28,662 DEBUG TRAIN Batch 56/3900 loss 7.872196 loss_att 12.076888 loss_ctc 7.188511 loss_rnnt 7.017503 hw_loss 0.196711 lr 0.00025632 rank 0
2023-03-01 20:21:00,278 DEBUG TRAIN Batch 56/4000 loss 3.244081 loss_att 7.331009 loss_ctc 6.495473 loss_rnnt 1.886370 hw_loss 0.200260 lr 0.00025631 rank 1
2023-03-01 20:21:00,280 DEBUG TRAIN Batch 56/4000 loss 4.152265 loss_att 6.414002 loss_ctc 8.504089 loss_rnnt 3.049500 hw_loss 0.131575 lr 0.00025631 rank 0
2023-03-01 20:21:21,132 DEBUG TRAIN Batch 56/4100 loss 7.018893 loss_att 9.348450 loss_ctc 13.414949 loss_rnnt 5.612724 hw_loss 0.163969 lr 0.00025630 rank 1
2023-03-01 20:21:21,133 DEBUG TRAIN Batch 56/4100 loss 6.127545 loss_att 10.343204 loss_ctc 13.317900 loss_rnnt 4.234323 hw_loss 0.171331 lr 0.00025630 rank 0
2023-03-01 20:21:42,710 DEBUG TRAIN Batch 56/4200 loss 4.031032 loss_att 6.067348 loss_ctc 7.444419 loss_rnnt 3.082992 hw_loss 0.160608 lr 0.00025629 rank 1
2023-03-01 20:21:42,713 DEBUG TRAIN Batch 56/4200 loss 3.198194 loss_att 6.204597 loss_ctc 5.638213 loss_rnnt 2.139010 hw_loss 0.248564 lr 0.00025629 rank 0
2023-03-01 20:22:15,155 DEBUG TRAIN Batch 56/4300 loss 12.136135 loss_att 18.055418 loss_ctc 18.628284 loss_rnnt 9.996431 hw_loss 0.169174 lr 0.00025628 rank 1
2023-03-01 20:22:15,157 DEBUG TRAIN Batch 56/4300 loss 4.397357 loss_att 7.402454 loss_ctc 7.323046 loss_rnnt 3.302494 hw_loss 0.194536 lr 0.00025628 rank 0
2023-03-01 20:22:37,950 DEBUG TRAIN Batch 56/4400 loss 7.193784 loss_att 9.841858 loss_ctc 13.676661 loss_rnnt 5.676157 hw_loss 0.231803 lr 0.00025627 rank 1
2023-03-01 20:22:37,951 DEBUG TRAIN Batch 56/4400 loss 5.332837 loss_att 6.558523 loss_ctc 9.526651 loss_rnnt 4.337392 hw_loss 0.358374 lr 0.00025628 rank 0
2023-03-01 20:23:00,103 DEBUG TRAIN Batch 56/4500 loss 2.771685 loss_att 7.899068 loss_ctc 6.791885 loss_rnnt 1.141930 hw_loss 0.127971 lr 0.00025626 rank 1
2023-03-01 20:23:00,104 DEBUG TRAIN Batch 56/4500 loss 5.411429 loss_att 8.667573 loss_ctc 6.460377 loss_rnnt 4.538696 hw_loss 0.153082 lr 0.00025627 rank 0
2023-03-01 20:23:22,220 DEBUG TRAIN Batch 56/4600 loss 3.246327 loss_att 6.800573 loss_ctc 8.173653 loss_rnnt 1.787599 hw_loss 0.170440 lr 0.00025625 rank 1
2023-03-01 20:23:22,221 DEBUG TRAIN Batch 56/4600 loss 10.966459 loss_att 13.670218 loss_ctc 18.859930 loss_rnnt 9.255665 hw_loss 0.220461 lr 0.00025626 rank 0
2023-03-01 20:23:54,229 DEBUG TRAIN Batch 56/4700 loss 9.159288 loss_att 11.287098 loss_ctc 11.183314 loss_rnnt 8.370429 hw_loss 0.175175 lr 0.00025625 rank 1
2023-03-01 20:23:54,231 DEBUG TRAIN Batch 56/4700 loss 10.164872 loss_att 16.355232 loss_ctc 24.304804 loss_rnnt 6.908977 hw_loss 0.248438 lr 0.00025625 rank 0
2023-03-01 20:24:16,080 DEBUG TRAIN Batch 56/4800 loss 4.994583 loss_att 7.609376 loss_ctc 7.099023 loss_rnnt 4.109432 hw_loss 0.153000 lr 0.00025624 rank 1
2023-03-01 20:24:16,081 DEBUG TRAIN Batch 56/4800 loss 11.227150 loss_att 16.608509 loss_ctc 14.870952 loss_rnnt 9.557286 hw_loss 0.202034 lr 0.00025624 rank 0
2023-03-01 20:24:38,557 DEBUG TRAIN Batch 56/4900 loss 8.495817 loss_att 12.201481 loss_ctc 15.426305 loss_rnnt 6.731311 hw_loss 0.186202 lr 0.00025623 rank 1
2023-03-01 20:24:38,558 DEBUG TRAIN Batch 56/4900 loss 6.253466 loss_att 7.246394 loss_ctc 8.761964 loss_rnnt 5.633837 hw_loss 0.162332 lr 0.00025623 rank 0
2023-03-01 20:25:11,918 DEBUG TRAIN Batch 56/5000 loss 12.083387 loss_att 12.795262 loss_ctc 22.072235 loss_rnnt 10.532106 hw_loss 0.144486 lr 0.00025622 rank 1
2023-03-01 20:25:11,920 DEBUG TRAIN Batch 56/5000 loss 10.183962 loss_att 10.562511 loss_ctc 17.279902 loss_rnnt 9.074858 hw_loss 0.163626 lr 0.00025623 rank 0
2023-03-01 20:25:34,153 DEBUG TRAIN Batch 56/5100 loss 3.539066 loss_att 5.792416 loss_ctc 8.119014 loss_rnnt 2.305492 hw_loss 0.322959 lr 0.00025621 rank 1
2023-03-01 20:25:34,155 DEBUG TRAIN Batch 56/5100 loss 6.498815 loss_att 10.251511 loss_ctc 8.100813 loss_rnnt 5.471336 hw_loss 0.118762 lr 0.00025622 rank 0
2023-03-01 20:25:56,639 DEBUG TRAIN Batch 56/5200 loss 1.966753 loss_att 4.256706 loss_ctc 7.390248 loss_rnnt 0.710169 hw_loss 0.141488 lr 0.00025620 rank 1
2023-03-01 20:25:56,640 DEBUG TRAIN Batch 56/5200 loss 7.976151 loss_att 14.380627 loss_ctc 15.927696 loss_rnnt 5.572505 hw_loss 0.117271 lr 0.00025621 rank 0
2023-03-01 20:26:19,546 DEBUG TRAIN Batch 56/5300 loss 5.257964 loss_att 8.136080 loss_ctc 11.102936 loss_rnnt 3.799938 hw_loss 0.193261 lr 0.00025620 rank 1
2023-03-01 20:26:19,549 DEBUG TRAIN Batch 56/5300 loss 9.046173 loss_att 10.748266 loss_ctc 15.039173 loss_rnnt 7.796950 hw_loss 0.205759 lr 0.00025620 rank 0
2023-03-01 20:26:51,441 DEBUG TRAIN Batch 56/5400 loss 7.460106 loss_att 8.643903 loss_ctc 9.007584 loss_rnnt 6.848171 hw_loss 0.316586 lr 0.00025619 rank 1
2023-03-01 20:26:51,444 DEBUG TRAIN Batch 56/5400 loss 8.787088 loss_att 10.022466 loss_ctc 11.870129 loss_rnnt 8.057373 hw_loss 0.134189 lr 0.00025619 rank 0
2023-03-01 20:27:14,185 DEBUG TRAIN Batch 56/5500 loss 11.216870 loss_att 12.559278 loss_ctc 18.897717 loss_rnnt 9.765937 hw_loss 0.296886 lr 0.00025618 rank 1
2023-03-01 20:27:14,187 DEBUG TRAIN Batch 56/5500 loss 4.048851 loss_att 7.982822 loss_ctc 6.953148 loss_rnnt 2.735233 hw_loss 0.261718 lr 0.00025618 rank 0
2023-03-01 20:27:37,167 DEBUG TRAIN Batch 56/5600 loss 6.431494 loss_att 8.079041 loss_ctc 11.357744 loss_rnnt 5.325094 hw_loss 0.225106 lr 0.00025617 rank 0
2023-03-01 20:27:37,167 DEBUG TRAIN Batch 56/5600 loss 3.552629 loss_att 4.377528 loss_ctc 5.494616 loss_rnnt 3.026639 hw_loss 0.191397 lr 0.00025617 rank 1
2023-03-01 20:28:09,122 DEBUG TRAIN Batch 56/5700 loss 9.776943 loss_att 11.590563 loss_ctc 15.323474 loss_rnnt 8.505760 hw_loss 0.316727 lr 0.00025616 rank 1
2023-03-01 20:28:09,124 DEBUG TRAIN Batch 56/5700 loss 6.487970 loss_att 9.111752 loss_ctc 11.854281 loss_rnnt 5.213871 hw_loss 0.063439 lr 0.00025617 rank 0
2023-03-01 20:28:31,752 DEBUG TRAIN Batch 56/5800 loss 6.638815 loss_att 8.179686 loss_ctc 10.476236 loss_rnnt 5.777277 hw_loss 0.078203 lr 0.00025615 rank 1
2023-03-01 20:28:31,753 DEBUG TRAIN Batch 56/5800 loss 4.059440 loss_att 8.499013 loss_ctc 8.461677 loss_rnnt 2.509360 hw_loss 0.141000 lr 0.00025616 rank 0
2023-03-01 20:28:54,250 DEBUG TRAIN Batch 56/5900 loss 6.724164 loss_att 9.086083 loss_ctc 10.456424 loss_rnnt 5.627056 hw_loss 0.238294 lr 0.00025615 rank 1
2023-03-01 20:28:54,252 DEBUG TRAIN Batch 56/5900 loss 7.179771 loss_att 9.776424 loss_ctc 10.260051 loss_rnnt 6.155622 hw_loss 0.176466 lr 0.00025615 rank 0
2023-03-01 20:29:17,021 DEBUG TRAIN Batch 56/6000 loss 2.471807 loss_att 6.169281 loss_ctc 7.380779 loss_rnnt 0.904569 hw_loss 0.324774 lr 0.00025614 rank 1
2023-03-01 20:29:17,023 DEBUG TRAIN Batch 56/6000 loss 12.470724 loss_att 15.623075 loss_ctc 17.157621 loss_rnnt 11.150396 hw_loss 0.121760 lr 0.00025614 rank 0
2023-03-01 20:29:49,705 DEBUG TRAIN Batch 56/6100 loss 10.424420 loss_att 12.508703 loss_ctc 15.402857 loss_rnnt 9.202228 hw_loss 0.265396 lr 0.00025613 rank 1
2023-03-01 20:29:49,707 DEBUG TRAIN Batch 56/6100 loss 4.045492 loss_att 8.225443 loss_ctc 8.291525 loss_rnnt 2.530645 hw_loss 0.211348 lr 0.00025613 rank 0
2023-03-01 20:30:12,211 DEBUG TRAIN Batch 56/6200 loss 7.769265 loss_att 10.094210 loss_ctc 11.787578 loss_rnnt 6.651140 hw_loss 0.220051 lr 0.00025612 rank 1
2023-03-01 20:30:12,212 DEBUG TRAIN Batch 56/6200 loss 11.976750 loss_att 14.067786 loss_ctc 16.788778 loss_rnnt 10.795934 hw_loss 0.226887 lr 0.00025612 rank 0
2023-03-01 20:30:35,345 DEBUG TRAIN Batch 56/6300 loss 2.187346 loss_att 4.428683 loss_ctc 3.382879 loss_rnnt 1.433955 hw_loss 0.273225 lr 0.00025611 rank 1
2023-03-01 20:30:35,346 DEBUG TRAIN Batch 56/6300 loss 6.145585 loss_att 11.705226 loss_ctc 13.231753 loss_rnnt 3.995131 hw_loss 0.175693 lr 0.00025612 rank 0
2023-03-01 20:31:08,192 DEBUG TRAIN Batch 56/6400 loss 8.064095 loss_att 9.183699 loss_ctc 11.681408 loss_rnnt 7.173246 hw_loss 0.346162 lr 0.00025610 rank 1
2023-03-01 20:31:08,195 DEBUG TRAIN Batch 56/6400 loss 13.257113 loss_att 15.973586 loss_ctc 24.035803 loss_rnnt 11.166113 hw_loss 0.207275 lr 0.00025611 rank 0
2023-03-01 20:31:31,047 DEBUG TRAIN Batch 56/6500 loss 4.972686 loss_att 7.386250 loss_ctc 9.151394 loss_rnnt 3.872241 hw_loss 0.113571 lr 0.00025609 rank 1
2023-03-01 20:31:31,047 DEBUG TRAIN Batch 56/6500 loss 5.700357 loss_att 7.065300 loss_ctc 8.781012 loss_rnnt 4.944285 hw_loss 0.135619 lr 0.00025610 rank 0
2023-03-01 20:31:53,638 DEBUG TRAIN Batch 56/6600 loss 11.905542 loss_att 16.671215 loss_ctc 16.024357 loss_rnnt 10.215937 hw_loss 0.351181 lr 0.00025609 rank 1
2023-03-01 20:31:53,640 DEBUG TRAIN Batch 56/6600 loss 6.384976 loss_att 8.652433 loss_ctc 8.967753 loss_rnnt 5.489618 hw_loss 0.182807 lr 0.00025609 rank 0
2023-03-01 20:32:16,136 DEBUG TRAIN Batch 56/6700 loss 6.218544 loss_att 11.128201 loss_ctc 13.339201 loss_rnnt 4.246632 hw_loss 0.076052 lr 0.00025608 rank 0
2023-03-01 20:32:16,136 DEBUG TRAIN Batch 56/6700 loss 8.534535 loss_att 12.636518 loss_ctc 12.452709 loss_rnnt 7.083597 hw_loss 0.202719 lr 0.00025608 rank 1
2023-03-01 20:32:48,840 DEBUG TRAIN Batch 56/6800 loss 8.210495 loss_att 11.877693 loss_ctc 13.592974 loss_rnnt 6.726970 hw_loss 0.060793 lr 0.00025607 rank 1
2023-03-01 20:32:48,842 DEBUG TRAIN Batch 56/6800 loss 6.413778 loss_att 9.662597 loss_ctc 9.663145 loss_rnnt 5.262507 hw_loss 0.127983 lr 0.00025607 rank 0
2023-03-01 20:33:12,010 DEBUG TRAIN Batch 56/6900 loss 6.810837 loss_att 9.997591 loss_ctc 12.343056 loss_rnnt 5.274210 hw_loss 0.303089 lr 0.00025606 rank 1
2023-03-01 20:33:12,011 DEBUG TRAIN Batch 56/6900 loss 6.743028 loss_att 8.249985 loss_ctc 13.482718 loss_rnnt 5.373041 hw_loss 0.318694 lr 0.00025607 rank 0
2023-03-01 20:33:34,717 DEBUG TRAIN Batch 56/7000 loss 7.540738 loss_att 8.318675 loss_ctc 11.419116 loss_rnnt 6.738838 hw_loss 0.242241 lr 0.00025605 rank 1
2023-03-01 20:33:34,717 DEBUG TRAIN Batch 56/7000 loss 4.757026 loss_att 8.536131 loss_ctc 8.505730 loss_rnnt 3.382937 hw_loss 0.222076 lr 0.00025606 rank 0
2023-03-01 20:33:57,870 DEBUG TRAIN Batch 56/7100 loss 3.452548 loss_att 5.778773 loss_ctc 4.687759 loss_rnnt 2.791701 hw_loss 0.057949 lr 0.00025604 rank 1
2023-03-01 20:33:57,870 DEBUG TRAIN Batch 56/7100 loss 6.317933 loss_att 9.096590 loss_ctc 12.278419 loss_rnnt 4.898272 hw_loss 0.129746 lr 0.00025605 rank 0
2023-03-01 20:34:30,778 DEBUG TRAIN Batch 56/7200 loss 7.819901 loss_att 10.624359 loss_ctc 9.584693 loss_rnnt 6.874207 hw_loss 0.280307 lr 0.00025604 rank 1
2023-03-01 20:34:30,781 DEBUG TRAIN Batch 56/7200 loss 7.689381 loss_att 11.567067 loss_ctc 15.396100 loss_rnnt 5.797210 hw_loss 0.167007 lr 0.00025604 rank 0
2023-03-01 20:34:53,041 DEBUG TRAIN Batch 56/7300 loss 5.680521 loss_att 7.727400 loss_ctc 11.233217 loss_rnnt 4.474993 hw_loss 0.104611 lr 0.00025603 rank 1
2023-03-01 20:34:53,043 DEBUG TRAIN Batch 56/7300 loss 6.789684 loss_att 9.157582 loss_ctc 10.611868 loss_rnnt 5.806241 hw_loss 0.000447 lr 0.00025603 rank 0
2023-03-01 20:35:15,609 DEBUG TRAIN Batch 56/7400 loss 1.330378 loss_att 3.810946 loss_ctc 2.409035 loss_rnnt 0.585946 hw_loss 0.195932 lr 0.00025602 rank 1
2023-03-01 20:35:15,610 DEBUG TRAIN Batch 56/7400 loss 5.215666 loss_att 7.838546 loss_ctc 10.300945 loss_rnnt 3.856713 hw_loss 0.293138 lr 0.00025602 rank 0
2023-03-01 20:35:48,051 DEBUG TRAIN Batch 56/7500 loss 4.176363 loss_att 6.480699 loss_ctc 4.234293 loss_rnnt 3.617349 hw_loss 0.169542 lr 0.00025601 rank 1
2023-03-01 20:35:48,052 DEBUG TRAIN Batch 56/7500 loss 7.060213 loss_att 7.170380 loss_ctc 9.009051 loss_rnnt 6.729267 hw_loss 0.092001 lr 0.00025602 rank 0
2023-03-01 20:36:10,945 DEBUG TRAIN Batch 56/7600 loss 8.891730 loss_att 9.584965 loss_ctc 11.291937 loss_rnnt 8.334569 hw_loss 0.184661 lr 0.00025600 rank 1
2023-03-01 20:36:10,948 DEBUG TRAIN Batch 56/7600 loss 6.690470 loss_att 9.155174 loss_ctc 8.625303 loss_rnnt 5.823555 hw_loss 0.217494 lr 0.00025601 rank 0
2023-03-01 20:36:33,691 DEBUG TRAIN Batch 56/7700 loss 3.895935 loss_att 6.931714 loss_ctc 7.528348 loss_rnnt 2.726695 hw_loss 0.145805 lr 0.00025599 rank 1
2023-03-01 20:36:33,691 DEBUG TRAIN Batch 56/7700 loss 5.603919 loss_att 7.254115 loss_ctc 8.600101 loss_rnnt 4.700050 hw_loss 0.326886 lr 0.00025600 rank 0
2023-03-01 20:36:56,356 DEBUG TRAIN Batch 56/7800 loss 3.736432 loss_att 7.439843 loss_ctc 7.401621 loss_rnnt 2.358239 hw_loss 0.279035 lr 0.00025599 rank 1
2023-03-01 20:36:56,357 DEBUG TRAIN Batch 56/7800 loss 8.946350 loss_att 12.962749 loss_ctc 13.007347 loss_rnnt 7.518363 hw_loss 0.156079 lr 0.00025599 rank 0
2023-03-01 20:37:28,916 DEBUG TRAIN Batch 56/7900 loss 5.986887 loss_att 14.387654 loss_ctc 13.804573 loss_rnnt 3.173740 hw_loss 0.169940 lr 0.00025598 rank 1
2023-03-01 20:37:28,917 DEBUG TRAIN Batch 56/7900 loss 8.525648 loss_att 11.746450 loss_ctc 10.980944 loss_rnnt 7.501184 hw_loss 0.099245 lr 0.00025598 rank 0
2023-03-01 20:37:51,124 DEBUG TRAIN Batch 56/8000 loss 5.140068 loss_att 8.188339 loss_ctc 7.246675 loss_rnnt 4.195636 hw_loss 0.101055 lr 0.00025597 rank 1
2023-03-01 20:37:51,125 DEBUG TRAIN Batch 56/8000 loss 3.746761 loss_att 6.255721 loss_ctc 4.229715 loss_rnnt 3.160421 hw_loss 0.037788 lr 0.00025597 rank 0
2023-03-01 20:38:14,503 DEBUG TRAIN Batch 56/8100 loss 6.783309 loss_att 10.673203 loss_ctc 12.032267 loss_rnnt 5.202159 hw_loss 0.193708 lr 0.00025596 rank 1
2023-03-01 20:38:14,504 DEBUG TRAIN Batch 56/8100 loss 7.354884 loss_att 8.153783 loss_ctc 10.565860 loss_rnnt 6.587764 hw_loss 0.336020 lr 0.00025596 rank 0
2023-03-01 20:38:47,615 DEBUG TRAIN Batch 56/8200 loss 9.998253 loss_att 10.102203 loss_ctc 14.589429 loss_rnnt 9.252021 hw_loss 0.212408 lr 0.00025595 rank 1
2023-03-01 20:38:47,618 DEBUG TRAIN Batch 56/8200 loss 4.876688 loss_att 7.676654 loss_ctc 6.870535 loss_rnnt 3.898335 hw_loss 0.285961 lr 0.00025596 rank 0
2023-03-01 20:39:10,488 DEBUG TRAIN Batch 56/8300 loss 3.311862 loss_att 5.163075 loss_ctc 4.525566 loss_rnnt 2.746773 hw_loss 0.061909 lr 0.00025595 rank 0
2023-03-01 20:39:10,489 DEBUG TRAIN Batch 56/8300 loss 8.133365 loss_att 9.276601 loss_ctc 11.408643 loss_rnnt 7.361206 hw_loss 0.200264 lr 0.00025594 rank 1
2023-03-01 20:39:32,918 DEBUG TRAIN Batch 56/8400 loss 10.888349 loss_att 14.727581 loss_ctc 16.720352 loss_rnnt 9.169453 hw_loss 0.325218 lr 0.00025594 rank 1
2023-03-01 20:39:32,918 DEBUG TRAIN Batch 56/8400 loss 6.623055 loss_att 10.560180 loss_ctc 9.671758 loss_rnnt 5.346931 hw_loss 0.154134 lr 0.00025594 rank 0
2023-03-01 20:39:55,810 DEBUG TRAIN Batch 56/8500 loss 6.663896 loss_att 9.786826 loss_ctc 10.390059 loss_rnnt 5.482394 hw_loss 0.112678 lr 0.00025593 rank 1
2023-03-01 20:39:55,812 DEBUG TRAIN Batch 56/8500 loss 5.404248 loss_att 8.462124 loss_ctc 7.389127 loss_rnnt 4.451945 hw_loss 0.142644 lr 0.00025593 rank 0
2023-03-01 20:40:27,612 DEBUG TRAIN Batch 56/8600 loss 11.269190 loss_att 13.839201 loss_ctc 19.137331 loss_rnnt 9.587502 hw_loss 0.222375 lr 0.00025592 rank 1
2023-03-01 20:40:27,616 DEBUG TRAIN Batch 56/8600 loss 5.214846 loss_att 7.584170 loss_ctc 8.846590 loss_rnnt 4.135526 hw_loss 0.227292 lr 0.00025592 rank 0
2023-03-01 20:40:49,819 DEBUG TRAIN Batch 56/8700 loss 5.986339 loss_att 10.080870 loss_ctc 8.925549 loss_rnnt 4.701188 hw_loss 0.139406 lr 0.00025591 rank 1
2023-03-01 20:40:49,821 DEBUG TRAIN Batch 56/8700 loss 5.079256 loss_att 7.929259 loss_ctc 11.879885 loss_rnnt 3.518510 hw_loss 0.157490 lr 0.00025591 rank 0
2023-03-01 20:41:12,371 DEBUG TRAIN Batch 56/8800 loss 5.338223 loss_att 9.364630 loss_ctc 9.505278 loss_rnnt 3.886324 hw_loss 0.170644 lr 0.00025590 rank 1
2023-03-01 20:41:12,371 DEBUG TRAIN Batch 56/8800 loss 3.188108 loss_att 4.196927 loss_ctc 5.690033 loss_rnnt 2.520830 hw_loss 0.247357 lr 0.00025591 rank 0
2023-03-01 20:41:45,296 DEBUG TRAIN Batch 56/8900 loss 5.528069 loss_att 7.438611 loss_ctc 9.705665 loss_rnnt 4.532304 hw_loss 0.106208 lr 0.00025589 rank 1
2023-03-01 20:41:45,297 DEBUG TRAIN Batch 56/8900 loss 8.858779 loss_att 13.525898 loss_ctc 15.515079 loss_rnnt 6.956033 hw_loss 0.153404 lr 0.00025590 rank 0
2023-03-01 20:42:07,835 DEBUG TRAIN Batch 56/9000 loss 4.548015 loss_att 7.333855 loss_ctc 9.294143 loss_rnnt 3.270035 hw_loss 0.164991 lr 0.00025589 rank 1
2023-03-01 20:42:07,835 DEBUG TRAIN Batch 56/9000 loss 1.917952 loss_att 3.990093 loss_ctc 4.525196 loss_rnnt 0.970363 hw_loss 0.347865 lr 0.00025589 rank 0
2023-03-01 20:42:30,387 DEBUG TRAIN Batch 56/9100 loss 3.594599 loss_att 5.332052 loss_ctc 3.998859 loss_rnnt 3.117515 hw_loss 0.141925 lr 0.00025588 rank 1
2023-03-01 20:42:30,389 DEBUG TRAIN Batch 56/9100 loss 1.994451 loss_att 4.086528 loss_ctc 5.088414 loss_rnnt 0.997883 hw_loss 0.310544 lr 0.00025588 rank 0
2023-03-01 20:42:53,044 DEBUG TRAIN Batch 56/9200 loss 17.067678 loss_att 19.114544 loss_ctc 25.785019 loss_rnnt 15.495800 hw_loss 0.000361 lr 0.00025587 rank 1
2023-03-01 20:42:53,046 DEBUG TRAIN Batch 56/9200 loss 9.544626 loss_att 12.645083 loss_ctc 16.029701 loss_rnnt 7.995052 hw_loss 0.121511 lr 0.00025587 rank 0
2023-03-01 20:43:26,407 DEBUG TRAIN Batch 56/9300 loss 4.251770 loss_att 7.161136 loss_ctc 9.303432 loss_rnnt 2.911129 hw_loss 0.159775 lr 0.00025586 rank 1
2023-03-01 20:43:26,408 DEBUG TRAIN Batch 56/9300 loss 7.991456 loss_att 11.771322 loss_ctc 14.491669 loss_rnnt 6.209208 hw_loss 0.299212 lr 0.00025586 rank 0
2023-03-01 20:43:49,403 DEBUG TRAIN Batch 56/9400 loss 4.950698 loss_att 7.257665 loss_ctc 8.404566 loss_rnnt 3.949039 hw_loss 0.149531 lr 0.00025585 rank 1
2023-03-01 20:43:49,404 DEBUG TRAIN Batch 56/9400 loss 5.505712 loss_att 8.072826 loss_ctc 10.653704 loss_rnnt 4.206466 hw_loss 0.186421 lr 0.00025586 rank 0
2023-03-01 20:44:12,778 DEBUG TRAIN Batch 56/9500 loss 8.258443 loss_att 10.636548 loss_ctc 14.674479 loss_rnnt 6.783107 hw_loss 0.270456 lr 0.00025584 rank 1
2023-03-01 20:44:12,779 DEBUG TRAIN Batch 56/9500 loss 6.248883 loss_att 9.815605 loss_ctc 11.486967 loss_rnnt 4.666163 hw_loss 0.320558 lr 0.00025585 rank 0
2023-03-01 20:44:45,534 DEBUG TRAIN Batch 56/9600 loss 4.644324 loss_att 8.403046 loss_ctc 9.499438 loss_rnnt 3.126408 hw_loss 0.222795 lr 0.00025584 rank 0
2023-03-01 20:44:45,538 DEBUG TRAIN Batch 56/9600 loss 5.402064 loss_att 6.139654 loss_ctc 8.619114 loss_rnnt 4.702085 hw_loss 0.231604 lr 0.00025583 rank 1
2023-03-01 20:45:08,202 DEBUG TRAIN Batch 56/9700 loss 2.207283 loss_att 6.887612 loss_ctc 5.898414 loss_rnnt 0.710101 hw_loss 0.129310 lr 0.00025583 rank 1
2023-03-01 20:45:08,204 DEBUG TRAIN Batch 56/9700 loss 2.356243 loss_att 7.027232 loss_ctc 7.313959 loss_rnnt 0.760848 hw_loss 0.000318 lr 0.00025583 rank 0
2023-03-01 20:45:30,597 DEBUG TRAIN Batch 56/9800 loss 8.966228 loss_att 16.599018 loss_ctc 21.020336 loss_rnnt 5.668170 hw_loss 0.308033 lr 0.00025582 rank 1
2023-03-01 20:45:30,600 DEBUG TRAIN Batch 56/9800 loss 10.636979 loss_att 14.141096 loss_ctc 17.534786 loss_rnnt 8.983109 hw_loss 0.062512 lr 0.00025582 rank 0
2023-03-01 20:45:53,228 DEBUG TRAIN Batch 56/9900 loss 6.153300 loss_att 13.099020 loss_ctc 14.864952 loss_rnnt 3.526350 hw_loss 0.142971 lr 0.00025581 rank 1
2023-03-01 20:45:53,230 DEBUG TRAIN Batch 56/9900 loss 7.501754 loss_att 10.343590 loss_ctc 12.857277 loss_rnnt 6.173144 hw_loss 0.086575 lr 0.00025581 rank 0
2023-03-01 20:46:25,318 DEBUG TRAIN Batch 56/10000 loss 6.762559 loss_att 10.957584 loss_ctc 11.971109 loss_rnnt 5.066932 hw_loss 0.304031 lr 0.00025580 rank 1
2023-03-01 20:46:25,320 DEBUG TRAIN Batch 56/10000 loss 5.335715 loss_att 7.694873 loss_ctc 8.625038 loss_rnnt 4.358365 hw_loss 0.125516 lr 0.00025581 rank 0
2023-03-01 20:46:48,022 DEBUG TRAIN Batch 56/10100 loss 11.603426 loss_att 14.334396 loss_ctc 23.598217 loss_rnnt 9.406453 hw_loss 0.096513 lr 0.00025579 rank 1
2023-03-01 20:46:48,023 DEBUG TRAIN Batch 56/10100 loss 9.011655 loss_att 9.437288 loss_ctc 12.486663 loss_rnnt 8.429368 hw_loss 0.063422 lr 0.00025580 rank 0
2023-03-01 20:47:10,747 DEBUG TRAIN Batch 56/10200 loss 3.801462 loss_att 5.080086 loss_ctc 7.262340 loss_rnnt 3.001455 hw_loss 0.155308 lr 0.00025578 rank 1
2023-03-01 20:47:10,749 DEBUG TRAIN Batch 56/10200 loss 6.172417 loss_att 7.770991 loss_ctc 7.698727 loss_rnnt 5.554131 hw_loss 0.178245 lr 0.00025579 rank 0
2023-03-01 20:47:33,427 DEBUG TRAIN Batch 56/10300 loss 5.294345 loss_att 8.148179 loss_ctc 8.469154 loss_rnnt 4.230108 hw_loss 0.131555 lr 0.00025578 rank 1
2023-03-01 20:47:33,427 DEBUG TRAIN Batch 56/10300 loss 9.383630 loss_att 11.948009 loss_ctc 14.882467 loss_rnnt 8.056289 hw_loss 0.152414 lr 0.00025578 rank 0
2023-03-01 20:48:05,766 DEBUG TRAIN Batch 56/10400 loss 3.959301 loss_att 7.147070 loss_ctc 9.482989 loss_rnnt 2.486976 hw_loss 0.184274 lr 0.00025577 rank 1
2023-03-01 20:48:05,768 DEBUG TRAIN Batch 56/10400 loss 3.315982 loss_att 5.604317 loss_ctc 4.533216 loss_rnnt 2.612275 hw_loss 0.157017 lr 0.00025577 rank 0
2023-03-01 20:48:28,158 DEBUG TRAIN Batch 56/10500 loss 6.952533 loss_att 8.552471 loss_ctc 14.144470 loss_rnnt 5.555969 hw_loss 0.220595 lr 0.00025576 rank 1
2023-03-01 20:48:28,160 DEBUG TRAIN Batch 56/10500 loss 5.861368 loss_att 6.868141 loss_ctc 14.485317 loss_rnnt 4.421638 hw_loss 0.165966 lr 0.00025576 rank 0
2023-03-01 20:48:50,681 DEBUG TRAIN Batch 56/10600 loss 4.658636 loss_att 8.332936 loss_ctc 9.497801 loss_rnnt 3.161571 hw_loss 0.219341 lr 0.00025575 rank 1
2023-03-01 20:48:50,681 DEBUG TRAIN Batch 56/10600 loss 10.840833 loss_att 13.866671 loss_ctc 16.085938 loss_rnnt 9.357752 hw_loss 0.334811 lr 0.00025576 rank 0
2023-03-01 20:49:23,595 DEBUG TRAIN Batch 56/10700 loss 5.246725 loss_att 7.944631 loss_ctc 9.113686 loss_rnnt 4.120476 hw_loss 0.133262 lr 0.00025574 rank 1
2023-03-01 20:49:23,596 DEBUG TRAIN Batch 56/10700 loss 3.964760 loss_att 5.377740 loss_ctc 7.082122 loss_rnnt 3.118769 hw_loss 0.277024 lr 0.00025575 rank 0
2023-03-01 20:49:46,799 DEBUG TRAIN Batch 56/10800 loss 5.286064 loss_att 6.469406 loss_ctc 8.646921 loss_rnnt 4.499512 hw_loss 0.190817 lr 0.00025573 rank 1
2023-03-01 20:49:46,800 DEBUG TRAIN Batch 56/10800 loss 10.862068 loss_att 15.477411 loss_ctc 20.591290 loss_rnnt 8.609524 hw_loss 0.060463 lr 0.00025574 rank 0
